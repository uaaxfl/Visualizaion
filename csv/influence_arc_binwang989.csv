2020.autosimtrans-1.5,P18-1118,0,0.0638382,"s Figure 1: The architecture of our proposed encoder 2 Related Work the discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the document (Voita et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2018), while some researchers employed hierarchical context networks to catch document context information for Transformer (Miculicich et al., 2018"
2020.autosimtrans-1.5,D18-1325,0,0.679378,"016; Nejat et al., 2017). However, to the best of our knowledge, discourse structure has not been explicitly used in document-level NMT. Recently, document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structure information. Our encoder is based on a hierarchical attention network (HAN) (Miculicich et al., 2018). Specifically, we first parse the input document to obtain its discourse structure. Then, we introduce a Transformer-based path encoder to embed the discourse structure information of each word. Finally, we combine the discourse structure information with the word embedding before it is fed into the encoder. Experimental results on the Englishto-German dataset show that our model can significantly outperform both Transformer and Transformer+HAN. 1 To address the above problem, we propose to improve document-level NMT with the aid of discourse structure information. First, we represent each in"
2020.autosimtrans-1.5,C18-1203,0,0.0264746,"rd using its discourse path from root node to its corresponding leaf node. Each path is a mixed label sequence composed of the discourse relationship and the importance label (e.g., NUCLEUS ELABORATION, SATELLITE BACKGROUND). Please note that all tokens in the same EDU share the same discourse 3.3 HAN-based Context Modeling Following (2018), we apply hierarchical attention network (HAN) as our context encoder. Due to the advantage of accurately capturing different levels of contexts, HAN has been widely used in many tasks, such as document classification (Yang et al., 2016), stance detection (Sun et al., 2018), sentencelevel NMT (Su et al., 2018b). Using this encoder, we mainly focus on two levels of context modeling: Sentence-level Context Modeling For the i-th word of the current sentence, we employ muti-head 32 attention (Vaswani et al., 2017) to summarize the context from the k-th context sentence: csi,k = MultiHead(fs (hi ), Hk ), Training Development Test (2) Settings We use Transformer (Vaswani et al., 2017) as our context-agnostic baseline system and Transformer+HAN (Miculicich et al., 2018) as our context-aware baseline system. We conduct experiments using the same configuration as HAN. Sp"
2020.autosimtrans-1.5,W17-5535,0,0.0190985,"ina 2 Xiaomi AI Lab, Xiaomi Inc., Beijing, China {chenjx,zhangjiarui,clzhou}@stu.xmu.edu.cn jssu@xmu.edu.cn {lixiang21,cuijianwei,wangbin11}@xiaomi.com Abstract 2017a; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019; Yang et al., 2019). Discourse structure, as well as raw contextual sentences, is a major component of the document. And it has been proved to be effective in many other tasks, such as automatic document summarization (Yoshida et al., 2014; Isonuma et al., 2019) and sentiment classification (Schouten and Frasincar, 2016; Nejat et al., 2017). However, to the best of our knowledge, discourse structure has not been explicitly used in document-level NMT. Recently, document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structure information. Our encoder is based on a hierarchical attention network (HAN) (Miculicich et al., 2018)"
2020.autosimtrans-1.5,W17-4811,0,0.0446932,"ntext information in a hierarchical manner. Introduction Neural machine translation (NMT) has made great progress in the past decade. In practical applications, the need for NMT systems has expanded from individual sentences to complete documents. Therefore, document-level NMT has gradually drawn much more attention. Contextual information is particularly important for obtaining highquality document translation. To get better contextual information, researchers have proposed many methods (e.g., memory network and hierarchical attention network) for document-level translation (Sim Smith, 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017a; Tu et al., 2017; Wang et al., Our contributions are as follows: (i) We propose a novel and efficient approach to explicitly exploit discourse structure information for documentlevel NMT. Particularly, our approach is applicable for any other context encoder of document-level NMT; (ii) We carry out experiments on English-toGerman translation task and experimental results show that our model outperforms competitive baselines. ∗ This work is done when Junxuan Chen was interning at Xiaomi AI Lab, Xiaomi Inc., Beijing, China. † Corresponding author. 30 Proceedings of the 1st W"
2020.autosimtrans-1.5,P02-1040,0,0.112505,"Specifically, both sentence encoder and decoder are composed of 6 hidden layers, while path encoder is composed of 2 hidden layers. We use three previous sentences as contextual sentences for current sentence. The hidden size and pointwise FFN size are 512 and 2048 respectively. The dropout rates for all hidden states are set to 0.1. The source and target vocabulary sizes are both 30K. At the training phase, we use the Adam optimizer (Kingma and Ba, 2015) and the batch sizes of context-agnostic model and context-aware model are 4096 and 1024, respectively. Finally, we use case-sensitive BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) to measure the translation quality. Document-level Context Modeling Unlike the above modeling, here we mainly on capturing the context information from previous K sentences for the i-th word of the current sentence. (3) where fd is a linear transformation, and CSi = [csi,1 , csi,2 , · · ·, csi,K ] is the sentence-level context of K contextual sentences. Integrating Document-level Context into the Translation Encoder Finally, we integrate the above-mentioned document-level context into the translation encoder via a gating operation: λi = σ(Wh hi + Wcd cdi ) (4) he"
2020.autosimtrans-1.5,Q17-1007,0,0.0376646,"uction Neural machine translation (NMT) has made great progress in the past decade. In practical applications, the need for NMT systems has expanded from individual sentences to complete documents. Therefore, document-level NMT has gradually drawn much more attention. Contextual information is particularly important for obtaining highquality document translation. To get better contextual information, researchers have proposed many methods (e.g., memory network and hierarchical attention network) for document-level translation (Sim Smith, 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017a; Tu et al., 2017; Wang et al., Our contributions are as follows: (i) We propose a novel and efficient approach to explicitly exploit discourse structure information for documentlevel NMT. Particularly, our approach is applicable for any other context encoder of document-level NMT; (ii) We carry out experiments on English-toGerman translation task and experimental results show that our model outperforms competitive baselines. ∗ This work is done when Junxuan Chen was interning at Xiaomi AI Lab, Xiaomi Inc., Beijing, China. † Corresponding author. 30 Proceedings of the 1st Workshop on Automatic Simultaneous Tra"
2020.autosimtrans-1.5,S16-1057,0,0.020639,"Xiamen University, Xiamen, China 2 Xiaomi AI Lab, Xiaomi Inc., Beijing, China {chenjx,zhangjiarui,clzhou}@stu.xmu.edu.cn jssu@xmu.edu.cn {lixiang21,cuijianwei,wangbin11}@xiaomi.com Abstract 2017a; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019; Yang et al., 2019). Discourse structure, as well as raw contextual sentences, is a major component of the document. And it has been proved to be effective in many other tasks, such as automatic document summarization (Yoshida et al., 2014; Isonuma et al., 2019) and sentiment classification (Schouten and Frasincar, 2016; Nejat et al., 2017). However, to the best of our knowledge, discourse structure has not been explicitly used in document-level NMT. Recently, document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structure information. Our encoder is based on a hierarchical attention network (HAN) (Mic"
2020.autosimtrans-1.5,P18-1117,0,0.0696288,"utational Linguistics Figure 1: The architecture of our proposed encoder 2 Related Work the discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the document (Voita et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2018), while some researchers employed hierarchical context networks to catch document context information for Transformer"
2020.autosimtrans-1.5,W17-4814,0,0.0255062,"r to model context information in a hierarchical manner. Introduction Neural machine translation (NMT) has made great progress in the past decade. In practical applications, the need for NMT systems has expanded from individual sentences to complete documents. Therefore, document-level NMT has gradually drawn much more attention. Contextual information is particularly important for obtaining highquality document translation. To get better contextual information, researchers have proposed many methods (e.g., memory network and hierarchical attention network) for document-level translation (Sim Smith, 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017a; Tu et al., 2017; Wang et al., Our contributions are as follows: (i) We propose a novel and efficient approach to explicitly exploit discourse structure information for documentlevel NMT. Particularly, our approach is applicable for any other context encoder of document-level NMT; (ii) We carry out experiments on English-toGerman translation task and experimental results show that our model outperforms competitive baselines. ∗ This work is done when Junxuan Chen was interning at Xiaomi AI Lab, Xiaomi Inc., Beijing, China. † Corresponding autho"
2020.autosimtrans-1.5,2006.amta-papers.25,0,0.0432281,"der and decoder are composed of 6 hidden layers, while path encoder is composed of 2 hidden layers. We use three previous sentences as contextual sentences for current sentence. The hidden size and pointwise FFN size are 512 and 2048 respectively. The dropout rates for all hidden states are set to 0.1. The source and target vocabulary sizes are both 30K. At the training phase, we use the Adam optimizer (Kingma and Ba, 2015) and the batch sizes of context-agnostic model and context-aware model are 4096 and 1024, respectively. Finally, we use case-sensitive BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) to measure the translation quality. Document-level Context Modeling Unlike the above modeling, here we mainly on capturing the context information from previous K sentences for the i-th word of the current sentence. (3) where fd is a linear transformation, and CSi = [csi,1 , csi,2 , · · ·, csi,K ] is the sentence-level context of K contextual sentences. Integrating Document-level Context into the Translation Encoder Finally, we integrate the above-mentioned document-level context into the translation encoder via a gating operation: λi = σ(Wh hi + Wcd cdi ) (4) hei = λi hi + (1 − λi )cdi (5) D"
2020.autosimtrans-1.5,D17-1301,0,0.270329,"hical manner. Introduction Neural machine translation (NMT) has made great progress in the past decade. In practical applications, the need for NMT systems has expanded from individual sentences to complete documents. Therefore, document-level NMT has gradually drawn much more attention. Contextual information is particularly important for obtaining highquality document translation. To get better contextual information, researchers have proposed many methods (e.g., memory network and hierarchical attention network) for document-level translation (Sim Smith, 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017a; Tu et al., 2017; Wang et al., Our contributions are as follows: (i) We propose a novel and efficient approach to explicitly exploit discourse structure information for documentlevel NMT. Particularly, our approach is applicable for any other context encoder of document-level NMT; (ii) We carry out experiments on English-toGerman translation task and experimental results show that our model outperforms competitive baselines. ∗ This work is done when Junxuan Chen was interning at Xiaomi AI Lab, Xiaomi Inc., Beijing, China. † Corresponding author. 30 Proceedings of the 1st Workshop on Automati"
2020.autosimtrans-1.5,P12-1048,1,0.795182,"ijing, China. † Corresponding author. 30 Proceedings of the 1st Workshop on Automatic Simultaneous Translation, pages 30–36 c July 10, 2020. 2020 Association for Computational Linguistics Figure 1: The architecture of our proposed encoder 2 Related Work the discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the document (Voita et al."
2020.autosimtrans-1.5,P17-2029,0,0.0985072,"hical manner. Introduction Neural machine translation (NMT) has made great progress in the past decade. In practical applications, the need for NMT systems has expanded from individual sentences to complete documents. Therefore, document-level NMT has gradually drawn much more attention. Contextual information is particularly important for obtaining highquality document translation. To get better contextual information, researchers have proposed many methods (e.g., memory network and hierarchical attention network) for document-level translation (Sim Smith, 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017a; Tu et al., 2017; Wang et al., Our contributions are as follows: (i) We propose a novel and efficient approach to explicitly exploit discourse structure information for documentlevel NMT. Particularly, our approach is applicable for any other context encoder of document-level NMT; (ii) We carry out experiments on English-toGerman translation task and experimental results show that our model outperforms competitive baselines. ∗ This work is done when Junxuan Chen was interning at Xiaomi AI Lab, Xiaomi Inc., Beijing, China. † Corresponding author. 30 Proceedings of the 1st Workshop on Automati"
2020.autosimtrans-1.5,2011.mtsummit-papers.13,0,0.0299431,"ab, Xiaomi Inc., Beijing, China. † Corresponding author. 30 Proceedings of the 1st Workshop on Automatic Simultaneous Translation, pages 30–36 c July 10, 2020. 2020 Association for Computational Linguistics Figure 1: The architecture of our proposed encoder 2 Related Work the discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the docum"
2020.autosimtrans-1.5,P12-1079,0,0.0311157,"orresponding author. 30 Proceedings of the 1st Workshop on Automatic Simultaneous Translation, pages 30–36 c July 10, 2020. 2020 Association for Computational Linguistics Figure 1: The architecture of our proposed encoder 2 Related Work the discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the document (Voita et al., 2018; Maruf and H"
2020.autosimtrans-1.5,D19-1164,0,0.0122307,"e discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the document (Voita et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2018), while some researchers employed hierarchical context networks to catch document context information for Transformer (Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Specifically, Miculicic"
2020.autosimtrans-1.5,N16-1174,0,0.0850258,"ourse structure information of each word using its discourse path from root node to its corresponding leaf node. Each path is a mixed label sequence composed of the discourse relationship and the importance label (e.g., NUCLEUS ELABORATION, SATELLITE BACKGROUND). Please note that all tokens in the same EDU share the same discourse 3.3 HAN-based Context Modeling Following (2018), we apply hierarchical attention network (HAN) as our context encoder. Due to the advantage of accurately capturing different levels of contexts, HAN has been widely used in many tasks, such as document classification (Yang et al., 2016), stance detection (Sun et al., 2018), sentencelevel NMT (Su et al., 2018b). Using this encoder, we mainly focus on two levels of context modeling: Sentence-level Context Modeling For the i-th word of the current sentence, we employ muti-head 32 attention (Vaswani et al., 2017) to summarize the context from the k-th context sentence: csi,k = MultiHead(fs (hi ), Hk ), Training Development Test (2) Settings We use Transformer (Vaswani et al., 2017) as our context-agnostic baseline system and Transformer+HAN (Miculicich et al., 2018) as our context-aware baseline system. We conduct experiments us"
2020.autosimtrans-1.5,D14-1196,0,0.029133,", Jiarui Zhang1 , Chulun Zhou1 , Jianwei Cui2 , Bin Wang2 , Jinsong Su1† 1 Xiamen University, Xiamen, China 2 Xiaomi AI Lab, Xiaomi Inc., Beijing, China {chenjx,zhangjiarui,clzhou}@stu.xmu.edu.cn jssu@xmu.edu.cn {lixiang21,cuijianwei,wangbin11}@xiaomi.com Abstract 2017a; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019; Yang et al., 2019). Discourse structure, as well as raw contextual sentences, is a major component of the document. And it has been proved to be effective in many other tasks, such as automatic document summarization (Yoshida et al., 2014; Isonuma et al., 2019) and sentiment classification (Schouten and Frasincar, 2016; Nejat et al., 2017). However, to the best of our knowledge, discourse structure has not been explicitly used in document-level NMT. Recently, document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structur"
2020.autosimtrans-1.5,D16-1050,1,0.66979,"Missing"
2020.autosimtrans-1.5,D18-1049,0,0.109063,"Missing"
2020.coling-main.2,N19-1423,0,0.0156195,"aining compared with a pure CMTM (Clark et al., 2020). Besides, our work can also be regarded as a multi-task learning, where we enhance our D ECODER with the bidirectional contextual information as well as the left-to-right correlations of target words. 18 3.3 Length Prediction Typically, an autoregressive NMT model generates the target sentence word-by-word, and thus it decides the length of target sentence by encountering a special token <EOS>. However, our model adopts the strategy of non-autoregressive decoding, namely, it predicts the entire target sentence in a parallel way. Following (Devlin et al., 2019; Ghazvininejad et al., 2019), we add a special token <LEN> to the begining of source input. In this sense, our E NCODER is also required to predict the length of target sentence L, i.e., predict a token from [1, N ] given the source input x, where N is the maximum length of target sentences in our corpus. Mathematically, we define the loss of length prediction as: Llen = N X −(L = i) log p(L|x) (9) i 3.4 Optimization and Inference Overall, the whole model is jointly trained by minimizing the total loss L, which is a combination of Equation 2,8,9: L = Ldec + Llen + Lrev (10) where Ldec = − log"
2020.coling-main.2,D19-1633,0,0.32925,"break the bottleneck of autoregression, several non-autoregressive models have been proposed to induce the decoder to generate all target words simultaneously (Gu et al., 2018; Łukasz Kaiser et al., 2018; Li et al., 2019; Ma et al., 2019). Despite the acceleration of computation efficiency, these models usually suffers from the cost of translation accuracy. Even worse, they decode a target only in one shot, and thus miss a chance to remedy a flawed translation. Against them, a promising research line is to perform refinement on the generated result within several iterations (Lee et al., 2018; Ghazvininejad et al., 2019). Along this line, Ghazvininejad et al. (2019) propose a Mask-Predict decoding strategy, which iteratively refines the generated translation given the most confident target words predicted from the previous iteration. This model is trained using an objective of conditional masked translation modeling (CMTM), by predicting the masked words conditioned on the rest of observed words. However, CMTM just learns from a subset of words instead of the entire target in terms of a training step. As a result, it will iterate more times over the training dataset to explore the contextual relationship with"
2020.coling-main.2,D18-1149,0,0.19902,"et al., 2018). To break the bottleneck of autoregression, several non-autoregressive models have been proposed to induce the decoder to generate all target words simultaneously (Gu et al., 2018; Łukasz Kaiser et al., 2018; Li et al., 2019; Ma et al., 2019). Despite the acceleration of computation efficiency, these models usually suffers from the cost of translation accuracy. Even worse, they decode a target only in one shot, and thus miss a chance to remedy a flawed translation. Against them, a promising research line is to perform refinement on the generated result within several iterations (Lee et al., 2018; Ghazvininejad et al., 2019). Along this line, Ghazvininejad et al. (2019) propose a Mask-Predict decoding strategy, which iteratively refines the generated translation given the most confident target words predicted from the previous iteration. This model is trained using an objective of conditional masked translation modeling (CMTM), by predicting the masked words conditioned on the rest of observed words. However, CMTM just learns from a subset of words instead of the entire target in terms of a training step. As a result, it will iterate more times over the training dataset to explore the"
2020.coling-main.2,D19-1573,0,0.0299041,"Missing"
2020.coling-main.2,D19-1437,0,0.23791,"au et al., 2015; Cho et al., 2014; Kalchbrenner et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Typically, NMTs use autoregressive decoders, where the words are generated one-by-one. However, due to the left-to-right dependency, this computationally-intensive decoding process cannot be easily parallelized, and therefore causes a large latency (Gu et al., 2018). To break the bottleneck of autoregression, several non-autoregressive models have been proposed to induce the decoder to generate all target words simultaneously (Gu et al., 2018; Łukasz Kaiser et al., 2018; Li et al., 2019; Ma et al., 2019). Despite the acceleration of computation efficiency, these models usually suffers from the cost of translation accuracy. Even worse, they decode a target only in one shot, and thus miss a chance to remedy a flawed translation. Against them, a promising research line is to perform refinement on the generated result within several iterations (Lee et al., 2018; Ghazvininejad et al., 2019). Along this line, Ghazvininejad et al. (2019) propose a Mask-Predict decoding strategy, which iteratively refines the generated translation given the most confident target words predicted from the previous iter"
2020.coling-main.2,P02-1040,0,0.10901,"erations. For more details, please refer to (Ghazvininejad et al., 2019). 4 Experiments 4.1 Setting Datasets We conduct experiments on two benchmark datasets, WMT14 En↔De (4.5M sentence pairs) and WMT16 En↔Ro (610k pairs). After preprocessing the two datasets following (Lee et al., 2018), we tokenize them into subword units using BPE (Sennrich et al., 2016). We use newstest-2013 and newstest-2014 as our development and test datasets for WMT14 En↔De, while use newsdev-2016 and newstest-2016 as our development and test datasets for WMT16 En↔Ro. Evaluation Metrics We adopt the widely-used BLEU1 (Papineni et al., 2002) to evaluate the translation accuracy. To compare the training speed, we also use Floating-Point Operations per second (FLOPs)2 to measure the computational complexity. Implementation Details We follow the base configuration of Transformer (Vaswani et al., 2017): The dimension of model is set to 512, and the dimension of inner layers is set to 2048. The E NCODER is consisted of a stack of 6 layers , as well as the D ECODER and A R D ECODER. The weights of our model are all randomly initialized with a uniform distribution N (0, 0.02). Besides, we set the parameters of layer normalization as β ="
2020.coling-main.2,2020.acl-main.15,0,0.0581639,"we can conclude that iterative decoding is an effective technique for non-autoregressive NMTs. Although IRNAT and MaskPredict are able to turn the initial bad translation into a much better one through multiple iterations of decoding, there is still a gap of the translation accuracy when comparing against the SOTA autoregressive model, i.e., Transformer. Still, this deficiency is attributed to the lack of a mechanism or strategy to capture the strong correlations among the target words, which is also the root cause why non-autoregressive models are hard to generated satisfactory translation (Ren et al., 2020). In contrast, our model, which is additionally optimized with our proposed Self-Review Mechanism, significantly achieves a performance boost over these non-autoregressive models. Meanwhile, our model has a huge lead in BLEU on the dataset of WMT 14 EN→DE compared with Seq2Seq and ConvS2s, and even accomplishes comparable performance with Transformer. More specifically, compared with Transformer, our model (w/o kd) achieves 34.54 (+0.26 gains) and 34.36 (+0.37 gains) of BLEU on WMT16 EN→RO and WMT16 RO→EN, respectively. Even with the help of knowledge distillation, our model outperforms Transf"
2020.coling-main.2,P16-1162,0,0.0359902,"arting with an entirely masked target given a new input source. Upon this raw sequence, we conduct refinement by masking-out and re-predicting a subset of words whose probabilities are under a threshold. This refinement is repeated within a heuristic number of iterations. For more details, please refer to (Ghazvininejad et al., 2019). 4 Experiments 4.1 Setting Datasets We conduct experiments on two benchmark datasets, WMT14 En↔De (4.5M sentence pairs) and WMT16 En↔Ro (610k pairs). After preprocessing the two datasets following (Lee et al., 2018), we tokenize them into subword units using BPE (Sennrich et al., 2016). We use newstest-2013 and newstest-2014 as our development and test datasets for WMT14 En↔De, while use newsdev-2016 and newstest-2016 as our development and test datasets for WMT16 En↔Ro. Evaluation Metrics We adopt the widely-used BLEU1 (Papineni et al., 2002) to evaluate the translation accuracy. To compare the training speed, we also use Floating-Point Operations per second (FLOPs)2 to measure the computational complexity. Implementation Details We follow the base configuration of Transformer (Vaswani et al., 2017): The dimension of model is set to 512, and the dimension of inner layers i"
2020.coling-main.340,D18-1017,0,0.62385,"character information into word representations based on sequence labeling. Different from English NER, East Asian languages including Chinese are written without explicit word boundary. One intuitive way to solve this problem is to segment the input sentences into words first, and then to apply word sequence labeling (Yang et al., 2016; He and Sun, 2017a). However, such methods suffer from error propagation between these two subtasks. To overcome this limitation, efforts have been devoted to incorporating word information by leveraging lexicon features and gazetteers (Peng and Dredze, 2015; Cao et al., 2018; Wu et al., 2019; Lin et al., 2019). As recent state-of-the-art (SOTA) lattice-based method, Zhang and Yang (2018) integrated matched lexical words information into character sequence with a directed acyclic graph (DAG) structure using lattice LSTM. While obtaining promising results, this model faces two challenges. First, as an extension to the non-parallelizable sequential LSTM to a DAG structured model, lattice LSTM is restricted to preprocess one character at a time, which can make it infeasibly to deploy. Second, due to the inherently unidirectional sequential nature, lattice LSTM fails"
2020.coling-main.340,N13-1006,0,0.0799335,"Missing"
2020.coling-main.340,N19-1423,0,0.671136,"m to enhance the local dependencies among neighboring tokens. The key insight is to modify the self-attention architecture by replacing the fully-connected topology with a pivot-shared structure. In this particular, every two non-neighboring tokens are connected by a shared pivot node to strengthen the dependency for two neighboring tokens. Experimental results on four datasets demonstrate that our model performs up to 11.4 times faster than baselines and achieves better performance. Furthermore, we show that our model can be easily integrated into the pre-trained language model such as BERT (Devlin et al., 2019), and combining them further improves the state of the art. In summary, this paper makes the following contributions: (1) We investigate lattice transformer encoder for Chinese NER, which is capable of handling lattices in batch mode and capturing dependencies between characters and matched lexical words. (2) We revise lattice-aware attention distribution via a porous mechanism, which enhances the ability of capturing useful local context. (3) Experimental results show that the proposed model is effective and efficient. The source code of this paper can be obtained from https://github.com/stra"
2020.coling-main.340,D19-1096,0,0.169166,"Missing"
2020.coling-main.340,E17-2113,0,0.263991,"al (Berger and Lafferty, 2017), relation extraction (Yu et al., 2019) and entity linking (Xue et al., 2019) require the NER as one of their preprocessing components. Recent studies show that English NER models have achieved improved performance by integrating character information into word representations based on sequence labeling. Different from English NER, East Asian languages including Chinese are written without explicit word boundary. One intuitive way to solve this problem is to segment the input sentences into words first, and then to apply word sequence labeling (Yang et al., 2016; He and Sun, 2017a). However, such methods suffer from error propagation between these two subtasks. To overcome this limitation, efforts have been devoted to incorporating word information by leveraging lexicon features and gazetteers (Peng and Dredze, 2015; Cao et al., 2018; Wu et al., 2019; Lin et al., 2019). As recent state-of-the-art (SOTA) lattice-based method, Zhang and Yang (2018) integrated matched lexical words information into character sequence with a directed acyclic graph (DAG) structure using lattice LSTM. While obtaining promising results, this model faces two challenges. First, as an extension"
2020.coling-main.340,N16-1030,0,0.111089,"-aware attention distribution via a porous mechanism, which enhances the ability of capturing useful local context. (3) Experimental results show that the proposed model is effective and efficient. The source code of this paper can be obtained from https://github.com/strawberryx/PLTE. 2 Related Work Our work is in line with NER models based on neural networks and lattice transformer models. Huang et al. (2015) proposed a BiLSTM-CRF model for NER and achieved strong performance. Santos and Guimaraes (2015) used word- and character-level representations based on the CharWNN deep neural network. Lample et al. (2016) designed a character LSTM and word LSTM for NER. Compared to our work, these word-based methods suffer from segmentation errors. 3832 To avoid segmentation errors, most recent NER models are built upon character sequence labeling. Peng and Dredze (2015) proposesd a joint training objective for three types of neural embeddings to better recognize entity boundary. Lu et al. (2016) presented a position-sensitive skip-gram model to learn multi-prototype Chinese character embeddings. He and Sun (2017a) took the positional character embeddings into account. Although these methods achieve promising"
2020.coling-main.340,W06-0115,0,0.44398,"set of manually labeled training data {(Si , yi )}|N i=1 , sentence-level log-likelihood loss with L2 regularization is used to train the model: L= N X log(P (yi |Si )) + i=1 λ k Θ k2 , 2 (7) where λ is the L2 regularization weight and Θ represents the parameter set. 5 Experiments We conduct experiments to investigate the effectiveness of our proposed PLTE method across different domains. Standard precision (P), recall (R) and F1-score (F1) are used as evaluation metrics. 5.1 Experimental Setup 5.1.1 Data We evaluate our model on four datasets, including OntoNotes (Ralph et al., 2011), MSRA (Levow, 2006), Weibo NER (Peng and Dredze, 2015; He and Sun, 2017b) and a Chinese Resume dataset (Zhang and Yang, 2018). We use the same training, valid and test split as (Zhang and Yang, 2018). For these datasets, both OntoNotes and MSRA are in news domain, while Weibo and Resume come from social media. 3836 OntoNotes Input Gold seg No seg Models Che et al. (2013) Wang et al. (2013) Yang et al. (2016) Lattice LSTM (2018) LR-CNN (2019a) CAN-NER(2019) PLTE BERT-Tagger Lattice LSTM[BERT] LR-CNN[BERT] PLTE[BERT] Resume Models Lattice LSTM (2018) CAN-NER(2019) LR-CNN (2019a) PLTE BERT-Tagger Lattice LSTM[BERT]"
2020.coling-main.340,D19-1646,0,0.0553706,"Missing"
2020.coling-main.340,N19-1247,0,0.163269,"ee types of neural embeddings to better recognize entity boundary. Lu et al. (2016) presented a position-sensitive skip-gram model to learn multi-prototype Chinese character embeddings. He and Sun (2017a) took the positional character embeddings into account. Although these methods achieve promising performance, they ignore word information lying in character sequence. Some work exploits rich word boundary and semantic information in character sequence. Cao et al. (2018) applied an adversarial transfer learning framework to integrate the task-shared word boundary information into Chinese NER. Liu et al. (2019) explored four different strategies for Word-Character LSTM. Gui et al. (2019a) proposed a CNN-based NER model that incorporates lexicons using a rethinking mechanism. Recent state-of-the-art methods exploit lattice-structured models to integrate latent word information into character sequence, which has been proven effective on various NLP tasks (Su et al., 2017; Tan et al., 2018) . Specifically, Zhang and Yang (2018) utilized the lattice LSTM to leverage explicit word information over character sequence labeling. Based on this method, Gui et al. (2019b) and Sui et al. (2019) formulated the l"
2020.coling-main.340,L16-1138,1,0.877329,"models. Huang et al. (2015) proposed a BiLSTM-CRF model for NER and achieved strong performance. Santos and Guimaraes (2015) used word- and character-level representations based on the CharWNN deep neural network. Lample et al. (2016) designed a character LSTM and word LSTM for NER. Compared to our work, these word-based methods suffer from segmentation errors. 3832 To avoid segmentation errors, most recent NER models are built upon character sequence labeling. Peng and Dredze (2015) proposesd a joint training objective for three types of neural embeddings to better recognize entity boundary. Lu et al. (2016) presented a position-sensitive skip-gram model to learn multi-prototype Chinese character embeddings. He and Sun (2017a) took the positional character embeddings into account. Although these methods achieve promising performance, they ignore word information lying in character sequence. Some work exploits rich word boundary and semantic information in character sequence. Cao et al. (2018) applied an adversarial transfer learning framework to integrate the task-shared word boundary information into Chinese NER. Liu et al. (2019) explored four different strategies for Word-Character LSTM. Gui e"
2020.coling-main.340,D15-1064,0,0.365373,"formance by integrating character information into word representations based on sequence labeling. Different from English NER, East Asian languages including Chinese are written without explicit word boundary. One intuitive way to solve this problem is to segment the input sentences into words first, and then to apply word sequence labeling (Yang et al., 2016; He and Sun, 2017a). However, such methods suffer from error propagation between these two subtasks. To overcome this limitation, efforts have been devoted to incorporating word information by leveraging lexicon features and gazetteers (Peng and Dredze, 2015; Cao et al., 2018; Wu et al., 2019; Lin et al., 2019). As recent state-of-the-art (SOTA) lattice-based method, Zhang and Yang (2018) integrated matched lexical words information into character sequence with a directed acyclic graph (DAG) structure using lattice LSTM. While obtaining promising results, this model faces two challenges. First, as an extension to the non-parallelizable sequential LSTM to a DAG structured model, lattice LSTM is restricted to preprocess one character at a time, which can make it infeasibly to deploy. Second, due to the inherently unidirectional sequential nature, l"
2020.coling-main.340,P16-2025,0,0.250544,"Missing"
2020.coling-main.340,W15-3904,0,0.0198522,"lattices in batch mode and capturing dependencies between characters and matched lexical words. (2) We revise lattice-aware attention distribution via a porous mechanism, which enhances the ability of capturing useful local context. (3) Experimental results show that the proposed model is effective and efficient. The source code of this paper can be obtained from https://github.com/strawberryx/PLTE. 2 Related Work Our work is in line with NER models based on neural networks and lattice transformer models. Huang et al. (2015) proposed a BiLSTM-CRF model for NER and achieved strong performance. Santos and Guimaraes (2015) used word- and character-level representations based on the CharWNN deep neural network. Lample et al. (2016) designed a character LSTM and word LSTM for NER. Compared to our work, these word-based methods suffer from segmentation errors. 3832 To avoid segmentation errors, most recent NER models are built upon character sequence labeling. Peng and Dredze (2015) proposesd a joint training objective for three types of neural embeddings to better recognize entity boundary. Lu et al. (2016) presented a position-sensitive skip-gram model to learn multi-prototype Chinese character embeddings. He an"
2020.coling-main.340,N18-2074,0,0.0232375,"and “京(Capital)”, although the semantics and boundary information of “南京市(NanJing City)” can be useful knowledge for predicting the tag of “南(South)” as “B-LOC”. In this paper, we address these issues by considering a novel Porous Lattice Transformer Encoder (PLTE). Inspired by previous research on machine translation (Xiao et al., 2019; Sperber et al., 2019), which integrated lattice-structured inputs into self-attention models, we propose a lattice transformer encoder for Chinese NER by introducing lattice-aware self-attention, which borrows the idea from the relative positional embedding (Shaw et al., 2018) to make self-attention aware of the relative position information in lattice structure. Considering that self-attention network calculates attention weights between each pair of tokens in a sequence regardless of their distance, we simply concatenate all the characters and lexical words as input to consume lattices without resorting to the DAG structure. In this way, characters coupled with lexical words can be processed in batches. A lexical word representation is allowed to build a direct relation with the included characters by lattice-aware self-attention, thus addressing the second issue"
2020.coling-main.340,P19-1115,0,0.114745,"onstituent characters ”南(South)” and ”京(Capital)”, resulting in the loss of crucial information for tagging. Besides, lattice LSTM cannot perform batched computation due to the directed acyclic graph input structure. the other two inside characters “南(South)” and “京(Capital)”, although the semantics and boundary information of “南京市(NanJing City)” can be useful knowledge for predicting the tag of “南(South)” as “B-LOC”. In this paper, we address these issues by considering a novel Porous Lattice Transformer Encoder (PLTE). Inspired by previous research on machine translation (Xiao et al., 2019; Sperber et al., 2019), which integrated lattice-structured inputs into self-attention models, we propose a lattice transformer encoder for Chinese NER by introducing lattice-aware self-attention, which borrows the idea from the relative positional embedding (Shaw et al., 2018) to make self-attention aware of the relative position information in lattice structure. Considering that self-attention network calculates attention weights between each pair of tokens in a sequence regardless of their distance, we simply concatenate all the characters and lexical words as input to consume lattices without resorting to the D"
2020.coling-main.340,D19-1396,0,0.0435755,"nto Chinese NER. Liu et al. (2019) explored four different strategies for Word-Character LSTM. Gui et al. (2019a) proposed a CNN-based NER model that incorporates lexicons using a rethinking mechanism. Recent state-of-the-art methods exploit lattice-structured models to integrate latent word information into character sequence, which has been proven effective on various NLP tasks (Su et al., 2017; Tan et al., 2018) . Specifically, Zhang and Yang (2018) utilized the lattice LSTM to leverage explicit word information over character sequence labeling. Based on this method, Gui et al. (2019b) and Sui et al. (2019) formulated the lattice structure as a graph and leveraged Graph Neural Networks (GNNs) to integrate lexical knowledge. However, for the NER task, coupling pre-trained language models such as BERT (Devlin et al., 2019) with GNNs and fine-tuning them can be non-trivial. Lattice transformer has been exploited in NMT (Xiao et al., 2019), as well as speech translation (Sperber et al., 2019; Zhang et al., 2019). Compared with existing work, our proposed porous lattice transformer encoder is different in both motivation and structure. We revise the fully-connected attention distribution with a pivot"
2020.coling-main.340,P19-1176,0,0.0245516,"ion mechanism has attracted increasing attention due to their flexibility in parallel computation and dependency modeling. Given an input sequence representation X = {x1 , · · · , xn } ∈ Rn×d , we can first transform it into queries Q = XWQ ∈ Rn×dk , keys K = XWK ∈ Rn×dk , and values V = XWV ∈ Rn×dv , where {WQ , WK , WV } are trainable parameters. The output sequence representation is calculated as: QKT Att(Q, K, V) = Softmax( √ )V, dk where 3.2 √ (1) dk is the scaling factor. Lattice Transformer Transformer has been used for many NLP tasks, notably machine translation and language modeling (Wang et al., 2019; Devlin et al., 2019). By invoking multi-layer self-attention for global context modeling, Transformer enables paralleled computation and addresses the inherent sequential computation shortcoming of RNNs. Lattice Transformer is a generalization of the standard transformer architecture to accept lattice-structured inputs, it linearizes the lattice structure and introduces a position relation score matrix to make self-attention aware of the topological structure of lattice: Att(Q, K, V) = Softmax( QKT + R √ )V, dk (2) where R ∈ Rn×n encodes the lattice-dependent relations between each pair of e"
2020.coling-main.340,P19-1298,0,0.175846,"ng City)” and its constituent characters ”南(South)” and ”京(Capital)”, resulting in the loss of crucial information for tagging. Besides, lattice LSTM cannot perform batched computation due to the directed acyclic graph input structure. the other two inside characters “南(South)” and “京(Capital)”, although the semantics and boundary information of “南京市(NanJing City)” can be useful knowledge for predicting the tag of “南(South)” as “B-LOC”. In this paper, we address these issues by considering a novel Porous Lattice Transformer Encoder (PLTE). Inspired by previous research on machine translation (Xiao et al., 2019; Sperber et al., 2019), which integrated lattice-structured inputs into self-attention models, we propose a lattice transformer encoder for Chinese NER by introducing lattice-aware self-attention, which borrows the idea from the relative positional embedding (Shaw et al., 2018) to make self-attention aware of the relative position information in lattice structure. Considering that self-attention network calculates attention weights between each pair of tokens in a sequence regardless of their distance, we simply concatenate all the characters and lexical words as input to consume lattices wit"
2020.coling-main.340,D18-1475,0,0.0282653,"-attention aware of the relative position information in lattice structure. Considering that self-attention network calculates attention weights between each pair of tokens in a sequence regardless of their distance, we simply concatenate all the characters and lexical words as input to consume lattices without resorting to the DAG structure. In this way, characters coupled with lexical words can be processed in batches. A lexical word representation is allowed to build a direct relation with the included characters by lattice-aware self-attention, thus addressing the second issue. Some work (Yang et al., 2018; Yang et al., 2019) demonstrates that self-attention benefits from locality modeling, especially for the NER task. As we can see from the example in Figure 1, the word “位 于(Locates In)” is the immediate and most obvious feature to guide the neighboring character “桥(Bridge)” to be identified as “E-LOC” instead of “E-PER”, while ”中国(China)” has no contribution to this decision. Given this observation, we further introduce a novel porous mechanism to enhance the local dependencies among neighboring tokens. The key insight is to modify the self-attention architecture by replacing the fully-connec"
2020.coling-main.340,N19-1407,0,0.0213322,"the relative position information in lattice structure. Considering that self-attention network calculates attention weights between each pair of tokens in a sequence regardless of their distance, we simply concatenate all the characters and lexical words as input to consume lattices without resorting to the DAG structure. In this way, characters coupled with lexical words can be processed in batches. A lexical word representation is allowed to build a direct relation with the included characters by lattice-aware self-attention, thus addressing the second issue. Some work (Yang et al., 2018; Yang et al., 2019) demonstrates that self-attention benefits from locality modeling, especially for the NER task. As we can see from the example in Figure 1, the word “位 于(Locates In)” is the immediate and most obvious feature to guide the neighboring character “桥(Bridge)” to be identified as “E-LOC” instead of “E-PER”, while ”中国(China)” has no contribution to this decision. Given this observation, we further introduce a novel porous mechanism to enhance the local dependencies among neighboring tokens. The key insight is to modify the self-attention architecture by replacing the fully-connected topology with a"
2020.coling-main.340,P18-1144,1,0.939307,"Asian languages including Chinese are written without explicit word boundary. One intuitive way to solve this problem is to segment the input sentences into words first, and then to apply word sequence labeling (Yang et al., 2016; He and Sun, 2017a). However, such methods suffer from error propagation between these two subtasks. To overcome this limitation, efforts have been devoted to incorporating word information by leveraging lexicon features and gazetteers (Peng and Dredze, 2015; Cao et al., 2018; Wu et al., 2019; Lin et al., 2019). As recent state-of-the-art (SOTA) lattice-based method, Zhang and Yang (2018) integrated matched lexical words information into character sequence with a directed acyclic graph (DAG) structure using lattice LSTM. While obtaining promising results, this model faces two challenges. First, as an extension to the non-parallelizable sequential LSTM to a DAG structured model, lattice LSTM is restricted to preprocess one character at a time, which can make it infeasibly to deploy. Second, due to the inherently unidirectional sequential nature, lattice LSTM fails to incorporate the word-level semantics into the representation of the characters except for the last character in"
2020.coling-main.340,P19-1649,0,0.0189236,"al., 2018) . Specifically, Zhang and Yang (2018) utilized the lattice LSTM to leverage explicit word information over character sequence labeling. Based on this method, Gui et al. (2019b) and Sui et al. (2019) formulated the lattice structure as a graph and leveraged Graph Neural Networks (GNNs) to integrate lexical knowledge. However, for the NER task, coupling pre-trained language models such as BERT (Devlin et al., 2019) with GNNs and fine-tuning them can be non-trivial. Lattice transformer has been exploited in NMT (Xiao et al., 2019), as well as speech translation (Sperber et al., 2019; Zhang et al., 2019). Compared with existing work, our proposed porous lattice transformer encoder is different in both motivation and structure. We revise the fully-connected attention distribution with a pivot-shared structure via the porous mechanism to enhance the local dependencies among neighboring tokens.1 To our knowledge, we are the first to design a lattice transformer for Chinese NER. 3 Background In this section, we first briefly review the self-attention mechanism, then move on to current lattice Transformer that our PLTE model is built upon. 3.1 Self-Attention Self-attention mechanism has attracted"
2020.coling-main.340,N19-1342,0,0.073775,"Missing"
2020.coling-main.341,H05-1091,0,0.207839,"validation studies demonstrate the effectiveness of our pruning with rethinking method. 2 Related Work Our work is inspired by two lines of research: enhancing relation extractor through syntactic dependency information and refining neural network with the rethinking mechanism. Dependency-based relation extraction. Syntactic dependency information has been widely explored in relation extraction approaches for many years. Some early works introduced syntactic features into statistical classifiers and found them to be beneficial (Zelenko et al., 2003). Instead of using the full dependency tree, Bunescu and Mooney (2005) observed that the information relevant to relation extraction is almost entirely concentrated in the shortest dependency path (SDP) between two entities, and designed the dependency path kernel based on the SDP features. Based on the idea that SDP contains essential information, many studies exploited it with several refinements. Ebrahimi and Dou (2015) modified the original recursive neural network (RecNN) and presented an SDP-based RecNN for relation classification. Xu et al. (2015a) proposed to learn relation representations from SDP through CNN. Xu et 3843 x1 1 0 1 1 x2 Dependency Graph x"
2020.coling-main.341,P16-1072,0,0.0854381,"express the relation of the target entity pair (Xu et al., 2015b; Zhang et al., 2018), and some target-irrelevant tokens could introduce noise and cause confusion to the classification. Therefore, multiple pruning strategies are proposed to eliminate unimportant tokens and distill the dependency information. Xu et al. (2015b) applied neural networks only on the shortest dependency path (SDP) between the two entities in the dependency tree, which soon became dominant with many works demonstrating that using SDP brings better experimental results than using the whole sentence (Xu et al., 2015a; Cai et al., 2016). Miwa and Bansal (2016) reduced the full tree to the subtree below the lowest common ancestor (LCA) of the entities. Zhang et al. (2018) expanded SDP by including tokens that are up to distance K away from the dependency path in the LCA subtree. However, these hand-crafted pruning rules may lead to the omission of useful information due to the variability and ambiguity of natural language. Look at a concrete example shown in Figure 1, the key relational token “Founded” is always excluded from the pruned tree no matter what kind of pruning rule mentioned above is deployed. In fact, it’s unreal"
2020.coling-main.341,N19-1298,0,0.0129618,"n (RE) aims to detect the semantic relationship between two specific entities appearing in a sentence (often termed subject and object, respectively). This task plays an important role in many downstream NLP applications that require a relational understanding of unstructured text such as question answering (Dai et al., 2016) and dialogue systems (Young et al., 2018). Models leveraging the dependency tree of the input sentence have proven to be effective in relation extraction because they can effortlessly exploit long-term relations that are obscure from the surface form (Zhang et al., 2018; Can et al., 2019). Recent studies also stated that not all tokens in the dependency tree are needed to express the relation of the target entity pair (Xu et al., 2015b; Zhang et al., 2018), and some target-irrelevant tokens could introduce noise and cause confusion to the classification. Therefore, multiple pruning strategies are proposed to eliminate unimportant tokens and distill the dependency information. Xu et al. (2015b) applied neural networks only on the shortest dependency path (SDP) between the two entities in the dependency tree, which soon became dominant with many works demonstrating that using SD"
2020.coling-main.341,P16-1076,0,0.0233287,"introduce a rethinking mechanism to guide and refine the pruning operation by feeding back the high-level learned features repeatedly. Extensive experimental results demonstrate that our model achieves impressive performance compared to strong competitors. 1 Introduction Relation extraction (RE) aims to detect the semantic relationship between two specific entities appearing in a sentence (often termed subject and object, respectively). This task plays an important role in many downstream NLP applications that require a relational understanding of unstructured text such as question answering (Dai et al., 2016) and dialogue systems (Young et al., 2018). Models leveraging the dependency tree of the input sentence have proven to be effective in relation extraction because they can effortlessly exploit long-term relations that are obscure from the surface form (Zhang et al., 2018; Can et al., 2019). Recent studies also stated that not all tokens in the dependency tree are needed to express the relation of the target entity pair (Xu et al., 2015b; Zhang et al., 2018), and some target-irrelevant tokens could introduce noise and cause confusion to the classification. Therefore, multiple pruning strategies"
2020.coling-main.341,N15-1133,0,0.0207207,"widely explored in relation extraction approaches for many years. Some early works introduced syntactic features into statistical classifiers and found them to be beneficial (Zelenko et al., 2003). Instead of using the full dependency tree, Bunescu and Mooney (2005) observed that the information relevant to relation extraction is almost entirely concentrated in the shortest dependency path (SDP) between two entities, and designed the dependency path kernel based on the SDP features. Based on the idea that SDP contains essential information, many studies exploited it with several refinements. Ebrahimi and Dou (2015) modified the original recursive neural network (RecNN) and presented an SDP-based RecNN for relation classification. Xu et al. (2015a) proposed to learn relation representations from SDP through CNN. Xu et 3843 x1 1 0 1 1 x2 Dependency Graph x3 x4 ℱ GCN Semantic Graph ℱ 1 0 1 1 Contextual Encoder Pooling DP − GCN z 1 0 1 1 ℱ Selection Rethinking Mechanism Figure 2: Model architecture (Best viewed in color). x1 and x4 denote subject entity and object entity, respectively. The model first encodes the contextual information, and then L layers of DP-GCNs are deployed. In each layer, a selection m"
2020.coling-main.341,P19-1024,0,0.249797,"text embedding of an entity over its dependency subtree in bottom-up order. Zhang et al. (2018) claimed that keeping only the SDP could lead to loss of crucial information and conversely hurt robustness, and proposed a path-centric pruning strategy to incorporate nodes that are directly attached to the path. Tran et al. (2019) built RNN on the SDP to gain long-distance features, which are combined with a CNN to preserve the full information. Unlike these methods that remove edges in preprocessing with hard rules, our model learns to prune the dependency tree in an endto-end fashion. Recently, Guo et al. (2019) constructed a fully-connected graph for relation extraction via multi-head self-attention mechanism. Sun et al. (2020) proposed a learnable syntax-transport attention graph convolutional network which operates on the syntax-transport graph. However, they neglect the target entity information in the graph learning process and constructs a denser entity-unaware graph. In contrast, our approach not only constructs an entity-specific graph but also removes noisy information explicitly by the dynamic pruning strategy. Rethinking mechanism. Previous attempts to use a rethinking mechanism in neural"
2020.coling-main.341,D19-1022,0,0.0649844,"al 2010 task 8) dataset contains 9 directed relations and a no-relation class. It is smaller and simpler than TACRED with 8000 training samples and 2717 test samples. We use this dataset to evaluate the generalization ability of our proposed model. On SemEval, we follow the convention and report the macro-averaged F1 scores. For fair comparisons, we report the averaged test results ± one standard deviation over 5 randomly initialized runs. 3847 System SDP-LSTM (Xu et al., 2015b) LR (Zhang et al., 2017) PA-LSTM (Zhang et al., 2017) C-GCN (Zhang et al., 2018) SA-LSTM (Yu et al., 2019) KnwlSelf (Li et al., 2019) ERNIE (Zhang et al., 2019) AGGCN (Guo et al., 2019) Precision Recall F1 66.3 73.5 65.7 69.9 69.0 67.1 70.0 73.1 (71.6 ± 0.4)† 52.7 49.9 64.5 63.3 66.2 68.4 66.1 64.2 (63.6 ± 0.3)† 58.7 59.4 65.1 66.4 67.6 67.8 68.0 68.2 (67.4 ± 0.3)† 72.2 ± 0.3 66.5 ± 0.2? 69.2 ± 0.2? DP-GCN Table 1: Micro-averaged precision, recall and F1 score on the TACRED test set. The best performance is in bold for each metric. † marks results produced from re-running the official source code, which are consistent with the numbers reported by other researchers 2 . ? marks statistically significant improvements over AGGC"
2020.coling-main.341,P15-2047,0,0.0230547,"entity-specific features as input to select the relevant nodes and prune the dependency graph (self-loops are omitted for simplification). A pruned semantic graph generated by self-attention is also introduced to ensure the graph connectivity. Then the resulting graph is passed to a GCN module to propagate messages. Finally, a pooling module is leveraged to aggregate information. The obtained relational features are fed back to the selection module of each layer to adjust the pruning operation. al. (2015a) designed a multi-channel LSTM to pick up heterogeneous information along with the SDP. Liu et al. (2015) augmented SDP with the subtrees attached to the shortest path, and utilized two neural networks to model the obtained structure. Cai et al. (2016) combined CNN and two-channel LSTM to make use of dependency relations information in the SDP. Miwa and Bansal (2016) found it to be effective when applying a Tree-LSTM to the subtree rooted at the lowest common ancestor (LCA) of the two entities. He et al. (2018) derived the context embedding of an entity over its dependency subtree in bottom-up order. Zhang et al. (2018) claimed that keeping only the SDP could lead to loss of crucial information a"
2020.coling-main.341,P14-5010,0,0.00280225,"each metric. † marks results produced from re-running the official source code, which are consistent with the numbers reported by other researchers 2 . ? marks statistically significant improvements over AGGCN with p &lt; 0.01 under a bootstrap test. 4.2 Implementation Details The model is trained with SGD optimizer with the initial learning rate of 0.7 and the weight decay of 0.9. Following previous studies (Zhang et al., 2018; Guo et al., 2019), we exploit 300-dimensional Glove (Pennington et al., 2014) vectors for the word embeddings, and generate dependency parse trees with Stanford CoreNLP (Manning et al., 2014). We choose the temperature τ in Gumbel-Softmax from the set {0.1, 0.3, 0.5, 0.7}, the rethinking times from {1, 2, 3, 4, 5}. We use 3 DP-GCN layers in our experiments, and to fully capture the dependency information, in the first layer, we directly feed the original dependency tree to the GCN module without any pruning. To avoid deleting all nodes in the graph, we set the gates of the target entity nodes as open during training and test time. The hidden state size of BiLSTM and DP-GCN are both set to 300. To ease overfitting, we apply dropout on the word embeddings and each DP-GCN layer with"
2020.coling-main.341,P16-1105,0,0.10243,"n of the target entity pair (Xu et al., 2015b; Zhang et al., 2018), and some target-irrelevant tokens could introduce noise and cause confusion to the classification. Therefore, multiple pruning strategies are proposed to eliminate unimportant tokens and distill the dependency information. Xu et al. (2015b) applied neural networks only on the shortest dependency path (SDP) between the two entities in the dependency tree, which soon became dominant with many works demonstrating that using SDP brings better experimental results than using the whole sentence (Xu et al., 2015a; Cai et al., 2016). Miwa and Bansal (2016) reduced the full tree to the subtree below the lowest common ancestor (LCA) of the entities. Zhang et al. (2018) expanded SDP by including tokens that are up to distance K away from the dependency path in the LCA subtree. However, these hand-crafted pruning rules may lead to the omission of useful information due to the variability and ambiguity of natural language. Look at a concrete example shown in Figure 1, the key relational token “Founded” is always excluded from the pruned tree no matter what kind of pruning rule mentioned above is deployed. In fact, it’s unrealistic to expect an empir"
2020.coling-main.341,D14-1162,0,0.084497,"N Table 1: Micro-averaged precision, recall and F1 score on the TACRED test set. The best performance is in bold for each metric. † marks results produced from re-running the official source code, which are consistent with the numbers reported by other researchers 2 . ? marks statistically significant improvements over AGGCN with p &lt; 0.01 under a bootstrap test. 4.2 Implementation Details The model is trained with SGD optimizer with the initial learning rate of 0.7 and the weight decay of 0.9. Following previous studies (Zhang et al., 2018; Guo et al., 2019), we exploit 300-dimensional Glove (Pennington et al., 2014) vectors for the word embeddings, and generate dependency parse trees with Stanford CoreNLP (Manning et al., 2014). We choose the temperature τ in Gumbel-Softmax from the set {0.1, 0.3, 0.5, 0.7}, the rethinking times from {1, 2, 3, 4, 5}. We use 3 DP-GCN layers in our experiments, and to fully capture the dependency information, in the first layer, we directly feed the original dependency tree to the GCN module without any pruning. To avoid deleting all nodes in the graph, we set the gates of the target entity nodes as open during training and test time. The hidden state size of BiLSTM and DP"
2020.coling-main.341,N19-1286,0,0.0221,"al. (2016) combined CNN and two-channel LSTM to make use of dependency relations information in the SDP. Miwa and Bansal (2016) found it to be effective when applying a Tree-LSTM to the subtree rooted at the lowest common ancestor (LCA) of the two entities. He et al. (2018) derived the context embedding of an entity over its dependency subtree in bottom-up order. Zhang et al. (2018) claimed that keeping only the SDP could lead to loss of crucial information and conversely hurt robustness, and proposed a path-centric pruning strategy to incorporate nodes that are directly attached to the path. Tran et al. (2019) built RNN on the SDP to gain long-distance features, which are combined with a CNN to preserve the full information. Unlike these methods that remove edges in preprocessing with hard rules, our model learns to prune the dependency tree in an endto-end fashion. Recently, Guo et al. (2019) constructed a fully-connected graph for relation extraction via multi-head self-attention mechanism. Sun et al. (2020) proposed a learnable syntax-transport attention graph convolutional network which operates on the syntax-transport graph. However, they neglect the target entity information in the graph lear"
2020.coling-main.341,D15-1062,0,0.304438,"This task plays an important role in many downstream NLP applications that require a relational understanding of unstructured text such as question answering (Dai et al., 2016) and dialogue systems (Young et al., 2018). Models leveraging the dependency tree of the input sentence have proven to be effective in relation extraction because they can effortlessly exploit long-term relations that are obscure from the surface form (Zhang et al., 2018; Can et al., 2019). Recent studies also stated that not all tokens in the dependency tree are needed to express the relation of the target entity pair (Xu et al., 2015b; Zhang et al., 2018), and some target-irrelevant tokens could introduce noise and cause confusion to the classification. Therefore, multiple pruning strategies are proposed to eliminate unimportant tokens and distill the dependency information. Xu et al. (2015b) applied neural networks only on the shortest dependency path (SDP) between the two entities in the dependency tree, which soon became dominant with many works demonstrating that using SDP brings better experimental results than using the whole sentence (Xu et al., 2015a; Cai et al., 2016). Miwa and Bansal (2016) reduced the full tree"
2020.coling-main.341,D15-1206,0,0.496882,"This task plays an important role in many downstream NLP applications that require a relational understanding of unstructured text such as question answering (Dai et al., 2016) and dialogue systems (Young et al., 2018). Models leveraging the dependency tree of the input sentence have proven to be effective in relation extraction because they can effortlessly exploit long-term relations that are obscure from the surface form (Zhang et al., 2018; Can et al., 2019). Recent studies also stated that not all tokens in the dependency tree are needed to express the relation of the target entity pair (Xu et al., 2015b; Zhang et al., 2018), and some target-irrelevant tokens could introduce noise and cause confusion to the classification. Therefore, multiple pruning strategies are proposed to eliminate unimportant tokens and distill the dependency information. Xu et al. (2015b) applied neural networks only on the shortest dependency path (SDP) between the two entities in the dependency tree, which soon became dominant with many works demonstrating that using SDP brings better experimental results than using the whole sentence (Xu et al., 2015a; Cai et al., 2016). Miwa and Bansal (2016) reduced the full tree"
2020.coling-main.341,D17-1004,0,0.451229,"network (FFNN), followed by a Softmax normalization layer to yield a probability distribution over relational decision space: p(ˆ y|r) = Softmax(FFNN(r)), (13) ˆ is the predicted relational distribution. During the training, we optimize the parameters of the where y entire network to minimize the cross-entropy loss: J(θ) = − N 1 X yi logp(ˆ yi |ri ), N (14) i=1 where yi is the one-hot vector represented ground truth of the i-th instance, and N denotes the number of training instances. 4 4.1 Experiments Dataset and Metric We conduct experiments on two relation extraction datasets: (1) TACRED (Zhang et al., 2017): It is the currently largest benchmark dataset for supervised relation extraction, which contains 41 relations and a specially no-relation class indicating that the relation expressed in the sentence is not among the 41 types. TACRED is partitioned into training (68124 samples), dev (22631 samples) and test (15509 samples) sets, we tune the hyper-parameters according to results on the dev set. Mentions in TACRED are typed to avoid overfitting on specific entities and provide entity type information, in which subjects fall into 2 categories, and objects are categorized into 16 types. We report"
2020.coling-main.341,D18-1244,0,0.487025,"n Relation extraction (RE) aims to detect the semantic relationship between two specific entities appearing in a sentence (often termed subject and object, respectively). This task plays an important role in many downstream NLP applications that require a relational understanding of unstructured text such as question answering (Dai et al., 2016) and dialogue systems (Young et al., 2018). Models leveraging the dependency tree of the input sentence have proven to be effective in relation extraction because they can effortlessly exploit long-term relations that are obscure from the surface form (Zhang et al., 2018; Can et al., 2019). Recent studies also stated that not all tokens in the dependency tree are needed to express the relation of the target entity pair (Xu et al., 2015b; Zhang et al., 2018), and some target-irrelevant tokens could introduce noise and cause confusion to the classification. Therefore, multiple pruning strategies are proposed to eliminate unimportant tokens and distill the dependency information. Xu et al. (2015b) applied neural networks only on the shortest dependency path (SDP) between the two entities in the dependency tree, which soon became dominant with many works demonstr"
2020.coling-main.341,P19-1139,0,0.080111,"contains 9 directed relations and a no-relation class. It is smaller and simpler than TACRED with 8000 training samples and 2717 test samples. We use this dataset to evaluate the generalization ability of our proposed model. On SemEval, we follow the convention and report the macro-averaged F1 scores. For fair comparisons, we report the averaged test results ± one standard deviation over 5 randomly initialized runs. 3847 System SDP-LSTM (Xu et al., 2015b) LR (Zhang et al., 2017) PA-LSTM (Zhang et al., 2017) C-GCN (Zhang et al., 2018) SA-LSTM (Yu et al., 2019) KnwlSelf (Li et al., 2019) ERNIE (Zhang et al., 2019) AGGCN (Guo et al., 2019) Precision Recall F1 66.3 73.5 65.7 69.9 69.0 67.1 70.0 73.1 (71.6 ± 0.4)† 52.7 49.9 64.5 63.3 66.2 68.4 66.1 64.2 (63.6 ± 0.3)† 58.7 59.4 65.1 66.4 67.6 67.8 68.0 68.2 (67.4 ± 0.3)† 72.2 ± 0.3 66.5 ± 0.2? 69.2 ± 0.2? DP-GCN Table 1: Micro-averaged precision, recall and F1 score on the TACRED test set. The best performance is in bold for each metric. † marks results produced from re-running the official source code, which are consistent with the numbers reported by other researchers 2 . ? marks statistically significant improvements over AGGCN with p &lt; 0.01 under a boo"
2020.emnlp-main.514,D11-1141,0,0.0975347,"ncluding lexical words, gazetteers and anchors in Wikipedia have been proved to be useful for a wide range of sentiment analysis tasks. For supervised NER task, some researchers utilize lattice structure to incorporate the lexical information into character-based NER and avoid the segmentation error propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant s"
2020.emnlp-main.514,W03-2201,0,0.0163477,"Missing"
2020.emnlp-main.514,D19-1025,0,0.244338,"named entity information which can provide rich knowledge for NER. Due to little knowledge connection between NER and general language modeling, how to adapt public pre-trained models to be NER-specific remains an open problem. To this end, injecting named entity knowledge during pre-training is a possible solution. However, this process of knowledge acquisition may be inefficient and expensive. In fact, there are extensive weakly labeled annotations that naturally exist on the web yet to be explored for NER model pre-training, which are relatively easier to obtain compared with labeled data (Cao et al., 2019). One can collect them from online resources, such as the Wikipedia anchors and gazetteers (named entity dictionaries). Although automatically derived corpora usually contain massive noisy data, it still contains some extend the valuable semantic information required for NER (Peng et al., 2019). In this paper, we propose a Coarse-to-Fine Entity knowledge Enhanced (CoFEE) pre-training framework for NER task, aiming to gather and utilize knowledge related to named entities. In particular, we first extract anchors from Wikipedia and use them as training corpora for entity span identification. Whi"
2020.emnlp-main.514,N19-1423,0,0.478224,"the task of discovering information entities and identifying their corresponding categories, such as mentions of people, organizations, locations, temporal and numeric expressions (Freitag, 2004). It is an essential component in many applications including machine translation (Babych and Hartley, 2003), relation extraction (Yu et al., 2019), entity linking (Xue et al., 2019a), and so on. ∗ Corresponding Author The source code can be https://github.com/strawberryx/CoFEE 1 obtained from Recently, NER has seen remarkable advances with the help of pre-trained representation models, such as BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019). Providing contextual representation, these pre-trained models could be easily applied to NER applications as an encoder by just fine-tuning it. Despite refreshing the state-of-theart performance of NER, the current pre-training techniques are not directly optimized for NER. Typically, these models build unsupervised training objectives to capture dependency between words and learn a general language representation (Tian et al., 2020), while rarely considering incorporating named entity information which can provide rich knowledge for NER. Due to little knowledge"
2020.emnlp-main.514,P19-1141,0,0.251407,"o be useful for a wide range of sentiment analysis tasks. For supervised NER task, some researchers utilize lattice structure to incorporate the lexical information into character-based NER and avoid the segmentation error propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al."
2020.emnlp-main.514,P18-2039,0,0.0588523,"rds, gazetteers and anchors in Wikipedia have been proved to be useful for a wide range of sentiment analysis tasks. For supervised NER task, some researchers utilize lattice structure to incorporate the lexical information into character-based NER and avoid the segmentation error propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al.,"
2020.emnlp-main.514,W04-3234,0,0.136435,"Missing"
2020.emnlp-main.514,D19-1355,0,0.0222868,"a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019). Task Specific Pre-training. Unsupervised language model pre-training and task-specific finetuning achieve SOTA results on many NLP tasks, including NER (Peters et al., 2018; Devlin et al., 2019; Li et al., 2020). Recently, with the help of automatically minded knowledge lying in the web, researchers devoted them to the pre-training models for specific tasks, including word sense disambiguation (Huang et al., 2019), word-in-context tasks (Levine et al., 2020), entity-linking and relation classification (Zhang et al., 2019), sentiment classification (Tian et al., 2020). 3 Background In this section, we give a brief introduction to MRC-NER (Li et al., 2020), which achieves satisfying performance in NER and thus is chosen as the foundation of our work. Given an input paragraph X = {x1 , x2 , · · · , xn } where xi denotes the i-th character, NER aims at discovering each entity xstart,end in X and identify its corresponding type y ∈ Y , where Y is the set of predefined tags(e.g., PER, LOC). xstart,end = {xst"
2020.emnlp-main.514,P19-1236,1,0.84352,"propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019). Task Specific Pre-training. Unsupervised language model pre-training and task-specific finetuning achieve SOTA results on many NLP tasks, including NER (Peters et al., 2018; Devlin et al., 2019; Li et al., 2020)."
2020.emnlp-main.514,2020.acl-main.519,0,0.636007,"Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019). Task Specific Pre-training. Unsupervised language model pre-training and task-specific finetuning achieve SOTA results on many NLP tasks, including NER (Peters et al., 2018; Devlin et al., 2019; Li et al., 2020). Recently, with the help of automatically minded knowledge lying in the web, researchers devoted them to the pre-training models for specific tasks, including word sense disambiguation (Huang et al., 2019), word-in-context tasks (Levine et al., 2020), entity-linking and relation classification (Zhang et al., 2019), sentiment classification (Tian et al., 2020). 3 Background In this section, we give a brief introduction to MRC-NER (Li et al., 2020), which achieves satisfying performance in NER and thus is chosen as the foundation of our work. Given an input paragraph X = {x1 , x2 , · · · , xn }"
2020.emnlp-main.514,D18-1226,0,0.02176,"segmentation error propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019). Task Specific Pre-training. Unsupervised language model pre-training and task-specific finetuning achieve SOTA results on many NLP tasks, including NER (Peters et al., 2018; Devlin et al., 2019;"
2020.emnlp-main.514,P19-1524,0,0.0812722,"ide range of sentiment analysis tasks. For supervised NER task, some researchers utilize lattice structure to incorporate the lexical information into character-based NER and avoid the segmentation error propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019). Task Speci"
2020.emnlp-main.514,P16-1101,0,0.182032,"performs other competitive baselines, often by large margins. We also demonstrate that CoFEE pre-training can work well in more challenging, label-free and low-resource scenarios. Further ablation studies show the impact of each pre-training task in achieving these strong performance. To the best of our knowledge, this is the first work that has tackled NER-specific representation during pre-training. 2 Related Work Entity Knowledge for NER. Recently, neural networks have been used for NER and achieved great success (Collobert et al., 2011; dos Santos and Guimar˜aes, 2015; Huang et al., 2015; Ma and Hovy, 2016). Specifically, various types of entity knowledge, including lexical words, gazetteers and anchors in Wikipedia have been proved to be useful for a wide range of sentiment analysis tasks. For supervised NER task, some researchers utilize lattice structure to incorporate the lexical information into character-based NER and avoid the segmentation error propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gaz"
2020.emnlp-main.514,P17-1135,0,0.0644342,"Missing"
2020.emnlp-main.514,P19-1231,0,0.40576,"is a possible solution. However, this process of knowledge acquisition may be inefficient and expensive. In fact, there are extensive weakly labeled annotations that naturally exist on the web yet to be explored for NER model pre-training, which are relatively easier to obtain compared with labeled data (Cao et al., 2019). One can collect them from online resources, such as the Wikipedia anchors and gazetteers (named entity dictionaries). Although automatically derived corpora usually contain massive noisy data, it still contains some extend the valuable semantic information required for NER (Peng et al., 2019). In this paper, we propose a Coarse-to-Fine Entity knowledge Enhanced (CoFEE) pre-training framework for NER task, aiming to gather and utilize knowledge related to named entities. In particular, we first extract anchors from Wikipedia and use them as training corpora for entity span identification. While anchors have no entity type information, the model could get general-typed entity 6345 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6345–6354, c November 16–20, 2020. 2020 Association for Computational Linguistics knowledge from them and learn"
2020.emnlp-main.514,N18-1202,0,0.0560047,"main (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019). Task Specific Pre-training. Unsupervised language model pre-training and task-specific finetuning achieve SOTA results on many NLP tasks, including NER (Peters et al., 2018; Devlin et al., 2019; Li et al., 2020). Recently, with the help of automatically minded knowledge lying in the web, researchers devoted them to the pre-training models for specific tasks, including word sense disambiguation (Huang et al., 2019), word-in-context tasks (Levine et al., 2020), entity-linking and relation classification (Zhang et al., 2019), sentiment classification (Tian et al., 2020). 3 Background In this section, we give a brief introduction to MRC-NER (Li et al., 2020), which achieves satisfying performance in NER and thus is chosen as the foundation of our work. Given an inpu"
2020.emnlp-main.514,P18-1144,1,0.852076,"re-training. 2 Related Work Entity Knowledge for NER. Recently, neural networks have been used for NER and achieved great success (Collobert et al., 2011; dos Santos and Guimar˜aes, 2015; Huang et al., 2015; Ma and Hovy, 2016). Specifically, various types of entity knowledge, including lexical words, gazetteers and anchors in Wikipedia have been proved to be useful for a wide range of sentiment analysis tasks. For supervised NER task, some researchers utilize lattice structure to incorporate the lexical information into character-based NER and avoid the segmentation error propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et"
2020.emnlp-main.514,P19-1139,0,0.023365,"ant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019). Task Specific Pre-training. Unsupervised language model pre-training and task-specific finetuning achieve SOTA results on many NLP tasks, including NER (Peters et al., 2018; Devlin et al., 2019; Li et al., 2020). Recently, with the help of automatically minded knowledge lying in the web, researchers devoted them to the pre-training models for specific tasks, including word sense disambiguation (Huang et al., 2019), word-in-context tasks (Levine et al., 2020), entity-linking and relation classification (Zhang et al., 2019), sentiment classification (Tian et al., 2020). 3 Background In this section, we give a brief introduction to MRC-NER (Li et al., 2020), which achieves satisfying performance in NER and thus is chosen as the foundation of our work. Given an input paragraph X = {x1 , x2 , · · · , xn } where xi denotes the i-th character, NER aims at discovering each entity xstart,end in X and identify its corresponding type y ∈ Y , where Y is the set of predefined tags(e.g., PER, LOC). xstart,end = {xstart , xstart+1 , · · · , xend−1 , xend } is a substring of X satisfying start ≤ end. Specifically, MRCNER form"
2020.emnlp-main.514,P19-1336,0,0.0401103,"Missing"
2020.emnlp-main.514,W15-3904,0,0.077935,"Missing"
2020.emnlp-main.514,D19-1396,0,0.0553228,"have been used for NER and achieved great success (Collobert et al., 2011; dos Santos and Guimar˜aes, 2015; Huang et al., 2015; Ma and Hovy, 2016). Specifically, various types of entity knowledge, including lexical words, gazetteers and anchors in Wikipedia have been proved to be useful for a wide range of sentiment analysis tasks. For supervised NER task, some researchers utilize lattice structure to incorporate the lexical information into character-based NER and avoid the segmentation error propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of wea"
2020.emnlp-main.514,2020.acl-main.374,0,0.0760166,"FEE 1 obtained from Recently, NER has seen remarkable advances with the help of pre-trained representation models, such as BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019). Providing contextual representation, these pre-trained models could be easily applied to NER applications as an encoder by just fine-tuning it. Despite refreshing the state-of-theart performance of NER, the current pre-training techniques are not directly optimized for NER. Typically, these models build unsupervised training objectives to capture dependency between words and learn a general language representation (Tian et al., 2020), while rarely considering incorporating named entity information which can provide rich knowledge for NER. Due to little knowledge connection between NER and general language modeling, how to adapt public pre-trained models to be NER-specific remains an open problem. To this end, injecting named entity knowledge during pre-training is a possible solution. However, this process of knowledge acquisition may be inefficient and expensive. In fact, there are extensive weakly labeled annotations that naturally exist on the web yet to be explored for NER model pre-training, which are relatively easi"
2020.emnlp-main.514,D18-1034,0,0.0182069,"., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019). Task Specific Pre-training. Unsupervised language model pre-training and task-specific finetuning achieve SOTA results on many NLP tasks, including NER (Peters et al., 2018; Devlin et al., 2019; Li et al., 2020). Recently, with the help of automatically minded know"
2020.findings-emnlp.183,C18-1142,0,0.0125009,"the response generation is driven by a single word, and connected each latent variable with words in the vocabulary. Nevertheless, the difficulty is how to target the driving word for a specific post-response pair. More importantly, all of these methods rely on the coarse-grained discourse-level information, which might be insufficient in leading to a satisfactory response. Notably, our work induces the response generation with focus, a fine-grained feature extracted from the discourse-level latent variable. Compared with the variational attention that models the alignment as latent variable (Bahuleyan et al., 2018; Deng et al., 2018), we are mainly inspired by the idea of coverage vector (Tu et al., 2016) to dynamically adjust the attention based on the attention history and the proposed focus. The difference is that Tu et al. (2016) addressed the under/over translation problem and the decoder in their work pays equal attention to the source words. In contrast, our work constrains the decoder to align the decoding attention with the fine-grained focus to generate diverse responses. 3 3.1 Model Preliminaries and Model Overview A neural generative model is trained on a collection of post-response pairs {"
2020.findings-emnlp.183,D19-1198,0,0.35154,"nd to generate high-frequency but trivial responses such as “I don’t know” or “I’m ok” (Li et al., 2016). To address this issue, one promising research line resorts to Conditional Variational Autoencoder (CVAE), which introduces a latent variable to Seq2Seq models through variational learning. The latent variable is supposed to capture the discourselevel semantics of target response and in turn encourage the response informativeness. Recent literature along this line attempted to improve the model performance by putting extra control on the latent variable (Zhao et al., 2017; Gu et al., 2018; Gao et al., 2019). Despite the control, these methods still relied on the discourse-level latent variable, which is too coarse for the decoders to mine sufficient guiding signals at each generation step. As a result, these variational models are observed to ignore the latent variable (Zhao et al., 2017; Gu et al., 2018; Gao et al., 2019) and to generate semantically irrelevant or grammatically disfluent responses (Qiu et al., 2019). In this paper, we propose a novel CVAE-based model, which exploits fine-grained word-level information for diverse response generation. Firstly, we transform the discourse-level in"
2020.findings-emnlp.183,N16-1014,0,0.499623,"ed on 1 https://github.com/cuizhi555/Focus-ConstrainedAttention-Mechanism-for-CVAE-based-ResponseGeneration. the Seq2Seq architecture using maximum likelihood (MLE) training objective. This kind of objective induces the model to treat the post-response relationship as one-to-one mappings. However, the conversations in the real world often embodies one-to-many relationships, where a post is often associated with multiple valid responses (Zhou et al., 2017). Due to this discrepancy, standard Seq2Seq models tend to generate high-frequency but trivial responses such as “I don’t know” or “I’m ok” (Li et al., 2016). To address this issue, one promising research line resorts to Conditional Variational Autoencoder (CVAE), which introduces a latent variable to Seq2Seq models through variational learning. The latent variable is supposed to capture the discourselevel semantics of target response and in turn encourage the response informativeness. Recent literature along this line attempted to improve the model performance by putting extra control on the latent variable (Zhao et al., 2017; Gu et al., 2018; Gao et al., 2019). Despite the control, these methods still relied on the discourse-level latent variabl"
2020.findings-emnlp.183,D16-1230,0,0.0720369,"Missing"
2020.findings-emnlp.183,D15-1166,0,0.425901,"semantically different response directed by a different focus. Our contributions can be summarized as three folds: 1) We propose a novel CVAE-based model for diverse response generation, by directing the decoder with fine-grained information. 2) We introduce focus to represent the fine-grained information, and propose a focus-constrained attention mechanism to make full use of it. 3). Experimental results demonstrate our model outperforms several state-of-the-art models in terms of response’s diversity as well as appropriateness. 2 Related Work The attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) has become a widely-used component for Seq2Seq (Sutskever et al., 2014; Cho et al., 2014) to model Short-Text Conversation (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015). Although promising results have been achieved, attention-based Seq2Seq models still tend to generate generic and trivial responses (Li et al., 2016). There have been many approaches attempted to address this problem. Li et al. (2016) reranked the n-best generated responses based on Maximum Mutual Information (MMI). Shao et al. (2017) adopted segement-level reranking to encourage diversity during early decod"
2020.findings-emnlp.183,P02-1040,0,0.10709,"3.33 9.33 1.26 2.36 2.82 Table 1: The results from automatic and human evaluations. The Kappa score is 0.45 and 0.70 for quality and diversity labeling coding attention at each step is calculated with only the first two terms in Equation 5. 2) OursFocCoverage involves both of the focus F and the coverage vector Dt , where the only difference from Ours-FocConstrain is that it is optimized without the focus constraint Lf oc in Equation 9. 4.4 Evaluation Metrics All models are required to generate 3 responses and are evaluated using both automatic metrics and human judgements: Multi-BLEU: BLEU (Papineni et al., 2002)3 is a common automatic metric to evaluate the response quality. It measures word overlaps between the generated responses and references. We report MultiBLEU scores where each generated response is compared with 5 references. Dist-1/2: Dist-1/2 measures the diversity of generated responses by counting the distinct uni-grams and bi-grams (Li et al., 2016). In our setting, both Intra-Dist and Inter-Dist are evaluated on the results to calculate Dist of responses for a post and the whole testing set, respectively. Human Labeling: Since there is a gap between automatic metrics and human annotatio"
2020.findings-emnlp.183,P19-1372,0,0.0112618,"se informativeness. Recent literature along this line attempted to improve the model performance by putting extra control on the latent variable (Zhao et al., 2017; Gu et al., 2018; Gao et al., 2019). Despite the control, these methods still relied on the discourse-level latent variable, which is too coarse for the decoders to mine sufficient guiding signals at each generation step. As a result, these variational models are observed to ignore the latent variable (Zhao et al., 2017; Gu et al., 2018; Gao et al., 2019) and to generate semantically irrelevant or grammatically disfluent responses (Qiu et al., 2019). In this paper, we propose a novel CVAE-based model, which exploits fine-grained word-level information for diverse response generation. Firstly, we transform the discourse-level information into word-level signals, i.e., focus. By attending the latent variable to the post words, the focus weight measures the response’s correlation with the post 2021 1 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2021–2030 c November 16 - 20, 2020. 2020 Association for Computational Linguistics words. The higher the weight, the semantics is more likely to concentrate on the cor"
2020.findings-emnlp.183,P15-1152,0,0.444209,"-grained signal, our model can generate more diverse and informative responses compared with several state-of-the-art models.1 1 Introduction Nowadays, developing intelligent open-domain conversational systems has become an active research field (Perez-Marin and Pascual-Nieto, 2011; Shum et al., 2018). Compared with rule-based and retrieval-based methods, neural generative models have attracted increasing attention because they do not need extensive feature engineering and have achieved promising results recently with largescale conversational data (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015). Typically, neural generative models are trained to learn the post-response mappings based on 1 https://github.com/cuizhi555/Focus-ConstrainedAttention-Mechanism-for-CVAE-based-ResponseGeneration. the Seq2Seq architecture using maximum likelihood (MLE) training objective. This kind of objective induces the model to treat the post-response relationship as one-to-one mappings. However, the conversations in the real world often embodies one-to-many relationships, where a post is often associated with multiple valid responses (Zhou et al., 2017). Due to this discrepancy, standard Seq2Seq models t"
2020.findings-emnlp.183,D17-1235,0,0.0176546,"riateness. 2 Related Work The attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) has become a widely-used component for Seq2Seq (Sutskever et al., 2014; Cho et al., 2014) to model Short-Text Conversation (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015). Although promising results have been achieved, attention-based Seq2Seq models still tend to generate generic and trivial responses (Li et al., 2016). There have been many approaches attempted to address this problem. Li et al. (2016) reranked the n-best generated responses based on Maximum Mutual Information (MMI). Shao et al. (2017) adopted segement-level reranking to encourage diversity during early decoding steps. However, these reranking-based methods only introduce a few variants of decoded words. Another group of researches attempted to encourage diversity by incorporating extra information. Xing et al. (2017) injected topic words and Yao et al. (2017) introduced a cue word based on Point-wise Mutual Information (PMI) into generation models. Ghazvininejad et al. (2018) grounded on knowledge bases to provide factual information for the decoder. However, it is difficult to ensure these external information are always"
2020.findings-emnlp.183,P16-1008,0,0.126399,"n the vocabulary. Nevertheless, the difficulty is how to target the driving word for a specific post-response pair. More importantly, all of these methods rely on the coarse-grained discourse-level information, which might be insufficient in leading to a satisfactory response. Notably, our work induces the response generation with focus, a fine-grained feature extracted from the discourse-level latent variable. Compared with the variational attention that models the alignment as latent variable (Bahuleyan et al., 2018; Deng et al., 2018), we are mainly inspired by the idea of coverage vector (Tu et al., 2016) to dynamically adjust the attention based on the attention history and the proposed focus. The difference is that Tu et al. (2016) addressed the under/over translation problem and the decoder in their work pays equal attention to the source words. In contrast, our work constrains the decoder to align the decoding attention with the fine-grained focus to generate diverse responses. 3 3.1 Model Preliminaries and Model Overview A neural generative model is trained on a collection of post-response pairs {(x, y)}, and aimed to generate a response y word-by-word given an input x. At the basis of ou"
2020.findings-emnlp.183,D17-1233,0,0.0144798,"tention-based Seq2Seq models still tend to generate generic and trivial responses (Li et al., 2016). There have been many approaches attempted to address this problem. Li et al. (2016) reranked the n-best generated responses based on Maximum Mutual Information (MMI). Shao et al. (2017) adopted segement-level reranking to encourage diversity during early decoding steps. However, these reranking-based methods only introduce a few variants of decoded words. Another group of researches attempted to encourage diversity by incorporating extra information. Xing et al. (2017) injected topic words and Yao et al. (2017) introduced a cue word based on Point-wise Mutual Information (PMI) into generation models. Ghazvininejad et al. (2018) grounded on knowledge bases to provide factual information for the decoder. However, it is difficult to ensure these external information are always appropriate to the conversation context. Another line of research introduced a set of latent responding mechanisms and generated responses based on a selected mechanism. Zhou et al. (2017) learned the post-response mappings as a mixture of the mechanisms, but it is questionable that they only relied on one single mechanism when g"
2020.findings-emnlp.183,P17-1061,0,0.334599,"crepancy, standard Seq2Seq models tend to generate high-frequency but trivial responses such as “I don’t know” or “I’m ok” (Li et al., 2016). To address this issue, one promising research line resorts to Conditional Variational Autoencoder (CVAE), which introduces a latent variable to Seq2Seq models through variational learning. The latent variable is supposed to capture the discourselevel semantics of target response and in turn encourage the response informativeness. Recent literature along this line attempted to improve the model performance by putting extra control on the latent variable (Zhao et al., 2017; Gu et al., 2018; Gao et al., 2019). Despite the control, these methods still relied on the discourse-level latent variable, which is too coarse for the decoders to mine sufficient guiding signals at each generation step. As a result, these variational models are observed to ignore the latent variable (Zhao et al., 2017; Gu et al., 2018; Gao et al., 2019) and to generate semantically irrelevant or grammatically disfluent responses (Qiu et al., 2019). In this paper, we propose a novel CVAE-based model, which exploits fine-grained word-level information for diverse response generation. Firstly,"
2020.findings-emnlp.183,N15-1020,0,0.0686856,"Missing"
2020.iwslt-1.18,P19-1309,0,0.0172207,"e corpus if its source side or target side fails to obey any of the following wild rules reflecting what ‘good data’ should look like. Some of the heuristic filtering methods can deal with aspects that can not be captured with models. • SBERT model: Recently, contextualized word embeddings derived from large-scale pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have achieved new state-of-the-arts in various monolingual NLP tasks. The success has also been extended to cross-lingual scenarios (Schwenk, 2018; Conneau and Lample, 2019; Mulcaire et al., 2019; Artetxe and Schwenk, 2019). Recently, Reimers and Gurevych (2019) proposed sentence BERT (SBERT) to derive semantically meaningful sentence embeddings. According to the training framework of SBERT, we use the multilingual pre-train BERT model5 and finetune it on RFPD to yield useful Chinese and Japanese sentence embeddings in the same space. We reject sentence pairs with a cosine-similarity score below 0.2. • The token (i.e. character sequence between two spaces) length of every sentence is limited less than 50. • Sentence pairs with a length ratio greater than 4 are removed. • Chinese sentences with Chinese characters"
2020.iwslt-1.18,D18-1338,0,0.0221222,"synthetic data with the original data to train the baseline models from scratch in L2R, R2L, and T2S ways, respectively. Finally, we conduct the aforementioned approach based on ensemble models again to achieve better baseline systems. • Wider model: Dimension is an important factor to enhance the Transformer model capacity and performance. Based on the standard Transformer-Big model, we train a Transformer-Wide model with a inner dimension of position-wise feed-forward layers 8,192. • Deeper model: Building deeper networks via stacking more encoder and decoder layers has been a trend in NMT (Bapna et al., 2018; Wu et al., 2019; Zhang et al., 2019). We also exploit three deeper Transformer models by simply increasing the layer size of Transformer-Big, including TransformerDeep-12-12, Transformer-Deep-12-6, and Transformer-Deep-6-12 in which the first number represents the layer size of the encoder and the second number represents the layer size of the decoder. In addition to the standard Transformer in which the residual connection is applied between two adjacent 151 • T2S model: Back-translation has thus far been the most effective technique effective for NMT (Sennrich et al., 2016b). Instead of us"
2020.iwslt-1.18,W17-3209,0,0.0162021,"(Kudo, 2006) and then tokenized only for the non-Japanese part by the Moses script2 . 1 https://github.com/BYVoid/OpenCC https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ tokenizer/tokenizer.perl 2 149 Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 149–157 c July 9-10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 2.2 Parallel Data Filtering • Duplicated sentence pairs are discarded. Though the NMT performance is highly correlated to the huge amounts of training data, a robust body of studies (Carpuat et al., 2017; Khayrallah and Koehn, 2018; Wang et al., 2018; Koehn et al., 2018) has shown the bad impact of noisy data on general NMT translation accuracy. In addition to a small amount of Japanese-Chinese parallel data3 from various public sources, the organizers also provide a large-scale but noisy parallel data4 extracted from a non-parallel web-crawled data through some similarity measures for parallel data mining. We apply a two-stage process consisting of rule-based filtering and model-based scoring to further filter harmful sentence pairs that are bound to negatively affect the quality of NMT syst"
2020.iwslt-1.18,W19-5206,0,0.0182554,"he layer size of the encoder and the second number represents the layer size of the decoder. In addition to the standard Transformer in which the residual connection is applied between two adjacent 151 • T2S model: Back-translation has thus far been the most effective technique effective for NMT (Sennrich et al., 2016b). Instead of using the synthetic training data produced by translating monolingual data in the target language into the source language conventionally, we prepend a special tag to all the source sentences from the synthetic data to distinguish synthetic data from original data (Caswell et al., 2019). • R2L model: Generally, most NMT systems produce translations in an L2R way, which suffers from the issue of exposure bias and consequent error propagation (Ranzato et al., 2016). It has been observed that the accuracy of the right part words in its translation results is usually worse than the left part words (Zhang et al., 2018; Zhou et al., 2019). We train all the baseline systems separately using L2R 6 https://github.com/tensorflow/ tensor2tensor/blob/master/tensor2tensor/ models/evolved_transformer.py and R2L decoding (Wang et al., 2017; Hassan et al., 2018). • L2R model: Knowledge dist"
2020.iwslt-1.18,P17-1176,0,0.0202212,"h suffers from the issue of exposure bias and consequent error propagation (Ranzato et al., 2016). It has been observed that the accuracy of the right part words in its translation results is usually worse than the left part words (Zhang et al., 2018; Zhou et al., 2019). We train all the baseline systems separately using L2R 6 https://github.com/tensorflow/ tensor2tensor/blob/master/tensor2tensor/ models/evolved_transformer.py and R2L decoding (Wang et al., 2017; Hassan et al., 2018). • L2R model: Knowledge distillation has been widely applied to NMT (Kim and Rush, 2016; Freitag et al., 2017; Chen et al., 2017; Gu et al., 2018; Tan et al., 2019). Recent work (Furlanello et al., 2018) demonstrates that the student model can surpass the accuracy of the teacher model, even if the student model is identical to their teacher model. Following this work, the teacher and student models in our experiments keep the same architecture. 3.3 Model Ensembling Ensemble decoding is an effective approach to boost the accuracy of NMT systems via averaging the word distributions output from multiple single models at each decoding step. We select the top 4 systems with the highest BLEU evaluated on the development data"
2020.iwslt-1.18,N19-1423,0,0.0085212,"llel corpus (RFPD). Afterward, we select better sentences according to these scores. 2.2.1 Rule-based Filtering During the first stage, we remove some illegal parallel sentences by applying several rule-based heuristics. A sentence pair is deleted from the corpus if its source side or target side fails to obey any of the following wild rules reflecting what ‘good data’ should look like. Some of the heuristic filtering methods can deal with aspects that can not be captured with models. • SBERT model: Recently, contextualized word embeddings derived from large-scale pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have achieved new state-of-the-arts in various monolingual NLP tasks. The success has also been extended to cross-lingual scenarios (Schwenk, 2018; Conneau and Lample, 2019; Mulcaire et al., 2019; Artetxe and Schwenk, 2019). Recently, Reimers and Gurevych (2019) proposed sentence BERT (SBERT) to derive semantically meaningful sentence embeddings. According to the training framework of SBERT, we use the multilingual pre-train BERT model5 and finetune it on RFPD to yield useful Chinese and Japanese sentence embeddings in the same space. We reject sentence p"
2020.iwslt-1.18,N13-1073,0,0.0357366,"emoved. 3 https://iwslt.oss-cn-beijing.aliyuncs. com/existing_parallel.tgz 4 https://iwslt.oss-cn-beijing.aliyuncs. com/web_crawled_parallel_filtered_1.1. tgz 150 • Translation model: We construct parallel NMT systems based on the standard Transformer-big model in both directions using RFPD to obtain the target synthetic translation as the reference. BEER (Stanojevi´c and Sima’an, 2014) is used as a sentence-level metric of sentence similarity. We prune the sentence pairs with the BEER score of lower than 0.2. • Word alignment model: We perform a word alignment model on RFPD using fast align (Dyer et al., 2013) to check whether the sentence pair has the same meaning. Sentence pairs with the alignment probability of being each other translation less than 0.1 are discarded. • N-gram LM: It is beneficial to use fluent sentences for training NMT models. We train a 5-gram LM that is estimated with modified Kneser-Ney smoothing (Kneser and Ney, 5 https://storage.googleapis.com/bert_ models/2018_11_23/multi_cased_L-12_ H-768_A-12.zip 1995) using KenLM (Heafield, 2011) on each side of the parallel sentences to evaluate sentences’ naturalness. We normalize the LM perplexity (PPL) scores of all the sentences"
2020.iwslt-1.18,W11-2123,0,0.0586679,"Missing"
2020.iwslt-1.18,N16-1046,0,0.0227976,"tems via averaging the word distributions output from multiple single models at each decoding step. We select the top 4 systems with the highest BLEU evaluated on the development dataset from all the available baseline systems of each direction for models ensembling. 3.4 Reranking Reranking technique (Shen et al., 2004) has been applied in the recent years’ WMT tasks (Sennrich et al., 2016a; Wang et al., 2017; Ng et al., 2019) and have provided significant improvements. We first use the S2T-L2R and S2T-R2L ensemble systems to generate more diverse translation hypotheses for a source sentence (Liu et al., 2016). Then we use ensemble models of S2T-L2R, S2T-R2L and T2S-L2R to calculate 3 different likelihood scores for each sentence pair. We obtain the perplexity score for the translation candidates with a neural LM based on the Transformer encoder. We also employ SBERT to calculate the similarity score for each sentence pair. Each model’s score is treated as an individual feature. Considering the ranking problem as a classification problem, we employ the implementation of pairwise ranking in scikit-learn7 RankSVM (Joachims, 2006) to learn the weights of all the features on the development data for re"
2020.iwslt-1.18,2021.ccl-1.108,0,0.115026,"Missing"
2020.iwslt-1.18,N19-1392,0,0.0173608,"pair is deleted from the corpus if its source side or target side fails to obey any of the following wild rules reflecting what ‘good data’ should look like. Some of the heuristic filtering methods can deal with aspects that can not be captured with models. • SBERT model: Recently, contextualized word embeddings derived from large-scale pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have achieved new state-of-the-arts in various monolingual NLP tasks. The success has also been extended to cross-lingual scenarios (Schwenk, 2018; Conneau and Lample, 2019; Mulcaire et al., 2019; Artetxe and Schwenk, 2019). Recently, Reimers and Gurevych (2019) proposed sentence BERT (SBERT) to derive semantically meaningful sentence embeddings. According to the training framework of SBERT, we use the multilingual pre-train BERT model5 and finetune it on RFPD to yield useful Chinese and Japanese sentence embeddings in the same space. We reject sentence pairs with a cosine-similarity score below 0.2. • The token (i.e. character sequence between two spaces) length of every sentence is limited less than 50. • Sentence pairs with a length ratio greater than 4 are removed. • Chinese sente"
2020.iwslt-1.18,W19-5333,0,0.0191262,"this work, the teacher and student models in our experiments keep the same architecture. 3.3 Model Ensembling Ensemble decoding is an effective approach to boost the accuracy of NMT systems via averaging the word distributions output from multiple single models at each decoding step. We select the top 4 systems with the highest BLEU evaluated on the development dataset from all the available baseline systems of each direction for models ensembling. 3.4 Reranking Reranking technique (Shen et al., 2004) has been applied in the recent years’ WMT tasks (Sennrich et al., 2016a; Wang et al., 2017; Ng et al., 2019) and have provided significant improvements. We first use the S2T-L2R and S2T-R2L ensemble systems to generate more diverse translation hypotheses for a source sentence (Liu et al., 2016). Then we use ensemble models of S2T-L2R, S2T-R2L and T2S-L2R to calculate 3 different likelihood scores for each sentence pair. We obtain the perplexity score for the translation candidates with a neural LM based on the Transformer encoder. We also employ SBERT to calculate the similarity score for each sentence pair. Each model’s score is treated as an individual feature. Considering the ranking problem as a"
2020.iwslt-1.18,P02-1040,0,0.112141,"U. All models are trained on one machine with 8 NVIDIA V100 GPUs each of which has 16GB memory for a total of 200K steps. We optimize all models against BLEU using the development set provided by the organizer, stopping early if BLEU does not improve for 16 checkpoints of 2,000 updates each. We set dropout 0.1 for Chinese→Japanese and 0.2 is for Japanese→Chinese. We average the top 10 checkpoints evaluated against the development set as the final model for decoding. During decoding, the beam size is set to 4 for the single model and 10 for ensemble models. We report the 4-gram character BLEU (Papineni et al., 2002) evaluated by the provided automatic evaluation script8 . The approach of two-stage parallel data filtering in Section 2.2 enables us to drastically reduce the training data from 19M to 12M. In order to enlarge the size of bilingual data, we also exploit to extract more high-quality sentence pairs from the provided pre-filtered parallel data9 . We first pre-process the data and use the rules in Section 2.2 to remove 7 https://scikit-learn.org/stable/ modules/generated/sklearn.svm.LinearSVC. html 152 8 https://github.com/didi/iwslt2020_ open_domain_translation/blob/master/ scripts/multi-bleu-de"
2020.iwslt-1.18,D19-1410,0,0.0140789,"side fails to obey any of the following wild rules reflecting what ‘good data’ should look like. Some of the heuristic filtering methods can deal with aspects that can not be captured with models. • SBERT model: Recently, contextualized word embeddings derived from large-scale pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have achieved new state-of-the-arts in various monolingual NLP tasks. The success has also been extended to cross-lingual scenarios (Schwenk, 2018; Conneau and Lample, 2019; Mulcaire et al., 2019; Artetxe and Schwenk, 2019). Recently, Reimers and Gurevych (2019) proposed sentence BERT (SBERT) to derive semantically meaningful sentence embeddings. According to the training framework of SBERT, we use the multilingual pre-train BERT model5 and finetune it on RFPD to yield useful Chinese and Japanese sentence embeddings in the same space. We reject sentence pairs with a cosine-similarity score below 0.2. • The token (i.e. character sequence between two spaces) length of every sentence is limited less than 50. • Sentence pairs with a length ratio greater than 4 are removed. • Chinese sentences with Chinese characters ratio less than 0.15 or any character"
2020.iwslt-1.18,W18-2709,0,0.0130258,"tokenized only for the non-Japanese part by the Moses script2 . 1 https://github.com/BYVoid/OpenCC https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ tokenizer/tokenizer.perl 2 149 Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 149–157 c July 9-10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 2.2 Parallel Data Filtering • Duplicated sentence pairs are discarded. Though the NMT performance is highly correlated to the huge amounts of training data, a robust body of studies (Carpuat et al., 2017; Khayrallah and Koehn, 2018; Wang et al., 2018; Koehn et al., 2018) has shown the bad impact of noisy data on general NMT translation accuracy. In addition to a small amount of Japanese-Chinese parallel data3 from various public sources, the organizers also provide a large-scale but noisy parallel data4 extracted from a non-parallel web-crawled data through some similarity measures for parallel data mining. We apply a two-stage process consisting of rule-based filtering and model-based scoring to further filter harmful sentence pairs that are bound to negatively affect the quality of NMT systems from the original parall"
2020.iwslt-1.18,D16-1139,0,0.0257798,"s produce translations in an L2R way, which suffers from the issue of exposure bias and consequent error propagation (Ranzato et al., 2016). It has been observed that the accuracy of the right part words in its translation results is usually worse than the left part words (Zhang et al., 2018; Zhou et al., 2019). We train all the baseline systems separately using L2R 6 https://github.com/tensorflow/ tensor2tensor/blob/master/tensor2tensor/ models/evolved_transformer.py and R2L decoding (Wang et al., 2017; Hassan et al., 2018). • L2R model: Knowledge distillation has been widely applied to NMT (Kim and Rush, 2016; Freitag et al., 2017; Chen et al., 2017; Gu et al., 2018; Tan et al., 2019). Recent work (Furlanello et al., 2018) demonstrates that the student model can surpass the accuracy of the teacher model, even if the student model is identical to their teacher model. Following this work, the teacher and student models in our experiments keep the same architecture. 3.3 Model Ensembling Ensemble decoding is an effective approach to boost the accuracy of NMT systems via averaging the word distributions output from multiple single models at each decoding step. We select the top 4 systems with the highe"
2020.iwslt-1.18,W18-6453,0,0.0220119,"Moses script2 . 1 https://github.com/BYVoid/OpenCC https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ tokenizer/tokenizer.perl 2 149 Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 149–157 c July 9-10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 2.2 Parallel Data Filtering • Duplicated sentence pairs are discarded. Though the NMT performance is highly correlated to the huge amounts of training data, a robust body of studies (Carpuat et al., 2017; Khayrallah and Koehn, 2018; Wang et al., 2018; Koehn et al., 2018) has shown the bad impact of noisy data on general NMT translation accuracy. In addition to a small amount of Japanese-Chinese parallel data3 from various public sources, the organizers also provide a large-scale but noisy parallel data4 extracted from a non-parallel web-crawled data through some similarity measures for parallel data mining. We apply a two-stage process consisting of rule-based filtering and model-based scoring to further filter harmful sentence pairs that are bound to negatively affect the quality of NMT systems from the original parallel corpora as follows. 2.2.2 Model-based"
2020.iwslt-1.18,P18-2037,0,0.0212184,"everal rule-based heuristics. A sentence pair is deleted from the corpus if its source side or target side fails to obey any of the following wild rules reflecting what ‘good data’ should look like. Some of the heuristic filtering methods can deal with aspects that can not be captured with models. • SBERT model: Recently, contextualized word embeddings derived from large-scale pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have achieved new state-of-the-arts in various monolingual NLP tasks. The success has also been extended to cross-lingual scenarios (Schwenk, 2018; Conneau and Lample, 2019; Mulcaire et al., 2019; Artetxe and Schwenk, 2019). Recently, Reimers and Gurevych (2019) proposed sentence BERT (SBERT) to derive semantically meaningful sentence embeddings. According to the training framework of SBERT, we use the multilingual pre-train BERT model5 and finetune it on RFPD to yield useful Chinese and Japanese sentence embeddings in the same space. We reject sentence pairs with a cosine-similarity score below 0.2. • The token (i.e. character sequence between two spaces) length of every sentence is limited less than 50. • Sentence pairs with a length"
2020.iwslt-1.18,W16-2323,0,0.415961,"ed filtering and model-based scoring. In the aspect of NMT architecture, we exploit some recent Transformer variants, including different Transformer models with deeper layers or wider inner dimension of feed-forward layers than the standard Transformer-Big model, Transformer with a dynamic linear combination of layers (DLCL) (Wang et al., 2019) and neural architecture search (NAS) based Transformer-Evolved (So et al., 2019), to increase the diversity of the system. We further strengthen our systems by diversifying the training data via some effective methods, including back-translation (BT) (Sennrich et al., 2016b), 2 Data 2.1 Pre-processing Our pre-processing pipeline begins by removing non-printable ASCII characters, lowercasing text, normalizing additional white-space, and control character and replacing any escaped characters with the corresponding symbol by our in-house script. All the data is further normalized so all full-width Roman characters and digits are normalized to half-width. All the traditional characters of Chinese data are converted to simplified characters using OpenCC1 . For all corpora, Chinese sentences are segmented by our in-house Chinese word segmenter, and Japanese sentences"
2020.iwslt-1.18,P16-1009,0,0.599685,"ed filtering and model-based scoring. In the aspect of NMT architecture, we exploit some recent Transformer variants, including different Transformer models with deeper layers or wider inner dimension of feed-forward layers than the standard Transformer-Big model, Transformer with a dynamic linear combination of layers (DLCL) (Wang et al., 2019) and neural architecture search (NAS) based Transformer-Evolved (So et al., 2019), to increase the diversity of the system. We further strengthen our systems by diversifying the training data via some effective methods, including back-translation (BT) (Sennrich et al., 2016b), 2 Data 2.1 Pre-processing Our pre-processing pipeline begins by removing non-printable ASCII characters, lowercasing text, normalizing additional white-space, and control character and replacing any escaped characters with the corresponding symbol by our in-house script. All the data is further normalized so all full-width Roman characters and digits are normalized to half-width. All the traditional characters of Chinese data are converted to simplified characters using OpenCC1 . For all corpora, Chinese sentences are segmented by our in-house Chinese word segmenter, and Japanese sentences"
2020.iwslt-1.18,P16-1162,0,0.772165,"ed filtering and model-based scoring. In the aspect of NMT architecture, we exploit some recent Transformer variants, including different Transformer models with deeper layers or wider inner dimension of feed-forward layers than the standard Transformer-Big model, Transformer with a dynamic linear combination of layers (DLCL) (Wang et al., 2019) and neural architecture search (NAS) based Transformer-Evolved (So et al., 2019), to increase the diversity of the system. We further strengthen our systems by diversifying the training data via some effective methods, including back-translation (BT) (Sennrich et al., 2016b), 2 Data 2.1 Pre-processing Our pre-processing pipeline begins by removing non-printable ASCII characters, lowercasing text, normalizing additional white-space, and control character and replacing any escaped characters with the corresponding symbol by our in-house script. All the data is further normalized so all full-width Roman characters and digits are normalized to half-width. All the traditional characters of Chinese data are converted to simplified characters using OpenCC1 . For all corpora, Chinese sentences are segmented by our in-house Chinese word segmenter, and Japanese sentences"
2020.iwslt-1.18,N04-1023,0,0.138253,"ass the accuracy of the teacher model, even if the student model is identical to their teacher model. Following this work, the teacher and student models in our experiments keep the same architecture. 3.3 Model Ensembling Ensemble decoding is an effective approach to boost the accuracy of NMT systems via averaging the word distributions output from multiple single models at each decoding step. We select the top 4 systems with the highest BLEU evaluated on the development dataset from all the available baseline systems of each direction for models ensembling. 3.4 Reranking Reranking technique (Shen et al., 2004) has been applied in the recent years’ WMT tasks (Sennrich et al., 2016a; Wang et al., 2017; Ng et al., 2019) and have provided significant improvements. We first use the S2T-L2R and S2T-R2L ensemble systems to generate more diverse translation hypotheses for a source sentence (Liu et al., 2016). Then we use ensemble models of S2T-L2R, S2T-R2L and T2S-L2R to calculate 3 different likelihood scores for each sentence pair. We obtain the perplexity score for the translation candidates with a neural LM based on the Transformer encoder. We also employ SBERT to calculate the similarity score for eac"
2020.iwslt-1.18,W14-3354,0,0.0684995,"Missing"
2020.iwslt-1.18,W18-6312,0,0.0159087,"verage BLEU score of 0.89. We attribute this finding to the quality gap between the provided Chinese and Japanese data. Table 2a shows that adding large-scale synthetic parallel data back-translated from external monolingual data further boost the performance in different degree. Both the best baseline systems obtain a significant improvement by 1.32 1.32 BLEU score for Zh→Ja. However, it is currently not clear to us how to interpret on the marginal improvement for Ja→Zh. There is a reason to conjecture that we might be suffering from reference bias towards translationese and non-native data (Toral et al., 2018). Unsurprisingly, utilizing diverse models with homogeneous architectures to the ensemble improves translation quality across both the tasks in different degrees. In constrained condition, the Zh→Ja ensemble models gain a substantial improvement compared to the baseline From the Table 2a, our reranking model finally achieves a significant improvement of about 1.4 BLEU score for Zh→Ja, even when applied on top of an ensemble of very strong KD+BT models. However, the improvement of reranking is relatively inconsiderable for Ja→Zh, and we also attribute this to the issue of translationese referen"
2020.iwslt-1.18,W17-4742,0,0.0234332,"distinguish synthetic data from original data (Caswell et al., 2019). • R2L model: Generally, most NMT systems produce translations in an L2R way, which suffers from the issue of exposure bias and consequent error propagation (Ranzato et al., 2016). It has been observed that the accuracy of the right part words in its translation results is usually worse than the left part words (Zhang et al., 2018; Zhou et al., 2019). We train all the baseline systems separately using L2R 6 https://github.com/tensorflow/ tensor2tensor/blob/master/tensor2tensor/ models/evolved_transformer.py and R2L decoding (Wang et al., 2017; Hassan et al., 2018). • L2R model: Knowledge distillation has been widely applied to NMT (Kim and Rush, 2016; Freitag et al., 2017; Chen et al., 2017; Gu et al., 2018; Tan et al., 2019). Recent work (Furlanello et al., 2018) demonstrates that the student model can surpass the accuracy of the teacher model, even if the student model is identical to their teacher model. Following this work, the teacher and student models in our experiments keep the same architecture. 3.3 Model Ensembling Ensemble decoding is an effective approach to boost the accuracy of NMT systems via averaging the word dist"
2020.iwslt-1.18,P19-1558,0,0.0153448,"the original data to train the baseline models from scratch in L2R, R2L, and T2S ways, respectively. Finally, we conduct the aforementioned approach based on ensemble models again to achieve better baseline systems. • Wider model: Dimension is an important factor to enhance the Transformer model capacity and performance. Based on the standard Transformer-Big model, we train a Transformer-Wide model with a inner dimension of position-wise feed-forward layers 8,192. • Deeper model: Building deeper networks via stacking more encoder and decoder layers has been a trend in NMT (Bapna et al., 2018; Wu et al., 2019; Zhang et al., 2019). We also exploit three deeper Transformer models by simply increasing the layer size of Transformer-Big, including TransformerDeep-12-12, Transformer-Deep-12-6, and Transformer-Deep-6-12 in which the first number represents the layer size of the encoder and the second number represents the layer size of the decoder. In addition to the standard Transformer in which the residual connection is applied between two adjacent 151 • T2S model: Back-translation has thus far been the most effective technique effective for NMT (Sennrich et al., 2016b). Instead of using the synthetic"
2020.iwslt-1.18,D19-1083,0,0.0138457,"to train the baseline models from scratch in L2R, R2L, and T2S ways, respectively. Finally, we conduct the aforementioned approach based on ensemble models again to achieve better baseline systems. • Wider model: Dimension is an important factor to enhance the Transformer model capacity and performance. Based on the standard Transformer-Big model, we train a Transformer-Wide model with a inner dimension of position-wise feed-forward layers 8,192. • Deeper model: Building deeper networks via stacking more encoder and decoder layers has been a trend in NMT (Bapna et al., 2018; Wu et al., 2019; Zhang et al., 2019). We also exploit three deeper Transformer models by simply increasing the layer size of Transformer-Big, including TransformerDeep-12-12, Transformer-Deep-12-6, and Transformer-Deep-6-12 in which the first number represents the layer size of the encoder and the second number represents the layer size of the decoder. In addition to the standard Transformer in which the residual connection is applied between two adjacent 151 • T2S model: Back-translation has thus far been the most effective technique effective for NMT (Sennrich et al., 2016b). Instead of using the synthetic training data produc"
2020.iwslt-1.18,Q19-1006,0,0.0141934,"aining data produced by translating monolingual data in the target language into the source language conventionally, we prepend a special tag to all the source sentences from the synthetic data to distinguish synthetic data from original data (Caswell et al., 2019). • R2L model: Generally, most NMT systems produce translations in an L2R way, which suffers from the issue of exposure bias and consequent error propagation (Ranzato et al., 2016). It has been observed that the accuracy of the right part words in its translation results is usually worse than the left part words (Zhang et al., 2018; Zhou et al., 2019). We train all the baseline systems separately using L2R 6 https://github.com/tensorflow/ tensor2tensor/blob/master/tensor2tensor/ models/evolved_transformer.py and R2L decoding (Wang et al., 2017; Hassan et al., 2018). • L2R model: Knowledge distillation has been widely applied to NMT (Kim and Rush, 2016; Freitag et al., 2017; Chen et al., 2017; Gu et al., 2018; Tan et al., 2019). Recent work (Furlanello et al., 2018) demonstrates that the student model can surpass the accuracy of the teacher model, even if the student model is identical to their teacher model. Following this work, the teache"
2020.semeval-1.69,P18-1043,0,0.0834403,"ommons.org/licenses/by/4.0/. 556 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 556–561 Barcelona, Spain (Online), December 12, 2020. The rest of our paper is structured as follows. Section 2 introduces related work. Model and data preparation are described in Section 3. Experiments and evaluation are described in Section 4. The conclusions are drawn in Section 5. 2 Related Work In natural language processing, the problem of common sense verification has always been an important one. The common sense verification problem has a large data set. Such as, Event2Mind (Rashkin et al., 2018) is a crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. Situations with Adversarial Generations (SWAG) (Zellers et al., 2018) is a dataset consisting of 113k multiple choice questions about a rich spectrum of grounded situations. The winograd schema challenge (Trinh and Le, 2018) is a dataset for common sense reasoning. It employs winograd Schema questions that require the resolution of anaphora: the system must identify the antecedent of an ambiguous pronoun in a statement. Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD)"
2020.semeval-1.69,P19-1393,0,0.0629995,"ence have become our common knowledge through summarization and verification. In our daily life, common sense can often tell us some other people’s practical experience, so that we can avoid the repetition of some errors. After scientific verification, a lot of wrong common sense is slowly being corrected. However, it takes a lot of human resources to manually correct normal knowledge. At the same time, in recent years, due to the development of natural language processing and neural networks, common sense revision has entered the era of mechanization. Semeval 2020 task 4 (Wang et al., 2020) (Wang et al., 2019) is designed for common sense verification and interpretation. This task is to directly test whether the system can distinguish meaningful natural language statements from unreasonable natural language statements. In this way, we can save a lot of time to distinguish between common sense and extraordinary knowledge. Subtask A is to choose from two natural language statements with similar wording, one of which is meaningful and the other is meaningless. This subtask is mainly to directly distinguish whether the discourse has common sense. Subtask B is to find out the key reason why this sentenc"
2020.semeval-1.69,2020.semeval-1.39,0,0.0316577,"ge amounts of experience have become our common knowledge through summarization and verification. In our daily life, common sense can often tell us some other people’s practical experience, so that we can avoid the repetition of some errors. After scientific verification, a lot of wrong common sense is slowly being corrected. However, it takes a lot of human resources to manually correct normal knowledge. At the same time, in recent years, due to the development of natural language processing and neural networks, common sense revision has entered the era of mechanization. Semeval 2020 task 4 (Wang et al., 2020) (Wang et al., 2019) is designed for common sense verification and interpretation. This task is to directly test whether the system can distinguish meaningful natural language statements from unreasonable natural language statements. In this way, we can save a lot of time to distinguish between common sense and extraordinary knowledge. Subtask A is to choose from two natural language statements with similar wording, one of which is meaningful and the other is meaningless. This subtask is mainly to directly distinguish whether the discourse has common sense. Subtask B is to find out the key rea"
2020.semeval-1.69,D18-1009,0,0.0545026,"paper is structured as follows. Section 2 introduces related work. Model and data preparation are described in Section 3. Experiments and evaluation are described in Section 4. The conclusions are drawn in Section 5. 2 Related Work In natural language processing, the problem of common sense verification has always been an important one. The common sense verification problem has a large data set. Such as, Event2Mind (Rashkin et al., 2018) is a crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. Situations with Adversarial Generations (SWAG) (Zellers et al., 2018) is a dataset consisting of 113k multiple choice questions about a rich spectrum of grounded situations. The winograd schema challenge (Trinh and Le, 2018) is a dataset for common sense reasoning. It employs winograd Schema questions that require the resolution of anaphora: the system must identify the antecedent of an ambiguous pronoun in a statement. Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) (Zhang et al., 2018) is a large-scale reading comprehension dataset which requires commonsense reasoning. For these data sets, there are some neural networks that can get good per"
2020.semeval-1.86,D15-1075,0,0.0199023,"t a rich spectrum of grounded situations. The winograd schema challenge (Trinh and Le, 2018) is a dataset for common sense reasoning. It employs winograd Schema questions that require the resolution of anaphora: the system must identify the antecedent of an ambiguous pronoun in a statement. Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) (Zhang et al., 2018) is a large-scale reading comprehension dataset which requires commonsense reasoning. In the field of natural inference, there are also a large number of data sets. Such as, The Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) Corpus contains around 550k hypothesis/premise pairs. The Multi-Genre Natural Language Inference (MultiNLI) (Williams et al., 2017) corpus contains around 433k hypothesis/premise pairs. It is similar to the SNLI corpus, but covers a range of genres of spoken and written text and supports crossgenre evaluation. The SciTail (Khot et al., 2018) entailment dataset consists of 27k. In contrast to the SNLI and MultiNLI, it was not crowd-sourced but created from sentences that already exist in the wild. 3 Data Preparation In this part, we mainly introduce our processing of the data set. At the same"
2020.semeval-1.86,P18-1043,0,0.0126697,"3 describes data preparation. Methods are described in Section 4. Experiments and evaluation are described in Section 5. The conclusions are drawn in Section 6. 2 Related Work In the field of natural language processing, common sense problems and natural inference problems are relevant to our task. These two problems have large data sets. By describing the data set, we can show the development of these two fields from the content of the data set. Meanwhile, the enlightenment of related solutions can also be obtained. In the field of common sense, this question has some data sets. Event2Mind (Rashkin et al., 2018) is a crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. Situations with Adversarial Generations (SWAG) (Zellers et al., 2018) is a dataset consisting of 113k multiple choice questions about a rich spectrum of grounded situations. The winograd schema challenge (Trinh and Le, 2018) is a dataset for common sense reasoning. It employs winograd Schema questions that require the resolution of anaphora: the system must identify the antecedent of an ambiguous pronoun in a statement. Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD)"
2020.semeval-1.86,2020.semeval-1.40,0,0.0270643,"tion is right or wrong. However, this will consume a lot of labor costs and cause a waste of resources. At the same time, there are usually some misjudgments in human detection. Therefore, we need some new methods to deal. In the past two years, with the rapid development of artificial intelligence, some related problems have emerged in the field of deep learning. For example, common sense discrimination and natural language inference. The emergence of these problems provides some mechanical methods to solve the problem of error messages. Our research is also carried out. Semeval 2020 task 5 (Yang et al., 2020) is to model causal inference in language: detect counterfactuals. This task is through the establishment of a model, and at the same time to detect whether it is a factual statement based on a large amount of text data. The generation of this task can help us to make a machine judgment of information correctness to a great extent. Subtask A is to detect counterfactual statements. This subtask requires us to determine whether a given statement is counterfactual. A counterfactual statement describes an event that has not actually occurred or cannot occur, and the possible consequences of the ev"
2020.semeval-1.86,D18-1009,0,0.0165651,"the field of natural language processing, common sense problems and natural inference problems are relevant to our task. These two problems have large data sets. By describing the data set, we can show the development of these two fields from the content of the data set. Meanwhile, the enlightenment of related solutions can also be obtained. In the field of common sense, this question has some data sets. Event2Mind (Rashkin et al., 2018) is a crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. Situations with Adversarial Generations (SWAG) (Zellers et al., 2018) is a dataset consisting of 113k multiple choice questions about a rich spectrum of grounded situations. The winograd schema challenge (Trinh and Le, 2018) is a dataset for common sense reasoning. It employs winograd Schema questions that require the resolution of anaphora: the system must identify the antecedent of an ambiguous pronoun in a statement. Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) (Zhang et al., 2018) is a large-scale reading comprehension dataset which requires commonsense reasoning. In the field of natural inference, there are also a large number of data"
2021.emnlp-main.764,W12-3010,0,0.0188034,"-word sentence for are the first to introduce the Open Information 2n times to construct the fact graph. This means Extraction (OpenIE) paradigm, and propose Texour model is more time-consuming (O(M ) vs. tRunner, the first highly scalable model for the O(n)). Actually, IGL-OIE and MacroIE both use task. In the following, various OpenIE systems ap9703 plying costly hand-crafted rules or self-supervised learning paradigm based on linguistic patterns such as part-of-speech tags and syntactic features have been proposed over the years (Wu and Weld, 2010; Fader et al., 2011; Schmitz et al., 2012; Akbik and Löser, 2012; Mesquita et al., 2013; Del Corro and Gemulla, 2013; Yahya et al., 2014; Angeli et al., 2015; Falke et al., 2016; White et al., 2016). They strongly rely on external NLP tools. Thus, their performance depends on the quality of the features obtained from these NLP tools. However, these features are not always accurate for various domains and contexts (Bekoulis et al., 2018). Recently, OpenIE has achieved great advances with the help of supervised neural networks to bypass the handcrafted patterns and alleviate error propagation. There are two main paradigms in the relevant research. The first"
2021.emnlp-main.764,P15-1034,0,0.024596,"ct graph. This means Extraction (OpenIE) paradigm, and propose Texour model is more time-consuming (O(M ) vs. tRunner, the first highly scalable model for the O(n)). Actually, IGL-OIE and MacroIE both use task. In the following, various OpenIE systems ap9703 plying costly hand-crafted rules or self-supervised learning paradigm based on linguistic patterns such as part-of-speech tags and syntactic features have been proposed over the years (Wu and Weld, 2010; Fader et al., 2011; Schmitz et al., 2012; Akbik and Löser, 2012; Mesquita et al., 2013; Del Corro and Gemulla, 2013; Yahya et al., 2014; Angeli et al., 2015; Falke et al., 2016; White et al., 2016). They strongly rely on external NLP tools. Thus, their performance depends on the quality of the features obtained from these NLP tools. However, these features are not always accurate for various domains and contexts (Bekoulis et al., 2018). Recently, OpenIE has achieved great advances with the help of supervised neural networks to bypass the handcrafted patterns and alleviate error propagation. There are two main paradigms in the relevant research. The first one, called taggingbased model (Stanovsky et al., 2018; Roy et al., 2019; Jiang et al., 2019)"
2021.emnlp-main.764,D19-1651,0,0.0271529,"Missing"
2021.emnlp-main.764,2020.emnlp-main.306,0,0.159337,"reshold. During predicate) of each node v in the corresponding fact training, we minimize the negative log-likelihood clique c (lines 12-19). Specifically, we enumerate I ) over the correct tags with the binary of P (yi,j all token pairs (wi , wj ) in the EP table when wi cross-entropy loss. The losses from the tag table of is the boundary token of v and wj is the boundary SE and EP are aggregated as the training objective. word of another node in c, and count the outcomOpenIE models typically assign a confidence ing role part of the predicted tags. For example, if value to an predicted fact (Kolluru et al., 2020a). In the role tag of (wi , wj ) ∈ {S2S, S2P, S2O}, then MacroIE, each fact is assigned a confidence value we will increase the counter of subject. The most by summing the log probabilities of the nodes and predicted type ∈ {subject, predicate, object} is reedges in the respective clique and normalizing this garded as the role of v in c. Finally, all nodes c are by the number of edges and nodes. assembled according to their roles to output the de9699 By learning different table filling parameters for I ) ∈ SE and EP, we can generate different P (yi,j OpenIE4 # of sentences # of facts 91,277 1"
2021.emnlp-main.764,2020.acl-main.521,0,0.0283047,"Missing"
2021.emnlp-main.764,P18-2065,0,0.312166,"-S2S-Loc,B-O2S-Loc,B-O2O-Loc, E-S2O-Loc, E-S2S-Loc,E-O2S-Loc, E-O2O-Loc}, in which B-S2O-Loc means the relation between subject and object is Location. During decoding, If Loc appears in all edges of a clique, and there is no predicate in the clique, then we take Location as the predicate of the fact represented by the clique. 3.3 Baselines We summarize the OpenIE studies and compare our model against several recent neural systems following previous work. They include labeling (RnnOIE (Stanovsky et al., 2018), SenseOIE (Roy et al., 2019) and IGL-OIE (Kolluru et al., 2020a)), generation (NOIE (Cui et al., 2018) and IMoJIE (Kolluru et al., 2020b)) and span-based (SpanOIE (Zhan and Zhao, 2020)) systems. To make comparison on SAOKE, we re-implement the state-of-the-art models IGL-OIE and IMoJIE based on the BERT-base-Chinese encoder using official implementations. Their hyper-parameters have been carefully tuned on the dev set. Note that we compare against IGL-OIE rather than the final system OpenIE6 in (Kolluru et al., 2020a). OpenIE6 is an OpenIE system based on IGL-OIE with human-designed soft rules (generated by POS tools) and a coordination analyzer (trained with additional data). In our experimen"
2021.emnlp-main.764,D13-1043,0,0.0264645,"the first to introduce the Open Information 2n times to construct the fact graph. This means Extraction (OpenIE) paradigm, and propose Texour model is more time-consuming (O(M ) vs. tRunner, the first highly scalable model for the O(n)). Actually, IGL-OIE and MacroIE both use task. In the following, various OpenIE systems ap9703 plying costly hand-crafted rules or self-supervised learning paradigm based on linguistic patterns such as part-of-speech tags and syntactic features have been proposed over the years (Wu and Weld, 2010; Fader et al., 2011; Schmitz et al., 2012; Akbik and Löser, 2012; Mesquita et al., 2013; Del Corro and Gemulla, 2013; Yahya et al., 2014; Angeli et al., 2015; Falke et al., 2016; White et al., 2016). They strongly rely on external NLP tools. Thus, their performance depends on the quality of the features obtained from these NLP tools. However, these features are not always accurate for various domains and contexts (Bekoulis et al., 2018). Recently, OpenIE has achieved great advances with the help of supervised neural networks to bypass the handcrafted patterns and alleviate error propagation. There are two main paradigms in the relevant research. The first one, called taggingbase"
2021.emnlp-main.764,C18-1326,0,0.0122639,"der, as well as the error propagation between facts. Experiments conducted on two benchmark datasets show that our proposed model significantly outperforms current state-of-theart methods, beats the previous systems by as much as 5.7 absolute gain in F1 score. John is the premier and first minister of British Columbia Subject Predicate Object John premier of British Columbia John first minister of British Columbia Figure 1: An example of OpenIE in a sentence. Starting from rule-based systems to neural networks, OpenIE has attracted increasing attention in recent years but remains challenging (Niklaus et al., 2018), due to the intrinsic difficulty in identifying complicated facts, including: (1) Overlapping, one fact element (either subject, predicate, or object) may belong to multiple facts in a sentence. For example, in Figure 1, the entity pairs of two facts are identical but the predicates are different; (2) Discontinuous, one fact element can consist of spans that are separated by intervals, as the predicate of the first fact in Figure 1 comprising two spans premier and of ; (3) Nested, one fact element could contain other elements or share words with other elements. We can see that the two predica"
2021.emnlp-main.764,N19-1423,0,0.0191837,"ssifying the relations between each of these to-end neural architecture MacroIE (Figure 5) to pairs. While being easy to implement, this jointly extract nodes and predict edges. Our arprocess is vulnerable to errors cascading down the chitecture first encodes the n-token sentence to 9698 Span extraction table Edge prediction table Algorithm 1 Overall workflow BERT J o h n i s th e pr e m i e r an d fi r s t m i n i s te r o f Br i ti sh C o lu m b i a Figure 5: Model Architecture. produce contextualized token embedding sequence [h1 , · · · , hn ] with pre-trained language models such as BERT (Devlin et al., 2019). Then we can generate a representation hi,j for the token pair (wi , wj ) as follows: hIi,j = WaI [hi ; hj ; hi hj ; |hi − hj |] + bIa , (1) where [; ] is the vector concatenation, is the element-wise multiplication. WaI is a weight matrix and bIa is a bias vector to be learned during training, I ∈ {SE, EP}is the subtask indicator. Then, we feed hIi,j into a fully-connected layer, which is followed by a Sigmoid function to compute label probability: I P (yi,j ) = Sigmoid(WbI hIi,j + bIb ). Input: Sentence S = {w1 , w2 , · · · , wn } Output: The fact set expressed in S, denoted as F. 1: Fill t"
2021.emnlp-main.764,D19-1067,0,0.15395,"example, we designed eight additional tags to represent it: {B-S2O-Loc, B-S2S-Loc,B-O2S-Loc,B-O2O-Loc, E-S2O-Loc, E-S2S-Loc,E-O2S-Loc, E-O2O-Loc}, in which B-S2O-Loc means the relation between subject and object is Location. During decoding, If Loc appears in all edges of a clique, and there is no predicate in the clique, then we take Location as the predicate of the fact represented by the clique. 3.3 Baselines We summarize the OpenIE studies and compare our model against several recent neural systems following previous work. They include labeling (RnnOIE (Stanovsky et al., 2018), SenseOIE (Roy et al., 2019) and IGL-OIE (Kolluru et al., 2020a)), generation (NOIE (Cui et al., 2018) and IMoJIE (Kolluru et al., 2020b)) and span-based (SpanOIE (Zhan and Zhao, 2020)) systems. To make comparison on SAOKE, we re-implement the state-of-the-art models IGL-OIE and IMoJIE based on the BERT-base-Chinese encoder using official implementations. Their hyper-parameters have been carefully tuned on the dev set. Note that we compare against IGL-OIE rather than the final system OpenIE6 in (Kolluru et al., 2020a). OpenIE6 is an OpenIE system based on IGL-OIE with human-designed soft rules (generated by POS tools) an"
2021.emnlp-main.764,D11-1142,0,0.0852942,"l. (2007) facts, our model tags the same n-word sentence for are the first to introduce the Open Information 2n times to construct the fact graph. This means Extraction (OpenIE) paradigm, and propose Texour model is more time-consuming (O(M ) vs. tRunner, the first highly scalable model for the O(n)). Actually, IGL-OIE and MacroIE both use task. In the following, various OpenIE systems ap9703 plying costly hand-crafted rules or self-supervised learning paradigm based on linguistic patterns such as part-of-speech tags and syntactic features have been proposed over the years (Wu and Weld, 2010; Fader et al., 2011; Schmitz et al., 2012; Akbik and Löser, 2012; Mesquita et al., 2013; Del Corro and Gemulla, 2013; Yahya et al., 2014; Angeli et al., 2015; Falke et al., 2016; White et al., 2016). They strongly rely on external NLP tools. Thus, their performance depends on the quality of the features obtained from these NLP tools. However, these features are not always accurate for various domains and contexts (Bekoulis et al., 2018). Recently, OpenIE has achieved great advances with the help of supervised neural networks to bypass the handcrafted patterns and alleviate error propagation. There are two main p"
2021.emnlp-main.764,D16-1086,0,0.0161297,"Extraction (OpenIE) paradigm, and propose Texour model is more time-consuming (O(M ) vs. tRunner, the first highly scalable model for the O(n)). Actually, IGL-OIE and MacroIE both use task. In the following, various OpenIE systems ap9703 plying costly hand-crafted rules or self-supervised learning paradigm based on linguistic patterns such as part-of-speech tags and syntactic features have been proposed over the years (Wu and Weld, 2010; Fader et al., 2011; Schmitz et al., 2012; Akbik and Löser, 2012; Mesquita et al., 2013; Del Corro and Gemulla, 2013; Yahya et al., 2014; Angeli et al., 2015; Falke et al., 2016; White et al., 2016). They strongly rely on external NLP tools. Thus, their performance depends on the quality of the features obtained from these NLP tools. However, these features are not always accurate for various domains and contexts (Bekoulis et al., 2018). Recently, OpenIE has achieved great advances with the help of supervised neural networks to bypass the handcrafted patterns and alleviate error propagation. There are two main paradigms in the relevant research. The first one, called taggingbased model (Stanovsky et al., 2018; Roy et al., 2019; Jiang et al., 2019), labels each word i"
2021.emnlp-main.764,P19-1523,0,0.0145982,"Angeli et al., 2015; Falke et al., 2016; White et al., 2016). They strongly rely on external NLP tools. Thus, their performance depends on the quality of the features obtained from these NLP tools. However, these features are not always accurate for various domains and contexts (Bekoulis et al., 2018). Recently, OpenIE has achieved great advances with the help of supervised neural networks to bypass the handcrafted patterns and alleviate error propagation. There are two main paradigms in the relevant research. The first one, called taggingbased model (Stanovsky et al., 2018; Roy et al., 2019; Jiang et al., 2019), labels each word in the sentence as either subject, predicate, object, or None for extraction. To identify complicated facts containing overlapping, discontinuous, or nested elements, the recent tagging-based model (Kolluru et al., 2020a) generates a list of tag sequences for one sentence where each sequence corresponding to one extracted fact. The tag sequences are labeled one by one iteratively, e.g., the predicted labels of one tag sequence are passed to the next iteration to fill up another sequence to avoid redundant extraction. Generating-based methods belong to another major paradigm."
2021.emnlp-main.764,D12-1048,0,0.0572479,"model tags the same n-word sentence for are the first to introduce the Open Information 2n times to construct the fact graph. This means Extraction (OpenIE) paradigm, and propose Texour model is more time-consuming (O(M ) vs. tRunner, the first highly scalable model for the O(n)). Actually, IGL-OIE and MacroIE both use task. In the following, various OpenIE systems ap9703 plying costly hand-crafted rules or self-supervised learning paradigm based on linguistic patterns such as part-of-speech tags and syntactic features have been proposed over the years (Wu and Weld, 2010; Fader et al., 2011; Schmitz et al., 2012; Akbik and Löser, 2012; Mesquita et al., 2013; Del Corro and Gemulla, 2013; Yahya et al., 2014; Angeli et al., 2015; Falke et al., 2016; White et al., 2016). They strongly rely on external NLP tools. Thus, their performance depends on the quality of the features obtained from these NLP tools. However, these features are not always accurate for various domains and contexts (Bekoulis et al., 2018). Recently, OpenIE has achieved great advances with the help of supervised neural networks to bypass the handcrafted patterns and alleviate error propagation. There are two main paradigms in the releva"
2021.emnlp-main.764,N18-1081,0,0.137646,"9700 Train Dev Test Location as an example, we designed eight additional tags to represent it: {B-S2O-Loc, B-S2S-Loc,B-O2S-Loc,B-O2O-Loc, E-S2O-Loc, E-S2S-Loc,E-O2S-Loc, E-O2O-Loc}, in which B-S2O-Loc means the relation between subject and object is Location. During decoding, If Loc appears in all edges of a clique, and there is no predicate in the clique, then we take Location as the predicate of the fact represented by the clique. 3.3 Baselines We summarize the OpenIE studies and compare our model against several recent neural systems following previous work. They include labeling (RnnOIE (Stanovsky et al., 2018), SenseOIE (Roy et al., 2019) and IGL-OIE (Kolluru et al., 2020a)), generation (NOIE (Cui et al., 2018) and IMoJIE (Kolluru et al., 2020b)) and span-based (SpanOIE (Zhan and Zhao, 2020)) systems. To make comparison on SAOKE, we re-implement the state-of-the-art models IGL-OIE and IMoJIE based on the BERT-base-Chinese encoder using official implementations. Their hyper-parameters have been carefully tuned on the dev set. Note that we compare against IGL-OIE rather than the final system OpenIE6 in (Kolluru et al., 2020a). OpenIE6 is an OpenIE system based on IGL-OIE with human-designed soft rule"
2021.emnlp-main.764,2020.findings-emnlp.69,0,0.0424174,"Missing"
2021.emnlp-main.764,2021.acl-long.63,1,0.676275,"e OpenIE model, which is able to extract all kinds of facts without relying on the dependency among facts, realizing non-autoregressive open information extraction. Maximal clique discovery is to find the clique with most nodes in a given graph (Lu et al., 2017). This problem has been extensively studied in graph theory and directly applied in various fields, such as community search in social networks (Papadopoulos et al., 2012), team formation in expert networks (Lappas et al., 2009), anomaly detection in complex networks (Leung and Leckie, 2005), and discontinuous named entity recognition (Wang et al., 2021). Motivated by the finding that all the elements in a fact of OpenIE have pairwise strong connections, which is similar to the property of maximal clique in the graph, we extend the concept of maximal clique discovery to OpenIE, and successfully implement task transformation. Our results show that OpenIE can be cleverly cast as a maximal clique discovery problem on a fact graph. 5 Conclusion In this paper, we present a non-autoregressive OpenIE system MacroIE. It predicts the fact set at once based on a novel view of OpenIE as a maximal clique discovery problem, thus be relieved of predicting"
2021.emnlp-main.764,D16-1177,0,0.070073,"Missing"
2021.emnlp-main.764,P10-1013,0,0.056776,", 2018). Banko et al. (2007) facts, our model tags the same n-word sentence for are the first to introduce the Open Information 2n times to construct the fact graph. This means Extraction (OpenIE) paradigm, and propose Texour model is more time-consuming (O(M ) vs. tRunner, the first highly scalable model for the O(n)). Actually, IGL-OIE and MacroIE both use task. In the following, various OpenIE systems ap9703 plying costly hand-crafted rules or self-supervised learning paradigm based on linguistic patterns such as part-of-speech tags and syntactic features have been proposed over the years (Wu and Weld, 2010; Fader et al., 2011; Schmitz et al., 2012; Akbik and Löser, 2012; Mesquita et al., 2013; Del Corro and Gemulla, 2013; Yahya et al., 2014; Angeli et al., 2015; Falke et al., 2016; White et al., 2016). They strongly rely on external NLP tools. Thus, their performance depends on the quality of the features obtained from these NLP tools. However, these features are not always accurate for various domains and contexts (Bekoulis et al., 2018). Recently, OpenIE has achieved great advances with the help of supervised neural networks to bypass the handcrafted patterns and alleviate error propagation."
2021.emnlp-main.764,D14-1038,0,0.0179275,"to construct the fact graph. This means Extraction (OpenIE) paradigm, and propose Texour model is more time-consuming (O(M ) vs. tRunner, the first highly scalable model for the O(n)). Actually, IGL-OIE and MacroIE both use task. In the following, various OpenIE systems ap9703 plying costly hand-crafted rules or self-supervised learning paradigm based on linguistic patterns such as part-of-speech tags and syntactic features have been proposed over the years (Wu and Weld, 2010; Fader et al., 2011; Schmitz et al., 2012; Akbik and Löser, 2012; Mesquita et al., 2013; Del Corro and Gemulla, 2013; Yahya et al., 2014; Angeli et al., 2015; Falke et al., 2016; White et al., 2016). They strongly rely on external NLP tools. Thus, their performance depends on the quality of the features obtained from these NLP tools. However, these features are not always accurate for various domains and contexts (Bekoulis et al., 2018). Recently, OpenIE has achieved great advances with the help of supervised neural networks to bypass the handcrafted patterns and alleviate error propagation. There are two main paradigms in the relevant research. The first one, called taggingbased model (Stanovsky et al., 2018; Roy et al., 2019"
2021.findings-acl.3,2020.acl-main.128,0,0.173499,"port set S. Formally, a {S, Q} pair is called a N -way-K-shot task T . There exist two datasets consisting of a set of tasks train test : Dtrain = {T (i) }M and Dtest = {T (i) }M i=1 i=1 where Mtrain and Mtest denote the number of the task in two datasets respectively. As the name suggests, Dtrain is used to train models in the training phase while Dtest is for evaluation. It is noted that these two datasets have their own event types, which means that the label space of two datasets is disjoint with each other. Few-shot Sequence Labeling In recent years, several works (Fritzler et al., 2019; Hou et al., 2020; Yang and Katiyar, 2020) have been proposed to solve the few-shot named entity recognition using sequence labeling methods. Fritzler et al. (2019) applied the vanilla CRF in the few-shot scenario directly. Hou et al. (2020) proposed a collapsed dependency transfer mechanism (CDT) into CRF, which learns label dependency patterns of a set of task-agnostic abstract labels and utilizes these patterns as transition scores for novel labels. Yang and Katiyar (2020) trains their model on the training data in a standard supervised learning manner and then uses the prototypical networks and the CDT for"
2021.findings-acl.3,W06-0901,0,0.212784,"Missing"
2021.findings-acl.3,P15-1017,0,0.115125,"9), we treat the transition score as the random variable and utilize the Gaussian distribution to approximate its distribution to model the uncertainty. Thus, our PA-CRF is to estimate the parameters of the Gaussian distribution rather than the transition scores directly, i.e., in the amortized manner (Kingma and Welling, 2014; Gordon et al., 2019). The Probabilistic In2 Related Work Few-shot Event Detection Event Detection (ED) aims to recognize the specific type of events in a sentence. In recent years, various neural-based models have been proposed and achieved promising performance in ED (Chen et al., 2015; Nguyen and Grishman, 2015, 2018; Liu et al., 2018; Yan et al., 2019; Cui et al., 2020b). Chen et al. (2015) and Nguyen and Grishman (2015) proposed the convolution architecture to capture the semantic information in the sentence. Nguyen et al. (2016) introduced the recurrent neural network to model the sequence contextual information of words. Recently, GCN-based models (Nguyen and Grishman, 2018; Liu et al., 2018; Yan et al., 2019; Cui et al., 2020b) have been proposed to exploit the syntactic dependency information and achieved state-of-theart performance. However, all these models are dat"
2021.findings-acl.3,2020.nuse-1.5,0,0.0730152,"sentence. Nguyen et al. (2016) introduced the recurrent neural network to model the sequence contextual information of words. Recently, GCN-based models (Nguyen and Grishman, 2018; Liu et al., 2018; Yan et al., 2019; Cui et al., 2020b) have been proposed to exploit the syntactic dependency information and achieved state-of-theart performance. However, all these models are data-hungry, limiting dramatically their usability and deployability in real-world scenarios. Recently, there has been an increasing research interest in solving event detection in the few-shot scenarios (Deng et al., 2020; Lai et al., 2020a,b), by exploiting the Few-Shot Learning (Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017; Sung et al., 2018; Cong et al., 2020). Lai et al. (2020a) proposed LoLoss which splits the part of the support set to act as the auxiliary query set to train the 29 model. Lai et al. (2020b) introduced two regularization matching losses to improve the performance of models. These works only focus on the fewshot trigger classification which classifies the event type of the annotated trigger according to the context based on few samples. This is unrealistic as triggers of novel events are pred"
2021.findings-acl.3,D18-1156,0,0.0452529,"Missing"
2021.findings-acl.3,2020.findings-emnlp.211,1,0.904604,"label prototypes. Then Gaussian distribution is introduced for modeling of the transition scores to alleviate the uncertain estimation resulting from insufficient data. Experimental results show that the unified models work better than existing identifythen-classify models and our PA-CRF further achieves the best results on the benchmark dataset FewEvent. Our code and data are available at http://github.com/congxin95/ PA-CRF. 1 Figure 1: An example from FewEvent dataset revealing the trigger discrepancy. “[·]” marks the event trigger. Grishman, 2015, 2018; Liu et al., 2018; Yan et al., 2019; Cui et al., 2020b,a). These models have achieved promising performance and proved the effectiveness of solving ED in the joint framework. But they almost followed the supervised learning paradigm and depended on the large-scale humanannotated dataset, while new event types emerge every day and most of them suffer from the lack of sufficient annotated data. In the case of insufficient resources, existing joint models cannot recognize the novel event types with only few samples, i.e., Few-Shot Event Detection (FSED). One intuitive way to solve this problem is to first identify event triggers in the conventional"
2021.findings-acl.3,N16-1034,0,0.0193807,"sition scores directly, i.e., in the amortized manner (Kingma and Welling, 2014; Gordon et al., 2019). The Probabilistic In2 Related Work Few-shot Event Detection Event Detection (ED) aims to recognize the specific type of events in a sentence. In recent years, various neural-based models have been proposed and achieved promising performance in ED (Chen et al., 2015; Nguyen and Grishman, 2015, 2018; Liu et al., 2018; Yan et al., 2019; Cui et al., 2020b). Chen et al. (2015) and Nguyen and Grishman (2015) proposed the convolution architecture to capture the semantic information in the sentence. Nguyen et al. (2016) introduced the recurrent neural network to model the sequence contextual information of words. Recently, GCN-based models (Nguyen and Grishman, 2018; Liu et al., 2018; Yan et al., 2019; Cui et al., 2020b) have been proposed to exploit the syntactic dependency information and achieved state-of-theart performance. However, all these models are data-hungry, limiting dramatically their usability and deployability in real-world scenarios. Recently, there has been an increasing research interest in solving event detection in the few-shot scenarios (Deng et al., 2020; Lai et al., 2020a,b), by exploi"
2021.findings-acl.3,P15-2060,0,0.0326737,"ansition score as the random variable and utilize the Gaussian distribution to approximate its distribution to model the uncertainty. Thus, our PA-CRF is to estimate the parameters of the Gaussian distribution rather than the transition scores directly, i.e., in the amortized manner (Kingma and Welling, 2014; Gordon et al., 2019). The Probabilistic In2 Related Work Few-shot Event Detection Event Detection (ED) aims to recognize the specific type of events in a sentence. In recent years, various neural-based models have been proposed and achieved promising performance in ED (Chen et al., 2015; Nguyen and Grishman, 2015, 2018; Liu et al., 2018; Yan et al., 2019; Cui et al., 2020b). Chen et al. (2015) and Nguyen and Grishman (2015) proposed the convolution architecture to capture the semantic information in the sentence. Nguyen et al. (2016) introduced the recurrent neural network to model the sequence contextual information of words. Recently, GCN-based models (Nguyen and Grishman, 2018; Liu et al., 2018; Yan et al., 2019; Cui et al., 2020b) have been proposed to exploit the syntactic dependency information and achieved state-of-theart performance. However, all these models are data-hungry, limiting dramatic"
2021.findings-acl.3,N19-1423,0,0.465153,"g the previous work (Snell et al., 2017), we calculate the prototype of each label by averaging all the word representations with that label in the support set S: Figure 2 gives an illustration of PA-CRF. We detail each component from the bottom to the top. 4.2 Emission Module The Emission Module assigns the emission scores to each token of sentences in the query set Q with regard to each label based on the support set S. 4.2.1 Base Encoder Base Encoder aims to embed tokens in both support set S and query set Q into real-value embedding vectors to capture the semantic information. Since BERT (Devlin et al., 2019) shows its advanced ability to capture the sequence information and has been widely used in NLP tasks recently, we use it as the backbone. Given an input word sequence x, BERT first maps all tokens into hidden embedding representations. We denote this operation as: {h1 , h2 , . . . , hn } = BERT(x) ci = 1 |S(yi )| X h, i = 1, 2, . . . , 2N + 1, (2) w∈S(yi ) where ci denotes the prototype for label yi , S(yi ) refers to the token set containing all words in the support set S with label yi , h represents the corresponding hidden representation of token w, and |· | is the number of set elements."
2021.findings-acl.3,D19-1582,0,0.0572784,"abels based on the label prototypes. Then Gaussian distribution is introduced for modeling of the transition scores to alleviate the uncertain estimation resulting from insufficient data. Experimental results show that the unified models work better than existing identifythen-classify models and our PA-CRF further achieves the best results on the benchmark dataset FewEvent. Our code and data are available at http://github.com/congxin95/ PA-CRF. 1 Figure 1: An example from FewEvent dataset revealing the trigger discrepancy. “[·]” marks the event trigger. Grishman, 2015, 2018; Liu et al., 2018; Yan et al., 2019; Cui et al., 2020b,a). These models have achieved promising performance and proved the effectiveness of solving ED in the joint framework. But they almost followed the supervised learning paradigm and depended on the large-scale humanannotated dataset, while new event types emerge every day and most of them suffer from the lack of sufficient annotated data. In the case of insufficient resources, existing joint models cannot recognize the novel event types with only few samples, i.e., Few-Shot Event Detection (FSED). One intuitive way to solve this problem is to first identify event triggers i"
2021.findings-acl.3,W19-3502,0,0.0199588,"ce phase, the Viterbi algorithm (Forney, 1973) is employed to decode the best-predicted label sequence for the query set. 5 Evaluation Baselines To investigate the effectiveness of our proposed method, we compare it with a range of baselines and state-of-the-art models, which can be categorized into three classes: fine-tuning paradigm, identify-then-classify paradigm and unified paradigm. Fine-tuning paradigm solves the FSED in the standard supervised learning, i.e., pre-training on the large scale dataset and fine-tuning on the handful target data. We adopt the state-of-the-art model, PLMEE (Yang et al., 2019), of the standard ED into the FSED directly. Identify-then-classify models first perform trigger identification (named as TI) and then classify the event types based on the few-shot learning methods (named as FSTC). We investigate two typed of identify-then-classify paradigms: separate and multi-task. For the separate models, the trigger identifier and few-shot trigger classifier are Experiment Dataset We conduct experiments on the benchmark FewEvent dataset introduced in the previous work (Deng et al., 2020), which is the currently largest few-shot dataset for event detection. It contains 70,"
2021.findings-acl.3,2020.emnlp-main.516,0,0.111325,"ly, a {S, Q} pair is called a N -way-K-shot task T . There exist two datasets consisting of a set of tasks train test : Dtrain = {T (i) }M and Dtest = {T (i) }M i=1 i=1 where Mtrain and Mtest denote the number of the task in two datasets respectively. As the name suggests, Dtrain is used to train models in the training phase while Dtest is for evaluation. It is noted that these two datasets have their own event types, which means that the label space of two datasets is disjoint with each other. Few-shot Sequence Labeling In recent years, several works (Fritzler et al., 2019; Hou et al., 2020; Yang and Katiyar, 2020) have been proposed to solve the few-shot named entity recognition using sequence labeling methods. Fritzler et al. (2019) applied the vanilla CRF in the few-shot scenario directly. Hou et al. (2020) proposed a collapsed dependency transfer mechanism (CDT) into CRF, which learns label dependency patterns of a set of task-agnostic abstract labels and utilizes these patterns as transition scores for novel labels. Yang and Katiyar (2020) trains their model on the training data in a standard supervised learning manner and then uses the prototypical networks and the CDT for prediction in the infere"
2021.naacl-main.436,D15-1141,1,0.91683,"component for other NLP ever, there are multiple inconsistent segmentation tasks like named entity recognition (Li et al., 2020), criteria for CWS, where each criterion represents a dependency parsing (Yan et al., 2020), and seman- unique style of segmenting Chinese sentence into tic role labeling (Xia et al., 2019), etc. words, as shown in Table 1. Meanwhile, we can easily observe that different segmentation criteria Generally, most previous studies model the CWS could share a large proportion of word boundaries task as a character-based sequence labeling task (Xue, 2003; Zheng et al., 2013; Chen et al., 2015; between them, such as the boundaries between Ma et al., 2018; Qiu et al., 2020). Recently, pre- word units “李娜(Li Na)”, “进入(entered)” and trained models (PTMs) such as BERT (Devlin et al., “半决赛(the semi-final)”, which are the same for all segmentation criteria. It shows that the com2019) have been introduced into CWS tasks, which mon prior segmentation knowledge is shared by could provide prior semantic knowledge and boost the performance of CWS systems. Yang (2019) di- different criteria. rectly fine-tunes BERT on several CWS benchmark In this paper, we propose a CWS-specific predatasets. H"
2021.naacl-main.436,P17-1110,1,0.828203,"Missing"
2021.naacl-main.436,N19-1423,0,0.154029,"e. In this section, we will describe M ETA S EG in three parts. First, we introduce the Transformerbased unified architecture. Second, we elaborate on the multi-criteria pre-training task with meta learning algorithm. Finally, we give a brief description of the downstream fine-tuning phase. 3.1 The Unified Architecture In traditional CWS systems (Chen et al., 2015; Ma et al., 2018), CWS model usually adopts a sepa2 Related Work rate architecture for each segmentation criterion. Recently, PTMs have been used for CWS and An instance of the CWS model is created for each achieve good performance (Devlin et al., 2019). criterion and trained on the corresponding dataset These PTMs usually exploit fine-tuning as the independently. Thus, a model instance can only main way of transferring prior knowledge to down- serve one criterion, without sharing any segmentastream CWS tasks. Specifically, some methods di- tion knowledge with other different criteria. rectly fine-tune PTMs on CWS tasks (Yang, 2019), To better leverage the common segmentation while others fine-tune them in a multi-task frame- knowledge shared by multiple criteria, M ETA S EG work (Huang et al., 2020). Besides, other features employs a unifie"
2021.naacl-main.436,2020.coling-main.186,0,0.742128,"5; between them, such as the boundaries between Ma et al., 2018; Qiu et al., 2020). Recently, pre- word units “李娜(Li Na)”, “进入(entered)” and trained models (PTMs) such as BERT (Devlin et al., “半决赛(the semi-final)”, which are the same for all segmentation criteria. It shows that the com2019) have been introduced into CWS tasks, which mon prior segmentation knowledge is shared by could provide prior semantic knowledge and boost the performance of CWS systems. Yang (2019) di- different criteria. rectly fine-tunes BERT on several CWS benchmark In this paper, we propose a CWS-specific predatasets. Huang et al. (2020) fine-tunes BERT in a trained model M ETA S EG. To leverage shared 5514 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5514–5523 June 6–11, 2021. ©2021 Association for Computational Linguistics segmentation knowledge of different criteria, M ETA S EG utilizes a unified architecture and introduces a multi-criteria pre-training task. Moreover, to alleviate the discrepancy between pretrained models and downstream unseen criteria, meta learning algorithm (Finn et al., 2017) is incorporated into t"
2021.naacl-main.436,I08-4010,0,0.702909,"Missing"
2021.naacl-main.436,2020.emnlp-main.567,0,0.042885,"k-specific prior knowledge into segment and position embeddings. The Transmultiple NLP tasks. Specifically designed pre- former network is used as the shared encoder layer, training tasks are introduced to obtain the task- encoding the input representations into hidden repspecific pre-trained models, and then these models resentations through blocks of multi-head attention are fine-tuned on corresponding downstream NLP and position-wise feed-forward modules (Vaswani tasks, such as named entity recognition (Xue et al., et al., 2017). Then a shared linear decoder with 2020), sentiment analysis (Ke et al., 2020) and text softmax is followed to map hidden representations summarization (Zhang et al., 2020). In this pa- to the probability distribution of segmentation laper, we propose a CWS-specific pre-trained model bels. The segmentation labels consist of four CWS M ETA S EG. labels {B, M, E, S}, denoting the word beginning, middle, ending and single word respectively. 3 Approach Formally, the unified architecture can be conAs other task-specific pre-trained models (Ke et al., cluded as a probabilistic model Pθ (Y |X), which 2020), the pipeline of M ETA S EG is divided into represents the probability"
2021.naacl-main.436,2020.acl-main.611,1,0.798971,"the discrepancy between pre-training 1 Introduction tasks and downstream CWS tasks. Chinese Word Segmentation (CWS) is a fundamenTo deal with aforementioned problems of PTMs, tal task for Chinese natural language processing we consider introducing a CWS-specific pre(NLP), which aims at identifying word boundaries trained model based on existing CWS corpora, to in a sentence composed of continuous Chinese char- leverage the prior segmentation knowledge. Howacters. It provides a basic component for other NLP ever, there are multiple inconsistent segmentation tasks like named entity recognition (Li et al., 2020), criteria for CWS, where each criterion represents a dependency parsing (Yan et al., 2020), and seman- unique style of segmenting Chinese sentence into tic role labeling (Xia et al., 2019), etc. words, as shown in Table 1. Meanwhile, we can easily observe that different segmentation criteria Generally, most previous studies model the CWS could share a large proportion of word boundaries task as a character-based sequence labeling task (Xue, 2003; Zheng et al., 2013; Chen et al., 2015; between them, such as the boundaries between Ma et al., 2018; Qiu et al., 2020). Recently, pre- word units “李"
2021.naacl-main.436,2020.emnlp-main.317,0,0.632169,"Missing"
2021.naacl-main.436,I05-3017,0,0.834388,"Missing"
2021.naacl-main.436,D18-1529,0,0.19393,"Missing"
2021.naacl-main.436,2020.tacl-1.6,1,0.857152,"e Word Segmentation (CWS) is a fundamenTo deal with aforementioned problems of PTMs, tal task for Chinese natural language processing we consider introducing a CWS-specific pre(NLP), which aims at identifying word boundaries trained model based on existing CWS corpora, to in a sentence composed of continuous Chinese char- leverage the prior segmentation knowledge. Howacters. It provides a basic component for other NLP ever, there are multiple inconsistent segmentation tasks like named entity recognition (Li et al., 2020), criteria for CWS, where each criterion represents a dependency parsing (Yan et al., 2020), and seman- unique style of segmenting Chinese sentence into tic role labeling (Xia et al., 2019), etc. words, as shown in Table 1. Meanwhile, we can easily observe that different segmentation criteria Generally, most previous studies model the CWS could share a large proportion of word boundaries task as a character-based sequence labeling task (Xue, 2003; Zheng et al., 2013; Chen et al., 2015; between them, such as the boundaries between Ma et al., 2018; Qiu et al., 2020). Recently, pre- word units “李娜(Li Na)”, “进入(entered)” and trained models (PTMs) such as BERT (Devlin et al., “半决赛(the se"
2021.naacl-main.436,2020.findings-emnlp.260,1,0.822043,"like named entity recognition (Li et al., 2020), criteria for CWS, where each criterion represents a dependency parsing (Yan et al., 2020), and seman- unique style of segmenting Chinese sentence into tic role labeling (Xia et al., 2019), etc. words, as shown in Table 1. Meanwhile, we can easily observe that different segmentation criteria Generally, most previous studies model the CWS could share a large proportion of word boundaries task as a character-based sequence labeling task (Xue, 2003; Zheng et al., 2013; Chen et al., 2015; between them, such as the boundaries between Ma et al., 2018; Qiu et al., 2020). Recently, pre- word units “李娜(Li Na)”, “进入(entered)” and trained models (PTMs) such as BERT (Devlin et al., “半决赛(the semi-final)”, which are the same for all segmentation criteria. It shows that the com2019) have been introduced into CWS tasks, which mon prior segmentation knowledge is shared by could provide prior semantic knowledge and boost the performance of CWS systems. Yang (2019) di- different criteria. rectly fine-tunes BERT on several CWS benchmark In this paper, we propose a CWS-specific predatasets. Huang et al. (2020) fine-tunes BERT in a trained model M ETA S EG. To leverage sha"
2021.naacl-main.436,N19-1278,0,0.167018,"Missing"
2021.naacl-main.436,2020.acl-main.734,0,0.759193,"between pre-trained models and downstream CWS tasks. Besides, M ETA S EG can achieve new state-of-the-art performance on twelve widely-used CWS datasets and significantly improve model performance in lowresource settings. Criteria CTB6 PKU MSRA Li Na 李娜 李 娜 李娜 entered 进入 进入 进入 the semi-final 半决赛 半 决赛 半 决赛 Table 1: An example of CWS on different criteria. multi-criteria learning framework, where each criterion shares a common BERT-based feature extraction layer and has separate projection layer. Meng et al. (2019) combines Chinese character glyph features with pre-trained BERT representations. Tian et al. (2020) proposes a neural CWS framework WMS EG, which utilizes memory networks to incorporate wordhood information into the pre-trained model ZEN (Diao et al., 2019). PTMs have been proved quite effective by finetuning on downstream CWS tasks. However, PTMs used in previous works usually adopt language modeling as pre-training tasks. Thus, they usually lack task-specific prior knowledge for CWS and ignore the discrepancy between pre-training 1 Introduction tasks and downstream CWS tasks. Chinese Word Segmentation (CWS) is a fundamenTo deal with aforementioned problems of PTMs, tal task for Chinese na"
2021.naacl-main.436,D14-1122,0,0.348795,"Missing"
2021.naacl-main.436,D19-1541,0,0.0589616,"Missing"
2021.naacl-main.436,2020.emnlp-main.514,1,0.799817,"Missing"
2021.naacl-main.436,O03-4002,0,0.578191,"Howacters. It provides a basic component for other NLP ever, there are multiple inconsistent segmentation tasks like named entity recognition (Li et al., 2020), criteria for CWS, where each criterion represents a dependency parsing (Yan et al., 2020), and seman- unique style of segmenting Chinese sentence into tic role labeling (Xia et al., 2019), etc. words, as shown in Table 1. Meanwhile, we can easily observe that different segmentation criteria Generally, most previous studies model the CWS could share a large proportion of word boundaries task as a character-based sequence labeling task (Xue, 2003; Zheng et al., 2013; Chen et al., 2015; between them, such as the boundaries between Ma et al., 2018; Qiu et al., 2020). Recently, pre- word units “李娜(Li Na)”, “进入(entered)” and trained models (PTMs) such as BERT (Devlin et al., “半决赛(the semi-final)”, which are the same for all segmentation criteria. It shows that the com2019) have been introduced into CWS tasks, which mon prior segmentation knowledge is shared by could provide prior semantic knowledge and boost the performance of CWS systems. Yang (2019) di- different criteria. rectly fine-tunes BERT on several CWS benchmark In this paper, w"
2021.naacl-main.436,K17-3001,0,0.0698996,"Missing"
2021.naacl-main.436,E14-1062,0,0.471673,"Missing"
2021.naacl-main.436,D12-1046,0,0.0705684,"Missing"
C16-1139,P11-1055,0,0.782102,"high performance (Zhou et al., 2005; Surdeanu and Ciaramita, 2007). Such methods, however, usually require intensive human annotation and can be time-consuming. To address this issue, distant supervision is proposed to generate labeled data automatically, by aligning facts in a knowledge base (KB) with sentences mentioning these facts (Mintz et al., 2009; Riedel et al., 2010; Riedel et al., 2013). Traditional (distantly) supervised RE methods use as input numerous lexical and syntactic features, e.g., POS tags, dependency paths, and named entity tags (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervision methods on faulty features inevitably leads to error propagation, the main culprit responsible for performance degradation. Recent studies have shown promising results on using deep neural networks for automatic feature extraction (Zeng et al., 2014; Liu et a"
C16-1139,P15-2047,0,0.0421651,"l., 2011; Surdeanu et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervision methods on faulty features inevitably leads to error propagation, the main culprit responsible for performance degradation. Recent studies have shown promising results on using deep neural networks for automatic feature extraction (Zeng et al., 2014; Liu et al., 2015; Xu et al., 2015). Particularly, Zeng et al. (2015) proposed a piecewise convolutional neural network (PCNN) architecture, which can build an extractor based on distant supervision. PCNN automatically extracts features with convolutional neural networks, and introduces piecewise max-pooling to better fit the RE scenario. Although PCNN achieves substantial improvements in distantly supervised relation extraction, it still has the following deficiencies. First, PCNN uses the expressed-at-least-once assumption (Riedel et al., 2010) for labeled data generation, which states that “if two entities"
C16-1139,D07-1013,0,0.00967108,"roposed to generate labeled data automatically, by aligning facts in a knowledge base (KB) with sentences mentioning these facts (Mintz et al., 2009; Riedel et al., 2010; Riedel et al., 2013). Traditional (distantly) supervised RE methods use as input numerous lexical and syntactic features, e.g., POS tags, dependency paths, and named entity tags (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervision methods on faulty features inevitably leads to error propagation, the main culprit responsible for performance degradation. Recent studies have shown promising results on using deep neural networks for automatic feature extraction (Zeng et al., 2014; Liu et al., 2015; Xu et al., 2015). Particularly, Zeng et al. (2015) proposed a piecewise convolutional neural network (PCNN) architecture, which can build an extractor based on distant supervision. PCNN automatically ext"
C16-1139,P09-1113,0,0.971432,"f-the-art methods. 1 Introduction Relation extraction (RE), defined as the task of extracting binary relations from plain text, has long been a crucial task in natural language processing. Supervised methods are widely used for this task due to their relatively high performance (Zhou et al., 2005; Surdeanu and Ciaramita, 2007). Such methods, however, usually require intensive human annotation and can be time-consuming. To address this issue, distant supervision is proposed to generate labeled data automatically, by aligning facts in a knowledge base (KB) with sentences mentioning these facts (Mintz et al., 2009; Riedel et al., 2010; Riedel et al., 2013). Traditional (distantly) supervised RE methods use as input numerous lexical and syntactic features, e.g., POS tags, dependency paths, and named entity tags (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervi"
C16-1139,W15-1506,0,0.118558,"s between an entity pair. Further effects are also made to model missing data (Ritter et al., 2013), reduce noise (Roth et al., 2013), inject logical background knowledge (Rockt¨aschel et al., 2015), etc. In recent years, deep neural network has proven its ability to learn task-specific representation automatically, so that avoiding error propagation suffered by traditional feature-based models. In particular, many neural network approaches have been proposed and shown better performance in relation classification (Zeng et al., 2014; Liu et al., 2015; Xu et al., 2015) and relation extraction (Nguyen and Grishman, 2015). However, these two tasks differ from ours in that relations are extracted at sentence-level, while annotation data is readily available. In distant supervision paradigm, Zeng et al. (2015) is a known neural network model that uses expressed-at-least-once assumption for multi-instance single-label learning. Nevertheless, it selects only one sentence as the representation of an entity pair in training phrase, which wastes the information in the neglected sentences. Besides, it also fails to consider other relations that might hold between this entity pair. The proposed method, on the other han"
C16-1139,N13-1008,0,0.0140916,"n extraction (RE), defined as the task of extracting binary relations from plain text, has long been a crucial task in natural language processing. Supervised methods are widely used for this task due to their relatively high performance (Zhou et al., 2005; Surdeanu and Ciaramita, 2007). Such methods, however, usually require intensive human annotation and can be time-consuming. To address this issue, distant supervision is proposed to generate labeled data automatically, by aligning facts in a knowledge base (KB) with sentences mentioning these facts (Mintz et al., 2009; Riedel et al., 2010; Riedel et al., 2013). Traditional (distantly) supervised RE methods use as input numerous lexical and syntactic features, e.g., POS tags, dependency paths, and named entity tags (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervision methods on faulty features inevitably"
C16-1139,Q13-1030,0,0.0114875,"approach in distant supervision is Mintz et al. (2009), which aligns Freebase with Wikipedia articles and extracts relations with logistic regression. Follow-up studies use the feature set developed in this approach, but with deeper understanding on the nature of distant supervision. For example, Riedel et al. (2010) relaxes the assumption used in Mintz et al. (2009) and formulates distant supervision as a multi-instance learning issue; Hoffmann et al. (2011) and Surdeanu et al. (2012) consider overlapping relations between an entity pair. Further effects are also made to model missing data (Ritter et al., 2013), reduce noise (Roth et al., 2013), inject logical background knowledge (Rockt¨aschel et al., 2015), etc. In recent years, deep neural network has proven its ability to learn task-specific representation automatically, so that avoiding error propagation suffered by traditional feature-based models. In particular, many neural network approaches have been proposed and shown better performance in relation classification (Zeng et al., 2014; Liu et al., 2015; Xu et al., 2015) and relation extraction (Nguyen and Grishman, 2015). However, these two tasks differ from ours in that relations are extract"
C16-1139,N15-1118,0,0.0204669,"Missing"
C16-1139,D12-1042,0,0.506569,"et al., 2005; Surdeanu and Ciaramita, 2007). Such methods, however, usually require intensive human annotation and can be time-consuming. To address this issue, distant supervision is proposed to generate labeled data automatically, by aligning facts in a knowledge base (KB) with sentences mentioning these facts (Mintz et al., 2009; Riedel et al., 2010; Riedel et al., 2013). Traditional (distantly) supervised RE methods use as input numerous lexical and syntactic features, e.g., POS tags, dependency paths, and named entity tags (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervision methods on faulty features inevitably leads to error propagation, the main culprit responsible for performance degradation. Recent studies have shown promising results on using deep neural networks for automatic feature extraction (Zeng et al., 2014; Liu et al., 2015; Xu et al., 201"
C16-1139,D15-1062,0,0.130702,"et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervision methods on faulty features inevitably leads to error propagation, the main culprit responsible for performance degradation. Recent studies have shown promising results on using deep neural networks for automatic feature extraction (Zeng et al., 2014; Liu et al., 2015; Xu et al., 2015). Particularly, Zeng et al. (2015) proposed a piecewise convolutional neural network (PCNN) architecture, which can build an extractor based on distant supervision. PCNN automatically extracts features with convolutional neural networks, and introduces piecewise max-pooling to better fit the RE scenario. Although PCNN achieves substantial improvements in distantly supervised relation extraction, it still has the following deficiencies. First, PCNN uses the expressed-at-least-once assumption (Riedel et al., 2010) for labeled data generation, which states that “if two entities participate in a r"
C16-1139,C14-1220,0,0.842528,"2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervision methods on faulty features inevitably leads to error propagation, the main culprit responsible for performance degradation. Recent studies have shown promising results on using deep neural networks for automatic feature extraction (Zeng et al., 2014; Liu et al., 2015; Xu et al., 2015). Particularly, Zeng et al. (2015) proposed a piecewise convolutional neural network (PCNN) architecture, which can build an extractor based on distant supervision. PCNN automatically extracts features with convolutional neural networks, and introduces piecewise max-pooling to better fit the RE scenario. Although PCNN achieves substantial improvements in distantly supervised relation extraction, it still has the following deficiencies. First, PCNN uses the expressed-at-least-once assumption (Riedel et al., 2010) for labeled data generation, which states that"
C16-1139,D15-1203,0,0.579398,"s mentioning these facts (Mintz et al., 2009; Riedel et al., 2010; Riedel et al., 2013). Traditional (distantly) supervised RE methods use as input numerous lexical and syntactic features, e.g., POS tags, dependency paths, and named entity tags (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervision methods on faulty features inevitably leads to error propagation, the main culprit responsible for performance degradation. Recent studies have shown promising results on using deep neural networks for automatic feature extraction (Zeng et al., 2014; Liu et al., 2015; Xu et al., 2015). Particularly, Zeng et al. (2015) proposed a piecewise convolutional neural network (PCNN) architecture, which can build an extractor based on distant supervision. PCNN automatically extracts features with convolutional neural networks, and introduces piecewise max-pooling to better"
C16-1139,P05-1053,0,0.0762115,"-once assumption, and employs cross-sentence max-pooling so as to enable information sharing across different sentences. Then it handles overlapping relations by multi-label learning with a neural network classifier. Experimental results show that our approach performs significantly and consistently better than state-of-the-art methods. 1 Introduction Relation extraction (RE), defined as the task of extracting binary relations from plain text, has long been a crucial task in natural language processing. Supervised methods are widely used for this task due to their relatively high performance (Zhou et al., 2005; Surdeanu and Ciaramita, 2007). Such methods, however, usually require intensive human annotation and can be time-consuming. To address this issue, distant supervision is proposed to generate labeled data automatically, by aligning facts in a knowledge base (KB) with sentences mentioning these facts (Mintz et al., 2009; Riedel et al., 2010; Riedel et al., 2013). Traditional (distantly) supervised RE methods use as input numerous lexical and syntactic features, e.g., POS tags, dependency paths, and named entity tags (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et a"
D08-1111,W04-3236,0,0.0470214,"Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1061–1069, c Honolulu, October 2008. 2008 Association for Computational Linguistics In our method, we first predict the ranking result of all internal association strength (IAS) between each pair of adjacent characters in a sentence using Ranking SVM model, and then, we segment the sentence into sub-sentences with smaller and smaller granularity by cutting adjacent character pairs according to this rank. Other machine-learning based segmentation algorithms (Zhang et al., 2003; Lafferty et al., 2001; Ng and Low, 2004) treat segmentation problem as a character sequence tagging problem based on classification. However, these methods cannot directly obtain different segmentation granularity. Experiments show that our method can actually improve information retrieval performance. This paper is structured as follows. It starts with a brief introduction of the related work on the word segmentation approaches. Then in Section 3, we introduce our segmentation method. Section 4 evaluates the method based on experimental results. Finally, Section 5 makes summary of this whole paper and proposes the future research o"
D08-1111,C02-1148,0,0.438165,"nology Chinese Academy of Sciences Beijing, 100190, P.R.China {liuyixuan, wangbin, dingfan, xusheng}@ict.ac.cn Abstract Overlapping ambiguity and combinatory ambiguity are two forms of segmentation ambiguity. The first one refers to that ABC can be segmented into AB C or A BC. The second one refers to that string AB can be a word, or A can be a word and B can be a word. In CIR, the combinatory ambiguity is also called segmentation granularity problem (Fan et al., 2007). There are many researches on the relationship between word segmentation and Chinese information retrieval (Foo and Li, 2004; Peng et al., 2002a; Peng et al., 2002b; Jin and Wong, 2002). Their studies show that the segmentation accuracy does not monotonically influence subsequent retrieval performance. Especially the overlapping ambiguity, as shown in experiments of (Wang, 2006), will cause more performance decrement of CIR. Thus a CIR system with a word segmenter better solving the overlapping ambiguity, may achieve better performance. Besides, it also showed that the precision of new word identification was more important than the recall. This paper presents a novel, ranking-style word segmentation approach, called RSVMSeg, which i"
D08-1111,W03-1730,0,0.0941626,"tation granularity for the first time. 1061 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1061–1069, c Honolulu, October 2008. 2008 Association for Computational Linguistics In our method, we first predict the ranking result of all internal association strength (IAS) between each pair of adjacent characters in a sentence using Ranking SVM model, and then, we segment the sentence into sub-sentences with smaller and smaller granularity by cutting adjacent character pairs according to this rank. Other machine-learning based segmentation algorithms (Zhang et al., 2003; Lafferty et al., 2001; Ng and Low, 2004) treat segmentation problem as a character sequence tagging problem based on classification. However, these methods cannot directly obtain different segmentation granularity. Experiments show that our method can actually improve information retrieval performance. This paper is structured as follows. It starts with a brief introduction of the related work on the word segmentation approaches. Then in Section 3, we introduce our segmentation method. Section 4 evaluates the method based on experimental results. Finally, Section 5 makes summary of this whol"
D14-1118,N04-1009,0,0.0397239,"a key to automatically finding materials at appreciate reading levels for students, and also helps in personalized web search (Collins-Thompson et al., 2011). In the task of predicting reading difficulty levels, documents targeting different grade levels are taken as ground truth, which can be easily obtained from the web. However, there is no naturally annotated data for our QDE task on the web. Other related problems include query difficulty estimation for search engines (Carmel et al., 2006; Yom-Tov et al., 2005) and question difficulty estimation for automatic question answering systems (Lange et al., 2004). In these tasks, query/question difficulty is system-oriented and irrelevant with human knowledge, which is a different setting from ours. 7 Conclusion and Future Work In this paper, we have proposed a novel method for estimating question difficulty levels in CQA services, called Regularized Competition Model (RCM). It takes fully advantage of questions’ textual descriptions besides question-user comparisons, and thus can effectively deal with data sparsity and perform more accurate estimation. A K-Nearest Neighbor approach is further adopted to estimate difficulty levels of cold-start questi"
D14-1118,W11-0823,0,0.0281763,"r enterprise question answering and MOOCs question answering, where human resources are expensive. 2) Incentive mechanism design. Nam et al. (2009) have found that winning point awards offered by reputation systems is a driving factor for user participation in CQA services. Assigning higher point awards to more difficult questions will significantly improve user participation and satisfaction. 3) Linguistics analysis. Researchers in computational linguistics are always interested in investigating the correlation between language and knowledge, to see how the language reflects one’s knowledge (Church, 2011). As we will show in Section 5.4, QDE provides an automatic way to quantitatively measure the knowledge levels of words. Liu et al. (2013) have done the pioneer work on QDE, by leveraging question-user comparisons extracted from the question answering threads. Specifically, they assumed that the difficulty level of a question is higher than the expertise level of the asker (i.e. the user who asked the question), but lower than that of the best answerer (i.e. the user who provided the best answer). A TrueSkill al2 3 http://answers.yahoo.com/ http://stackoverflow.com/ http://coursera.org/ 1115 P"
D14-1118,P05-1065,0,0.059425,"rning (Zhou et al., 2004; Zhu and Lafferty, 2005) algorithms have been constructed. In dimensionality reduction, manifold regularization is utilized to guarantee that nearby points will have similar low-dimensional representations (Cai et al., 2008), while in semi-supervised learning it is utilized to ensure that nearby points will have similar labels (Zhou et al., 2004). In our work, we assume that nearby questions (in terms of textual similarities) will have similar difficulty levels. Predicting reading difficulty levels of text is also a relevant problem (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005). It is a key to automatically finding materials at appreciate reading levels for students, and also helps in personalized web search (Collins-Thompson et al., 2011). In the task of predicting reading difficulty levels, documents targeting different grade levels are taken as ground truth, which can be easily obtained from the web. However, there is no naturally annotated data for our QDE task on the web. Other related problems include query difficulty estimation for search engines (Carmel et al., 2006; Yom-Tov et al., 2005) and question difficulty estimation for automatic question answering sy"
D14-1118,N04-1025,0,\N,Missing
D14-1118,D13-1009,1,\N,Missing
D15-1191,D14-1165,0,0.0504513,"obtain entity and relation embeddings, a margin-based ranking loss, i.e., X X   L= γ + fr (h, t) − fr (h0 , t0 ) + , t+ ∈O t− ∈Nt+ is minimized. Here, t+ = (h, r, t) ∈ O is an observed (positive) triple; Nt+ is the set of negative triples constructed by replacing entities in t+ , and t− = (h0 , r, t0 ) ∈ Nt+ ; γ is a margin separating positive and negative triples; [x]+ = max(0, x). Table 1 summarizes the entity/relation embeddings and the energy functions used in SME, TansE, and SE. For other KG embedding models, please refer to (Nickel et al., 2011; Riedel et al., 2013; Wang et al., 2014; Chang et al., 2014). We adopt stochastic gradient descent to solve the minimization problem, by taking entity and relation embeddings pre-trained in the first stage as 1657 Method Entity/Relation embedding Energy function SME (linear) (Bordes et al., 2014) h, t ∈ R , r ∈ R fr (h, t) = (Wu1 r + Wu2 h + bu )T (Wv1 r + Wv2 t + bv ) k k ¯ 3 r) h + bu )T ((Wv × ¯ 3 r) t + bv ) h, t ∈ R , r ∈ R fr (h, t) = ((Wu × SME (bilinear) (Bordes et al., 2014) k k TransE (Bordes et al., 2013) h, t ∈ R , r ∈ R fr (h, t) = kh + r − tk`1 SE (Bordes et al., 2011) h, t ∈ Rk , Ru , Rv ∈ Rk×k fr (h, t) = kRu h − Rv tk`1 k k Table 1: En"
D15-1191,N15-1165,0,0.0201072,"ar classifiers are employed to predict the current word. In Skip-gram, the current word is projected to its embedding, and log-linear classifiers are further adopted to predict its context. We 1 Two entities connected to a same node are always expected to have some implicit relationships, no matter how they are connected to the intermediate node. restrain the context of a word (i.e. entity/relation) within each knowledge path. The entity and relation embeddings pre-trained in this way are required to be compatible within each knowledge path, and thus can encode CCPs. Perozzi et al. (2014) and Goikoetxea et al. (2015) have proposed similar ideas, i.e., to generate random walks from online social networks or from the WordNet knowledge base, and then employ word embedding techniques on these random walks. But our approach has two differences. 1) It deals with heterogeneous graphs with different types of edges. Both nodes (entities) and edges (relations) are included during knowledge path extraction. However, the previous studies focus only on nodes. 2) We devise a two-stage scheme where the embeddings learned in the first stage will be fine-tuned in the second one, while the previous studies take such embedd"
D15-1191,N13-1008,0,0.0443856,"sumed to have low energies. Finally, to obtain entity and relation embeddings, a margin-based ranking loss, i.e., X X   L= γ + fr (h, t) − fr (h0 , t0 ) + , t+ ∈O t− ∈Nt+ is minimized. Here, t+ = (h, r, t) ∈ O is an observed (positive) triple; Nt+ is the set of negative triples constructed by replacing entities in t+ , and t− = (h0 , r, t0 ) ∈ Nt+ ; γ is a margin separating positive and negative triples; [x]+ = max(0, x). Table 1 summarizes the entity/relation embeddings and the energy functions used in SME, TansE, and SE. For other KG embedding models, please refer to (Nickel et al., 2011; Riedel et al., 2013; Wang et al., 2014; Chang et al., 2014). We adopt stochastic gradient descent to solve the minimization problem, by taking entity and relation embeddings pre-trained in the first stage as 1657 Method Entity/Relation embedding Energy function SME (linear) (Bordes et al., 2014) h, t ∈ R , r ∈ R fr (h, t) = (Wu1 r + Wu2 h + bu )T (Wv1 r + Wv2 t + bv ) k k ¯ 3 r) h + bu )T ((Wv × ¯ 3 r) t + bv ) h, t ∈ R , r ∈ R fr (h, t) = ((Wu × SME (bilinear) (Bordes et al., 2014) k k TransE (Bordes et al., 2013) h, t ∈ R , r ∈ R fr (h, t) = kh + r − tk`1 SE (Bordes et al., 2011) h, t ∈ Rk , Ru , Rv ∈ Rk×k fr"
D15-1191,P15-1009,1,0.587679,"a, i.e., to use embeddings learned from an auxiliary corpus as initial values. However, linking entities recognized in an auxiliary corpus to those occurring in the KG is always a non-trivial task. Our approach requires no auxiliary data, and naturally avoids the entity linking task. 3 Experiments We test our approach on the tasks of link prediction and triple classification. Two publicly available data sets are used. The first is WN18 released by Bordes et al. (2013)3 . It is a subset of WordNet, consisting of 18 relations and the entities connected by them. The second is NELL186 released by Guo et al. (2015)4 , containing the most frequent 186 relations in NELL (Carlson et al., 2010) and the associated entities. Triples are split into training/validation/test sets, used for model training, parameter tuning, and evaluation respectively. Knowledge paths are extracted from training sets. Table 2 gives some statistics of the data sets. To perform context-dependent KG embedding, we use CBOW and Skip-gram in the pre-training stage, and SME, TransE, and SE in the fine-tuning stage. We take randomly initialized SME, TransE, and SE as baselines, denoted as *-Random. We do not compare to the setting that e"
D16-1019,D14-1165,0,0.0556549,"–202, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics /RJLF (QWLWHPEHGGLQJV 5HODWLRQHPEHGGLQJV 7UXWKYDOXHVLQ&gt;@ /RJLFDOFRQQHFWLYHV .QRZOHGJH Figure 1: Simple illustration of KALE. better embeddings. Rockt¨aschel et al. (2015) recently proposed a joint model which injects first-order logic into embeddings. But it focuses on the relation extraction task, and creates vector embeddings for entity pairs rather than individual entities. Since entities do not have their own embeddings, relations between unpaired entities cannot be effectively discovered (Chang et al., 2014). In this paper we introduce KALE, a new approach that learns entity and relation Embeddings by jointly modeling Knowledge And Logic. Knowledge triples are taken as atoms and modeled by the translation assumption, i.e., relations act as translations between head and tail entities (Bordes et al., 2013). A triple (ei , rk , ej ) is scored by ∥ei + rk − ej ∥1 , where ei , rk , and ej are the vector embeddings for entities and relations. The score is then mapped to the unit interval [0, 1] to indicate the truth value of that triple. Logical rules are taken as complex formulae constructed by combin"
D16-1019,W16-1314,0,0.0213066,"Missing"
D16-1019,P15-1009,1,0.723754,"elations as translating operations, achieves a good trade-off between prediction accuracy and computational efficiency. Various extensions like TransH (Wang et al., 2014) and TransR (Lin et al., 2015b) are later proposed to further enhance the prediction accuracy of TransE. Most existing methods perform the embedding task based solely on triples contained in a KG. Some recent work tries to further incorporate other types of information available, e.g., relation paths (Neelakantan et al., 2015; Lin et al., 2015a; Luo et al., 2015), relation type-constraints (Krompaßet al., 2015), entity types (Guo et al., 2015), and entity descriptions (Zhong et al., 2015), to learn better embeddings. Logical rules have been widely studied in knowledge acquisition and inference, usually on the basis of Markov logic networks (Richardson and Domingos, 2006; Br¨ocheler et al., 2010; Pujara et al., 2013; Beltagy and Mooney, 2014). Recently, there has been growing interest in combining logical rules and embedding models. Wang et al. (2015) and Wei et al. (2015) tried to utilize rules to refine predictions made by embedding models, via integer linear programming or Markov logic networks. In their work, however, rules are"
D16-1019,P11-1055,0,0.0262553,"Missing"
D16-1019,D15-1082,0,0.0196767,"Missing"
D16-1019,D15-1191,1,0.857849,"r et al., 2009). Among these methods, TransE (Bordes et al., 2013), which models relations as translating operations, achieves a good trade-off between prediction accuracy and computational efficiency. Various extensions like TransH (Wang et al., 2014) and TransR (Lin et al., 2015b) are later proposed to further enhance the prediction accuracy of TransE. Most existing methods perform the embedding task based solely on triples contained in a KG. Some recent work tries to further incorporate other types of information available, e.g., relation paths (Neelakantan et al., 2015; Lin et al., 2015a; Luo et al., 2015), relation type-constraints (Krompaßet al., 2015), entity types (Guo et al., 2015), and entity descriptions (Zhong et al., 2015), to learn better embeddings. Logical rules have been widely studied in knowledge acquisition and inference, usually on the basis of Markov logic networks (Richardson and Domingos, 2006; Br¨ocheler et al., 2010; Pujara et al., 2013; Beltagy and Mooney, 2014). Recently, there has been growing interest in combining logical rules and embedding models. Wang et al. (2015) and Wei et al. (2015) tried to utilize rules to refine predictions made by embedding models, via integ"
D16-1019,N13-1090,0,0.0326256,"both triples and rules. In what follows, we describe the key components of KALE, including triple modeling, rule modeling, and joint learning. 3.2 Triple Modeling To model triples we follow TransE (Bordes et al., 2013), as it is simple and efficient while achieving state-of-the-art predictive performance. Specifically, given a triple (ei , rk , ej ), we model the relation embedding rk as a translation between the entity embeddings ei and ej , i.e., we want ei + rk ≈ ej when the triple holds. The intuition here originates from linguistic regularities such as France − Paris = Germany − Berlin (Mikolov et al., 2013). In relational data, such analogy holds because of the certain relation Capital-Of, through which we will get Paris + Capital-Of = France and Berlin + Capital-Of = Germany. Then, we score each triple on the basis of ∥ei + rk − ej ∥1 , and define its soft truth value as 1 I (ei , rk , ej ) = 1 − √ ∥ei + rk − ej ∥1 , 3 d (1) where d is the dimension of the embedding space. It is easy to see that I (ei , rk , ej ) ∈ [0, 1] with the constraints ∥ei ∥2 ≤ 1, ∥ej ∥2 ≤ 1, and ∥rk ∥2 ≤ 1 Our approach actually takes as input rules represented in first-order logic, i.e., those with quantifiers such as ∀"
D16-1019,P15-1016,0,0.0148308,"(Kemp et al., 2006; Xu et al., 2006; Sutskever et al., 2009). Among these methods, TransE (Bordes et al., 2013), which models relations as translating operations, achieves a good trade-off between prediction accuracy and computational efficiency. Various extensions like TransH (Wang et al., 2014) and TransR (Lin et al., 2015b) are later proposed to further enhance the prediction accuracy of TransE. Most existing methods perform the embedding task based solely on triples contained in a KG. Some recent work tries to further incorporate other types of information available, e.g., relation paths (Neelakantan et al., 2015; Lin et al., 2015a; Luo et al., 2015), relation type-constraints (Krompaßet al., 2015), entity types (Guo et al., 2015), and entity descriptions (Zhong et al., 2015), to learn better embeddings. Logical rules have been widely studied in knowledge acquisition and inference, usually on the basis of Markov logic networks (Richardson and Domingos, 2006; Br¨ocheler et al., 2010; Pujara et al., 2013; Beltagy and Mooney, 2014). Recently, there has been growing interest in combining logical rules and embedding models. Wang et al. (2015) and Wei et al. (2015) tried to utilize rules to refine predictio"
D16-1019,N13-1008,0,0.0310734,"consistent improvements over state-of-the-art methods. Particularly, joint embedding enhances the prediction of new facts which cannot even be directly inferred by pure logical inference, demonstrating the capability of KALE to learn more predictive embeddings. 2 Related Work Recent years have seen rapid growth in KG embedding methods. Given a KG, such methods aim to encode its entities and relations into a continuous vector space, by using neural network architectures (Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix/tensor factorization techniques (Nickel et al., 2011; Riedel et al., 2013; Chang et al., 2014), or Bayesian clustering strategies (Kemp et al., 2006; Xu et al., 2006; Sutskever et al., 2009). Among these methods, TransE (Bordes et al., 2013), which models relations as translating operations, achieves a good trade-off between prediction accuracy and computational efficiency. Various extensions like TransH (Wang et al., 2014) and TransR (Lin et al., 2015b) are later proposed to further enhance the prediction accuracy of TransE. Most existing methods perform the embedding task based solely on triples contained in a KG. Some recent work tries to further incorporate oth"
D16-1019,W14-2409,0,0.0370595,"Missing"
D16-1019,N15-1118,0,0.103062,"Missing"
D16-1019,D15-1192,0,0.0773168,"Missing"
D16-1019,D13-1136,0,0.0116805,"g the capability of our method to learn more predictive embeddings. 1 Recently, a promising approach, namely knowledge graph embedding, has been proposed and successfully applied to various KGs (Nickel et al., 2012; Socher et al., 2013; Bordes et al., 2014). The key idea is to embed components of a KG including entities and relations into a continuous vector space, so as to simplify the manipulation while preserving the inherent structure of the KG. The embeddings contain rich semantic information about entities and relations, and can significantly enhance knowledge acquisition and inference (Weston et al., 2013). Introduction Knowledge graphs (KGs) provide rich structured information and have become extremely useful resources for many NLP related applications like ∗ Corresponding author: Quan Wang. Most existing methods perform the embedding task based solely on fact triples (Bordes et al., 2013; Wang et al., 2014; Nickel et al., 2016). The only requirement is that the learned embeddings should be compatible with those facts. While logical rules contain rich background information and are extremely useful for knowledge acquisition and inference (Jiang et al., 2012; Pujara et al., 2013), they have not"
D16-1019,D15-1031,0,0.0162405,"a good trade-off between prediction accuracy and computational efficiency. Various extensions like TransH (Wang et al., 2014) and TransR (Lin et al., 2015b) are later proposed to further enhance the prediction accuracy of TransE. Most existing methods perform the embedding task based solely on triples contained in a KG. Some recent work tries to further incorporate other types of information available, e.g., relation paths (Neelakantan et al., 2015; Lin et al., 2015a; Luo et al., 2015), relation type-constraints (Krompaßet al., 2015), entity types (Guo et al., 2015), and entity descriptions (Zhong et al., 2015), to learn better embeddings. Logical rules have been widely studied in knowledge acquisition and inference, usually on the basis of Markov logic networks (Richardson and Domingos, 2006; Br¨ocheler et al., 2010; Pujara et al., 2013; Beltagy and Mooney, 2014). Recently, there has been growing interest in combining logical rules and embedding models. Wang et al. (2015) and Wei et al. (2015) tried to utilize rules to refine predictions made by embedding models, via integer linear programming or Markov logic networks. In their work, however, rules are modeled separately from embedding models, and"
D16-1100,P15-1162,0,0.00806446,"Missing"
D16-1100,P16-1020,0,0.0228684,"mbedding (MGE) which learns embeddings jointly for words, characters, and radicals. The framework of MGE is sketched in Figure 1. Given a word, we learn its embedding on the basis of 1) the context words (blue bars in the figure), 2) their constituent characters (green bars), and 3) the radicals found in the target word (orange bars). Compared to utilizing context words alone, MGE enriches the embeddings by further incorporating finer-grained semantics from characters and radicals. Similar ideas of adaptively using multiple levels of embeddings have also been investigated in English recently (Kazuma and Yoshimasa, 2016; Miyamoto and Cho, 2016). We evaluate MGE with the benchmark tasks of word similarity computation and analogical reasoning, and demonstrate its superiority over state-ofthe-art metods. A qualitative analysis further shows the capability of MGE to identify finer-grained semantic meanings of words. 2 Multi-Granularity Word Embedding This section introduces MGE based on the continuous bag-of-words model (CBOW) (Mikolov et al., 2013b) and the character-enhanced word embedding 982 model (CWE) (Chen et al., 2015). MGE aims at improving word embedding by leveraging both characters and radicals. We d"
D16-1100,D14-1181,0,0.015234,"uantitative evaluation demonstrates the superiority of MGE in word similarity computation and analogical reasoning. Qualitative analysis further shows its capability to identify finer-grained semantic meanings of words. 1 Introduction Word embedding, also known as distributed word representation, is to represent each word as a realvalued low-dimensional vector, through which the semantic meaning of the word can be encoded. Recent years have witnessed tremendous success of word embedding in various NLP tasks (Bengio et al., 2006; Mnih and Hinton, 2009; Collobert et al., 2011; Zou et al., 2013; Kim, 2014; Liu et al., 2015; Iyyer et al., 2015). The basic idea behind is to learn the distributed representation of a word using its context. Among existing approaches, the continuous bag-of-words model (CBOW) and Skip-Gram model are simple and effective, capable of learning word embeddings efficiently from large-scale text corpora (Mikolov et al., 2013a; Mikolov et al., 2013b). ∗ Besides the success in English, word embedding has also been demonstrated to be extremely useful for Chinese language processing (Xu et al., 2015; Yu et al., 2015; Zhou et al., 2015; Zou et al., 2013). The work on Chinese g"
D16-1100,N15-1092,0,0.0122923,"evaluation demonstrates the superiority of MGE in word similarity computation and analogical reasoning. Qualitative analysis further shows its capability to identify finer-grained semantic meanings of words. 1 Introduction Word embedding, also known as distributed word representation, is to represent each word as a realvalued low-dimensional vector, through which the semantic meaning of the word can be encoded. Recent years have witnessed tremendous success of word embedding in various NLP tasks (Bengio et al., 2006; Mnih and Hinton, 2009; Collobert et al., 2011; Zou et al., 2013; Kim, 2014; Liu et al., 2015; Iyyer et al., 2015). The basic idea behind is to learn the distributed representation of a word using its context. Among existing approaches, the continuous bag-of-words model (CBOW) and Skip-Gram model are simple and effective, capable of learning word embeddings efficiently from large-scale text corpora (Mikolov et al., 2013a; Mikolov et al., 2013b). ∗ Besides the success in English, word embedding has also been demonstrated to be extremely useful for Chinese language processing (Xu et al., 2015; Yu et al., 2015; Zhou et al., 2015; Zou et al., 2013). The work on Chinese generally follows t"
D16-1100,N13-1090,0,0.629158,"ed low-dimensional vector, through which the semantic meaning of the word can be encoded. Recent years have witnessed tremendous success of word embedding in various NLP tasks (Bengio et al., 2006; Mnih and Hinton, 2009; Collobert et al., 2011; Zou et al., 2013; Kim, 2014; Liu et al., 2015; Iyyer et al., 2015). The basic idea behind is to learn the distributed representation of a word using its context. Among existing approaches, the continuous bag-of-words model (CBOW) and Skip-Gram model are simple and effective, capable of learning word embeddings efficiently from large-scale text corpora (Mikolov et al., 2013a; Mikolov et al., 2013b). ∗ Besides the success in English, word embedding has also been demonstrated to be extremely useful for Chinese language processing (Xu et al., 2015; Yu et al., 2015; Zhou et al., 2015; Zou et al., 2013). The work on Chinese generally follows the same idea as on English, i.e., to learn the embedding of a word on the basis of its context. However, in contrast to English where words are usually taken as basic semantic units, Chinese words may have a complicated composition structure of their semantic meanings. More specifically, a Chinese word is often composed of sever"
D16-1100,D16-1209,0,0.0157198,"embeddings jointly for words, characters, and radicals. The framework of MGE is sketched in Figure 1. Given a word, we learn its embedding on the basis of 1) the context words (blue bars in the figure), 2) their constituent characters (green bars), and 3) the radicals found in the target word (orange bars). Compared to utilizing context words alone, MGE enriches the embeddings by further incorporating finer-grained semantics from characters and radicals. Similar ideas of adaptively using multiple levels of embeddings have also been investigated in English recently (Kazuma and Yoshimasa, 2016; Miyamoto and Cho, 2016). We evaluate MGE with the benchmark tasks of word similarity computation and analogical reasoning, and demonstrate its superiority over state-ofthe-art metods. A qualitative analysis further shows the capability of MGE to identify finer-grained semantic meanings of words. 2 Multi-Granularity Word Embedding This section introduces MGE based on the continuous bag-of-words model (CBOW) (Mikolov et al., 2013b) and the character-enhanced word embedding 982 model (CWE) (Chen et al., 2015). MGE aims at improving word embedding by leveraging both characters and radicals. We denote the Chinese word vo"
D16-1100,P15-2098,0,0.432,"nd “饭 (meal)” the radical of “ 饣 (food)”. The semantic meaning of “吃饭” can be revealed by the constituent characters as well as their radicals. Despite being the linguistic nature of Chinese and containing rich semantic information, such wordcharacter-radical composition has not been fully exploited by existing approaches. Chen et al. (2015) introduced a character-enhanced word embedding model (CWE), which learns embeddings jointly for words and characters but ignores radicals. Sun et al. (2014) and Li et al. (2015) utilized radical information to learn better character embeddings. Similarly, Shi et al. (2015) split characters into small components based on the Wubi method,2 and took into account those components during the learning process. In their work, however, embeddings are learned only for characters. For a word, the embedding is generated by simply combining the embeddings of the constituent characters. Since not all Chinese word1 2 Corresponding author: Peng Li. https://en.wikipedia.org/wiki/Radical (Chinese characters) https://en.wikipedia.org/wiki/Wubi method 981 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 981–986, c Austin, Texas, Novemb"
D16-1100,N15-1155,0,0.0310919,"Missing"
D16-1100,P15-1025,0,0.0225002,", 2009; Collobert et al., 2011; Zou et al., 2013; Kim, 2014; Liu et al., 2015; Iyyer et al., 2015). The basic idea behind is to learn the distributed representation of a word using its context. Among existing approaches, the continuous bag-of-words model (CBOW) and Skip-Gram model are simple and effective, capable of learning word embeddings efficiently from large-scale text corpora (Mikolov et al., 2013a; Mikolov et al., 2013b). ∗ Besides the success in English, word embedding has also been demonstrated to be extremely useful for Chinese language processing (Xu et al., 2015; Yu et al., 2015; Zhou et al., 2015; Zou et al., 2013). The work on Chinese generally follows the same idea as on English, i.e., to learn the embedding of a word on the basis of its context. However, in contrast to English where words are usually taken as basic semantic units, Chinese words may have a complicated composition structure of their semantic meanings. More specifically, a Chinese word is often composed of several characters, and most of the characters themselves can be further divided into components such as radicals (部首).1 Both characters and radicals may suggest the semantic meaning of a word, regardless of its con"
D16-1100,D13-1141,0,0.0118533,"rs and radicals. Quantitative evaluation demonstrates the superiority of MGE in word similarity computation and analogical reasoning. Qualitative analysis further shows its capability to identify finer-grained semantic meanings of words. 1 Introduction Word embedding, also known as distributed word representation, is to represent each word as a realvalued low-dimensional vector, through which the semantic meaning of the word can be encoded. Recent years have witnessed tremendous success of word embedding in various NLP tasks (Bengio et al., 2006; Mnih and Hinton, 2009; Collobert et al., 2011; Zou et al., 2013; Kim, 2014; Liu et al., 2015; Iyyer et al., 2015). The basic idea behind is to learn the distributed representation of a word using its context. Among existing approaches, the continuous bag-of-words model (CBOW) and Skip-Gram model are simple and effective, capable of learning word embeddings efficiently from large-scale text corpora (Mikolov et al., 2013a; Mikolov et al., 2013b). ∗ Besides the success in English, word embedding has also been demonstrated to be extremely useful for Chinese language processing (Xu et al., 2015; Yu et al., 2015; Zhou et al., 2015; Zou et al., 2013). The work o"
D19-5717,W16-3009,0,0.0535187,"Missing"
D19-5717,W16-3001,0,0.0492311,"Missing"
D19-5717,W13-2001,0,0.0876743,"Missing"
D19-5717,D11-1142,0,0.0486265,"tation construction, such as word embedding, distance embedding and entity type embedding; and CNN-LSTM model. The F1 value of our participated task on the test data set of all types was 0.342. We achieved the second highest in the task. The results showed that our proposed method performed effectively in the binary relation extraction. 1 Introduction The goal of Information Extraction (IE) (Finkel et al., 2005) is to transform textual information into structured information, and to focus on quickly locating and finding useful information in large amounts of data. Information Extraction (IE) (Fader et al., 2011) is also capable of mining useful data and hiding knowledge from a large number of corpus texts, which has led to some new research methods in many disciplines. For example, with the growing demand for key issues related to life and biology, many biological problems have fallen into the bottleneck due to inadequate methods. Biological information extraction (BioIE) emerges in time and attracts more and more researchers to solve problems. For instance, in the identification of named entities, the classification of relationships between proteins and the extraction of links between drugs. In addi"
D19-5717,P05-1045,0,0.0516624,"rt term memory networks (LSTM). The full text information and context information were collected using the advantages of CNN and LSTM. The model consisted of two main modules: distributed semantic representation construction, such as word embedding, distance embedding and entity type embedding; and CNN-LSTM model. The F1 value of our participated task on the test data set of all types was 0.342. We achieved the second highest in the task. The results showed that our proposed method performed effectively in the binary relation extraction. 1 Introduction The goal of Information Extraction (IE) (Finkel et al., 2005) is to transform textual information into structured information, and to focus on quickly locating and finding useful information in large amounts of data. Information Extraction (IE) (Fader et al., 2011) is also capable of mining useful data and hiding knowledge from a large number of corpus texts, which has led to some new research methods in many disciplines. For example, with the growing demand for key issues related to life and biology, many biological problems have fallen into the bottleneck due to inadequate methods. Biological information extraction (BioIE) emerges in time and attracts"
D19-5717,W16-3013,0,0.0256573,"et al., 2016) aims to promote complex event extraction on regulations in plants from scientific articles. It focuses on events describing genetic and molecular mechanisms involved in seed development of the model plant, Arabidopsis thaliana. It involves n-ary and binary relation extraction. Meanwhile, the SeeDev task was proposed for the first time at BioNLP Shared Task 2016(N´edellec et al., 2016) (Mehryary et al., 2016). This 2019 edition is a rerun of the task, with an evaluation methodology more focused on the biological contribution. Many teams participated in the BioNLP 2016 Shared Task(He et al., 2016). For example, VERSE uses a support vector machine (SVM) and k-fold cross-validation to identify the best parameters.(Lever and Jones, 2016) DUTIR uses a deep learning method that utilizes a convolutional neuWe participated in the BioNLP 2019 Open Shared Tasks: binary relation extraction of SeeDev task. The model was constructed using convolutional neural networks (CNN) and long short term memory networks (LSTM). The full text information and context information were collected using the advantages of CNN and LSTM. The model consisted of two main modules: distributed semantic representation con"
D19-5717,P82-1020,0,0.778993,"Missing"
D19-5717,W16-3012,0,0.0311938,") emerges in time and attracts more and more researchers to solve problems. For instance, in the identification of named entities, the classification of relationships between proteins and the extraction of links between drugs. In addition, information extraction in the field of biology, especially event extraction, has entered people’s views. This will be a far-reaching task and a major biological 110 Proceedings of the 5th Workshop on BioNLP Open Shared Tasks, pages 110–114 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics the preprocessed data. ral network(Li et al., 2016). Motivated by the previous study, based on CNN, we have integrated LSTM(Hochreiter and Schmidhuber, 1997) to solve the defect that convolutional neural networks can not obtain context information. After improving the method, we got good results. The rest of our paper is structured as follows. Section 2 introduces models. Section 3 describes results and discussion. Conclusions are described in Section 4. 2 2.2 We use the context of two entities to predict the type of relationship. In our task, the context is represented by words between two entities in a sentence. Then, by analyzing the data,"
D19-5717,P14-5010,0,0.00406116,", < W T >type(E1 ) ] LTW d (S) = [< W d >d(E1 ,E1 ) , ..., < W d >d(E2 ,E1 ) , 0, 0] where S stands for the sentences. E1 and E2 are the type 1 and type 2 respectively. W1 stands for the first word. W is the word embedding table. W T is type embedding table and W d stands for the distance embedding table. LTW (S) is the representation of word embedding. LTW ,W T (S) is the representation type embedding. LTW d (S) is the distance embedding. In the distance embedding, zero vector(0) is used to pad the sentence. Data preprocessing When doing data preprocessing, first we use the Stanford CoreNLP(Manning et al., 2014) tool to process the task’s data. The text is divided into sentences and tokenized. Parts-of-speech and lemmas are identified and a dependency parse is generated for each sentence. Then, we further process 1 111 https://github.com/cambridgeltl/BioNLP-2016 Figure 1: Our proposed CNN-LSTM based model model CNN CNN-LSTM dropout 0.5 0.5 batch 64 64 epoch 120 120 F1 0.52 0.60 Table 1: The F1 score of CNN and CNN-LSTM on the dev data set for SeeDev-binary task 2.3 Model training F1 0.5 0.25 0.34 0.23 0.35 0.22 Recall 0.6 0.19 0.47 0.24 0.57 0.16 Precision 0.43 0.35 0.27 0.22 0.25 0.33 Table 2: The F"
N19-1103,W17-2609,0,0.0927227,"0.3, 0.4, 0.5}. On each dataset, we choose the optimal conﬁguration with the highest MRR on the validation set within 1000 epochs, and report its performance on the test set. Table 2 lists the optimal conﬁgurations of ConvR on the four datasets. Baseline methods We compare ConvR against a variety of competitive baselines, which can be roughly categorized into two groups: 4.3 Parameter Efﬁciency of ConvR • Methods that use (relatively) simple operations in vector space to model multi-relational data, including TransE (Bordes et al., 2013), DistMult (Yang et al., 2015) and its reimplementation (Kadlec et al., 2017), HolE (Nickel et al., 2016b), ComplEx (Trouillon et al., 2016), ANALOGY (Liu et al., 2017), TorusE (Ebisu and Ichise, 2017), Gaifman (Niepert, 2016), KBGAN (Cai and Wang, 2017), KBLRN (Garcia-Duran and Niepert, 2018), and Node+LinkFeat (Toutanova and Chen, 2015). • Methods that further introduce multi-layer structures and non-linearity, in particular those based on neural networks, including RGCN (Schlichtkrull et al., 2017a), Neural LP (Yang et al., 2017), ConvE (Dettmers et al., 2018), and ConvKB (Nguyen et al., 2018). We further investigate parameter efﬁciency of ConvR against ConvE on FB1"
N19-1103,N18-2053,0,0.283475,"des et al., 2013), DistMult (Yang et al., 2015) and its reimplementation (Kadlec et al., 2017), HolE (Nickel et al., 2016b), ComplEx (Trouillon et al., 2016), ANALOGY (Liu et al., 2017), TorusE (Ebisu and Ichise, 2017), Gaifman (Niepert, 2016), KBGAN (Cai and Wang, 2017), KBLRN (Garcia-Duran and Niepert, 2018), and Node+LinkFeat (Toutanova and Chen, 2015). • Methods that further introduce multi-layer structures and non-linearity, in particular those based on neural networks, including RGCN (Schlichtkrull et al., 2017a), Neural LP (Yang et al., 2017), ConvE (Dettmers et al., 2018), and ConvKB (Nguyen et al., 2018). We further investigate parameter efﬁciency of ConvR against ConvE on FB15k-237. Specifically, we tune the number of ﬁlters c ∈ {20, 40, 60, 80, 100} and the ﬁlter size h × w ∈ {2 × 2, 3 × 3, 4 × 4, 5 × 5}, ﬁx the other hyperparameters to their optimal conﬁgurations (see Table 2 for details), and show how the performance of ConvR (on the test set) will change as the number of parameters varies. For comparison, we directly show the performance and parameter efﬁciency of the optimal ConvE model, as reported in (Dettmers et al., 2018). The results are given in Table 5.5 From the results, we can"
N19-1103,W15-4007,0,0.731154,"id # Test FB15k WN18 FB15k-237 WN18RR 1,345 18 237 11 14,951 40,943 14,541 40,943 483,142 141,442 272,115 86,835 50,000 5,000 17,535 3,034 59,071 5,000 20,466 3,134 Table 1: Statistics of the four datasets. Columns stand for the number of relations, number of entities, and number of triples in training/validation/test sets. and FB15k encode a relation and its inverse relation at the same time. That means, once a fact is observed, there are usually two distinct triples created for it, e.g., (s, hyponym, o) and (o, hypernym, s), or (s, director-of, o) and (o, directed-by, s). As pointed out by (Toutanova and Chen, 2015) and (Dettmers et al., 2018), encoding inverse relations might suffer from test leakage, i.e., for each test triple (s, r, o), it is likely to ﬁnd its inverse (o, r−1 , s) in the training set. To avoid this test leakage issue, we further use WN18RR (Dettmers et al., 2018),3 a subset of WN18 with inverse relations removed, and FB15k-237 (Toutanova and Chen, 2015),4 a ﬁltered version of FB15k with both inverse and duplicate relations removed. Table 1 summarizes the statistics of the four datasets, where the training sets are used for parameter learning, the validation sets for hyperparameter tun"
P03-1058,P91-1034,0,0.712303,"his alone would have accounted for 0.088 of the accuracy difference between the two approaches. That domain dependence is an important issue affecting the performance of WSD programs has been pointed out by (Escudero et al., 2000). Our work confirms the importance of domain dependence in WSD. As to the problem of insufficient sense coverage, with the steady increase and availability of parallel corpora, we believe that getting sufficient sense coverage from larger parallel corpora should not be a problem in the near future for most of the commonly occurring words in a language. 5 Related Work Brown et al. (1991) is the first to have explored statistical methods in word sense disambiguation in the context of machine translation. However, they only looked at assigning at most two senses to a word, and their method only asked a single question about a single word of context. Li and Li (2002) investigated a bilingual bootstrapping technique, which differs from the method we implemented here. Their method also does not require a parallel corpus. The research of (Chugur et al., 2002) dealt with sense distinctions across multiple languages. Ide et al. (2002) investigated word sense distinctions using parall"
P03-1058,W02-0805,0,0.0193174,"lel corpora should not be a problem in the near future for most of the commonly occurring words in a language. 5 Related Work Brown et al. (1991) is the first to have explored statistical methods in word sense disambiguation in the context of machine translation. However, they only looked at assigning at most two senses to a word, and their method only asked a single question about a single word of context. Li and Li (2002) investigated a bilingual bootstrapping technique, which differs from the method we implemented here. Their method also does not require a parallel corpus. The research of (Chugur et al., 2002) dealt with sense distinctions across multiple languages. Ide et al. (2002) investigated word sense distinctions using parallel corpora. Resnik and Yarowsky (2000) considered word sense disambiguation using multiple languages. Our present work can be similarly extended beyond bilingual corpora to multilingual corpora. The research most similar to ours is the work of Diab and Resnik (2002). However, they used machine translated parallel corpus instead of human translated parallel corpus. In addition, they used an unsupervised method of noun group disambiguation, and evaluated on the English all"
P03-1058,P02-1033,0,0.421933,"ngle word of context. Li and Li (2002) investigated a bilingual bootstrapping technique, which differs from the method we implemented here. Their method also does not require a parallel corpus. The research of (Chugur et al., 2002) dealt with sense distinctions across multiple languages. Ide et al. (2002) investigated word sense distinctions using parallel corpora. Resnik and Yarowsky (2000) considered word sense disambiguation using multiple languages. Our present work can be similarly extended beyond bilingual corpora to multilingual corpora. The research most similar to ours is the work of Diab and Resnik (2002). However, they used machine translated parallel corpus instead of human translated parallel corpus. In addition, they used an unsupervised method of noun group disambiguation, and evaluated on the English all-words task. 6 Conclusion In this paper, we reported an empirical study to evaluate an approach of automatically acquiring sense-tagged training data from English-Chinese parallel corpora, which were then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task. Our investigation reveals that this method of acquiring sense-tagged data is promising and provides an al"
P03-1058,S01-1001,0,0.0187407,"lthough attempts have been made to mine parallel corpora from the Web (Resnik, 1999). However, large-scale, good-quality parallel corpora have recently become available. For example, six English-Chinese parallel corpora are now available from Linguistic Data Consortium. These parallel corpora are listed in Table 2, with a combined size of 280 MB. In this paper, we address the above issues and report our findings, exploiting the English-Chinese parallel corpora in Table 2 for word sense disambiguation. We evaluated our approach on all the nouns in the English lexical sample task of SENSEVAL-2 (Edmonds and Cotton, 2001; Kilgarriff 2001), which used the WordNet 1.7 sense inventory (Miller, 1990). While our approach has only been tested on English and Chinese, it is completely general and applicable to other language pairs. 2 Approach Our approach of exploiting parallel texts for word sense disambiguation consists of four steps: (1) parallel text alignment (2) manual selection of target translations (3) training of WSD classifier (4) WSD of words in new contexts. 2.1 Parallel Text Alignment In this step, parallel texts are first sentence-aligned and then word-aligned. Various alignment algorithms (Melamed 200"
P03-1058,W00-1322,0,0.0422819,"Missing"
P03-1058,W02-1004,0,0.00976279,"surrounding channel then forms a training example for a supervised WSD program in the next step. The average time taken to perform manual selection of target translations for one SENSEVAL-2 English noun is less than 15 minutes. This is a relatively short time, especially when compared to the effort that we would otherwise need to spend to perform manual sense-tagging of training examples. This step could also be potentially automated if we have a suitable bilingual translation lexicon. 2.3 Training of WSD Classifier Much research has been done on the best supervised learning approach for WSD (Florian and Yarowsky, 2002; Lee and Ng, 2002; Mihalcea and Moldovan, 2001; Yarowsky et al., 2001). In this paper, we used the WSD program reported in (Lee and Ng, 2002). In particular, our method made use of the knowledge sources of part-of-speech, surrounding words, and local collocations. We used naïve Bayes as the learning algorithm. Our previous research demonstrated that such an approach leads to a state-of-the-art WSD program with good performance. 2.4 WSD of Words in New Contexts Given an occurrence of w in a new context, we then used the naïve Bayes classifier to determine the most probable sense of w. noun chi"
P03-1058,W02-0808,0,0.218976,"y occurring words in a language. 5 Related Work Brown et al. (1991) is the first to have explored statistical methods in word sense disambiguation in the context of machine translation. However, they only looked at assigning at most two senses to a word, and their method only asked a single question about a single word of context. Li and Li (2002) investigated a bilingual bootstrapping technique, which differs from the method we implemented here. Their method also does not require a parallel corpus. The research of (Chugur et al., 2002) dealt with sense distinctions across multiple languages. Ide et al. (2002) investigated word sense distinctions using parallel corpora. Resnik and Yarowsky (2000) considered word sense disambiguation using multiple languages. Our present work can be similarly extended beyond bilingual corpora to multilingual corpora. The research most similar to ours is the work of Diab and Resnik (2002). However, they used machine translated parallel corpus instead of human translated parallel corpus. In addition, they used an unsupervised method of noun group disambiguation, and evaluated on the English all-words task. 6 Conclusion In this paper, we reported an empirical study to"
P03-1058,S01-1004,0,0.0358498,"made to mine parallel corpora from the Web (Resnik, 1999). However, large-scale, good-quality parallel corpora have recently become available. For example, six English-Chinese parallel corpora are now available from Linguistic Data Consortium. These parallel corpora are listed in Table 2, with a combined size of 280 MB. In this paper, we address the above issues and report our findings, exploiting the English-Chinese parallel corpora in Table 2 for word sense disambiguation. We evaluated our approach on all the nouns in the English lexical sample task of SENSEVAL-2 (Edmonds and Cotton, 2001; Kilgarriff 2001), which used the WordNet 1.7 sense inventory (Miller, 1990). While our approach has only been tested on English and Chinese, it is completely general and applicable to other language pairs. 2 Approach Our approach of exploiting parallel texts for word sense disambiguation consists of four steps: (1) parallel text alignment (2) manual selection of target translations (3) training of WSD classifier (4) WSD of words in new contexts. 2.1 Parallel Text Alignment In this step, parallel texts are first sentence-aligned and then word-aligned. Various alignment algorithms (Melamed 2001; Och and Ney 200"
P03-1058,W02-1006,1,0.369826,"ms a training example for a supervised WSD program in the next step. The average time taken to perform manual selection of target translations for one SENSEVAL-2 English noun is less than 15 minutes. This is a relatively short time, especially when compared to the effort that we would otherwise need to spend to perform manual sense-tagging of training examples. This step could also be potentially automated if we have a suitable bilingual translation lexicon. 2.3 Training of WSD Classifier Much research has been done on the best supervised learning approach for WSD (Florian and Yarowsky, 2002; Lee and Ng, 2002; Mihalcea and Moldovan, 2001; Yarowsky et al., 2001). In this paper, we used the WSD program reported in (Lee and Ng, 2002). In particular, our method made use of the knowledge sources of part-of-speech, surrounding words, and local collocations. We used naïve Bayes as the learning algorithm. Our previous research demonstrated that such an approach leads to a state-of-the-art WSD program with good performance. 2.4 WSD of Words in New Contexts Given an occurrence of w in a new context, we then used the naïve Bayes classifier to determine the most probable sense of w. noun child detention feeli"
P03-1058,P02-1044,0,0.0625916,"in WSD. As to the problem of insufficient sense coverage, with the steady increase and availability of parallel corpora, we believe that getting sufficient sense coverage from larger parallel corpora should not be a problem in the near future for most of the commonly occurring words in a language. 5 Related Work Brown et al. (1991) is the first to have explored statistical methods in word sense disambiguation in the context of machine translation. However, they only looked at assigning at most two senses to a word, and their method only asked a single question about a single word of context. Li and Li (2002) investigated a bilingual bootstrapping technique, which differs from the method we implemented here. Their method also does not require a parallel corpus. The research of (Chugur et al., 2002) dealt with sense distinctions across multiple languages. Ide et al. (2002) investigated word sense distinctions using parallel corpora. Resnik and Yarowsky (2000) considered word sense disambiguation using multiple languages. Our present work can be similarly extended beyond bilingual corpora to multilingual corpora. The research most similar to ours is the work of Diab and Resnik (2002). However, they"
P03-1058,S01-1031,0,0.0159488,"ple for a supervised WSD program in the next step. The average time taken to perform manual selection of target translations for one SENSEVAL-2 English noun is less than 15 minutes. This is a relatively short time, especially when compared to the effort that we would otherwise need to spend to perform manual sense-tagging of training examples. This step could also be potentially automated if we have a suitable bilingual translation lexicon. 2.3 Training of WSD Classifier Much research has been done on the best supervised learning approach for WSD (Florian and Yarowsky, 2002; Lee and Ng, 2002; Mihalcea and Moldovan, 2001; Yarowsky et al., 2001). In this paper, we used the WSD program reported in (Lee and Ng, 2002). In particular, our method made use of the knowledge sources of part-of-speech, surrounding words, and local collocations. We used naïve Bayes as the learning algorithm. Our previous research demonstrated that such an approach leads to a state-of-the-art WSD program with good performance. 2.4 WSD of Words in New Contexts Given an occurrence of w in a new context, we then used the naïve Bayes classifier to determine the most probable sense of w. noun child detention feeling holiday lady material yew"
P03-1058,P00-1056,0,0.0171877,"ilgarriff 2001), which used the WordNet 1.7 sense inventory (Miller, 1990). While our approach has only been tested on English and Chinese, it is completely general and applicable to other language pairs. 2 Approach Our approach of exploiting parallel texts for word sense disambiguation consists of four steps: (1) parallel text alignment (2) manual selection of target translations (3) training of WSD classifier (4) WSD of words in new contexts. 2.1 Parallel Text Alignment In this step, parallel texts are first sentence-aligned and then word-aligned. Various alignment algorithms (Melamed 2001; Och and Ney 2000) have been developed in the past. For the six bilingual corpora that we used, they already come with sentences pre-aligned, either manually when the corpora were prepared or automatically by sentencealignment programs. After sentence alignment, the English texts are tokenized so that a punctuation symbol is separated from its preceding word. For the Chinese texts, we performed word segmentation, so that Chinese characters are segmented into words. The resulting parallel texts are then input to the GIZA++ software (Och and Ney 2000) for word alignment. In the output of GIZA++, each English word"
P03-1058,P99-1068,0,0.00954089,"agged corpus without manual annotation. However, are current word alignment algorithms accurate enough for our purpose? (iii) Ultimately, using a state-of-the-art supervised WSD program, what is its disambiguation accuracy when it is trained on a “sense-tagged” corpus obtained via parallel text alignment, compared with training on a manually sense-tagged corpus? Much research remains to be done to investigate all of the above issues. The lack of large-scale parallel corpora no doubt has impeded progress in this direction, although attempts have been made to mine parallel corpora from the Web (Resnik, 1999). However, large-scale, good-quality parallel corpora have recently become available. For example, six English-Chinese parallel corpora are now available from Linguistic Data Consortium. These parallel corpora are listed in Table 2, with a combined size of 280 MB. In this paper, we address the above issues and report our findings, exploiting the English-Chinese parallel corpora in Table 2 for word sense disambiguation. We evaluated our approach on all the nouns in the English lexical sample task of SENSEVAL-2 (Edmonds and Cotton, 2001; Kilgarriff 2001), which used the WordNet 1.7 sense invento"
P03-1058,W97-0213,0,0.0607482,"e inventory in a dictionary. This annotated corpus then serves as the training material for a learning algorithm. After training, a model is automatically learned and it is used to assign the correct sense to any previously unseen occurrence of w in a new context. While the supervised learning approach has been successful, it has the drawback of requiring manually sense-tagged data. This problem is particular severe for WSD, since sense-tagged data must be collected separately for each word in a language. One source to look for potential training data for WSD is parallel texts, as proposed by Resnik and Yarowsky (1997). Given a word-aligned parallel corpus, the different translations in a target language serve as the “sense-tags” of an ambiguous word in the source language. For example, some possible Chinese translations of the English noun channel are listed in Table 1. To illustrate, if the sense of an occurrence of the noun channel is “a path over which electrical signals can pass”, then this occurrence can be translated as “频道” in Chinese. WordNet 1.7 sense id 1 2 3 4 5 6 7 Lumped sense id 1 2 3 4 5 6 1 Chinese translations WordNet 1.7 English sense descriptions 频道 水道 水渠 排水渠 沟 海峡 途径 导管 频道 A path over wh"
P03-1058,S01-1040,0,0.00957864,"ram in the next step. The average time taken to perform manual selection of target translations for one SENSEVAL-2 English noun is less than 15 minutes. This is a relatively short time, especially when compared to the effort that we would otherwise need to spend to perform manual sense-tagging of training examples. This step could also be potentially automated if we have a suitable bilingual translation lexicon. 2.3 Training of WSD Classifier Much research has been done on the best supervised learning approach for WSD (Florian and Yarowsky, 2002; Lee and Ng, 2002; Mihalcea and Moldovan, 2001; Yarowsky et al., 2001). In this paper, we used the WSD program reported in (Lee and Ng, 2002). In particular, our method made use of the knowledge sources of part-of-speech, surrounding words, and local collocations. We used naïve Bayes as the learning algorithm. Our previous research demonstrated that such an approach leads to a state-of-the-art WSD program with good performance. 2.4 WSD of Words in New Contexts Given an occurrence of w in a new context, we then used the naïve Bayes classifier to determine the most probable sense of w. noun child detention feeling holiday lady material yew bar bum chair day dyke f"
P03-1058,J04-1001,0,\N,Missing
P15-1009,W02-1109,0,0.0568239,"Missing"
P15-1009,D13-1167,0,0.0142141,"Missing"
P15-1009,D14-1165,0,0.392462,"Missing"
P15-1009,N13-1008,0,0.0299875,", so as to simplify the manipulation while preserving the inherent structure of the original graph. Specifically, given a KG, entities and relations are first represented in a low-dimensional vector space, and for each triple, a scoring function is defined to measure its plausibility in that space. Then the representations of entities and relations (i.e. embeddings) are learned by maximizing the total plausibility of observed triples. The learned embeddings can further be used to benefit all kinds of tasks, such as KG completion (Socher et al., 2013; Bordes et al., 2013), relation extraction (Riedel et al., 2013; Weston et al., 2013), and entity resolution (Bordes et al., 2014). To our knowledge, most of existing KG embedding methods perform the embedding task based solely on observed facts. The only requirement is that the learned embeddings should be compatible within each individual fact. In this paper we propose Semantically Smooth Embedding (SSE), a new approach which further imposes constraints on the geometric structure of the embedding space. The key idea of SSE is to make fulThis paper considers the problem of embedding Knowledge Graphs (KGs) consisting of entities and relations into lowdime"
P15-1009,P11-1055,0,0.0231326,"Smooth Knowledge Graph Embedding † Shu Guo† , Quan Wang†∗, Bin Wang† , Lihong Wang‡ , Li Guo† Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100093, China {guoshu,wangquan,wangbin,guoli}@iie.ac.cn ‡ National Computer Network Emergency Response Technical Team Coordination Center of China, Beijing 100029, China wlh@isc.org.cn Abstract pedia (Lehmann et al., 2014) have become extremely useful resources for many NLP related applications, such as word sense disambiguation (Agirre et al., 2014), named entity recognition (Magnini et al., 2002), and information extraction (Hoﬀmann et al., 2011). A KG is a multirelational directed graph composed of entities as nodes and relations as edges. Each edge is represented as a triple of fact ⟨ei , rk , e j ⟩, indicating that head entity ei and tail entity e j are connected by relation rk . Although powerful in representing structured data, the underlying symbolic nature makes KGs hard to manipulate. Recently a new research direction called knowledge graph embedding has attracted much attention (Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014; Lin et al., 2015). It attempts to embed components of a KG into continuous vector spac"
P15-1009,D14-1118,1,0.714314,"Missing"
P15-1009,D13-1136,0,0.0117642,"he manipulation while preserving the inherent structure of the original graph. Specifically, given a KG, entities and relations are first represented in a low-dimensional vector space, and for each triple, a scoring function is defined to measure its plausibility in that space. Then the representations of entities and relations (i.e. embeddings) are learned by maximizing the total plausibility of observed triples. The learned embeddings can further be used to benefit all kinds of tasks, such as KG completion (Socher et al., 2013; Bordes et al., 2013), relation extraction (Riedel et al., 2013; Weston et al., 2013), and entity resolution (Bordes et al., 2014). To our knowledge, most of existing KG embedding methods perform the embedding task based solely on observed facts. The only requirement is that the learned embeddings should be compatible within each individual fact. In this paper we propose Semantically Smooth Embedding (SSE), a new approach which further imposes constraints on the geometric structure of the embedding space. The key idea of SSE is to make fulThis paper considers the problem of embedding Knowledge Graphs (KGs) consisting of entities and relations into lowdimensional vector spaces."
P15-1009,J14-1003,0,\N,Missing
P15-1076,P01-1003,0,0.148431,"Missing"
P15-1076,P04-1007,0,0.0486202,"these two sets of parameters are improved to be −152 and −119 respectively. The average test log-likelihoods are both −96 for the two corresponding TDRF models in Table 3. This is some evidence for the model deficiency of the WSME distribution as defined in (3), and introducing the empirical length probabilities gives a more reasonable model assumption. TDRF vs conditional ME. After training, TDRF models are computationally more efficient in computing sentence probability, simply summing up weights for the activated features in the sentence. The conditional ME models (Khudanpur and Wu, 2000; Roark et al., 2004) suffer from the expensive computation of local normalization factors. This computational bottleneck hinders their use in practice (Goodman, 2001b; Rosenfeld et al., 2001). Partly for this reason, although building conditional ME models with sophisticated features as in Table 1 is theoretically possible, such work has not been pursued so far. TDRF vs RNN. The RNN models suffer from the expensive softmax computation in the output layer 8 . Empirically in our experiments, the average time costs for re-ranking of the 1000-best list for a sentence are 0.16 sec vs 40 sec, based on TDRF and RNN resp"
P15-1076,N09-1053,0,0.0315936,"Algorithm 3), we first generate C ∼ Qk+1 (c) and then generate Y from class C by g˘k+1 (y|xk , C) = P p(k + 1, {xk , y}; λ, ζ) k w∈VC p(k + 1, {x , w}; λ, ζ) (31) with xk = X (t−1) . Then we set L(t) = j and X (t) = {X (t−1) , Y } with probability as defined in (21), by setting gk+1 (y|xk ) = Qk+1 (C)˘ gk+1 (y|xk , C). Table 1: Feature definition in TDRF LMs 1999), denoted by KN4. We use the RNNLM toolkit5 to train a RNNLM (Mikolov et al., 2011). The number of hidden units is 250 and other configurations are set by default6 . Word classing has been shown to be useful in conditional ME models (Chen, 2009). For our TDRF models, we consider a variety of features as shown in Table 1, mainly based on word and class information. Each word is deterministically assigned to a single class, by running the automatic clustering algorithm proposed in (Martin et al., 1998) on the training data. In Table 1, wi , ci , i = 0, −1, . . . , −5 denote the word and its class at different position offset i, e.g. w0 , c0 denotes the current word and its class. We first introduce the classic word/class n-gram features (denoted by “w”/“c”) and the word/class skipping n-gram features (denoted by “ws”/“cs”) (Goodman, 20"
P16-1124,P15-1127,0,0.0172451,"n via Coupled Path Ranking Quan Wang† , Jing Liu‡ , Yuanfei Luo† , Bin Wang† , Chin-Yew Lin‡ † Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100093, China {wangquan,luoyuanfei,wangbin}@iie.ac.cn ‡ Microsoft Research, Beijing 100080, China {liudani,cyl}@microsoft.com Abstract their relations, typically stored as (head entity, relation, tail entity) triples, e.g., (Paris, capitalOf, France). Although such KBs can be impressively large, they are still quite incomplete and missing crucial facts, which may reduce their usefulness in downstream tasks (West et al., 2014; Choi et al., 2015). KB completion, i.e., automatically inferring missing facts by examining existing ones, has thus attracted increasing attention. Approaches to this task roughly fall into three categories: (i) path ranking algorithms (PRA) (Lao et al., 2011); (ii) embedding techniques (Bordes et al., 2013; Guo et al., 2015); and (iii) graphical models such as Markov logic networks (MLN) (Richardson and Domingos, 2006). This paper focuses on PRA, which is easily interpretable (as opposed to embedding techniques) and requires no external logic rules (as opposed to MLN). Knowledge bases (KBs) are often greatly i"
P16-1124,D07-1074,0,0.0704711,"ion type that exists between two entities. Given a specific relation, random walks are first employed to find paths between two entities that have the given relation. Here a path is a sequence of relations linking bornIn capitalOf two entities, e.g., h −−−−−→ e −−−−−−−−→ t. These paths are then used as features in a binary classifier to predict if new instances (i.e., entity pairs) have the given relation. Introduction Knowledge bases (KBs) like Freebase (Bollacker et al., 2008), DBpedia (Lehmann et al., 2014), and NELL (Carlson et al., 2010) are extremely useful resources for many NLP tasks (Cucerzan, 2007; Schuhmacher and Ponzetto, 2014). They provide large collections of facts about entities and While KBs are naturally composed of multiple relations, PRA models these relations separately during the inference phase, by learning an individual classifier for each relation. We argue, however, that it will be beneficial for PRA to model certain relations in a collective way, particularly when the relations are closely related to each other. For example, given two relations bornIn and livedIn, 1308 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1308–1"
P16-1124,D13-1107,0,0.0278819,"hat learning multiple related tasks simulta1309 neously can provide significant benefits relative to learning them independently (Caruana, 1997). A key ingredient of multi-task learning is to model the notion of task relatedness, through either parameter sharing (Evgeniou and Pontil, 2004; Ando and Zhang, 2005) or feature sharing (Argyriou et al., 2007; He et al., 2014). In recent years, there has been increasing work showing the benefits of multi-task learning in NLP-related tasks, such as relation extraction (Jiang, 2009; Carlson et al., 2010) and machine translation (Sennrich et al., 2013; Cui et al., 2013; Dong et al., 2015). This paper investigates the possibility of multi-task learning with PRA, in a parameter sharing manner. Connection with previous methods. Actually, modeling multiple relations collectively is a common practice in embedding-based approaches. In such a method, embeddings are learned jointly for all relations, over a set of shared latent features (entity embeddings), and hence can capture meaningful associations among different relations. As shown by (Toutanova and Chen, 2015), observed features such as PRA paths usually perform better than latent features for KB completion."
P16-1124,P15-1166,0,0.021292,"ple related tasks simulta1309 neously can provide significant benefits relative to learning them independently (Caruana, 1997). A key ingredient of multi-task learning is to model the notion of task relatedness, through either parameter sharing (Evgeniou and Pontil, 2004; Ando and Zhang, 2005) or feature sharing (Argyriou et al., 2007; He et al., 2014). In recent years, there has been increasing work showing the benefits of multi-task learning in NLP-related tasks, such as relation extraction (Jiang, 2009; Carlson et al., 2010) and machine translation (Sennrich et al., 2013; Cui et al., 2013; Dong et al., 2015). This paper investigates the possibility of multi-task learning with PRA, in a parameter sharing manner. Connection with previous methods. Actually, modeling multiple relations collectively is a common practice in embedding-based approaches. In such a method, embeddings are learned jointly for all relations, over a set of shared latent features (entity embeddings), and hence can capture meaningful associations among different relations. As shown by (Toutanova and Chen, 2015), observed features such as PRA paths usually perform better than latent features for KB completion. In this context, CP"
P16-1124,D15-1173,0,0.719743,"013; Jiang et al., 2012). This paper focuses on PRA, since it is easily interpretable (as opposed to embedding-based models) and requires no external logic rules (as opposed to MLN and its variants). PRA and its extensions. PRA is a random walk inference technique designed for predicting new relation instances in KBs, first proposed by Lao and Cohen (2010). Recently various extensions have been explored, ranging from incorporating a text corpus as additional evidence during inference (Gardner et al., 2013; Gardner et al., 2014), to introducing better schemes to generate more predictive paths (Gardner and Mitchell, 2015; Shi and Weninger, 2015), or using PRA in a broader context such as Google’s Knowledge Vault (Dong et al., 2014). All these approaches are based on some single-task version of PRA, while our work explores multi-task learning for it. Multi-task learning. Numerous studies have shown that learning multiple related tasks simulta1309 neously can provide significant benefits relative to learning them independently (Caruana, 1997). A key ingredient of multi-task learning is to model the notion of task relatedness, through either parameter sharing (Evgeniou and Pontil, 2004; Ando and Zhang, 2005) or"
P16-1124,D13-1080,0,0.0208147,"(iii) probabilistic graphical models such as the Markov logic network (MLN) and its variants (Pujara et al., 2013; Jiang et al., 2012). This paper focuses on PRA, since it is easily interpretable (as opposed to embedding-based models) and requires no external logic rules (as opposed to MLN and its variants). PRA and its extensions. PRA is a random walk inference technique designed for predicting new relation instances in KBs, first proposed by Lao and Cohen (2010). Recently various extensions have been explored, ranging from incorporating a text corpus as additional evidence during inference (Gardner et al., 2013; Gardner et al., 2014), to introducing better schemes to generate more predictive paths (Gardner and Mitchell, 2015; Shi and Weninger, 2015), or using PRA in a broader context such as Google’s Knowledge Vault (Dong et al., 2014). All these approaches are based on some single-task version of PRA, while our work explores multi-task learning for it. Multi-task learning. Numerous studies have shown that learning multiple related tasks simulta1309 neously can provide significant benefits relative to learning them independently (Caruana, 1997). A key ingredient of multi-task learning is to model th"
P16-1124,D14-1044,0,0.467605,"aphical models such as the Markov logic network (MLN) and its variants (Pujara et al., 2013; Jiang et al., 2012). This paper focuses on PRA, since it is easily interpretable (as opposed to embedding-based models) and requires no external logic rules (as opposed to MLN and its variants). PRA and its extensions. PRA is a random walk inference technique designed for predicting new relation instances in KBs, first proposed by Lao and Cohen (2010). Recently various extensions have been explored, ranging from incorporating a text corpus as additional evidence during inference (Gardner et al., 2013; Gardner et al., 2014), to introducing better schemes to generate more predictive paths (Gardner and Mitchell, 2015; Shi and Weninger, 2015), or using PRA in a broader context such as Google’s Knowledge Vault (Dong et al., 2014). All these approaches are based on some single-task version of PRA, while our work explores multi-task learning for it. Multi-task learning. Numerous studies have shown that learning multiple related tasks simulta1309 neously can provide significant benefits relative to learning them independently (Caruana, 1997). A key ingredient of multi-task learning is to model the notion of task relate"
P16-1124,P15-1009,1,0.717368,"relations, typically stored as (head entity, relation, tail entity) triples, e.g., (Paris, capitalOf, France). Although such KBs can be impressively large, they are still quite incomplete and missing crucial facts, which may reduce their usefulness in downstream tasks (West et al., 2014; Choi et al., 2015). KB completion, i.e., automatically inferring missing facts by examining existing ones, has thus attracted increasing attention. Approaches to this task roughly fall into three categories: (i) path ranking algorithms (PRA) (Lao et al., 2011); (ii) embedding techniques (Bordes et al., 2013; Guo et al., 2015); and (iii) graphical models such as Markov logic networks (MLN) (Richardson and Domingos, 2006). This paper focuses on PRA, which is easily interpretable (as opposed to embedding techniques) and requires no external logic rules (as opposed to MLN). Knowledge bases (KBs) are often greatly incomplete, necessitating a demand for KB completion. The path ranking algorithm (PRA) is one of the most promising approaches to this task. Previous work on PRA usually follows a single-task learning paradigm, building a prediction model for each relation independently with its own training data. It ignores"
P16-1124,P09-1114,0,0.00817837,"ores multi-task learning for it. Multi-task learning. Numerous studies have shown that learning multiple related tasks simulta1309 neously can provide significant benefits relative to learning them independently (Caruana, 1997). A key ingredient of multi-task learning is to model the notion of task relatedness, through either parameter sharing (Evgeniou and Pontil, 2004; Ando and Zhang, 2005) or feature sharing (Argyriou et al., 2007; He et al., 2014). In recent years, there has been increasing work showing the benefits of multi-task learning in NLP-related tasks, such as relation extraction (Jiang, 2009; Carlson et al., 2010) and machine translation (Sennrich et al., 2013; Cui et al., 2013; Dong et al., 2015). This paper investigates the possibility of multi-task learning with PRA, in a parameter sharing manner. Connection with previous methods. Actually, modeling multiple relations collectively is a common practice in embedding-based approaches. In such a method, embeddings are learned jointly for all relations, over a set of shared latent features (entity embeddings), and hence can capture meaningful associations among different relations. As shown by (Toutanova and Chen, 2015), observed f"
P16-1124,P13-1082,0,0.0262895,"us studies have shown that learning multiple related tasks simulta1309 neously can provide significant benefits relative to learning them independently (Caruana, 1997). A key ingredient of multi-task learning is to model the notion of task relatedness, through either parameter sharing (Evgeniou and Pontil, 2004; Ando and Zhang, 2005) or feature sharing (Argyriou et al., 2007; He et al., 2014). In recent years, there has been increasing work showing the benefits of multi-task learning in NLP-related tasks, such as relation extraction (Jiang, 2009; Carlson et al., 2010) and machine translation (Sennrich et al., 2013; Cui et al., 2013; Dong et al., 2015). This paper investigates the possibility of multi-task learning with PRA, in a parameter sharing manner. Connection with previous methods. Actually, modeling multiple relations collectively is a common practice in embedding-based approaches. In such a method, embeddings are learned jointly for all relations, over a set of shared latent features (entity embeddings), and hence can capture meaningful associations among different relations. As shown by (Toutanova and Chen, 2015), observed features such as PRA paths usually perform better than latent features"
P16-1124,D11-1049,0,0.327152,"Missing"
P16-1124,W15-4007,0,0.0659011,"h as relation extraction (Jiang, 2009; Carlson et al., 2010) and machine translation (Sennrich et al., 2013; Cui et al., 2013; Dong et al., 2015). This paper investigates the possibility of multi-task learning with PRA, in a parameter sharing manner. Connection with previous methods. Actually, modeling multiple relations collectively is a common practice in embedding-based approaches. In such a method, embeddings are learned jointly for all relations, over a set of shared latent features (entity embeddings), and hence can capture meaningful associations among different relations. As shown by (Toutanova and Chen, 2015), observed features such as PRA paths usually perform better than latent features for KB completion. In this context, CPRA is designed in a way that gets the multi-relational benefit of embedding techniques while keeping PRA-style path features. Nickel et al. (2014) and Neelakantan et al. (2015) have tried similar ideas. However, their work focuses on improving embedding techniques with observed features, while our approach aims at improving PRA with multi-task learning. 3 Path Ranking Algorithm PRA was first proposed by Lao and Cohen (2010), and later slightly modified in various ways (Gardne"
P16-1124,P15-1016,0,0.114913,"odeling multiple relations collectively is a common practice in embedding-based approaches. In such a method, embeddings are learned jointly for all relations, over a set of shared latent features (entity embeddings), and hence can capture meaningful associations among different relations. As shown by (Toutanova and Chen, 2015), observed features such as PRA paths usually perform better than latent features for KB completion. In this context, CPRA is designed in a way that gets the multi-relational benefit of embedding techniques while keeping PRA-style path features. Nickel et al. (2014) and Neelakantan et al. (2015) have tried similar ideas. However, their work focuses on improving embedding techniques with observed features, while our approach aims at improving PRA with multi-task learning. 3 Path Ranking Algorithm PRA was first proposed by Lao and Cohen (2010), and later slightly modified in various ways (Gardner et al., 2014; Gardner and Mitchell, 2015). The key idea of PRA is to explicitly use paths that connect two entities as features to predict potential relations between them. Here a path is a sequence of relations ⟨r1 , r2 , · · · , rℓ ⟩ that link two entities. For example, ⟨bornIn, capitalOf⟩ i"
P18-1011,D14-1165,0,0.0377694,"or space, so as to simplify manipulation while preserving the inherent structure of the KG. Early works on this topic learned such vectorial representations (i.e., embeddings) via just simple models developed over KG triples (Bordes et al., 2011, 2013; Jenatton et al., 2012; Nickel et al., 2011). Recent attempts focused on either designing more complicated triple scoring models (Socher et al., 2013; Bordes et al., 2014; Wang et al., 2014; Lin et al., 2015b; Xiao et al., 2016; Nickel et al., 2016b; Trouillon et al., 2016; Liu et al., 2017), or incorporating extra information beyond KG triples (Chang et al., 2014; Zhong et al., 2015; Lin et al., 2015a; Neelakantan et al., 2015; Guo et al., 2015; Luo et al., 2015b; Xie et al., 2016a,b; Xiao et al., 2017). See (Wang et al., 2017) for a thorough review. This paper, by contrast, investigates the potential of using very simple constraints to improve the KG embedding task. Specifically, we examine two types of constraints: (i) non-negativity constraints on entity representations and (ii) approximate entailment constraints over relation representations. By using the former, we learn compact representations for entities, which would naturally induce sparsity"
P18-1011,P11-1055,0,0.0524685,"tion Engineering, Chinese Academy of Sciences 2 School of Cyber Security, University of Chinese Academy of Sciences 3 State Key Laboratory of Information Security, Chinese Academy of Sciences {dingboyang,wangquan,wangbin,guoli}@iie.ac.cn Abstract Vault (Dong et al., 2014). A typical KG is a multirelational graph composed of entities as nodes and relations as different types of edges, where each edge is represented as a triple of the form (head entity, relation, tail entity). Such KGs contain rich structured knowledge, and have proven useful for many NLP tasks (Wasserman-Pritsker et al., 2015; Hoffmann et al., 2011; Yang and Mitchell, 2017). Recently, the concept of knowledge graph embedding has been presented and quickly become a hot research topic. The key idea there is to embed components of a KG (i.e., entities and relations) into a continuous vector space, so as to simplify manipulation while preserving the inherent structure of the KG. Early works on this topic learned such vectorial representations (i.e., embeddings) via just simple models developed over KG triples (Bordes et al., 2011, 2013; Jenatton et al., 2012; Nickel et al., 2011). Recent attempts focused on either designing more complicated"
P18-1011,D16-1146,0,0.0690775,"Missing"
P18-1011,P15-1067,0,0.0890668,"Missing"
P18-1011,W17-2609,0,0.134452,"Missing"
P18-1011,Q15-1027,0,0.0503449,"Missing"
P18-1011,N15-1004,0,0.0485874,"Missing"
P18-1011,D15-1082,0,0.0565993,"en presented and quickly become a hot research topic. The key idea there is to embed components of a KG (i.e., entities and relations) into a continuous vector space, so as to simplify manipulation while preserving the inherent structure of the KG. Early works on this topic learned such vectorial representations (i.e., embeddings) via just simple models developed over KG triples (Bordes et al., 2011, 2013; Jenatton et al., 2012; Nickel et al., 2011). Recent attempts focused on either designing more complicated triple scoring models (Socher et al., 2013; Bordes et al., 2014; Wang et al., 2014; Lin et al., 2015b; Xiao et al., 2016; Nickel et al., 2016b; Trouillon et al., 2016; Liu et al., 2017), or incorporating extra information beyond KG triples (Chang et al., 2014; Zhong et al., 2015; Lin et al., 2015a; Neelakantan et al., 2015; Guo et al., 2015; Luo et al., 2015b; Xie et al., 2016a,b; Xiao et al., 2017). See (Wang et al., 2017) for a thorough review. This paper, by contrast, investigates the potential of using very simple constraints to improve the KG embedding task. Specifically, we examine two types of constraints: (i) non-negativity constraints on entity representations and (ii) approximate e"
P18-1011,P15-1009,1,0.876888,"he KG. Early works on this topic learned such vectorial representations (i.e., embeddings) via just simple models developed over KG triples (Bordes et al., 2011, 2013; Jenatton et al., 2012; Nickel et al., 2011). Recent attempts focused on either designing more complicated triple scoring models (Socher et al., 2013; Bordes et al., 2014; Wang et al., 2014; Lin et al., 2015b; Xiao et al., 2016; Nickel et al., 2016b; Trouillon et al., 2016; Liu et al., 2017), or incorporating extra information beyond KG triples (Chang et al., 2014; Zhong et al., 2015; Lin et al., 2015a; Neelakantan et al., 2015; Guo et al., 2015; Luo et al., 2015b; Xie et al., 2016a,b; Xiao et al., 2017). See (Wang et al., 2017) for a thorough review. This paper, by contrast, investigates the potential of using very simple constraints to improve the KG embedding task. Specifically, we examine two types of constraints: (i) non-negativity constraints on entity representations and (ii) approximate entailment constraints over relation representations. By using the former, we learn compact representations for entities, which would naturally induce sparsity and interpretability (Murphy et al., 2012). By using the latter, we further encode"
P18-1011,D16-1019,1,0.859791,"Missing"
P18-1011,D15-1196,0,0.0221553,"on this topic learned such vectorial representations (i.e., embeddings) via just simple models developed over KG triples (Bordes et al., 2011, 2013; Jenatton et al., 2012; Nickel et al., 2011). Recent attempts focused on either designing more complicated triple scoring models (Socher et al., 2013; Bordes et al., 2014; Wang et al., 2014; Lin et al., 2015b; Xiao et al., 2016; Nickel et al., 2016b; Trouillon et al., 2016; Liu et al., 2017), or incorporating extra information beyond KG triples (Chang et al., 2014; Zhong et al., 2015; Lin et al., 2015a; Neelakantan et al., 2015; Guo et al., 2015; Luo et al., 2015b; Xie et al., 2016a,b; Xiao et al., 2017). See (Wang et al., 2017) for a thorough review. This paper, by contrast, investigates the potential of using very simple constraints to improve the KG embedding task. Specifically, we examine two types of constraints: (i) non-negativity constraints on entity representations and (ii) approximate entailment constraints over relation representations. By using the former, we learn compact representations for entities, which would naturally induce sparsity and interpretability (Murphy et al., 2012). By using the latter, we further encode regularities of lo"
P18-1011,D15-1191,1,0.8213,"on this topic learned such vectorial representations (i.e., embeddings) via just simple models developed over KG triples (Bordes et al., 2011, 2013; Jenatton et al., 2012; Nickel et al., 2011). Recent attempts focused on either designing more complicated triple scoring models (Socher et al., 2013; Bordes et al., 2014; Wang et al., 2014; Lin et al., 2015b; Xiao et al., 2016; Nickel et al., 2016b; Trouillon et al., 2016; Liu et al., 2017), or incorporating extra information beyond KG triples (Chang et al., 2014; Zhong et al., 2015; Lin et al., 2015a; Neelakantan et al., 2015; Guo et al., 2015; Luo et al., 2015b; Xie et al., 2016a,b; Xiao et al., 2017). See (Wang et al., 2017) for a thorough review. This paper, by contrast, investigates the potential of using very simple constraints to improve the KG embedding task. Specifically, we examine two types of constraints: (i) non-negativity constraints on entity representations and (ii) approximate entailment constraints over relation representations. By using the former, we learn compact representations for entities, which would naturally induce sparsity and interpretability (Murphy et al., 2012). By using the latter, we further encode regularities of lo"
P18-1011,C12-1118,0,0.0291941,"Lin et al., 2015a; Neelakantan et al., 2015; Guo et al., 2015; Luo et al., 2015b; Xie et al., 2016a,b; Xiao et al., 2017). See (Wang et al., 2017) for a thorough review. This paper, by contrast, investigates the potential of using very simple constraints to improve the KG embedding task. Specifically, we examine two types of constraints: (i) non-negativity constraints on entity representations and (ii) approximate entailment constraints over relation representations. By using the former, we learn compact representations for entities, which would naturally induce sparsity and interpretability (Murphy et al., 2012). By using the latter, we further encode regularities of logical entailment between relations into their Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Early works performed this task via simple models developed over KG triples. Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples. This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding. We examine non-negativity constraints on entity representations and approximat"
P18-1011,P15-1016,0,0.0342747,"he inherent structure of the KG. Early works on this topic learned such vectorial representations (i.e., embeddings) via just simple models developed over KG triples (Bordes et al., 2011, 2013; Jenatton et al., 2012; Nickel et al., 2011). Recent attempts focused on either designing more complicated triple scoring models (Socher et al., 2013; Bordes et al., 2014; Wang et al., 2014; Lin et al., 2015b; Xiao et al., 2016; Nickel et al., 2016b; Trouillon et al., 2016; Liu et al., 2017), or incorporating extra information beyond KG triples (Chang et al., 2014; Zhong et al., 2015; Lin et al., 2015a; Neelakantan et al., 2015; Guo et al., 2015; Luo et al., 2015b; Xie et al., 2016a,b; Xiao et al., 2017). See (Wang et al., 2017) for a thorough review. This paper, by contrast, investigates the potential of using very simple constraints to improve the KG embedding task. Specifically, we examine two types of constraints: (i) non-negativity constraints on entity representations and (ii) approximate entailment constraints over relation representations. By using the former, we learn compact representations for entities, which would naturally induce sparsity and interpretability (Murphy et al., 2012). By using the latter,"
P18-1011,D15-1192,0,0.0180083,"Li Guo1,2 1 Institute of Information Engineering, Chinese Academy of Sciences 2 School of Cyber Security, University of Chinese Academy of Sciences 3 State Key Laboratory of Information Security, Chinese Academy of Sciences {dingboyang,wangquan,wangbin,guoli}@iie.ac.cn Abstract Vault (Dong et al., 2014). A typical KG is a multirelational graph composed of entities as nodes and relations as different types of edges, where each edge is represented as a triple of the form (head entity, relation, tail entity). Such KGs contain rich structured knowledge, and have proven useful for many NLP tasks (Wasserman-Pritsker et al., 2015; Hoffmann et al., 2011; Yang and Mitchell, 2017). Recently, the concept of knowledge graph embedding has been presented and quickly become a hot research topic. The key idea there is to embed components of a KG (i.e., entities and relations) into a continuous vector space, so as to simplify manipulation while preserving the inherent structure of the KG. Early works on this topic learned such vectorial representations (i.e., embeddings) via just simple models developed over KG triples (Bordes et al., 2011, 2013; Jenatton et al., 2012; Nickel et al., 2011). Recent attempts focused on either des"
P18-1011,P16-1219,0,0.233633,"Missing"
P18-1011,N15-1118,0,0.0334101,"Missing"
P18-1011,P17-1132,0,0.0269004,"se Academy of Sciences 2 School of Cyber Security, University of Chinese Academy of Sciences 3 State Key Laboratory of Information Security, Chinese Academy of Sciences {dingboyang,wangquan,wangbin,guoli}@iie.ac.cn Abstract Vault (Dong et al., 2014). A typical KG is a multirelational graph composed of entities as nodes and relations as different types of edges, where each edge is represented as a triple of the form (head entity, relation, tail entity). Such KGs contain rich structured knowledge, and have proven useful for many NLP tasks (Wasserman-Pritsker et al., 2015; Hoffmann et al., 2011; Yang and Mitchell, 2017). Recently, the concept of knowledge graph embedding has been presented and quickly become a hot research topic. The key idea there is to embed components of a KG (i.e., entities and relations) into a continuous vector space, so as to simplify manipulation while preserving the inherent structure of the KG. Early works on this topic learned such vectorial representations (i.e., embeddings) via just simple models developed over KG triples (Bordes et al., 2011, 2013; Jenatton et al., 2012; Nickel et al., 2011). Recent attempts focused on either designing more complicated triple scoring models (So"
P18-1011,D15-1031,0,0.0220982,"implify manipulation while preserving the inherent structure of the KG. Early works on this topic learned such vectorial representations (i.e., embeddings) via just simple models developed over KG triples (Bordes et al., 2011, 2013; Jenatton et al., 2012; Nickel et al., 2011). Recent attempts focused on either designing more complicated triple scoring models (Socher et al., 2013; Bordes et al., 2014; Wang et al., 2014; Lin et al., 2015b; Xiao et al., 2016; Nickel et al., 2016b; Trouillon et al., 2016; Liu et al., 2017), or incorporating extra information beyond KG triples (Chang et al., 2014; Zhong et al., 2015; Lin et al., 2015a; Neelakantan et al., 2015; Guo et al., 2015; Luo et al., 2015b; Xie et al., 2016a,b; Xiao et al., 2017). See (Wang et al., 2017) for a thorough review. This paper, by contrast, investigates the potential of using very simple constraints to improve the KG embedding task. Specifically, we examine two types of constraints: (i) non-negativity constraints on entity representations and (ii) approximate entailment constraints over relation representations. By using the former, we learn compact representations for entities, which would naturally induce sparsity and interpretability"
S19-2095,S19-2007,0,0.0189947,"race theory to identify racism and gender discrimination, they use n-gram models for research; Tulkens et al. studied racism detection in Dutch social media (Tulkens et al., 2016). A recent discussion of the challenge of identifying hate speech was proposed by Kumar et al. (Kumar et al., 2018). The results show that it is difficult to distinguish between open and covert attacks in social media. SemEval 2019 Task 5 is proposed to identify hate speech about immigrants and women in Twitter for English or Spanish, and classify hate speech and judge whether the target is an individual or a group (Basile et al., 2019). Hate speech is often defined as any communication that attacks an individual or group through certain characteristics (such as gender, nationality, religion, or other characteristics) in social media platforms. This task gives us some text data from Twitter, we need to classify the content through computational analysis. The task has two subtasks, in which Subtask A is Hate speech detection for immigrants and women: It’s a binary classification task, the system must judge whether a tweet with a specific goal (female or immigrant) in English or Spanish is hate speech; Subtask B is Aggressive"
S19-2095,E17-2068,0,0.0214039,"Ba, 2014). • Hashtags are important markers for determining sentiment or user intention. The “#” symbol is removed and the word itself is retained. e.g.: in the sentence, “#BuildTheWall and #BuildThatWall” are marked as 1 in most cases in the training data set. • Username mentions, e.g.: words starting with “@”, generally provide no information in terms of sentiment. Hence such terms are removed completely from the tweets. 3.3 • Repeated full stops, question marks and exclamation marks are replaced with a single instance with a special token “repeat” added. For this task, we select fastText (Joulin et al., 2017), because in this task we find that the result of fastText is much better than other word vectors such as Word2vec and Glove. Table 1 is the result of different word vectors as embedding. • All contractions are split into two tokens by using regular expression (e.g.: “it’s” is changed to “it” and “is”). Word Vector Word2vec Glove-twitter BPEmb fastText • All URLs, phone numbers and date numbers are replaced respectively as “URL”, “PHONENUMBER”, “NUMBER”. • Emoticons (such as, ‘:(’, ‘:)’, ‘:P’ and emoji etc.) are replaced as their own meanings by emotion lexicons1 . Hyperparameter setting macro"
S19-2095,W18-4401,0,0.0176478,"g Yunnan University, Yunnan, P.R. China hyding@ynu.edu.cn Abstract (Chen et al., 2012), most methods are based on surveillance methods (Schmidt and Wiegand, 2017). There are also some (racial discrimination) bias towards specific goals. In (Waseem and Hovy, 2016), the authors proposed a series of criteria based on critical race theory to identify racism and gender discrimination, they use n-gram models for research; Tulkens et al. studied racism detection in Dutch social media (Tulkens et al., 2016). A recent discussion of the challenge of identifying hate speech was proposed by Kumar et al. (Kumar et al., 2018). The results show that it is difficult to distinguish between open and covert attacks in social media. SemEval 2019 Task 5 is proposed to identify hate speech about immigrants and women in Twitter for English or Spanish, and classify hate speech and judge whether the target is an individual or a group (Basile et al., 2019). Hate speech is often defined as any communication that attacks an individual or group through certain characteristics (such as gender, nationality, religion, or other characteristics) in social media platforms. This task gives us some text data from Twitter, we need to cla"
S19-2095,W17-5205,0,0.0624626,"Missing"
S19-2095,W17-1101,0,0.0263823,"Missing"
S19-2095,W12-2103,0,0.0551638,"users often abuse this freedom to spread abuse or hateful posts or comments. In many cases, these usergenerated content is inherently offensive or proactive, and users may have to deal with threats such as cyber attacks or cyberbullying, as well as other undesirable phenomena (Whittaker and Kowalski, 2015). So the problem of detecting and possibly limiting the spread of hate speech is becoming more and more important. In order to solve the problem of abuse of language in social media platforms, some related research has been published, such as cyberbullying (Dadvar et al., 2013), hate speech (Warner and Hirschberg, 2012) and abusive language ∗ Corresponding author 529 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 529–534 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics Hierarchical Attention Network to make it more suitable for this task, the detailed description of the Attention-LSTM model is provided in Section 2.2. Next, we build a BiGRU-Capsule model using the latest “Capsule” model proposed by (Sabour et al., 2017), the detailed description is provided in Section 2.3. In Section 2.4, we describe the use of stacking"
S19-2095,N16-2013,0,0.0379887,"Missing"
S19-2095,I17-1102,0,0.029356,"alanced, since the ratio of 0 and 1 in the label HS is close to balance, we have not dealt with the data imbalance in this task. 2.2 ui = tanh(Ws ∗ hi + bs ) (1) Then, we compute the similarity between ui and word-level context vector us , and obtain a normalized weight αi of importance by softmax function. exp(uTi ∗ us ) αi = P T i exp(ui ∗ us ) (2) Finally, we compute the sentence vector s by a weighted sum of the word annotations hi based on the normalized importance weights. s summarizes all the information of words in a context. Attention-LSTM Model Here we have made some changes to HAN (Yang et al., 2017). The overall structure is shown in Figure 1. The replacement of BiGRU with LSTM (Hochreiter and Schmidhuber, 1997) is found to be significantly better than the original model for this task. The architecture of Attention-LSTM model is shown in Figure 1. We use an LSTM to encode the sentences and to get annotations of words by summarizing information for word. Not all words contribute equally to the expression of the emotion in the sentence. Emotion s= X αi ∗ hi (3) i 2.3 BiGRU-Capsule model In order to improve the performance, in this system we use BiGRU and the latest capsule model (Sabour et"
S19-2143,N12-1084,0,0.102254,"supervised machine learning methods to obtain tagged data from different Twitter accounts in an inexpensive way, and to learn the binary classifiers of the “racist” and “nonracist” tags (Kwok and Wang, 2013). Gamb¨ack et al. built a convolutional neural network model based on word2vec embedding(Gamb¨ack and Sikdar, 2017). And a new method for deep neural networks based on convolution and gated recursive networks was proposed by Zhang et al. (Zhang et al., 2018). In the field of research on hate speech, originally Xu et al. introduced the social study of bullying and formulated it as NLP tasks(Xu et al., 2012), then Ross et al. suggested that the existence of hate speech should not be considered as a binary yes or no decision, and the evaluator needs a more detailed commentary(Ross et al., 2016), and now ElSherief et al. believed that it is necessary to further deepen the understanding of online hate language to determine whether the target is individual or group (ElSherief et al., 2018). 3 • Convolutional layer: In this layer, the obtained word vectors are subjected to convolution operations to obtain multiple feature maps. The specific operation is: a sentence contains L words, each of which has"
S19-2143,N18-2031,0,0.0200118,"its dimension is 300. Glove is provided by Jeffrey Pennington et al. (Pennington et al., 2014), it is a 2.2 million word vector trained using subword information on Common Crawl with 840B tokens, and its dimension is also 300. We use a mathematical mean of the word vectors by fastText and Glove to produce a high performance word vector. Since the principles between fastText and Glove are different, the word vector representation of the same word is also slightly different, and the average embedding set retains semantic information through preservation of the relative distances between words (Coates and Bollegala, 2018). 3.4 Experiment and results • The “#” symbol is removed and the word itself is retained for hashtags. • All of “@user” is replaced with username. Username mentions, i.e. words starting with “@”, generally provide no information in terms of sentiment. Hence such terms are removed completely from the tweet. • Repeated full stops, question marks and exclamation marks are replaced with a single instance with a special token ”repeat” added. • All contractions are split into two tokens(e.g.: “it’s” is changed to “it” and “is”). • Emoticons (for example, ‘:(’, ‘:)’, ‘:P’ and emoji etc) are replaced"
S19-2143,N19-1144,0,0.118614,"rmance. Our model achieves a Macro F1-score of 0.802 (ranked 9/103) in the Sub-task A. 1 Introduction In the past ten years, with the popularity of the Internet, social media platforms such as facebook and twitter have gradually become important tools for people’s daily communication, and users can publish their own content on these platforms. As the number of people interacting on social media platforms increases, online aggression language behavior also grows, and now it has become a major source of social conflict. Semeval 2019 Task 6 is proposed for identifying online offensive languages (Zampieri et al., 2019b). Its goal is to use computational methods to identify offense, aggression and hate speech in user-generated content on online social media platforms. We can prevent abuse of offensive language by using this approach in social media platforms. This task gives us some data from the social media platform, and classifies the content through computational analysis. In this competition, we only participate in Subtask A: identification of offensive language. For this task, we use a deep learning method to build a K-max pooling convolutional neural network model which uses convolutional neural netw"
S19-2143,S19-2010,0,0.13506,"rmance. Our model achieves a Macro F1-score of 0.802 (ranked 9/103) in the Sub-task A. 1 Introduction In the past ten years, with the popularity of the Internet, social media platforms such as facebook and twitter have gradually become important tools for people’s daily communication, and users can publish their own content on these platforms. As the number of people interacting on social media platforms increases, online aggression language behavior also grows, and now it has become a major source of social conflict. Semeval 2019 Task 6 is proposed for identifying online offensive languages (Zampieri et al., 2019b). Its goal is to use computational methods to identify offense, aggression and hate speech in user-generated content on online social media platforms. We can prevent abuse of offensive language by using this approach in social media platforms. This task gives us some data from the social media platform, and classifies the content through computational analysis. In this competition, we only participate in Subtask A: identification of offensive language. For this task, we use a deep learning method to build a K-max pooling convolutional neural network model which uses convolutional neural netw"
S19-2143,W17-3013,0,0.109172,"Missing"
S19-2143,D14-1181,0,0.00314883,"ese Top K eigenvalues is preserved, it should be said that it retains part of the position information. However, this location information is only the relative order between features, not absolute location information. For example: “I think the scenery in this place is not bad, but there are too many people.” Although the first half reflects the positive emotions, the global text expresses the negative emotions, and K-max pooling can capture such information. K-max pooling CNN model Our network architecture is shown in Figure 1. It is a variant of the CNN model structure proposed by Yoon Kim (Kim, 2014). Next we explain the details of our system. • Input layer: This layer mainly inputs all the preprocessed text data into the model. 819 Figure 1: The architecture of K-max pooling CNN model • Fully connected layer: In this model architecture, there are two layers of fully connected layers. The first layer receives the feature vectors obtained by the pooling layer, and the last layer is used for classification and prediction. 3.3 loop that scales the loop magnitude by a factor while keeping the initial learning rate constant. 4 4.1 Data preprocessing Text from tweets are inherently noisy. Tweet"
S19-2143,L18-1008,0,0.0286811,"receives the feature vectors obtained by the pooling layer, and the last layer is used for classification and prediction. 3.3 loop that scales the loop magnitude by a factor while keeping the initial learning rate constant. 4 4.1 Data preprocessing Text from tweets are inherently noisy. Tweets are processed using tweettokenize tool. Cleaning the text before further processing helps to generate better features and semantics. We perform the following preprocessing steps. Word embedding We use two different pre-trained word embeddings, fastText and Glove. FastText is provided by Mikolov et al. (Mikolov et al., 2018), it is a 2 million word vector trained using subword information on Common Crawl with 600B tokens, and its dimension is 300. Glove is provided by Jeffrey Pennington et al. (Pennington et al., 2014), it is a 2.2 million word vector trained using subword information on Common Crawl with 840B tokens, and its dimension is also 300. We use a mathematical mean of the word vectors by fastText and Glove to produce a high performance word vector. Since the principles between fastText and Glove are different, the word vector representation of the same word is also slightly different, and the average em"
S19-2143,D14-1162,0,0.0861105,"itial learning rate constant. 4 4.1 Data preprocessing Text from tweets are inherently noisy. Tweets are processed using tweettokenize tool. Cleaning the text before further processing helps to generate better features and semantics. We perform the following preprocessing steps. Word embedding We use two different pre-trained word embeddings, fastText and Glove. FastText is provided by Mikolov et al. (Mikolov et al., 2018), it is a 2 million word vector trained using subword information on Common Crawl with 600B tokens, and its dimension is 300. Glove is provided by Jeffrey Pennington et al. (Pennington et al., 2014), it is a 2.2 million word vector trained using subword information on Common Crawl with 840B tokens, and its dimension is also 300. We use a mathematical mean of the word vectors by fastText and Glove to produce a high performance word vector. Since the principles between fastText and Glove are different, the word vector representation of the same word is also slightly different, and the average embedding set retains semantic information through preservation of the relative distances between words (Coates and Bollegala, 2018). 3.4 Experiment and results • The “#” symbol is removed and the wor"
U19-1024,P18-1068,0,0.153232,"rse2fine model with a control mechanism, with which our method can control the influence of the sketch on the final results in the fine stage. Even if the sketch is wrong, our model still has the opportunity to get a correct result. We have experimented our model on the tasks of semantic parsing and math word problem solving. The results have shown the effectiveness of our proposed model. 1 Introduction The coarse-to-fine (coarse2fine) methods have been applied in many generation tasks such as machine translation (Xia et al., 2017) , abstract writing (Wang et al., 2018b) and semantic parsing (Dong and Lapata, 2018). They have shown excellent performances but still have many disadvantages. Traditional coarse2fine models usually tackle one task in two stages. In the first stage (coarse stage), a low-level seq2seq model is used to generate a rough sketch, which makes the data more compact and alleviates the problem of data sparsity. Some examples of sketches are shown in Table 1. Besides, Sketches in this stage are also easier to generate. Then, in the fine stage, both text and previous sketches will be input to another high-level seq2seq model to predict the final result so that the high-level model can p"
U19-1024,C18-1018,0,0.021539,"017). Some cases of math word problems, math equations, templates and sketches are shown in Table 2 These methods are intuitive, but it is difficult to obtain high-quality templates due to data sparsity and transfer them to other datasets. The second category of methods mainly exploits the seq2seq framework to generate the solution equations (Wang et al., 2018a). Recently this kind of methods have shown outstanding performance without manual feature engineering, but they are prone to generate wrong numbers due to its generation flexibility. Some researches have applied reinforcement learning (Huang et al., 2018) or a stack (Chiang and Chen, 2018) to improve the decoding process. 2.3 Coarse-to-fine method Generalized coarse-to-fine method divides problems into different stages and solves them from coarse to fine. This method is widely applied in computer vision (Gangaputra and Geman, 2006; Pedersoli et al., 2011; Wen et al., 2019) and natural language process (Mei et al., 2016; Choi et al., 2017). The special coarse-to-fine method in this paper is based on end-to-end framework. It has two seq2seq models, generating target data from a coarse stage to a fine stage. Xia et al. (2017) proposed polish mech"
U19-1024,D17-1084,0,0.0302057,"rees (ASTs) (Rabinovich et al., 2017). Its decoder uses a dynamically-determined modular structure paralleling the structure of the output tree. 2.2 Math Word Problem Math word problem (MWP) aims to teach computers to read the questions in natural language and generate the corresponding math equations. The methods of solving math word problems can be mainly classified into two categories. The first category is the template-based models which summarize some templates through locating similar questions from a given dataset and then fill the concrete numbers into the templates to solve problems (Huang et al., 2017; Wang et al., 2017). Some cases of math word problems, math equations, templates and sketches are shown in Table 2 These methods are intuitive, but it is difficult to obtain high-quality templates due to data sparsity and transfer them to other datasets. The second category of methods mainly exploits the seq2seq framework to generate the solution equations (Wang et al., 2018a). Recently this kind of methods have shown outstanding performance without manual feature engineering, but they are prone to generate wrong numbers due to its generation flexibility. Some researches have applied reinforc"
U19-1024,P13-2009,0,0.0149845,"de], substitute the result for decode. x = 150 + 2 − 50 x = hnumi + hnumi − hnumi There are 150 science books, and the storybooks are 50 books less than the science books. How many books are there in the storybooks? Table 1: Examples of text, sketches and generating goals in different datasets. method we applied. 2.1 Semantic Parsing Semantic parsing is a task of translating natural language into computer executable language such as logic form, code in computer language and SQL query. Traditional semantic parsing usually adopts rule based method Tang and Mooney (2000); Wong and Mooney (2007); Andreas et al. (2013). Recently, with the development of neural network techniques, there are many new semantic parsing models with neural methods. Of them, Seq2seq models have been widely applied in semantic parsing tasks. The encoder encodes the text and the decoder predicts the logic symbols (Dong and Lapata, 2016). The seq2tree model encodes inputs by LSTM and generates the logic form by conditioning the output sequences or trees on the encoding vectors.(Dong and Lapata, 2016). Abstract syntax networks (ASN) represent the output as the abstract syntax trees (ASTs) (Rabinovich et al., 2017). Its decoder uses a"
U19-1024,N16-1086,0,0.0342508,"this kind of methods have shown outstanding performance without manual feature engineering, but they are prone to generate wrong numbers due to its generation flexibility. Some researches have applied reinforcement learning (Huang et al., 2018) or a stack (Chiang and Chen, 2018) to improve the decoding process. 2.3 Coarse-to-fine method Generalized coarse-to-fine method divides problems into different stages and solves them from coarse to fine. This method is widely applied in computer vision (Gangaputra and Geman, 2006; Pedersoli et al., 2011; Wen et al., 2019) and natural language process (Mei et al., 2016; Choi et al., 2017). The special coarse-to-fine method in this paper is based on end-to-end framework. It has two seq2seq models, generating target data from a coarse stage to a fine stage. Xia et al. (2017) proposed polish mechanism with two levels of decoders. The first decoder generates a raw sequence and the second decoder polishes and refines the raw sentence with deliberation. Their model performs excellently on machine translation and text summarization, which is also the first time to use this kind of coarse-to-fine model. Wang et al. (2018b) used a similar framework to write paper ab"
U19-1024,N19-1272,0,0.0324643,"Missing"
U19-1024,P17-1020,0,0.030628,"ods have shown outstanding performance without manual feature engineering, but they are prone to generate wrong numbers due to its generation flexibility. Some researches have applied reinforcement learning (Huang et al., 2018) or a stack (Chiang and Chen, 2018) to improve the decoding process. 2.3 Coarse-to-fine method Generalized coarse-to-fine method divides problems into different stages and solves them from coarse to fine. This method is widely applied in computer vision (Gangaputra and Geman, 2006; Pedersoli et al., 2011; Wen et al., 2019) and natural language process (Mei et al., 2016; Choi et al., 2017). The special coarse-to-fine method in this paper is based on end-to-end framework. It has two seq2seq models, generating target data from a coarse stage to a fine stage. Xia et al. (2017) proposed polish mechanism with two levels of decoders. The first decoder generates a raw sequence and the second decoder polishes and refines the raw sentence with deliberation. Their model performs excellently on machine translation and text summarization, which is also the first time to use this kind of coarse-to-fine model. Wang et al. (2018b) used a similar framework to write paper abstracts. In the fiel"
U19-1024,P17-1105,0,0.0327583,"Wong and Mooney (2007); Andreas et al. (2013). Recently, with the development of neural network techniques, there are many new semantic parsing models with neural methods. Of them, Seq2seq models have been widely applied in semantic parsing tasks. The encoder encodes the text and the decoder predicts the logic symbols (Dong and Lapata, 2016). The seq2tree model encodes inputs by LSTM and generates the logic form by conditioning the output sequences or trees on the encoding vectors.(Dong and Lapata, 2016). Abstract syntax networks (ASN) represent the output as the abstract syntax trees (ASTs) (Rabinovich et al., 2017). Its decoder uses a dynamically-determined modular structure paralleling the structure of the output tree. 2.2 Math Word Problem Math word problem (MWP) aims to teach computers to read the questions in natural language and generate the corresponding math equations. The methods of solving math word problems can be mainly classified into two categories. The first category is the template-based models which summarize some templates through locating similar questions from a given dataset and then fill the concrete numbers into the templates to solve problems (Huang et al., 2017; Wang et al., 2017"
U19-1024,W00-1317,0,0.11608,"method and method set to bytes.decode[bytes.decode], substitute the result for decode. x = 150 + 2 − 50 x = hnumi + hnumi − hnumi There are 150 science books, and the storybooks are 50 books less than the science books. How many books are there in the storybooks? Table 1: Examples of text, sketches and generating goals in different datasets. method we applied. 2.1 Semantic Parsing Semantic parsing is a task of translating natural language into computer executable language such as logic form, code in computer language and SQL query. Traditional semantic parsing usually adopts rule based method Tang and Mooney (2000); Wong and Mooney (2007); Andreas et al. (2013). Recently, with the development of neural network techniques, there are many new semantic parsing models with neural methods. Of them, Seq2seq models have been widely applied in semantic parsing tasks. The encoder encodes the text and the decoder predicts the logic symbols (Dong and Lapata, 2016). The seq2tree model encodes inputs by LSTM and generates the logic form by conditioning the output sequences or trees on the encoding vectors.(Dong and Lapata, 2016). Abstract syntax networks (ASN) represent the output as the abstract syntax trees (ASTs)"
U19-1024,D18-1132,0,0.0462238,"in this paper, we propose an improved coarse2fine model with a control mechanism, with which our method can control the influence of the sketch on the final results in the fine stage. Even if the sketch is wrong, our model still has the opportunity to get a correct result. We have experimented our model on the tasks of semantic parsing and math word problem solving. The results have shown the effectiveness of our proposed model. 1 Introduction The coarse-to-fine (coarse2fine) methods have been applied in many generation tasks such as machine translation (Xia et al., 2017) , abstract writing (Wang et al., 2018b) and semantic parsing (Dong and Lapata, 2018). They have shown excellent performances but still have many disadvantages. Traditional coarse2fine models usually tackle one task in two stages. In the first stage (coarse stage), a low-level seq2seq model is used to generate a rough sketch, which makes the data more compact and alleviates the problem of data sparsity. Some examples of sketches are shown in Table 1. Besides, Sketches in this stage are also easier to generate. Then, in the fine stage, both text and previous sketches will be input to another high-level seq2seq model to predict the"
U19-1024,P18-2042,0,0.120966,"in this paper, we propose an improved coarse2fine model with a control mechanism, with which our method can control the influence of the sketch on the final results in the fine stage. Even if the sketch is wrong, our model still has the opportunity to get a correct result. We have experimented our model on the tasks of semantic parsing and math word problem solving. The results have shown the effectiveness of our proposed model. 1 Introduction The coarse-to-fine (coarse2fine) methods have been applied in many generation tasks such as machine translation (Xia et al., 2017) , abstract writing (Wang et al., 2018b) and semantic parsing (Dong and Lapata, 2018). They have shown excellent performances but still have many disadvantages. Traditional coarse2fine models usually tackle one task in two stages. In the first stage (coarse stage), a low-level seq2seq model is used to generate a rough sketch, which makes the data more compact and alleviates the problem of data sparsity. Some examples of sketches are shown in Table 1. Besides, Sketches in this stage are also easier to generate. Then, in the fine stage, both text and previous sketches will be input to another high-level seq2seq model to predict the"
U19-1024,D17-1088,0,0.0506169,"Missing"
U19-1024,P07-1121,0,0.0455695,"bytes.decode[bytes.decode], substitute the result for decode. x = 150 + 2 − 50 x = hnumi + hnumi − hnumi There are 150 science books, and the storybooks are 50 books less than the science books. How many books are there in the storybooks? Table 1: Examples of text, sketches and generating goals in different datasets. method we applied. 2.1 Semantic Parsing Semantic parsing is a task of translating natural language into computer executable language such as logic form, code in computer language and SQL query. Traditional semantic parsing usually adopts rule based method Tang and Mooney (2000); Wong and Mooney (2007); Andreas et al. (2013). Recently, with the development of neural network techniques, there are many new semantic parsing models with neural methods. Of them, Seq2seq models have been widely applied in semantic parsing tasks. The encoder encodes the text and the decoder predicts the logic symbols (Dong and Lapata, 2016). The seq2tree model encodes inputs by LSTM and generates the logic form by conditioning the output sequences or trees on the encoding vectors.(Dong and Lapata, 2016). Abstract syntax networks (ASN) represent the output as the abstract syntax trees (ASTs) (Rabinovich et al., 201"
U19-1024,P17-1041,0,0.0304312,"in23K. Examples of original data and sketches of these three datasets are shown in Table 1. Task Text2logic Text2code MWP Emb 150 200 128 Hidden 250 300 512 Epoch 50 150 150 LR 0.005 0.005 0.01 Table 4: Model parameters and training settings 4.2 Preprocess To compare the result equally, we made our preprocessing in accord with Dong and Lapata (2018)’s experiment as much as possible. For GEO, we followed Dong and Lapata’s work, transforming all words into lower type and replacing the entity mentions with a sign and a counting number. And for Django, we chosed to use the processed data given by Yin and Neubig (2017). They tokenized and POS tagged sentences using NLTK. In MWP, we followed Wang et al.’s work. To reduce the influence of OOV, we normalized numbers as the order of their appearance. Examples of some processed cases of Math23k are shown in Table 2, who have same sketches. 4.3 Results We has compared our improved coarse2fine model with different published models. The optimizer is Adam and many details of training and testing are shown in Table 3 and Table 4. To compare the result equally, we chose the same model parameters as Dong and Lapata (2018) in semantic parsing tasks. Accuracy in this pap"
