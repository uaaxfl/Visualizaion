2021.mtsummit-up.17,Field Experiments of Real Time Foreign News Distribution Powered by {MT},2021,-1,-1,3,0,4955,keiji yasuda,Proceedings of Machine Translation Summit XVIII: Users and Providers Track,0,"Field experiments on a foreign news distribution system using two key technologies are reported. The first technology is a summarization component, which is used for generating news headlines. This component is a transformer-based abstractive text summarization system which is trained to output headlines from the leading sentences of news articles. The second technology is machine translation (MT), which enables users to read foreign news articles in their mother language. Since the system uses MT, users can immediately access the latest foreign news. 139 Japanese LINE users participated in the field experiments for two weeks, viewing about 40,000 articles which had been translated from English to Japanese. We carried out surveys both during and after the experiments. According to the results, 79.3{\%} of users evaluated the headlines as adequate, while 74.7{\%} of users evaluated the automatically translated articles as intelligible. According to the post-experiment survey, 59.7{\%} of users wished to continue using the system; 11.5{\%} of users did not. We also report several statistics of the experiments."
2021.inlg-1.6,Predicting Antonyms in Context using {BERT},2021,-1,-1,3,0,5917,ayana niwa,Proceedings of the 14th International Conference on Natural Language Generation,0,"We address the task of antonym prediction in a context, which is a fill-in-the-blanks problem. This task setting is unique and practical because it requires contrastiveness to the other word and naturalness as a text in filling a blank. We propose methods for fine-tuning pre-trained masked language models (BERT) for context-aware antonym prediction. The experimental results demonstrate that these methods have positive impacts on the prediction of antonyms within a context. Moreover, human evaluation reveals that more than 85{\%} of predictions using the proposed method are acceptable as antonyms."
2021.findings-acl.21,Joint Optimization of Tokenization and Downstream Model,2021,-1,-1,5,1,7542,tatsuya hiraoka,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.335,Transformer-based Lexically Constrained Headline Generation,2021,-1,-1,5,0,8399,kosuke yamada,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"This paper explores a variant of automatic headline generation methods, where a generated headline is required to include a given phrase such as a company or a product name. Previous methods using Transformer-based models generate a headline including a given phrase by providing the encoder with additional information corresponding to the given phrase. However, these methods cannot always include the phrase in the generated headline. Inspired by previous RNN-based methods generating token sequences in backward and forward directions from the given phrase, we propose a simple Transformer-based method that guarantees to include the given phrase in the high-quality generated headline. We also consider a new headline generation strategy that takes advantage of the controllable generation order of Transformer. Our experiments with the Japanese News Corpus demonstrate that our methods, which are guaranteed to include the phrase in the generated headline, achieve ROUGE scores comparable to previous Transformer-based methods. We also show that our generation strategy performs better than previous strategies."
2020.semeval-1.51,{SWAG}ex at {S}em{E}val-2020 Task 4: Commonsense Explanation as Next Event Prediction,2020,-1,-1,2,0,15055,wiem rim,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"We describe the system submitted by the SWAGex team to the SemEval-2020 Commonsense Validation and Explanation Task. We use multiple methods on the pre-trained language model BERT (Devlin et al., 2018) for tasks that require the system to recognize sentences against commonsense and justify the reasoning behind this decision. Our best performing model is BERT trained on SWAG and fine-tuned for the task. We investigate the ability to transfer commonsense knowledge from SWAG to SemEval-2020 by training a model for the Explanation task with Next Event Prediction data"
2020.semeval-1.221,{T}ext{L}earner at {S}em{E}val-2020 Task 10: A Contextualized Ranking System in Solving Emphasis Selection in Text,2020,-1,-1,3,1,15312,zhishen yang,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This paper describes the emphasis selection system of the team TextLearner for SemEval 2020 Task 10: Emphasis Selection For Written Text in Visual Media. The system aims to learn the emphasis selection distribution using contextual representations extracted from pre-trained language models and a two-staged ranking model. The experimental results demonstrate the strong contextual representation power of the recent advanced transformer-based language model RoBERTa, which can be exploited using a simple but effective architecture on top."
2020.lrec-1.429,Jamo Pair Encoding: Subcharacter Representation-based Extreme {K}orean Vocabulary Compression for Efficient Subword Tokenization,2020,-1,-1,2,0,16049,sangwhan moon,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In the context of multilingual language model pre-training, vocabulary size for languages with a broad set of potential characters is an unsolved problem. We propose two algorithms applicable in any unsupervised multilingual pre-training task, increasing the elasticity of budget required for building the vocabulary in Byte-Pair Encoding inspired tokenizers, significantly reducing the cost of supporting Korean in a multilingual model."
2020.lrec-1.447,Evaluation Dataset for Zero Pronoun in {J}apanese to {E}nglish Translation,2020,-1,-1,4,0,17595,sho shimazu,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In natural language, we often omit some words that are easily understandable from the context. In particular, pronouns of subject, object, and possessive cases are often omitted in Japanese; these are known as zero pronouns. In translation from Japanese to other languages, we need to find a correct antecedent for each zero pronoun to generate a correct and coherent translation. However, it is difficult for conventional automatic evaluation metrics (e.g., BLEU) to focus on the success of zero pronoun resolution. Therefore, we present a hand-crafted dataset to evaluate whether translation models can resolve the zero pronoun problems in Japanese to English translations. We manually and statistically validate that our dataset can effectively evaluate the correctness of the antecedents selected in translations. Through the translation experiments using our dataset, we reveal shortcomings of an existing context-aware neural machine translation model."
2020.findings-emnlp.120,Optimizing Word Segmentation for Downstream Task,2020,-1,-1,5,1,7542,tatsuya hiraoka,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"In traditional NLP, we tokenize a given sentence as a preprocessing, and thus the tokenization is unrelated to a target downstream task. To address this issue, we propose a novel method to explore a tokenization which is appropriate for the downstream task. Our proposed method, optimizing tokenization (OpTok), is trained to assign a high probability to such appropriate tokenization based on the downstream task loss. OpTok can be used for any downstream task which uses a vector representation of a sentence such as text classification. Experimental results demonstrate that OpTok improves the performance of sentiment analysis and textual entailment. In addition, we introduce OpTok into BERT, the state-of-the-art contextualized embeddings and report a positive effect."
2020.emnlp-main.631,"{P}atch{BERT}: Just-in-Time, Out-of-Vocabulary Patching",2020,-1,-1,2,0,16049,sangwhan moon,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Large scale pre-trained language models have shown groundbreaking performance improvements for transfer learning in the domain of natural language processing. In our paper, we study a pre-trained multilingual BERT model and analyze the OOV rate on downstream tasks, how it introduces information loss, and as a side-effect, obstructs the potential of the underlying model. We then propose multiple approaches for mitigation and demonstrate that it improves performance with the same parameter count when combined with fine-tuning."
2020.coling-main.176,Image Caption Generation for News Articles,2020,-1,-1,2,1,15312,zhishen yang,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this paper, we address the task of news-image captioning, which generates a description of an image given the image and its article body as input. This task is more challenging than the conventional image captioning, because it requires a joint understanding of image and text. We present a Transformer model that integrates text and image modalities and attends to textual features from visual features in generating a caption. Experiments based on automatic evaluation metrics and human evaluation show that an article text provides primary information to reproduce news-image captions written by journalists. The results also demonstrate that the proposed model outperforms the state-of-the-art model. In addition, we also confirm that visual features contribute to improving the quality of news-image captions."
2020.acl-main.123,Improving Truthfulness of Headline Generation,2020,35,0,3,0,22650,kazuki matsumaru,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Most studies on abstractive summarization report ROUGE scores between system and reference summaries. However, we have a concern about the truthfulness of generated summaries: whether all facts of a generated summary are mentioned in the source text. This paper explores improving the truthfulness in headline generation on two popular datasets. Analyzing headlines generated by the state-of-the-art encoder-decoder model, we show that the model sometimes generates untruthful headlines. We conjecture that one of the reasons lies in untruthful supervision data used for training the model. In order to quantify the truthfulness of article-headline pairs, we consider the textual entailment of whether an article entails its headline. After confirming quite a few untruthful instances in the datasets, this study hypothesizes that removing untruthful instances from the supervision data may remedy the problem of the untruthful behaviors of the model. Building a binary classifier that predicts an entailment relation between an article and its headline, we filter out untruthful instances from the supervision data. Experimental results demonstrate that the headline generation model trained on filtered supervision data shows no clear difference in ROUGE scores but remarkable improvements in automatic and manual evaluations of the generated headlines."
2020.acl-main.147,Enhancing Machine Translation with Dependency-Aware Self-Attention,2020,-1,-1,2,0,10181,emanuele bugliarello,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism. In this work, we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios. We show the efficacy of each approach on WMT English-German and English-Turkish, and WAT English-Japanese translation tasks."
2020.acl-main.149,It{'}s Easier to Translate out of {E}nglish than into it: {M}easuring Neural Translation Difficulty by Cross-Mutual Information,2020,29,0,5,0,10181,emanuele bugliarello,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"The performance of neural machine translation systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model. In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models. XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task. We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems. Code for replicating our experiments is available online at https://github.com/e-bug/nmt-difficulty."
2020.aacl-main.89,You May Like This Hotel Because ...: Identifying Evidence for Explainable Recommendations,2020,-1,-1,5,0.833333,23269,shin kanouchi,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"Explainable recommendation is a good way to improve user satisfaction. However, explainable recommendation in dialogue is challenging since it has to handle natural language as both input and output. To tackle the challenge, this paper proposes a novel and practical task to explain evidences in recommending hotels given vague requests expressed freely in natural language. We decompose the process into two subtasks on hotel reviews: Evidence Identification and Evidence Explanation. The former predicts whether or not a sentence contains evidence that expresses why a given request is satisfied. The latter generates a recommendation sentence given a request and an evidence sentence. In order to address these subtasks, we build an Evidence-based Explanation dataset, which is the largest dataset for explaining evidences in recommending hotels for vague requests. The experimental results demonstrate that the BERT model can find evidence sentences with respect to various vague requests and that the LSTM-based model can generate recommendation sentences."
W19-8613,Neural Question Generation using Interrogative Phrases,2019,0,0,3,0,23321,yuichi sasazawa,Proceedings of the 12th International Conference on Natural Language Generation,0,"Question Generation (QG) is the task of generating questions from a given passage. One of the key requirements of QG is to generate a question such that it results in a target answer. Previous works used a target answer to obtain a desired question. However, we also want to specify how to ask questions and improve the quality of generated questions. In this study, we explore the use of interrogative phrases as additional sources to control QG. By providing interrogative phrases, we expect that QG can generate a more reliable sequence of words subsequent to an interrogative phrase. We present a baseline sequence-to-sequence model with the attention, copy, and coverage mechanisms, and show that the simple baseline achieves state-of-the-art performance. The experiments demonstrate that interrogative phrases contribute to improving the performance of QG. In addition, we report the superiority of using interrogative phrases in human evaluation. Finally, we show that a question answering system can provide target answers more correctly when the questions are generated with interrogative phrases."
W19-8641,A Large-Scale Multi-Length Headline Corpus for Analyzing Length-Constrained Headline Generation Model Evaluation,2019,0,0,6,1,9399,yuta hitomi,Proceedings of the 12th International Conference on Natural Language Generation,0,"Browsing news articles on multiple devices is now possible. The lengths of news article headlines have precise upper bounds, dictated by the size of the display of the relevant device or interface. Therefore, controlling the length of headlines is essential when applying the task of headline generation to news production. However, because there is no corpus of headlines of multiple lengths for a given article, previous research on controlling output length in headline generation has not discussed whether the system outputs could be adequately evaluated without multiple references of different lengths. In this paper, we introduce two corpora, which are Japanese News Corpus (JNC) and JApanese MUlti-Length Headline Corpus (JAMUL), to confirm the validity of previous evaluation settings. The JNC provides common supervision data for headline generation. The JAMUL is a large-scale evaluation dataset for headlines of three different lengths composed by professional editors. We report new findings on these corpora; for example, although the longest length reference summary can appropriately evaluate the existing methods controlling output length, this evaluation setting has several problems."
S19-2061,{T}okyo{T}ech{\\_}{NLP} at {S}em{E}val-2019 Task 3: Emotion-related Symbols in Emotion Detection,2019,0,0,3,1,15312,zhishen yang,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"This paper presents our contextual emotion detection system in approaching the SemEval2019 shared task 3: EmoContext: Contextual Emotion Detection in Text. This system cooperates with an emotion detection neural network method (Poria et al., 2017), emoji2vec (Eisner et al., 2016) embedding, word2vec embedding (Mikolov et al., 2013), and our proposed emoticon and emoji preprocessing method. The experimental results demonstrate the usefulness of our emoticon and emoji prepossessing method, and representations of emoticons and emoji contribute model{'}s emotion detection."
P19-1202,"Learning to Select, Track, and Generate for Data-to-Text",2019,0,1,8,0,7220,hayate iso,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We propose a data-to-text generation model with two modules, one for tracking and the other for text generation. Our tracking module selects and keeps track of salient information and memorizes which record has been mentioned. Our generation module generates a summary conditioned on the state of tracking module. Our proposed model is considered to simulate the human-like writing process that gradually selects the information by determining the intermediate variables while writing the summary. In addition, we also explore the effectiveness of the writer information for generations. Experimental results show that our proposed model outperforms existing models in all evaluation metrics even without writer information. Incorporating writer information further improves the performance, contributing to content planning and surface realization."
N19-1401,Positional Encoding to Control Output Sequence Length,2019,0,8,2,1,4614,sho takase,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Neural encoder-decoder models have been successful in natural language generation tasks. However, real applications of abstractive summarization must consider an additional constraint that a generated summary should not exceed a desired length. In this paper, we propose a simple but effective extension of a sinusoidal positional encoding (Vaswani et al., 2017) so that a neural encoder-decoder model preserves the length constraint. Unlike previous studies that learn length embeddings, the proposed method can generate a text of any length even if the target length is unseen in training data. The experimental results show that the proposed method is able not only to control generation length but also improve ROUGE scores."
Y18-1001,Multi-dialect Neural Machine Translation and Dialectometry,2018,0,0,3,0,295,kaori abe,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
Y18-1034,Reducing Odd Generation from Neural Headline Generation,2018,0,1,4,0,4615,shun kiyono,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
W18-5607,Investigating the Challenges of Temporal Relation Extraction from Clinical Text,2018,0,3,2,0,27896,diana galvan,Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis,0,"Temporal reasoning remains as an unsolved task for Natural Language Processing (NLP), particularly demonstrated in the clinical domain. The complexity of temporal representation in language is evident as results of the 2016 Clinical TempEval challenge indicate: the current state-of-the-art systems perform well in solving mention-identification tasks of event and time expressions but poorly in temporal relation extraction, showing a gap of around 0.25 point below human performance. We explore to adapt the tree-based LSTM-RNN model proposed by Miwa and Bansal (2016) to temporal relation extraction from clinical text, obtaining a five point improvement over the best 2016 Clinical TempEval system and two points over the state-of-the-art. We deliver a deep analysis of the results and discuss the next step towards human-like temporal reasoning."
W18-5410,Unsupervised Token-wise Alignment to Improve Interpretation of Encoder-Decoder Models,2018,0,1,4,0,4615,shun kiyono,Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP},0,"Developing a method for understanding the inner workings of black-box neural methods is an important research endeavor. Conventionally, many studies have used an attention matrix to interpret how Encoder-Decoder-based models translate a given source sentence to the corresponding target sentence. However, recent studies have empirically revealed that an attention matrix is not optimal for token-wise translation analyses. We propose a method that explicitly models the token-wise alignment between the source and target sequences to provide a better analysis. Experiments show that our method can acquire token-wise alignments that are superior to those of an attention mechanism."
L18-1477,Incorporating Semantic Attention in Video Description Generation,2018,0,0,2,1,30047,natsuda laokulrat,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-1286,Predicting Stances from Social Media Posts using Factorization Machines,2018,0,1,3,1,30916,akira sasaki,Proceedings of the 27th International Conference on Computational Linguistics,0,"Social media provide platforms to express, discuss, and shape opinions about events and issues in the real world. An important step to analyze the discussions on social media and to assist in healthy decision-making is stance detection. This paper presents an approach to detect the stance of a user toward a topic based on their stances toward other topics and the social media posts of the user. We apply factorization machines, a widely used method in item recommendation, to model user preferences toward topics from the social media data. The experimental results demonstrate that users{'} posts are useful to model topic preferences and therefore predict stances of silent users."
Y17-1045,A Crowdsourcing Approach for Annotating Causal Relation Instances in {W}ikipedia,2017,20,0,3,1,5977,kazuaki hanawa,"Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation",0,None
W17-6937,Handling Multiword Expressions in Causality Estimation,2017,6,7,4,0,22505,shota sasaki,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,None
W17-4208,Analyzing the Revision Logs of a {J}apanese Newspaper for Article Quality Assessment,2017,2,0,3,1,9400,hideaki tamori,Proceedings of the 2017 {EMNLP} Workshop: Natural Language Processing meets Journalism,0,We address the issue of the quality of journalism and analyze daily article revision logs from a Japanese newspaper company. The revision logs contain data that can help reveal the requirements of quality journalism such as the types and number of edit operations and aspects commonly focused in revision. This study also discusses potential applications such as quality assessment and automatic article revision as our future research directions.
P17-1037,Other Topics You May Also Agree or Disagree: Modeling Inter-Topic Preferences using Tweets and Matrix Factorization,2017,31,0,3,1,30916,akira sasaki,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We presents in this paper our approach for modeling inter-topic preferences of Twitter users: for example, {``}those who agree with the Trans-Pacific Partnership (TPP) also agree with free trade{''}. This kind of knowledge is useful not only for stance detection across multiple topics but also for various real-world applications including public opinion survey, electoral prediction, electoral campaigns, and online debates. In order to extract users{'} preferences on Twitter, we design linguistic patterns in which people agree and disagree about specific topics (e.g., {``}A is completely wrong{''}). By applying these linguistic patterns to a collection of tweets, we extract statements agreeing and disagreeing with various topics. Inspired by previous work on item recommendation, we formalize the task of modeling inter-topic preferences as matrix factorization: representing users{'} preference as a user-topic matrix and mapping both users and topics onto a latent feature space that abstracts the preferences. Our experimental results demonstrate both that our presented approach is useful in predicting missing preferences of users and that the latent vector representations of topics successfully encode inter-topic preferences."
I17-2074,Proofread Sentence Generation as Multi-Task Learning with Editing Operation Prediction,2017,15,1,3,1,9399,yuta hitomi,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"This paper explores the idea of robot editors, automated proofreaders that enable journalists to improve the quality of their articles. We propose a novel neural model of multi-task learning that both generates proofread sentences and predicts the editing operations required to rewrite the source sentences and create the proofread ones. The model is trained using logs of the revisions made professional editors revising draft newspaper articles written by journalists. Experiments demonstrate the effectiveness of our multi-task learning approach and the potential value of using revision logs for this task."
I17-1048,A Neural Language Model for Dynamically Representing the Meanings of Unknown Words and Entities in a Discourse,2017,34,3,2,1,9187,sosuke kobayashi,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"This study addresses the problem of identifying the meaning of unknown words or entities in a discourse with respect to the word embedding approaches used in neural language models. We proposed a method for on-the-fly construction and exploitation of word embeddings in both the input and output layers of a neural model by tracking contexts. This extends the dynamic entity representation used in Kobayashi et al. (2016) and incorporates a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al. (2016). In addition, we construct a new task and dataset called Anonymized Language Modeling for evaluating the ability to capture word meanings while reading. Experiments conducted using our novel dataset show that the proposed variant of RNN language model outperformed the baseline model. Furthermore, the experiments also demonstrate that dynamic updates of an output layer help a model predict reappearing entities, whereas those of an input layer are effective to predict words following reappearing entities."
Y16-3027,Neural Joint Learning for Classifying {W}ikipedia Articles into Fine-grained Named Entity Types,2016,20,3,4,0,33283,masatoshi suzuki,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Posters",0,None
Y16-2022,Recognizing Open-Vocabulary Relations between Objects in Images,2016,23,0,5,1,17721,masayasu muraoka,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers",0,None
Y16-2026,Toward the automatic extraction of knowledge of usable goods,2016,-1,-1,3,0,33301,mei uemura,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers",0,None
S16-1065,Tohoku at {S}em{E}val-2016 Task 6: Feature-based Model versus Convolutional Neural Network for Stance Detection,2016,3,8,4,0,34254,yuki igarashi,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
P16-3021,Building a Corpus for {J}apanese Wikification with Fine-Grained Entity Classes,2016,13,3,2,0,34395,davaajav jargalsaikhan,Proceedings of the {ACL} 2016 Student Research Workshop,0,"In this research, we build a Wikification corpus for advancing Japanese Entity Linking. This corpus consists of 340 Japanese newspaper articles with 25,675 entity mentions. All entity mentions are labeled by a fine-grained semantic classes (200 classes), and 19,121 mentions were successfully linked to Japanese Wikipedia articles. Even with the fine-grained semantic classes, we found it hard to define the target of entity linking annotations and to utilize the fine-grained semantic classes to improve the accuracy of entity linking."
P16-1121,Learning Semantically and Additively Compositional Distributional Representations,2016,52,9,2,1,20138,ran tian,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper connects a vector-based composition model to a formal semantics, the Dependency-based Compositional Semantics (DCS). We show theoretical evidence that the vector compositions in our model conform to the logic of DCS. Experimentally, we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors, achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification; meanwhile, DCS can guide building vectors for structured queries that can be directly executed. We evaluate this utility on sentence completion task and report a new state-of-the-art."
P16-1215,Composing Distributed Representations of Relational Patterns,2016,28,6,2,1,4614,sho takase,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
N16-1099,Dynamic Entity Representation with Max-pooling Improves Machine Reading,2016,19,27,3,1,9187,sosuke kobayashi,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We propose a novel neural network model for machine reading, DER Network, which explicitly implements a reader building dynamic meaning representations for entities by gathering and accumulating information around the entities as it reads a document. Evaluated on a recent large scale dataset (Hermann et al., 2015), our model exhibits better results than previous research, and we find that max-pooling is suited for modeling the accumulation of information on entities. Further analysis suggests that our model can put together multiple pieces of information encoded in different sentences to answer complicated questions. Our code for the model is available at https://github. com/soskek/der-network"
D16-1112,Neural Headline Generation on {A}bstract {M}eaning {R}epresentation,2016,13,60,3,1,4614,sho takase,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1005,Generating Video Description using Sequence-to-sequence Model with Temporal Attention,2016,14,3,6,1,30047,natsuda laokulrat,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Automatic video description generation has recently been getting attention after rapid advancement in image caption generation. Automatically generating description for a video is more challenging than for an image due to its temporal dynamics of frames. Most of the work relied on Recurrent Neural Network (RNN) and recently attentional mechanisms have also been applied to make the model learn to focus on some frames of the video while generating each word in a describing sentence. In this paper, we focus on a sequence-to-sequence approach with temporal attention mechanism. We analyze and compare the results from different attention model configuration. By applying the temporal attention mechanism to the system, we can achieve a METEOR score of 0.310 on Microsoft Video Description dataset, which outperformed the state-of-the-art system so far."
C16-1184,Modeling Discourse Segments in Lyrics Using Repeated Patterns,2016,13,1,4,0,16335,kento watanabe,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"This study proposes a computational model of the discourse segments in lyrics to understand and to model the structure of lyrics. To test our hypothesis that discourse segmentations in lyrics strongly correlate with repeated patterns, we conduct the first large-scale corpus study on discourse segments in lyrics. Next, we propose the task to automatically identify segment boundaries in lyrics and train a logistic regression model for the task with the repeated pattern and textual features. The results of our empirical experiments illustrate the significance of capturing repeated patterns in predicting the boundaries of discourse segments in lyrics."
C16-1266,Modeling Context-sensitive Selectional Preference with Distributed Representations,2016,25,3,4,1,7295,naoya inoue,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"This paper proposes a novel problem setting of selectional preference (SP) between a predicate and its arguments, called as context-sensitive SP (CSP). CSP models the narrative consistency between the predicate and preceding contexts of its arguments, in addition to the conventional SP based on semantic types. Furthermore, we present a novel CSP model that extends the neural SP model (Van de Cruys, 2014) to incorporate contextual information into the distributed representations of arguments. Experimental results demonstrate that the proposed CSP model successfully learns CSP and outperforms the conventional SP model in coreference cluster ranking."
Y15-1012,Fast and Large-scale Unsupervised Relation Extraction,2015,29,3,2,1,4614,sho takase,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"A common approach to unsupervised relation extraction builds clusters of patterns expressing the same relation. In order to obtain clusters of relational patterns of good quality, we have two major challenges: the semantic representation of relational patterns and the scalability to large data. In this paper, we explore various methods for modeling the meaning of a pattern and for computing the similarity of patterns mined from huge data. In order to achieve this goal, we apply algorithms for approximate frequency counting and efficient dimension reduction to unsupervised relation extraction. The experimental results show that approximate frequency counting and dimension reduction not only speeds up similarity computation but also improves the quality of pattern vectors."
Y15-1013,Reducing Lexical Features in Parsing by Word Embeddings,2015,26,3,3,0,34255,hiroya komatsu,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"The high-dimensionality of lexical features in parsing can be memory consuming and cause over-fitting problems. We propose a general framework to replace all lexical feature templates by low-dimensional features induced from word embeddings. Applied to a near state-of-the-art dependency parser (Huang et al., 2012), our method improves the baseline, performs better than using cluster bit string features, and outperforms a recent neural network based parser. A further analysis shows that our framework has the effect hypothesized by Andreas and Klein (2014), namely (i) connecting unseen words to known ones, and (ii) encouraging common behaviors among invocabulary words."
W15-1609,Annotating Geographical Entities on Microblog Text,2015,16,6,3,1,18558,koji matsuda,Proceedings of The 9th Linguistic Annotation Workshop,0,"This paper presents a discussion of the problems surrounding the task of annotating geographical entities on microblogs and reports the preliminary results of our efforts to annotate Japanese microblog texts. Unlike prior work, we not only annotate geographical location entities but also facility entities, such as stations, restaurants, shopping stores, hospitals and schools. We discuss ways in which to build a gazetteer, the types of ambiguities that need to be considered, reasons why the annotator tends to disagree, and the problems that need to be solved to automate the task of annotating the geographical entities. All the annotation data and the annotation guidelines are publicly available for research purposes from our web site."
W15-0507,A Computational Approach for Generating Toulmin Model Argumentation,2015,20,6,3,1,24185,paul reisert,Proceedings of the 2nd Workshop on Argumentation Mining,0,"Automatic generation of arguments is an important task that can be useful for many applications. For instance, the ability to generate coherent arguments during a debate can be useful when determining strengths of supporting evidence. However, with limited technologies that automatically generate arguments, the development of computational models for debates, as well as other areas, is becoming increasingly important. For this task, we focused on a promising argumentation model: the Toulmin model. The Toulmin model is both well-structured and general, and has been shown to be useful for policy debates. In this preliminary work we attempted to generate, with a given topic motion keyword or phrase, Toulmin model arguments by developing a computational model for detecting arguments spanned across multiple documents. This paper discusses our subjective results, observations, and future work."
P15-3005,Disease Event Detection based on Deep Modality Analysis,2015,19,1,4,0,27505,yoshiaki kitagawa,Proceedings of the {ACL}-{IJCNLP} 2015 Student Research Workshop,0,"Social media has attracted attention because of its potential for extraction of information of various types. For example, information collected from Twitter enables us to build useful applications such as predicting an epidemic of influenza. However, using text information from social media poses challenges for event detection because of the unreliable nature of user-generated texts, which often include counter-factual statements. Consequently, this study proposes the use of modality features to improve disease event detection from Twitter messages, or xe2x80x9ctweetsxe2x80x9d. Experimental results demonstrate that the combination of a modality dictionary and a modality analyzer improves the F1-score by 3.5 points."
P15-1160,Who caught a cold ? - Identifying the subject of a symptom,2015,39,7,3,0.833333,23269,shin kanouchi,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"The development and proliferation of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is health surveillance, e.g., predicting the outbreak of an epidemic by recognizing diseases and symptoms from text messages posted on social media platforms. In this paper, we propose a novel task that is crucial and generic from the viewpoint of health surveillance: estimating a subject (carrier) of a disease or symptommentioned in a Japanese tweet. By designing an annotation guideline for labeling the subject of a disease/symptom in a tweet, we perform annotations on an existing corpus for public surveillance. In addition, we present a supervised approach for predicting the subject of a disease/symptom. The results of our experiments demonstrate the impact of subject identification on the effective detection of an episode of a disease/symptom. Moreover, the results suggest that our task is independent of the type of disease/symptom."
Y14-1010,Finding The Best Model Among Representative Compositional Models,2014,29,11,5,1,17721,masayasu muraoka,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,"The field of distributional-compositional semantics has yielded a range of computational models for composing the vector of a phrase from those of constituent word vectors. Existing models have various ranges of their expressiveness, recursivity, and trainability. However, these models have not been examined closely for their compositionality. We implement and compare these models under the same conditions. The experimentally obtained results demonstrate that the model using different composition matrices for different dependency relations achieved state-ofthe-art performance on a dataset for two-word compositions (Mitchell and Lapata, 2010)."
W14-4910,A Corpus Study for Identifying Evidence on Microblogs,2014,13,2,4,1,24185,paul reisert,Proceedings of {LAW} {VIII} - The 8th Linguistic Annotation Workshop,0,"Microblogs are a popular way for users to communicate and have recently caught the attention of researchers in the natural language processing (NLP) field. However, regardless of their rising popularity, little attention has been given towards determining the properties of discourse relations for the rapid, large-scale microblog data. Therefore, given their importance for various NLP tasks, we begin a study of discourse relations on microblogs by focusing on evidence relations. As no annotated corpora for evidence relations on microblogs exist, we conduct a corpus study to identify such relations on Twitter, a popular microblogging service. We create annotation guidelines, conduct a large-scale annotation phase, and develop a corpus of annotated evidence relations. Finally, we report our observations, annotation difficulties, and data statistics."
W13-4505,Extracting and Aggregating False Information from Microblogs,2013,2,4,1,1,4956,naoaki okazaki,Proceedings of the Workshop on Language Processing and Crisis Information 2013,0,"During the 2011 East Japan Earthquake and Tsunami Disaster, we had found a number of false information spread on Twitter, e.g., xe2x80x9cThe Cosmo Oil explosion causes toxic rain.xe2x80x9d This paper extracts pieces of false information exhaustively from all the tweets within one week after the earthquake. Designing a set of linguistic patterns that correct false information, this paper proposes a method for detecting false information. More specifically, the method extracts text passages that match to the correction patterns, clusters the passages into topics of false information, and selects, for each topic, a passage explaining the false information the most suitably. In the experiment, we report the performance of the proposed method on the data set extracted manually from Web sites that are specialized in collecting false information."
P13-3016,Detecting Chronic Critics Based on Sentiment Polarity and User{'}s Behavior in Social Media,2013,14,2,4,1,4614,sho takase,51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Student Research Workshop,0,"There are some chronic critics who always complain about the entity in social media. We are working to automatically detect these chronic critics to prevent the spread of bad rumors about the reputation of the entity. In social media, most comments are informal, and, there are sarcastic and incomplete contexts. This means that it is difficult for current NLP technology such as opinion mining to recognize the complaints. As an alternative approach for social media, we can assume that users who share the same opinions will link to each other. Thus, we propose a method that combines opinion mining with graph analysis for the connections between users to identify the chronic critics. Our experimental results show that the proposed method outperforms analysis based only on opinion mining techniques."
P13-1038,Is a 204 cm Man Tall or Small ? Acquisition of Numerical Common Sense from the Web,2013,22,10,4,0,41475,katsuma narisawa,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper presents novel methods for modeling numerical common sense: the ability to infer whether a given number (e.g., three billion) is large, small, or normal for a given context (e.g., number of people facing a water shortage). We first discuss the necessity of numerical common sense in solving textual entailment problems. We explore two approaches for acquiring numerical common sense. Both approaches start with extracting numerical expressions and their context from the Web. One approach estimates the distribution of numbers co-occurring within a context and examines whether a given value is large, small, or normal, based on the distribution. Another approach utilizes textual patterns with which speakers explicitly expresses their judgment about the value of a numerical expression. Experimental results demonstrate the effectiveness of both approaches."
Y12-1057,Set Expansion using Sibling Relations between Semantic Categories,2012,18,0,2,1,4614,sho takase,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"Most set expansion algorithms assume to acquire new instances of different semantic categories independently even when we have seed instances of multiple semantic categories. However, in the setting of set expansion with multiple semantic categories, we might leverage other types of prior knowledge about semantic categories. In this paper, we present a method of set expansion when ontological information related to target semantic categories is available. More specifically, the proposed method makes use of sibling relations between semantic categories as an additional type of prior knowledge. We demonstrate the effectiveness of sibling relations in set expansion on the dataset in which instances and sibling relations are extracted from Wikipedia in a semi-automatic manner."
C12-2118,Acquiring and Generalizing Causal Inference Rules from Deverbal Noun Constructions,2012,22,4,2,1,1438,shohei tanaka,Proceedings of {COLING} 2012: Posters,0,"This paper presents a novel approach for inducing causal rules by using deverbal nouns as a clue for finding causal relations. We collect verbs and their deverbal forms from FrameNet, and extract pairs of sentences in which event verbs and their corresponding deverbal forms co-occur in documents. The most challenging part of this work is to generalize an instance of causal relation into a rule. This paper proposes a method to generalize and constrain causal rules so that the obtained rules have the high chance of applicability and reusability. In order to find a suitable constraint for a causal rule, we utilize relation instances extracted by an open-information extractor, and build a classifier to choose the most suitable constraint. We demonstrate that deverbal nouns provide a good clue for causal relations and that the proposed method can induce causal rules from deverbal noun constructions."
C12-1171,A Latent Discriminative Model for Compositional Entailment Relation Recognition using Natural Logic,2012,26,8,4,0.931234,5308,yotaro watanabe,Proceedings of {COLING} 2012,0,"Recognizing semantic relations between sentences, such as entailment and contradiction, is a challenging task that requires detailed analysis of the interaction between diverse linguistic phenomena. In this paper, we propose a latent discriminative model that unifies a statistical framework and a theory of Natural Logic to capture complex interactions between linguistic phenomena. The proposed approach jointly models alignments, their local semantic relations, and a sentence-level semantic relation, and has hidden variables including alignment edits between sentences and their semantic relations, only requires sentences pairs annotated with sentence-level semantic relations as training data to learn appropriate alignments. In evaluation on a dataset including diverse linguistic phenomena, our proposed method achieved a competitive results on alignment prediction, and significant improvements on a sentence-level semantic relation recognition task compared to an alignment supervised model. Our analysis did not provide evidence that directly learning alignments and their labels using gold standard alignments contributed to semantic relation recognition performance and instead suggests that they can be detrimental to performance if used in a manner that prevents the learning of globally optimal alignments."
W11-0208,Automatic Acquisition of Huge Training Data for Bio-Medical Named Entity Recognition,2011,18,18,3,0,44431,yu usami,Proceedings of {B}io{NLP} 2011 Workshop,0,"Named Entity Recognition (NER) is an important first step for BioNLP tasks, e.g., gene normalization and event extraction. Employing supervised machine learning techniques for achieving high performance recent NER systems require a manually annotated corpus in which every mention of the desired semantic types in a text is annotated. However, great amounts of human effort is necessary to build and maintain an annotated corpus. This study explores a method to build a high-performance NER without a manually annotated corpus, but using a comprehensible lexical database that stores numerous expressions of semantic types and with huge amount of unannotated texts. We underscore the effectiveness of our approach by comparing the performance of NERs trained on an automatically acquired training data and on a manually annotated corpus."
C10-2141,Learning Web Query Patterns for Imitating {W}ikipedia Articles,2010,26,1,2,1,1438,shohei tanaka,Coling 2010: Posters,0,"This paper presents a novel method for acquiring a set of query patterns to retrieve documents containing important information about an entity. Given an existing Wikipedia category that contains the target entity, we extract and select a small set of query patterns by presuming that formulating search queries with these patterns optimizes the overall precision and coverage of the returned Web information. We model this optimization problem as a weighted maximum satisfiability (weighted Max-SAT) problem. The experimental results demonstrate that the proposed method outperforms other methods based on statistical measures such as frequency and point-wise mutual information (PMI), which are widely used in relation extraction."
C10-1096,Simple and Efficient Algorithm for Approximate Dictionary Matching,2010,24,42,1,1,4956,naoaki okazaki,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"This paper presents a simple and efficient algorithm for approximate dictionary matching designed for similarity measures such as cosine, Dice, Jaccard, and overlap coefficients. We propose this algorithm, called CPMerge, for the xcfx84-overlap join of inverted lists. First we show that this task is solvable exactly by a xcfx84-overlap join. Given inverted lists retrieved for a query, the algorithm collects fewer candidate strings and prunes unlikely candidates to efficiently find strings that satisfy the constraint of the xcfx84-overlap join. We conducted experiments of approximate dictionary matching on three large-scale datasets that include person names, biomedical names, and general English words. The algorithm exhibited scalable performance on the datasets. For example, it retrieved strings in 1.1 ms from the string collection of Google Web1T unigrams (with cosine similarity and threshold 0.7)."
P09-1003,A Comparative Study on Generalization of Semantic Roles in {F}rame{N}et,2009,20,20,2,0.625,9338,yuichiroh matsubayashi,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"A number of studies have presented machine-learning approaches to semantic role labeling with availability of corpora such as FrameNet and PropBank. These corpora define the semantic roles of predicates for each frame independently. Thus, it is crucial for the machine-learning approach to generalize semantic roles across different frames, and to increase the size of training instances. This paper explores several criteria for generalizing semantic roles in FrameNet: role hierarchy, human-understandable descriptors of roles, semantic types of filler phrases, and mappings from FrameNet roles to thematic roles of VerbNet. We also propose feature functions that naturally combine and weight these criteria, based on the training data. The experimental result of the role classification shows 19.16% and 7.42% improvements in error reduction rate and macro-averaged F1 score, respectively. We also provide in-depth analyses of the proposed criteria."
P09-1102,Robust Approach to Abbreviating Terms: A Discriminative Latent Variable Model with Global Information,2009,20,13,2,0,3749,xu sun,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"The present paper describes a robust approach for abbreviating terms. First, in order to incorporate non-local information into abbreviation generation tasks, we present both implicit and explicit solutions: the latent variable model, or alternatively, the label encoding approach with global information. Although the two approaches compete with one another, we demonstrate that these approaches are also complementary. By combining these two approaches, experiments revealed that the proposed abbreviation generator achieved the best results for both the Chinese and English languages. Moreover, we directly apply our generator to perform a very different task from tradition, the abbreviation recognition. Experiments revealed that the proposed model worked robustly, and outperformed five out of six state-of-the-art abbreviation recognizers."
P09-1115,Unsupervised Relation Extraction by Mining {W}ikipedia Texts Using Information from the Web,2009,22,61,2,0,41586,yulan yan,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"This paper presents an unsupervised relation extraction method for discovering and enhancing relations in which a specified concept in Wikipedia participates. Using respective characteristics of Wikipedia articles and Web corpus, we develop a clustering approach based on combinations of patterns: dependency patterns from dependency analysis of texts in Wikipedia, and surface patterns generated from highly redundant information related to the Web. Evaluations of the proposed approach on two different domains demonstrate the superiority of the pattern combination over existing approaches. Fundamentally, our method demonstrates how deep linguistic patterns contribute complementarily with Web surface patterns to the generation of various relations."
N09-1048,Semi-Supervised Lexicon Mining from Parenthetical Expressions in Monolingual Web Pages,2009,21,7,2,1,6319,xianchao wu,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper presents a semi-supervised learning framework for mining Chinese-English lexicons from large amount of Chinese Web pages. The issue is motivated by the observation that many Chinese neologisms are accompanied by their English translations in the form of parenthesis. We classify parenthetical translations into bilingual abbreviations, transliterations, and translations. A frequency-based term recognition approach is applied for extracting bilingual abbreviations. A self-training algorithm is proposed for mining transliteration and translation lexicons. In which, we employ available lexicons in terms of morpheme levels, i.e., phoneme correspondences in transliteration and grapheme (e.g., suffix, stem, and prefix) correspondences in translation. The experimental results verified the effectiveness of our approaches."
2009.iwslt-evaluation.15,The {UOT} system,2009,-1,-1,3,1,6319,xianchao wu,Proceedings of the 6th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"We present the UOT Machine Translation System that was used in the IWSLT-09 evaluation campaign. This year, we participated in the BTEC track for Chinese-to-English translation. Our system is based on a string-to-tree framework. To integrate deep syntactic information, we propose the use of parse trees and semantic dependencies on English sentences described respectively by Head-driven Phrase Structure Grammar and Predicate-Argument Structures. We report the results of our system on both the development and test sets."
tsunakawa-etal-2008-building-bilingual,Building Bilingual Lexicons using Lexical Translation Probabilities via Pivot Languages,2008,15,13,2,0,37902,takashi tsunakawa,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper proposes a method of increasing the size of a bilingual lexicon obtained from two other bilingual lexicons via a pivot language. When we apply this approach, there are two main challenges, ambiguity and mismatch of terms; we target the latter problem by improving the utilization ratio of the bilingual lexicons. Given two bilingual lexicons between language pairs Lf-Lp and Lp-Le, we compute lexical translation probabilities of word pairs by using a statistical word-alignment model, and term decomposition/composition techniques. We compare three approaches to generate the bilingual lexicon: exact merging, word-based merging, and our proposed alignment-based merging. In our method, we combine lexical translation probabilities and a simple language model for estimating the probabilities of translation pairs. The experimental results show that our method could drastically improve the number of translation terms compared to the two methods mentioned above. Additionally, we evaluated and discussed the quality of the translation outputs."
saetre-etal-2008-connecting,Connecting Text Mining and Pathways using the {P}ath{T}ext Resource,2008,10,0,4,0,32381,rune saetre,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Many systems have been developed in the past few years to assist researchers in the discovery of knowledge published as English text, for example in the PubMed database. At the same time, higher level collective knowledge is often published using a graphical notation representing all the entities in a pathway and their interactions. We believe that these pathway visualizations could serve as an effective user interface for knowledge discovery if they can be linked to the text in publications. Since the graphical elements in a Pathway are of a very different nature than their corresponding descriptions in English text, we developed a prototype system called PathText. The goal of PathText is to serve as a bridge between these two different representations. In this paper, we first describe the overall architecture and the interfaces of the PathText system, and then provide some details about the core Text Mining components."
I08-2127,A Discriminative Approach to {J}apanese Abbreviation Extraction,2008,15,7,1,1,4956,naoaki okazaki,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"This paper addresses the difficulties in recognizing Japanese abbreviations through the use of previous approaches, examining actual usages of parenthetical expressions in newspaper articles. In order to bridge the gap between Japanese abbreviations and their full forms, we present a discriminative approach to abbreviation recognition. More specifically, we formalize the abbreviation recognition task as a binary classification problem in which a classifier determines a positive (abbreviation) or negative (nonabbreviation) class, given a candidate of abbreviation definition. The proposed method achieved 95.7% accuracy, 90.0% precision, and 87.6% recall on the evaluation corpus containing 7,887 (1,430 abbreviations and 6,457 non-abbreviation) instances of parenthetical expressions."
I08-1050,Identifying Sections in Scientific Abstracts using Conditional Random Fields,2008,18,99,2,0,48675,kenji hirohata,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"OBJECTIVE: The prior knowledge about the rhetorical structure of scientific abstracts is useful for various text-mining tasks such as information extraction, information retrieval, and automatic summarization. This paper presents a novel approach to categorize sentences in scientific abstracts into four sections, objective, methods, results, and conclusions. METHOD: Formalizing the categorization task as a sequential labeling problem, we employ Conditional Random Fields (CRFs) to annotate section labels into abstract sentences. The training corpus is acquired automatically from Medline abstracts. RESULTS: The proposed method outperformed the previous approaches, achieving 95.5% per-sentence accuracy and 68.8% per-abstract accuracy. CONCLUSION: The experimental results showed that CRFs could model the rhetorical structure of abstracts more suitably."
D08-1047,A Discriminative Candidate Generator for String Transformations,2008,26,27,1,1,4956,naoaki okazaki,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"String transformation, which maps a source string s into its desirable form t*, is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1-regularized logistic regression model. We also propose a procedure to generate negative instances that affect the decision boundary of the model. The advantage of this approach is that candidate strings can be enumerated by an efficient algorithm because the processes of string transformation are tractable in the model. We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations."
C08-2032,Building a Bilingual Lexicon Using Phrase-based Statistical Machine Translation via a Pivot Language,2008,12,7,2,0,37902,takashi tsunakawa,Coling 2008: Companion volume: Posters,0,"This paper proposes a novel method for building a bilingual lexicon through a pivot language by using phrase-based statistical machine translation (SMT). Given two bilingual lexicons between language pairs Lf xe2x80x93Lp and Lpxe2x80x93Le, we assume these lexicons as parallel corpora. Then, we merge the extracted two phrase tables into one phrase table between Lf and Le. Finally, we construct a phrase-based SMT system for translating the terms in the lexicon Lf xe2x80x93Lp into terms of Le and, obtain a new lexicon Lf xe2x80x93Le. In our experiments with Chinese-English and JapaneseEnglish lexicons, our system could cover 72.8% of Chinese terms and drastically improve the utilization ratio."
C08-1083,A Discriminative Alignment Model for Abbreviation Recognition,2008,23,13,1,1,4956,naoaki okazaki,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper presents a discriminative alignment model for extracting abbreviations and their full forms appearing in actual text. The task of abbreviation recognition is formalized as a sequential alignment problem, which finds the optimal alignment (origins of abbreviation letters) between two strings (abbreviation and full form). We design a large amount of finegrained features that directly express the events where letters produce or do not produce abbreviations. We obtain the optimal combination of features on an aligned abbreviation corpus by using the maximum entropy framework. The experimental results show the usefulness of the alignment model and corpus for improving abbreviation recognition."
2008.amta-papers.19,Improving {E}nglish-to-{C}hinese Translation for Technical Terms using Morphological Information,2008,-1,-1,2,1,6319,xianchao wu,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"The continuous emergence of new technical terms and the difficulty of keeping up with neologism in parallel corpora deteriorate the performance of statistical machine translation (SMT) systems. This paper explores the use of morphological information to improve English-to-Chinese translation for technical terms. To reduce the morpheme-level translation ambiguity, we group the morphemes into morpheme phrases and propose the use of domain information for translation candidate selection. In order to find correspondences of morpheme phrases between the source and target languages, we propose an algorithm to mine morpheme phrase translation pairs from a bilingual lexicon. We also build a cascaded translation model that dynamically shifts translation units from phrase level to word and morpheme phrase levels. The experimental results show the significant improvements over the current phrase-based SMT systems."
P06-2083,A Term Recognition Approach to Acronym Recognition,2006,19,30,1,1,4956,naoaki okazaki,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"We present a term recognition approach to extract acronyms and their definitions from a large text collection. Parenthetical expressions appearing in a text collection are identified as potential acronyms. Assuming terms appearing frequently in the proximity of an acronym to be the expanded forms (definitions) of the acronyms, we apply a term recognition method to enumerate such candidates and to measure the likelihood scores of the expanded forms. Based on the list of the expanded forms and their likelihood scores, the proposed algorithm determines the final acronym-definition pairs. The proposed method combined with a letter matching algorithm achieved 78% precision and 85% recall on an evaluation corpus with 4,212 acronym-definition pairs."
P06-1049,A Bottom-Up Approach to Sentence Ordering for Multi-Document Summarization,2006,36,29,2,1,9798,danushka bollegala,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Ordering information is a difficult but important task for applications generating natural-language text. We present a bottom-up approach to arranging sentences extracted for multi-document summarization. To capture the association and order of two textual segments (eg, sentences), we define four criteria, chronology, topical-closeness, precedence, and succession. These criteria are integrated into a criterion by a supervised learning approach. We repeatedly concatenate two textual segments into one segment based on the criterion until we obtain the overall segment with all sentences arranged. Our experimental results show a significant improvement over existing sentence ordering strategies."
okazaki-ananiadou-2006-clustering,Clustering acronyms in biomedical text for disambiguation,2006,10,22,1,1,4956,naoaki okazaki,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Given the increasing number of neologisms in biomedicine (names of genes, diseases, molecules, etc.), the rate of acronyms used in literature also increases. Existing acronym dictionaries cannot keep up with the rate of new creations. Thus, discovering and disambiguating acronyms and their expanded forms are essential aspects of text mining and terminology management. We present a method for clustering long forms identified by an acronym recognition method. Applying the acronym recognition method to MEDLINE abstracts, we obtained a list of short/long forms. The recognized short/long forms were classified by abiologist to construct an evaluation set for clustering sets of similar long forms. We observed five types of term variation in the evaluation set and defined four similarity measures to gathers the similar longforms (i.e., orthographic, morphological, syntactic, lexico semantic variants, nested abbreviations). The complete-link clustering with the four similarity measures achieved 87.5{\%} precision and 84.9{\%} recall on the evaluation set."
I05-1055,A Machine Learning Approach to Sentence Ordering for Multidocument Summarization and Its Evaluation,2005,20,23,2,1,9798,danushka bollegala,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Ordering information is a difficult but a important task for natural language generation applications. A wrong order of information not only makes it difficult to understand, but also conveys an entirely different idea to the reader. This paper proposes an algorithm that learns orderings from a set of human ordered texts. Our model consists of a set of ordering experts. Each expert gives its precedence preference between two sentences. We combine these preferences and order sentences. We also propose two new metrics for the evaluation of sentence orderings. Our experimental results show that the proposed algorithm outperforms the existing methods in all evaluation metrics."
C04-1108,Improving Chronological Sentence Ordering by Precedence Relation,2004,13,40,1,1,4956,naoaki okazaki,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"It is necessary to find a proper arrangement of sentences in order to generate a well-organized summary from multiple documents. In this paper we describe an approach to coherent sentence ordering for summarizing newspaper articles. Since there is no guarantee that chronological ordering of extracted sentences, which is widely used by conventional summarization system, arranges each sentence behind presupposed information of the sentence, we improve chronological ordering by resolving antecedent sentences of arranged sentences. Combining the refinement algorithm with topical segmentation and chronological ordering, we address our experiment to test the effectiveness of the proposed method. The results reveal that the proposed method improves chronological sentence ordering."
