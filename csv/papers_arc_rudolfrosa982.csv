2020.sigtyp-1.4,Predicting Typological Features in {WALS} using Language Embeddings and Conditional Probabilities: {{\\'U}FAL} Submission to the {SIGTYP} 2020 Shared Task,2020,-1,-1,3,0,14783,martin vastl,Proceedings of the Second Workshop on Computational Research in Linguistic Typology,0,"We present our submission to the SIGTYP 2020 Shared Task on the prediction of typological features. We submit a constrained system, predicting typological features only based on the WALS database. We investigate two approaches. The simpler of the two is a system based on estimating correlation of feature values within languages by computing conditional probabilities and mutual information. The second approach is to train a neural predictor operating on precomputed language embeddings based on WALS features. Our submitted system combines the two approaches based on their self-estimated confidence scores. We reach the accuracy of 70.7{\%} on the test data and rank first in the shared task."
2020.lantern-1.1,Eyes on the Parse: Using Gaze Features in Syntactic Parsing,2020,-1,-1,2,0,18553,abhishek agrawal,Proceedings of the Second Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN),0,"In this paper, we explore the potential benefits of leveraging eye-tracking information for dependency parsing on the English part of the Dundee corpus. To achieve this, we cast dependency parsing as a sequence labelling task and then augment the neural model for sequence labelling with eye-tracking features. We also augment a graph-based parser with eye-tracking features and parse the Dundee Corpus to corroborate our findings from the sequence labelling parser. We then experiment with a variety of parser setups ranging from parsing with all features to a delexicalized parser. Our experiments show that for a parser with all features, although the improvements are positive for the LAS score they are not significant whereas our delexicalized parser significantly outperforms the baseline we established. We also analyze the contribution of various eye-tracking features towards the different parser setups and find that eye-tracking features contain information which is complementary in nature, thus implying that augmenting the parser with various gaze features grouped together provides better performance than any individual gaze feature."
2020.findings-emnlp.150,On the Language Neutrality of Pre-trained Multilingual Representations,2020,37,0,2,0,13977,jindvrich libovicky,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Multilingual contextual embeddings, such as multilingual BERT and XLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work probed the cross-linguality of the representations indirectly using zero-shot transfer learning on morphological and syntactic tasks. We instead investigate the language-neutrality of multilingual contextual embeddings directly and with respect to lexical semantics. Our results show that contextual embeddings are more language-neutral and, in general, more informative than aligned static word-type embeddings, which are explicitly trained for language neutrality. Contextual embeddings are still only moderately language-neutral by default, so we propose two simple methods for achieving stronger language neutrality: first, by unsupervised centering of the representation for each language and second, by fitting an explicit projection on small parallel data. Besides, we show how to reach state-of-the-art accuracy on language identification and match the performance of statistical methods for word alignment of parallel sentences without using parallel data."
2020.findings-emnlp.245,{U}niversal {D}ependencies {A}ccording to {BERT}: {B}oth {M}ore {S}pecific and {M}ore {G}eneral,2020,11,0,3,0,9477,tomasz limisiewicz,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"This work focuses on analyzing the form and extent of syntactic abstraction captured by BERT by extracting labeled dependency trees from self-attentions. Previous work showed that individual BERT heads tend to encode particular dependency relation types. We extend these findings by explicitly comparing BERT relations to Universal Dependencies (UD) annotations, showing that they often do not match one-to-one. We suggest a method for relation identification and syntactic tree construction. Our approach produces significantly more consistent dependency trees than previous work, showing that it better explains the syntactic abstractions in BERT. At the same time, it can be successfully applied with only a minimal amount of supervision and generalizes well across languages."
W19-8508,Attempting to separate inflection and derivation using vector space representations,2019,-1,-1,1,1,14784,rudolf rosa,Proceedings of the Second International Workshop on Resources and Tools for Derivational Morphology,0,None
W19-4827,From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self-Attentions,2019,21,0,2,0.374648,9478,david marevcek,Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,0,"We inspect the multi-head self-attention in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation. In many of the attention heads, we frequently find sequences of consecutive states attending to the same position, which resemble syntactic phrases. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase-structure trees from the phrase-like sequences. We compare the resulting trees to existing constituency treebanks, both manually and by computing precision and recall."
W18-5444,Extracting Syntactic Trees from Transformer Encoder Self-Attentions,2018,0,6,2,0.374648,9478,david marevcek,Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP},0,"This is a work in progress about extracting the sentence tree structures from the encoder{'}s self-attention weights, when translating into another language using the Transformer neural network architecture. We visualize the structures and discuss their characteristics with respect to the existing syntactic theories and annotations."
K18-2019,{CUNI} x-ling: Parsing Under-Resourced Languages in {C}o{NLL} 2018 {UD} Shared Task,2018,0,4,1,1,14784,rudolf rosa,Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"This is a system description paper for the CUNI x-ling submission to the CoNLL 2018 UD Shared Task. We focused on parsing under-resourced languages, with no or little training data available. We employed a wide range of approaches, including simple word-based treebank translation, combination of delexicalized parsers, and exploitation of available morphological dictionaries, with a dedicated setup tailored to each of the languages. In the official evaluation, our submission was identified as the clear winner of the Low-resource languages category."
W17-7615,Error Analysis of Cross-lingual Tagging and Parsing,2017,-1,-1,1,1,14784,rudolf rosa,Proceedings of the 16th International Workshop on Treebanks and Linguistic Theories,0,None
W17-4719,Findings of the {WMT} 2017 Biomedical Translation Shared Task,2017,9,7,13,0,4214,antonio yepes,Proceedings of the Second Conference on Machine Translation,0,None
W17-4769,{CUNI} Experiments for {WMT}17 Metrics Task,2017,4,0,4,0.367221,9478,david marevcek,Proceedings of the Second Conference on Machine Translation,0,None
W17-1226,"{S}lavic Forest, {N}orwegian Wood",2017,0,0,1,1,14784,rudolf rosa,"Proceedings of the Fourth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial)",0,"We once had a corp, or should we say, it once had us They showed us its tags, isn{'}t it great, unified tags They asked us to parse and they told us to use everything So we looked around and we noticed there was near nothing We took other langs, bitext aligned: words one-to-one We played for two weeks, and then they said, here is the test The parser kept training till morning, just until deadline So we had to wait and hope what we get would be just fine And, when we awoke, the results were done, we saw we{'}d won So, we wrote this paper, isn{'}t it good, Norwegian wood."
W16-6401,{M}oses {\\&} Treex Hybrid {MT} Systems Bestiary,2016,19,0,1,1,14784,rudolf rosa,Proceedings of the 2nd Deep Machine Translation Workshop,0,None
W16-2334,Dictionary-based Domain Adaptation of {MT} Systems without Retraining,2016,8,4,1,1,14784,rudolf rosa,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"We describe our submission to the ITdomain translation task of WMT 2016. We perform domain adaptation with dictionary data on already trained MT systems with no further retraining. We apply our approach to two conceptually different systems developed within the QTLeap project: TectoMT and Moses, as well as Chimera, their combination. In all settings, our method improves the translation quality. Moreover, the basic variant of our approach is applicable to any MT system, including a black-box one."
2016.eamt-2.1,{T}ecto{MT} {--} a deep linguistic core of the combined Cimera {MT} system,2016,-1,-1,4,0.538076,227,martin popel,Proceedings of the 19th Annual Conference of the European Association for Machine Translation: Projects/Products,0,None
W15-5711,Translation Model Interpolation for Domain Adaptation in {T}ecto{MT},2015,-1,-1,1,1,14784,rudolf rosa,Proceedings of the 1st Deep Machine Translation Workshop,0,None
W15-3009,New Language Pairs in {T}ecto{MT},2015,20,7,5,0.454545,2976,ondvrej duvsek,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"The TectoMT tree-to-tree machine translation system has been updated this year to support easier retraining for more translation directions. We use multilingual standards for morphology and syntax annotation and language-independent base rules. We include a simple, non-parametric way of combining TectoMTxe2x80x99s transfer model outputs. We submitted translations by the Englishto-Czech and Czech-to-English TectoMT pipelines to the WMT shared task. While the former offers a stable performance, the latter is completely new and will require more tuning and debugging."
W15-2209,{MSTP}arser Model Interpolation for Multi-Source Delexicalized Transfer,2015,18,2,1,1,14784,rudolf rosa,Proceedings of the 14th International Conference on Parsing Technologies,0,"We introduce interpolation of trained MSTParser models as a resource combination method for multi-source delexicalized parser transfer. We present both an unweighted method, as well as a variant in which each source model is weighted by the similarity of the source language to the target language. Evaluation on the HamleDT treebank collection shows that the weighted model interpolation performs comparably to weighted parse tree combination method, while being computationally much less demanding."
W15-2104,Targeted Paraphrasing on Deep Syntactic Layer for {MT} Evaluation,2015,30,0,2,1,32025,petra baranvcikova,Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015),0,"In this paper, we present a method of improving quality of machine translation (MT) evaluation of Czech sentences via targeted paraphrasing of reference sentences on a deep syntactic layer. For this purpose, we employ NLP framework Treex and extend it with modules for targeted paraphrasing and word order changes. Automatic scores computed using these paraphrased reference sentences show higher correlation with human judgment than scores computed on the original reference sentences."
W15-2131,Multi-source Cross-lingual Delexicalized Parser Transfer: {P}rague or {S}tanford?,2015,22,6,1,1,14784,rudolf rosa,Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015),0,"We compare two annotation styles, Prague dependencies and Universal Stanford Dependencies, in their adequacy for parsing. We specifically focus on comparing the adposition attachment style, used in these two formalisms, applied in multisource cross-lingual delexicalized dependency parser transfer performed by parse tree combination. We show that in our setting, converting the adposition annotation to Stanford style in the Prague style training treebanks leads to promising results. We find that best results can be obtained by parsing the target sentences with parsers trained on treebanks using both of the adposition annotation styles in parallel, and combining all the resulting parse trees together after having converted them to the Stanford adposition style (0.39% UAS over Prague style baseline). The score improvements are considerably more significant when using a smaller set of diverse source treebanks (up to 2.24% UAS over the baseline)."
P15-2040,{KL}cpos3 - a Language Similarity Measure for Delexicalized Parser Transfer,2015,25,15,1,1,14784,rudolf rosa,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"We present KLcpos3 , a language similarity measure based on Kullback-Leibler divergence of coarse part-of-speech tag trigram distributions in tagged corpora. It has been designed for multilingual delexicalized parsing, both for source treebank selection in single-source parser transfer, and for source treebank weighting in multi-source transfer. In the selection task, KLcpos3 identifies the best source treebank in 8 out of 18 cases. In the weighting task, it brings 4.5% UAS absolute, compared to unweighted parse tree combination."
W14-3322,{CUNI} in {WMT}14: Chimera Still Awaits Bellerophon,2014,12,7,3,0,4973,alevs tamchyna,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"We present our English!Czech and English!Hindi submissions for this yearxe2x80x99s WMT translation task. For English!Czech, we build upon last yearxe2x80x99s CHIMERA and evaluate several setups. English!Hindi is a new language pair for this year. We experimented with reverse self-training to acquire more (synthetic) parallel data and with modeling target-side morphology."
W14-3326,Machine Translation of Medical Texts in the Khresmoi Project,2014,37,10,6,0.454545,2976,ondvrej duvsek,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper presents the participation of the Charles University team in the WMT 2014 Medical Translation Task. Our systems are developed within the Khresmoi project, a large integrated project aiming to deliver a multi-lingual multi-modal search and access system for biomedical information and documents. Being involved in the organization of the Medical Translation Task, our primary goal is to set up a baseline for both its subtasks (summary translation and query translation) and for all translation directions. Our systems are based on the phrasebased Moses system and standard methods for domain adaptation. The constrained/unconstrained systems differ in the training data only."
rosa-etal-2014-hamledt,{H}amle{DT} 2.0: Thirty Dependency Treebanks Stanfordized,2014,30,20,1,1,14784,rudolf rosa,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present HamleDT 2.0 (HArmonized Multi-LanguagE Dependency Treebank). HamleDT 2.0 is a collection of 30 existing treebanks harmonized into a common annotation style, the Prague Dependencies, and further transformed into Stanford Dependencies, a treebank annotation style that became popular in recent years. We use the newest basic Universal Stanford Dependencies, without added language-specific subtypes. We describe both of the annotation styles, including adjustments that were necessary to make, and provide details about the conversion process. We also discuss the differences between the two styles, evaluating their advantages and disadvantages, and note the effects of the differences on the conversion. We regard the stanfordization as generally successful, although we admit several shortcomings, especially in the distinction between direct and indirect objects, that have to be addressed in future. We release part of HamleDT 2.0 freely; we are not allowed to redistribute the whole dataset, but we do provide the conversion pipeline."
barancikova-etal-2014-improving,Improving Evaluation of {E}nglish-{C}zech {MT} through Paraphrasing,2014,14,5,2,1,32025,petra baranvcikova,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we present a method of improving the accuracy of machine translation evaluation of Czech sentences. Given a reference sentence, our algorithm transforms it by targeted paraphrasing into a new synthetic reference sentence that is closer in wording to the machine translation output, but at the same time preserves the meaning of the original reference sentence. Grammatical correctness of the new reference sentence is provided by applying Depfix on newly created paraphrases. Depfix is a system for post-editing English-to-Czech machine translation outputs. We adjusted it to fix the errors in paraphrased sentences. Due to a noisy source of our paraphrases, we experiment with adding word alignment. However, the alignment reduces the number of paraphrases found and the best results were achieved by a simple greedy method with only one-word paraphrases thanks to their intensive filtering. BLEU scores computed using these new reference sentences show significantly higher correlation with human judgment than scores computed on the original reference sentences."
W13-2208,Chimera {--} Three Heads for {E}nglish-to-{C}zech Translation,2013,22,11,2,0,292,ondvrej bojar,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"This paper describes our WMT submissions CU-BOJAR and CU-DEPFIX, the latter dubbed xe2x80x9cCHIMERAxe2x80x9d because it combines on three diverse approaches: TectoMT, a system with transfer at the deep syntactic level of representation, factored phrase-based translation using Moses, and finally automatic rule-based correction of frequent grammatical and meaning errors. We do not use any off-the-shelf systemcombination method."
P13-3025,{D}eepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis,2013,20,7,1,1,14784,rudolf rosa,51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Student Research Workshop,0,"Deepfix is a statistical post-editing system for improving the quality of statistical machine translation outputs. It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. On the English-to-Czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge."
W12-4205,Using Parallel Features in Parsing of Machine-Translated Sentences for Correction of Grammatical Errors,2012,32,6,1,1,14784,rudolf rosa,"Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"In this paper, we present two dependency parser training methods appropriate for parsing outputs of statistical machine translation (SMT), which pose problems to standard parsers due to their frequent ungrammaticality. We adapt the MST parser by exploiting additional features from the source language, and by introducing artificial grammatical errors in the parser training data, so that the training sentences resemble SMT output.n n We evaluate the modified parser on DEPFIX, a system that improves English-Czech SMT outputs using automatic rule-based corrections of grammatical mistakes which requires parsed SMT output sentences as its input. Both parser modifications led to improvements in BLEU score; their combination was evaluated manually, showing a statistically significant improvement of the translation quality."
W12-3146,{DEPFIX}: A System for Automatic Correction of {C}zech {MT} Outputs,2012,15,27,1,1,14784,rudolf rosa,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"We present an improved version of DEPFIX (Marecek et al., 2011), a system for automatic rule-based post-processing of English-to-Czech MT outputs designed to increase their fluency. We enhanced the rule set used by the original DEPFIX system and measured the performance of the individual rules.n n We also modified the dependency parser of McDonald et al. (2005) in two ways to adjust it for the parsing of MT outputs. We show that our system is able to improve the quality of the state-of-the-art MT systems."
W11-2152,Two-step translation with grammatical post-processing,2011,11,17,2,0,9478,david marevcek,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,This paper describes an experiment in which we try to automatically correct mistakes in grammatical agreement in English to Czech MT outputs. We perform several rule-based corrections on sentences parsed to dependency trees. We prove that it is possible to improve the MT quality of majority of the systems participating in WMT shared task. We made both automatic (BLEU) and manual evaluations.
