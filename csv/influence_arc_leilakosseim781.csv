2001.jeptalnrecital-poster.7,M95-1012,0,0.0444057,"Missing"
2001.jeptalnrecital-poster.7,P98-1045,0,0.0602964,"Missing"
2003.jeptalnrecital-poster.1,J95-4004,0,0.0378135,"Missing"
2003.jeptalnrecital-poster.1,W02-1033,0,0.0705251,"Missing"
2003.jeptalnrecital-poster.1,duclaye-etal-2002-using,0,0.0377692,"Missing"
2010.jeptalnrecital-long.11,W08-1301,0,0.0888169,"Missing"
2010.jeptalnrecital-long.11,W97-0713,0,0.259391,"Missing"
2010.jeptalnrecital-long.11,W09-4301,1,0.839316,"Missing"
2010.jeptalnrecital-long.11,P09-1024,0,0.0750912,"Missing"
2010.jeptalnrecital-long.11,N03-1030,0,0.106716,"Missing"
2010.jeptalnrecital-long.11,prasad-etal-2008-penn,0,\N,Missing
2018.jeptalnrecital-deft.3,W02-0109,0,0.147478,"Missing"
2018.jeptalnrecital-deft.3,S13-2053,0,0.0856585,"Missing"
2018.jeptalnrecital-deft.3,pak-paroubek-2010-twitter,0,0.174731,"Missing"
2018.jeptalnrecital-deft.3,W15-2914,0,0.0574556,"Missing"
2020.coling-main.58,N19-1423,0,0.166226,"t al., 2019). In our approach, we aim to develop a toponym detection system solely based on deep learning techniques in order to sidestep many design choices and avoid pattern mining that comes along with rule-based techniques and gazetteer approaches. Contextualized Embeddings Previous attempts to toponym detection in the medical domain have used contextualized embeddings, specifically ELMo (Peters et al., 2018), as the core of their models (Li et al., 2019; Yadav et al., 2019). In this paper, we experiment with a different contextualized embedding model and choose the pretrained BERT model (Devlin et al., 2019) as the backbone of our network architecture. Linguistic Features Previous works on toponym detection in the medical domain have typically taken advantage of handcrafted features to achieve competitive performance. Most notable features include: (1) orthographic features that capture character level attributes of each token (Magge et al., 2018; Wang et al., 2019; Davari et al., 2019), and (2) part of speech (POS) tags (Wang et al., 2019; Davari et al., 2019; Qi et al., 2019). In Section 4, we will evaluate the influence of these features on our proposed model. Network Architecture There are tw"
2020.coling-main.58,H05-1046,0,0.134812,"Missing"
2020.coling-main.58,S19-2231,0,0.0168747,"system should identify Wuhan, Hubei Province, and China as toponyms, and all other words as non-toponyms. Toponym detection tackles ambiguities between toponyms and other classes of Named Entity Recognition as well as metonymic usage of toponyms. Toponym resolution in the epidemiology domain was the objective of the SemEval 2019 task 12 (Weissenbacher et al., 2019). The majority of current approaches to toponym detection in the medical domain are based on a combination of rule-based techniques, geographical gazetteer approaches, and deep learning models (Magge et al., 2018; Wang et al., 2019; Li et al., 2019; Qi et al., 2019). In our approach, we aim to develop a toponym detection system solely based on deep learning techniques in order to sidestep many design choices and avoid pattern mining that comes along with rule-based techniques and gazetteer approaches. Contextualized Embeddings Previous attempts to toponym detection in the medical domain have used contextualized embeddings, specifically ELMo (Peters et al., 2018), as the core of their models (Li et al., 2019; Yadav et al., 2019). In this paper, we experiment with a different contextualized embedding model and choose the pretrained BERT m"
2020.coling-main.58,S19-2230,0,0.0180657,"ari et al., 2019), and (2) part of speech (POS) tags (Wang et al., 2019; Davari et al., 2019; Qi et al., 2019). In Section 4, we will evaluate the influence of these features on our proposed model. Network Architecture There are two paradigms governing the network architectures used for this task: namely, whether localized contextual information is enough, or all available contextual information should be taken into account when making predictions. The former leads to models that only have access to a sliding window of information such as CNNs or MLPs (Magge et al., 2018; Davari et al., 2019; Magnusson and Dietz, 2019). The latter leads to sequential models operating at the sentence level; among which the BiLSTM-CRF architecture is the most favored and provides state-of-the-art results (Wang et al., 2019; Yadav et al., 2019; Qi et al., 2019; Magnusson and Dietz, 2019). In our experiments, we focused on neural architectures that considered all available contextual information within a sentence. However, we deviated from the trend of Recurrent Neural Networks (RNN) and based our network on the Transformer models (Vaswani et al., 2017) due to its ability to process variable length inputs, while being much more"
2020.coling-main.58,J93-2004,0,0.0696498,"Missing"
2020.coling-main.58,P09-1113,0,0.0470425,"Missing"
2020.coling-main.58,N18-1202,0,0.0478357,"tection in the medical domain are based on a combination of rule-based techniques, geographical gazetteer approaches, and deep learning models (Magge et al., 2018; Wang et al., 2019; Li et al., 2019; Qi et al., 2019). In our approach, we aim to develop a toponym detection system solely based on deep learning techniques in order to sidestep many design choices and avoid pattern mining that comes along with rule-based techniques and gazetteer approaches. Contextualized Embeddings Previous attempts to toponym detection in the medical domain have used contextualized embeddings, specifically ELMo (Peters et al., 2018), as the core of their models (Li et al., 2019; Yadav et al., 2019). In this paper, we experiment with a different contextualized embedding model and choose the pretrained BERT model (Devlin et al., 2019) as the backbone of our network architecture. Linguistic Features Previous works on toponym detection in the medical domain have typically taken advantage of handcrafted features to achieve competitive performance. Most notable features include: (1) orthographic features that capture character level attributes of each token (Magge et al., 2018; Wang et al., 2019; Davari et al., 2019), and (2)"
2020.coling-main.58,S19-2229,0,0.0786851,"ntify Wuhan, Hubei Province, and China as toponyms, and all other words as non-toponyms. Toponym detection tackles ambiguities between toponyms and other classes of Named Entity Recognition as well as metonymic usage of toponyms. Toponym resolution in the epidemiology domain was the objective of the SemEval 2019 task 12 (Weissenbacher et al., 2019). The majority of current approaches to toponym detection in the medical domain are based on a combination of rule-based techniques, geographical gazetteer approaches, and deep learning models (Magge et al., 2018; Wang et al., 2019; Li et al., 2019; Qi et al., 2019). In our approach, we aim to develop a toponym detection system solely based on deep learning techniques in order to sidestep many design choices and avoid pattern mining that comes along with rule-based techniques and gazetteer approaches. Contextualized Embeddings Previous attempts to toponym detection in the medical domain have used contextualized embeddings, specifically ELMo (Peters et al., 2018), as the core of their models (Li et al., 2019; Yadav et al., 2019). In this paper, we experiment with a different contextualized embedding model and choose the pretrained BERT model (Devlin et al"
2020.coling-main.58,W03-0419,0,0.110207,"Missing"
2020.coling-main.58,W18-5446,0,0.0420195,"Missing"
2020.coling-main.58,S19-2156,0,0.106714,"toponym detection system should identify Wuhan, Hubei Province, and China as toponyms, and all other words as non-toponyms. Toponym detection tackles ambiguities between toponyms and other classes of Named Entity Recognition as well as metonymic usage of toponyms. Toponym resolution in the epidemiology domain was the objective of the SemEval 2019 task 12 (Weissenbacher et al., 2019). The majority of current approaches to toponym detection in the medical domain are based on a combination of rule-based techniques, geographical gazetteer approaches, and deep learning models (Magge et al., 2018; Wang et al., 2019; Li et al., 2019; Qi et al., 2019). In our approach, we aim to develop a toponym detection system solely based on deep learning techniques in order to sidestep many design choices and avoid pattern mining that comes along with rule-based techniques and gazetteer approaches. Contextualized Embeddings Previous attempts to toponym detection in the medical domain have used contextualized embeddings, specifically ELMo (Peters et al., 2018), as the core of their models (Li et al., 2019; Yadav et al., 2019). In this paper, we experiment with a different contextualized embedding model and choose the"
2020.coling-main.58,S19-2155,0,0.0411725,"Missing"
2020.coling-main.58,S19-2232,0,0.0716015,"ed techniques, geographical gazetteer approaches, and deep learning models (Magge et al., 2018; Wang et al., 2019; Li et al., 2019; Qi et al., 2019). In our approach, we aim to develop a toponym detection system solely based on deep learning techniques in order to sidestep many design choices and avoid pattern mining that comes along with rule-based techniques and gazetteer approaches. Contextualized Embeddings Previous attempts to toponym detection in the medical domain have used contextualized embeddings, specifically ELMo (Peters et al., 2018), as the core of their models (Li et al., 2019; Yadav et al., 2019). In this paper, we experiment with a different contextualized embedding model and choose the pretrained BERT model (Devlin et al., 2019) as the backbone of our network architecture. Linguistic Features Previous works on toponym detection in the medical domain have typically taken advantage of handcrafted features to achieve competitive performance. Most notable features include: (1) orthographic features that capture character level attributes of each token (Magge et al., 2018; Wang et al., 2019; Davari et al., 2019), and (2) part of speech (POS) tags (Wang et al., 2019; Davari et al., 2019;"
2020.jeptalnrecital-taln.7,E17-1012,0,0.026709,"Missing"
2020.jeptalnrecital-taln.7,D14-1179,0,0.0200081,"Missing"
2020.jeptalnrecital-taln.7,N19-1423,0,0.0120574,"Missing"
2020.jeptalnrecital-taln.7,2020.lrec-1.302,0,0.052986,"Missing"
2020.jeptalnrecital-taln.7,D15-1168,0,0.0456459,"Missing"
2020.jeptalnrecital-taln.7,N16-1152,0,0.0363463,"Missing"
2020.lrec-1.134,D18-1465,0,0.013165,"elp towards improving the quality of automaticallygenerated text or detecting authors with specific linguistic deficiencies (Abdalla et al., 2018). In order to perform automatic coherence evaluation, a corpus including both coherent and incoherent samples is needed. Coherent texts, are easy to find; however incoherent texts are not. Most corpora for textual coherence evaluation are synthetic data sets composed of randomly shuffled sentences (Lapata and Barzilay, 2005; Li and Jurafsky, 2017a; Logeswaran et al., 2018) which are commonly used for sentence ordering tasks (Logeswaran et al., 2018; Cui et al., 2018; Gong et al., 2016; Chen et al., 2016). However, these corpora do not consider if the original pairs of sentences are related by a discourse relation or not; hence, the difficulty of the sentence ordering task may vary significantly. To our knowledge, no publicly available corpus exists for coherence evaluation of known discursive units where the sentence pairs are known to have a specific discourse relation. In this paper, we describe our approach to build a corpus of grammatically correct, but incoherent pairs of sentences. We experimented with a variety of corruption strategies to create s"
2020.lrec-1.134,D14-1181,0,0.00266417,"ppropriate to treat intra-discursive coherence evaluation as a regression task instead of a binary classification task. These instances can have different degrees of coherence, rather than being absolutely coherent / incoherent. 5. Conclusion and Future Work In this paper, we highlighted the challenges of building intra-discursive incoherent instances through corruption techniques. We used the Penn Discourse Tree Bank (Prasad et al., 2008) to generate incoherent instances, by swapping either the discourse connective (DC) or Argument 2 (Arg2) of known discursive units. We used the CNN model of Kim (2014) and Denny (2015) to classify these instances, but were unable to reach a performance greater than a random baseline. A manual evaluation through crowdsourcing revealed that the generated corpora were in fact not incoherent enough. The annotations showed that a large percentage of the incoherent samples were actually perceived coherent by the annotators. It also provided evidence that corruption methods for generating incoherent instances based on selecting a discourse argument or discourse connective with a different sense does not seem to significantly reduce coherence. Overall, these result"
2020.lrec-1.134,D14-1218,0,0.0279748,"(2011) experiment with the use of discourse relations for the automatic evaluation of text coherence. In order to have a large collection of texts for training, they create synthetic data from a collection of source documents by permuting their sentences. They design a discourse role matrix which includes occurrences of terms and their discourse roles and use it to model transitions between textual units. They find this approach effective in distinguishing between an original coherent text and a permuted version of that text lacking coherence. Following the same approach as Lin et al. (2011), Li and Hovy (2014) build a synthetic dataset for coherence detection which consists of source documents and their permuted versions (with a different ordering of their sentences). They feed distributed representations of tokens to a recursive neural network which computes sentence representations based on the tree structure of sentences. These distributed sentence representations are later used for coherence detection. Li and Jurafsky (2017b) develop a neural model for coherence evaluation that is trained on a collection of coherent documents and their incoherent permuted versions (similar to the dataset used b"
2020.lrec-1.134,D17-1019,0,0.3654,"r language skills or with health issues affecting language. In these cases, the automatic evaluation of textual coherence can help towards improving the quality of automaticallygenerated text or detecting authors with specific linguistic deficiencies (Abdalla et al., 2018). In order to perform automatic coherence evaluation, a corpus including both coherent and incoherent samples is needed. Coherent texts, are easy to find; however incoherent texts are not. Most corpora for textual coherence evaluation are synthetic data sets composed of randomly shuffled sentences (Lapata and Barzilay, 2005; Li and Jurafsky, 2017a; Logeswaran et al., 2018) which are commonly used for sentence ordering tasks (Logeswaran et al., 2018; Cui et al., 2018; Gong et al., 2016; Chen et al., 2016). However, these corpora do not consider if the original pairs of sentences are related by a discourse relation or not; hence, the difficulty of the sentence ordering task may vary significantly. To our knowledge, no publicly available corpus exists for coherence evaluation of known discursive units where the sentence pairs are known to have a specific discourse relation. In this paper, we describe our approach to build a corpus of gra"
2020.lrec-1.134,P11-1100,0,0.0343637,"rence level by human annotators. In order to automatically evaluate the coherence level of the machine-generated summaries and compare the results with human judgement, they use a syntactic model that takes into account entity transitions to distinguish between coherent and incoherent text, and a semantic model that evaluates coherence by using various measures of semantic similarity between sentences. Based on their experiments, a combined approach that makes use of both syntactic and semantic models outperforms a single one. Assuming that coherent texts exhibit certain discourse structures, Lin et al. (2011) experiment with the use of discourse relations for the automatic evaluation of text coherence. In order to have a large collection of texts for training, they create synthetic data from a collection of source documents by permuting their sentences. They design a discourse role matrix which includes occurrences of terms and their discourse roles and use it to model transitions between textual units. They find this approach effective in distinguishing between an original coherent text and a permuted version of that text lacking coherence. Following the same approach as Lin et al. (2011), Li and"
2020.lrec-1.134,J93-2004,0,0.0779318,"modeling has focused on sentence ordering by creating permutations of source documents with a different ordering of their sentences. This paper goes beyond this as it focuses on coherence modeling at the intra-discursive level by evaluating the coherence between sentence pairs with known discourse relations. 3. 3.1. Methodology Dataset In order to create a corpus of incoherent pairs of sentences, we used the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). The PDTB contains 40,600 annotated discourse connectives along with their discourse arguments. The PDTB follows the DLTAG framework (Marcus et al., 1993) which takes a shallow view of discourse structures where relations are defined only between adjacent sentences or close text spans. The two textual units related by a discourse relation are known as arguments (Arg1 and Arg2). The PDTB annotates the beginning and end of Arg1 and Arg2, a possible discourse connective (DC) (for example, because) and the discourse relation (known as sense). The PDTB contains 18,459 instances with an explicit DC (from an inventory of 100 DCs) and 16,053 instances with an implicit DC where the annotators inferred a DC. Example 5 shows an instance of an implicit dis"
2020.lrec-1.134,W02-1011,0,0.0256301,"Missing"
2020.lrec-1.134,prasad-etal-2008-penn,0,0.757535,"ordering, regardless of whether the sentences were originally related by a discourse relation. To the best of our knowledge, no publicly available corpus has been designed specifically for the evaluation of coherence of known discursive units. In this paper, we focus on coherence modeling at the intra-discursive level and describe our approach to build a corpus of incoherent pairs of sentences. We experimented with a variety of corruption strategies to create synthetic incoherent pairs of discourse arguments from coherent ones. Using discourse argument pairs from the Penn Discourse Tree Bank (Prasad et al., 2008), we generate incoherent discourse argument pairs, by swapping either their discourse connective or a discourse argument. To evaluate how incoherent the generated corpora are, we use a convolutional neural network to try to distinguish the original pairs from the corrupted ones. Results of the classifier as well as a manual inspection of the corpora show that generating such corpora is still a challenge as the generated instances are clearly not “incoherent enough”, indicating that more effort should be spent on developing more robust ways of generating incoherent corpora. Keywords: Discourse"
2020.lrec-1.134,P17-1121,0,0.0330824,"Missing"
2020.lrec-1.615,Q17-1010,0,0.0383967,"e presented below. 4.1. The Input Layer Each recipe includes a title and a preparation section. In order to feed the recipes to the model, first, the title and the preparation sections of each recipe are concatenated. These samples are then fed to an embedder. As shown in Figure 1, the embedder takes in a sample as input, tokenizes it based on the type of embedding to be produced (e.g. contextual versus non-contextual), then outputs a dense vector representation for each token. To create these vector representations, three different types of embedders are used: fastText The fastText embedder (Bojanowski et al., 2017) is used to output 300-dimensional pretrained embeddings. fastText embeddings are trained based on the skipgram model proposed by Mikolov et al. (2013). These embeddings are created by taking into account the morphology of words, instead of treating them as distinct units. Therefore, using this method, each word is represented as a sum of the representations of the character N-grams 5001 Model CNN-fastText GRU-fastText LSTM-fastText CNN-BERT GRU-BERT LSTM-BERT Transformer-fastText CNN-CamemBERT Hyperparameters 300 unigram, 200 bigram, 100 trigram, and 100 4-gram filters, with max pooling two l"
2020.lrec-1.615,D14-1179,0,0.0463686,"Missing"
2020.lrec-1.615,N19-1423,0,0.0399765,"Missing"
2020.lrec-1.615,2020.acl-main.645,0,0.0733801,"Missing"
2020.lrec-1.615,P12-2018,0,0.0438163,"18) performed automatic recipe multilabel classification and feature analysis for a dataset of 5,000 noisy recipes from the web and 87 predefined classes. Su et al. (2014) and Naik and Polamreddi (2015) addressed recipe classification into cuisines based on ingredients using support vector machine (SVM) (Cortes and Vapnik, 1995; Joachims, 1998). The general task of text classification can be found in various use-cases, such as filtering (e.g. spam filtering), information retrieval (e.g., document classification, probabilistic retrieval models), and sentiment analysis (Aggarwal and Zhai, 2012; Wang and Manning, 2012; Yang et al., 2016). These tasks vary from binary to multi-class, to multi-label classifications. Traditional text classification approaches, such as support vector machine (SVM) (Cortes and Vapnik, 1995; Joachims, 1998) and Naive Bayes (Manning et al., 2008; McCallum and Nigam, 1998), rely heavily on feature selection and feature engineering. The most commonly used features are usually human-designed (Lai et al., 2015) such as bag-of-words, n-grams, part-of-speech, and phrases. In general, linear classifiers (such as linear SVM), do not share parameters across features and classes (Joulin et"
2020.lrec-1.615,N16-1174,0,0.0264362,"recipe multilabel classification and feature analysis for a dataset of 5,000 noisy recipes from the web and 87 predefined classes. Su et al. (2014) and Naik and Polamreddi (2015) addressed recipe classification into cuisines based on ingredients using support vector machine (SVM) (Cortes and Vapnik, 1995; Joachims, 1998). The general task of text classification can be found in various use-cases, such as filtering (e.g. spam filtering), information retrieval (e.g., document classification, probabilistic retrieval models), and sentiment analysis (Aggarwal and Zhai, 2012; Wang and Manning, 2012; Yang et al., 2016). These tasks vary from binary to multi-class, to multi-label classifications. Traditional text classification approaches, such as support vector machine (SVM) (Cortes and Vapnik, 1995; Joachims, 1998) and Naive Bayes (Manning et al., 2008; McCallum and Nigam, 1998), rely heavily on feature selection and feature engineering. The most commonly used features are usually human-designed (Lai et al., 2015) such as bag-of-words, n-grams, part-of-speech, and phrases. In general, linear classifiers (such as linear SVM), do not share parameters across features and classes (Joulin et al., 2016). Researc"
2020.msr-1.7,de-marneffe-etal-2014-universal,0,0.0618372,"Missing"
2020.msr-1.7,D19-6302,0,0.0550253,"pically involves three tasks: syntactic realization, morphological realization, and orthographic realization (Reiter and Dale, 2000). Syntactic realization tries to identify the proper ordering of the input data, whereas morphological and orthographic realization are responsible for word inflections, punctuation, and formatting. Several surface realization models presented at the previous Surface Realization Shared Task (Mille et al., 2019) used a cascade of pointer-based models for syntactic realization followed by another neural network module for morphological and orthographic realization (Du and Black, 2019; Farahnak et al., 2019; Mazzei and Basile, 2019). For example, Du et al. (2019) utilized a graph attention network (GAT) (Veliˇckovi´c et al., 2018) for encoding the input sentences and a pointer decoder (Vinyals et al., 2015) to select the next element from their graph. Whereas Yu et al. (2019) used a bidirectional TreeLSTM (Zhou et al., 2016) as the encoder and an LSTM (Hochreiter and Schmidhuber, 1997) as the decoder and multiple LSTM modules for morphological and orthographic realization tasks. These two approaches achieved the highest performance among all participating systems at SR’19."
2020.msr-1.7,D17-1090,0,0.0178705,"ules for morphological and orthographic realization tasks. These two approaches achieved the highest performance among all participating systems at SR’19. On the other hand, when the text to generate is conditioned on the content provided in the form of graphs, tables, etc then data-to-text generation models are utilized. As a family of data-to-text models, graph-to-text generation tries to generate natural text given its input graph. Graph-to-text generation models employ graph encoders to obtain a suitable representation from the input graph. Several applications such as text summarization (Duan et al., 2017), question answering (Fan et al., 2019), as well as surface realization (Du and Black, 2019) have used these types of generation models. Recently, several works have proposed to use language models in the form of text-to-text for what is inherently data-to-text and particularly graph-to-text tasks (Kale, 2020; Mager et al., 2020; Harkous et al., 2020). Instead of modeling the node and edges of the graphs, they mapped the graph structure as sequences of words and let the language model encode the graph information. Kale (2020) takes advantage of T5 (Raffel et al., 2019) for data-to-text problem"
2020.msr-1.7,D19-1428,0,0.0237671,"realization tasks. These two approaches achieved the highest performance among all participating systems at SR’19. On the other hand, when the text to generate is conditioned on the content provided in the form of graphs, tables, etc then data-to-text generation models are utilized. As a family of data-to-text models, graph-to-text generation tries to generate natural text given its input graph. Graph-to-text generation models employ graph encoders to obtain a suitable representation from the input graph. Several applications such as text summarization (Duan et al., 2017), question answering (Fan et al., 2019), as well as surface realization (Du and Black, 2019) have used these types of generation models. Recently, several works have proposed to use language models in the form of text-to-text for what is inherently data-to-text and particularly graph-to-text tasks (Kale, 2020; Mager et al., 2020; Harkous et al., 2020). Instead of modeling the node and edges of the graphs, they mapped the graph structure as sequences of words and let the language model encode the graph information. Kale (2020) takes advantage of T5 (Raffel et al., 2019) for data-to-text problems, whereas Mager et al. (2020) and Hark"
2020.msr-1.7,D19-6308,1,0.844518,"ee tasks: syntactic realization, morphological realization, and orthographic realization (Reiter and Dale, 2000). Syntactic realization tries to identify the proper ordering of the input data, whereas morphological and orthographic realization are responsible for word inflections, punctuation, and formatting. Several surface realization models presented at the previous Surface Realization Shared Task (Mille et al., 2019) used a cascade of pointer-based models for syntactic realization followed by another neural network module for morphological and orthographic realization (Du and Black, 2019; Farahnak et al., 2019; Mazzei and Basile, 2019). For example, Du et al. (2019) utilized a graph attention network (GAT) (Veliˇckovi´c et al., 2018) for encoding the input sentences and a pointer decoder (Vinyals et al., 2015) to select the next element from their graph. Whereas Yu et al. (2019) used a bidirectional TreeLSTM (Zhou et al., 2016) as the encoder and an LSTM (Hochreiter and Schmidhuber, 1997) as the decoder and multiple LSTM modules for morphological and orthographic realization tasks. These two approaches achieved the highest performance among all participating systems at SR’19. On the other hand, whe"
2020.msr-1.7,P16-1154,0,0.0655473,"Missing"
2020.msr-1.7,2020.coling-main.218,0,0.0245793,"t models, graph-to-text generation tries to generate natural text given its input graph. Graph-to-text generation models employ graph encoders to obtain a suitable representation from the input graph. Several applications such as text summarization (Duan et al., 2017), question answering (Fan et al., 2019), as well as surface realization (Du and Black, 2019) have used these types of generation models. Recently, several works have proposed to use language models in the form of text-to-text for what is inherently data-to-text and particularly graph-to-text tasks (Kale, 2020; Mager et al., 2020; Harkous et al., 2020). Instead of modeling the node and edges of the graphs, they mapped the graph structure as sequences of words and let the language model encode the graph information. Kale (2020) takes advantage of T5 (Raffel et al., 2019) for data-to-text problems, whereas Mager et al. (2020) and Harkous et al. (2020) utilize GPT2 (Radford et al., 2019). Following this recent trend of text-to-text models for graph-based problems, we developed an end-toend approach based on a language model for the problem of data-to-text generation. The approach maps the given Universal Dependency structures to surface forms"
2020.msr-1.7,2020.inlg-1.14,0,0.0242518,"ized. As a family of data-to-text models, graph-to-text generation tries to generate natural text given its input graph. Graph-to-text generation models employ graph encoders to obtain a suitable representation from the input graph. Several applications such as text summarization (Duan et al., 2017), question answering (Fan et al., 2019), as well as surface realization (Du and Black, 2019) have used these types of generation models. Recently, several works have proposed to use language models in the form of text-to-text for what is inherently data-to-text and particularly graph-to-text tasks (Kale, 2020; Mager et al., 2020; Harkous et al., 2020). Instead of modeling the node and edges of the graphs, they mapped the graph structure as sequences of words and let the language model encode the graph information. Kale (2020) takes advantage of T5 (Raffel et al., 2019) for data-to-text problems, whereas Mager et al. (2020) and Harkous et al. (2020) utilize GPT2 (Radford et al., 2019). Following this recent trend of text-to-text models for graph-based problems, we developed an end-toend approach based on a language model for the problem of data-to-text generation. The approach maps the given Univer"
2020.msr-1.7,2020.acl-main.703,0,0.135061,"nferred by the systems. Therefor, in addition to determining the order and the inflection of tokens, systems participating in the deep track had to guess the omitted words. Considering that the input data is in the form of Universal Dependency (UD) structure, data-to-text models, and graph-to-text models in particular, seem to be the right choice for the task of surface realization. However, in this study, we take a different path to tackle the problem. Our proposed model is designed based on text-to-text approaches using a pretrained encoder-decoder language model. More specifically, a BART (Lewis et al., 2020) language model is used for the task of surface realization for both the shallow and the deep tracks. The proposed approach is an end-to-end model trained on a linearized representation of the graph of the sentences with their corresponding Universal Dependency This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 57 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 57–63 Barcelona, Spain (Online), December 12, 2020. information. The results on the English datasets"
2020.msr-1.7,2020.tacl-1.47,0,0.0493985,"Missing"
2020.msr-1.7,2020.acl-main.167,0,0.0179121,"amily of data-to-text models, graph-to-text generation tries to generate natural text given its input graph. Graph-to-text generation models employ graph encoders to obtain a suitable representation from the input graph. Several applications such as text summarization (Duan et al., 2017), question answering (Fan et al., 2019), as well as surface realization (Du and Black, 2019) have used these types of generation models. Recently, several works have proposed to use language models in the form of text-to-text for what is inherently data-to-text and particularly graph-to-text tasks (Kale, 2020; Mager et al., 2020; Harkous et al., 2020). Instead of modeling the node and edges of the graphs, they mapped the graph structure as sequences of words and let the language model encode the graph information. Kale (2020) takes advantage of T5 (Raffel et al., 2019) for data-to-text problems, whereas Mager et al. (2020) and Harkous et al. (2020) utilize GPT2 (Radford et al., 2019). Following this recent trend of text-to-text models for graph-based problems, we developed an end-toend approach based on a language model for the problem of data-to-text generation. The approach maps the given Universal Dependency struc"
2020.msr-1.7,D19-6311,0,0.109329,"lization, morphological realization, and orthographic realization (Reiter and Dale, 2000). Syntactic realization tries to identify the proper ordering of the input data, whereas morphological and orthographic realization are responsible for word inflections, punctuation, and formatting. Several surface realization models presented at the previous Surface Realization Shared Task (Mille et al., 2019) used a cascade of pointer-based models for syntactic realization followed by another neural network module for morphological and orthographic realization (Du and Black, 2019; Farahnak et al., 2019; Mazzei and Basile, 2019). For example, Du et al. (2019) utilized a graph attention network (GAT) (Veliˇckovi´c et al., 2018) for encoding the input sentences and a pointer decoder (Vinyals et al., 2015) to select the next element from their graph. Whereas Yu et al. (2019) used a bidirectional TreeLSTM (Zhou et al., 2016) as the encoder and an LSTM (Hochreiter and Schmidhuber, 1997) as the decoder and multiple LSTM modules for morphological and orthographic realization tasks. These two approaches achieved the highest performance among all participating systems at SR’19. On the other hand, when the text to generate is"
2020.msr-1.7,W18-6527,0,0.0234518,"given structured data. This involves both content planning (selecting the content to communicate) and surface realization (selecting, ordering, and inflecting the actual words) (Hovy et al., 1997; Reiter and Dale, 2000). This paper focuses on the second sub-task: Surface Realization (SR). Unlike many tasks in Natural Language Processing (NLP), the performance of SR models are still below human performance. In order to fill this gap and encourage more research in this field, several shared tasks in NLG and SR have been proposed. In particular, since 2018, the Surface Realization Shared Tasks (Mille et al., 2018; Mille et al., 2019; Mille et al., 2020) were introduced to provide common-ground datasets for developing and evaluating NLG systems. This year, the task (Mille et al., 2020) proposed two tracks in several languages including English: 1) Track1: shallow track and 2) Track2: deep track. In the shallow track, unordered and lemmatized tokens with Universal Dependency (UD) structures (de Marneffe et al., 2014) were provided to participants and systems were required to reorder and inflect the tokens to produce final sentences. The deep track was similar to the shallow track but functional words an"
2020.msr-1.7,D19-6301,0,0.0886813,"ta. This involves both content planning (selecting the content to communicate) and surface realization (selecting, ordering, and inflecting the actual words) (Hovy et al., 1997; Reiter and Dale, 2000). This paper focuses on the second sub-task: Surface Realization (SR). Unlike many tasks in Natural Language Processing (NLP), the performance of SR models are still below human performance. In order to fill this gap and encourage more research in this field, several shared tasks in NLG and SR have been proposed. In particular, since 2018, the Surface Realization Shared Tasks (Mille et al., 2018; Mille et al., 2019; Mille et al., 2020) were introduced to provide common-ground datasets for developing and evaluating NLG systems. This year, the task (Mille et al., 2020) proposed two tracks in several languages including English: 1) Track1: shallow track and 2) Track2: deep track. In the shallow track, unordered and lemmatized tokens with Universal Dependency (UD) structures (de Marneffe et al., 2014) were provided to participants and systems were required to reorder and inflect the tokens to produce final sentences. The deep track was similar to the shallow track but functional words and surface-oriented m"
2020.msr-1.7,2020.msr-1.1,0,0.391917,"th content planning (selecting the content to communicate) and surface realization (selecting, ordering, and inflecting the actual words) (Hovy et al., 1997; Reiter and Dale, 2000). This paper focuses on the second sub-task: Surface Realization (SR). Unlike many tasks in Natural Language Processing (NLP), the performance of SR models are still below human performance. In order to fill this gap and encourage more research in this field, several shared tasks in NLG and SR have been proposed. In particular, since 2018, the Surface Realization Shared Tasks (Mille et al., 2018; Mille et al., 2019; Mille et al., 2020) were introduced to provide common-ground datasets for developing and evaluating NLG systems. This year, the task (Mille et al., 2020) proposed two tracks in several languages including English: 1) Track1: shallow track and 2) Track2: deep track. In the shallow track, unordered and lemmatized tokens with Universal Dependency (UD) structures (de Marneffe et al., 2014) were provided to participants and systems were required to reorder and inflect the tokens to produce final sentences. The deep track was similar to the shallow track but functional words and surface-oriented morphological informat"
2020.msr-1.7,D19-6306,0,0.0169628,"s, punctuation, and formatting. Several surface realization models presented at the previous Surface Realization Shared Task (Mille et al., 2019) used a cascade of pointer-based models for syntactic realization followed by another neural network module for morphological and orthographic realization (Du and Black, 2019; Farahnak et al., 2019; Mazzei and Basile, 2019). For example, Du et al. (2019) utilized a graph attention network (GAT) (Veliˇckovi´c et al., 2018) for encoding the input sentences and a pointer decoder (Vinyals et al., 2015) to select the next element from their graph. Whereas Yu et al. (2019) used a bidirectional TreeLSTM (Zhou et al., 2016) as the encoder and an LSTM (Hochreiter and Schmidhuber, 1997) as the decoder and multiple LSTM modules for morphological and orthographic realization tasks. These two approaches achieved the highest performance among all participating systems at SR’19. On the other hand, when the text to generate is conditioned on the content provided in the form of graphs, tables, etc then data-to-text generation models are utilized. As a family of data-to-text models, graph-to-text generation tries to generate natural text given its input graph. Graph-to-tex"
2020.msr-1.7,C16-1274,0,0.028061,"realization models presented at the previous Surface Realization Shared Task (Mille et al., 2019) used a cascade of pointer-based models for syntactic realization followed by another neural network module for morphological and orthographic realization (Du and Black, 2019; Farahnak et al., 2019; Mazzei and Basile, 2019). For example, Du et al. (2019) utilized a graph attention network (GAT) (Veliˇckovi´c et al., 2018) for encoding the input sentences and a pointer decoder (Vinyals et al., 2015) to select the next element from their graph. Whereas Yu et al. (2019) used a bidirectional TreeLSTM (Zhou et al., 2016) as the encoder and an LSTM (Hochreiter and Schmidhuber, 1997) as the decoder and multiple LSTM modules for morphological and orthographic realization tasks. These two approaches achieved the highest performance among all participating systems at SR’19. On the other hand, when the text to generate is conditioned on the content provided in the form of graphs, tables, etc then data-to-text generation models are utilized. As a family of data-to-text models, graph-to-text generation tries to generate natural text given its input graph. Graph-to-text generation models employ graph encoders to obtai"
C14-1058,al-saif-markert-2010-leeds,0,0.0258432,"-grounded discourse annotated corpora. The Penn Discourse Tree Bank (PDTB) (Prasad et al., 2008) is the largest resource to date that provides a discourse annotated corpus in English. In this corpus, discourse relations between two text spans are labeled with a DC. If a discourse relation is expressed without any explicit DC, an inferred DC which conveys the same discourse relation has been inserted between the text spans. This approach has been widely adopted to create discourse tree banks in several other languages such as Turkish (Zeyrek et al., 2010), Chinese (Zhou and Xue, 2012), Arabic (Al-Saif and Markert, 2010), Czech (Mladov´a et al., 2008), and Hindi (Oza et al., 2009). Several work have already investigated the use of discourse relations in machine translation (e.g. (Meyer and Webber, 2013; Meyer, 2011)). Others have attempted to generate discourse annotated corpora from parallel corpora (e.g. (Cartoni, 2013; Meyer, 2011; Popescu-Belis et al., 2012; Versley, 2010; Zhou et al., 2012)). Among these, the most similar approach to ours is Versley (2010) who has projected English DCs to their counterparts in German in a parallel corpus. Doing this, he produced a corpus where discourse vs. non-discourse"
C14-1058,W08-0509,0,0.0133252,". Therefore, two word-alignment models can be produced (i.e. when the target text is French (En2Fr) or when the target text is English (Fr2En)). In addition, Och and Ney (2003) have also presented another word-alignment model called Intersect word-alignment that uses a heuristic to combine En2Fr and Fr2En word alignments. Figure 2 presents the later alignment for two parallel sentences. An alignment between two words is shown by a line connecting them. For example, in these sentences, the connective “therefore” is aligned to the three French words “raison pour laquelle”. We have used MGIZA++ (Gao and Vogel, 2008) to generate En2Fr and Fr2En word-alignments; then used Moses (Koehn et al., 2007) to compute the Intersect word alignment. In this article, we only consider Intersect word-alignment, as it is able to map n-to-m mapping2 . Syntactic Filters: DCs are defined as syntactically well-defined terms (Prasad et al., 2008). The syntactic filters exploit this property and remove any constituent that is not categorized as a DC. In other words, these filters keep only Prepositional Phrases (PP), Coordinate Phrases (CP) or Adverbial Phrases (ADVP). We have implemented two types of Syntactic Filters. The fi"
C14-1058,D11-1067,0,0.0241698,"Missing"
C14-1058,2005.mtsummit-papers.11,0,0.0475417,"discourse connectives. In contrast, we have used and compared three approaches for inducing a DC list: word-alignment, POS patterns and syntactic information. 3 Method Our approach to the extraction of DCs consists of two steps. The first step is the preparation of the parallel corpus with discourse annotations; the next step is the mining of the parallel corpus to identify DCs. 3.1 Preparing the Parallel Corpus Our experiment has focused on building a French list of DCs from English. In order to build the EnglishFrench parallel corpus with discourse annotations, we used the Europarl corpus (Koehn, 2005). The Europarl corpus contains sentence-aligned texts in 21 European languages that have been extracted from the proceeding of the European parliament. For our study, we have only considered the English-French part of this corpus. To label discourse relations in the parallel text, we have automatically parsed the English side of the parallel text and assumed that the same relation existed in the French translation. Although this 611 assumption is not directly addressed in previous work, it has been implicitly used by many (e.g. (Cartoni, 2013; Meyer et al., 2011; Popescu-Belis et al., 2012; Ve"
C14-1058,W13-3303,0,0.078727,"is corpus, discourse relations between two text spans are labeled with a DC. If a discourse relation is expressed without any explicit DC, an inferred DC which conveys the same discourse relation has been inserted between the text spans. This approach has been widely adopted to create discourse tree banks in several other languages such as Turkish (Zeyrek et al., 2010), Chinese (Zhou and Xue, 2012), Arabic (Al-Saif and Markert, 2010), Czech (Mladov´a et al., 2008), and Hindi (Oza et al., 2009). Several work have already investigated the use of discourse relations in machine translation (e.g. (Meyer and Webber, 2013; Meyer, 2011)). Others have attempted to generate discourse annotated corpora from parallel corpora (e.g. (Cartoni, 2013; Meyer, 2011; Popescu-Belis et al., 2012; Versley, 2010; Zhou et al., 2012)). Among these, the most similar approach to ours is Versley (2010) who has projected English DCs to their counterparts in German in a parallel corpus. Doing this, he produced a corpus where discourse vs. non-discourse usage of German DCs were annotated and built a discourse parser from the corpus. Although Versley (2010) used a list of DCs in generating the dataset, he also tried to automatically in"
C14-1058,P11-3009,0,0.122035,"ations between two text spans are labeled with a DC. If a discourse relation is expressed without any explicit DC, an inferred DC which conveys the same discourse relation has been inserted between the text spans. This approach has been widely adopted to create discourse tree banks in several other languages such as Turkish (Zeyrek et al., 2010), Chinese (Zhou and Xue, 2012), Arabic (Al-Saif and Markert, 2010), Czech (Mladov´a et al., 2008), and Hindi (Oza et al., 2009). Several work have already investigated the use of discourse relations in machine translation (e.g. (Meyer and Webber, 2013; Meyer, 2011)). Others have attempted to generate discourse annotated corpora from parallel corpora (e.g. (Cartoni, 2013; Meyer, 2011; Popescu-Belis et al., 2012; Versley, 2010; Zhou et al., 2012)). Among these, the most similar approach to ours is Versley (2010) who has projected English DCs to their counterparts in German in a parallel corpus. Doing this, he produced a corpus where discourse vs. non-discourse usage of German DCs were annotated and built a discourse parser from the corpus. Although Versley (2010) used a list of DCs in generating the dataset, he also tried to automatically induce the DCs f"
C14-1058,mladova-etal-2008-sentence,0,0.0772448,"Missing"
C14-1058,J03-1002,0,0.00428068,"ch corpus. Word-Alignment Filter: This filter removes any DC candidate that does not align with any part of an English DC. In other words, this filter keeps any consecutive words in the French text if at least one of its composing words aligns to at least one word of an English DC when using a word-alignment model. A word-alignment model maps each word in the target text to its translation in the source text (creating an n-to-one mapping). Therefore, two word-alignment models can be produced (i.e. when the target text is French (En2Fr) or when the target text is English (Fr2En)). In addition, Och and Ney (2003) have also presented another word-alignment model called Intersect word-alignment that uses a heuristic to combine En2Fr and Fr2En word alignments. Figure 2 presents the later alignment for two parallel sentences. An alignment between two words is shown by a line connecting them. For example, in these sentences, the connective “therefore” is aligned to the three French words “raison pour laquelle”. We have used MGIZA++ (Gao and Vogel, 2008) to generate En2Fr and Fr2En word-alignments; then used Moses (Koehn et al., 2007) to compute the Intersect word alignment. In this article, we only conside"
C14-1058,W09-3029,0,0.0152372,"DTB) (Prasad et al., 2008) is the largest resource to date that provides a discourse annotated corpus in English. In this corpus, discourse relations between two text spans are labeled with a DC. If a discourse relation is expressed without any explicit DC, an inferred DC which conveys the same discourse relation has been inserted between the text spans. This approach has been widely adopted to create discourse tree banks in several other languages such as Turkish (Zeyrek et al., 2010), Chinese (Zhou and Xue, 2012), Arabic (Al-Saif and Markert, 2010), Czech (Mladov´a et al., 2008), and Hindi (Oza et al., 2009). Several work have already investigated the use of discourse relations in machine translation (e.g. (Meyer and Webber, 2013; Meyer, 2011)). Others have attempted to generate discourse annotated corpora from parallel corpora (e.g. (Cartoni, 2013; Meyer, 2011; Popescu-Belis et al., 2012; Versley, 2010; Zhou et al., 2012)). Among these, the most similar approach to ours is Versley (2010) who has projected English DCs to their counterparts in German in a parallel corpus. Doing this, he produced a corpus where discourse vs. non-discourse usage of German DCs were annotated and built a discourse par"
C14-1058,W11-0821,0,0.0253279,"variables X and Y. Note that in Figure 1, O refers to the observed frequencies, E refers to the expected frequencies and N refers to the total number of observations. 2 2 LLR(X,Y ) = 2 × ∑ ∑ Oi j × log( i=1 j=1 Ei j = ∑2k=1 Oik × ∑2k=1 Ok j N 2 Oi j ) Ei j X =u X = ¬u 2 , N = ∑ ∑ Oi j Y =v O11 O21 Y = ¬v O12 O22 i=1 j=1 Figure 1: The formula used to calculate LLR. In our configuration, our pairs of events consist of the observation of a discourse relation and a DC candidate. We have computed contingency tables of frequencies of these pairs from the French corpus and then used the NSP package (Pedersen et al., 2011) to calculate the LLR for each candidate to rank them. Once the initial list of DCs has been ranked, we have experimented with several types of filters to refine it. Frequency Filter: This simple filter tries to account for the fact that low frequent events may affect the reliability of the LLR measure. Therefore, as a simple baseline filter, we have removed DC candidates that appear less than a certain number of times in the French corpus. Word-Alignment Filter: This filter removes any DC candidate that does not align with any part of an English DC. In other words, this filter keeps any conse"
C14-1058,P09-2004,0,0.330679,"Missing"
C14-1058,popescu-belis-etal-2012-discourse,0,0.155247,"h conveys the same discourse relation has been inserted between the text spans. This approach has been widely adopted to create discourse tree banks in several other languages such as Turkish (Zeyrek et al., 2010), Chinese (Zhou and Xue, 2012), Arabic (Al-Saif and Markert, 2010), Czech (Mladov´a et al., 2008), and Hindi (Oza et al., 2009). Several work have already investigated the use of discourse relations in machine translation (e.g. (Meyer and Webber, 2013; Meyer, 2011)). Others have attempted to generate discourse annotated corpora from parallel corpora (e.g. (Cartoni, 2013; Meyer, 2011; Popescu-Belis et al., 2012; Versley, 2010; Zhou et al., 2012)). Among these, the most similar approach to ours is Versley (2010) who has projected English DCs to their counterparts in German in a parallel corpus. Doing this, he produced a corpus where discourse vs. non-discourse usage of German DCs were annotated and built a discourse parser from the corpus. Although Versley (2010) used a list of DCs in generating the dataset, he also tried to automatically induce the DCs from his corpus. However, Versley (2010) did not explicitly evaluate his list of DCs, but rather focused on his parser. The main difference between o"
C14-1058,prasad-etal-2008-penn,0,0.861608,"y induce discourse connectives. Our approach is based on identifying candidates and ranking them using Log-Likelihood Ratio. Then, it relies on several filters to filter the list of candidates, namely: Word-Alignment, POS patterns, and Syntax. Our experiment to induce French discourse connectives from an English-French parallel text shows that Syntactic filter achieves a much higher MAP value (0.39) than the other filters, when compared with LEXCONN resource. 1 Introduction Discourse relations are often categorized as being implicit or explicit depending on how they are marked linguistically (Prasad et al., 2008). Implicit relations between two text spans are inferred by the reader even if they are not explicitly connected through lexical cues. On the other hand, explicit relations are explicitly identified with syntactically well-defined terms, so called discourse markers or discourse connectives (DCs). A list of DCs is a valuable resource to help the automatic detection of discourse relations in a text. Discourse parsers (e.g. (Lin et al., 2010)) often use DCs as a powerful distinguishing feature to tag discourse relations (Pitler and Nenkova, 2009). A list of DCs is also instrumental in generating"
C14-1058,C10-2118,0,0.206564,"Missing"
C14-1058,P98-2202,0,0.605229,"rank them. These filters include Part-of-speech tags, syntactic tree and word-alignment. Our results show that syntactic information outperforms the other filtering methods for the DC identification task. This paper is organized as follow. Section 2 reviews related work. Section 3 describes our approach to extract DCs from a parallel text. Section 4 reports detailed experimental results, and finally Section 5 presents our conclusion and future work. 2 Related Work Currently, publicly available lists of DCs already exist for English (Knott, 1996), Spanish (Alonso Alemany et al., 2002), German (Stede and Umbach, 1998), and French (Roze et al., 2012). Typically, these lists have been manually constructed by applying systematic linguistic tests to a list of potential DCs. For example, (Roze et al., 2012) gathered a potential list of DCs (about 600 expressions) from English DC translations and various lists of subordinate conjunctions and prepositions. Then, they applied syntactic, semantic, and discourse tests to filter this initial list and identify DCs and their associated relations. A list of DCs can also be created automatically by analyzing lexically-grounded discourse annotated corpora. The Penn Discou"
C14-1058,N03-1033,0,0.0145873,"ther word-alignments but their performances were not better. The Intersect model outperformed the Fr2En word-alignment model and acheived similar results as the En2Fr word-alignment model. 613 French: Le Livre blanc prétend résoudre ces problèmes , raison pour laquelle nous soutenons les English: The White Paper intends to resolve these problems and we therefore support these propositions qu'il contient. proposals. Figure 2: Example of Word-Alignments between English and French Texts.3 French sentences, the Syntax Tree Filter only kept PPs, CPs and ADVPs. We have used the Stanford POS Tagger (Toutanova et al., 2003) and the Stanford PCFG Parser (Green et al., 2011) for POS tagging and parsing the French text, respectively. POS Pattern ADV C P ADV C ADV P CC NP Example alors et comme encore que en outre parce que histoire de POS Pattern P ADV PN PP VC NDP PNP PDN Example apr`es tout par exemple avant de consid´erant que de ce fait de mani`ere a` dans ce cas Table 2: POS Patterns Used in the POS Filter. 3.3 Gold Dataset To evaluate our final ranked list of French DCs candidates and compare the four filters, we have used the LEXCONN dataset (Roze et al., 2012). This manually constructed dataset includes 467"
C14-1058,P12-1008,0,0.0266288,"ically by analyzing lexically-grounded discourse annotated corpora. The Penn Discourse Tree Bank (PDTB) (Prasad et al., 2008) is the largest resource to date that provides a discourse annotated corpus in English. In this corpus, discourse relations between two text spans are labeled with a DC. If a discourse relation is expressed without any explicit DC, an inferred DC which conveys the same discourse relation has been inserted between the text spans. This approach has been widely adopted to create discourse tree banks in several other languages such as Turkish (Zeyrek et al., 2010), Chinese (Zhou and Xue, 2012), Arabic (Al-Saif and Markert, 2010), Czech (Mladov´a et al., 2008), and Hindi (Oza et al., 2009). Several work have already investigated the use of discourse relations in machine translation (e.g. (Meyer and Webber, 2013; Meyer, 2011)). Others have attempted to generate discourse annotated corpora from parallel corpora (e.g. (Cartoni, 2013; Meyer, 2011; Popescu-Belis et al., 2012; Versley, 2010; Zhou et al., 2012)). Among these, the most similar approach to ours is Versley (2010) who has projected English DCs to their counterparts in German in a parallel corpus. Doing this, he produced a corp"
C14-1058,C12-2138,0,0.265362,"een inserted between the text spans. This approach has been widely adopted to create discourse tree banks in several other languages such as Turkish (Zeyrek et al., 2010), Chinese (Zhou and Xue, 2012), Arabic (Al-Saif and Markert, 2010), Czech (Mladov´a et al., 2008), and Hindi (Oza et al., 2009). Several work have already investigated the use of discourse relations in machine translation (e.g. (Meyer and Webber, 2013; Meyer, 2011)). Others have attempted to generate discourse annotated corpora from parallel corpora (e.g. (Cartoni, 2013; Meyer, 2011; Popescu-Belis et al., 2012; Versley, 2010; Zhou et al., 2012)). Among these, the most similar approach to ours is Versley (2010) who has projected English DCs to their counterparts in German in a parallel corpus. Doing this, he produced a corpus where discourse vs. non-discourse usage of German DCs were annotated and built a discourse parser from the corpus. Although Versley (2010) used a list of DCs in generating the dataset, he also tried to automatically induce the DCs from his corpus. However, Versley (2010) did not explicitly evaluate his list of DCs, but rather focused on his parser. The main difference between our work and Versley (2010) is that"
C14-1058,P07-2045,0,\N,Missing
C14-1058,C98-2197,0,\N,Missing
C14-1058,W10-1844,0,\N,Missing
D19-6308,N19-4009,0,0.0553614,"Missing"
D19-6308,N19-1423,0,0.0158446,"equences, using an RNN-based encoder cannot provide a proper context representation for the decoder. 63 Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019), pages 63–67 c Hong Kong, China, November 3rd, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Transformer models constitute an alternative to RNNs as they entirely rely on the self-attention mechanism (Vaswani et al., 2017). Transformer models have achieved state of the art performance in many NLP tasks such as machine translation (Vaswani et al., 2017) and language modeling (Devlin et al., 2019; Radford et al., 2019). The encoder and decoder modules of transformer models consist of multiple layers of stacked self-attention and point-wise fully connected layers. The encoder of the transformer consists in several encoder layers, each of which is composed of two sub-layers. The first sub-layer has a multi-head attention which consists of several layers of self-attention computing on the same input, and the second sub-layer is a feed-forward network. The output of each sub-layer is added with a residual connection from their input followed by a normalization layer. The decoder module co"
D19-6308,D14-1162,0,0.0821118,"tences (Elder and Hokamp, 2018), we developed a similar model using a pointer network integrated with a transformer as its encoder and decoder. As shown in Figure 1, our model is composed of five modules: input embedding, encoder, decoder, pointer, and token generation. In the following, we describe each module in more detail. Dataset 4.1 Input embedding In order to train the model, we embedded each feature separately into vectors and then concatenated them. Table 1 indicates the embedding size of each feature. For the token embeddings, we employed the GloVe pretrained embeddings of size 300 (Pennington et al., 2014), while the remaining feature embeddings are trained from scratch. At the end, the concatenated vector (of size 393) is linearly mapped into the desired embedding size (512 in our case, see Section 5). For the shallow track, training and development sets were provided for 11 different languages. These were taken from the Universal Dependency (UD) datasets (de Marneffe et al., 2014). The correct token order within the sentences was removed by shuffling the tokens. In total, 7 features were provided by the organizers. Out of these features, FEATS contained more than 40 morphological sub-features"
D19-6308,W18-3606,0,0.0248928,"We concatenated all four training sets (en ewt-ud-train, en gum-ud-train, en lines-ud-train and en partut-ud-train) into a single one containing 19,976 sentences, with the longest sentence containing 209 words. Because the development sets provided by the SR’19 organizers are not labeled, we divided the training data in two parts; training (18, 000 sentences) and validation (1, 967 sentences). We removed all sentences longer than 100 tokens for efficiency reasons.1 4 Model Inspired by previous work from SR’18 (Mille et al., 2018) that used pointer networks to reconstruct unordered sentences (Elder and Hokamp, 2018), we developed a similar model using a pointer network integrated with a transformer as its encoder and decoder. As shown in Figure 1, our model is composed of five modules: input embedding, encoder, decoder, pointer, and token generation. In the following, we describe each module in more detail. Dataset 4.1 Input embedding In order to train the model, we embedded each feature separately into vectors and then concatenated them. Table 1 indicates the embedding size of each feature. For the token embeddings, we employed the GloVe pretrained embeddings of size 300 (Pennington et al., 2014), while"
D19-6308,de-marneffe-etal-2014-universal,0,0.0588692,"Missing"
D19-6308,S17-2090,0,0.0313002,"Surface Realization (Hovy et al., 1996; Reiter and Dale, 2000). While Content Planning focuses on selecting the most appropriate content to convey, Surface Realization produces the linear form of the text from this selected data following a given grammar. Although the field of Natural Language Processing (NLP) has witnessed significant progress in the last few years, NLG, and surface realization in particular, still performs significantly below human performance. Recently, several shared tasks have been proposed to improve the state of the art in specific NLG tasks (eg. Duˇsek et al. (2019); May and Priyadarshi (2017)). In particular, the Surface Realization Shared Task 2019 (SR’19) (Mille et al., 2019) aims to provide common-ground datasets for developing and evaluating NLG systems. Similarly to SR’18 (Mille et al., 2018), SR’19 proposed two tracks: a shallow track and a deep track. In the shallow track, unordered and lemmatized tokens with universal dependency (UD) 2 Background Pointer networks are types of encoder-decoder models where the output corresponds to a position in the input sequences (Vinyals et al., 2015). One of the main advantages of pointer networks compared to standard sequence-to-sequenc"
D19-6308,K18-2001,0,0.0366107,"Missing"
D19-6308,D19-6301,0,0.0831353,"s on selecting the most appropriate content to convey, Surface Realization produces the linear form of the text from this selected data following a given grammar. Although the field of Natural Language Processing (NLP) has witnessed significant progress in the last few years, NLG, and surface realization in particular, still performs significantly below human performance. Recently, several shared tasks have been proposed to improve the state of the art in specific NLG tasks (eg. Duˇsek et al. (2019); May and Priyadarshi (2017)). In particular, the Surface Realization Shared Task 2019 (SR’19) (Mille et al., 2019) aims to provide common-ground datasets for developing and evaluating NLG systems. Similarly to SR’18 (Mille et al., 2018), SR’19 proposed two tracks: a shallow track and a deep track. In the shallow track, unordered and lemmatized tokens with universal dependency (UD) 2 Background Pointer networks are types of encoder-decoder models where the output corresponds to a position in the input sequences (Vinyals et al., 2015). One of the main advantages of pointer networks compared to standard sequence-to-sequence models is that the number of output classes depends on the length of the input. This"
davoodi-kosseim-2017-automatic,D08-1020,0,\N,Missing
davoodi-kosseim-2017-automatic,prasad-etal-2008-penn,0,\N,Missing
davoodi-kosseim-2017-automatic,C14-1058,1,\N,Missing
davoodi-kosseim-2017-automatic,P16-1135,0,\N,Missing
davoodi-kosseim-2017-automatic,W16-3620,1,\N,Missing
hooda-kosseim-2017-argument,W04-2703,0,\N,Missing
hooda-kosseim-2017-argument,prasad-etal-2008-penn,0,\N,Missing
hooda-kosseim-2017-argument,K15-2001,0,\N,Missing
hooda-kosseim-2017-argument,D14-1162,0,\N,Missing
hooda-kosseim-2017-argument,K15-2010,0,\N,Missing
hooda-kosseim-2017-argument,D14-1196,0,\N,Missing
hooda-kosseim-2017-argument,D14-1008,0,\N,Missing
hooda-kosseim-2017-argument,K16-2010,0,\N,Missing
hooda-kosseim-2017-argument,K15-2008,1,\N,Missing
I13-1197,P12-1007,0,0.0151605,"ustrate a particular feature about a concept or an entity - e.g. “Picasa makes sure your pictures are always organized.”. The attributive relation, also included in Grimes’ predicates (Grimes, 1975), is considered because it describes attributes or features of an object or event and is often used in query-based summarization and question answering. 3.2 Automatic Discourse Tagging Once the manual analysis identified the most prevalent set of relations, we tried to measure their frequency by tagging them automatically within a larger corpus. Only recently, the HILDA (Hernault et al., 2010) and (Feng and Hirst, 2012)’s discourse parser were made publicly available. Both of these parsers work at the text-level, as opposed to the sentence-level, and hence currently achieve the highest tagging performance when compared to the state of the art. (Feng and Hirst, 2012)’s work showed a significant improvement on the performance of HILDA by enhancing its original feature set. However, at the time this research was done, the only publicly available discourse parser was SPADE (Soricut and Marcu, 2003) which operates on individual sentences. To identify illustration, contingency, comparison, and attribution relation"
I13-1197,W97-0713,0,0.136385,"pare these results to those found with the news articles of the Document Understanding Conference (DUC) 2007 Main task dataset2 . The results show that in both types of texts, discourse relations seem to be as useful: contingency, comparison, and illustration relations provide a statistically significant improvement on the summary content; while the attribution, topic-opinion, and attributive relations do not provide a consistent and significant improvement. 2 Related Work on Discourse Relations for Summarization The use of discourse relations for text summarization is not new. Most notably, (Marcu, 1997) used discourse relations for single document summarization and proposed a discourse relation identification parsing algorithm. In some work (e.g. (Bosma, 2004; Blair-Goldensohn and McKeown, 2006)), discourse relations have been exploited successfully for multi-document summarization. In particular, (Otterbacher et al., 2002) experimentally showed that discourse relations can improve the coherence of multi-document summaries. (Bosma, 2004) showed how discourse relations can be used effectively to incorporate additional contextual information for a given question in a query-based summarization."
I13-1197,W02-0404,0,0.254239,"ns that may or may not be explicitly marked. A text is not a linear combination of textual units but a hierarchial organized group of units placed together based on informational and intentional relations to one another. According to (Taboada, 2006), “Discourse relations - relations that hold together different parts (i.e. proposition, sentence, or paragraph) of the discourse - are partly responsible for the perceived coherence of a text”. For example, Several work have shown that discourse relations can improve the results of summarization in the case of factual texts or news articles (e.g. (Otterbacher et al., 2002)). However, to our knowledge no work has evaluated the usefulness of discourse relations for the summarization of informal and opinionated texts, as those found in the social media. In this paper, we consider the most frequent discourse relations found in blogs: namely comparison, contingency, illustration, attribution, topic-opinion, and attributive and evaluate the effect of each relation on informal text summarization using the Text Analysis Conference (TAC) 1401 International Joint Conference on Natural Language Processing, pages 1401–1409, Nagoya, Japan, 14-18 October 2013. 2008 opinion s"
I13-1197,radev-etal-2004-mead,0,0.106777,"Missing"
I13-1197,N03-1030,0,0.031031,"frequency by tagging them automatically within a larger corpus. Only recently, the HILDA (Hernault et al., 2010) and (Feng and Hirst, 2012)’s discourse parser were made publicly available. Both of these parsers work at the text-level, as opposed to the sentence-level, and hence currently achieve the highest tagging performance when compared to the state of the art. (Feng and Hirst, 2012)’s work showed a significant improvement on the performance of HILDA by enhancing its original feature set. However, at the time this research was done, the only publicly available discourse parser was SPADE (Soricut and Marcu, 2003) which operates on individual sentences. To identify illustration, contingency, comparison, and attribution relations, we have used SPADE discourse parser. However, we have complemented this parser with three other approaches: (Jindal and Liu, 2006)’s approach is used to identify intra-sentence comparison relations; we have designed a tagger based on (Fei et al., 2008)’s approach to identify topicopinion relations; and we have proposed a new approach to tag attributive relations (Mithun, 2012). A description and evaluation of these approaches can be found in (Mithun, 2012). By combining these"
I13-1197,prasad-etal-2008-penn,0,\N,Missing
I13-1197,Y06-1034,0,\N,Missing
K15-2008,bethard-etal-2014-cleartk,0,0.0606706,"Missing"
K15-2008,D14-1008,0,0.177981,"Missing"
K15-2008,N07-1051,0,0.0718929,"Missing"
K15-2008,P09-1077,0,0.165194,"Missing"
K16-2013,P09-1077,0,0.0889254,"urse relation signalled by the annotated discourse connectives with one of the 14 labels specified by the task. This component also uses a C4.5 decision tree classifier (Quinlan, 1993) with the same 10 features used by the Discourse Connective Annotator (see Section 2.1). Discourse Connective Annotator 2.3 The Discourse Connective Annotator annotates discourse connectives within a text. To label discourse connectives, the annotator first searches the input texts for terms that match any of the 100 discourse connectives listed in the Penn Discourse Treebank (Prasad et al., 2008a). Inspired by (Pitler et al., 2009), a C4.5 decision tree binary classifier (Quinlan, 1993) is used to detect if each discourse connective is used in a discourse usage or not. In addition to the six features proposed by (Pitler et al., 2009), this year we also used four of the features proposed by (Lin et al., 2014). In total 10 Discourse Argument Segmenter The goal of the Discourse Argument Segmenter is to detect the discourse argument boundaries. This module first assumes that both discourse arguments (i.e. A RG 1 and A RG 2) are located in the same sentence that contains the discourse connective. If A RG 1 is not found in th"
K16-2013,prasad-etal-2008-penn,0,0.752272,"ctive Sense Labeler labels the discourse relation signalled by the annotated discourse connectives with one of the 14 labels specified by the task. This component also uses a C4.5 decision tree classifier (Quinlan, 1993) with the same 10 features used by the Discourse Connective Annotator (see Section 2.1). Discourse Connective Annotator 2.3 The Discourse Connective Annotator annotates discourse connectives within a text. To label discourse connectives, the annotator first searches the input texts for terms that match any of the 100 discourse connectives listed in the Penn Discourse Treebank (Prasad et al., 2008a). Inspired by (Pitler et al., 2009), a C4.5 decision tree binary classifier (Quinlan, 1993) is used to detect if each discourse connective is used in a discourse usage or not. In addition to the six features proposed by (Pitler et al., 2009), this year we also used four of the features proposed by (Lin et al., 2014). In total 10 Discourse Argument Segmenter The goal of the Discourse Argument Segmenter is to detect the discourse argument boundaries. This module first assumes that both discourse arguments (i.e. A RG 1 and A RG 2) are located in the same sentence that contains the discourse con"
K16-2013,C14-1089,0,0.0194393,"rough the presence of discourse connectives in the text. However, the relation is alternatively lexicalized by some nonconnective expression, hence inserting an implicit discourse connective to express the inferred relation would lead to a redundancy. Introduction Shallow discourse parsing is defined as the identification of two discourse units, or discourse arguments, and labeling their relation. Although the topic of shallow discourse parsing has received much interest in the past few years (e.g. (Zhang et al., 2015; Weiss, 2015; Ji et al., 2015; Rutherford and Xue, 2014; Kong et al., 2014; Feng et al., 2014)), the performance of the state-of-the-art discourse parsers is not yet adequate to be used in other downstream Natural Language Processing applications. For example, the best parser submitted at CoNLL-2015 (Wang and Lan, 2015) achieved an F1 score of 0.2400 on the blind test dataset. 4. EntRel Discourse Relations: EntRel discourse relations are defined between two discourse arguments where only an entity-based coherence relation could be perceived. In this paper, we report on the development and results of our discourse parser for the CoNLL 2016 shared task. As shown in Figure 1, our parser,"
K16-2013,I11-1120,0,0.151863,"course connective to the root of the sentence and classify them into to one of three categories: part-of-A RG 1, part-of-A RG 2 or N ON (i.e. not part of any discourse argument). Then, all constituents which are tagged as part of A RG 1 or as part of A RG 2 are merged to obtain the actual boundaries of A RG 1 and A RG 2. Instead of using integer programming as proposed by Kong et al. (2014), we used a Conditional Random Field (CRF) in order to leverage global information (i.e. information across all constituent candidates). CRFs have been previously used for discourse argument identification (Ghosh et al., 2011) but at the token level. Kong et al. (2014)’s approach generates a sequence of constituents and therefore, CRFs can be applied at the constituent level. We used the following categories of features for the CRF: Discourse Argument Trimmer According to the PDTB manual (Prasad et al., 2008b), annotators should keep the span of two discourse arguments as small as possible and should remove any extra information that is not necessary for the discourse relation. Following this idea, the Discourse Argument Trimmer is a classifier that excludes any constituent from the discourse argument span that is"
K16-2013,E14-1068,0,0.117466,"Missing"
K16-2013,D15-1264,0,0.020463,"y to implicit discourse relations, AltLex are not signalled through the presence of discourse connectives in the text. However, the relation is alternatively lexicalized by some nonconnective expression, hence inserting an implicit discourse connective to express the inferred relation would lead to a redundancy. Introduction Shallow discourse parsing is defined as the identification of two discourse units, or discourse arguments, and labeling their relation. Although the topic of shallow discourse parsing has received much interest in the past few years (e.g. (Zhang et al., 2015; Weiss, 2015; Ji et al., 2015; Rutherford and Xue, 2014; Kong et al., 2014; Feng et al., 2014)), the performance of the state-of-the-art discourse parsers is not yet adequate to be used in other downstream Natural Language Processing applications. For example, the best parser submitted at CoNLL-2015 (Wang and Lan, 2015) achieved an F1 score of 0.2400 on the blind test dataset. 4. EntRel Discourse Relations: EntRel discourse relations are defined between two discourse arguments where only an entity-based coherence relation could be perceived. In this paper, we report on the development and results of our discourse parser f"
K16-2013,D14-1181,0,0.00298527,"rse relation, the Non-Explicit Discourse Relation Annotator first sends each text segment to a binary ConvNet to identify which segments contain a discourse relations and which do not. The Non-Explicit Discourse Relation Annotator trims trailing discourse punctuation as per the shared task requirement. Only discourses with two consecutive arguments are considered as possible non-explicit discourses. Non-discourse segments are removed from the pipeline. Sense labelling is then performed on the remaining segments using a multiclass ConvNet. 3.1 3.2 The network configuration is largely based on (Kim, 2014). We applied a narrow convolution over Q with height w (i.e. w words) and width d (the entire word vector) defined as region h ∈ Rd×w . We added a bias b and applied a nonlinear function f on the convolution to give us features ci , where i is the ith word in the discourse input. This is shown in Formula 1. ci = f (h · Qi:i+w−1 + b) (1) The nonlinear function f in our case was the exponential linear unit (ELU) (Clevert et al., 2016), indicated in Formula 2. ( x if x &gt; 0 f (x) = (2) α(exp(x) − 1) if x ≤ 0 Since the convolution is narrow, there are l − w + 1 such features, giving us a feature ma"
K16-2013,K15-2002,0,0.186787,"uld lead to a redundancy. Introduction Shallow discourse parsing is defined as the identification of two discourse units, or discourse arguments, and labeling their relation. Although the topic of shallow discourse parsing has received much interest in the past few years (e.g. (Zhang et al., 2015; Weiss, 2015; Ji et al., 2015; Rutherford and Xue, 2014; Kong et al., 2014; Feng et al., 2014)), the performance of the state-of-the-art discourse parsers is not yet adequate to be used in other downstream Natural Language Processing applications. For example, the best parser submitted at CoNLL-2015 (Wang and Lan, 2015) achieved an F1 score of 0.2400 on the blind test dataset. 4. EntRel Discourse Relations: EntRel discourse relations are defined between two discourse arguments where only an entity-based coherence relation could be perceived. In this paper, we report on the development and results of our discourse parser for the CoNLL 2016 shared task. As shown in Figure 1, our parser, named CLaC Discourse Parser, consists of two main components: the Explicit Discourse Relation Annotator and the Non-Explicit Discourse Relation Annotator . The Explicit Discourse Relation Annotator is based on the parser that w"
K16-2013,P15-3003,0,0.0198905,"ons: Similarly to implicit discourse relations, AltLex are not signalled through the presence of discourse connectives in the text. However, the relation is alternatively lexicalized by some nonconnective expression, hence inserting an implicit discourse connective to express the inferred relation would lead to a redundancy. Introduction Shallow discourse parsing is defined as the identification of two discourse units, or discourse arguments, and labeling their relation. Although the topic of shallow discourse parsing has received much interest in the past few years (e.g. (Zhang et al., 2015; Weiss, 2015; Ji et al., 2015; Rutherford and Xue, 2014; Kong et al., 2014; Feng et al., 2014)), the performance of the state-of-the-art discourse parsers is not yet adequate to be used in other downstream Natural Language Processing applications. For example, the best parser submitted at CoNLL-2015 (Wang and Lan, 2015) achieved an F1 score of 0.2400 on the blind test dataset. 4. EntRel Discourse Relations: EntRel discourse relations are defined between two discourse arguments where only an entity-based coherence relation could be perceived. In this paper, we report on the development and results of our d"
K16-2013,D14-1008,0,0.196392,"Missing"
K16-2013,K16-2001,0,0.0415603,"n Annotator is based on the parser that we submitted last year to CoNLL 2015 (Laali et al., 2015). For this year’s submission, we improved its components by (1) adding new features (see Section 2 for more details), (2) using a sequence classifier instead of a multiclass classifier in the Discourse Argument Segmenter, and (3) defining a new component, the Discourse Argument Trimmer, to identify attributes and prune discourse arguments. For the CoNLL 2016 task of shallow discourse parsing, four types of discourse relations have to be annotated in texts (more details of the task can be found in (Xue et al., 2016)): 1. Explicit Discourse Relations: explicit discourse relations are explicitly signalled within the text through discourse connectives such as because, however, since, etc. ∗ Both authors contributed equally 92 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 92–99, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Explicit Discourse Relation Annotator Discourse Connective Annotator Discourse Connective Sense Labeler Non-Explicit Discourse Relation Annotator Non-Explicit Relation Labeler Non-Explicit Sense Labele"
K16-2013,K15-2008,1,0.812362,"Missing"
K16-2013,D15-1266,0,0.0165123,"Lex Discourse Relations: Similarly to implicit discourse relations, AltLex are not signalled through the presence of discourse connectives in the text. However, the relation is alternatively lexicalized by some nonconnective expression, hence inserting an implicit discourse connective to express the inferred relation would lead to a redundancy. Introduction Shallow discourse parsing is defined as the identification of two discourse units, or discourse arguments, and labeling their relation. Although the topic of shallow discourse parsing has received much interest in the past few years (e.g. (Zhang et al., 2015; Weiss, 2015; Ji et al., 2015; Rutherford and Xue, 2014; Kong et al., 2014; Feng et al., 2014)), the performance of the state-of-the-art discourse parsers is not yet adequate to be used in other downstream Natural Language Processing applications. For example, the best parser submitted at CoNLL-2015 (Wang and Lan, 2015) achieved an F1 score of 0.2400 on the blind test dataset. 4. EntRel Discourse Relations: EntRel discourse relations are defined between two discourse arguments where only an entity-based coherence relation could be perceived. In this paper, we report on the development and res"
L18-1306,P13-2013,0,0.0155639,"15), coreference resolution (Lee et al., 2017) and cloze-style reading comprehension (Cui et al., 2017). To improve translation, notably for longer sentences, a neural translation model is augmented with an attention mechanism uniquely purposed for capturing alignment (Bahdanau et al., 2015). The alignment model scores how well the input words from the source language match output words in the target language. Inspired by recent advances in the use of attention, we used attention to detect alignment scoring for IDR as word-pair features have be shown to contribute to IDR (Pitler et al., 2009; Biran and McKeown, 2013). However, unlike these methods we make no feature engineering. (R¨onnqvist et al., 2017) also uses an attention mechanism to recognize implicit discourse relations. However, their approach differs from ours in two important ways: in (R¨onnqvist et al., 2017), the two discourse arguments are concatenated to form a single input and the attention mechanism is applied over the entire input, which is fundamentally different to our sequence-tosequence approach. Furthermore, their work is evaluated on the Chinese Discourse Treebank (Zhou and Xue, 2012). 1946 Top Level Temporal Contingency Comparison"
L18-1306,P16-1163,0,0.0285069,"Missing"
L18-1306,D14-1179,0,0.0478526,"Missing"
L18-1306,P17-1055,0,0.0217402,"er et al., 2009; Xue et al., 2015). Given our correlation assumption, we sought a model that could successfully identify and exploit word pairs across arguments that are strong signals of a discourse relation, leading us to explore attention models. Although several neural network approaches have been proposed for IDR, to our knowledge none have investigated the use of encoderdecoder models with attention, an approach successfully applied to many applications including machine translation (Bahdanau et al., 2015), coreference resolution (Lee et al., 2017) and cloze-style reading comprehension (Cui et al., 2017). To improve translation, notably for longer sentences, a neural translation model is augmented with an attention mechanism uniquely purposed for capturing alignment (Bahdanau et al., 2015). The alignment model scores how well the input words from the source language match output words in the target language. Inspired by recent advances in the use of attention, we used attention to detect alignment scoring for IDR as word-pair features have be shown to contribute to IDR (Pitler et al., 2009; Biran and McKeown, 2013). However, unlike these methods we make no feature engineering. (R¨onnqvist et"
L18-1306,D14-1181,0,0.00826964,"oss-argument word-pair alignment statistic in this context. We show that our model, with an F1 score of 38.25, outperforms other approaches on fine-grained classification, while performing comparatively with the state-of-theart on coarse-grained classification. 2. Previous Work Beginning with (Zhang et al., 2015a) and notably in the past year with the CoNLL SDP (Xue et al., 2016), neural network techniques have been used for IDR. Most of these models are based on convolutional neural networks (CNN), inspired by (Zhang et al., 2015a) and other work on sentence classification with CNN (such as (Kim, 2014; Zhang et al., 2015b)). The insight into these many works is that neural networks are better suited at capturing semantic clues between the two arguments of an implicit relation than traditional methods heavily reliant on feature engineering, as in (Pitler et al., 2009; Xue et al., 2015). Given our correlation assumption, we sought a model that could successfully identify and exploit word pairs across arguments that are strong signals of a discourse relation, leading us to explore attention models. Although several neural network approaches have been proposed for IDR, to our knowledge none ha"
L18-1306,D17-1018,0,0.0314597,"thods heavily reliant on feature engineering, as in (Pitler et al., 2009; Xue et al., 2015). Given our correlation assumption, we sought a model that could successfully identify and exploit word pairs across arguments that are strong signals of a discourse relation, leading us to explore attention models. Although several neural network approaches have been proposed for IDR, to our knowledge none have investigated the use of encoderdecoder models with attention, an approach successfully applied to many applications including machine translation (Bahdanau et al., 2015), coreference resolution (Lee et al., 2017) and cloze-style reading comprehension (Cui et al., 2017). To improve translation, notably for longer sentences, a neural translation model is augmented with an attention mechanism uniquely purposed for capturing alignment (Bahdanau et al., 2015). The alignment model scores how well the input words from the source language match output words in the target language. Inspired by recent advances in the use of attention, we used attention to detect alignment scoring for IDR as word-pair features have be shown to contribute to IDR (Pitler et al., 2009; Biran and McKeown, 2013). However, unlike thes"
L18-1306,K16-2014,0,0.0134536,"LSTM and GRU (Cho et al., 2014) cells, opting for LSTM since it showed slightly better results. The number of cell parameters were randomly searched at each training run. We randomly switched between bidirectional encoder or single direction. For the CSA, we additionally performed hyper-parameter search on the number of hidden https://code.google.com/archive/p/word2vec/ Blind 34.18 34.51 35.38 36.75 37.67 35.07 38.25 Test 40.91 39.19 38.20 34.95 36.13 28.05 35.63 Dev 46.40 40.32 46.33 40.72 40.32 36.58 39.42 Table 3: F1 scores of fine-grained IDR compared to top 5 teams. (Wang and Lan, 2016; Mihaylov and Frank, 2016; Qin et al., 2016; Rutherford and Xue, 2016) units. Our main parameters that produced the best performance are listed in Table 2. Our models were optimized with the Adam algorithm (Kingma and Ba, 2015). Models evaluated on the test sets are based on optimal validation set F1 score. 6. Results & Analysis Given the unbalanced datasets, performance is evaluated solely on F1 scores. Table 3, summarizes our top-level classification results on the PDTB dataset in comparison with other authors and Table 4 our fine-grained classification results4 on the CoNLL SDP dataset. As shown in Table 3, our CA"
L18-1306,W12-1614,0,0.0147248,"n states are used for classification. Note that there is no backpropagation through time from output predictions at each time step. 1949 Author Pitler Zhou Park Rutherford Ji Chen CSA CA Comp. 21.96 31.79 31.32 39.70 35.93 40.17 27.02 30.56 Cont. 47.13 47.16 49.82 54.42 52.78 54.76 49.86 54.80 Exp. 76.42 70.11 79.22 80.44 80.02 80.62 77.45 80.72 Temp. 16.76 20.30 26.57 28.69 27.63 31.32 24.43 27.15 Table 4: F1 scores of top-level IDR for: comparison, contingency, expansion, temporal. Note that entrel is merged into expansion, as done in previous works. (Pitler et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014; Ji and Eisenstein, 2015; Chen et al., 2016) We were surprised by the CSA’s lower performance in all cases. We believed the model would be more robust if the classification layer had inputs from all decoded hidden states directly. However, using only the final state vector resulted in higher classification score while using less parameters. This may be due to overfitting. We would need to reevaluate the model on a larger dataset. 7. Conclusion We presented an efficient encoder-decoder model with attention for implicit discourse relation recognition. Our model compute"
L18-1306,P09-1077,0,0.44141,". 2. Previous Work Beginning with (Zhang et al., 2015a) and notably in the past year with the CoNLL SDP (Xue et al., 2016), neural network techniques have been used for IDR. Most of these models are based on convolutional neural networks (CNN), inspired by (Zhang et al., 2015a) and other work on sentence classification with CNN (such as (Kim, 2014; Zhang et al., 2015b)). The insight into these many works is that neural networks are better suited at capturing semantic clues between the two arguments of an implicit relation than traditional methods heavily reliant on feature engineering, as in (Pitler et al., 2009; Xue et al., 2015). Given our correlation assumption, we sought a model that could successfully identify and exploit word pairs across arguments that are strong signals of a discourse relation, leading us to explore attention models. Although several neural network approaches have been proposed for IDR, to our knowledge none have investigated the use of encoderdecoder models with attention, an approach successfully applied to many applications including machine translation (Bahdanau et al., 2015), coreference resolution (Lee et al., 2017) and cloze-style reading comprehension (Cui et al., 201"
L18-1306,prasad-etal-2008-penn,0,0.0862506,"950 4185 2832 8861 16828 with a single classifier. Additionally, the WSJ section breakdown is different compared to the top-level dataset. The SDP training set consists of WSJ sections 2-21, section 22 for development, and section 23 for testing. 4. Table 1: Top-level breakdown of the PDTB with entrel merged into expansion 3. 3.1. Datasets & Tasks Following the standard in the field, we used both the PDTB and the CoNLL SDP datasets. The PDTB dataset (Rashmi Prasad, 2008) contains 40,600 annotated discourse relations and their arguments over the 1 million word Wall Street Journal (WSJ) corpus (Prasad et al., 2008). The dataset includes four top-level classes of discourse relations; temporal, contingency, comparison and expansion; as well as level 2 and lever 3 types. For example, in the PDTB: (3) USAir has great promise.By the second half of 1990, USAir stock could hit 60. is labeled as “Contingency.Cause.Reason”. A fifth toplevel relation, entrel (short for entity-based coherence), is also defined but has no lower-level types. Table 1 shows statistics of the PDTB dataset. The CoNLL SDP dataset consists of the full PDTB dataset with a minor reduction in the number of subtypes (Xue et al., 2016). Additi"
L18-1306,K16-2010,0,0.0219499,"2014) cells, opting for LSTM since it showed slightly better results. The number of cell parameters were randomly searched at each training run. We randomly switched between bidirectional encoder or single direction. For the CSA, we additionally performed hyper-parameter search on the number of hidden https://code.google.com/archive/p/word2vec/ Blind 34.18 34.51 35.38 36.75 37.67 35.07 38.25 Test 40.91 39.19 38.20 34.95 36.13 28.05 35.63 Dev 46.40 40.32 46.33 40.72 40.32 36.58 39.42 Table 3: F1 scores of fine-grained IDR compared to top 5 teams. (Wang and Lan, 2016; Mihaylov and Frank, 2016; Qin et al., 2016; Rutherford and Xue, 2016) units. Our main parameters that produced the best performance are listed in Table 2. Our models were optimized with the Adam algorithm (Kingma and Ba, 2015). Models evaluated on the test sets are based on optimal validation set F1 score. 6. Results & Analysis Given the unbalanced datasets, performance is evaluated solely on F1 scores. Table 3, summarizes our top-level classification results on the PDTB dataset in comparison with other authors and Table 4 our fine-grained classification results4 on the CoNLL SDP dataset. As shown in Table 3, our CA model scored 38.25"
L18-1306,P17-2040,0,0.102472,"Missing"
L18-1306,E14-1068,0,0.0823007,"n state ht = f (xt , ht−1 ). Functions f and q are nonlinearities, in our case Bidirectional RNN (Schuster and Paliwal, 1997) of type long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997). Normally, the decoder predicts a sequence of words yt where each yt prediction is conditioned on past predictions and context vector c, maximizing the following joint probability: Tasks Given the difficulty of automatic IDR, most work focuses only on top-level classification; i.e. classifying only the four top-level relations with entrel merged into expansion as preferred by (Pitler et al., 2009; Rutherford and Xue, 2014; Ji and Eisenstein, 2015). The standard WSJ section breakdown is to use sections 2-20 for training, sections 21-22 for testing, and the other sections for development. Given the unbalanced dataset, as shown in Table 1, the task has traditionally been formulated as four binary classifiers. For the development and test sets, the negative samples consist of all other relations. The training set is evenly balanced between positive and negative where negatives samples are randomly drawn from WSJ sections 2 to 20 (excluding positives). A notable exception to only top-level IDR was the 2015 and 2016"
L18-1306,K16-2007,0,0.0264811,"Missing"
L18-1306,K16-2004,0,0.0128091,"We experiment using LSTM and GRU (Cho et al., 2014) cells, opting for LSTM since it showed slightly better results. The number of cell parameters were randomly searched at each training run. We randomly switched between bidirectional encoder or single direction. For the CSA, we additionally performed hyper-parameter search on the number of hidden https://code.google.com/archive/p/word2vec/ Blind 34.18 34.51 35.38 36.75 37.67 35.07 38.25 Test 40.91 39.19 38.20 34.95 36.13 28.05 35.63 Dev 46.40 40.32 46.33 40.72 40.32 36.58 39.42 Table 3: F1 scores of fine-grained IDR compared to top 5 teams. (Wang and Lan, 2016; Mihaylov and Frank, 2016; Qin et al., 2016; Rutherford and Xue, 2016) units. Our main parameters that produced the best performance are listed in Table 2. Our models were optimized with the Adam algorithm (Kingma and Ba, 2015). Models evaluated on the test sets are based on optimal validation set F1 score. 6. Results & Analysis Given the unbalanced datasets, performance is evaluated solely on F1 scores. Table 3, summarizes our top-level classification results on the PDTB dataset in comparison with other authors and Table 4 our fine-grained classification results4 on the CoNLL SDP dataset. As"
L18-1306,K15-2001,0,0.10037,"ginning with (Zhang et al., 2015a) and notably in the past year with the CoNLL SDP (Xue et al., 2016), neural network techniques have been used for IDR. Most of these models are based on convolutional neural networks (CNN), inspired by (Zhang et al., 2015a) and other work on sentence classification with CNN (such as (Kim, 2014; Zhang et al., 2015b)). The insight into these many works is that neural networks are better suited at capturing semantic clues between the two arguments of an implicit relation than traditional methods heavily reliant on feature engineering, as in (Pitler et al., 2009; Xue et al., 2015). Given our correlation assumption, we sought a model that could successfully identify and exploit word pairs across arguments that are strong signals of a discourse relation, leading us to explore attention models. Although several neural network approaches have been proposed for IDR, to our knowledge none have investigated the use of encoderdecoder models with attention, an approach successfully applied to many applications including machine translation (Bahdanau et al., 2015), coreference resolution (Lee et al., 2017) and cloze-style reading comprehension (Cui et al., 2017). To improve tran"
L18-1306,K16-2001,0,0.0582674,"pus (Prasad et al., 2008). The dataset includes four top-level classes of discourse relations; temporal, contingency, comparison and expansion; as well as level 2 and lever 3 types. For example, in the PDTB: (3) USAir has great promise.By the second half of 1990, USAir stock could hit 60. is labeled as “Contingency.Cause.Reason”. A fifth toplevel relation, entrel (short for entity-based coherence), is also defined but has no lower-level types. Table 1 shows statistics of the PDTB dataset. The CoNLL SDP dataset consists of the full PDTB dataset with a minor reduction in the number of subtypes (Xue et al., 2016). Additionally, the SDP dataset includes a blind test set, a second test set created specifically for the 2015 and 2016 editions of the shared task. The blind test set consists of newswire text selected from English Wikinews1 consistent with WSJ-style text and manually annotated with discourse relations and connectives (Xue et al., 2015). 3.2. p(y) = T Y p(yt |{y1 , . . . , yt−1 }, c) (1) t=1 In the context of RNNs, the conditional probability of each yt in the joint probability of Eq.1 is modeled as a nonlinear function g with input yt , context vector c and hidden state st : p(yt |{y1 , . ."
L18-1306,D15-1266,0,0.0295849,"Missing"
L18-1306,P12-1008,0,0.0258755,"to contribute to IDR (Pitler et al., 2009; Biran and McKeown, 2013). However, unlike these methods we make no feature engineering. (R¨onnqvist et al., 2017) also uses an attention mechanism to recognize implicit discourse relations. However, their approach differs from ours in two important ways: in (R¨onnqvist et al., 2017), the two discourse arguments are concatenated to form a single input and the attention mechanism is applied over the entire input, which is fundamentally different to our sequence-tosequence approach. Furthermore, their work is evaluated on the Chinese Discourse Treebank (Zhou and Xue, 2012). 1946 Top Level Temporal Contingency Comparison Expansion Total Nb Implicit Instances 950 4185 2832 8861 16828 with a single classifier. Additionally, the WSJ section breakdown is different compared to the top-level dataset. The SDP training set consists of WSJ sections 2-21, section 22 for development, and section 23 for testing. 4. Table 1: Top-level breakdown of the PDTB with entrel merged into expansion 3. 3.1. Datasets & Tasks Following the standard in the field, we used both the PDTB and the CoNLL SDP datasets. The PDTB dataset (Rashmi Prasad, 2008) contains 40,600 annotated discourse r"
L18-1306,C10-2172,0,0.0263268,". The decoder hidden states are used for classification. Note that there is no backpropagation through time from output predictions at each time step. 1949 Author Pitler Zhou Park Rutherford Ji Chen CSA CA Comp. 21.96 31.79 31.32 39.70 35.93 40.17 27.02 30.56 Cont. 47.13 47.16 49.82 54.42 52.78 54.76 49.86 54.80 Exp. 76.42 70.11 79.22 80.44 80.02 80.62 77.45 80.72 Temp. 16.76 20.30 26.57 28.69 27.63 31.32 24.43 27.15 Table 4: F1 scores of top-level IDR for: comparison, contingency, expansion, temporal. Note that entrel is merged into expansion, as done in previous works. (Pitler et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014; Ji and Eisenstein, 2015; Chen et al., 2016) We were surprised by the CSA’s lower performance in all cases. We believed the model would be more robust if the classification layer had inputs from all decoded hidden states directly. However, using only the final state vector resulted in higher classification score while using less parameters. This may be due to overfitting. We would need to reevaluate the model on a larger dataset. 7. Conclusion We presented an efficient encoder-decoder model with attention for implicit discourse relation recogni"
laali-kosseim-2017-improving,C10-2118,0,\N,Missing
laali-kosseim-2017-improving,J93-2003,0,\N,Missing
laali-kosseim-2017-improving,W12-3102,0,\N,Missing
laali-kosseim-2017-improving,popescu-belis-etal-2012-discourse,0,\N,Missing
laali-kosseim-2017-improving,P07-2045,0,\N,Missing
laali-kosseim-2017-improving,W10-1703,0,\N,Missing
laali-kosseim-2017-improving,prasad-etal-2008-penn,0,\N,Missing
laali-kosseim-2017-improving,2005.mtsummit-papers.11,0,\N,Missing
laali-kosseim-2017-improving,C14-1058,1,\N,Missing
laali-kosseim-2017-improving,W15-1824,0,\N,Missing
laali-kosseim-2017-improving,P16-1135,0,\N,Missing
laali-kosseim-2017-improving,W13-3303,0,\N,Missing
R11-1066,P03-1069,0,0.0302673,"or this summary would be 4-3-1-2 or 4-3-2-1. Figure 1: A Sample Summary Topic: Carmax Question: What motivated positive opinions of Carmax from car buyers? Summary: (1) It’s like going to disney world for car buyers. (2) have to say that Carmax rocks. (3) We bought it at Carmax, and I continue to have nothing bad to say about that company. (4) After our last big car milestone, we’ve had an odyssey with cars. A summary with poor coherence confuses the readers and degrades the quality and readability of the summary. The proper sentence order significantly improves the readability of summaries. (Lapata, 2003) experimentally showed that the time to read a summary strongly correlates with the arrangement of sentences. 1.2 State of the Art Currently, most of the automatic summarization systems for news articles use an extractive approach. In general, this approach works in two steps: in the first step, the most salient sentences are extracted from the source documents and in the second step, these sentences are ordered to create a summary. Since in the first step, sentences may be selected from multiple documents or without consideration to their interdependency with other sentences this may cause te"
R11-1066,W97-0713,0,0.219897,"Missing"
R11-1066,W02-0404,0,0.0881412,"Missing"
R11-1066,D10-1007,0,0.0502647,"Missing"
R11-1066,N03-1030,0,0.104028,"dentification In our approach, candidate sentences need to be classified into a predefined set of rhetorical predicates to fill the various slots of the matched schema - we called this process predicate identification. In (Mithun and Kosseim, 2011), we have introduced a domain independent approach to identify which rhetorical predicates are conveyed by a sentence. As specified in (Mithun and Kosseim, 2011), predicates can describe a single proposition or the relation between propositions. To identify the predicates between propositions - e.g. evidence, we have used the SPADE discourse parser (Soricut and Marcu, 2003). On the other hand, in order to identify predicates within a single proposition - e.g. attributive, we have used three other taggers: comparison (Jindal and Liu, 2006), topicopinion (Fei et al., 2008), and our attributive tagger (Mithun and Kosseim, 2011). By combining these approaches, a sentence is tagged with all possible predicates that it may contain and ready to be used in a schema. 3.1.4 3. Context: To improve discourse coherence further, if a potential sentence starts with a pronoun without having a potential antecedent, we include its previous sentence from the source document as a c"
R11-1066,N04-1015,0,0.144567,"Missing"
R11-1066,C08-1019,0,0.19499,"Missing"
R11-1066,P09-1024,0,\N,Missing
R11-1066,Y06-1034,0,\N,Missing
R13-1080,N09-1003,0,0.0603657,"Missing"
R13-1080,D07-1061,0,0.0285969,"t also all other available semantic relations found in WordNet in addition to word definitions. On the other hand, corpus-based approaches rely mainly on distributional properties of words learned from a large corpus to compute semantic relatedness. Such as the work of Finkelstein et al. (2001) that used Latent Semantic Analysis, and the work of Strube and Ponzetto (2006) and Gabrilovich and Markovitch (2007), which both used the distributional hypothesis on Wikipedia. Finally, hybrid approaches use a combination of corpus-based and lexicon-based methods. For example, the approach proposed by Hughes and Ramage (2007) used a random walk method over a lexicon-based semantic graph supplemented with corpus-based probabilities. Another example is the work of Agirre et al. (2009) that used a supervised machine learning approach to combine three methods: WordNet-based similarity, a bag of word based similarity, and a context window based similarity. The approach presented in this paper belongs to the lexicon-based category. However, as opposed to the typical lexicon-based approaches described above, our approach uses all 26 semantic relations found in WordNet in addition to information found in glosses. The nove"
R13-1080,O97-1002,0,0.169128,"Missing"
R13-1080,N03-1032,0,0.0571687,"Missing"
R13-1080,J07-2002,0,0.0841818,"Missing"
R13-1080,2003.mtsummit-papers.42,0,0.18072,"Missing"
R13-1080,P94-1019,0,\N,Missing
R19-1091,P17-1067,0,0.0620362,"Missing"
R19-1091,S18-1053,0,0.0466622,"Missing"
R19-1091,D18-1147,0,0.0401872,"Missing"
R19-1091,S19-2005,0,0.0252718,"Missing"
R19-1091,D14-1179,0,0.0467491,"Missing"
R19-1091,N16-2011,0,0.0275136,"Missing"
R19-1091,P18-1256,0,0.0199643,"ted using Equation 2, where yt is the output of the hidden layer at time-step t, and fy is the function that calculates the output value based on ht . ht = fh (xt , ht±1 ) (1) yt = fy (ht ) (2) The Attention Layer is a function that automatically assigns weights to the output of the recurrent layer at each time-step, and calculates the weighted sum of the outputs using their corresponding weights (Vaswani et al., 2017). Following several works that have shown significant improvement in text classification with the use of attention (e.g. Yang et al., 2016; Zhou et al., 2016; Wang et al., 2016; Cianflone et al., 2018), we incorporated an attention mechanism in our contextual emotion detection framework. Equation 3 shows the overall mechanism of our attention layer, where ωt′ represents the corresponding weight for the output of the recurrent layer at timestep t′ in, and n is the number of time-steps (i.e. the length of the dialogue turn). As shown in Figure 1, the neural feature extractor is a recurrent neural network with an attention mechanism. The feature extractor is responsible for creating dense vector representations for each dialogue turn. As a result, the model uses 3 feature extractors, one for e"
R19-1091,J93-2004,0,0.0645567,"ith two types of classifiers at the output layer: A fullyconnected neural network, followed by a softmax activation function, and an SVM, which takes as input the neural representations generated by the 3 latent feature extractors for each dialogue turn. The neural classifier is trained jointly with the neural feature extractors, while the SVM classifier is completely trained after each training epoch of the neural network, using the features extracted by the 3 neural feature extractors. 5 5.2 The spaCy library4 was used for tokenization and POS tagging, and the Penn Treebank tagset standard (Marcus et al., 1993) was followed for assigning POS tags to tokens. This lead to one-hot vectors for POS information of size 51. 5.3 Recurrent Units Long Short-term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Units (GRU) (Cho et al., 2014) were both experimented with as the building blocks of the recurrent layer. For both LSTM and GRU, 2 layers of 25 bidirectional recurrent units were stacked. Experimental Setup The neural network components of our model were developed using PyTorch (Paszke et al., 2017) and the SVM was developed using the Scikit-learn library (Pedregosa et al., 2011). In"
R19-1091,L16-1624,0,0.0237639,"2-056-4_091 focused on domain-specific emotion detection. They created a dataset of 2107 sentences taken from online forums on the Cancer Survivors Network website1 . In order to combine the strengths of lexicon-based and machine learning approaches, they proposed a model that uses word2vec embeddings as input to a Convolutional Neural Network (CNN) (LeCun et al., 1999). The CNN generates feature vectors which are then augmented with domain-specific lexical features. The combined features are then used as input to an LSTM network which classifies the texts into 6 different emotion categories. Dini and Bittar (2016) broke down the task of emotion detection from tweets into a cascade of decisions: classifying tweets into emotional and non-emotional categories, and then tagging the emotional tweets with the appropriate emotion label. For the latter, they compared a symbolic system using gazetteers, regular expressions, and graph transformation, with a machine learning system using a linear classifier with words, lemmas, noun phrases, and dependencies as features. Using their collected corpus of emotional tweets, the rule-based approach achieved an F1 score of 0.41, while the machine learning approach yield"
R19-1091,S17-1007,0,0.0437755,"Missing"
R19-1091,S18-1001,0,0.0610056,"Missing"
R19-1091,D16-1058,0,0.0295028,"en layer is calculated using Equation 2, where yt is the output of the hidden layer at time-step t, and fy is the function that calculates the output value based on ht . ht = fh (xt , ht±1 ) (1) yt = fy (ht ) (2) The Attention Layer is a function that automatically assigns weights to the output of the recurrent layer at each time-step, and calculates the weighted sum of the outputs using their corresponding weights (Vaswani et al., 2017). Following several works that have shown significant improvement in text classification with the use of attention (e.g. Yang et al., 2016; Zhou et al., 2016; Wang et al., 2016; Cianflone et al., 2018), we incorporated an attention mechanism in our contextual emotion detection framework. Equation 3 shows the overall mechanism of our attention layer, where ωt′ represents the corresponding weight for the output of the recurrent layer at timestep t′ in, and n is the number of time-steps (i.e. the length of the dialogue turn). As shown in Figure 1, the neural feature extractor is a recurrent neural network with an attention mechanism. The feature extractor is responsible for creating dense vector representations for each dialogue turn. As a result, the model uses 3 feat"
R19-1091,N16-1174,0,0.0209118,". Subsequently, the output of the hidden layer is calculated using Equation 2, where yt is the output of the hidden layer at time-step t, and fy is the function that calculates the output value based on ht . ht = fh (xt , ht±1 ) (1) yt = fy (ht ) (2) The Attention Layer is a function that automatically assigns weights to the output of the recurrent layer at each time-step, and calculates the weighted sum of the outputs using their corresponding weights (Vaswani et al., 2017). Following several works that have shown significant improvement in text classification with the use of attention (e.g. Yang et al., 2016; Zhou et al., 2016; Wang et al., 2016; Cianflone et al., 2018), we incorporated an attention mechanism in our contextual emotion detection framework. Equation 3 shows the overall mechanism of our attention layer, where ωt′ represents the corresponding weight for the output of the recurrent layer at timestep t′ in, and n is the number of time-steps (i.e. the length of the dialogue turn). As shown in Figure 1, the neural feature extractor is a recurrent neural network with an attention mechanism. The feature extractor is responsible for creating dense vector representations for each dialogue tu"
R19-1091,D14-1162,0,0.0807412,"Missing"
R19-1091,P16-2034,0,0.0147939,"output of the hidden layer is calculated using Equation 2, where yt is the output of the hidden layer at time-step t, and fy is the function that calculates the output value based on ht . ht = fh (xt , ht±1 ) (1) yt = fy (ht ) (2) The Attention Layer is a function that automatically assigns weights to the output of the recurrent layer at each time-step, and calculates the weighted sum of the outputs using their corresponding weights (Vaswani et al., 2017). Following several works that have shown significant improvement in text classification with the use of attention (e.g. Yang et al., 2016; Zhou et al., 2016; Wang et al., 2016; Cianflone et al., 2018), we incorporated an attention mechanism in our contextual emotion detection framework. Equation 3 shows the overall mechanism of our attention layer, where ωt′ represents the corresponding weight for the output of the recurrent layer at timestep t′ in, and n is the number of time-steps (i.e. the length of the dialogue turn). As shown in Figure 1, the neural feature extractor is a recurrent neural network with an attention mechanism. The feature extractor is responsible for creating dense vector representations for each dialogue turn. As a result, th"
R19-1091,N18-1202,0,0.0096226,"h dialogue turn, and n is the length of the i-th turn. The vector representation for each token (xi,t ) is composed of the word embedding corresponding to the token, concatenated with a one-hot representation of the token’s part-of-speech (POS) tag. n Attention = ∑ yt′ ωt′ (3) t′ =1 In our model, the weights are calculated by applying a single N -to-1 feed-forward layer on 787 Figure 1: Architecture of the model. et al., 2014), which is pretrained on 840B tokens of web data from Common Crawl, and provides 300d vectors as word embeddings. As our second word embedder, we experimented with ELMo (Peters et al., 2018), which produces word embeddings of size 1024, and is pretrained on the 1 Billion Word Language Model Benchmark3 (Chelba et al., 2014). The main reason for choosing these two word embedders was to evaluate the effect of their embedding mechanisms for our task. As opposed to GloVe which assigns a word embedding to each token, the ELMo word embedder calculates the embedding for each token from its constituent characters by also taking into account its textual context. We suspected that this approach would lead to better results in our task (see Section 6). the output of the recurrent layer at ea"
razmara-kosseim-2008-answering,N03-1022,0,\N,Missing
S13-2019,D11-1129,0,0.0377917,"Missing"
S13-2019,W99-0501,0,0.0319051,"of concepts are a suitable indicator of their semantic relatedness. The type of relations considered includes not only the hy108 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 108–113, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Figure 1: Example of the semantic network around the word car. ponym/hypernym relations but also all 26 available semantic relations found in WordNet in addition to relations extracted from each of the eXtended WordNet (Harabagiu et al., 1999) synset’s logical form. To implement our idea, we created a weighted and directed semantic network based on the relations of WordNet and eXtended WordNet. We used WordNet’s words and synsets as the nodes of the network. Each word is connected by an edge to its synsets, and each synset is in turn connected to other synsets based on the semantic relations included in WordNet. In addition each synset is connected by a labeled edge to the predicate arguments that are extracted from the eXtended WordNet synset’s logical form. Every synset in the eXtended WordNet is related to a logical form, which"
S13-2019,P94-1019,0,\N,Missing
S13-2019,P08-1028,0,\N,Missing
S19-2023,P82-1020,0,0.788949,"Missing"
S19-2023,N16-2011,0,0.051809,"Missing"
S19-2023,W14-6905,0,0.045662,"Missing"
S19-2023,J93-2004,0,0.0647421,"sent to the neural network by using their word embeddings, concatenated to their one-hot part-ofspeech (POS) tag. For word embeddings, we used a pretrained ELMo word embedder (Peters et al., 2018), which extracts the embeddings of each token in a textual context from their constituent characters. The suitability of ELMo for the current task lies in its ability to take into consideration the context of the tokens when generating the word embeddings, and also the handling of out-of-vocabulary words. We used pretrained ELMo embeddings of size 1024. We followed the Penn Treebank tagset standard (Marcus et al., 1993) for assigning POS tags to each token. For this, we used spaCy1 for tokenization and POS tagging of the data. (4) ω = Sof tmax([ν1 , ν2 , ν3 , . . . , νN ]) (5) attn = N X ωt0 yt0 (6) t0 =1 The Classifier The outputs of the attention layers from the three utterances are concatenated, and fed to a fully-connected feed-forward layer, which uses a softmax activation function at the end. The output of the classifier includes a vector of 4 reals, which represent the estimated probability for each of the 4 classes (happy, sad, angry, and others). Optimization Technique Cross-entropy is used as the l"
S19-2023,S19-2005,0,0.0789853,"results by Volkova and Bachrach (2016). Although much research has focused on the detection of emotions in tweets and blog posts (e.g. Mohammad, 2012; Desmet and Hoste, 2013; Liew and Turtle, 2016), emotion detection in dialogues, as well as in single utterances has received very little attention. This topic can have significant impact for the development of social chatbots, with the aim of creating an emotional connection between a user and a chatbot (Banchs, 2017). Task 3 of SemEval 2019 (EmoContext) has focussed on contextual emotion detection over 4 classes: happy, sad, angry, and others (Chatterjee et al., 2019b). We participated in this shared task under the name CLaC Lab and used a combination of artificial neural networks (with recurrent units and attention mechanism) and Support Vector Machines (SVM) to address this multi-class classification task. Introduction Automatic emotion detection has been the focus of much research in a variety of fields, including emotion detection based on images (Rao et al., 2019), speech signals (Davletcharova et al., 2015), electroencephalography (EEG) signals (Ackermann et al., 2016), and texts (Tafreshi and Diab, 2018). With the advent of social media, emotion de"
S19-2023,S12-1033,0,0.0996913,"Missing"
S19-2023,D14-1179,0,0.0544159,"Missing"
S19-2023,P18-1256,0,0.0125371,"consecutive utterances of a dialogue between two interlocutors. We consider each utterance as a sequence of tokens (words). As a result, each utterance is represented as a vector, such as [xi,1 , xi,2 , . . . , xi,t , . . . , xi,n ], where xi,t is the vector representation of the t-th word in the i-th utterance, and n is the length of the i-th utternace. Although originally developed for the task of machine translation (Bahdanau et al., 2014), attention mechanisms have been shown to significantly improve text classification tasks (e.g. Yang et al., 2016; Zhou et al., 2016; Wang et al., 2016; Cianflone et al., 2018). Following these works, we used attention in our system (see Figure 1). Classification Once the neural network has created a representation of the input, a final feedforward classification network, which takes as input the concatenated vectors from the attention units of the three utterances, performs the classification task. The Recurrent Component The input layer is followed by a bidirectional hidden recurrent component. Each utterance is fed to a separate hidden component, who is responsible to process that specific utterance in a forward and backward pass. For the forward pass, the conten"
S19-2023,D14-1162,0,0.082039,"nd as a result, more penalty had to be assigned to errors during training as opposed to trying to achieve larger decision margins. We tested the system using two different configurations: 1) Using the neural network for feature extraction and classification (NN); and 2) Using the neural network for feature extraction and the SVM for classification (NN+SVM). We compared the two systems with the baseline system provided by the EmoContext organizers (Chatterjee et al., 2019a), which uses a neural network with LSTM units (Hochreiter and Schmidhuber, 1997) in the hidden layer and GloVe embeddings (Pennington et al., 2014). Table 1 shows the results from our two models in comparison to the baseline configuration. System NN+SVM NN Baseline angry 0.7130 0.6206 N/A happy 0.6667 0.6374 N/A sad 0.7443 0.6800 N/A 6 This paper presented the system that we developed for our participation to SemEval 2019, Task 3 (EmoContext). The task focused on detecting four classes of emotions, happy, sad, angry, and others in a dataset consisting of small dialogues between two people. For this task, we developed a system that used pretrained ELMo word embeddings alongside POS tags as input features to a bidirectional GRU, followed b"
S19-2023,N18-1202,0,0.0156045,"ed information on the final system’s architecture. 154 Figure 1: The overall framework of the neural network model. 3.1 The Neural Network represents the number of time-steps for each utterance, i.e. the length of the utterance). We developed the neural network using PyTorch (Paszke et al., 2017). Detailed explanations of the neural network’s architecture are provided below. Input Features Each word in each utterance is sent to the neural network by using their word embeddings, concatenated to their one-hot part-ofspeech (POS) tag. For word embeddings, we used a pretrained ELMo word embedder (Peters et al., 2018), which extracts the embeddings of each token in a textual context from their constituent characters. The suitability of ELMo for the current task lies in its ability to take into consideration the context of the tokens when generating the word embeddings, and also the handling of out-of-vocabulary words. We used pretrained ELMo embeddings of size 1024. We followed the Penn Treebank tagset standard (Marcus et al., 1993) for assigning POS tags to each token. For this, we used spaCy1 for tokenization and POS tagging of the data. (4) ω = Sof tmax([ν1 , ν2 , ν3 , . . . , νN ]) (5) attn = N X ωt0 y"
S19-2023,P16-2034,0,0.0296957,"1, each input sample consists of three consecutive utterances of a dialogue between two interlocutors. We consider each utterance as a sequence of tokens (words). As a result, each utterance is represented as a vector, such as [xi,1 , xi,2 , . . . , xi,t , . . . , xi,n ], where xi,t is the vector representation of the t-th word in the i-th utterance, and n is the length of the i-th utternace. Although originally developed for the task of machine translation (Bahdanau et al., 2014), attention mechanisms have been shown to significantly improve text classification tasks (e.g. Yang et al., 2016; Zhou et al., 2016; Wang et al., 2016; Cianflone et al., 2018). Following these works, we used attention in our system (see Figure 1). Classification Once the neural network has created a representation of the input, a final feedforward classification network, which takes as input the concatenated vectors from the attention units of the three utterances, performs the classification task. The Recurrent Component The input layer is followed by a bidirectional hidden recurrent component. Each utterance is fed to a separate hidden component, who is responsible to process that specific utterance in a forward and bac"
S19-2023,C18-1246,0,0.0297523,"Missing"
S19-2023,P16-1148,0,0.0399927,"Missing"
S19-2023,D16-1058,0,0.0264235,"e consists of three consecutive utterances of a dialogue between two interlocutors. We consider each utterance as a sequence of tokens (words). As a result, each utterance is represented as a vector, such as [xi,1 , xi,2 , . . . , xi,t , . . . , xi,n ], where xi,t is the vector representation of the t-th word in the i-th utterance, and n is the length of the i-th utternace. Although originally developed for the task of machine translation (Bahdanau et al., 2014), attention mechanisms have been shown to significantly improve text classification tasks (e.g. Yang et al., 2016; Zhou et al., 2016; Wang et al., 2016; Cianflone et al., 2018). Following these works, we used attention in our system (see Figure 1). Classification Once the neural network has created a representation of the input, a final feedforward classification network, which takes as input the concatenated vectors from the attention units of the three utterances, performs the classification task. The Recurrent Component The input layer is followed by a bidirectional hidden recurrent component. Each utterance is fed to a separate hidden component, who is responsible to process that specific utterance in a forward and backward pass. For the"
S19-2023,N16-1174,0,0.0589418,"As shown in Figure 1, each input sample consists of three consecutive utterances of a dialogue between two interlocutors. We consider each utterance as a sequence of tokens (words). As a result, each utterance is represented as a vector, such as [xi,1 , xi,2 , . . . , xi,t , . . . , xi,n ], where xi,t is the vector representation of the t-th word in the i-th utterance, and n is the length of the i-th utternace. Although originally developed for the task of machine translation (Bahdanau et al., 2014), attention mechanisms have been shown to significantly improve text classification tasks (e.g. Yang et al., 2016; Zhou et al., 2016; Wang et al., 2016; Cianflone et al., 2018). Following these works, we used attention in our system (see Figure 1). Classification Once the neural network has created a representation of the input, a final feedforward classification network, which takes as input the concatenated vectors from the attention units of the three utterances, performs the classification task. The Recurrent Component The input layer is followed by a bidirectional hidden recurrent component. Each utterance is fed to a separate hidden component, who is responsible to process that specific utterance i"
W04-0833,J98-1001,0,0.0319487,"Missing"
W04-0833,C02-1039,0,0.100155,"of the earliest WSD experiments and showed that the accuracy of sense resolution does not improve when more than four words around the target are considered (Ide and V´eronis, 1998). While researchers such as Masterman (1961), Gougenheim and Michea (1961), agree with this observation (Ide and V´eronis, 1998), our results demonstrate that this does not generally apply to all words. A large context window provides domain information which increases the accuracy for some target words such as bank.n, but not others like different.a or use.v (see Section 3). This confirms Mihalcea’s observations (Mihalcea, 2002). In our system we allow a larger context window size and for most of the words such context window is selected by the system. Another trend consists in defining and using semantic preferences for the target word. For example, the verb drink prefers an animate subject in its imbibe sense. Boguraev shows that this does not work for polysemous verbs because of metaphoric expressions (Ide and V´eronis, 1998). Furthermore, the grammatical structures the target word takes part in can be used as a distinguishing tool: “the word ‘keep’, can be disambiguated by determining whether its object is gerund"
W04-0833,W96-0208,0,0.0299178,"rd. For example, the verb drink prefers an animate subject in its imbibe sense. Boguraev shows that this does not work for polysemous verbs because of metaphoric expressions (Ide and V´eronis, 1998). Furthermore, the grammatical structures the target word takes part in can be used as a distinguishing tool: “the word ‘keep’, can be disambiguated by determining whether its object is gerund (He kept eating), adjectival phrase (He kept calm), or noun phrase (He kept a record)” (Reifler, 1955). In our second system we approximate the syntactic structures of a word, in its different senses. Mooney (Mooney, 1996) has discussed the effect of bias on inductive learning methods. In this work we also show sensitivity of Na¨ıve Bayes to the distribution of samples. 3 Na¨ıve Bayes for Learning Context Words In our approach, a large window and a smaller sub-window are centered around the target word. We account for all words within the subwindow but use a POS filter as well as a short stop-word list to filter out non-content words Figure 1: The effect of choosing different window and sub-window sizes for the word bank.n. The best accuracy is achieved with a window and sub-window size of around 450 and 50 cha"
W04-0833,W97-0323,0,0.0550672,"Missing"
W09-4301,W04-1013,0,0.00294214,"uestions they are interested in, instead of retrieving an entire document. At the TAC 2008 opinion summarization track, a set of target topics on various events or entities were 2 Evaluation Evaluation of blog summaries use the same criteria as for traditional news text summarization. The quality of a summary is assessed mostly on its content and linguistic quality [14]. Content evaluation of a querybased summary is performed based on the relevance assessment (with the topic and query) and inclusion of important contents from the input documents. Currently, the automatic evaluation tool ROUGE [11] is the most popular evaluation approach for content evaluation. ROUGE automatically compares system generated summaries with a set of model summaries (human generated) by computing n-gram word overlaps between them. Conferences and workshops such as TAC and DUC (Document Understanding Conference) [2] use ROUGE. The pyramid method [18] is also used for content evaluation. In the pyramid method, multiple human generated summaries are analyzed manually to generated a gold standard. In this process, summary analysis is done semantically such that information with the same meaning (expressed using"
W09-4301,N04-1019,0,0.044869,"content and linguistic quality [14]. Content evaluation of a querybased summary is performed based on the relevance assessment (with the topic and query) and inclusion of important contents from the input documents. Currently, the automatic evaluation tool ROUGE [11] is the most popular evaluation approach for content evaluation. ROUGE automatically compares system generated summaries with a set of model summaries (human generated) by computing n-gram word overlaps between them. Conferences and workshops such as TAC and DUC (Document Understanding Conference) [2] use ROUGE. The pyramid method [18] is also used for content evaluation. In the pyramid method, multiple human generated summaries are analyzed manually to generated a gold standard. In this process, summary analysis is done semantically such that information with the same meaning (expressed using different wording) is marked as summary content unit (SCU). A weight is assigned for each SCU based on the number of human summarizers that express it in their summaries. In this method, the pyramid score for a system generated summary is calculated as follows [17]: score = (the sum of weights of SCUs expressed in a generated summary)"
W09-4301,W02-1011,0,0.0105106,"s. 6 6.1 Related Work NLP on blogs Recently, the availability of opinions on current events on weblogs opened up new directions in natural language research. Even though natural language processing on blogs is a fairly new trend, its popularity is growing rapidly. Many conferences and workshops (e.g. [1, 3, 4, 15]) are taking place to address different aspects of the analysis of blog entries. Current NLP work on blog entries include: subjectivity and sentiment analysis; question answering; and opinion summarization. Subjectivity and sentiment analysis include classifying sentiments of reviews [19] and analyzing blogger mood and sentiment on various events [16]. Sentiment classification of reviews on different events is often done on movie or product reviews. Rating indicators of reviews are used to identify the polarity of the blogs namely positive, negative or neutral. To analyze blogger mood and sentiment, systems make use of information regarding bloggers’ mood varying over time. To record bloggers’ varying mood, the polarity information of the blog post is often used. Some works (e.g. [16]) are done to measure how bloggers’ varying mood affects different events. In addition, the TR"
W12-2606,radev-etal-2004-mead,0,0.118842,"Missing"
W12-2606,C08-1019,0,0.0353421,"Missing"
W12-2606,R11-1066,1,0.854292,"sion. 3 Table 3: Automatic Evaluation of MEAD based on Summary Content on TAC 2008 System MEAD Average OList BlogSum We have designed an extractive query-based summrizer called BlogSum. In BlogSum, we have developed our own sentence extractor to retrieve the initial list of candidate sentences (we called it OList) based on question similarity, topic similarity, and subjectivity scores. Given a set of initial candidate sentences, BlogSum generates summaries using discourse relations within a schema-based framework. Details of BlogSum is outside the scope of this paper. For details, please see (Mithun and Kosseim, 2011). 4 In our evaluation, BlogSum-generated summaries were compared with the original candidate list generated by our approach without the discourse reordering (OList). However, we have validated our original candidate list with a publicly available sentence ranker. Specifically, we have conducted an experiment to verify whether MEAD-generated summaries (Radev et al., 2004), a widely used publicly available summarizer5 , were better than our candidate list (OList). In this evaluation, we have generated summaries using MEAD with centroid, query title, and query narrative features. In MEAD, query t"
W12-2606,W04-1013,0,\N,Missing
W16-3620,P11-2117,0,0.205363,"rder, context and adjacency. Such discourse information plays an important role in text complexity assessment. Traditional methods do not consider the flow of information in terms of word ordering, phrase adjacency and connection between text segments; all of which can make a text hard to follow, noncoherent and more complex. 3.1 Data Sets To perform the experiments, we created two different data sets using standard corpora. The first data set was created from the Penn Discourse Treebank (PDTB) (Prasad et al., 2008); while, the other was created from the Simple English Wikipedia (SEW) corpus (Coster and Kauchak, 2011). These two data sets are described below and summarized in Table 1. 3.1.1 The PDTB-based Data Set Since we aimed to analyze the contribution of different features, we needed a corpus with different complexity levels where features were already annotated or could automatically be tagged. Surface, lexical, syntactic and cohesion features can be easily extracted; however, coherence features are more difficult to extract. Standard resources typically used in computational complexity analysis such as the Simple English Wikipedia (Coster and Kauchak, 2011), Common Core Appendix B1 and Weebit (Vajja"
W16-3620,D08-1020,0,0.196917,"l by using different language models (e.g. a language model for children using children’s book, a language model for more advanced readers using scientific papers, etc.). Discourse features can refer to text cohesion and coherence. Text cohesion refers to the grammatical and lexical links which connect linguistic entities together; whereas text coherence refers to the connection between ideas. Several theories have been developed to model both cohesion (e.g. centering theory (Grosz et al., 1995)) and coherence (e.g. Rhetorical Structure Theory (Mann and Thompson, 1987), DLTAG (Webber, 2004)). Pitler and Nenkova (2008) examined a set 1 167 https://www.engageny.org Source # of pairs of articles # of positive pairs # of negative pairs Discourse Annotation PDTB-based Data Set Penn Discourse Treebank Corpus 378 194 184 Manually Annotated SEW-based Data Set Simple English Wikipedia Corpus 1988 944 944 Extracted using End-to-End parser (Lin et al., 2014) Table 1: Summary of the two data sets. in Example 2.b by removing the discourse marker because. of the articles is indicated on a scale of 1.0 (easy) to 5.0 (difficult). Using this set of articles, we built a data set containing pairs of articles whose complexity"
W16-3620,P09-2004,0,0.129899,"Surface, lexical, syntactic and cohesion features can be easily extracted; however, coherence features are more difficult to extract. Standard resources typically used in computational complexity analysis such as the Simple English Wikipedia (Coster and Kauchak, 2011), Common Core Appendix B1 and Weebit (Vajjala and Meurers, 2012) are not annotated with coherence information; hence these features would have to be induced automatically using a discourse parser (e.g. Lin et al. (2014), Laali et al. (2015)). In order to have better quality discourse annotations, we used the data set generated by Pitler and Nenkova (2009). This data set contains 30 articles from the PDTB (Prasad et al., 2008) which are annotated manually with both complexity level and discourse information. The complexity level More recently, some efforts have been made to improve text complexity assessment by considering richer linguistic features. For example, Schwarm and Ostendorf (2005) and Callan and Eskenazi (2007) used language models to predict readability level by using different language models (e.g. a language model for children using children’s book, a language model for more advanced readers using scientific papers, etc.). Discour"
W16-3620,J95-2003,0,0.590857,"r example, Schwarm and Ostendorf (2005) and Callan and Eskenazi (2007) used language models to predict readability level by using different language models (e.g. a language model for children using children’s book, a language model for more advanced readers using scientific papers, etc.). Discourse features can refer to text cohesion and coherence. Text cohesion refers to the grammatical and lexical links which connect linguistic entities together; whereas text coherence refers to the connection between ideas. Several theories have been developed to model both cohesion (e.g. centering theory (Grosz et al., 1995)) and coherence (e.g. Rhetorical Structure Theory (Mann and Thompson, 1987), DLTAG (Webber, 2004)). Pitler and Nenkova (2008) examined a set 1 167 https://www.engageny.org Source # of pairs of articles # of positive pairs # of negative pairs Discourse Annotation PDTB-based Data Set Penn Discourse Treebank Corpus 378 194 184 Manually Annotated SEW-based Data Set Simple English Wikipedia Corpus 1988 944 944 Extracted using End-to-End parser (Lin et al., 2014) Table 1: Summary of the two data sets. in Example 2.b by removing the discourse marker because. of the articles is indicated on a scale of"
W16-3620,prasad-etal-2008-penn,0,0.679074,"o account. Webber and Joshi (2012) define discourse using fours aspects: position of constitutes, order, context and adjacency. Such discourse information plays an important role in text complexity assessment. Traditional methods do not consider the flow of information in terms of word ordering, phrase adjacency and connection between text segments; all of which can make a text hard to follow, noncoherent and more complex. 3.1 Data Sets To perform the experiments, we created two different data sets using standard corpora. The first data set was created from the Penn Discourse Treebank (PDTB) (Prasad et al., 2008); while, the other was created from the Simple English Wikipedia (SEW) corpus (Coster and Kauchak, 2011). These two data sets are described below and summarized in Table 1. 3.1.1 The PDTB-based Data Set Since we aimed to analyze the contribution of different features, we needed a corpus with different complexity levels where features were already annotated or could automatically be tagged. Surface, lexical, syntactic and cohesion features can be easily extracted; however, coherence features are more difficult to extract. Standard resources typically used in computational complexity analysis su"
W16-3620,C10-1062,0,0.0339603,"Missing"
W16-3620,P13-1151,0,0.0176126,"nguistic and surface features used for this task. Results show that with both data sets coherence features are more correlated to text complexity than the other types of features. In addition, feature selection revealed that with both data sets the top most discriminating feature is a coherence feature. 1 Introduction 2 Measuring text complexity is a crucial step in automatic text simplification where various aspects of a text need to be simplified in order to make it more accessible (Siddharthan, 2014). Despite much research on identifying and resolving lexical and syntactic complexity (e.g. Kauchak (2013), Rello et al. (2013), Bott et al. (2012), Carroll et al. ˇ (1998), Barlacchi and Tonelli (2013), Stajner et al. (2013)), discourse-level complexity remain understudied (Siddharthan, 2006; Siddharthan, 2003). Current approaches to text complexity assessment consider a text as a bag of words or a bag of syntactic constituents; which is not powerful enough to take into account deeper textual aspects such as flow of ideas, inconsistencies, etc. that can influence text complexity. For example, according to Williams et al. (2003), Example 1.a below is more complex than Example 1.b even though both"
W16-3620,P05-1065,0,0.0589261,"t annotated with coherence information; hence these features would have to be induced automatically using a discourse parser (e.g. Lin et al. (2014), Laali et al. (2015)). In order to have better quality discourse annotations, we used the data set generated by Pitler and Nenkova (2009). This data set contains 30 articles from the PDTB (Prasad et al., 2008) which are annotated manually with both complexity level and discourse information. The complexity level More recently, some efforts have been made to improve text complexity assessment by considering richer linguistic features. For example, Schwarm and Ostendorf (2005) and Callan and Eskenazi (2007) used language models to predict readability level by using different language models (e.g. a language model for children using children’s book, a language model for more advanced readers using scientific papers, etc.). Discourse features can refer to text cohesion and coherence. Text cohesion refers to the grammatical and lexical links which connect linguistic entities together; whereas text coherence refers to the connection between ideas. Several theories have been developed to model both cohesion (e.g. centering theory (Grosz et al., 1995)) and coherence (e.g"
W16-3620,W03-2314,0,0.0604711,"lection revealed that with both data sets the top most discriminating feature is a coherence feature. 1 Introduction 2 Measuring text complexity is a crucial step in automatic text simplification where various aspects of a text need to be simplified in order to make it more accessible (Siddharthan, 2014). Despite much research on identifying and resolving lexical and syntactic complexity (e.g. Kauchak (2013), Rello et al. (2013), Bott et al. (2012), Carroll et al. ˇ (1998), Barlacchi and Tonelli (2013), Stajner et al. (2013)), discourse-level complexity remain understudied (Siddharthan, 2006; Siddharthan, 2003). Current approaches to text complexity assessment consider a text as a bag of words or a bag of syntactic constituents; which is not powerful enough to take into account deeper textual aspects such as flow of ideas, inconsistencies, etc. that can influence text complexity. For example, according to Williams et al. (2003), Example 1.a below is more complex than Example 1.b even though both sentences use exactly the same nouns and verbs. Background A reader may find a text easy to read, cohesive, coherent, grammatically and lexically sound or on the other hand may find it complex, hard to follo"
W16-3620,K15-2008,1,0.888151,"Missing"
W16-3620,N03-1033,0,0.0139197,"Missing"
W16-3620,W12-2019,0,0.02258,"2011). These two data sets are described below and summarized in Table 1. 3.1.1 The PDTB-based Data Set Since we aimed to analyze the contribution of different features, we needed a corpus with different complexity levels where features were already annotated or could automatically be tagged. Surface, lexical, syntactic and cohesion features can be easily extracted; however, coherence features are more difficult to extract. Standard resources typically used in computational complexity analysis such as the Simple English Wikipedia (Coster and Kauchak, 2011), Common Core Appendix B1 and Weebit (Vajjala and Meurers, 2012) are not annotated with coherence information; hence these features would have to be induced automatically using a discourse parser (e.g. Lin et al. (2014), Laali et al. (2015)). In order to have better quality discourse annotations, we used the data set generated by Pitler and Nenkova (2009). This data set contains 30 articles from the PDTB (Prasad et al., 2008) which are annotated manually with both complexity level and discourse information. The complexity level More recently, some efforts have been made to improve text complexity assessment by considering richer linguistic features. For ex"
W16-3620,W12-3205,0,0.0264931,"ith surface features such as word length (the number of characters or number of syllables per word) or sentence length. One of the most well-known readability indexes, the Flesch-Kincaid index (Kincaid et al., 1975), measures a text’s complexity level and maps it to an educational level. Traditional complexity measures (e.g. (Chall, 1958; Klare and others, 1963; Zakaluk and Samuels, 1988)) mostly consider a text as a bag of words or bag of sentences and rely on the complexity of a text’s building blocks (e.g. words or phrases). This perspective does not take discourse properties into account. Webber and Joshi (2012) define discourse using fours aspects: position of constitutes, order, context and adjacency. Such discourse information plays an important role in text complexity assessment. Traditional methods do not consider the flow of information in terms of word ordering, phrase adjacency and connection between text segments; all of which can make a text hard to follow, noncoherent and more complex. 3.1 Data Sets To perform the experiments, we created two different data sets using standard corpora. The first data set was created from the Penn Discourse Treebank (PDTB) (Prasad et al., 2008); while, the o"
W16-3620,W03-2317,0,0.290466,"research on identifying and resolving lexical and syntactic complexity (e.g. Kauchak (2013), Rello et al. (2013), Bott et al. (2012), Carroll et al. ˇ (1998), Barlacchi and Tonelli (2013), Stajner et al. (2013)), discourse-level complexity remain understudied (Siddharthan, 2006; Siddharthan, 2003). Current approaches to text complexity assessment consider a text as a bag of words or a bag of syntactic constituents; which is not powerful enough to take into account deeper textual aspects such as flow of ideas, inconsistencies, etc. that can influence text complexity. For example, according to Williams et al. (2003), Example 1.a below is more complex than Example 1.b even though both sentences use exactly the same nouns and verbs. Background A reader may find a text easy to read, cohesive, coherent, grammatically and lexically sound or on the other hand may find it complex, hard to follow, grammatically heavy or full of uncommon words. Focusing only on textual characteristics and ignoring the influence of the readers, Siddharthan (2014) defines text complexity as a metric to measure linguistic complexities at different levels of analysis: 1) lexical (e.g. the use of less frequent, uncommon and even obsol"
W16-4831,N10-1027,0,0.0679037,"Missing"
W16-4831,W15-5413,0,0.433683,"Missing"
W16-4831,L16-1284,0,0.249367,"Missing"
W16-4831,W15-5407,0,0.153305,"Missing"
W16-4831,W16-4801,0,0.10777,"Missing"
W16-4831,W15-5401,0,0.198081,"Missing"
W17-5501,C14-1058,1,0.808737,"urse connectives on machine translation systems (Meyer, 2011; Meyer et al., 2011; Cartoni et al., 2013) or on a small number of discourse connectives due to the cost of manual annotations ´ (Taboada and de los Angeles G´omez-Gonz´alez, 2012; Zufferey and Degand, 2014; Zufferey and Cartoni, 2014; Zufferey and Gygax, 2015; Hoek and Zufferey, 2015). To our knowledge, very little research has addressed the automatic construction of lexicons of DCs. Hidey and McKeown (2016) proposed an automatic approach to identify English expressions that signal the C AUSAL discourse relation. On the other hand, Laali and Kosseim (2014) automatically extracted French DCs from parallel texts; however, they did not associate discourse relations to the extracted DCs. The proposed approach goes beyond this work by mapping DCs to their associated discourse relations. 3 3.2 To label French DCs with a PDTB discourse relation, we assumed that if a French DC is aligned to an English DC tagged with a discourse relation Rel, then it should signal the same discourse relation Rel. For our experiment, we used the inventory of 100 English DCs from the PDTB (Prasad et al., 2008a) and the 371 French DCs from LEXCONN V2.1 (Danlos et al., 2015"
W17-5501,J93-2003,0,0.064347,"7 ≤ 50 55 &gt; 50 309 Total 371 Table 1: Distribution of LEXCONN French DCs in the Europarl corpus. We used the Moses statistical machine translation system (Koehn et al., 2007) to extract the number of alignments between French DCs and English DCs. As part of its translation model, Moses generates a phrase table (see Table 2) which aligns phrases between the language pairs. The phrase table is constructed based on statistical word alignment models and contains the frequency of the alignments between phrase pairs. We used the Och and Ney (2003) heuristic and combined IBM Model 4 word alignments (Brown et al., 1993) to construct the phrase table. Because an English DC can signal different discourse relations, to ensure that Moses’s phrase table distinguishes the different usages of the same English DC, we modified its English tokenizer so that each English DC and its discourse relation make up a single token. For example, the token 2,007,723 to be exact. 2 ‘although-C ONCESSION’ will be created for the DC although when it signals the discourse relation C ONCESSION. Table 2 shows a few entries of the phrase table for the French DC mˆeme si. As the table shows, mˆeme si was aligned to three English DCs: al"
W17-5501,P11-3009,0,0.0406786,"Missing"
W17-5501,2015.jeptalnrecital-court.6,0,0.0225131,"ives (DCs) (e.g. because, although) are terms that explicitly signal discourse relations within a text. Building a lexicon of DCs, where each connective is mapped to the discourse relations it can signal, is not an easy task. To build such lexicons, it is necessary to have linguists manually analyse the usage of individual DCs through a corpus study, which is an expensive endeavour both in terms of time and expertise. For example, LEXCONN (Roze et al., 2012), a manually built lexicon of French DCs, was initiated in 2010 and released its first edition in 2012. The latest version, LEXCONN V2.1 (Danlos et al., 2015), contains 343 DCs mapped to an average of 1.3 discourse relations. This project is still ongoing as 37 DCs still have not been assigned to any discourse relation. Because of this, only a limited number of languages currently possess such lexicons (e.g. French (Roze et al., 2012), Spanish (Alonso Alemany et al., 2002), German (Stede and Umbach, 1998)). In this paper, we propose an approach to automatically map French DCs to their associated PDTB discourse relations using parallel texts. Our 2 Related Work Lexicons of DCs have been developed for several languages: English (Knott, 1996), Spanish"
W17-5501,J03-1002,0,0.0177893,"genre such as Europarl. 2 Mapping Discourse Relations Freq. # FR-DC =0 7 ≤ 50 55 &gt; 50 309 Total 371 Table 1: Distribution of LEXCONN French DCs in the Europarl corpus. We used the Moses statistical machine translation system (Koehn et al., 2007) to extract the number of alignments between French DCs and English DCs. As part of its translation model, Moses generates a phrase table (see Table 2) which aligns phrases between the language pairs. The phrase table is constructed based on statistical word alignment models and contains the frequency of the alignments between phrase pairs. We used the Och and Ney (2003) heuristic and combined IBM Model 4 word alignments (Brown et al., 1993) to construct the phrase table. Because an English DC can signal different discourse relations, to ensure that Moses’s phrase table distinguishes the different usages of the same English DC, we modified its English tokenizer so that each English DC and its discourse relation make up a single token. For example, the token 2,007,723 to be exact. 2 ‘although-C ONCESSION’ will be created for the DC although when it signals the discourse relation C ONCESSION. Table 2 shows a few entries of the phrase table for the French DC mˆe"
W17-5501,P16-1135,0,0.0259545,"Missing"
W17-5501,W15-0205,0,0.0129414,"constructing such lexicons requires linguistic expertise and is a time-consuming task. Discourse connectives and their translations have been studied within parallel texts by many (Meyer, 2011; Meyer et al., 2011; Taboada and ´ de los Angeles G´omez-Gonz´alez, 2012; Cartoni et al., 2013; Zufferey and Degand, 2014; Zufferey and Cartoni, 2014; Zufferey and Gygax, 2015; 1 ConcoLeDisCo is publicly available at https:// github.com/mjlaali/ConcoLeDisCo. 1 Proceedings of the SIGDIAL 2017 Conference, pages 1–6, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics Hoek and Zufferey, 2015). These works have either focused on the effect of the translation of discourse connectives on machine translation systems (Meyer, 2011; Meyer et al., 2011; Cartoni et al., 2013) or on a small number of discourse connectives due to the cost of manual annotations ´ (Taboada and de los Angeles G´omez-Gonz´alez, 2012; Zufferey and Degand, 2014; Zufferey and Cartoni, 2014; Zufferey and Gygax, 2015; Hoek and Zufferey, 2015). To our knowledge, very little research has addressed the automatic construction of lexicons of DCs. Hidey and McKeown (2016) proposed an automatic approach to identify English"
W17-5501,I13-1011,0,0.0223045,"Missing"
W17-5501,2005.mtsummit-papers.11,0,0.0175541,"Abstract approach can also automatically identify the usage of a DC where the DC signals a specific discourse relation. This can help linguists to study a DC in parallel texts and/or to find evidence for an association between discourse relations and DCs. Our approach is based on phrase tables generated by statistical machine translation and makes no assumption about the target language except the availability of a parallel corpus with another language for which a discourse parser exists; hence the approach is easy to expand to other languages. We applied our approach to the Europarl corpus (Koehn, 2005) and generated ConcoLeDisCo1 , a lexicon mapping French DCs to their associated Penn Discourse Treebank (PDTB) discourse relations (Prasad et al., 2008a). To our knowledge, ConcoLeDisCo is the first lexicon of French discourse connectives mapped to the PDTB relation set. When compared to LEXCONN, ConcoLeDisCo achieves a recall of 0.81 and an Average Precision of 0.68 for the C ONCES SION and C ONDITION discourse relations. In this paper, we present an approach to exploit phrase tables generated by statistical machine translation in order to map French discourse connectives to discourse relatio"
W17-5501,prasad-etal-2008-penn,0,0.168727,"to study a DC in parallel texts and/or to find evidence for an association between discourse relations and DCs. Our approach is based on phrase tables generated by statistical machine translation and makes no assumption about the target language except the availability of a parallel corpus with another language for which a discourse parser exists; hence the approach is easy to expand to other languages. We applied our approach to the Europarl corpus (Koehn, 2005) and generated ConcoLeDisCo1 , a lexicon mapping French DCs to their associated Penn Discourse Treebank (PDTB) discourse relations (Prasad et al., 2008a). To our knowledge, ConcoLeDisCo is the first lexicon of French discourse connectives mapped to the PDTB relation set. When compared to LEXCONN, ConcoLeDisCo achieves a recall of 0.81 and an Average Precision of 0.68 for the C ONCES SION and C ONDITION discourse relations. In this paper, we present an approach to exploit phrase tables generated by statistical machine translation in order to map French discourse connectives to discourse relations. Using this approach, we created ConcoLeDisCo, a lexicon of French discourse connectives and their PDTB relations. When evaluated against LEXCONN, C"
W17-5501,P98-2202,0,0.0779362,"dy, which is an expensive endeavour both in terms of time and expertise. For example, LEXCONN (Roze et al., 2012), a manually built lexicon of French DCs, was initiated in 2010 and released its first edition in 2012. The latest version, LEXCONN V2.1 (Danlos et al., 2015), contains 343 DCs mapped to an average of 1.3 discourse relations. This project is still ongoing as 37 DCs still have not been assigned to any discourse relation. Because of this, only a limited number of languages currently possess such lexicons (e.g. French (Roze et al., 2012), Spanish (Alonso Alemany et al., 2002), German (Stede and Umbach, 1998)). In this paper, we propose an approach to automatically map French DCs to their associated PDTB discourse relations using parallel texts. Our 2 Related Work Lexicons of DCs have been developed for several languages: English (Knott, 1996), Spanish (Alonso Alemany et al., 2002), German (Stede and Umbach, 1998), Czech (Pol´akov´a et al., 2013), and French (Roze et al., 2012). However, constructing such lexicons requires linguistic expertise and is a time-consuming task. Discourse connectives and their translations have been studied within parallel texts by many (Meyer, 2011; Meyer et al., 2011;"
W17-5501,K15-2001,0,0.0232414,"Missing"
W17-5501,P07-2045,0,\N,Missing
W17-5501,C98-2197,0,\N,Missing
W19-3004,N18-1202,0,0.0104882,"omponent is explained in the following sections. This paper summarizes our participation to the CLPsych 2019 shared task, under the name CLaC. The goal of the shared task was to detect and assess suicide risk based on a collection of online posts. For our participation, we used an ensemble method which utilizes 8 neural sub-models to extract neural features and predict class probabilities, which are then used by an SVM classifier. Our team ranked first in 2 out of the 3 tasks (tasks A and C). 1 System Overview 2.1 Word Embeddings As shown in Figure 1, GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) have been used as pretrained word embeddings. The 300d GloVe word embedder has been pretrained on 840B tokens of web data from Common Crawl. For ELMo, the original 1024d version, pretrained on the 1 Billion Word Language Model Benchmark (Chelba et al., 2014) has been used. Introduction The CLPsych 2019 shared task (Zirikly et al., 2019) focuses on the prediction of a person’s degree of suicide risk based on a collection of their Reddit posts (Shing et al., 2018). It is a multi-class classification task where a subject can be assigned to one of the four categories of no (class a), low (class b"
W19-3004,W18-0603,0,0.363341,"Missing"
W19-3004,D14-1179,0,0.0273694,"Missing"
W19-3004,W19-3003,0,0.0912981,"tract neural features and predict class probabilities, which are then used by an SVM classifier. Our team ranked first in 2 out of the 3 tasks (tasks A and C). 1 System Overview 2.1 Word Embeddings As shown in Figure 1, GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) have been used as pretrained word embeddings. The 300d GloVe word embedder has been pretrained on 840B tokens of web data from Common Crawl. For ELMo, the original 1024d version, pretrained on the 1 Billion Word Language Model Benchmark (Chelba et al., 2014) has been used. Introduction The CLPsych 2019 shared task (Zirikly et al., 2019) focuses on the prediction of a person’s degree of suicide risk based on a collection of their Reddit posts (Shing et al., 2018). It is a multi-class classification task where a subject can be assigned to one of the four categories of no (class a), low (class b), moderate (class c), or severe risk (class d), and consists of three different tasks: 2.2 Hidden Layers Four different types of hidden layers have been used: a Convolutional Neural Network (CNN) (LeCun et al., 1999), a Bidirectional vanilla Recurrent Neural Network (Bi-RNN), a Bidirectional Long Short-term Memory network (Bi-LSTM) (Hoc"
W19-3004,P82-1020,0,0.664914,"Missing"
W19-3004,D14-1162,0,0.0829277,"architecture of the system. Each component is explained in the following sections. This paper summarizes our participation to the CLPsych 2019 shared task, under the name CLaC. The goal of the shared task was to detect and assess suicide risk based on a collection of online posts. For our participation, we used an ensemble method which utilizes 8 neural sub-models to extract neural features and predict class probabilities, which are then used by an SVM classifier. Our team ranked first in 2 out of the 3 tasks (tasks A and C). 1 System Overview 2.1 Word Embeddings As shown in Figure 1, GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) have been used as pretrained word embeddings. The 300d GloVe word embedder has been pretrained on 840B tokens of web data from Common Crawl. For ELMo, the original 1024d version, pretrained on the 1 Billion Word Language Model Benchmark (Chelba et al., 2014) has been used. Introduction The CLPsych 2019 shared task (Zirikly et al., 2019) focuses on the prediction of a person’s degree of suicide risk based on a collection of their Reddit posts (Shing et al., 2018). It is a multi-class classification task where a subject can be assigned to one of the four categorie"
W94-0307,J89-4002,0,0.0342313,"Missing"
W94-0307,C92-2114,0,\N,Missing
