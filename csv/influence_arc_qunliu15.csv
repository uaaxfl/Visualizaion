2005.mtsummit-invited.7,J03-1002,0,0.00441278,"Missing"
2005.mtsummit-invited.7,2003.mtsummit-papers.51,0,0.0620587,"Missing"
2005.mtsummit-invited.7,H93-1040,0,0.0440308,"Missing"
2005.mtsummit-papers.18,J93-2003,0,0.0071722,"of the multi-aligner. 1 Introduction In a parallel corpus, automatic word alignment is to identify the translation relations between the words in a source sentence and those in a target sentence. A word-aligned parallel corpus has many applications, such as machine translation, machineaided translation, bilingual lexicography, and wordsense disambiguation. For these applications, much research on automatic word alignment has been conducted and reported. The statistics-based approach is widely studied (Och and Ney, 2003), and is mainly based on the research of statistical machine translation (Brown et al., 1993). However, this approach incorrectly aligns less frequently occurring words when statistically significant evidence is not available. Instead of word-based statistics, Ker proposed a class-based approach by using lexicon resources (Ker and Chang, 1997). Based on this idea, various 2 Japanese-Chinese Parallel Corpus The corpus we used in this study consists of 38,383 Japanese sentences from Mainichi newspaper and their Chinese translations. The corpus has been morphological annotated (word segmented and part-of-speech tagged) in the first phase of the project. For Japanese morphological 133 spe"
2005.mtsummit-papers.18,P00-1050,0,0.0436575,"Missing"
2005.mtsummit-papers.18,J97-2004,0,0.207324,"such as machine translation, machineaided translation, bilingual lexicography, and wordsense disambiguation. For these applications, much research on automatic word alignment has been conducted and reported. The statistics-based approach is widely studied (Och and Ney, 2003), and is mainly based on the research of statistical machine translation (Brown et al., 1993). However, this approach incorrectly aligns less frequently occurring words when statistically significant evidence is not available. Instead of word-based statistics, Ker proposed a class-based approach by using lexicon resources (Ker and Chang, 1997). Based on this idea, various 2 Japanese-Chinese Parallel Corpus The corpus we used in this study consists of 38,383 Japanese sentences from Mainichi newspaper and their Chinese translations. The corpus has been morphological annotated (word segmented and part-of-speech tagged) in the first phase of the project. For Japanese morphological 133 special case when k = 0 . Actually, the case of more-to-one has also been considered in the study. For simplicity of describtion, however, only the case of one-to-more is described here. Three kinds of lexical resources used for the estimation are describ"
2005.mtsummit-papers.18,maekawa-etal-2000-spontaneous,1,0.796201,"Missing"
2005.mtsummit-papers.18,J03-1002,0,0.0088417,"statistics-based aligner at the same time. Quantitative results confirmed the effectiveness of the multi-aligner. 1 Introduction In a parallel corpus, automatic word alignment is to identify the translation relations between the words in a source sentence and those in a target sentence. A word-aligned parallel corpus has many applications, such as machine translation, machineaided translation, bilingual lexicography, and wordsense disambiguation. For these applications, much research on automatic word alignment has been conducted and reported. The statistics-based approach is widely studied (Och and Ney, 2003), and is mainly based on the research of statistical machine translation (Brown et al., 1993). However, this approach incorrectly aligns less frequently occurring words when statistically significant evidence is not available. Instead of word-based statistics, Ker proposed a class-based approach by using lexicon resources (Ker and Chang, 1997). Based on this idea, various 2 Japanese-Chinese Parallel Corpus The corpus we used in this study consists of 38,383 Japanese sentences from Mainichi newspaper and their Chinese translations. The corpus has been morphological annotated (word segmented and"
2005.mtsummit-papers.18,C94-2209,0,0.0449107,"ir Chinese translations. The corpus has been morphological annotated (word segmented and part-of-speech tagged) in the first phase of the project. For Japanese morphological 133 special case when k = 0 . Actually, the case of more-to-one has also been considered in the study. For simplicity of describtion, however, only the case of one-to-more is described here. Three kinds of lexical resources used for the estimation are described below . annotation, the definition of the Corpus of Spontaneous Japanese was adopted (Maekawa, 2000). For Chinese, the definition of Peking University was adopted (Zhou and Yu, 1994). The average lengths of the sentences on both sides are about 30 words. The study, word alignment, aims to assist to word alignment annotation, which is a task in the second phase of the project. 3 Orthography About half of Japanese words contain kanji, the Chinese characters used in Japanese writing. We call them kanji words. Japanese words may also contain hiragana or katakana, which are phonetic characters. Because some kanji words were adapted directly from China, their Chinese translations are the same as the words themselves. For example, the Chinese translations for the Japanese words"
2007.iwslt-1.17,P96-1021,0,0.0581114,"Missing"
2007.iwslt-1.17,N03-1017,0,0.00951,"Missing"
2007.iwslt-1.17,P02-1038,0,0.132631,"Missing"
2007.iwslt-1.17,koen-2004-pharaoh,0,\N,Missing
2007.iwslt-1.17,I05-1007,1,\N,Missing
2007.iwslt-1.17,P06-1077,1,\N,Missing
2007.iwslt-1.17,P06-1066,1,\N,Missing
2007.iwslt-1.17,P00-1056,0,\N,Missing
2007.iwslt-1.17,P03-1021,0,\N,Missing
2008.iwslt-evaluation.7,P06-1077,1,0.887612,"el. This year, we participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanni"
2008.iwslt-evaluation.7,P07-1089,1,0.885881,"el. This year, we participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanni"
2008.iwslt-evaluation.7,P08-1023,1,0.802664,"e participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanning from position"
2008.iwslt-evaluation.7,D08-1022,1,0.818999,"e participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanning from position"
2008.iwslt-evaluation.7,P89-1018,0,0.0164582,"ws. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanning from position i through j (that is, wi+1 . . . wj ). Each hyperedge e ∈ E is a triple e = hT (e), h(e), f (e)i, where h(e) ∈ V is its head, T (e) ∈ V ∗ is a vector of tail nodes, and f (e) is a weight function from R|T (e) |to R. Figure"
2008.iwslt-evaluation.7,N04-1035,0,0.105473,"the outside probability of its root, the inside probabilities of its leaf nodes, and the probabilities of hyperedges involved in the fragment: Y Y αβ(t) = α(root(t)) × P (e) × β(v) (2) (1) where plm (s) is the language model score, |d |is the number of rules in a derivation, and |s |is the number of target words produced. The derivation probability P r(d|T ) is the product of probabilities of translation rules involved in d: Y P r(d|T ) = P r(r) (5) d∈D r∈d Table 1 gives a derivation for the example forest-string pair. To learn tree-to-string rules from annotated training data, we follow GHKM [6] to first identify minimal rules and then obtain composed rules. Like in tree-based extraction, we extract rules from a packed forest F in two steps: frontier set computation (where to cut) and fragmentation (how to cut). It turns out that the exact formulation developed for frontier set in tree-based case can be applied to a forest without change. The fragmentation step, however, becomes much more complicated since we now face a choice of multiple hyperedges at each node. We develop a breadth-first search algorithm for extracting tree-to-string rules from packed forests. The basic idea is to"
2008.iwslt-evaluation.7,P02-1038,0,0.152821,"ves(t) where α(·) and β(·) are the outside and inside probabilities of nodes, root(·) returns the root of a tree fragment and leaves(·) returns the leaf nodes of a tree fragment. Now, the fractional count of a rule r is simply lines denote word alignments. Each hyperedge is associated with a probability, which we omit in Figure 1 for clarity. In a forest, a node usually has multiple incoming hyperedges. For example, the source node IP0,6 has two incoming hyperedges: c(r) = αβ(lhs(r)) αβ(¯ v) (3) where v¯ denotes the root of the forest. We extend the simple model in Eq. 1 to a log-linear model [7]: dˆ = argmax P r(d|T )λ1 × plm (s)λ2 × eλ3 |d |× eλ4 |s| e1 = h(NP-B0,1 , VP1,6 ), IP0,6 , 0.6i e2 = h(NP0,3 , VP-B3,6 ), IP0,6 , 0.4i (4) d∈D Silenus searches for the best derivation (a sequence of translation rules) dˆ that converts a source tree T in the packed forest into a target-language string s: dˆ = argmax P r(d|T ) should be penalized accordingly and should have fractional counts instead of unit count. We penalize a rule r by the posterior probability of the corresponding tree fragment t = lhs(r), which can be computed as the product of the outside probability of its root, the insid"
2008.iwslt-evaluation.7,J07-2003,0,0.096572,"ach tree has its own probability (that is, product of hyperedge probabilities). As a result, a rule extracted from non 1-best parse - 53 - where each P r(r) can be decomposed into the product of six probabilities: P r(r) = p(r|lhs(r))λ5 × p(r|rhs(r))λ6 ×p(r|root(lhs(r)))λ7 ×plex (lhs(r)|rhs(r))λ8 ×plex (rhs(r)|lhs(r))λ9 ×p(T )λ10 (6) where the first three terms are conditional probabilities based on fractional counts, plex (·) denotes lexical weighting, and p(T ) denotes the probability of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9]. 2.2. Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] Proceedings of IWSLT 2008, Hawaii - U.S.A. rules. Bruin employs the following three BTG rules to direct tr"
2008.iwslt-evaluation.7,W05-1506,0,0.0292297,"))λ6 ×p(r|root(lhs(r)))λ7 ×plex (lhs(r)|rhs(r))λ8 ×plex (rhs(r)|lhs(r))λ9 ×p(T )λ10 (6) where the first three terms are conditional probabilities based on fractional counts, plex (·) denotes lexical weighting, and p(T ) denotes the probability of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9]. 2.2. Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] Proceedings of IWSLT 2008, Hawaii - U.S.A. rules. Bruin employs the following three BTG rules to direct translation: [] A → (A1 , A2 ) b3 b4 (7) target hi A → (A1 , A2 ) A → (x, y) (8) (9) b2 b The first two rules are used to merge two neighboring blocks into one larger block either in a monotonic or an inverted order. A block is a pair of source and t"
2008.iwslt-evaluation.7,P06-1066,1,0.895245,"ties based on fractional counts, plex (·) denotes lexical weighting, and p(T ) denotes the probability of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9]. 2.2. Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] Proceedings of IWSLT 2008, Hawaii - U.S.A. rules. Bruin employs the following three BTG rules to direct translation: [] A → (A1 , A2 ) b3 b4 (7) target hi A → (A1 , A2 ) A → (x, y) (8) (9) b2 b The first two rules are used to merge two neighboring blocks into one larger block either in a monotonic or an inverted order. A block is a pair of source and target contiguous sequences of words. The last rule translates a source phrase x into a target phrase y and generate a block A. In the followin"
2008.iwslt-evaluation.7,J97-3002,0,0.019739,"of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9]. 2.2. Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] Proceedings of IWSLT 2008, Hawaii - U.S.A. rules. Bruin employs the following three BTG rules to direct translation: [] A → (A1 , A2 ) b3 b4 (7) target hi A → (A1 , A2 ) A → (x, y) (8) (9) b2 b The first two rules are used to merge two neighboring blocks into one larger block either in a monotonic or an inverted order. A block is a pair of source and target contiguous sequences of words. The last rule translates a source phrase x into a target phrase y and generate a block A. In the following, we will define the model by separating different features (including the language model) from the ru"
2008.iwslt-evaluation.7,P08-2041,1,0.861149,"asures how precisely a feature f predicts a class c: IGR(f, c) = E(c) − E(c|f ) Ef (13) where E(·) is an entropy and E(·|·) is a conditional entropy. Surprisingly, the IGR for boundary words (0.2637) is very close to that of blocks (0.2655), suggesting that boundary words do provide sufficient information for predicting reordering. Based on CKY algorithm, the decoder finds the best derivation that produces the input sentence and its translation. To speed up the computation, Bruin also makes use of cube pruning. The lazy Algorithm 3 [9] are used for n-best list generation. 2.3. Mencius Mencius [12] is a phrase-based system that is very similar to Moses [13]. The major difference is that we introduce Proceedings of IWSLT 2008, Hawaii - U.S.A. similarity-based partial matching for bilingual phrases to alleviate data sparseness problem. J Given two source phrases fˆ1J and fˆ0 1 , their matching similarity is given by J SIM(fˆ1J , fˆ0 1 ) = PJ j=1 δ(fj , fj0 ) J 2.5. System Combination (14) where  0 δ(f, f ) = 1 if f = f 0 0 otherwise than . We find that large number of items will often be enumerated under this condition in our experiments. To tackle this problem, we further limit the num"
2008.iwslt-evaluation.7,P07-2045,0,0.00671916,"c) = E(c) − E(c|f ) Ef (13) where E(·) is an entropy and E(·|·) is a conditional entropy. Surprisingly, the IGR for boundary words (0.2637) is very close to that of blocks (0.2655), suggesting that boundary words do provide sufficient information for predicting reordering. Based on CKY algorithm, the decoder finds the best derivation that produces the input sentence and its translation. To speed up the computation, Bruin also makes use of cube pruning. The lazy Algorithm 3 [9] are used for n-best list generation. 2.3. Mencius Mencius [12] is a phrase-based system that is very similar to Moses [13]. The major difference is that we introduce Proceedings of IWSLT 2008, Hawaii - U.S.A. similarity-based partial matching for bilingual phrases to alleviate data sparseness problem. J Given two source phrases fˆ1J and fˆ0 1 , their matching similarity is given by J SIM(fˆ1J , fˆ0 1 ) = PJ j=1 δ(fj , fj0 ) J 2.5. System Combination (14) where  0 δ(f, f ) = 1 if f = f 0 0 otherwise than . We find that large number of items will often be enumerated under this condition in our experiments. To tackle this problem, we further limit the number of items taken from the heap. (15) We combine the output"
2008.iwslt-evaluation.7,D07-1105,0,0.0588403,". similarity-based partial matching for bilingual phrases to alleviate data sparseness problem. J Given two source phrases fˆ1J and fˆ0 1 , their matching similarity is given by J SIM(fˆ1J , fˆ0 1 ) = PJ j=1 δ(fj , fj0 ) J 2.5. System Combination (14) where  0 δ(f, f ) = 1 if f = f 0 0 otherwise than . We find that large number of items will often be enumerated under this condition in our experiments. To tackle this problem, we further limit the number of items taken from the heap. (15) We combine the outputs of single SMT systems at sentence level, similarly to the work by Macherey and Och [14]. Global linear models are used as a framework for reranking a merged n-best list: yˆ = argmax f (x, y) · W (17) y∈GEN(x) Note that we only consider two source phrases that have the same length. To make partially matching more reliable, we further restrict that they share with the same parts-ofspeech sequence. Our hope is that similar bilingual phrases can be used to create translation templates if one source phrase cannot find translations in the phrase table. For example, suppose that we cannot find translations for a source phrase “yu zuotian dida taiguo” in a phrase table, in which we find"
2008.iwslt-evaluation.7,P03-1021,0,0.031464,"se table, in which we find a similar source phrase “yu zuowan dida bulage” with its translation “arrived in Prague last evening”. According to the alignment information, we obtain a translation template: where x is a source sentence, y is a translation, f (x, y) is a feature vector, W is a weight vector, and GEN(x) is the set of possible candidate translations. There types of features are used: (1) relative BLEU scores against 1-best translations from other candidates, (2) language model scores, and (3) length of the translation. The feature weights are tuned using minimum-error-rate training [15]. In this year’s evaluation, each single SMT system generated 200-best list translations, which were merged and served as the input to the combiner. hyu X1 dida X2 , arrived in X2 X1 i 3. Experimental Results Then, the unmatched source substrings “zuotian” and “taiguo” can be translated into “yesterday” and “Thailand”, respectively. As a result, the translation for “yu zuotian dida taiguo” is “arrived in Thailand yesterday”. Given a source sentence, the decoder firstly search for all possible translation options from the phrase table by exact matching. For source phrases which have no translat"
2008.iwslt-evaluation.7,I05-1007,1,0.800287,"ran GIZA++ and used the “growdiagfinal” heuristic to get many-to-many word alignments. We observe that in a sentence some phrases are more likely to appear at the beginning, while other phrases are more likely to be located at the end. Inspired by the literature in language modeling, we mark the beginning and ending of word aligned sentences with two tags, “hsi” and “h/si”, to capture such reordering information. The sentences to be translated will also be annotated with the two tags, which will be removed after decoding. To get packed forests for Silenus, we used the Chinese parser modified [17] by Haitao Mi and the English parser [18] modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts [19] provided by Liang Huang to output packed forests. To prune the packed forests, Huang [19] uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned if the difference is greater than a threshold. Nodes with all incoming hyperedges pruned are also pruned. 3.3. Results Table 2 presents the BLEU scores (case-sensitive, with punctuations) of our"
2008.iwslt-evaluation.7,P05-1022,0,0.131775,"heuristic to get many-to-many word alignments. We observe that in a sentence some phrases are more likely to appear at the beginning, while other phrases are more likely to be located at the end. Inspired by the literature in language modeling, we mark the beginning and ending of word aligned sentences with two tags, “hsi” and “h/si”, to capture such reordering information. The sentences to be translated will also be annotated with the two tags, which will be removed after decoding. To get packed forests for Silenus, we used the Chinese parser modified [17] by Haitao Mi and the English parser [18] modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts [19] provided by Liang Huang to output packed forests. To prune the packed forests, Huang [19] uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned if the difference is greater than a threshold. Nodes with all incoming hyperedges pruned are also pruned. 3.3. Results Table 2 presents the BLEU scores (case-sensitive, with punctuations) of our five systems achieved on the IWSLT 2007 C"
2008.iwslt-evaluation.7,P08-1067,0,0.0287062,"more likely to appear at the beginning, while other phrases are more likely to be located at the end. Inspired by the literature in language modeling, we mark the beginning and ending of word aligned sentences with two tags, “hsi” and “h/si”, to capture such reordering information. The sentences to be translated will also be annotated with the two tags, which will be removed after decoding. To get packed forests for Silenus, we used the Chinese parser modified [17] by Haitao Mi and the English parser [18] modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts [19] provided by Liang Huang to output packed forests. To prune the packed forests, Huang [19] uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned if the difference is greater than a threshold. Nodes with all incoming hyperedges pruned are also pruned. 3.3. Results Table 2 presents the BLEU scores (case-sensitive, with punctuations) of our five systems achieved on the IWSLT 2007 Chinese-English development set. Prior to the evaluation, we used the development sets from 200"
2009.iwslt-evaluation.8,P06-1121,0,0.0208436,"ligned bilingual corpus with source forests F (Figure 1 (a)) in two steps: 3. Challenge task, English-Chinese direction. For each task of IWSLT 2009, the final submition is one of the four single systems who achieved a maximum BLEU score on development set. The four different systems are listed below: 1. Silenus, a linguistically syntax-based system that converts source-forest into target-string with tree-to-string rules acquired from packed forests; - 55 - (1) frontier set computation (where to cut), and (2) fragmentation (how to cut). Basically, we compute the frontier set according to GHKM [6] algorithm. We highlight the nodes in frontier set by gray shades in Figure 1(a). 1 http://www.statmt.org./moses/ Proceedings of IWSLT 2009, Tokyo - Japan P (lex(lhs(r)|rhs(r))) (4) P (lex(rhs(r)|lhs(r))) (5) P (f (r|root(lhs(r)))) =P f (r) (6) 0 root(lhs(r 0 ))=root(lhs(r)) f (r ) When computing the lexical translation probabilities described in [7], we only take the terminals into account. If there are no terminals, we set the feature value to 1. At the decoding time, we first parse the input sentences into forests. and then we convert the parse forest into a translation forest(Figure 1(b))"
2009.iwslt-evaluation.8,N03-1017,0,0.00351692,"that converts source-forest into target-string with tree-to-string rules acquired from packed forests; - 55 - (1) frontier set computation (where to cut), and (2) fragmentation (how to cut). Basically, we compute the frontier set according to GHKM [6] algorithm. We highlight the nodes in frontier set by gray shades in Figure 1(a). 1 http://www.statmt.org./moses/ Proceedings of IWSLT 2009, Tokyo - Japan P (lex(lhs(r)|rhs(r))) (4) P (lex(rhs(r)|lhs(r))) (5) P (f (r|root(lhs(r)))) =P f (r) (6) 0 root(lhs(r 0 ))=root(lhs(r)) f (r ) When computing the lexical translation probabilities described in [7], we only take the terminals into account. If there are no terminals, we set the feature value to 1. At the decoding time, we first parse the input sentences into forests. and then we convert the parse forest into a translation forest(Figure 1(b)) by pattern-matching. Finally, Silenus searches for the best derivation on the translation forest and outputs the target string. Beside the features we computed in rule extraction procedure, the additional features used in decoding step are listed here: • The number of rules in the derivation; Figure 1: Forest-based Rule Extraction and Translation • T"
2009.iwslt-evaluation.8,P08-1023,1,0.889278,"ranslation (IWSLT) 2009. For this year’s evaluation, our group participated in three tasks: 2. Bruin, a formally syntax-based system that implements a maximum entropy based reordering model on BTG rules; 3. Chiero, a formally syntax-based system that employs hierarchical phrases; 4. Moses, a phrase-based open source system 1 . This paper is organized as follows: Section 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English d"
2009.iwslt-evaluation.8,D08-1022,1,0.876308,"ranslation (IWSLT) 2009. For this year’s evaluation, our group participated in three tasks: 2. Bruin, a formally syntax-based system that implements a maximum entropy based reordering model on BTG rules; 3. Chiero, a formally syntax-based system that employs hierarchical phrases; 4. Moses, a phrase-based open source system 1 . This paper is organized as follows: Section 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English d"
2009.iwslt-evaluation.8,P06-1077,1,0.841838,"g model on BTG rules; 3. Chiero, a formally syntax-based system that employs hierarchical phrases; 4. Moses, a phrase-based open source system 1 . This paper is organized as follows: Section 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English direction; d∈D 2. Challenge task, Chinese-English direction; (1) We extract rules from word-aligned bilingual corpus with source forests F (Figure 1 (a)) in two steps: 3. Challenge ta"
2009.iwslt-evaluation.8,P07-1089,1,0.882882,"g model on BTG rules; 3. Chiero, a formally syntax-based system that employs hierarchical phrases; 4. Moses, a phrase-based open source system 1 . This paper is organized as follows: Section 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English direction; d∈D 2. Challenge task, Chinese-English direction; (1) We extract rules from word-aligned bilingual corpus with source forests F (Figure 1 (a)) in two steps: 3. Challenge ta"
2009.iwslt-evaluation.8,P89-1018,0,0.0587337,"ection 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English direction; d∈D 2. Challenge task, Chinese-English direction; (1) We extract rules from word-aligned bilingual corpus with source forests F (Figure 1 (a)) in two steps: 3. Challenge task, English-Chinese direction. For each task of IWSLT 2009, the final submition is one of the four single systems who achieved a maximum BLEU score on development set. The four diff"
2009.iwslt-evaluation.8,W05-1506,0,0.153435,"d-side of r, while the root(lhs(r) denotes the root node of the tree-fragment lhs(r). f (r) P (f (r|lhs(r)) = P lhs(r 0 )=lhs(r) P (f (r|rhs(r))) = P f (r0 ) f (r) rhs(r 0 )=rhs(r) f (r0 ) (2) (3) - 56 - • The language model score for the target translation; • The source side parsing probability of the tree traversed by the deviation. The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and kbest search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing [8]. For 1-best search, we use the cube pruning technique [9, 10] which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the bestfirst expansion idea from the Algorithm 2 of [8] to speed up the computation. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of [8] that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM"
2009.iwslt-evaluation.8,J07-2003,0,0.10516,"f the tree-fragment lhs(r). f (r) P (f (r|lhs(r)) = P lhs(r 0 )=lhs(r) P (f (r|rhs(r))) = P f (r0 ) f (r) rhs(r 0 )=rhs(r) f (r0 ) (2) (3) - 56 - • The language model score for the target translation; • The source side parsing probability of the tree traversed by the deviation. The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and kbest search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing [8]. For 1-best search, we use the cube pruning technique [9, 10] which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the bestfirst expansion idea from the Algorithm 2 of [8] to speed up the computation. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of [8] that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM forest, resulting from the intersection of the translation fo"
2009.iwslt-evaluation.8,P07-1019,0,0.0142273,"f the tree-fragment lhs(r). f (r) P (f (r|lhs(r)) = P lhs(r 0 )=lhs(r) P (f (r|rhs(r))) = P f (r0 ) f (r) rhs(r 0 )=rhs(r) f (r0 ) (2) (3) - 56 - • The language model score for the target translation; • The source side parsing probability of the tree traversed by the deviation. The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and kbest search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing [8]. For 1-best search, we use the cube pruning technique [9, 10] which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the bestfirst expansion idea from the Algorithm 2 of [8] to speed up the computation. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of [8] that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM forest, resulting from the intersection of the translation fo"
2009.iwslt-evaluation.8,J97-3002,0,0.00823424,"uting the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM forest, resulting from the intersection of the translation forest and the LM, with its nodes being the +LM items during cube pruning. Although this new forest is prohibitively large, Algorithm 3 is very efficient with minimal overhead on top of 1-best. For more details, please refer to [1] and [2]. Proceedings of IWSLT 2009, Tokyo - Japan 2.2. Bruin Bruin is a formally syntax-based SMT system, which implements the maximum entropy based reordering model on BTG [11] rules. This model considers the reorder as a problem of classification, where the Maximum Entropy model is introduced. To complete the decoding procedure, three BTG rules are used to derive the translation: [] A → (A1 , A2 ) hi (7) A → (A1 , A2 ) (8) A → (x, y) (9) The lexical rule (3) is used to translate source phrase y into target phrase x and generate a block A. The merging rules (1) and (2) are used to merge two consecutive blocks into a single larger block in the straight or inverted order. Three essential elements must be illustrated in Bruin. The first one is a stochastic BTG, whose r"
2009.iwslt-evaluation.8,P03-1021,0,0.0105949,"nguage model score of the two blocks according to their final order, λLM is its weight. For the lexical rule, applying it is assigned a probability P rl (A): P rl (A) = p(x|y)λ1 · p(y|x)λ2 · plex (x|y)λ3 ·plex (y|x)λ4 · exp(1)λ5 · exp(|x|)λ6 LM ·pλLM (x) (11) where p(·) are the phrase translation probabilities in both directions, plex (·) are the lexical translation probabilities in both directions, and exp(1) and exp(|x|) are the phrase penalty and word penalty, respectively. The feature weights λs are tuned to maximize the BLEU score on the development set, using minimum-error-rate training [12]. The MaxEnt-based Reordering Model (MRM) is defined on the two consecutive blocks A1 and A2 together with their - 57 - order o ∈ {straight, inverted} according to the maximum entropy framework. P exp( i θi hi (o, A1 , A2 )) 1 2 P Ω = pθ (o|A , A ) = P (12) 1 2 o exp( i θi hi (o, A , A )) where the functions hi ∈ {0, 1} are model features and the θi are the weights. The decoder is built upon the CKY chart-based algorithm. We use cube pruning technology to speed up the decoding. For more details, please refer to [13]. 2.3. Chiero Chiero is a re-implementation of the state-of-the-art hierarchica"
2009.iwslt-evaluation.8,P06-1066,1,0.846081,"imize the BLEU score on the development set, using minimum-error-rate training [12]. The MaxEnt-based Reordering Model (MRM) is defined on the two consecutive blocks A1 and A2 together with their - 57 - order o ∈ {straight, inverted} according to the maximum entropy framework. P exp( i θi hi (o, A1 , A2 )) 1 2 P Ω = pθ (o|A , A ) = P (12) 1 2 o exp( i θi hi (o, A , A )) where the functions hi ∈ {0, 1} are model features and the θi are the weights. The decoder is built upon the CKY chart-based algorithm. We use cube pruning technology to speed up the decoding. For more details, please refer to [13]. 2.3. Chiero Chiero is a re-implementation of the state-of-the-art hierarchical phrase-based model [9]. This model can be formalized as a synchronous contextfree grammar, which is automatically acquired from wordaligned parallel data without any syntactic information. X →&lt; γ, α, ∼&gt; (13) Where X is a non-terminal, γ, α are strings of terminals and non-terminals, and ∼ is one-to-one correspondence between the non-terminal in γ, α. Our work faithfully followed Chiang’s [9] work. The only exception is the condition for terminating cube pruning. Chiang’s [9] implementation quits upon considering t"
2009.iwslt-evaluation.8,I05-1007,1,0.670097,"T07 dev IWSLT08 dev selection 0.350 0.345 0.340 BLEU score ysis system ICTCLAS for splitting Chinese characters into words and a rule-based tokenizer for tokenizing English sentences. Then,we convert all alphanumeric characters to their 2-byte representation. Finally, we ran GIZA++ and used the “grow-diagfinal” heuristic to get many-to-many word alignments. We used the SRI Language Modeling Toolkit [14] to train the Chinese/English 5-gram language model with KneserNey smoothing on the Chinese/English side of the training corpus respectively. Regarding to Silenus, we used the Chinese parser of [15] and English parser of [16] to parse the source and target side of the bilingual corpus into packed forests respectively. Then we pruned the forests with the marginal probabilitybased inside-outside algorithm [17] with a pruning threshold pe = 3. At the decoding time, we use a large pruning threshold pd = 12 to generate the packed forest. 0.335 0.330 #143 0.325 #113 0.320 #413 0.315 #249 #191 0.310 0.305 0.300 0 1 2 3 Threshold 4 5 6 Figure 2: The BLEU scores and sentence # of dev selection with different thresholds. 4.2. Results on IWSLT09 3.1. Development Set Selection Our development set fo"
2009.iwslt-evaluation.8,P05-1022,0,0.209137,"on 0.350 0.345 0.340 BLEU score ysis system ICTCLAS for splitting Chinese characters into words and a rule-based tokenizer for tokenizing English sentences. Then,we convert all alphanumeric characters to their 2-byte representation. Finally, we ran GIZA++ and used the “grow-diagfinal” heuristic to get many-to-many word alignments. We used the SRI Language Modeling Toolkit [14] to train the Chinese/English 5-gram language model with KneserNey smoothing on the Chinese/English side of the training corpus respectively. Regarding to Silenus, we used the Chinese parser of [15] and English parser of [16] to parse the source and target side of the bilingual corpus into packed forests respectively. Then we pruned the forests with the marginal probabilitybased inside-outside algorithm [17] with a pruning threshold pe = 3. At the decoding time, we use a large pruning threshold pd = 12 to generate the packed forest. 0.335 0.330 #143 0.325 #113 0.320 #413 0.315 #249 #191 0.310 0.305 0.300 0 1 2 3 Threshold 4 5 6 Figure 2: The BLEU scores and sentence # of dev selection with different thresholds. 4.2. Results on IWSLT09 3.1. Development Set Selection Our development set for this year’s evaluation is"
2009.iwslt-evaluation.8,P08-1067,0,0.0326858,"ric characters to their 2-byte representation. Finally, we ran GIZA++ and used the “grow-diagfinal” heuristic to get many-to-many word alignments. We used the SRI Language Modeling Toolkit [14] to train the Chinese/English 5-gram language model with KneserNey smoothing on the Chinese/English side of the training corpus respectively. Regarding to Silenus, we used the Chinese parser of [15] and English parser of [16] to parse the source and target side of the bilingual corpus into packed forests respectively. Then we pruned the forests with the marginal probabilitybased inside-outside algorithm [17] with a pruning threshold pe = 3. At the decoding time, we use a large pruning threshold pd = 12 to generate the packed forest. 0.335 0.330 #143 0.325 #113 0.320 #413 0.315 #249 #191 0.310 0.305 0.300 0 1 2 3 Threshold 4 5 6 Figure 2: The BLEU scores and sentence # of dev selection with different thresholds. 4.2. Results on IWSLT09 3.1. Development Set Selection Our development set for this year’s evaluation is selected automatically from all the development sentences according to the n-gram similarity, which is calculated against the current test set sentences. Our method works as follows: Fi"
2009.mtsummit-papers.20,D08-1064,0,0.259405,"core? Callison-Burch et al. (2009) used the results of the manual evaluation to analyze the translation quality of the different systems that were submitted to the WMT09 and found that in general, system combinations performed as well as the best individual systems, but not statistically significantly better than them. Some practical cases in our evaluation suggest that the higher BLEU score doesn’t always mean higher translation adequacy no matter in single system MT task or in system combination task. For the evaluation measurement, we choose two novel metrics as our alternatives: BLEU-SBP (Chiang et al., 2008) and linguistic check-point method (Zhou et al., 2008). We encountered two actual cases which happened to be very similar with those in (Chiang et al., 2008). These cases can be traced to the fact that BLEU (Papineni et al., 2002) is not decomposable at the sentence level, which means if a system generates a long translation for one sentence, it can generate a short translation for another sentence without facing a penalty. Our experiments validated BLEU-SBP’s effectivity in resolving the nondecomposability problem of both NIST-BLEU and IBM-BLEU at sentence level. On the other hand, we choose"
2009.mtsummit-papers.20,P05-1066,0,0.0871819,"and LCM, the automatic evaluation metrics of CWMT2008 evaluation also include: BLEU, NIST, GTM, mWER, mPER and ICT (a metric developed by the Institute of Computing Technology, CAS). All these metrics are case-sensitive. The evaluation of Chinese translation is based on Chinese characters instead of words. 2.5 Evaluation Results Figures 1-4 show the evaluation results. 3 BLEU-SBP 3.1 BLEU’s Deficiency We encountered the following two practical cases in our evaluation. In this paper, if not specified particularly, all the BLEU means NIST-BLEU. 3.1.1 The Sign Test When we applied the sign test (Collins et al., 2005) for significance testing with BLEU, we encountered such problem (Table 4): when comparing system A and system B, if we select A as the baseline system, we found B is significantly better than A, but if we select B as the baseline system, we found A is significantly better than B. We tried two kinds of BLEU: NIST-BLUE and IBM-BLEU, the results are similar (Table 4). This is because the sign test requires a function (ai,bi) that indicates whether bi is better, worse or same quality translation relative to ai. Because BLEU is not defined on single sentences, Collins et al. (2005) use an approxim"
2009.mtsummit-papers.20,P02-1040,0,0.0823026,"s well as the best individual systems, but not statistically significantly better than them. Some practical cases in our evaluation suggest that the higher BLEU score doesn’t always mean higher translation adequacy no matter in single system MT task or in system combination task. For the evaluation measurement, we choose two novel metrics as our alternatives: BLEU-SBP (Chiang et al., 2008) and linguistic check-point method (Zhou et al., 2008). We encountered two actual cases which happened to be very similar with those in (Chiang et al., 2008). These cases can be traced to the fact that BLEU (Papineni et al., 2002) is not decomposable at the sentence level, which means if a system generates a long translation for one sentence, it can generate a short translation for another sentence without facing a penalty. Our experiments validated BLEU-SBP’s effectivity in resolving the nondecomposability problem of both NIST-BLEU and IBM-BLEU at sentence level. On the other hand, we choose linguistic checkpoint method (LCM) as another alternative metric with an attempt to detect and report richer linguistic information on the system. Now most MT evaluation methods only generate a general similarity score. At the pre"
2009.mtsummit-papers.20,H93-1040,0,0.212743,"Missing"
2009.mtsummit-papers.20,W09-0401,0,\N,Missing
2009.mtsummit-papers.20,W03-1709,1,\N,Missing
2009.mtsummit-papers.20,C08-1141,1,\N,Missing
2009.mtsummit-papers.20,J03-1002,0,\N,Missing
2010.iwslt-evaluation.8,I05-1007,1,\N,Missing
2010.iwslt-evaluation.8,C10-1135,1,\N,Missing
2010.iwslt-evaluation.8,N04-1023,0,\N,Missing
2010.iwslt-evaluation.8,D08-1022,1,\N,Missing
2010.iwslt-evaluation.8,D09-1108,0,\N,Missing
2010.iwslt-evaluation.8,P10-1146,0,\N,Missing
2010.iwslt-evaluation.8,W06-3110,0,\N,Missing
2010.iwslt-evaluation.8,P07-1089,1,\N,Missing
2010.iwslt-evaluation.8,P08-1023,1,\N,Missing
2010.iwslt-evaluation.8,P06-1077,1,\N,Missing
2010.iwslt-evaluation.8,P06-1066,1,\N,Missing
2010.iwslt-evaluation.8,P07-2045,0,\N,Missing
2010.iwslt-evaluation.8,P05-1022,0,\N,Missing
2010.iwslt-evaluation.8,P08-1067,0,\N,Missing
2010.iwslt-evaluation.8,P05-1033,0,\N,Missing
2010.iwslt-evaluation.8,P05-1066,0,\N,Missing
2010.iwslt-evaluation.8,2009.iwslt-evaluation.8,1,\N,Missing
2010.iwslt-evaluation.8,2006.iwslt-papers.4,0,\N,Missing
2011.mtsummit-papers.3,P08-1024,0,0.0179087,"and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its"
2011.mtsummit-papers.3,W08-0304,0,0.013488,", 2009). In MIRA (Chiang et al., 2008), if we choose candidates whose SBLEU are 0.5 and 0.2 as positive and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Ran"
2011.mtsummit-papers.3,D10-1059,0,0.0118331,"each (c)andicate is given by  p(ci ) = exp (γ · score(c))/ i exp (γ · score(ci )) with γ = 1 (Li and Eisner, 2009). In MIRA (Chiang et al., 2008), if we choose candidates whose SBLEU are 0.5 and 0.2 as positive and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), bu"
2011.mtsummit-papers.3,D08-1024,0,0.257247,"n such scenarios. The ﬂuctuation in quality can even be statistically perceivable when the number of features is larger than 25 or 30 in practice; on the other hand, Smith (2006) ﬁnds that, MERT relies heavily on the behavior of parameters on the error surface, which is likely to be affected by random variances in the N-best list, and also lead to less generalizable results especially when the development set and the test set are not from exactly the same domain. Both the former and the latter shortcomings have been studied in recent research. e.g. The Margin Infused Relaxed Algorithm (MIRA: (Chiang et al., 2008; Chiang et al., 2009)) is shown to be capable of handling tens of thousands of features in training, while Cer et. al (2008) try to overcome irregularities on the error surface of MERT. In this paper, we focus on improving the generalizability of MERT by introduce a new objective function MRC, which restricts the permutation of the whole N-best list. In the MERT paradigm, the tuning objective is based on the 1-best error surface of the N-best list of an in-domain test set. As MERT actually optimizes parameters for the 1-best for a particular domain, the resulting parameters become domain spec"
2011.mtsummit-papers.3,N09-1025,0,0.01381,"ﬂuctuation in quality can even be statistically perceivable when the number of features is larger than 25 or 30 in practice; on the other hand, Smith (2006) ﬁnds that, MERT relies heavily on the behavior of parameters on the error surface, which is likely to be affected by random variances in the N-best list, and also lead to less generalizable results especially when the development set and the test set are not from exactly the same domain. Both the former and the latter shortcomings have been studied in recent research. e.g. The Margin Infused Relaxed Algorithm (MIRA: (Chiang et al., 2008; Chiang et al., 2009)) is shown to be capable of handling tens of thousands of features in training, while Cer et. al (2008) try to overcome irregularities on the error surface of MERT. In this paper, we focus on improving the generalizability of MERT by introduce a new objective function MRC, which restricts the permutation of the whole N-best list. In the MERT paradigm, the tuning objective is based on the 1-best error surface of the N-best list of an in-domain test set. As MERT actually optimizes parameters for the 1-best for a particular domain, the resulting parameters become domain speciﬁc, and does not gene"
2011.mtsummit-papers.3,P05-1033,0,0.0319721,"te that our method can bring steady improvement for cross-domain translation. Given the fact that purely in-domain data is rarely found in the real world use cases, our method’s ability to generalize to unknown domains is desirable in real world translation tasks. 6 Conclusion 6.1 Conclusions and Future Work The most important characteristic of our method is that it is easily extensible. We therefore plan to experiment with more new optimizing objectives and other optimizing algorithms, to exploit more features in translation, and to extend our method to other formalisms, such as hierachical (Chiang, 2005; Chiang, 2007) or syntax-based (Galley et al., 2006; Liu et al., 2006) translation. 7 Acknowledgement Daqi Zheng, Yang Liu and Qun Liu are supported by National Natural Science Foundation of China Contract 60903138, 90920004, and 60736014. Yifan He is supported by Science Foundation Ireland (Grant No 07/CE/I1142). This work is part funded under FP7 of the EC within the EuroMatrix+ project (Grant No 231720). We thank Jennifer Foster and Joachim Wagner for their valuable suggestions. References In this paper we presented Maximum Ranking Correlation Training (MRCT) for tuning MT systems which wa"
2011.mtsummit-papers.3,J07-2003,0,0.0396422,"thod can bring steady improvement for cross-domain translation. Given the fact that purely in-domain data is rarely found in the real world use cases, our method’s ability to generalize to unknown domains is desirable in real world translation tasks. 6 Conclusion 6.1 Conclusions and Future Work The most important characteristic of our method is that it is easily extensible. We therefore plan to experiment with more new optimizing objectives and other optimizing algorithms, to exploit more features in translation, and to extend our method to other formalisms, such as hierachical (Chiang, 2005; Chiang, 2007) or syntax-based (Galley et al., 2006; Liu et al., 2006) translation. 7 Acknowledgement Daqi Zheng, Yang Liu and Qun Liu are supported by National Natural Science Foundation of China Contract 60903138, 90920004, and 60736014. Yifan He is supported by Science Foundation Ireland (Grant No 07/CE/I1142). This work is part funded under FP7 of the EC within the EuroMatrix+ project (Grant No 231720). We thank Jennifer Foster and Joachim Wagner for their valuable suggestions. References In this paper we presented Maximum Ranking Correlation Training (MRCT) for tuning MT systems which was different fro"
2011.mtsummit-papers.3,P09-1064,0,0.0119997,"y people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its search space, we ﬁnd the optimal set of paˆ (Fig 1), which maximizes the weighted rameters λ sum of correlation on a set with M sentences, as i"
2011.mtsummit-papers.3,P08-2010,0,0.02105,"= 1.37 via assuming the objective of Min-Risk is the expectation of BLEU, and the probability of each (c)andicate is given by  p(ci ) = exp (γ · score(c))/ i exp (γ · score(ci )) with γ = 1 (Li and Eisner, 2009). In MIRA (Chiang et al., 2008), if we choose candidates whose SBLEU are 0.5 and 0.2 as positive and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives"
2011.mtsummit-papers.3,W09-0439,0,0.0115063,"core is perfectly the same as their SBLEU. We obtain λ = 1.37 via assuming the objective of Min-Risk is the expectation of BLEU, and the probability of each (c)andicate is given by  p(ci ) = exp (γ · score(c))/ i exp (γ · score(ci )) with γ = 1 (Li and Eisner, 2009). In MIRA (Chiang et al., 2008), if we choose candidates whose SBLEU are 0.5 and 0.2 as positive and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2"
2011.mtsummit-papers.3,P06-1121,0,0.041395,"nt for cross-domain translation. Given the fact that purely in-domain data is rarely found in the real world use cases, our method’s ability to generalize to unknown domains is desirable in real world translation tasks. 6 Conclusion 6.1 Conclusions and Future Work The most important characteristic of our method is that it is easily extensible. We therefore plan to experiment with more new optimizing objectives and other optimizing algorithms, to exploit more features in translation, and to extend our method to other formalisms, such as hierachical (Chiang, 2005; Chiang, 2007) or syntax-based (Galley et al., 2006; Liu et al., 2006) translation. 7 Acknowledgement Daqi Zheng, Yang Liu and Qun Liu are supported by National Natural Science Foundation of China Contract 60903138, 90920004, and 60736014. Yifan He is supported by Science Foundation Ireland (Grant No 07/CE/I1142). This work is part funded under FP7 of the EC within the EuroMatrix+ project (Grant No 231720). We thank Jennifer Foster and Joachim Wagner for their valuable suggestions. References In this paper we presented Maximum Ranking Correlation Training (MRCT) for tuning MT systems which was different from the MER. We optimized to maximize t"
2011.mtsummit-papers.3,2009.mtsummit-posters.8,1,0.85082,"ability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its search space, we ﬁnd the optimal set of paˆ (Fig 1), which maximizes the weighted rameters λ sum of correlation on a set with M sentences, as in Eq. (1). ˆ = arg max( λ λ M  wi · Corri (λ)) (1) i=1 where wi is the weight"
2011.mtsummit-papers.3,D11-1125,0,0.0152855,"terjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its search space, we ﬁnd the optimal set of paˆ (Fig 1), which maximizes the weighted rameters λ sum of correlation on a set with M sentences, as in Eq. (1). ˆ = arg max( λ λ M  wi · Corri (λ)) (1) i=1 where wi is the weight of the i–th sentence, and Corri (λ) is the correlation between the model scores and the translation quality of the translation candidates approximated by SBLEU, as in Eq."
2011.mtsummit-papers.3,N03-1017,0,0.0047482,"hared translation task. The training data is the Europarl v3b release (Koehn, 2005). The language model corpus is the English part of monolingual language model training data provided by the organizers of WMT From the data sets provided by WMT, we use dev2006 as the tuning set for λ, test2005 as tuning set for α, and test2006, test2007 and test2008 as in-domain test sets. We use newstest2008, newstest2009 and We use Moses 2 as the baseline decoder, perform word alignment using the GIZA++ (Och and Ney, 2003) implementation of IBM Model 4, and extract phrases using the grow-diag-ﬁnal heuristic (Koehn et al., 2003). We train a 4-gram language model using the SRILM language modeling toolkit (Stolcke, 2002). We use the MERT implemented by Bertoldi et al., (2009), which is included in the Moses package. BLEU is calculated by the mteval script provided by NIST 3 . Statistical signiﬁcance of test results is computed by Koehn’s boosting tool (Koehn, 2004). We preprocess the data using the toolkits provided by WMT08 organizers, train and tune Moses following the WMT baseline description, with 100-best list, using up to 15 tuning iterations, and ﬁnally arriving at a model with 14 default features. 4.2 The Basel"
2011.mtsummit-papers.3,W04-3250,0,0.0707191,"08 as in-domain test sets. We use newstest2008, newstest2009 and We use Moses 2 as the baseline decoder, perform word alignment using the GIZA++ (Och and Ney, 2003) implementation of IBM Model 4, and extract phrases using the grow-diag-ﬁnal heuristic (Koehn et al., 2003). We train a 4-gram language model using the SRILM language modeling toolkit (Stolcke, 2002). We use the MERT implemented by Bertoldi et al., (2009), which is included in the Moses package. BLEU is calculated by the mteval script provided by NIST 3 . Statistical signiﬁcance of test results is computed by Koehn’s boosting tool (Koehn, 2004). We preprocess the data using the toolkits provided by WMT08 organizers, train and tune Moses following the WMT baseline description, with 100-best list, using up to 15 tuning iterations, and ﬁnally arriving at a model with 14 default features. 4.2 The Baseline System We tune 3 times on the tuning set and pick the experiment whose parameters achieve the highest BLEU score on the test05 as a baseline. We then use this parameter to decode all other sets, and obtain results 2 checkout from svn with version 3625 ftp://jaguar.ncsl.nist.gov/mt/ resources/mteval-v11b.pl 3 1 http://www.statmt.org/wmt"
2011.mtsummit-papers.3,2005.mtsummit-papers.11,0,0.0501157,"t-of-Domain test set, MRC not only performs better than MER, but also beat the interpolation’s perfomance. Notice: the directly decoding method outperform the reranking one. 32.75 reranking decoding baseline 32.7 newstest2010 as out-of-domain test sets. BLEU 32.65 4.1.2 Program 32.6 32.55 32.5 32.45 0 0.2 0.4 α 0.6 0.8 1 Figure 4: Test06: Another in-domain test set to demonstrate the advantage of interpolation of objectives 4.1 Experimental setting 4.1.1 Data We use the French–English parallel data provided by the WMT08 1 shared translation task. The training data is the Europarl v3b release (Koehn, 2005). The language model corpus is the English part of monolingual language model training data provided by the organizers of WMT From the data sets provided by WMT, we use dev2006 as the tuning set for λ, test2005 as tuning set for α, and test2006, test2007 and test2008 as in-domain test sets. We use newstest2008, newstest2009 and We use Moses 2 as the baseline decoder, perform word alignment using the GIZA++ (Och and Ney, 2003) implementation of IBM Model 4, and extract phrases using the grow-diag-ﬁnal heuristic (Koehn et al., 2003). We train a 4-gram language model using the SRILM language mode"
2011.mtsummit-papers.3,N04-1022,0,0.0380525,"e as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its search space, we ﬁnd the optimal set of paˆ (Fig 1), which maximizes the weighted"
2011.mtsummit-papers.3,P09-1019,0,0.012328,"the probability of each (c)andicate is given by  p(ci ) = exp (γ · score(c))/ i exp (γ · score(ci )) with γ = 1 (Li and Eisner, 2009). In MIRA (Chiang et al., 2008), if we choose candidates whose SBLEU are 0.5 and 0.2 as positive and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al.,"
2011.mtsummit-papers.3,2006.iwslt-papers.5,0,0.0181767,"with γ = 1 (Li and Eisner, 2009). In MIRA (Chiang et al., 2008), if we choose candidates whose SBLEU are 0.5 and 0.2 as positive and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this pa"
2011.mtsummit-papers.3,D09-1005,0,0.0130764,"I (NSGA-II) (Deb et al., 2002), a genetic algorithm to perform multi-objective optimization. For example, in Figure 1, MERT chooses the middle point of two cross points. By contrast, MRC tries to maximize the rank correlation between SBLEU and the model score. and will adjust λ into the open interval (1.5, 2), in which the order of candidates’ model score is perfectly the same as their SBLEU. We obtain λ = 1.37 via assuming the objective of Min-Risk is the expectation of BLEU, and the probability of each (c)andicate is given by  p(ci ) = exp (γ · score(c))/ i exp (γ · score(ci )) with γ = 1 (Li and Eisner, 2009). In MIRA (Chiang et al., 2008), if we choose candidates whose SBLEU are 0.5 and 0.2 as positive and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et"
2011.mtsummit-papers.3,P09-1067,0,0.0130105,"Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its search space, we ﬁnd the optimal set of paˆ (Fig 1), which maximizes the weighted rameters λ sum of correlation on a set"
2011.mtsummit-papers.3,P06-1077,1,0.815113,"ranslation. Given the fact that purely in-domain data is rarely found in the real world use cases, our method’s ability to generalize to unknown domains is desirable in real world translation tasks. 6 Conclusion 6.1 Conclusions and Future Work The most important characteristic of our method is that it is easily extensible. We therefore plan to experiment with more new optimizing objectives and other optimizing algorithms, to exploit more features in translation, and to extend our method to other formalisms, such as hierachical (Chiang, 2005; Chiang, 2007) or syntax-based (Galley et al., 2006; Liu et al., 2006) translation. 7 Acknowledgement Daqi Zheng, Yang Liu and Qun Liu are supported by National Natural Science Foundation of China Contract 60903138, 90920004, and 60736014. Yifan He is supported by Science Foundation Ireland (Grant No 07/CE/I1142). This work is part funded under FP7 of the EC within the EuroMatrix+ project (Grant No 231720). We thank Jennifer Foster and Joachim Wagner for their valuable suggestions. References In this paper we presented Maximum Ranking Correlation Training (MRCT) for tuning MT systems which was different from the MER. We optimized to maximize the correlation betw"
2011.mtsummit-papers.3,D08-1076,0,0.0299773,"xpectation of BLEU, and the probability of each (c)andicate is given by  p(ci ) = exp (γ · score(c))/ i exp (γ · score(ci )) with γ = 1 (Li and Eisner, 2009). In MIRA (Chiang et al., 2008), if we choose candidates whose SBLEU are 0.5 and 0.2 as positive and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner,"
2011.mtsummit-papers.3,P02-1038,0,0.0660037,"d signiﬁcantly outperforms MERT on out-of-domain data sets, and performs marginally better than MERT on in-domain data sets, which validates the usefulness of MRC on both domain speciﬁc and general domain data. Min−Risk MIRA <− MRC−> Model Score 3 2.5 0.5 2 1.5 1 −0.5 0.3 SBLEU=0.3 y=x SBLEU=0.2 y=−x+3 SBLEU=0.5 y=−0.5*x+3 0 0.5 0.2 1 λ 1.5 2 2.5 Figure 1: Made-up Example: The 3 sloping lines represent all 3 candidates in N-best list. Their SBLEU (BLEU = SBLEU when only one sentence) and funtions are in the legend of ﬁgure. 1 Introduction Searching for the optimal parameters in linear models (Och and Ney, 2002) of Statistical Machine Translation (SMT) has been a major challenge to the MT community. The most widely used approach todate is Minimum-Error-Rate Training (MERT:(Och, 2003)), which tries to ﬁnd the parameters that optimize the translation quality of the 1-best translation candidate, using the N-best list as an approximation of the decoder’s search space. In spite of its usefulness and high adoption, MERT suffers from shortcomings that the MT community is becoming aware of. On the one hand, 48 MERT is not designed for models with rich features and therefore leads to translations of unstable"
2011.mtsummit-papers.3,J03-1002,0,0.00260608,"ives 4.1 Experimental setting 4.1.1 Data We use the French–English parallel data provided by the WMT08 1 shared translation task. The training data is the Europarl v3b release (Koehn, 2005). The language model corpus is the English part of monolingual language model training data provided by the organizers of WMT From the data sets provided by WMT, we use dev2006 as the tuning set for λ, test2005 as tuning set for α, and test2006, test2007 and test2008 as in-domain test sets. We use newstest2008, newstest2009 and We use Moses 2 as the baseline decoder, perform word alignment using the GIZA++ (Och and Ney, 2003) implementation of IBM Model 4, and extract phrases using the grow-diag-ﬁnal heuristic (Koehn et al., 2003). We train a 4-gram language model using the SRILM language modeling toolkit (Stolcke, 2002). We use the MERT implemented by Bertoldi et al., (2009), which is included in the Moses package. BLEU is calculated by the mteval script provided by NIST 3 . Statistical signiﬁcance of test results is computed by Koehn’s boosting tool (Koehn, 2004). We preprocess the data using the toolkits provided by WMT08 organizers, train and tune Moses following the WMT baseline description, with 100-best lis"
2011.mtsummit-papers.3,P03-1021,0,0.0481218,"ﬁc and general domain data. Min−Risk MIRA <− MRC−> Model Score 3 2.5 0.5 2 1.5 1 −0.5 0.3 SBLEU=0.3 y=x SBLEU=0.2 y=−x+3 SBLEU=0.5 y=−0.5*x+3 0 0.5 0.2 1 λ 1.5 2 2.5 Figure 1: Made-up Example: The 3 sloping lines represent all 3 candidates in N-best list. Their SBLEU (BLEU = SBLEU when only one sentence) and funtions are in the legend of ﬁgure. 1 Introduction Searching for the optimal parameters in linear models (Och and Ney, 2002) of Statistical Machine Translation (SMT) has been a major challenge to the MT community. The most widely used approach todate is Minimum-Error-Rate Training (MERT:(Och, 2003)), which tries to ﬁnd the parameters that optimize the translation quality of the 1-best translation candidate, using the N-best list as an approximation of the decoder’s search space. In spite of its usefulness and high adoption, MERT suffers from shortcomings that the MT community is becoming aware of. On the one hand, 48 MERT is not designed for models with rich features and therefore leads to translations of unstable quality in such scenarios. The ﬂuctuation in quality can even be statistically perceivable when the number of features is larger than 25 or 30 in practice; on the other hand,"
2011.mtsummit-papers.3,P02-1040,0,0.0947049,"optimal set of paˆ (Fig 1), which maximizes the weighted rameters λ sum of correlation on a set with M sentences, as in Eq. (1). ˆ = arg max( λ λ M  wi · Corri (λ)) (1) i=1 where wi is the weight of the i–th sentence, and Corri (λ) is the correlation between the model scores and the translation quality of the translation candidates approximated by SBLEU, as in Eq. (2). N Corri (λ) = Corr(ΦN 1 (λ), SBLEU (e1 ))) (2) where e is the i–th sentence, eN 1 is the N-best deriva(λ) are the model scores tion of the decoder and ΦN 1 using parameters λ. We calculate SBLEU by for eN 1 applying the BLEU (Papineni et al., 2002) formulation directly to single sentence. There exist many coefﬁcients to measure the correlation between model scores and SBLEU scores. In our implementation, we use the Spearman’s ρ ranking correlation, as in Eq. (3)  (xi − x ¯)(yi − y¯) (3) ρ =  i  ¯)2 i (yi − y¯)2 i (xi − x 3.2 Combination of MRC and MER Training Inspired by Chiang et al. (2008), we also explore the possibility of combining the rank correlation with evaluation metric score as an alternative to the MER and the MRC objectives. Given a set of features λ, we perform a straightforward linear combination as in Eq. (4) ˆ = ar"
2011.mtsummit-papers.3,D09-1147,0,0.0187952,"r et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its search space, we ﬁnd the optimal set of paˆ (Fig 1), which maximizes the weighted rameters λ sum of correlation on a set with M sentences, as in Eq. (1). ˆ = arg max( λ λ M  wi · Corri (λ)) (1) i=1 where wi is the weight of the i–th sentence, and Corri (λ) is the correlation between the model scores and the translation quality of the translation candidates approxima"
2011.mtsummit-papers.3,P06-2101,0,0.0506745,"Missing"
2011.mtsummit-papers.3,D08-1065,0,0.0153584,"l no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its search space, we ﬁnd the optimal set of paˆ (Fig 1), which maximizes the weighted rameters λ sum of corr"
2011.mtsummit-papers.3,D07-1055,0,0.0177524,"s to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its search space, we ﬁnd the optimal set of paˆ (Fig 1), which maximizes the weighted rameters λ sum of correlation on a set with M sentences, as in Eq. (1). ˆ = arg max( λ λ M  wi · Corri (λ)) (1) i=1 where"
2011.mtsummit-papers.33,P02-1040,0,0.0842457,"Missing"
2011.mtsummit-papers.33,P03-1021,0,0.0258257,"btain the N unique translation candidates. It should be noticed that two translation candidates are identical only if their translation string and the corresponding feature vector values are identical at the same time 3) For each of the N translation candidates we calculate the total voting score of M systems by simply adding the voting score of each system. The score of each system for each candidate is calculated using the linear combination of the system’s weight vector and the candidate’s feature vector. The weight vector of each member system is gained in the training process using MERT (Och, 2003). While the feature vector of each translation candidate is gained in step 1). The formula for calculating the total voting score of candidate c is listed below:  ሬሬሬሬሬሬሬሬሬԦୡ  ሬሬሬԦ θ  ሺ ሻ ൌ    ୀଵ 4) Finally we re-rank these N translation candidates by the total voting scores and output the one with the highest score In order to be concise and to prove the effectiveness of bagging, we do not add any extra features in our implements and experiments. 4 Experiments 4.1 Experimental Setup We performed the experiments on Chinese-English translation using an in-house implementation of t"
2011.mtsummit-papers.33,P05-1033,0,0.088627,"candidate is gained in step 1). The formula for calculating the total voting score of candidate c is listed below:  ሬሬሬሬሬሬሬሬሬԦୡ  ሬሬሬԦ θ  ሺ ሻ ൌ    ୀଵ 4) Finally we re-rank these N translation candidates by the total voting scores and output the one with the highest score In order to be concise and to prove the effectiveness of bagging, we do not add any extra features in our implements and experiments. 4 Experiments 4.1 Experimental Setup We performed the experiments on Chinese-English translation using an in-house implementation of the hierarchical phrase based SMT model (David Chiang, 2005). The model is tuned using standard MERT (Och, 2003). We use the corpus of NTCIR9 Patent translation task3 Chinese-English part which contains one million sentence pairs. We obtain one thousand sentence pairs for tuning and testing respectively 3 http://ntcir.nii.ac.jp/PatentMT/ 295 without overlap. We use GIZA++4 to perform the bi-directional word alignment between source and target side of each sentence pair. The final word alignment is generated using the grow-diag-final method. And at last all sentence pairs with alignment information is used to extract rules and phrases. A 5-gram language"
2011.mtsummit-papers.33,A00-2005,0,0.0700437,"Missing"
2011.mtsummit-papers.33,N09-3001,0,0.0166854,"t from that of training set. Bagging uses the voting result of m classifiers each with a unique distribution of the same model, so generally it is stable in statistics. Secondly bagging can avoid the over-fitting problem which a plenty of classifiers suffer. Finally bagging can be seen as an unsupervised method which doesn’t need the labeled corpus used to train the recognizer in domain recognizing methods. Bagging has been used successfully in many NLP applications such as Syntactic Parsing (Hen294 derson and Brill, 2000), Semantic Parsing (Nielsen and Pradhan, 2004), Coreference Resolution (Vemulapalli et al., 2009; Vemulapalli et al., 2010), Word Sense Disambiguation (Nielsen and Pradhan, 2004) and so on. 3 Bagging-based domain adaptation Suppose that there are M available statistics machine systems {ɊሺɅଵ ሻǡ ɊሺɅଶ ሻǡ ǥ ǡ ɊሺɅ ሻ}, the task of system combination is to build a new translation system ɋሺɊሺɅଵ ሻǡ ɊሺɅଶ ሻǡ ǥ ǡ ɊሺɅ ሻሻ which denotes the combination system. It combines the translation outputs from each of its cell system ɊሺɅ୧ ሻ which we call here a member system of it. As discussed in section 1, hardly any single system can achieve a good performance on multidomain translation problem. Besides, th"
2011.mtsummit-papers.33,W02-1405,0,0.0201586,"data. We test the results with the cluster number from 2 to 5, and the results are listed below: clusters 2 3 4 5 BLEU 31.09 31.24 31.05 30.61 Table 4 results of the unsupervised domain recognizing based method From the above results we can see a similar situation: when there are too many clusters the translation performance drops due to data sparsity; and as the cluster number decreases, the performance ascends at first and reaches the highest record of 31.24 BLEU score when the cluster number is three; and finally drops as the discrimination of class recognizer becomes weak. 5 Related Work Langlais (2002) first mention Domain Adaptation problem in SMT area by mention the problem of how to use a SMT to translate a corpus far different from the one it has been trained on. Then he makes notable achievement by integrating specific lexicon tables. Eck et al. (2004) proposed a language model adaptation technique in SMT using information retrieval techniques. Firstly, each test document is translated with general language model; and then the translation is used to select the most similar documents; then the adapted language model is built using these documents; finally the test document is re-transla"
2011.mtsummit-papers.33,eck-etal-2004-language,0,0.0275279,"tuation: when there are too many clusters the translation performance drops due to data sparsity; and as the cluster number decreases, the performance ascends at first and reaches the highest record of 31.24 BLEU score when the cluster number is three; and finally drops as the discrimination of class recognizer becomes weak. 5 Related Work Langlais (2002) first mention Domain Adaptation problem in SMT area by mention the problem of how to use a SMT to translate a corpus far different from the one it has been trained on. Then he makes notable achievement by integrating specific lexicon tables. Eck et al. (2004) proposed a language model adaptation technique in SMT using information retrieval techniques. Firstly, each test document is translated with general language model; and then the translation is used to select the most similar documents; then the adapted language model is built using these documents; finally the test document is re-translated using the adapted language model. Hasan and Ney (2005) proposed a method for building class-based language models. He applies regular expressions based method to cluster the sentences into specific classes. And then he interpolates them with the main langu"
2011.mtsummit-papers.33,2005.eamt-1.17,0,0.0955281,"antly over a hierarchical phrase-based baseline by 0.82 BLEU points and it's even higher than some conventional classifier-based methods. 1 Introduction Conventional approaches usually resort to statistical classifiers and there are a plenty of notable jobs on it (Xu et al., 2007; Bertoldi and Federico, 2009; Banerjee et al., 2010). They all achieve significant improvement over baseline when a large amount of annotated monolingual data in multidomains is available for training the classifier. Yet they shrivel when the annotated monolingual data is deficient. Others proposed unsupervised ways (Hasan and Ney, 2005; Yamamoto and Sumita, 2007) and reported improvement over baseline. Yet their results show that they do not outperform the conventional approaches obviously. In recent years domain adaptation problem in SMT becomes more important (Banerjee et al., 2010). Since an SMT system trained on a corpus with heterogeneous topics may fail to achieve a good performance on domain-specific translation, while an SMT system trained on a domain-specific corpus may achieve a deteriorative performance for outof-domain translation (Haque et al., 2009). Besides more and more evaluation tasks begin to focus on mul"
2011.mtsummit-papers.33,W07-0733,0,0.0286338,"with general language model; and then the translation is used to select the most similar documents; then the adapted language model is built using these documents; finally the test document is re-translated using the adapted language model. Hasan and Ney (2005) proposed a method for building class-based language models. He applies regular expressions based method to cluster the sentences into specific classes. And then he interpolates them with the main language models to elude the data sparseness. And finally this method achieves improvements in terms of perplexity reduction and error rates. Koehn and Schroeder (2007) carried out a scheme of integrating in-domain and out-ofdomain language models using log-linear features of an SMT model, and used multiple decoding paths for combining multiple domain translation tables within the framework of the Moses decoder6. Xu et al. (2007) proposed a method which uses the information retrieval approaches to classify the input sentences. This method is based on domains along with domain-dependent language models and feature weights which are gained in the training process of SMT models. This method resulted in a significant improvement in domain-dependent translation."
2011.mtsummit-papers.33,2007.mtsummit-papers.68,0,0.343632,"sually resort to statistical classifiers, but they require annotated monolingual data in different domains, which may not be available in some cases. We instead propose a simple but effective bagging-based approach without using any annotated data. Large-scale experiments show that our new method improves translation quality significantly over a hierarchical phrase-based baseline by 0.82 BLEU points and it's even higher than some conventional classifier-based methods. 1 Introduction Conventional approaches usually resort to statistical classifiers and there are a plenty of notable jobs on it (Xu et al., 2007; Bertoldi and Federico, 2009; Banerjee et al., 2010). They all achieve significant improvement over baseline when a large amount of annotated monolingual data in multidomains is available for training the classifier. Yet they shrivel when the annotated monolingual data is deficient. Others proposed unsupervised ways (Hasan and Ney, 2005; Yamamoto and Sumita, 2007) and reported improvement over baseline. Yet their results show that they do not outperform the conventional approaches obviously. In recent years domain adaptation problem in SMT becomes more important (Banerjee et al., 2010). Since"
2011.mtsummit-papers.33,Y09-2027,0,0.0315654,"Missing"
2011.mtsummit-papers.33,2010.amta-papers.16,0,0.234962,"Missing"
2011.mtsummit-papers.33,D07-1054,0,0.0139736,"ical phrase-based baseline by 0.82 BLEU points and it's even higher than some conventional classifier-based methods. 1 Introduction Conventional approaches usually resort to statistical classifiers and there are a plenty of notable jobs on it (Xu et al., 2007; Bertoldi and Federico, 2009; Banerjee et al., 2010). They all achieve significant improvement over baseline when a large amount of annotated monolingual data in multidomains is available for training the classifier. Yet they shrivel when the annotated monolingual data is deficient. Others proposed unsupervised ways (Hasan and Ney, 2005; Yamamoto and Sumita, 2007) and reported improvement over baseline. Yet their results show that they do not outperform the conventional approaches obviously. In recent years domain adaptation problem in SMT becomes more important (Banerjee et al., 2010). Since an SMT system trained on a corpus with heterogeneous topics may fail to achieve a good performance on domain-specific translation, while an SMT system trained on a domain-specific corpus may achieve a deteriorative performance for outof-domain translation (Haque et al., 2009). Besides more and more evaluation tasks begin to focus on multi-domain translation. For e"
2011.mtsummit-papers.42,N06-1013,0,0.0186349,"analysis as a preprocessing step for word alignments. Elming and Habash (2007) ﬁrst tokenize words into smaller units to align, and then the alignments are mapped back to the original word form. Similarly, Carpuat et al. (2010) propose to reorder post-verbal subject (VS) constructions of Arabic sentences into SV order for word alignment only, and the phrase extraction and decoding are performed on the original word order. Moreover, there is still some work on alignment combination. Koehn et al. (2003) combine the alignments from two different directions, source-totarget and target-to-source. Ayan and Dorr (2006) propose a maximum entropy approach to combine multiple alignment from different models based on a set of linguistic and alignment features. Xiang et al. (2010) generate multiple sets of diversiﬁed alignments based on different motivations and then combine them according to conﬁdence scores (Huang, 2009). Zhang and Sumita (2007) use English lemmas in training which improves the quality of word alignment and yield better translation performance. The difference between our work and above is: we generate multi-granularity word alignments, and propose different methods to combine them; and to the"
2011.mtsummit-papers.42,J93-2003,0,0.0491615,"not appropriate for resource-scarce language pairs due to lexical sparsity problem. And we believe that offering each link a probability might help to distinguish ”good” alignment links from ”bad” ones. Since GIZA++ could provide n-best lists alignment results with probabilities, we can estimate the link probability from it. Formally, a weighted matrix A is a J × I matrix, in which each element stores a link probability p(j, i) to indicate how well fj and ei are aligned. The link probability is estimated from n-best lists by calculating relative frequencies, (1) Traditional alignment models (Brown et al., 1993; Vogel et al., 1996) treat word alignment as a hidden process and maximize the likelihood of bi-text using the Expectation Maximization (EM) algorithm. Usually, the SMT system takes 1-best result as the ﬁnal alignment result to generate translation tables, phrase tables, or syntactic transformation rules. In this work, we employ existing alignment models and produce multiple sets of alignment with different granularities, then combine them together to catch complementary information to get a better alignment result. Speciﬁcally, we feed aligners with two granularities: surface word and stem,"
2011.mtsummit-papers.42,P10-2033,0,0.0324504,"Missing"
2011.mtsummit-papers.42,P05-1033,0,0.307072,"Missing"
2011.mtsummit-papers.42,N07-2007,0,0.0169837,"es, we often subject to the problem of word sparsity. The reduction of sparsity can be achieved by increasing the number of training data or via morphological analysis. Nakov and Ng (2009) propose a method for improving SMT of resource-scarce languages by exploiting their similarity to resource-rich ones. Nießen and Ney (2004) use morphological decomposition to get better alignments. (Lee, 2004) changes the word segmentation of Arabic to induce morphological and syntactic symmetry between parallel sentences. Some are take the morphological analysis as a preprocessing step for word alignments. Elming and Habash (2007) ﬁrst tokenize words into smaller units to align, and then the alignments are mapped back to the original word form. Similarly, Carpuat et al. (2010) propose to reorder post-verbal subject (VS) constructions of Arabic sentences into SV order for word alignment only, and the phrase extraction and decoding are performed on the original word order. Moreover, there is still some work on alignment combination. Koehn et al. (2003) combine the alignments from two different directions, source-totarget and target-to-source. Ayan and Dorr (2006) propose a maximum entropy approach to combine multiple ali"
2011.mtsummit-papers.42,P06-1121,0,0.224485,"Missing"
2011.mtsummit-papers.42,N06-2013,0,0.072088,"gglutinative languages, however, data sparseness problem becomes much more serious. An inﬂectional language, such as English, expresses semantics by hierarchical structures of simple words, which explains why the tree-based translation models can alleviate lexical sparsity. But for agglutinative languages, semantics are expressed mainly by concatenation of stem and afﬁxes. Usually, a stem can attach with several preﬁxes or sufﬁxes, thus leading to tens of hundreds of possible derived words. Different from the Arabic-English translation that has achieved promising progress, such as (Lee, 2004; Habash and Sadat, 2006), most agglutinative languages are less-studied and suffer from resource-scarce problem. Our work focuses on the translation of Uyghur. According to the agglutinative property of Uyghur, we propose a novel strategy, multi-granularity integration, to optimize word alignment and decoding for translating from Uyghur to Chinese. To optimize the word alignment, multiple alignment results are combined by assigning each intertranslatable word pair an alignment probability. This procedure is performed for a series of alignment tasks with different lexical granularities, including stem-to-word alignmen"
2011.mtsummit-papers.42,P08-2041,1,0.843426,"hn et al., 2003; Chiang, 2005; Galley et al., 2006) often suffers much from the lexical sparsity, especially in word alignment and decoding. The widely-used word alignment tool, GIZA++ (Och and Ney, 2003), performs better on large parallel corpora, but much poorer on small ones. Researchers have tried many methods to optimize alignment result such as heuristic methods (Koehn et al., 2003), and also many strategies to extract better translation rules (Liu et al., 2009). While for decoding, a lot of work has been devoted to the alleviation of unknown word problem, such as fuzzy phrase matching (He et al., 2008). These strategies work 360 well for the translation of inﬂectional and isolating languages. For the translation of agglutinative languages, however, data sparseness problem becomes much more serious. An inﬂectional language, such as English, expresses semantics by hierarchical structures of simple words, which explains why the tree-based translation models can alleviate lexical sparsity. But for agglutinative languages, semantics are expressed mainly by concatenation of stem and afﬁxes. Usually, a stem can attach with several preﬁxes or sufﬁxes, thus leading to tens of hundreds of possible de"
2011.mtsummit-papers.42,P09-1105,0,0.0173075,"V order for word alignment only, and the phrase extraction and decoding are performed on the original word order. Moreover, there is still some work on alignment combination. Koehn et al. (2003) combine the alignments from two different directions, source-totarget and target-to-source. Ayan and Dorr (2006) propose a maximum entropy approach to combine multiple alignment from different models based on a set of linguistic and alignment features. Xiang et al. (2010) generate multiple sets of diversiﬁed alignments based on different motivations and then combine them according to conﬁdence scores (Huang, 2009). Zhang and Sumita (2007) use English lemmas in training which improves the quality of word alignment and yield better translation performance. The difference between our work and above is: we generate multi-granularity word alignments, and propose different methods to combine them; and to the best of our knowledge, this is the ﬁrst time that statistical method of translating Uyghur into Chinese is reported. 7 Conclusions and Future Work This paper proposes a novel strategy, multigranularity integration, to optimize the word alignment and decoding for agglutinative language translation. For wo"
2011.mtsummit-papers.42,D07-1091,0,0.0772522,"ed variants, standard statistical approaches, especially SMT, are apt to suffer from data spareness issues. There may exists many unknown words owing to the rich morphological features. Previous section has revealed that stemming can reduce vocabulary size notably. Intuitively, by employing the stem-granularity rules, we may alleviate the number of unknown words. That’s to say, except the use of word-granularity rules, we employ stemgranularity rules simultaneously. Perhaps it is a feasible way to incorporate the two different rule sets into log-linear model with different weights similar to (Koehn and Hoang, 2007). 5 Baseline Replace 0.5 0.6 0.7 0.8 0.9 1.0 Experiments Table 2: Word alignment results by threshold pruning on combined weighted alignment matrix. Baseline is the alignment produced by GIZA++ on word level. Replace is on stem level. bi-text sentences of Uyghur-Chinese1 which come from government document. Table 1 shows the statistics of the training corpus. In addition, there are 1, 000 sentences in the tuning set and 1, 000 sentences in the test set, both with one reference. For the language model, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model with the tar"
2011.mtsummit-papers.42,N03-1017,0,0.0248649,"rabic to induce morphological and syntactic symmetry between parallel sentences. Some are take the morphological analysis as a preprocessing step for word alignments. Elming and Habash (2007) ﬁrst tokenize words into smaller units to align, and then the alignments are mapped back to the original word form. Similarly, Carpuat et al. (2010) propose to reorder post-verbal subject (VS) constructions of Arabic sentences into SV order for word alignment only, and the phrase extraction and decoding are performed on the original word order. Moreover, there is still some work on alignment combination. Koehn et al. (2003) combine the alignments from two different directions, source-totarget and target-to-source. Ayan and Dorr (2006) propose a maximum entropy approach to combine multiple alignment from different models based on a set of linguistic and alignment features. Xiang et al. (2010) generate multiple sets of diversiﬁed alignments based on different motivations and then combine them according to conﬁdence scores (Huang, 2009). Zhang and Sumita (2007) use English lemmas in training which improves the quality of word alignment and yield better translation performance. The difference between our work and ab"
2011.mtsummit-papers.42,N04-4015,0,0.67119,"lation of agglutinative languages, however, data sparseness problem becomes much more serious. An inﬂectional language, such as English, expresses semantics by hierarchical structures of simple words, which explains why the tree-based translation models can alleviate lexical sparsity. But for agglutinative languages, semantics are expressed mainly by concatenation of stem and afﬁxes. Usually, a stem can attach with several preﬁxes or sufﬁxes, thus leading to tens of hundreds of possible derived words. Different from the Arabic-English translation that has achieved promising progress, such as (Lee, 2004; Habash and Sadat, 2006), most agglutinative languages are less-studied and suffer from resource-scarce problem. Our work focuses on the translation of Uyghur. According to the agglutinative property of Uyghur, we propose a novel strategy, multi-granularity integration, to optimize word alignment and decoding for translating from Uyghur to Chinese. To optimize the word alignment, multiple alignment results are combined by assigning each intertranslatable word pair an alignment probability. This procedure is performed for a series of alignment tasks with different lexical granularities, includ"
2011.mtsummit-papers.42,D09-1106,1,0.895743,"Missing"
2011.mtsummit-papers.42,J10-3002,1,0.867785,"Missing"
2011.mtsummit-papers.42,C10-1084,0,0.116046,"Missing"
2011.mtsummit-papers.42,H05-1011,0,0.057861,"Missing"
2011.mtsummit-papers.42,D09-1141,0,0.0124691,"esearch over the years. Some are focused on the improvement of word alignment models, notably in (Moore, 2005; Liu et al., 2010; Riesa and Marcu, 2010; Saers et al., 2010). Others are trying to incorporate morpho-syntactic knowledge (Xiang et al., 2010; Carpuat et al., 2010; Luong and Kan, 2010) which 365 makes it easier to determine corresponding words directly. As for the translation task with respect to agglutinative languages, we often subject to the problem of word sparsity. The reduction of sparsity can be achieved by increasing the number of training data or via morphological analysis. Nakov and Ng (2009) propose a method for improving SMT of resource-scarce languages by exploiting their similarity to resource-rich ones. Nießen and Ney (2004) use morphological decomposition to get better alignments. (Lee, 2004) changes the word segmentation of Arabic to induce morphological and syntactic symmetry between parallel sentences. Some are take the morphological analysis as a preprocessing step for word alignments. Elming and Habash (2007) ﬁrst tokenize words into smaller units to align, and then the alignments are mapped back to the original word form. Similarly, Carpuat et al. (2010) propose to reo"
2011.mtsummit-papers.42,J04-2003,0,0.0295465,"Marcu, 2010; Saers et al., 2010). Others are trying to incorporate morpho-syntactic knowledge (Xiang et al., 2010; Carpuat et al., 2010; Luong and Kan, 2010) which 365 makes it easier to determine corresponding words directly. As for the translation task with respect to agglutinative languages, we often subject to the problem of word sparsity. The reduction of sparsity can be achieved by increasing the number of training data or via morphological analysis. Nakov and Ng (2009) propose a method for improving SMT of resource-scarce languages by exploiting their similarity to resource-rich ones. Nießen and Ney (2004) use morphological decomposition to get better alignments. (Lee, 2004) changes the word segmentation of Arabic to induce morphological and syntactic symmetry between parallel sentences. Some are take the morphological analysis as a preprocessing step for word alignments. Elming and Habash (2007) ﬁrst tokenize words into smaller units to align, and then the alignments are mapped back to the original word form. Similarly, Carpuat et al. (2010) propose to reorder post-verbal subject (VS) constructions of Arabic sentences into SV order for word alignment only, and the phrase extraction and decodin"
2011.mtsummit-papers.42,P00-1056,0,0.108524,"I Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model with the target side of training corpus. To measure the quality of word alignment, we manually aligned 100 parallel sentences from the training corpus of Uyghur-Chinese. And Moses2 is used as our baseline SMT system. The decoding weights are optimized with minimum error rate training (MERT) (Och, 2003) to maximum word level BLEU (Papineni et al., 2002) scores. 5.2 Word Alignment The quality of alignment is computed as appropriately reﬁned precision and recall measures. Additionally, we also use the Alignment Error Rate (AER) (Och and Ney, 2000) which is derived from the well-known F-measure. AER requires gold alignments that are marked as ’sure’ or ’probable’. Here, we don’t distinguish them, so the AER is computed as: Pr = In this section, we ﬁrst describe the experimental settings, and then verify the effect of multigranularity word alignment and machine translation. |A ∩ S| |A ∩ S| , Rc = |A| |S| AER = 1 − 5.1 Experimental Setup 1 We conduct our experiments on Uyghur-Chinese language pair. We have collected about 120K 363 2P rRc P r + Rc (5) (6) http://mt.xmu.edu.cn/cwmt2011/en/index.html. Part of the corpus will be published for"
2011.mtsummit-papers.42,J03-1002,0,0.0215144,"Missing"
2011.mtsummit-papers.42,P03-1021,0,0.00908632,"e from government document. Table 1 shows the statistics of the training corpus. In addition, there are 1, 000 sentences in the tuning set and 1, 000 sentences in the test set, both with one reference. For the language model, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model with the target side of training corpus. To measure the quality of word alignment, we manually aligned 100 parallel sentences from the training corpus of Uyghur-Chinese. And Moses2 is used as our baseline SMT system. The decoding weights are optimized with minimum error rate training (MERT) (Och, 2003) to maximum word level BLEU (Papineni et al., 2002) scores. 5.2 Word Alignment The quality of alignment is computed as appropriately reﬁned precision and recall measures. Additionally, we also use the Alignment Error Rate (AER) (Och and Ney, 2000) which is derived from the well-known F-measure. AER requires gold alignments that are marked as ’sure’ or ’probable’. Here, we don’t distinguish them, so the AER is computed as: Pr = In this section, we ﬁrst describe the experimental settings, and then verify the effect of multigranularity word alignment and machine translation. |A ∩ S| |A ∩ S| , Rc"
2011.mtsummit-papers.42,P02-1040,0,0.0819668,"ws the statistics of the training corpus. In addition, there are 1, 000 sentences in the tuning set and 1, 000 sentences in the test set, both with one reference. For the language model, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model with the target side of training corpus. To measure the quality of word alignment, we manually aligned 100 parallel sentences from the training corpus of Uyghur-Chinese. And Moses2 is used as our baseline SMT system. The decoding weights are optimized with minimum error rate training (MERT) (Och, 2003) to maximum word level BLEU (Papineni et al., 2002) scores. 5.2 Word Alignment The quality of alignment is computed as appropriately reﬁned precision and recall measures. Additionally, we also use the Alignment Error Rate (AER) (Och and Ney, 2000) which is derived from the well-known F-measure. AER requires gold alignments that are marked as ’sure’ or ’probable’. Here, we don’t distinguish them, so the AER is computed as: Pr = In this section, we ﬁrst describe the experimental settings, and then verify the effect of multigranularity word alignment and machine translation. |A ∩ S| |A ∩ S| , Rc = |A| |S| AER = 1 − 5.1 Experimental Setup 1 We con"
2011.mtsummit-papers.42,P10-1017,0,0.0375828,"Missing"
2011.mtsummit-papers.42,N10-1050,0,0.0355064,"Missing"
2011.mtsummit-papers.42,C96-2141,0,0.249722,"resource-scarce language pairs due to lexical sparsity problem. And we believe that offering each link a probability might help to distinguish ”good” alignment links from ”bad” ones. Since GIZA++ could provide n-best lists alignment results with probabilities, we can estimate the link probability from it. Formally, a weighted matrix A is a J × I matrix, in which each element stores a link probability p(j, i) to indicate how well fj and ei are aligned. The link probability is estimated from n-best lists by calculating relative frequencies, (1) Traditional alignment models (Brown et al., 1993; Vogel et al., 1996) treat word alignment as a hidden process and maximize the likelihood of bi-text using the Expectation Maximization (EM) algorithm. Usually, the SMT system takes 1-best result as the ﬁnal alignment result to generate translation tables, phrase tables, or syntactic transformation rules. In this work, we employ existing alignment models and produce multiple sets of alignment with different granularities, then combine them together to catch complementary information to get a better alignment result. Speciﬁcally, we feed aligners with two granularities: surface word and stem, and stemming is only"
2011.mtsummit-papers.42,P10-2005,0,0.0177418,"ed back to the original word form. Similarly, Carpuat et al. (2010) propose to reorder post-verbal subject (VS) constructions of Arabic sentences into SV order for word alignment only, and the phrase extraction and decoding are performed on the original word order. Moreover, there is still some work on alignment combination. Koehn et al. (2003) combine the alignments from two different directions, source-totarget and target-to-source. Ayan and Dorr (2006) propose a maximum entropy approach to combine multiple alignment from different models based on a set of linguistic and alignment features. Xiang et al. (2010) generate multiple sets of diversiﬁed alignments based on different motivations and then combine them according to conﬁdence scores (Huang, 2009). Zhang and Sumita (2007) use English lemmas in training which improves the quality of word alignment and yield better translation performance. The difference between our work and above is: we generate multi-granularity word alignments, and propose different methods to combine them; and to the best of our knowledge, this is the ﬁrst time that statistical method of translating Uyghur into Chinese is reported. 7 Conclusions and Future Work This paper pr"
2011.mtsummit-papers.42,P07-2046,0,0.0127359,"rd alignment only, and the phrase extraction and decoding are performed on the original word order. Moreover, there is still some work on alignment combination. Koehn et al. (2003) combine the alignments from two different directions, source-totarget and target-to-source. Ayan and Dorr (2006) propose a maximum entropy approach to combine multiple alignment from different models based on a set of linguistic and alignment features. Xiang et al. (2010) generate multiple sets of diversiﬁed alignments based on different motivations and then combine them according to conﬁdence scores (Huang, 2009). Zhang and Sumita (2007) use English lemmas in training which improves the quality of word alignment and yield better translation performance. The difference between our work and above is: we generate multi-granularity word alignments, and propose different methods to combine them; and to the best of our knowledge, this is the ﬁrst time that statistical method of translating Uyghur into Chinese is reported. 7 Conclusions and Future Work This paper proposes a novel strategy, multigranularity integration, to optimize the word alignment and decoding for agglutinative language translation. For word alignment, we perform"
2011.mtsummit-wpt.4,P03-1021,0,0.40321,"tures in the traditional HPB model: phrase translation , inverse phrase translation probability probability , lexical translation probability , inverse lexical translation probability , word penalty, rule penalty and a target ngram language model. The log-linear model (Och and Ney, 2003) is used to combine different features: where is a feature function, is the weight of . In this paper, we add two new features for automatically acquired rules and manually acquired rules into the log-linear model to distinguish different rules. The feature weights are optimized by minimum error rate training (Och, 2003). 2.2 2 2.1 Manually acquired Rules Backgrounds Manually acquired rules are accumulated by human being. Compared with automatically acquired rules, manually acquired rules are of better quality and can capture long sentence structures. Also, there can be constraints for nonterminals in manually acquired rules so that they could match more accurately. Examples of manually acquired rules in our Chinese-to-English patent translation system are shown in Table 1. Hierarchical Phrase-based SMT Hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) is the state-of-the-art SMT model. By utilizin"
2011.mtsummit-wpt.4,P03-1057,0,0.00906221,"omatic Evaluation of MT Quality We evaluate the MT quality in our experiments using the BLEU automatic evaluation metric (Papineni et al., 2002). BLEU measures the similarity between machine translation results and humanmade translation results (called references) by Ngram precision scores and allows multiple reference translations to model the variety of possible translations. BLEU aims to replace subjective evaluation and speed up the development cycle of MT systems. It is now used not only to aid developers but also for automatically tuning of MT systems (e.g., (Och, 2003; Su et al., 1992; Imamura et al., 2003)). In this paper, we use case-insensitive BLEU-4 and there’s only one reference for each input sentence. where denotes the nonterminal, indicates the length constraint of the nonterminal (0 means no length constraint), indicates the nonterminal must/must-not contain these words according to the symbol . indicates whether to match the first valid position or the last one in case there’re several valid positions. The full definition of nonterminals in English . The number of nonterminals in part is simply source side and target side should be the same. With these constraints, a manually acquired"
2011.mtsummit-wpt.4,N03-1017,0,0.00409721,"nslation system (Lü et al., 2007). There’re totally 9283 manually acquired rules. Each of them has at least one matched sentence in the training set. The goal of our experiments was to pick out high quality manually acquired rules which could benefit translation quality. 4.2 Table 3: Examples of short manually acquired rules which caused problems Description of Experiments Baseline Our baseline system was a traditional hierarchical phrase-based system. We obtained word alignments of training data by running GIZA++ (Och and Ney, 2003) and then applied the refinement rule “grow-diag-and-final” (Koehn et al., 2003). After that, we extracted automatically acquired rules according to Chiang 2007. We tuned the feature weights with minimum error rate training (Och, 2003). The traditional hierarchical phrase-based system achieved a BLEU score of 30.55 on the test set. Applying all manually acquired rules Our refined decoder is able to integrate manually acquired rules into the traditional HPB machine translation system. We combined manually acquired rules with automatically acquired rules by assigning manually acquired rules probabilities so The main problem of such rules was no limits on nonterminal length,"
2011.mtsummit-wpt.4,N09-2055,0,0.0379091,"Missing"
2014.amta-researchers.16,D08-1064,0,0.0452662,"Missing"
2014.amta-researchers.16,P05-1066,0,0.0754753,"Missing"
2014.amta-researchers.16,2003.mtsummit-papers.32,0,0.183485,"Missing"
2014.amta-researchers.16,P07-2045,0,0.00708752,"Missing"
2014.amta-researchers.16,niessen-etal-2000-evaluation,0,0.0969868,"Missing"
2014.amta-researchers.16,P02-1040,0,0.0932539,"Missing"
2014.amta-researchers.16,2006.amta-papers.25,0,0.103963,"Missing"
2014.amta-researchers.16,P12-3004,0,\N,Missing
2014.amta-researchers.16,W05-0909,0,\N,Missing
2014.amta-researchers.16,2009.mtsummit-papers.20,1,\N,Missing
2014.amta-researchers.19,P96-1041,0,0.080713,"Feature name Z 0, Z 1, Z 2, Z 3, Z 4, Z 5, Z 6, Z 7, Z 8, Z 9, Z 10 SCM non, SCM high, SCM low, SCM medium SPL 1, SPL 2, SPL 3, SPL 4, SPL 5, SPL 6, SPL 7 SEP Y, SEP N TCM non, TCM high, TCM low, TCM medium NLN 2 2, NLN 2 1, NLN 2 0, NLN 1 1, NLN 1 0, NLN 0 0 CSS non, CSS single, CSS left, CSS right, CSS both LTC non, LTC original, LTC left, LTC right, LTC both, LTC medium CPM AdjacentSame, CPM AdjacentSubstitute, CPM LinkedInterlived, CPM LinkedReversed, CPM SkipForward, CPM LinkedCorss, CPM SkipReversed Table 2: The list of TM features extracted on the best match in our system. discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights.2 However, when TM features are incorporated, the number of features grows to more than 50 (Table 2 show the features used in our system when only best match is considered). As MERT is known to be weak when the number of features grows (Durrani et al., 2013), we use MIRA (Cherry and Foster, 2012) instead to tune weights in this case. We set the maximum iteration of MIRA to be 25. Case-insensitive BLEU (Papineni et al., 2002) is used to evaluate the translation results. Bootstrap resampling (Koehn, 2004) is also performed"
2014.amta-researchers.19,N12-1047,0,0.0399914,"medium CPM AdjacentSame, CPM AdjacentSubstitute, CPM LinkedInterlived, CPM LinkedReversed, CPM SkipForward, CPM LinkedCorss, CPM SkipReversed Table 2: The list of TM features extracted on the best match in our system. discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights.2 However, when TM features are incorporated, the number of features grows to more than 50 (Table 2 show the features used in our system when only best match is considered). As MERT is known to be weak when the number of features grows (Durrani et al., 2013), we use MIRA (Cherry and Foster, 2012) instead to tune weights in this case. We set the maximum iteration of MIRA to be 25. Case-insensitive BLEU (Papineni et al., 2002) is used to evaluate the translation results. Bootstrap resampling (Koehn, 2004) is also performed to compute statistical significance with 1000 iterations. We implement Wang et al. (2013)’s method in Moses for comparison. This method needs first to train three models3 with the factored language model toolkit (Kirchhoff et al., 2007) over the feature sequence of phrase pairs. To obtain such phrase pairs for training, we do cross-folder translation on two language p"
2014.amta-researchers.19,W13-2212,0,0.0138212,"LTC left, LTC right, LTC both, LTC medium CPM AdjacentSame, CPM AdjacentSubstitute, CPM LinkedInterlived, CPM LinkedReversed, CPM SkipForward, CPM LinkedCorss, CPM SkipReversed Table 2: The list of TM features extracted on the best match in our system. discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights.2 However, when TM features are incorporated, the number of features grows to more than 50 (Table 2 show the features used in our system when only best match is considered). As MERT is known to be weak when the number of features grows (Durrani et al., 2013), we use MIRA (Cherry and Foster, 2012) instead to tune weights in this case. We set the maximum iteration of MIRA to be 25. Case-insensitive BLEU (Papineni et al., 2002) is used to evaluate the translation results. Bootstrap resampling (Koehn, 2004) is also performed to compute statistical significance with 1000 iterations. We implement Wang et al. (2013)’s method in Moses for comparison. This method needs first to train three models3 with the factored language model toolkit (Kirchhoff et al., 2007) over the feature sequence of phrase pairs. To obtain such phrase pairs for training, we do cro"
2014.amta-researchers.19,P10-1064,1,0.941617,"Missing"
2014.amta-researchers.19,C10-2043,1,0.912336,"Missing"
2014.amta-researchers.19,W04-3250,0,0.0511954,"unting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights.2 However, when TM features are incorporated, the number of features grows to more than 50 (Table 2 show the features used in our system when only best match is considered). As MERT is known to be weak when the number of features grows (Durrani et al., 2013), we use MIRA (Cherry and Foster, 2012) instead to tune weights in this case. We set the maximum iteration of MIRA to be 25. Case-insensitive BLEU (Papineni et al., 2002) is used to evaluate the translation results. Bootstrap resampling (Koehn, 2004) is also performed to compute statistical significance with 1000 iterations. We implement Wang et al. (2013)’s method in Moses for comparison. This method needs first to train three models3 with the factored language model toolkit (Kirchhoff et al., 2007) over the feature sequence of phrase pairs. To obtain such phrase pairs for training, we do cross-folder translation on two language pairs. For the English–Chinese task, we split the training data into 50 parts and build 50 systems with the above settings by taking each part as test data and the rest as training data. Systems are tuned via the"
2014.amta-researchers.19,P07-2045,0,0.0282952,"elation between different features. 3 Our Method In this section, we present a generalized discriminative framework which can integrate TM into SMT at decoding time. Under this framework, we add features from Wang et al. (2013) into the phrase-based model as TM feature functions. In addition, we describe how to use multiple fuzzy matches efficiently to improve translation quality. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 250 3.1 Discriminative Framework Generally, in a state-of-the-art statistical translation framework like Moses (Koehn et al., 2007), the direct translation probability is given by a discriminative framework, as shown in Equation (1): PM exp{ m=1 λm hm (e, f )} (1) P (e |f ) = P PM 0 e0 exp{ m=1 λm hm (e , f )} where hm (e, f ) denotes the mth feature function for target e and source f , λm is the weight of this feature function, and M is the number of feature functions considered. This framework works well on pre-defined features, such as the translation model features and language model features, which are based on target e and source f . However, as is well-known, once these features have been induced, the training data"
2014.amta-researchers.19,N03-1017,0,0.0220482,"able JRC-Acquis corpus.1 Sentences are tokenized with scripts in Moses. We randomly select 3000 sentence pairs as dev data and 3000 as test data. We filter sentence pairs longer than 80 words in the training data and 100 words in the dev and test data. We also keep the length ratio less than or equal to 3 in all data sets. Table 1 also shows a summary of English–French corpus. 4.2 Baseline On both language-pairs, we take the phrase-based model in Moses with default settings as our baseline. Word alignment is performed by GIZA++ (Och and Ney, 2003), with heuristic function grow-diag-final-and (Koehn et al., 2003). We use SRILM (Stolcke, 2002) to train a 5-gram language model on the target side of the training data with modified Kneser-Ney 1 http://ipsc.jrc.ec.europa.eu/index.php?id=198 Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 255 Feature Set Zi SCM s SPL i SEP ¯ TCM s NLN x y CSS s LTC s CPM s Feature name Z 0, Z 1, Z 2, Z 3, Z 4, Z 5, Z 6, Z 7, Z 8, Z 9, Z 10 SCM non, SCM high, SCM low, SCM medium SPL 1, SPL 2, SPL 3, SPL 4, SPL 5, SPL 6, SPL 7 SEP Y, SEP N TCM non, TCM high, TCM low, TCM medium NLN 2 2, NLN 2 1, NLN 2 0, NLN 1 1, NLN 1 0"
2014.amta-researchers.19,2010.jec-1.4,0,0.517381,"of these models. Given a large amount of data, SMT can generate better results for unseen sentences than TM. However, unless sentence-caching is utilised, it treats a seen sentence (such as a sentence in the training data) as unseen. Clearly, TM and SMT complement one another on matched and unmatched segments, so both are receiving increasing attention from translators and researchers, who would like to combine TM and SMT together to obtain better translation quality with methods such as system recommendation (He et al., 2010a,b) or using fragments from TM in SMT (Bic¸ici and Dymetman, 2008; Koehn and Senellart, 2010; Ma et al., 2011; Wang et al., 2013) This paper is focused on integrating TM into SMT to improve translation quality. We present a discriminative framework which directly integrates TM-related feature functions into SMT. In this paper, we change features extracted from TM which are defined in a generative Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 249 model (Wang et al., 2013) to feature functions and add them into the phrase-based translation model. Experiments on English–Chinese and English–French tasks show that our method achiev"
2014.amta-researchers.19,P11-1124,1,0.888937,"Missing"
2014.amta-researchers.19,P03-1021,0,0.0154985,", Z 10 SCM non, SCM high, SCM low, SCM medium SPL 1, SPL 2, SPL 3, SPL 4, SPL 5, SPL 6, SPL 7 SEP Y, SEP N TCM non, TCM high, TCM low, TCM medium NLN 2 2, NLN 2 1, NLN 2 0, NLN 1 1, NLN 1 0, NLN 0 0 CSS non, CSS single, CSS left, CSS right, CSS both LTC non, LTC original, LTC left, LTC right, LTC both, LTC medium CPM AdjacentSame, CPM AdjacentSubstitute, CPM LinkedInterlived, CPM LinkedReversed, CPM SkipForward, CPM LinkedCorss, CPM SkipReversed Table 2: The list of TM features extracted on the best match in our system. discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights.2 However, when TM features are incorporated, the number of features grows to more than 50 (Table 2 show the features used in our system when only best match is considered). As MERT is known to be weak when the number of features grows (Durrani et al., 2013), we use MIRA (Cherry and Foster, 2012) instead to tune weights in this case. We set the maximum iteration of MIRA to be 25. Case-insensitive BLEU (Papineni et al., 2002) is used to evaluate the translation results. Bootstrap resampling (Koehn, 2004) is also performed to compute statistical significance with 1000 it"
2014.amta-researchers.19,J03-1002,0,0.00446471,"wn in Table 1. Our English–French data is from the publicly available JRC-Acquis corpus.1 Sentences are tokenized with scripts in Moses. We randomly select 3000 sentence pairs as dev data and 3000 as test data. We filter sentence pairs longer than 80 words in the training data and 100 words in the dev and test data. We also keep the length ratio less than or equal to 3 in all data sets. Table 1 also shows a summary of English–French corpus. 4.2 Baseline On both language-pairs, we take the phrase-based model in Moses with default settings as our baseline. Word alignment is performed by GIZA++ (Och and Ney, 2003), with heuristic function grow-diag-final-and (Koehn et al., 2003). We use SRILM (Stolcke, 2002) to train a 5-gram language model on the target side of the training data with modified Kneser-Ney 1 http://ipsc.jrc.ec.europa.eu/index.php?id=198 Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 255 Feature Set Zi SCM s SPL i SEP ¯ TCM s NLN x y CSS s LTC s CPM s Feature name Z 0, Z 1, Z 2, Z 3, Z 4, Z 5, Z 6, Z 7, Z 8, Z 9, Z 10 SCM non, SCM high, SCM low, SCM medium SPL 1, SPL 2, SPL 3, SPL 4, SPL 5, SPL 6, SPL 7 SEP Y, SEP N TCM non, TCM hig"
2014.amta-researchers.19,P02-1040,0,0.0899527,"Reversed Table 2: The list of TM features extracted on the best match in our system. discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights.2 However, when TM features are incorporated, the number of features grows to more than 50 (Table 2 show the features used in our system when only best match is considered). As MERT is known to be weak when the number of features grows (Durrani et al., 2013), we use MIRA (Cherry and Foster, 2012) instead to tune weights in this case. We set the maximum iteration of MIRA to be 25. Case-insensitive BLEU (Papineni et al., 2002) is used to evaluate the translation results. Bootstrap resampling (Koehn, 2004) is also performed to compute statistical significance with 1000 iterations. We implement Wang et al. (2013)’s method in Moses for comparison. This method needs first to train three models3 with the factored language model toolkit (Kirchhoff et al., 2007) over the feature sequence of phrase pairs. To obtain such phrase pairs for training, we do cross-folder translation on two language pairs. For the English–Chinese task, we split the training data into 50 parts and build 50 systems with the above settings by taking"
2014.amta-researchers.19,2008.amta-srw.6,0,0.0188191,"or comparison. This method needs first to train three models3 with the factored language model toolkit (Kirchhoff et al., 2007) over the feature sequence of phrase pairs. To obtain such phrase pairs for training, we do cross-folder translation on two language pairs. For the English–Chinese task, we split the training data into 50 parts and build 50 systems with the above settings by taking each part as test data and the rest as training data. Systems are tuned via the devset for the task. For the English–French task, we do 10-cross folder training. After training the systems, forced decoding (Schwartz, 2008) is used to generate the corresponding phrase segmentation on the test data. Then features are extracted on those phrase correspondences.4 We also implement our method in Moses. In this paper, training data is taken as the TM data, so phrase rules from the TM are already included during translation. After the SMT models are trained, word alignment of the TM is also produced as a by-product. 4.3 Experiment Results Table 3 shows our experiment results on two language pairs. We found that our system with TM features achieves comparable results (+0.24/+0.31 on the dev set and +0.17/-0.01 on the te"
2014.amta-researchers.19,2006.amta-papers.25,0,0.0258777,"k in Section 5. 2 Related Work As shown in experiments (e.g. Koehn and Senellart (2010) and Wang et al. (2013)), TM can give better translation than SMT for highly matched segments; SMT is more reliable than TM for other segments. Because of such complementariness, combining TM and SMT together has been explored by some researchers in recent years. He et al. (2010a) present a recommendation system which uses an SVM (Cortes and Vapnik, 1995) binary classifier to select a translation from the outputs of TM and SMT with the selected translation being more suitable to post-editing. They take TER (Snover et al., 2006) score as the measure of post-editing effort and use it to create training instances for SVM. He et al. (2010b) extend this work by re-ranking the N-best list of SMT and TM. However, these works are focused on sentence-level selection and thus the matched phrases in TM are not used so well. For an input sentence, even though it does not have an exact match in the TM, there are some matched phrases which could provide useful hints for translation. Bic¸ici and Dymetman (2008) present a dynamic TM approach which dynamically adds the longest matched noncontinuous phrase and its translation in the"
2014.amta-researchers.19,P13-1002,0,0.293096,"ta, SMT can generate better results for unseen sentences than TM. However, unless sentence-caching is utilised, it treats a seen sentence (such as a sentence in the training data) as unseen. Clearly, TM and SMT complement one another on matched and unmatched segments, so both are receiving increasing attention from translators and researchers, who would like to combine TM and SMT together to obtain better translation quality with methods such as system recommendation (He et al., 2010a,b) or using fragments from TM in SMT (Bic¸ici and Dymetman, 2008; Koehn and Senellart, 2010; Ma et al., 2011; Wang et al., 2013) This paper is focused on integrating TM into SMT to improve translation quality. We present a discriminative framework which directly integrates TM-related feature functions into SMT. In this paper, we change features extracted from TM which are defined in a generative Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 249 model (Wang et al., 2013) to feature functions and add them into the phrase-based translation model. Experiments on English–Chinese and English–French tasks show that our method achieves comparable results with Wang et al"
2014.amta-researchers.8,D11-1033,0,0.255256,"a transductive-learning framework to increase the count of important in-domain training instances, which results in phrase-pair weights being favourable to the development set. Bic¸ici and Yuret (2011) employ a feature decay algorithm which can be used in both active learning and transductive learning settings. The decay algorithm is used to increase the variety of the training set by devaluing features that have already been seen from a training set. In recent studies, a cross-entropy difference method has seen increasing interest for the problem of SMT data selection (Moore and Lewis, 2010; Axelrod et al., 2011). The training dataset is ranked using cross-entropy difference from some language models trained on in-domain or general-domain sentences. Then a threshold is set to select the pseudo indomain sentences. The intuition is to find sentences as close to the target domain and as far from the average of the general-domain as possible. Later, Mansour et al. (2011) argue that “An LM does not capture the connections between the source and target words, and scores the sentences independently”, and linearly interpolate IBM model 1 (Brown et al., 1993) into the cross-entropy difference framework. The tr"
2014.amta-researchers.8,W11-2131,0,0.0375125,"Missing"
2014.amta-researchers.8,2011.iwslt-evaluation.18,0,0.313433,".dcu.ie liangyouli@computing.dcu.ie away@computing.dcu.ie qliu@computing.dcu.ie The CNGL Centre for Global Intelligent Content,School of Computing,Dublin City University, Ireland Abstract In this paper, we describe an effective translation model combination approach based on the estimation of a probabilistic Support Vector Machine (SVM). We collect domain knowledge from both in-domain and general-domain corpora inspired by a commonly used data selection algorithm, which we then use as features for the SVM training. Drawing on previous work on binary-featured phrase table fill-up (Nakov, 2008; Bisazza et al., 2011), we substitute the binary feature in the original work with our probabilistic domain-likeness feature. Later, we design two experiments to evaluate the proposed probabilistic feature-based approach on the French-to-English language pair using data provided at WMT07, WMT13 and IWLST11 translation tasks. Our experiments demonstrate that translation performance can gain significant improvements of up to +0.36 and +0.82 BLEU scores by using our probabilistic feature-based translation model fill-up approach compared with the binary featured fill-up approach in both experiments. 1 Introduction Like"
2014.amta-researchers.8,J93-2003,0,0.0338094,"lem of SMT data selection (Moore and Lewis, 2010; Axelrod et al., 2011). The training dataset is ranked using cross-entropy difference from some language models trained on in-domain or general-domain sentences. Then a threshold is set to select the pseudo indomain sentences. The intuition is to find sentences as close to the target domain and as far from the average of the general-domain as possible. Later, Mansour et al. (2011) argue that “An LM does not capture the connections between the source and target words, and scores the sentences independently”, and linearly interpolate IBM model 1 (Brown et al., 1993) into the cross-entropy difference framework. The translation performance is improved on both Arabicto-English and English-to-French translation tasks compared with the standalone cross-entropy difference approach. Applying adaptation techniques to the statistical models, especially to the translation model, is another popular approach used in domain adaptation for SMT. Some research follows the path of adding in new features into the phrase table. Chen et al. (2013) add vector similarity into the phrase table and use it as a tuning- and decoding-time feature. The similarity is computed by com"
2014.amta-researchers.8,P13-1126,0,0.0147256,"connections between the source and target words, and scores the sentences independently”, and linearly interpolate IBM model 1 (Brown et al., 1993) into the cross-entropy difference framework. The translation performance is improved on both Arabicto-English and English-to-French translation tasks compared with the standalone cross-entropy difference approach. Applying adaptation techniques to the statistical models, especially to the translation model, is another popular approach used in domain adaptation for SMT. Some research follows the path of adding in new features into the phrase table. Chen et al. (2013) add vector similarity into the phrase table and use it as a tuning- and decoding-time feature. The similarity is computed by comparing the vectorized representation of phrase pairs extracted from the development set and the training set. Eidelman et al. (2012) achieve translation performance improvement by including a lexical weight topic feature into the translation model. The topic model used in their work is built based on the source side of the training sentences. There is also work which focuses on translation model combination. Foster and Kuhn (2007) and Koehn and Schroeder (2007) combi"
2014.amta-researchers.8,P12-2023,0,0.0146632,"and English-to-French translation tasks compared with the standalone cross-entropy difference approach. Applying adaptation techniques to the statistical models, especially to the translation model, is another popular approach used in domain adaptation for SMT. Some research follows the path of adding in new features into the phrase table. Chen et al. (2013) add vector similarity into the phrase table and use it as a tuning- and decoding-time feature. The similarity is computed by comparing the vectorized representation of phrase pairs extracted from the development set and the training set. Eidelman et al. (2012) achieve translation performance improvement by including a lexical weight topic feature into the translation model. The topic model used in their work is built based on the source side of the training sentences. There is also work which focuses on translation model combination. Foster and Kuhn (2007) and Koehn and Schroeder (2007) combine the translation models in a log-linear model at tuning and decoding time. Sennrich (2012) proposes an approach to interpolate the translation models based on perplexity minimization. Haddow and Koehn (2012) focus on the extracting and scoring steps when buil"
2014.amta-researchers.8,W07-0717,0,0.0252747,"g in new features into the phrase table. Chen et al. (2013) add vector similarity into the phrase table and use it as a tuning- and decoding-time feature. The similarity is computed by comparing the vectorized representation of phrase pairs extracted from the development set and the training set. Eidelman et al. (2012) achieve translation performance improvement by including a lexical weight topic feature into the translation model. The topic model used in their work is built based on the source side of the training sentences. There is also work which focuses on translation model combination. Foster and Kuhn (2007) and Koehn and Schroeder (2007) combine the translation models in a log-linear model at tuning and decoding time. Sennrich (2012) proposes an approach to interpolate the translation models based on perplexity minimization. Haddow and Koehn (2012) focus on the extracting and scoring steps when building a phrase table for SMT. One of the conclusions is that while out-of-domain data can improve the translation coverage for rare words, it may be harmful for common in-domain words. This suggests that the translations which contain a lot of in-domain evidence should be kept. 2 Related Work The trans"
2014.amta-researchers.8,W08-0509,0,0.0209798,"© The Authors 101 Experiment prob-fill-up(nc 2007,ep 2007) C 16 γ 0.125 Accuracy 0.8139 prob-fill-up(ted 11,nc v9) 2 0.03125 0.8565 Table 3: SVM-tuned parameters values C and γ, where C is the trade-off parameter in equation (3), and γ adjusts the width of RBF in equation (4). 5.3 Translation System Training All SMT systems in our experiments are trained using the phrase-based SMT with Moses 1.0 framework. The reordering model is not included in our translation system since we are interested only in measuring the system effects coming from translation models. We use the word aligner MGIZA++ (Gao and Vogel, 2008) for word alignment in both translation directions, and then symmetrize the word alignment models using the heuristic of grow-diag-final-and. We use all five default Moses 1.0 translation model features. The translation systems are tuned with minimum error rate training (Och, 2003) using case-insensitive BLEU (Papineni et al., 2002) as the optimization measure. A 5-gram language model is trained with the open source IRSTLM toolkit using all the available target sentences in each of the fill-up experiment scenarios. We use the Moses default language model toolkit KenLM at the tuning and decodin"
2014.amta-researchers.8,W12-3154,0,0.137586,"acted from the development set and the training set. Eidelman et al. (2012) achieve translation performance improvement by including a lexical weight topic feature into the translation model. The topic model used in their work is built based on the source side of the training sentences. There is also work which focuses on translation model combination. Foster and Kuhn (2007) and Koehn and Schroeder (2007) combine the translation models in a log-linear model at tuning and decoding time. Sennrich (2012) proposes an approach to interpolate the translation models based on perplexity minimization. Haddow and Koehn (2012) focus on the extracting and scoring steps when building a phrase table for SMT. One of the conclusions is that while out-of-domain data can improve the translation coverage for rare words, it may be harmful for common in-domain words. This suggests that the translations which contain a lot of in-domain evidence should be kept. 2 Related Work The translation model fill-up approach was introduced into SMT by Nakov (2008). In his work, the phrase tables are merged by keeping all the phrase pairs unchanged from the in-domain phrase table, and only adding in the phrase pairs from the general-domai"
2014.amta-researchers.8,W11-2123,0,0.020995,"es the same restrictions as in Moore and Lewis (2010), where a token is treated as an instance of &lt;UNK&gt; unless it appears at least twice at the in-domain training dataset. We keep T number of SVM training sentences to tune the parameters in equations (3) and (4). We test the accuracy of the trained SVM using the corresponding SMT development data. The data used for SVM training, language model training and SVM tuning are summarized in Table 2. The SVM-tuned parameters are presented in Table 3. We use the open source IRSTLM toolkit (Federico et al., 2008) for language model training and KenLM (Heafield, 2011) to compute the sentence perplexity. Experiment prob-fill-up(nc 2007,ep 2007) M 42,884 N 40,000 T 2,884 prob-fill-up(ted 11,nc v9) 50,000 45,000 5,000 Table 2: SVM data statistics, where M,N and T are the data sizes (in sentences) used for training, tuning and testing, respectively. 3 http://www.csie.ntu.edu.tw/ ˜cjlin/libsvm Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 101 Experiment prob-fill-up(nc 2007,ep 2007) C 16 γ 0.125 Accuracy 0.8139 prob-fill-up(ted 11,nc v9) 2 0.03125 0.8565 Table 3: SVM-tuned parameters values C and γ, wher"
2014.amta-researchers.8,W04-3250,0,0.0565569,"toolkit using all the available target sentences in each of the fill-up experiment scenarios. We use the Moses default language model toolkit KenLM at the tuning and decoding time. 5.4 Results We set our baseline systems to be the fill-up system of Bisazza et al. (2011) (fill-up(experiment)), which has been integrated within the Moses 1.0 framework. Tables 4 and 5 report our results using case-insensitive BLEU on the corresponding test sets. We use † to indicate where the probabilistic feature-based fill-up approach systems (prob-fill-up heuristic(experiment)) achieve significant improvement (Koehn, 2004) compared with the baseline systems at the level p = 0.01 level with 1000 iterations. System fill-up(nc 2007,ep 2007) Test (news-test2007) 28.01 prob-fill-up Min(nc 2007,ep 2007) 28.03 prob-fill-up-Arithmetic Mean(nc 2007,ep 2007) 28.21 prob-fill-up-Geometric Mean(nc 2007,ep 2007) 28.37† Table 4: prob-fill-up heuristic(nc 2007,ep 2007) experiment BLEU scores on testing data, the significance testing at the level p = 0.01 level with 1000 iterations. The result of the prob-fill-up heuristic(nc 2007,ep 2007) experiment in Table 4 shows that the probabilistic feature-based fill-up systems using th"
2014.amta-researchers.8,P07-2045,0,0.00464247,"he source-side perplexity difference and the target-side perplexity difference in this feature set. 5 Experiment 5.1 Corpora The experiments in this paper use data from WMT07, WMT13 and IWLST11 translation tasks. We choose our experiments on the French-to-English language pair. We first perform some standard data cleaning steps, including tokenization, punctuation normalization, replacement of special characters, lower casing and long sentence removal ( &lt;0 or &gt;80 ), resulting in the preprocessed data summarized in Table 1. We use scripts provided within Moses 1.0 translation system framework (Koehn et al., 2007)2 for all cleaning steps. Corpus News Commentary (nc 2007) Train 42,884 Tune 1,064 (nc-devtest200) Test 2,007 (news-test2007) Europarl (ep 2007) 1,257,436 n/a n/a TED (ted 11) 106,642 934 (dev2010) 1,664 (tst2010) news-commentary-v9 (nc v9) 181,274 n/a n/a Table 1: SMT training corpus statistics There are two fill-up experiments designed to evaluate our approach, defined as prob-fillup heuristic(in-domain,general-domain), such as prob-fill-up heuristic(nc 2007,ep 2007) and prob-fill-up heuristic(ted 11,nc v9), where heuristic refers to the heuristics stated in Section 3 of this paper. The expe"
2014.amta-researchers.8,W07-0733,0,0.0339872,"phrase table. Chen et al. (2013) add vector similarity into the phrase table and use it as a tuning- and decoding-time feature. The similarity is computed by comparing the vectorized representation of phrase pairs extracted from the development set and the training set. Eidelman et al. (2012) achieve translation performance improvement by including a lexical weight topic feature into the translation model. The topic model used in their work is built based on the source side of the training sentences. There is also work which focuses on translation model combination. Foster and Kuhn (2007) and Koehn and Schroeder (2007) combine the translation models in a log-linear model at tuning and decoding time. Sennrich (2012) proposes an approach to interpolate the translation models based on perplexity minimization. Haddow and Koehn (2012) focus on the extracting and scoring steps when building a phrase table for SMT. One of the conclusions is that while out-of-domain data can improve the translation coverage for rare words, it may be harmful for common in-domain words. This suggests that the translations which contain a lot of in-domain evidence should be kept. 2 Related Work The translation model fill-up approach w"
2014.amta-researchers.8,D07-1036,1,0.884701,"Missing"
2014.amta-researchers.8,2011.iwslt-papers.5,0,0.0375411,"Missing"
2014.amta-researchers.8,P10-2041,0,0.556658,"etrieval techniques on a transductive-learning framework to increase the count of important in-domain training instances, which results in phrase-pair weights being favourable to the development set. Bic¸ici and Yuret (2011) employ a feature decay algorithm which can be used in both active learning and transductive learning settings. The decay algorithm is used to increase the variety of the training set by devaluing features that have already been seen from a training set. In recent studies, a cross-entropy difference method has seen increasing interest for the problem of SMT data selection (Moore and Lewis, 2010; Axelrod et al., 2011). The training dataset is ranked using cross-entropy difference from some language models trained on in-domain or general-domain sentences. Then a threshold is set to select the pseudo indomain sentences. The intuition is to find sentences as close to the target domain and as far from the average of the general-domain as possible. Later, Mansour et al. (2011) argue that “An LM does not capture the connections between the source and target words, and scores the sentences independently”, and linearly interpolate IBM model 1 (Brown et al., 1993) into the cross-entropy diffe"
2014.amta-researchers.8,W08-0320,0,0.285391,"ngj@computing.dcu.ie liangyouli@computing.dcu.ie away@computing.dcu.ie qliu@computing.dcu.ie The CNGL Centre for Global Intelligent Content,School of Computing,Dublin City University, Ireland Abstract In this paper, we describe an effective translation model combination approach based on the estimation of a probabilistic Support Vector Machine (SVM). We collect domain knowledge from both in-domain and general-domain corpora inspired by a commonly used data selection algorithm, which we then use as features for the SVM training. Drawing on previous work on binary-featured phrase table fill-up (Nakov, 2008; Bisazza et al., 2011), we substitute the binary feature in the original work with our probabilistic domain-likeness feature. Later, we design two experiments to evaluate the proposed probabilistic feature-based approach on the French-to-English language pair using data provided at WMT07, WMT13 and IWLST11 translation tasks. Our experiments demonstrate that translation performance can gain significant improvements of up to +0.36 and +0.82 BLEU scores by using our probabilistic feature-based translation model fill-up approach compared with the binary featured fill-up approach in both experimen"
2014.amta-researchers.8,P03-1021,0,0.0249442,"on System Training All SMT systems in our experiments are trained using the phrase-based SMT with Moses 1.0 framework. The reordering model is not included in our translation system since we are interested only in measuring the system effects coming from translation models. We use the word aligner MGIZA++ (Gao and Vogel, 2008) for word alignment in both translation directions, and then symmetrize the word alignment models using the heuristic of grow-diag-final-and. We use all five default Moses 1.0 translation model features. The translation systems are tuned with minimum error rate training (Och, 2003) using case-insensitive BLEU (Papineni et al., 2002) as the optimization measure. A 5-gram language model is trained with the open source IRSTLM toolkit using all the available target sentences in each of the fill-up experiment scenarios. We use the Moses default language model toolkit KenLM at the tuning and decoding time. 5.4 Results We set our baseline systems to be the fill-up system of Bisazza et al. (2011) (fill-up(experiment)), which has been integrated within the Moses 1.0 framework. Tables 4 and 5 report our results using case-insensitive BLEU on the corresponding test sets. We use †"
2014.amta-researchers.8,P02-1038,0,0.192404,"atured fill-up approach in both experiments. 1 Introduction Like many machine-learning problems, Statistical Machine Translation (SMT) is a datadependent learning approach. The prerequisite is large amounts of training data in order to generate statistical models. In general, the training data has to be sentence-aligned and bilingual. Some heuristic approaches are often used when deconstructing the training data into phrase-level representations, and the statistical models are computed based on the phrase probability distributions. The generated models are then combined in a log-linear model (Och and Ney, 2002). A basic SMT system may consist of a translation model and a language model, where the translation model provides a target-language translation e for a source-language sentence f, and the language model ensures the fluency of the target-language translation e. One challenge which rises above others in SMT is that the translation performance decreases when there are dissimilarities between the training and the testing environments. This type of challenge is often defined as “domain adaptation” in previous work. The underlying reasons that caused domain adaptation challenge are many, but the ob"
2014.amta-researchers.8,P02-1040,0,0.0896526,"r experiments are trained using the phrase-based SMT with Moses 1.0 framework. The reordering model is not included in our translation system since we are interested only in measuring the system effects coming from translation models. We use the word aligner MGIZA++ (Gao and Vogel, 2008) for word alignment in both translation directions, and then symmetrize the word alignment models using the heuristic of grow-diag-final-and. We use all five default Moses 1.0 translation model features. The translation systems are tuned with minimum error rate training (Och, 2003) using case-insensitive BLEU (Papineni et al., 2002) as the optimization measure. A 5-gram language model is trained with the open source IRSTLM toolkit using all the available target sentences in each of the fill-up experiment scenarios. We use the Moses default language model toolkit KenLM at the tuning and decoding time. 5.4 Results We set our baseline systems to be the fill-up system of Bisazza et al. (2011) (fill-up(experiment)), which has been integrated within the Moses 1.0 framework. Tables 4 and 5 report our results using case-insensitive BLEU on the corresponding test sets. We use † to indicate where the probabilistic feature-based fi"
2014.amta-researchers.8,E12-1055,0,0.0141224,"ding-time feature. The similarity is computed by comparing the vectorized representation of phrase pairs extracted from the development set and the training set. Eidelman et al. (2012) achieve translation performance improvement by including a lexical weight topic feature into the translation model. The topic model used in their work is built based on the source side of the training sentences. There is also work which focuses on translation model combination. Foster and Kuhn (2007) and Koehn and Schroeder (2007) combine the translation models in a log-linear model at tuning and decoding time. Sennrich (2012) proposes an approach to interpolate the translation models based on perplexity minimization. Haddow and Koehn (2012) focus on the extracting and scoring steps when building a phrase table for SMT. One of the conclusions is that while out-of-domain data can improve the translation coverage for rare words, it may be harmful for common in-domain words. This suggests that the translations which contain a lot of in-domain evidence should be kept. 2 Related Work The translation model fill-up approach was introduced into SMT by Nakov (2008). In his work, the phrase tables are merged by keeping all t"
2015.eamt-1.12,2013.iwslt-evaluation.20,0,0.0150948,"commercial rule-based engine for English–Farsi translation. It contains 1.5 million words in its database and includes specific dictionaries for 33 different fields of science. Another English–Farsi MT system was developed by the Iran Supreme Council of Information.4 Postchi5 is a bidirectional system listed among the EuroMatrix6 systems for the Farsi language. These systems are not terribly robust or precise examples of Farsi SMT and are usually the by-products of research or commercial projects. The only system that has officially been reported for the purpose of Farsi SMT is FBK’s system (Bertoldi et al., 2013). It was tested on a publicly available dataset and from this viewpoint is the most important system for our purposes.7 2.2 Parallel Corpora for Farsi SMT The first attempts at generating Farsi–English parallel corpora are documented in the Shiraz project (Zajac et al., 2000). The authors constructed a corpus of 3000 parallel sentences, which were translated manually from monolingual online Farsi documents at New Mexico State University. More recently Qasemizadeh et al. (2007) participated in the Farsi part of MULTEXT-EAST8 project (Erjavec, 2010) and developed about 6000 sentences. There is a"
2015.eamt-1.12,2012.eamt-1.60,0,0.0349081,"Missing"
2015.eamt-1.12,erjavec-2010-multext,0,0.0270435,"purpose of Farsi SMT is FBK’s system (Bertoldi et al., 2013). It was tested on a publicly available dataset and from this viewpoint is the most important system for our purposes.7 2.2 Parallel Corpora for Farsi SMT The first attempts at generating Farsi–English parallel corpora are documented in the Shiraz project (Zajac et al., 2000). The authors constructed a corpus of 3000 parallel sentences, which were translated manually from monolingual online Farsi documents at New Mexico State University. More recently Qasemizadeh et al. (2007) participated in the Farsi part of MULTEXT-EAST8 project (Erjavec, 2010) and developed about 6000 sentences. There is also a corpus available in ELRA9 consisting of about 3,500,000 English and Farsi words aligned at sentence level (about 100,000 sentences). This is a mixed domain dataset including a variety of text types such as art, law, culture, literature, poetry, proverbs, religion etc. PEN (Parallel English–Persian News corpus) is another small corpus (Farajian, 2011) generated semi-automatically. It includes almost 30,000 sentences. Farajian developed a method to find similar sentence pairs and for quality assurance used Google Translate.10 All these corpora"
2015.eamt-1.12,N03-1017,0,0.0361792,"Missing"
2015.eamt-1.12,P07-2045,0,0.0261526,"scuss the problems with Mizan in Section 4.1 and perform error analysis on the output translations, where it is used as the SMT training data. In the second part using TEP and TEP++ we carry out monolingual translation between SF and FF (SF2FF) and discuss some use-cases for this type of translation task. Finally in the last part we show how SF2FF boosts the SMT quality for Farsi and report our results on the IWSLT–2013 dataset providing a comparison with FBK’s system. 4.1 Mizan, TEP and TEP++ To test the performance of our engines, they were trained using Mizan, TEP and TEP++. We used Moses (Koehn et al., 2007) with the default configuration for phrase-based translation. For the language modeling, SRILM (Stolcke and others, 2002) was used. The evaluation measure is BLEU (Papineni et al., 2002) and to tune the models, we applied MERT (Och, 2003). Table 1 summarizes our experimental results for the Mizan dataset. We evaluated with two types of language models, 3gram (LM3) and 5-gram (LM5). Numbers for both before and after tuning are reported. For all experiments training, tuning and test sets were selected randomly from the main corpus. The size of the test set is 1,000 and the tuning set is 2000 sen"
2015.eamt-1.12,W04-3250,0,0.337904,"Missing"
2015.eamt-1.12,P03-1021,0,0.0260544,"d discuss some use-cases for this type of translation task. Finally in the last part we show how SF2FF boosts the SMT quality for Farsi and report our results on the IWSLT–2013 dataset providing a comparison with FBK’s system. 4.1 Mizan, TEP and TEP++ To test the performance of our engines, they were trained using Mizan, TEP and TEP++. We used Moses (Koehn et al., 2007) with the default configuration for phrase-based translation. For the language modeling, SRILM (Stolcke and others, 2002) was used. The evaluation measure is BLEU (Papineni et al., 2002) and to tune the models, we applied MERT (Och, 2003). Table 1 summarizes our experimental results for the Mizan dataset. We evaluated with two types of language models, 3gram (LM3) and 5-gram (LM5). Numbers for both before and after tuning are reported. For all experiments training, tuning and test sets were selected randomly from the main corpus. The size of the test set is 1,000 and the tuning set is 2000 sentences. Training set sizes are reported in tables. For all experiments BLEU scores for Google Translate are reported as a baseline. LM3 LM5 Google Translate Training set Corpus EN–FA Before After 8.24 10.47 8.54 10.53 FA–EN Before After 1"
2015.eamt-1.12,P02-1040,0,0.0920216,"e carry out monolingual translation between SF and FF (SF2FF) and discuss some use-cases for this type of translation task. Finally in the last part we show how SF2FF boosts the SMT quality for Farsi and report our results on the IWSLT–2013 dataset providing a comparison with FBK’s system. 4.1 Mizan, TEP and TEP++ To test the performance of our engines, they were trained using Mizan, TEP and TEP++. We used Moses (Koehn et al., 2007) with the default configuration for phrase-based translation. For the language modeling, SRILM (Stolcke and others, 2002) was used. The evaluation measure is BLEU (Papineni et al., 2002) and to tune the models, we applied MERT (Och, 2003). Table 1 summarizes our experimental results for the Mizan dataset. We evaluated with two types of language models, 3gram (LM3) and 5-gram (LM5). Numbers for both before and after tuning are reported. For all experiments training, tuning and test sets were selected randomly from the main corpus. The size of the test set is 1,000 and the tuning set is 2000 sentences. Training set sizes are reported in tables. For all experiments BLEU scores for Google Translate are reported as a baseline. LM3 LM5 Google Translate Training set Corpus EN–FA Bef"
2015.eamt-1.12,I13-1144,0,0.0402882,"Missing"
2015.tc-1.3,W15-5202,0,0.0466142,"cant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a). Resources like MyMemory2 contain large number of bi-segments that can be used in translation memories, but not all the bi-segments are true translations. For this reason, 2 https://mymemory.translated.net/ 19 Barbu (2015) proposed a method based on machine learning for cleaning existing translation memories. 2.3 Incorporation of Language Technology in Translation Memories Translation memories are among the most successfully used tools by professional translators. However, most of these tools rely on little language processing when they match and retrieve segments. Research carried out in the EXPERT project shows that even incorporation of simple language processing such as paraphrasing can help translators (Gupta and Or˘asan, 2014). Rather than expanding the segments stored in a translation memory with all the"
2015.tc-1.3,2014.tc-1.6,1,0.723752,"metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a). Resources like MyMemory2 contain large number of bi-segments that can be used in translation memories, but not all the bi-segments are true translations. For this reason, 2 https://mymemory.translated.net/ 19 Barbu (2015) proposed a method based on machine learning for cleaning existing translation memorie"
2015.tc-1.3,D14-1062,1,0.857312,"Missing"
2015.tc-1.3,C14-1182,1,0.796863,"Missing"
2015.tc-1.3,N15-1043,1,0.871728,"Missing"
2015.tc-1.3,2015.mtsummit-papers.22,1,0.879605,"Missing"
2015.tc-1.3,2014.eamt-1.2,0,0.0541452,"Missing"
2015.tc-1.3,W15-4905,1,0.824476,"Missing"
2015.tc-1.3,2014.amta-researchers.19,1,0.706099,"professional translators reveals that the post-editing effort is also reduced. Logacheva and Specia (2015) investigate ways to collect and generate negative human feedback in various forms, including post-editing, and learn how to improve machine translation systems from this feedback, for example, by building word-level quality estimation models to mimic user feedback and introducing the predictions in SMT decoders. 2.5 Hybrid Approaches to Translation All the existing methods in MT have strengths and weaknesses and one of the most common ways to improve their performance is to combine them. Li et al. (2014) proposed a method for incorporating translation memories and linguistic knowledge in SMT, showing that for English-Chinese and English-French the proposed methods lead to better translations. Translation into morphological rich languages poses challenges to current methods in statistical machine translation. For this problem, Daiber and Sima’an (2015) propose a method which consists of two steps: first the source string is enriched with target morphological features and then fed into a translation model which takes care of reordering and lexical choice that 20 matches the provided morphologic"
2015.tc-1.3,W15-4907,1,0.836766,"by Scarton and Specia (2014) in the EXPERT project focuses on document level quality estimation. Automatic post-editing provides an additional way to simplifying the work of professional translators. Pal (2015) shows how it is possible to apply Hierarchical Phrase Based Statistical Machine Translation to the task of monolingual Statistical Automatic Post-editing. Evaluation using standard MT metrics shows that automatically post-edited texts are better than the raw translations. In addition, an experiment with four professional translators reveals that the post-editing effort is also reduced. Logacheva and Specia (2015) investigate ways to collect and generate negative human feedback in various forms, including post-editing, and learn how to improve machine translation systems from this feedback, for example, by building word-level quality estimation models to mimic user feedback and introducing the predictions in SMT decoders. 2.5 Hybrid Approaches to Translation All the existing methods in MT have strengths and weaknesses and one of the most common ways to improve their performance is to combine them. Li et al. (2014) proposed a method for incorporating translation memories and linguistic knowledge in SMT,"
2015.tc-1.3,P02-1040,0,0.0939051,"uggested was to generate segments on fly from fragments of previously translated segments. An implementation based on pattern matching showed that even such a simple approach can be potentially useful. Another way to address the needs of translators is to design flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible"
2015.tc-1.3,W15-4107,0,0.0154388,"n flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a)."
2015.tc-1.3,2015.mtsummit-wptp.4,0,0.0124165,"n flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a)."
2015.tc-1.3,2015.mtsummit-papers.11,0,0.0130963,"n flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a)."
2015.tc-1.3,W15-5201,0,0.0277695,"largely due to the fact that in many cases the real needs of translators were not considered when designing these tools. To this end, a survey with professional translators was carried out in order to find out their views and requirements regarding various technologies, and their current work practices. Thanks to the help of the commercial partners in the project, the survey received 736 complete responses, from a total of over 1300 responses, which is more than in other similar surveys. A first analysis of the data is presented in (Zaretskaya et al., 2015) with more analyses underway. Parra Escartín (2015) carried out another study with professional translators in an attempt to find out “missing functionalities” of translation memories that could potentially improve their productivity. An interesting feature suggested was to generate segments on fly from fragments of previously translated segments. An implementation based on pattern matching showed that even such a simple approach can be potentially useful. Another way to address the needs of translators is to design flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently test"
2015.tc-1.3,2014.eamt-1.21,1,0.710839,"em in translation memories and statistical machine translation. 2.4 The Human Translator in the Loop Post-editing is one of the most promising ways of integrating the output of machine translation methods in the workflows used by translation companies. Quality estimation methods are used to decide whether a sentence should be translated from scratch or it is good enough to be given to a post-editor. Most of the existing methods focus on estimating the quality of sentences, but in some cases it is necessary to estimate the quality of the translation of a whole document. The work carried out by Scarton and Specia (2014) in the EXPERT project focuses on document level quality estimation. Automatic post-editing provides an additional way to simplifying the work of professional translators. Pal (2015) shows how it is possible to apply Hierarchical Phrase Based Statistical Machine Translation to the task of monolingual Statistical Automatic Post-editing. Evaluation using standard MT metrics shows that automatically post-edited texts are better than the raw translations. In addition, an experiment with four professional translators reveals that the post-editing effort is also reduced. Logacheva and Specia (2015)"
2015.tc-1.3,W14-3323,0,0.0605642,"Missing"
2015.tc-1.3,2015.eamt-1.6,1,\N,Missing
2020.aacl-main.23,P19-1126,0,0.10131,"Missing"
2020.aacl-main.23,P17-1176,1,0.737421,"x|) denotes the earliest point when the system observes the full source sentence, λ = |y| |x |represents the target-to-source length ratio and d∗ ≥ 0 is a hyper-parameter called target delay that indicates the desired system latency. Note that the lower the values of AL are, the more rewards the TN controller receives. Policy Gradient We train the TN controller with policy gradient(Sutton et al., 1999), and the gradients are: ""T # τ X Rt ∇θ log πθ (at |·) , (15) ∇θ J = Eπθ t=1 P τ where Rt = Ti=t ri is the cumulative future rewards for the current decision. We can adopt any sampling approach (Chen et al., 2017, 2018; Shen et al., 2018) to estimate the expected gradient. In our experiments, we randomly sample multiple action trajectories from the current policy πθ and estimate the gradient with the collected accumulated reward. We try the variance reduction techniques by subtracting a baseline average reward estimated by a linear regression model from Rt and find that it does not help to improve the performance. Therefore, we just normalize the reward in each mini-batch without using baseline reward for simplicity. 4 Experiments 4.1 Settings Dataset We compare our approach with the baselines on WMT1"
2020.aacl-main.23,N18-2079,0,0.314055,"se approaches are either memory inefficient during training (Ma et al., 2018) or with hyper-parameters hard to tune (Arivazhagan et al., 2019). Others utilize a full-sentence base model to perform simultaneous translation by modifications to the encoder and the decoding process. To match the incremental source context, they replace the bidirectional encoder with a left-to-right encoder (Cho and Esipova, 2016; Satija and Pineau, 2016; Gu et al., 2017; Alinejad et al., 2018) or recompute the encoder hidden states (Zheng et al., 2019). On top of that, heuristic algorithms (Cho and Esipova, 2016; Dalvi et al., 2018) or a READ/WRITE model trained with reinforcement learning (Satija and Pineau, 2016; Gu et al., 2017; Alinejad et al., 2018) or supervised learning (Zheng et al., 2019) are used to decide, at every step, whether to wait for the next source token or output a target token. However, these models either cannot directly use a pretrained consecutive neural machine translation (CNMT) model with bidirectional encoder as the base model or work in a sub-optimal way in the decoding stage. In this paper, we study the problem of adapting neural machine translation to translate simultaneously. We formulate"
2020.aacl-main.23,D18-1337,0,0.196327,"them propose sophisticated training frameworks explicitly designed for simultaneous translation (Ma et al., 2018; Arivazhagan et al., 2019). These approaches are either memory inefficient during training (Ma et al., 2018) or with hyper-parameters hard to tune (Arivazhagan et al., 2019). Others utilize a full-sentence base model to perform simultaneous translation by modifications to the encoder and the decoding process. To match the incremental source context, they replace the bidirectional encoder with a left-to-right encoder (Cho and Esipova, 2016; Satija and Pineau, 2016; Gu et al., 2017; Alinejad et al., 2018) or recompute the encoder hidden states (Zheng et al., 2019). On top of that, heuristic algorithms (Cho and Esipova, 2016; Dalvi et al., 2018) or a READ/WRITE model trained with reinforcement learning (Satija and Pineau, 2016; Gu et al., 2017; Alinejad et al., 2018) or supervised learning (Zheng et al., 2019) are used to decide, at every step, whether to wait for the next source token or output a target token. However, these models either cannot directly use a pretrained consecutive neural machine translation (CNMT) model with bidirectional encoder as the base model or work in a sub-optimal wa"
2020.aacl-main.23,D14-1140,0,0.0511366,"Missing"
2020.aacl-main.23,E17-1099,0,0.0611419,"neural machine translation to translate simultaneously. Our framework contains two parts: prefix translation that utilizes a consecutive NMT model to translate source prefixes and a stopping criterion that determines when to stop the prefix translation. Experiments on three translation corpora and two language pairs show the efficacy of the proposed framework on balancing the quality and latency in adapting NMT to perform simultaneous translation. 1 Introduction Simultaneous translation (F¨ugen et al., 2007; Oda et al., 2014; Grissom et al., 2014; Niehues et al., 2016; Cho and Esipova, 2016; Gu et al., 2017; Ma et al., 2018), the task of producing a partial translation of a sentence before the whole input sentence ends, is useful in many scenarios including outbound tourism, international summit and multilateral negotiations. Different from the consecutive translation in which translation quality alone matters, simultaneous translation trades off between translation quality and latency. The syntactic structure difference between the source and target language makes simultaneous translation more challenging. For example, when translating from a verb-final (SOV) language (e.g., Japanese) to a verb"
2020.aacl-main.23,P04-1077,0,0.0518233,"Missing"
2020.aacl-main.23,P14-2090,0,0.130337,"simultaneity requirements. In this paper, we propose a general framework for adapting neural machine translation to translate simultaneously. Our framework contains two parts: prefix translation that utilizes a consecutive NMT model to translate source prefixes and a stopping criterion that determines when to stop the prefix translation. Experiments on three translation corpora and two language pairs show the efficacy of the proposed framework on balancing the quality and latency in adapting NMT to perform simultaneous translation. 1 Introduction Simultaneous translation (F¨ugen et al., 2007; Oda et al., 2014; Grissom et al., 2014; Niehues et al., 2016; Cho and Esipova, 2016; Gu et al., 2017; Ma et al., 2018), the task of producing a partial translation of a sentence before the whole input sentence ends, is useful in many scenarios including outbound tourism, international summit and multilateral negotiations. Different from the consecutive translation in which translation quality alone matters, simultaneous translation trades off between translation quality and latency. The syntactic structure difference between the source and target language makes simultaneous translation more challenging. For e"
2020.aacl-main.23,P02-1040,0,0.106459,"tion quality and latency, we define the reward function at inner decoding step k of outer step s as: predicting an EOS token or satisfying: ws = s&lt;S In the following, we talk about the details of the reward function and the training with policy gradient. Figure 2: Framework of our proposed model with the TN controller. n aτ [s−1]+ws = 1 where t = τ [s − 1]+k, and rtQ and rtD are rewards related to quality and delay, respectively. α ≥ 0 is a hyper-parameter that we adjust to balance the trade-off between translation quality and delay. Similar to Gu et al. (2017), we utilize sentencelevel BLEU (Papineni et al., 2002; Lin and Och, 2004) with reward shaping (Ng et al., 1999) as the reward for quality: n ∆BLEU(y∗ , y, t) rtQ = k 6= ws or s 6= S BLEU(y∗ , y) k = ws and s = S (11) where ∆BLEU(y∗ , y, t) = BLEU(y∗ , yt ) − BLEU(y∗ , yt−1 ) (12) is the intermediate reward. Note that the higher the values of BLEU are, the more rewards the TN controller receives. Following Ma et al. (2018), we use average lagging (AL) as the reward for latency: (8) where zτs[s−1]+k is the current decoder hidden state. We implement fθ with a feedforward network with two hidden layers, followed by a softmax layer. The prefix transl"
2020.aacl-main.23,P16-1162,0,0.0552962,"ther evaluate our approach’s efficacy in trading off translation quality and latency on other language pair and spoken language, we also 2 conduct experiments with the proposed LE and TN methods on NIST Chinese-to-English3 (ZH→EN) translation and IWSLT16 German-English4 (DEEN) translation in both directions. For WMT15, we use newstest2014 for validation and newstest2015 for test. For NIST, we use MT02 for validation, and MT05, MT06, MT08 for test. For IWSLT16, we use tst13 for validation and tst14 for test. All the data is tokenized and segmented into subword symbols using byte-pair encoding (Sennrich et al., 2016) to restrict the size of the vocabulary. We use 40,000 joint merge operations on WMT15, and 24,000 on IWSLT16. For NIST, we use 30,000 merge operations for source and target side separately. Without explicitly mention, we simulate simultaneous translation scenario at inference time with these datasets by assuming that the system observes one new source token at each outer step, i.e., cs = 1. Table 1 shows the data statistics. Pretrained NMT Model We use Transformer (Vaswani et al., 2017) trained with maximum likelihood estimation as the pretrained CNMT model and implement our method based on f"
2020.aacl-main.23,W18-1815,0,0.0169149,"rks in simultaneous translation divide the translation process into two stages. A segmentation component first divides the incoming text into segments, and then each segment is translated by a translator independently or with previous context. The segmentation boundaries can be predicted by prosodic pauses detected in speech (F¨ugen et al., 2007; Bangalore et al., 2012), linguistic cues (Sridhar et al., 2013; Matusov et al., 2007), or a classifier based on alignment information (Siahbani et al., 2014; Yarmohammadi et al., 2013) and translation accuracy (Oda et al., 2014; Grissom et al., 2014; Siahbani et al., 2018). Some authors have recently endeavored to perform simultaneous translation in the context of NMT. Niehues et al. (2018); Arivazhagan et al. (2020) adopt a re-translation approach where the source is repeatedly translated from scratch as it grows and propose methods to improve translation stability. Cho and Esipova (2016); Dalvi et al. (2018); Ma et al. (2018) introduce a manually designed criterion to control when to translate. Satija and Pineau (2016); Gu et al. (2017); Alinejad et al. (2018) extend the criterion into a trainable agent 198 1 2 吴邦国 出席 3 4 5 签字 仪式 并 LE wu TN wu bangguo attende"
2020.aacl-main.23,N13-1023,0,0.0214896,"tion “wu bangguo attended the signing ceremony and”. Such strategy helps to alleviate the problem of premature translation, i.e., translating before observing enough future context. Related Work A number of works in simultaneous translation divide the translation process into two stages. A segmentation component first divides the incoming text into segments, and then each segment is translated by a translator independently or with previous context. The segmentation boundaries can be predicted by prosodic pauses detected in speech (F¨ugen et al., 2007; Bangalore et al., 2012), linguistic cues (Sridhar et al., 2013; Matusov et al., 2007), or a classifier based on alignment information (Siahbani et al., 2014; Yarmohammadi et al., 2013) and translation accuracy (Oda et al., 2014; Grissom et al., 2014; Siahbani et al., 2018). Some authors have recently endeavored to perform simultaneous translation in the context of NMT. Niehues et al. (2018); Arivazhagan et al. (2020) adopt a re-translation approach where the source is repeatedly translated from scratch as it grows and propose methods to improve translation stability. Cho and Esipova (2016); Dalvi et al. (2018); Ma et al. (2018) introduce a manually desig"
2020.aacl-main.23,D19-1137,0,0.0739401,"Missing"
2020.acl-main.24,D19-1633,0,0.130703,"Missing"
2020.acl-main.24,P18-2124,0,0.0857057,"Missing"
2020.acl-main.24,N18-2074,0,0.0200042,"ization: i. A sequence S with all [MASK] tokens. ii. Unvisited index set U = {1, 2, ..., N }. while U is not empty do 1. Randomly pick a number n from U ; 2. Input u-PMLM with S and predict the n-th token xn ; 3. Replace the n-th token of S with the predicted token xn , i.e., S(n) ← xn ; 4. Remove n from U . Positional Embedding Most pretrained masked language models have employed absolute positional embedding to incorporate the positional information of the input tokens. We train two variants for u-PMLM, one with absolute positional embedding and the other with relative positional embedding (Shaw et al., 2018). The experiments show that NLG ability is not sensitive to relative or absolute positional embedding, while NLU ability is improved with relative positional embeddings. (8) where σ denote random permutations. The detail derivation is included in the Appendix A. Model Inference Although both u-PMLM and GPT generate sequences autoregressively based on 266 Step Prediction Index State of the sequence 0 n/a 1 3 a 2 7 a 3 1 This a 4 2 This is a 5 4 This is a sentence 6 6 This is a sentence in 7 5 This is a sentence generated in 8 8 This is a sentence generated in Generation Order: 3→7→1→2→4→6→5→8 O"
2020.acl-main.24,W19-2304,0,0.0420914,"ataset is proper. Hence we conclude u-PMLM-R is a better model than uPMLM-A considering both NLU and NLG tasks. 269 Model BERT(A) u-PMLM-A u-PMLM-R u-PMLM-R* COLA 52.1 56.5 58.0 56.9 SST2 93.5 94.3 94.0 94.2 MRPC 88.9/84.8 88.8/84.4 89.7/85.8 90.7/87.7 STSB 87.1/85.8 87.0/85.9 87.7/86.8 89.7/89.1 QQP 71.2/89.2 71.4/89.2 71.2/89.2 72.2/89.4 MNLI-m/mm 84.6/83.4 84.5/83.5 85.0/84.1 86.1/85.4 QNLI 90.5 91.8 92.3 92.1 RTE 66.4 66.1 69.8 78.5 AVG. 78.3 79.0 80.0 81.3 Table 5: Evaluation on GLUE test set. Model BERT(A) u-PMLM-A u-PMLM-R F1 76.85 78.31 81.52 EM 73.97 74.62 78.46 jad et al. (2019) and Wang and Cho (2019) employ masked language model for refinement-based non-autoregressive text generation, when a subset of tokens in a sequence are refined iteratively. Later, Mansimov et al. (2019) propose a generalized framework of sequence generation accommodating autoregressive, semi-autoregressive, and refinement-based non-autoregressive model. Strictly speaking, our proposed arbitrarily ordered autoregressive text generation is a special case of this generalized framework. We are the first work to address such kind of text generation, which enables a lot of new applications over tradition text generation."
2020.acl-main.24,W18-5446,0,\N,Missing
2020.acl-main.24,N19-1423,0,\N,Missing
2020.acl-main.24,P19-1285,0,\N,Missing
2020.acl-main.383,P18-1198,0,0.0607756,"Missing"
2020.acl-main.383,W18-5426,0,0.0227168,"uistically-motivated ones to a large extent, they are also empirically useful to downstream tasks, at least to ABSC. As future work, we plan to extend our analysis to more downstream tasks and models, like those reported in Shi (2018). 7 Related Work There has been substantial research investigating what pre-trained language models have learned about languages’ structures. One rising line of research uses probing classifiers to investigate the different syntactic properties captured by the model. They are generally referred to as “probing task” (Conneau et al., 2018), “diagnostic classifier” (Giulianelli et al., 2018), and “auxiliary prediction tasks” (Adi et al., 2017). The syntactic properties investigated range from basic ones like sentence length (Shi et al., 2016; Jawahar et al., 2019), syntactic tree depth (Jawahar et al., 2019), and segmentation (Liu et al., 2019) 4172 to challenging ones like syntactic labeling (Tenney et al., 2019a,b), dependency parsing (Hewitt and Manning, 2019; Clark et al., 2019), and constituency parsing (Peters et al., 2018a). However, when a probe achieves high accuracy, it’s difficult to differentiate if it is the representation that encodes targeted syntactic information,"
2020.acl-main.383,N18-1108,0,0.0261333,"hat there are no generalist heads that can do holistic parsing. Hence, analyzing attention weights directly may not reveal much of the syntactic knowledge that a model has learned. Recent dispute about attention as explanation (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) also suggests that the attention’s behavior does not necessarily represent that of the original model. Another group of research examine the outputs of language models on carefully chosen input sentences (Goldberg, 2019; Bacon and Regier, 2019). They extend previous works (Linzen et al., 2016; Gulordava et al., 2018; Marvin and Linzen, 2018) on subject-verb agreement test (generating the correct number of a verb far away from its subject) to provide a measure of the model’s syntactic ability. Their results show that the BERT model captures syntax-sensitive agreement patterns well in general. However, subject-verb agreement cannot provide more nuanced tests of other complex structures (e.g., dependency structure, constituency structure), which are the interest of our work. Two recent works also perturb the input sequence for model interpretability (Rosa and Mareˇcek, 2019; Li et al., 2019b). However, thes"
2020.acl-main.383,D19-1275,0,0.08785,"Manning, 2019; Clark et al., 2019; Tenney et al., 2019b; Jawahar et al., 2019). A probe is a simple neural network (with a small additional set of parameters) that uses the feature representations generated by a pre-trained model (e.g., hidden state activations, attention weights) and is trained to perform a supervised task (e.g., dependency labeling). The performance of a probe is used to measure the quality of the generated representations with the assumption that the measured quality is mostly attributable to the pre-trained language model. One downside of such approach, as pointed out in (Hewitt and Liang, 2019), is that a probe introduces a new set of additional parameters, which makes the results difficult to interpret. Is it the pretrained model that captures the linguistic information, or is it the probe that learns the downstream task itself and thus encodes the information in its additional parameter space? In this paper we propose a parameter-free probing technique called Perturbed Masking to analyze and interpret pre-trained models. The main idea is to introduce the Perturbed Masking technique into the masked language modeling (MLM) objective to measure the impact a word xj has on predicting"
2020.acl-main.383,W19-4828,0,0.210714,"to a human-designed dependency schema. 1 Introduction Recent prevalent pre-trained language models such as ELMo (Peters et al., 2018b), BERT (Devlin et al., 2018), and XLNet (Yang et al., 2019) achieve state-of-the-art performance for a diverse array of downstream NLP tasks. An interesting area of research is to investigate the interpretability of these pre-trained models (i.e., the linguistic properties they capture). Most recent approaches are built upon the idea of probing classifiers (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Peters et al., 2018a; Hewitt and Manning, 2019; Clark et al., 2019; Tenney et al., 2019b; Jawahar et al., 2019). A probe is a simple neural network (with a small additional set of parameters) that uses the feature representations generated by a pre-trained model (e.g., hidden state activations, attention weights) and is trained to perform a supervised task (e.g., dependency labeling). The performance of a probe is used to measure the quality of the generated representations with the assumption that the measured quality is mostly attributable to the pre-trained language model. One downside of such approach, as pointed out in (Hewitt and Liang, 2019), is that"
2020.acl-main.383,N19-1419,0,0.436651,"ble with or even superior to a human-designed dependency schema. 1 Introduction Recent prevalent pre-trained language models such as ELMo (Peters et al., 2018b), BERT (Devlin et al., 2018), and XLNet (Yang et al., 2019) achieve state-of-the-art performance for a diverse array of downstream NLP tasks. An interesting area of research is to investigate the interpretability of these pre-trained models (i.e., the linguistic properties they capture). Most recent approaches are built upon the idea of probing classifiers (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Peters et al., 2018a; Hewitt and Manning, 2019; Clark et al., 2019; Tenney et al., 2019b; Jawahar et al., 2019). A probe is a simple neural network (with a small additional set of parameters) that uses the feature representations generated by a pre-trained model (e.g., hidden state activations, attention weights) and is trained to perform a supervised task (e.g., dependency labeling). The performance of a probe is used to measure the quality of the generated representations with the assumption that the measured quality is mostly attributable to the pre-trained language model. One downside of such approach, as pointed out in (Hewitt and Li"
2020.acl-main.383,P04-1061,0,0.337611,"endency head. (2) A random BERT baseline, with which we randomly initialize weights of the BERT model (Htut et al., 2019), then use our methods to induce dependency trees. We measure model performance using Unlabeled Attachment Score (UAS). We note that UAS has been shown to be highly sensitive to annotation variations (Schwartz et al., 2011; Tsarfaty et al., 2011; K¨ubler et al., 2009). Therefore, it may not be a fair evaluation metric for analyzing and interpreting BERT. To reflect the real quality of the dependency structures that are retained in BERT, we also report Undirected UAS (UUAS) (Klein and Manning, 2004) and the Neutral Edge Direction (NED) scores (Schwartz et al., 2011). Results. Tables 1 and 2 show the results of our dependency probes. From Table 1, we see that although BERT is trained without any explicit supervision from syntactic dependencies, to some extent the syntax-aware representation already exists in it. The best UAS scores it achieves (Eisner+Dist) are substantially higher than that of the random BERT baseline with respect to both WSJ10-U(+41.7) and PUD(+31.5). Moreover, the Dist method significantly outperforms the Prob Right-chain Left-chain Random BERT 49.5 20.6 16.9 35.0 10.7"
2020.acl-main.383,P19-1338,0,0.152755,"nd evaluate our method on the 7,422 sentences in WSJ10 dataset and the PTB23 dataset (the traditional PTB test set for constituency parsing). Results. Table 3 shows the results of our constituency probes. From the table, we see that BERT outperforms most baselines on PTB23, except for the second layer of ON-LSTM. Note that all these baselines have specifically-designed architectures for the unsupervised parsing task, while BERT’s knowledge about constituent formalism emerges purely from self-supervised training on unlabeled text. It is also worth noting that recent results (Dyer et al., 2019; Li et al., 2019a) have suggested that the parsing algorithm used by ON-LSTM (PRPN) is biased towards the right-branching trees of English, leading to inflated F1 compared to unbiased parsers. To ensure a fair comparison with them, we also introduced this right-branching bias. However, our results show that our method is also robust without this bias (e.g., only 0.9 F1 drops on PTB23). To further understand the strengths and weaknesses of each system, we analyze their accuracies by constituent tags. In Table 3, we show the accuracies of five most common tags in PTB23. We find that the success of PRPN and ON-L"
2020.acl-main.383,W19-4827,0,0.139311,"Missing"
2020.acl-main.383,D18-1151,0,0.0381417,"ist heads that can do holistic parsing. Hence, analyzing attention weights directly may not reveal much of the syntactic knowledge that a model has learned. Recent dispute about attention as explanation (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) also suggests that the attention’s behavior does not necessarily represent that of the original model. Another group of research examine the outputs of language models on carefully chosen input sentences (Goldberg, 2019; Bacon and Regier, 2019). They extend previous works (Linzen et al., 2016; Gulordava et al., 2018; Marvin and Linzen, 2018) on subject-verb agreement test (generating the correct number of a verb far away from its subject) to provide a measure of the model’s syntactic ability. Their results show that the BERT model captures syntax-sensitive agreement patterns well in general. However, subject-verb agreement cannot provide more nuanced tests of other complex structures (e.g., dependency structure, constituency structure), which are the interest of our work. Two recent works also perturb the input sequence for model interpretability (Rosa and Mareˇcek, 2019; Li et al., 2019b). However, these works only perturb the s"
2020.acl-main.383,D18-1179,0,0.0997922,"nguage models (e.g., BERT). Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines. We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema. 1 Introduction Recent prevalent pre-trained language models such as ELMo (Peters et al., 2018b), BERT (Devlin et al., 2018), and XLNet (Yang et al., 2019) achieve state-of-the-art performance for a diverse array of downstream NLP tasks. An interesting area of research is to investigate the interpretability of these pre-trained models (i.e., the linguistic properties they capture). Most recent approaches are built upon the idea of probing classifiers (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Peters et al., 2018a; Hewitt and Manning, 2019; Clark et al., 2019; Tenney et al., 2019b; Jawahar et al., 2019). A probe is a simple neural network (with a small additional set of"
2020.acl-main.383,N18-1202,0,0.318843,"nguage models (e.g., BERT). Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines. We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema. 1 Introduction Recent prevalent pre-trained language models such as ELMo (Peters et al., 2018b), BERT (Devlin et al., 2018), and XLNet (Yang et al., 2019) achieve state-of-the-art performance for a diverse array of downstream NLP tasks. An interesting area of research is to investigate the interpretability of these pre-trained models (i.e., the linguistic properties they capture). Most recent approaches are built upon the idea of probing classifiers (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Peters et al., 2018a; Hewitt and Manning, 2019; Clark et al., 2019; Tenney et al., 2019b; Jawahar et al., 2019). A probe is a simple neural network (with a small additional set of"
2020.acl-main.383,P14-1003,0,0.0391493,"Missing"
2020.acl-main.383,S14-2004,0,0.0640817,"r BERT is learning an empirically useful or even better structure of a language. To answer this question, we turn to neural networks that adopt dependency parsing trees as the explicit structure prior to improve downstream 4 For reference, a supervised graph-based parser (Li et al., 2014) achieves an UAS of 57.6 on SciDTB 4171 tasks. We replace the ground-truth dependency trees those networks used with ones induced from BERT and approximate the effectiveness of different trees by the improvements they introduced. We conduct experiments on the Aspect Based Sentiment Classification (ABSC) task (Pontiki et al., 2014). ABSC is a fine-grained sentiment classification task aiming at identifying the sentiment expressed towards each aspect of a given target entity. As an example, in the following comment of a restaurant, “I hated their fajitas, but their salads were great”, the sentiment polarities for aspect fajitas is negative and that of salads is positive. It has been shown in Zhang et al. (2019) that injecting syntactic knowledge into neural networks can improve ABSC accuracy. Intuitively, given an aspect, a syntactically closer context word should play a more important role in predicting that aspect’s se"
2020.acl-main.383,P19-1124,0,0.13301,"nd evaluate our method on the 7,422 sentences in WSJ10 dataset and the PTB23 dataset (the traditional PTB test set for constituency parsing). Results. Table 3 shows the results of our constituency probes. From the table, we see that BERT outperforms most baselines on PTB23, except for the second layer of ON-LSTM. Note that all these baselines have specifically-designed architectures for the unsupervised parsing task, while BERT’s knowledge about constituent formalism emerges purely from self-supervised training on unlabeled text. It is also worth noting that recent results (Dyer et al., 2019; Li et al., 2019a) have suggested that the parsing algorithm used by ON-LSTM (PRPN) is biased towards the right-branching trees of English, leading to inflated F1 compared to unbiased parsers. To ensure a fair comparison with them, we also introduced this right-branching bias. However, our results show that our method is also robust without this bias (e.g., only 0.9 F1 drops on PTB23). To further understand the strengths and weaknesses of each system, we analyze their accuracies by constituent tags. In Table 3, we show the accuracies of five most common tags in PTB23. We find that the success of PRPN and ON-L"
2020.acl-main.383,Q16-1037,0,0.120695,"Missing"
2020.acl-main.383,W18-5431,0,0.0995381,"them into Universal Dependencies using Stanford CoreNLP. We denote this set as WSJ10-U. Next, two parsing algorithms, namely, the Eisner algorithm (1996) and Chu-Liu/Edmonds (CLE) algorithm (1965; 1967), are utilized to extract the projective and non-projective unlabeled dependency trees, respectively. Given that our impact matrices have no knowledge about the dependency root of the sentence, we use the gold root in our analysis. Introducing the gold root may artificially improve our results slightly. We thus apply this bias evenly across all baselines to ensure a fair comparison, as done in (Raganato and Tiedemann, 2018; Htut et al., 2019). We compared our approach against the following baselines: (1) right-(left-) chain baseline, which always selects the next(previous) word as dependency head. (2) A random BERT baseline, with which we randomly initialize weights of the BERT model (Htut et al., 2019), then use our methods to induce dependency trees. We measure model performance using Unlabeled Attachment Score (UAS). We note that UAS has been shown to be highly sensitive to annotation variations (Schwartz et al., 2011; Tsarfaty et al., 2011; K¨ubler et al., 2009). Therefore, it may not be a fair evaluation m"
2020.acl-main.383,N19-1112,0,0.136603,"substantial research investigating what pre-trained language models have learned about languages’ structures. One rising line of research uses probing classifiers to investigate the different syntactic properties captured by the model. They are generally referred to as “probing task” (Conneau et al., 2018), “diagnostic classifier” (Giulianelli et al., 2018), and “auxiliary prediction tasks” (Adi et al., 2017). The syntactic properties investigated range from basic ones like sentence length (Shi et al., 2016; Jawahar et al., 2019), syntactic tree depth (Jawahar et al., 2019), and segmentation (Liu et al., 2019) 4172 to challenging ones like syntactic labeling (Tenney et al., 2019a,b), dependency parsing (Hewitt and Manning, 2019; Clark et al., 2019), and constituency parsing (Peters et al., 2018a). However, when a probe achieves high accuracy, it’s difficult to differentiate if it is the representation that encodes targeted syntactic information, or it is the probe that just learns the task (Hewitt and Liang, 2019). In line with our work, recent studies seek to find correspondences between parts of the neural network and certain linguistic properties, without explicit supervision. Most of them focus"
2020.acl-main.383,P14-5010,0,0.00273337,"our experiments, we use the base, uncased version from (Wolf et al., 2019). 2 We also experimented with other alternatives, but observe no significant difference. 4167 5 [CLS] For those who follow social media transitions on Capitol Hill , this will be a little different . S] or se ho w ial ia ns on tol ill [CL F tho w follo soc mendsitio Capi H tra 4 , this will be a ittle ent l iffer d 3 follow social media transitions on Capitol Hill 2 Figure 2: Part of the constituency tree. 1 Constituency. Figure 2 shows part of the constituency tree of our example sentence generated by Stanford CoreNLP (Manning et al., 2014). In this sentence, “media” and “on” are two words that are adjacent to “transitions”. From the tree, however, we see that “media” is closer to “transitions” than “on” is in terms of syntactic distance. If a model is syntactically uninformed, we would expect “media” and “on” to have comparable impacts on the prediction of “transitions”, and vice versa. However, we observe a far greater impact (darker color) between “media” and “transitions” than that between “on” and “transitions”. We will further support this observation with empirical experiments in Section 4.2. Other Structures. Along the d"
2020.acl-main.383,P11-1067,0,0.0273384,"apply this bias evenly across all baselines to ensure a fair comparison, as done in (Raganato and Tiedemann, 2018; Htut et al., 2019). We compared our approach against the following baselines: (1) right-(left-) chain baseline, which always selects the next(previous) word as dependency head. (2) A random BERT baseline, with which we randomly initialize weights of the BERT model (Htut et al., 2019), then use our methods to induce dependency trees. We measure model performance using Unlabeled Attachment Score (UAS). We note that UAS has been shown to be highly sensitive to annotation variations (Schwartz et al., 2011; Tsarfaty et al., 2011; K¨ubler et al., 2009). Therefore, it may not be a fair evaluation metric for analyzing and interpreting BERT. To reflect the real quality of the dependency structures that are retained in BERT, we also report Undirected UAS (UUAS) (Klein and Manning, 2004) and the Neutral Edge Direction (NED) scores (Schwartz et al., 2011). Results. Tables 1 and 2 show the results of our dependency probes. From Table 1, we see that although BERT is trained without any explicit supervision from syntactic dependencies, to some extent the syntax-aware representation already exists in it."
2020.acl-main.383,J93-2004,0,0.0692908,"sed dependency parsing? If so, to what extent? We begin by using the token-level perturbed masking technique to extract an impact matrix F for each sentence. We then utilize graph-based algorithms to induce a dependency tree from F, and compare it against ground-truth whose annotations 4168 are linguistically motivated. Parsing UAS WSJ10-U PUD Model Experiment Setup. We evaluate the induced trees on two benchmarks: (1) the PUD treebank described in Section 3. (2) the WSJ10 treebank, which contains 7,422 sentences (all less than 10 words after punctuation removal) from the Penn Treebank (PTB) (Marcus et al., 1993). Note that the original PTB does not contain dependency annotations. Thus, we convert them into Universal Dependencies using Stanford CoreNLP. We denote this set as WSJ10-U. Next, two parsing algorithms, namely, the Eisner algorithm (1996) and Chu-Liu/Edmonds (CLE) algorithm (1965; 1967), are utilized to extract the projective and non-projective unlabeled dependency trees, respectively. Given that our impact matrices have no knowledge about the dependency root of the sentence, we use the gold root in our analysis. Introducing the gold root may artificially improve our results slightly. We thu"
2020.acl-main.383,P16-1162,0,0.0106925,"two options for d(x, y): • Dist: Euclidean distance between x and y • Prob: d(x, y) = a(x)xi − a(y)xi , where a(·) maps a vector into a probability distribution among the words in the vocabulary. a(x)xi represents the probability of predicting token xi base on x. By repeating the two-stage perturbation on each pair of tokens xi , xj ∈ x and calculating f (xi , xj ), we obtain an impact matrix F, where Fij ∈ RT ×T . Now, we can derive algorithms to extract syntactic trees from F and compare them with ground-truth trees that are obtained from benchmarks. Note that BERT uses byte-pair encoding (Sennrich et al., 2016) and may split a word into multiple tokens(subwords). To evaluate our approach on word-level tasks, we make the following changes to obtain inter-word impact matrices. In each perturbation, we mask all tokens of a split-up word. The impact on a split-up word is obtained by averaging2 the impacts over the split-up word’s tokens. To measure the impact exerted by a split-up word, we assume the impacts given by its tokens are the same; We use the impact given by the first token for convenience. 2.3 2.2 Token Perturbation Given a sentence as a list of tokens x = [x1 , . . . , xT ], BERT maps each x"
2020.acl-main.383,W18-5444,0,0.08003,"Missing"
2020.acl-main.383,P19-1282,0,0.0263989,"ert attention weights to syntactic trees. However, they do not quantitatively evaluate their approach. In their later study (Mareˇcek and Rosa, 2019), they propose a bottom-up algorithm to extract constituent trees from transformer-based NMT encoders and evaluate their results on three languages. Htut et al. (2019) reassess these works but find that there are no generalist heads that can do holistic parsing. Hence, analyzing attention weights directly may not reveal much of the syntactic knowledge that a model has learned. Recent dispute about attention as explanation (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) also suggests that the attention’s behavior does not necessarily represent that of the original model. Another group of research examine the outputs of language models on carefully chosen input sentences (Goldberg, 2019; Bacon and Regier, 2019). They extend previous works (Linzen et al., 2016; Gulordava et al., 2018; Marvin and Linzen, 2018) on subject-verb agreement test (generating the correct number of a verb far away from its subject) to provide a measure of the model’s syntactic ability. Their results show that the BERT model captures syntax-sensitive agreeme"
2020.acl-main.383,D18-1492,0,0.0406034,"Missing"
2020.acl-main.383,D19-1586,0,0.0251818,"line settings and evaluation procedure in Sec 4.1, except that we remove gold root from our evaluation since we want to compare the accuracy by syntactic distances. Results. Table 4 shows the performance of our discourse probes. We find that both Eisner and CLE achieve significantly higher UAS (+28) than the random BERT baseline. This suggests that BERT is aware of the structure of the document it is given. In particular, we observe a decent accuracy in identifying discourse relations between adjacent EDUs, perhaps due to the “next sentence prediction” task in pre-training, as pointed out in (Shi and Demberg, 2019). However, our probes fall behind the left-chain baseline, which benefits from its strong structural prior4 (principal clause mostly in front of its subordinate clause). Our finding sheds some lights on BERT’s success in downstream tasks that have paragraphs as input (e.g., Question Answering). 6 BERT-based Trees VS Parser-provided Trees Our probing results suggest that although BERT has captured a certain amount of syntax, there are still substantial disagreements between the syntax BERT learns and those designed by linguists. For instance, our constituency probe on PTB23 significantly outper"
2020.acl-main.383,P19-1452,0,0.407734,"dependency schema. 1 Introduction Recent prevalent pre-trained language models such as ELMo (Peters et al., 2018b), BERT (Devlin et al., 2018), and XLNet (Yang et al., 2019) achieve state-of-the-art performance for a diverse array of downstream NLP tasks. An interesting area of research is to investigate the interpretability of these pre-trained models (i.e., the linguistic properties they capture). Most recent approaches are built upon the idea of probing classifiers (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Peters et al., 2018a; Hewitt and Manning, 2019; Clark et al., 2019; Tenney et al., 2019b; Jawahar et al., 2019). A probe is a simple neural network (with a small additional set of parameters) that uses the feature representations generated by a pre-trained model (e.g., hidden state activations, attention weights) and is trained to perform a supervised task (e.g., dependency labeling). The performance of a probe is used to measure the quality of the generated representations with the assumption that the measured quality is mostly attributable to the pre-trained language model. One downside of such approach, as pointed out in (Hewitt and Liang, 2019), is that a probe introduces a"
2020.acl-main.383,P18-2071,0,0.026954,"58.0 56.7 19.6 42.1 39.8 9.0 44.6 25.0 11.3 47.0 71.8 0.8 50.6 42.4 5.0 66.1 74.2 44.1 51.9 68.8 5.5 Table 3: Unlabeled parsing F1 results evaluated on WSJ10 and PTB23. Model UAS Accuracy by distance 0 1 2 5 Right-chain Left-chain Random BERT Eisner+Dist CLE+Dist 10.7 41.5 6.3 34.2 34.4 20.5 79.5 20.4 61.6 63.8 7.5 7.3 3.3 3.5 7.6 3.5 0.0 12.8 2.6 Table 4: Performance of different discourse parser. The distance is defined as the number of EDUs between head and dependent. ture of a document. A document contains a series of coherent text spans, which are named Elementary Discourse Units (EDUs) (Yang and Li, 2018; Polanyi, 1988). EDUs are connected to each other by discourse relations to form a document. We devise a discourse probe to investigate how well BERT captures structural correlations between EDUs. As the foundation of the probe, we extract an EDU-EDU impact matrix for each document using span-level perturbation. Setup. We evaluate our probe on the discourse dependency corpus SciDTB (Yang and Li, 2018). We do not use the popular discourse corpora RSTDT (Carlson et al., 2003) and PDTB (Prasad et al.) because PDTB focuses on local discourse relations but ignores the whole document structure, whi"
2020.acl-main.383,K17-3001,0,0.0238182,"e impact matrix for the sentence “For those who follow social media transitions on Capitol Hill, this will be a little different.” 3 Visualization with Impact Maps Before we discuss specific syntactic phenomena, let us first analyze some example impact matrices derived from sample sentences. We visualize an impact matrix of a sentence by displaying a heatmap. We use the term “impact map” to refer to a heatmap of an impact matrix. Setup. We extract impact matrices by feeding BERT with 1,000 sentences from the English Parallel Universal Dependencies (PUD) treebank of the CoNLL 2017 Shared Task (Zeman et al., 2017). We follow the setup and pre-processing steps employed in pre-training BERT. An example impact map is shown in Figure 1. Dependency. We notice that the impact map contains many stripes, which are short series of vertical/horizontal cells, typically located along the diagonal. Take the word “different” as an example (which is illustrated by the second-to-last column in the impact matrix). We observe a clear vertical stripe above the main diagonal. The interpretation is that this particular occurrence of the word “different” strongly affects the occurrences of those words before it. These stron"
2020.acl-main.383,D11-1036,0,0.0234106,"across all baselines to ensure a fair comparison, as done in (Raganato and Tiedemann, 2018; Htut et al., 2019). We compared our approach against the following baselines: (1) right-(left-) chain baseline, which always selects the next(previous) word as dependency head. (2) A random BERT baseline, with which we randomly initialize weights of the BERT model (Htut et al., 2019), then use our methods to induce dependency trees. We measure model performance using Unlabeled Attachment Score (UAS). We note that UAS has been shown to be highly sensitive to annotation variations (Schwartz et al., 2011; Tsarfaty et al., 2011; K¨ubler et al., 2009). Therefore, it may not be a fair evaluation metric for analyzing and interpreting BERT. To reflect the real quality of the dependency structures that are retained in BERT, we also report Undirected UAS (UUAS) (Klein and Manning, 2004) and the Neutral Edge Direction (NED) scores (Schwartz et al., 2011). Results. Tables 1 and 2 show the results of our dependency probes. From Table 1, we see that although BERT is trained without any explicit supervision from syntactic dependencies, to some extent the syntax-aware representation already exists in it. The best UAS scores it"
2020.acl-main.383,D19-1002,0,0.0267941,"syntactic trees. However, they do not quantitatively evaluate their approach. In their later study (Mareˇcek and Rosa, 2019), they propose a bottom-up algorithm to extract constituent trees from transformer-based NMT encoders and evaluate their results on three languages. Htut et al. (2019) reassess these works but find that there are no generalist heads that can do holistic parsing. Hence, analyzing attention weights directly may not reveal much of the syntactic knowledge that a model has learned. Recent dispute about attention as explanation (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) also suggests that the attention’s behavior does not necessarily represent that of the original model. Another group of research examine the outputs of language models on carefully chosen input sentences (Goldberg, 2019; Bacon and Regier, 2019). They extend previous works (Linzen et al., 2016; Gulordava et al., 2018; Marvin and Linzen, 2018) on subject-verb agreement test (generating the correct number of a verb far away from its subject) to provide a measure of the model’s syntactic ability. Their results show that the BERT model captures syntax-sensitive agreement patterns well in general."
2020.acl-main.383,W03-3023,0,0.460055,"Missing"
2020.acl-main.383,W01-1605,0,\N,Missing
2020.acl-main.383,C96-1058,0,\N,Missing
2020.acl-main.383,J03-4003,0,\N,Missing
2020.acl-main.383,D16-1159,0,\N,Missing
2020.acl-main.383,N19-1357,0,\N,Missing
2020.acl-main.383,P19-1356,0,\N,Missing
2020.acl-main.540,P17-1187,1,0.873387,"s. In the field of NLP, sememe knowledge bases are built to utilize sememes in practical applications, where sememes are generally regarded as semantic labels of words (as shown in Figure 1). HowNet (Dong and Dong, 2006) is the most wellknown one. It annotates over one hundred thousand English and Chinese words with a predefined sets of about 2,000 sememes. Its sememe annotations are sense-level, i.e., each sense of a (polysemous) word is annotated with sememes separately. With the help of HowNet, sememes have been successfully applied to many NLP tasks including word representation learning (Niu et al., 2017), sentiment analysis (Fu et al., 2013), semantic composition (Qi et al., 2019), sequence modeling (Qin et al., 2019), reverse dictionary (Zhang et al., 2019b), etc. 6067 2.2 Particle Swarm Optimization Inspired by the social behaviors like bird flocking, particle swarm optimization (PSO) is a kind of metaheuristic population-based evolutionary computation paradigms (Eberhart and Kennedy, 1995). It has been proved effective in solving the optimization problems such as image classification (Omran et al., 2004), part-of-speech tagging (Silva et al., 2012) and text clustering (Cagnina et al., 2014"
2020.acl-main.540,D14-1162,0,0.093128,"challenging. For natural language inference (NLI), we use the popular Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). Each instance in SNLI comprises a premise-hypothesis sentence pair and is labelled one of three relations including entailment, contradiction and neutral. As for victim models, we choose two widely used universal sentence encoding models, namely bidirectional LSTM (BiLSTM) with max pooling (Conneau et al., 2017) and BERTBASE (BERT) (Devlin et al., 2019). For BiLSTM, its hidden states are 128-dimensional, and it uses 300-dimensional pre-trained GloVe (Pennington et al., 2014) word embeddings. Details of the datasets and the classification accuracy results of the victim models are listed in Table 1. 4.2 Baseline Methods We select two recent open-source word-level adversarial attack models as the baselines, which are typical and involve different search space reduction methods (step 1) and search algorithms (step 2). The first baseline method (Alzantot et al., 2018) uses the combination of restrictions on word embedding distance and language model prediction score to reduce search space. As for search algorithm, it adopts genetic algorithm, another popular metaheuri"
2020.acl-main.540,P19-1561,0,0.0649884,"ity and naturality. Unfortunately, the change of true label will make the adversarial attack invalid. For example, supposing an adversary changes “she” to “he” in an input 6066 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6066–6080 c July 5 - 10, 2020. 2020 Association for Computational Linguistics sentence to attack a gender identification model, although the victim model alters its prediction result, this is not a valid attack. And the adversarial examples with broken grammaticality and naturality (i.e., poor quality) can be easily defended (Pruthi et al., 2019). Various textual adversarial attack models have been proposed (Wang et al., 2019a), ranging from character-level flipping (Ebrahimi et al., 2018) to sentence-level paraphrasing (Iyyer et al., 2018). Among them, word-level attack models, mostly word substitution-based models, perform comparatively well on both attack efficiency and adversarial example quality (Wang et al., 2019b). Word-level adversarial attacking is actually a problem of combinatorial optimization (Wolsey and Nemhauser, 1999), as its goal is to craft adversarial examples which can successfully fool the victim model using a lim"
2020.acl-main.540,P19-1571,1,0.818847,"practical applications, where sememes are generally regarded as semantic labels of words (as shown in Figure 1). HowNet (Dong and Dong, 2006) is the most wellknown one. It annotates over one hundred thousand English and Chinese words with a predefined sets of about 2,000 sememes. Its sememe annotations are sense-level, i.e., each sense of a (polysemous) word is annotated with sememes separately. With the help of HowNet, sememes have been successfully applied to many NLP tasks including word representation learning (Niu et al., 2017), sentiment analysis (Fu et al., 2013), semantic composition (Qi et al., 2019), sequence modeling (Qin et al., 2019), reverse dictionary (Zhang et al., 2019b), etc. 6067 2.2 Particle Swarm Optimization Inspired by the social behaviors like bird flocking, particle swarm optimization (PSO) is a kind of metaheuristic population-based evolutionary computation paradigms (Eberhart and Kennedy, 1995). It has been proved effective in solving the optimization problems such as image classification (Omran et al., 2004), part-of-speech tagging (Silva et al., 2012) and text clustering (Cagnina et al., 2014). Empirical studies have proven it is more efficient than some other optimiza"
2020.acl-main.540,D13-1170,0,0.0290862,"Pmax − where k is a positive constant, xo represents the original input, and E measures the word-level edit distance (number of different words between two n o sentences). E(xD,x ) is defined as the modification rate of an adversarial example. After mutation, the algorithm returns to the Record step. 4 Experiments In this section, we conduct comprehensive experiments to evaluate our attack model on the tasks of sentiment analysis and natural language inference. 4.1 Datasets and Victim Models For sentiment analysis, we choose two benchmark datasets including IMDB (Maas et al., 2011) and SST-2 (Socher et al., 2013). Both of them are binary sentiment classification datasets. But the average sentence length of SST-2 (17 words) is much shorter than that of IMDB (234 words), which renders attacks on SST-2 more challenging. For natural language inference (NLI), we use the popular Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). Each instance in SNLI comprises a premise-hypothesis sentence pair and is labelled one of three relations including entailment, contradiction and neutral. As for victim models, we choose two widely used universal sentence encoding models, namely bidirectional"
2020.emnlp-main.37,N19-1423,0,0.0431228,"V . W⇤ = [W1⇤ , · · · , WN H The output of the multi-head attention is: MHAWQ ,WK ,WV ,WO (Hl ) = Concat(head1 , · · · , headNH )WO . (2) The FFN layer composes two linear layers parameterized by W1 2 Rd⇥df f , b1 2 Rdf f and W2 2 Rdf f ⇥d , b2 2 Rd respectively, where df f is the number of neurons in the intermediate layer of FFN. Denote the input to FFN as Xl 2 Rn⇥d , the output is then computed as: FFN(Xl ) = GeLU(Xl W1 + b1 )W2 + b2 . (3) Combining (2) and (3), the forward propagation for the l-th Transformer layer can be written as Xl = LN(Hl + MHA(Hl )) 3.1 Quantization The BERT model (Devlin et al., 2019) is built with Transformer layers (Vaswani et al., 2017). A standard Transformer layer includes two main sublayers: Multi-Head Attention (MHA) module and Feed-Forward Network (FFN). For the l-th Transformer layer, suppose the input to it is Hl 2 Rn⇥d where n and d are the sequence length and hidden state size, respectively. SupHl+1 = LN(Xl + FFN(Xl )), where LN is the layer normalization. The input to the first transformer layer H1 = EMBWE ,WS ,WP (z) (4) is the combination of the token embedding, segment embedding and position embedding. Here z 511 is the input sequence, and WE , WS , WP are"
2020.emnlp-main.37,D16-1139,0,0.0388551,"on MNLI, where our proposed method outperforms other BERT compression methods. More empirical results on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms other quantization methods, and even achieves comparable performance as the full-precision baseline, while being much smaller. 2 Related Work 2.1 Knowledge Distillation Knowledge distillation is first proposed in (Hinton et al., 2015) to transfer knowledge in the logits from a large teacher model to a more compact student model without sacrificing too much performance. It has achieved remarkable performance in NLP (Kim and Rush, 2016; Jiao et al., 2019) recently. Besides the logits (Hinton et al., 2015), knowledge from the intermediate representations (Romero et al., 2014; Jiao et al., 2019) and attentions (Jiao et al., 2019; Wang et al., 2020) are also used to guide the training of a smaller BERT. Instead of directly being used for compression, knowledge distillation can also be used in combination with other compression methods like pruning (McCarley, 2019; Mao et al., 2020), low-rank approximation (Mao et al., 2020) and dynamic networks (Hou et al., 2020), to fully leverage the knowledge of the teacher BERT model. Alth"
2020.emnlp-main.37,2020.acl-main.537,0,0.0617364,"point format, which is both computation and memory expensive during inference. This poses great challenges for these models to run on resource-constrained devices like cellphones. To alleviate this problem, various methods are proposed to compress these models, like using low-rank approximation (Ma et al., 2019; Lan et al., 2020), weight-sharing (Dehghani et al., 2019; Lan et al., 2020), knowledge distillation (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2019), ⇤ Authors contribute equally. pruning (Michel et al., 2019; Voita et al., 2019; Fan et al., 2019), adaptive depth and/or width (Liu et al., 2020; Hou et al., 2020), and quantization (Zafrir et al., 2019; Shen et al., 2020; Fan et al., 2020). Compared with other compression methods, quantization compresses a neural network by using lower bits for weight values without changing the model architecture, and is particularly useful for carefully-designed network architectures like Transformers. In addition to weight quantization, further quantizing activations can speed up inference with target hardware by turning floating-point operations into integer or bit operations. In (Prato et al., 2019; Zafrir et al., 2019), 8-bit quantization is su"
2020.emnlp-main.37,2020.coling-main.287,0,0.0680734,"0 Conference on Empirical Methods in Natural Language Processing, pages 509–521, c November 16–20, 2020. 2020 Association for Computational Linguistics Besides quantization, knowledge distillation (Hinton et al., 2015) which transfers knowledge learned in the prediction layer of a cumbersome teacher model to a smaller student model, is also widely used to compress BERT (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2019; Wang et al., 2020). Instead of directly being used to compress BERT, the distillation loss can also be used in combination with other compression methods (McCarley, 2019; Mao et al., 2020; Hou et al., 2020), to fully leverage the knowledge of teacher model. In this work, we propose TernaryBERT, whose weights are restricted to { 1, 0, +1}. Instead of directly using knowledge distillation to compress a model, we use it to improve the performance of ternarized student model with the same size as the teacher model. In this way, we wish to transfer the knowledge from the highly-accurate teacher model to the ternarized student model with smaller capacity, and to fully explore the compactness by combining quantization and distillation. We investigate the ternarization granularity of"
2020.emnlp-main.37,1983.tc-1.13,0,0.365614,"Missing"
2020.emnlp-main.37,P18-2124,0,0.0636917,"Missing"
2020.emnlp-main.37,D16-1264,0,0.14367,"Missing"
2020.emnlp-main.37,D19-1441,0,0.230576,"ually have hundreds of millions of parameters. For instance, the BERT-base model has 109M parameters, with the model size of 400+MB if represented in 32-bit floating-point format, which is both computation and memory expensive during inference. This poses great challenges for these models to run on resource-constrained devices like cellphones. To alleviate this problem, various methods are proposed to compress these models, like using low-rank approximation (Ma et al., 2019; Lan et al., 2020), weight-sharing (Dehghani et al., 2019; Lan et al., 2020), knowledge distillation (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2019), ⇤ Authors contribute equally. pruning (Michel et al., 2019; Voita et al., 2019; Fan et al., 2019), adaptive depth and/or width (Liu et al., 2020; Hou et al., 2020), and quantization (Zafrir et al., 2019; Shen et al., 2020; Fan et al., 2020). Compared with other compression methods, quantization compresses a neural network by using lower bits for weight values without changing the model architecture, and is particularly useful for carefully-designed network architectures like Transformers. In addition to weight quantization, further quantizing activations can speed up infe"
2020.emnlp-main.37,P19-1580,0,0.0384736,"rs, with the model size of 400+MB if represented in 32-bit floating-point format, which is both computation and memory expensive during inference. This poses great challenges for these models to run on resource-constrained devices like cellphones. To alleviate this problem, various methods are proposed to compress these models, like using low-rank approximation (Ma et al., 2019; Lan et al., 2020), weight-sharing (Dehghani et al., 2019; Lan et al., 2020), knowledge distillation (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2019), ⇤ Authors contribute equally. pruning (Michel et al., 2019; Voita et al., 2019; Fan et al., 2019), adaptive depth and/or width (Liu et al., 2020; Hou et al., 2020), and quantization (Zafrir et al., 2019; Shen et al., 2020; Fan et al., 2020). Compared with other compression methods, quantization compresses a neural network by using lower bits for weight values without changing the model architecture, and is particularly useful for carefully-designed network architectures like Transformers. In addition to weight quantization, further quantizing activations can speed up inference with target hardware by turning floating-point operations into integer or bit operations. In ("
2020.emnlp-main.37,W18-5446,0,0.0456017,"PT from the teacher model by the soft cross-entropy (SCE) loss: Lpred = SCE(PS , PT ). The overall objective of knowledge distillation in the training process of TernaryBERT is thus L = Ltrm + Lpred . (8) We use the full-precision BERT fine-tuned on the downstream task to initialize our quantized model, and the data augmentation method in (Jiao et al., 2019) to boost the performance. The whole procedure, which will be called Distillation-aware ternarization, is shown in Algorithm 1. 4 Experiments In this section, we evaluate the efficacy of the proposed TernaryBERT on both the GLUE benchmark (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016, 2018). The experimental code is modified from the huggingface transformer library.2 We use both TWN and LAT to ternarize the weights. We use layer-wise ternarization for weights in Transformer layers while row-wise ternarization 2 Given the superior performance of Huawei Ascend AI Processor and MindSpore computing framework, we are going to open source the code based on MindSpore (https:// www.mindspore.cn/en) soon. 513 Table 1: Development set results of quantized BERT and TinyBERT on the GLUE benchmark. We abbreviate the number of bits for weights of Trans"
2020.emnlp-main.42,W18-6318,0,0.357737,"ating that if the correct decoding step and layer are chosen, attention weights in vanilla Transformer are sufficient for generating accurate word alignment interpretation. • We further propose S HIFT-AET , which extracts alignments from an additional alignment module. The module is tightly integrated into vanilla Transformer and trained with supervision from symmetrized S HIFT-ATT alignments. S HIFT-AET does not affect the translation accuracy and significantly outperforms GIZA++ by 1.4-4.8 AER points in our experiments. • We compare our methods with NAIVE -ATT on dictionary-guided decoding (Alkhouli et al., 2018), an alignment-related downstream task. Both methods consistently outperform NAIVE -ATT, demonstrating the effectiveness of our methods in such alignment-related NLP tasks. To alleviate this problem, some researchers modify the transformer architecture by adding alignment modules that predict the to-be-aligned target token (Zenkel et al., 2019, 2020) or modify the training loss by designing an alignment loss computed with full target sentence (Garg et al., 2019; Zenkel et al., 2020). Others argue that using only attention weights is insufficient for generating clean word alignment and propose"
2020.emnlp-main.42,W17-4711,0,0.0449857,"alignments especially for sentence beginning compared to NAIVE -ATT. 5 Related Work Alignment induction from RNNSearch (Bahdanau et al., 2015) has been explored by a number of works. Bahdanau et al. (2015) are the first to show word alignment example using attention in RNNSearch. Ghader and Monz (2017) further demonstrate that the RNN-based NMT system achieves comparable alignment performance to that of GIZA++. Alignment has also been used to improve NMT performance, especially in low resource settings, by supervising the attention mechanisms of RNNSearch (Chen et al., 2016; Liu et al., 2016; Alkhouli and Ney, 2017). There is also a number of other studies that induce word alignment from Transformer. Li et al. (2019); Ding et al. (2019) claim that attention may not capture word alignment in Transformer, and propose to induce word alignment with prediction difference (Li et al., 2019) or gradient-based measures (Ding et al., 2019). Zenkel et al. (2019) modify the Transformer architecture for better align573 !   !  ""  !  "" ""   !  !  $ # # $  ! #$ # $ !  "" "" # $ #$  ""  $ # ""  (a) Naive-Att (b) Shift-Att Figure 4: AER on the test set v.s. BLEU on the validation set on the de→en translation, eva"
2020.emnlp-main.42,J93-2003,0,0.192205,"weiß ich . das weiß ich Source: das weiß ich . . Dec. input: <bos> i understand this . Dec. output: i understand this . <eos> Figure 1: An example to compare our method S HIFTATT and the baseline NAIVE -ATT. The left is an attention map from the third decoder layer of the vanilla Transformer and the right are the induced alignments. S HIFT-ATT induces alignments for target word yi at decoding step i + 1 when yi is the decoder input, while NAIVE -ATT at step i when yi is the decoder output. Introduction The task of word alignment is to find lexicon translation equivalents from parallel corpus (Brown et al., 1993). It is one of the fundamental tasks in natural language processing (NLP) and is widely studied by the community (Dyer et al., 2013; Brown et al., 1993; Liu and Sun, 2015). Word alignments are useful in many scenarios, such as error analysis (Ding et al., 2017; Li et al., 2019), the introduction of coverage and fertility models (Tu et al., 2016), inserting external constraints in interactive machine translation (Hasler et al., 2018; ∗ Corresponding author. Part of the work was done when Yun was in Huawei Noah’s Ark Lab. 1 Code can be found at https://github.com/ sufe-nlp/transformer-alignment."
2020.emnlp-main.42,2016.amta-researchers.10,0,0.06603,"airs than GIZA++, and extract better alignments especially for sentence beginning compared to NAIVE -ATT. 5 Related Work Alignment induction from RNNSearch (Bahdanau et al., 2015) has been explored by a number of works. Bahdanau et al. (2015) are the first to show word alignment example using attention in RNNSearch. Ghader and Monz (2017) further demonstrate that the RNN-based NMT system achieves comparable alignment performance to that of GIZA++. Alignment has also been used to improve NMT performance, especially in low resource settings, by supervising the attention mechanisms of RNNSearch (Chen et al., 2016; Liu et al., 2016; Alkhouli and Ney, 2017). There is also a number of other studies that induce word alignment from Transformer. Li et al. (2019); Ding et al. (2019) claim that attention may not capture word alignment in Transformer, and propose to induce word alignment with prediction difference (Li et al., 2019) or gradient-based measures (Ding et al., 2019). Zenkel et al. (2019) modify the Transformer architecture for better align573 !   !  ""  !  "" ""   !  !  $ # # $  ! #$ # $ !  "" "" # $ #$  ""  $ # ""  (a) Naive-Att (b) Shift-Att Figure 4: AER on the test set v.s. BLEU on the v"
2020.emnlp-main.42,W93-0301,0,0.721085,"by the community (Dyer et al., 2013; Brown et al., 1993; Liu and Sun, 2015). Word alignments are useful in many scenarios, such as error analysis (Ding et al., 2017; Li et al., 2019), the introduction of coverage and fertility models (Tu et al., 2016), inserting external constraints in interactive machine translation (Hasler et al., 2018; ∗ Corresponding author. Part of the work was done when Yun was in Huawei Noah’s Ark Lab. 1 Code can be found at https://github.com/ sufe-nlp/transformer-alignment. Chen et al., 2020) and providing guidance for human translators in computer-aided translation (Dagan et al., 1993). Word alignment is part of the pipeline in statistical machine translation (Koehn et al., 2003, SMT), but is not necessarily needed for neural machine translation (Bahdanau et al., 2015, NMT). The attention mechanism in NMT does not functionally play the role of word alignments between the source and the target, at least not in the same way as its analog in SMT. It is hard to interpret the attention activations and extract meaningful word alignments especially from Transformer (Garg et al., 2019). As a result, the most widely used word alignment tools are still external statistical models suc"
2020.emnlp-main.42,W19-5201,0,0.145507,"rd to interpret the attention activations and extract meaningful word alignments especially from Transformer (Garg et al., 2019). As a result, the most widely used word alignment tools are still external statistical models such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Brown et al., 1993; Och and Ney, 2003). 566 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 566–576, c November 16–20, 2020. 2020 Association for Computational Linguistics Recently, there is a resurgence of interest in the community to study word alignments for the Transformer (Ding et al., 2019; Li et al., 2019). One simple solution is NAIVE -ATT, which induces word alignments from the attention weights between the encoder and decoder. The next target word is aligned with the source word that has the maximum attention weight, as shown in Fig. 1. However, such schedule only captures noisy word alignments (Ding et al., 2019; Garg et al., 2019). One of the major problems is that it induces alignment before observing the to-be-aligned target token (Peter et al., 2017; Ding et al., 2019). Suppose for the same source sentence, there are two alternative translations that diverge at decodin"
2020.emnlp-main.42,P17-1106,1,0.862585,"yer of the vanilla Transformer and the right are the induced alignments. S HIFT-ATT induces alignments for target word yi at decoding step i + 1 when yi is the decoder input, while NAIVE -ATT at step i when yi is the decoder output. Introduction The task of word alignment is to find lexicon translation equivalents from parallel corpus (Brown et al., 1993). It is one of the fundamental tasks in natural language processing (NLP) and is widely studied by the community (Dyer et al., 2013; Brown et al., 1993; Liu and Sun, 2015). Word alignments are useful in many scenarios, such as error analysis (Ding et al., 2017; Li et al., 2019), the introduction of coverage and fertility models (Tu et al., 2016), inserting external constraints in interactive machine translation (Hasler et al., 2018; ∗ Corresponding author. Part of the work was done when Yun was in Huawei Noah’s Ark Lab. 1 Code can be found at https://github.com/ sufe-nlp/transformer-alignment. Chen et al., 2020) and providing guidance for human translators in computer-aided translation (Dagan et al., 1993). Word alignment is part of the pipeline in statistical machine translation (Koehn et al., 2003, SMT), but is not necessarily needed for neural m"
2020.emnlp-main.42,N13-1073,0,0.863936,"1: An example to compare our method S HIFTATT and the baseline NAIVE -ATT. The left is an attention map from the third decoder layer of the vanilla Transformer and the right are the induced alignments. S HIFT-ATT induces alignments for target word yi at decoding step i + 1 when yi is the decoder input, while NAIVE -ATT at step i when yi is the decoder output. Introduction The task of word alignment is to find lexicon translation equivalents from parallel corpus (Brown et al., 1993). It is one of the fundamental tasks in natural language processing (NLP) and is widely studied by the community (Dyer et al., 2013; Brown et al., 1993; Liu and Sun, 2015). Word alignments are useful in many scenarios, such as error analysis (Ding et al., 2017; Li et al., 2019), the introduction of coverage and fertility models (Tu et al., 2016), inserting external constraints in interactive machine translation (Hasler et al., 2018; ∗ Corresponding author. Part of the work was done when Yun was in Huawei Noah’s Ark Lab. 1 Code can be found at https://github.com/ sufe-nlp/transformer-alignment. Chen et al., 2020) and providing guidance for human translators in computer-aided translation (Dagan et al., 1993). Word alignment"
2020.emnlp-main.42,D19-1453,0,0.0693459,". Chen et al., 2020) and providing guidance for human translators in computer-aided translation (Dagan et al., 1993). Word alignment is part of the pipeline in statistical machine translation (Koehn et al., 2003, SMT), but is not necessarily needed for neural machine translation (Bahdanau et al., 2015, NMT). The attention mechanism in NMT does not functionally play the role of word alignments between the source and the target, at least not in the same way as its analog in SMT. It is hard to interpret the attention activations and extract meaningful word alignments especially from Transformer (Garg et al., 2019). As a result, the most widely used word alignment tools are still external statistical models such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Brown et al., 1993; Och and Ney, 2003). 566 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 566–576, c November 16–20, 2020. 2020 Association for Computational Linguistics Recently, there is a resurgence of interest in the community to study word alignments for the Transformer (Ding et al., 2019; Li et al., 2019). One simple solution is NAIVE -ATT, which induces word alignments from the attention weights"
2020.emnlp-main.42,I17-1004,0,0.0428756,"Missing"
2020.emnlp-main.42,N18-2081,0,0.0646567,"Missing"
2020.emnlp-main.42,P17-1141,1,0.848262,"over NAIVE -ATT by 1.1 and 1.5 BLEU scores on de→en and en→de translations, respectively. The results suggest the effectiveness of our methods in application to alignment-related NLP tasks. Downstream Task Results In addition to AER, we compare the performance of NAIVE -ATT, S HIFT-ATT and S HIFT-AET on dictionary-guided machine translation (Song et al., 2020), which is an alignment-based downstream task. Given source and target constraint pairs from dictionary, the NMT model is encouraged to translate with provided constraints via word alignments (Alkhouli et al., 2018; Hasler et al., 2018; Hokamp and Liu, 2017; Song et al., 2020). More specifically, at each decoding step, the last token of the candidate translation will be revised with target constraint if it is aligned to the corresponding source constraint according to the alignment induction method. To simulate the process of looking up dictionary, we follow Hasler et al. (2018) and extract the pre-specified constraints from the test set and its reference according to the golden word alignments. We exclude stop words, and sample up to 3 dictionary constraints per sentence. Each dic1 4.4 Analysis Layer Selection Criterion To test whether the laye"
2020.emnlp-main.42,W04-3250,0,0.0731991,"02E18, LDC2003E07, LDC2003E14, LDC2004T07, LDC2004T08 and LDC2005T06 Task de→en en→de NAIVE -ATT 33.7 26.5 S HIFT-ATT 34.3∗ 26.8 (a) Validation AER for Layer Selection S HIFT-AET 34.8∗ 28.0∗ de→en en→de 1 2 3 4 5 6 Table 5: Comparison of dictionary-guided decoding with different alignment methods. We report BLEU scores on the test set. Without dictionary-guided decoding, we obtain 32.3 and 24.2 BLEU on de→en and en→de translations respectively. “*” indicates the result is significantly better than that of NAIVE -ATT (p<0.05). All significance tests are measured by paired bootstrap resampling (Koehn, 2004) set. The test set includes 450 parallel sentence pairs with manually labelled word alignments.9 We use jieba10 for Chinese text segmentation and follow the settings in Section 4.1 for data pre-processing and model training. The results are shown in Table 4. It presents that both S HIFT-ATT and S HIFTAET outperform NAIVE -ATT to a large margin. When comparing the symmetrized alignment performance with GIZA++, S HIFT-AET performs better, while S HIFT-ATT is worse. The experimental results are roughly consistent with the observations on other language pairs, demonstrating the effectiveness of ou"
2020.emnlp-main.42,2005.iwslt-1.8,0,0.49688,"VE -ATT uses zil . l We argue using zi+1 is better. First, at bottom layl ers, we hypothesize that zi+1 could better represent the decoder input yi than output yi+1 . Therefore we l can use zi+1 with small l to represent yi . Second, l zi+1 is computed after observing yi , indicating that S HIFT-ATT is able to adapt the alignment induction with the to-be-aligned target token. Our proposed method involves inducing alignments from source-to-target and target-to-source vanilla Transformer models. Following Zenkel et al. (2019), we merge bidirectional alignments using the grow diagonal heuristic (Koehn et al., 2005). Layer Selection Criterion To select the best layer lb to induce alignments, we propose a surrogate layer selection criterion without manually labelled word alignments. Experiments show that this criterion correlates well with the AER metric. Given parallel sentence pairs hx, yi, we train a source-to-target model θx→y and a target-to-source model θy→x . We assume that the word alignments extracted from these two models should agree with each other (Cheng et al., 2016). Therefore, we evaluate the quality of the alignments by computing the AER score on the validation set with the source-to-targ"
2020.emnlp-main.42,N03-1017,0,0.342534,"Missing"
2020.emnlp-main.42,N16-1082,0,0.294355,"former iwslt de en model configuration following Ding et al. (2019). We train the models with a batch size of 36K tokens and set the maximum updates as 50K and 10K for 2 We simply normalize rows corresponding to target tokens ˆ that are aligned to at least one source token of A. 3 https://www-i6.informatik.rwth-aachen. de/goldAlignment/ 4 http://web.eecs.umich.edu/˜mihalcea/ wpt/index.html 5 https://github.com/pytorch/fairseq Method Inter. FAST-A LIGN (Dyer et al., 2013) GIZA++ (Brown et al., 1993) - NAIVE -ATT (Garg et al., 2019) NAIVE -ATT-LA (Garg et al., 2019) S HIFT-ATT-LA S MOOTH G RAD (Li et al., 2016) SD-S MOOTH G RAD (Ding et al., 2019) PD (Li et al., 2019) A DD SGD (Zenkel et al., 2019) M TL -F ULLC (Garg et al., 2019) Y Y Y Y Y Y N N M TL -F ULLC -GZ (Garg et al., 2019) N S HIFT-ATT S HIFT-AET Y N de-en de→en en→de bidir Statistical Methods Y 28.5 30.4 25.7 Y 18.8 19.6 17.8 Neural Methods N 33.3 36.5 28.1 N 40.9 50.8 39.8 N 54.7 46.2 45.5 N 36.4 45.8 30.3 N 36.4 43.0 29.0 N 38.1 44.8 34.4 N 26.6 30.4 21.2 Y - 20.2 Statistical + Neural Methods Y - 16.0 Our Neural Methods N 20.9 25.7 17.9 N 15.8 19.2 15.4 Fullc fr→en fr-en en→fr bidir ro→en ro-en en→ro bidir 16.3 7.1 17.1 7.2 12.1 6.1 33."
2020.emnlp-main.42,P19-1124,0,0.573565,"Transformer and the right are the induced alignments. S HIFT-ATT induces alignments for target word yi at decoding step i + 1 when yi is the decoder input, while NAIVE -ATT at step i when yi is the decoder output. Introduction The task of word alignment is to find lexicon translation equivalents from parallel corpus (Brown et al., 1993). It is one of the fundamental tasks in natural language processing (NLP) and is widely studied by the community (Dyer et al., 2013; Brown et al., 1993; Liu and Sun, 2015). Word alignments are useful in many scenarios, such as error analysis (Ding et al., 2017; Li et al., 2019), the introduction of coverage and fertility models (Tu et al., 2016), inserting external constraints in interactive machine translation (Hasler et al., 2018; ∗ Corresponding author. Part of the work was done when Yun was in Huawei Noah’s Ark Lab. 1 Code can be found at https://github.com/ sufe-nlp/transformer-alignment. Chen et al., 2020) and providing guidance for human translators in computer-aided translation (Dagan et al., 1993). Word alignment is part of the pipeline in statistical machine translation (Koehn et al., 2003, SMT), but is not necessarily needed for neural machine translation"
2020.emnlp-main.42,C16-1291,0,0.026905,"nd extract better alignments especially for sentence beginning compared to NAIVE -ATT. 5 Related Work Alignment induction from RNNSearch (Bahdanau et al., 2015) has been explored by a number of works. Bahdanau et al. (2015) are the first to show word alignment example using attention in RNNSearch. Ghader and Monz (2017) further demonstrate that the RNN-based NMT system achieves comparable alignment performance to that of GIZA++. Alignment has also been used to improve NMT performance, especially in low resource settings, by supervising the attention mechanisms of RNNSearch (Chen et al., 2016; Liu et al., 2016; Alkhouli and Ney, 2017). There is also a number of other studies that induce word alignment from Transformer. Li et al. (2019); Ding et al. (2019) claim that attention may not capture word alignment in Transformer, and propose to induce word alignment with prediction difference (Li et al., 2019) or gradient-based measures (Ding et al., 2019). Zenkel et al. (2019) modify the Transformer architecture for better align573 !   !  ""  !  "" ""   !  !  $ # # $  ! #$ # $ !  "" "" # $ #$  ""  $ # ""  (a) Naive-Att (b) Shift-Att Figure 4: AER on the test set v.s. BLEU on the validation set on t"
2020.emnlp-main.42,P00-1056,0,0.832681,"er model. The column Fullc denotes whether full target sentence is used to extract alignments at test time. The lower AER, the better. We mark best symmetrized interpretation results of vanilla Transformer with underlines, and best symmetrized results among all with boldface. Transformer and AET respectively. The last checkpoint of AET is used for evaluation. All models are trained in both translation directions and symmetrized with grow-diag (Koehn et al., 2005) using the script from Zenkel et al. (2019).6 Evaluation We evaluate the alignment quality of our methods with Alignment Error Rate (Och and Ney, 2000, AER). Since word alignments are useful for many downstream tasks as discussed in Section 1, we also evaluate our methods on dictionaryguided decoding, a downstream task of alignment induction, with the metric BLEU (Papineni et al., 2002). More details are in Section 4.3. Baselines We compare our methods with two statistical baselines FAST-A LIGN and GIZA++ and nine other baselines: • NAIVE -ATT (Garg et al., 2019): the approach we discuss in Section 2.2, which induces alignments from the attention weights of the penultimate layer of the Transformer. • NAIVE -ATT-LA (Garg et al., 2019): the N"
2020.emnlp-main.42,J03-1002,0,0.136128,"lation (Koehn et al., 2003, SMT), but is not necessarily needed for neural machine translation (Bahdanau et al., 2015, NMT). The attention mechanism in NMT does not functionally play the role of word alignments between the source and the target, at least not in the same way as its analog in SMT. It is hard to interpret the attention activations and extract meaningful word alignments especially from Transformer (Garg et al., 2019). As a result, the most widely used word alignment tools are still external statistical models such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Brown et al., 1993; Och and Ney, 2003). 566 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 566–576, c November 16–20, 2020. 2020 Association for Computational Linguistics Recently, there is a resurgence of interest in the community to study word alignments for the Transformer (Ding et al., 2019; Li et al., 2019). One simple solution is NAIVE -ATT, which induces word alignments from the attention weights between the encoder and decoder. The next target word is aligned with the source word that has the maximum attention weight, as shown in Fig. 1. However, such schedule only captures no"
2020.emnlp-main.42,P02-1040,0,0.106675,"mmetrized results among all with boldface. Transformer and AET respectively. The last checkpoint of AET is used for evaluation. All models are trained in both translation directions and symmetrized with grow-diag (Koehn et al., 2005) using the script from Zenkel et al. (2019).6 Evaluation We evaluate the alignment quality of our methods with Alignment Error Rate (Och and Ney, 2000, AER). Since word alignments are useful for many downstream tasks as discussed in Section 1, we also evaluate our methods on dictionaryguided decoding, a downstream task of alignment induction, with the metric BLEU (Papineni et al., 2002). More details are in Section 4.3. Baselines We compare our methods with two statistical baselines FAST-A LIGN and GIZA++ and nine other baselines: • NAIVE -ATT (Garg et al., 2019): the approach we discuss in Section 2.2, which induces alignments from the attention weights of the penultimate layer of the Transformer. • NAIVE -ATT-LA (Garg et al., 2019): the NAIVE ATT method without layer selection. It induces alignments from attention weights averaged across all layers. • S HIFT-ATT-LA: S HIFT-ATT method without layer selection. It induces alignments from attention weights averaged across all"
2020.emnlp-main.42,P16-1162,0,0.234929,". Once the alignment module is trained, we extract alignment scores S from it given a parallel sentence pair and induce alignments A using Eq. 3. 4 Experiments 4.1 Settings Dataset We follow previous work (Zenkel et al., 2019, 2020) in data setup and conduct experiments on publicly available datasets for German-English (de-en)3 , Romanian-English (ro-en) and FrenchEnglish (fr-en)4 . Since no validation set is provided, we follow Ding et al. (2019) to set the last 1,000 sentences of the training data before preprocessing as validation set. We learn a joint source and target Byte-Pair-Encoding (Sennrich et al., 2016) with 10k merge operations. Table 1 shows the detailed data statistics. > (hGK zilb GQ 1 X n) n )(˜ √ Si−1 = softmax( ), N n dk (5) Q K d ×d model k where Gn ,Gn ∈ R are the key and query projection matrices for the n-th head, N is the number of attention heads and dk = dmodel /N . Since we only care about the attention weights, the value-related parameters and computation are omitted in this module. To train the alignment module, we use the symmetrized S HIFT-ATT alignments extracted from 569 NMT Systems We implement the Transformer with fairseq-py5 and use the transformer iwslt de en model c"
2020.emnlp-main.42,P16-1008,1,0.866093,"ces alignments for target word yi at decoding step i + 1 when yi is the decoder input, while NAIVE -ATT at step i when yi is the decoder output. Introduction The task of word alignment is to find lexicon translation equivalents from parallel corpus (Brown et al., 1993). It is one of the fundamental tasks in natural language processing (NLP) and is widely studied by the community (Dyer et al., 2013; Brown et al., 1993; Liu and Sun, 2015). Word alignments are useful in many scenarios, such as error analysis (Ding et al., 2017; Li et al., 2019), the introduction of coverage and fertility models (Tu et al., 2016), inserting external constraints in interactive machine translation (Hasler et al., 2018; ∗ Corresponding author. Part of the work was done when Yun was in Huawei Noah’s Ark Lab. 1 Code can be found at https://github.com/ sufe-nlp/transformer-alignment. Chen et al., 2020) and providing guidance for human translators in computer-aided translation (Dagan et al., 1993). Word alignment is part of the pipeline in statistical machine translation (Koehn et al., 2003, SMT), but is not necessarily needed for neural machine translation (Bahdanau et al., 2015, NMT). The attention mechanism in NMT does no"
2020.emnlp-main.42,2020.acl-main.146,0,0.193692,"1.4-4.8 AER points in our experiments. • We compare our methods with NAIVE -ATT on dictionary-guided decoding (Alkhouli et al., 2018), an alignment-related downstream task. Both methods consistently outperform NAIVE -ATT, demonstrating the effectiveness of our methods in such alignment-related NLP tasks. To alleviate this problem, some researchers modify the transformer architecture by adding alignment modules that predict the to-be-aligned target token (Zenkel et al., 2019, 2020) or modify the training loss by designing an alignment loss computed with full target sentence (Garg et al., 2019; Zenkel et al., 2020). Others argue that using only attention weights is insufficient for generating clean word alignment and propose to induce alignments with feature importance measures, such as leaveone-out measures (Li et al., 2019) and gradientbased measures (Ding et al., 2019). However, all previous work induces alignment for target word yi at step i, when yi is the decoder output. Let x = {x1 , ..., x|x |} and y = {y1 , ..., y|y |} be source and target sentences. Neural machine translation models the target sentence given the source sentence as p(y|x; θ): In this work, we propose to induce alignment for tar"
2020.emnlp-main.42,D13-1176,0,\N,Missing
2020.emnlp-main.42,C96-2141,0,\N,Missing
2020.emnlp-main.42,P05-1033,0,\N,Missing
2020.emnlp-main.74,W19-4828,0,0.0251489,"the current checkpoint is a better model than the best existing checkpoint, S learns from it, otherwise the best stored checkpoint is considered as the teacher. In all models discussed so far, i) S usually has the same architecture as its teacher(s) but we know that recent NMT models, particularly Transformers, are deep models which makes them challenging to run on edge devices. Moreover, ii) the training criterion in the aforementioned models is to combine final predictions. Transformers have new components (e.g. self-attention) and multiple (sub-)layers that consist of valuable information (Clark et al., 2019) and we need more than an output-level combination to efficiently distill for/from these models. Therefore, a new technique that is capable of addressing i and ii is required. Authors of PKD spotted the problem and focused on internal layers (Sun et al., 2019). They studied the limitations of RKD for BERT (Devlin et al., 2019) models and introduced a layer-to-layer cost function. They select a subset of layers from T whose values are compared to S layers. They also showed that different internal components are important and play critical roles in KD. 1017 The layer-level supervision idea was s"
2020.emnlp-main.74,N19-1423,0,0.0342061,"s which makes them challenging to run on edge devices. Moreover, ii) the training criterion in the aforementioned models is to combine final predictions. Transformers have new components (e.g. self-attention) and multiple (sub-)layers that consist of valuable information (Clark et al., 2019) and we need more than an output-level combination to efficiently distill for/from these models. Therefore, a new technique that is capable of addressing i and ii is required. Authors of PKD spotted the problem and focused on internal layers (Sun et al., 2019). They studied the limitations of RKD for BERT (Devlin et al., 2019) models and introduced a layer-to-layer cost function. They select a subset of layers from T whose values are compared to S layers. They also showed that different internal components are important and play critical roles in KD. 1017 The layer-level supervision idea was successful for monolingual models but so far, no one has tried it in the context of NMT. In this paper, we investigate if the same idea holds for bilingual models or if NMT requires a different type of KD. Moreover, we address the skip problem in PKD (shown in Figure 1). It seems in deep teacher models we do not need to skip la"
2020.emnlp-main.74,P84-1044,0,0.436351,"Missing"
2020.emnlp-main.74,P18-4020,0,0.0211202,"Missing"
2020.emnlp-main.74,D16-1139,0,0.210521,"we have to select a subset of T layers and skip others as there are no peers for all of them on the S side. Clearly, we do not benefit from the skipped layers in this scenario. This type of KD introduces a problem of finding an optimal subset of T layers (to distill from). Although this might, to some extent, be mitigated via a search mechanism, our experimental results show that the problem is severe in NMT and each layer plays a unique role. Therefore, we prefer to keep all layers rather than skip them. KD has recently become popular in NMT but, to the best of our knowledge, all NMT models (Kim and Rush, 2016; Tan et al., 2019) are still trained using the original idea of KD (Hinton et al., 2015), which is referred to as Regular KD (RKD) throughout this paper. RKD only matches S and T outputs, regardless of their internal architecture. However, there exist techniques such as Patient KD (PKD) (Sun et al., 2019) proposed for other tasks that not only match final predictions but also focus on internal components and distill their information too (Sun et al., 2020). In this research, we borrowed those ideas and adapted them to NMT. This is the first contribution of the paper. PKD and other similar mod"
2020.emnlp-main.74,P17-4012,0,0.0988482,"Missing"
2020.emnlp-main.74,D18-2012,0,0.038594,"Missing"
2020.emnlp-main.74,N19-4009,0,0.034692,"Missing"
2020.emnlp-main.74,P02-1040,0,0.106021,"Missing"
2020.emnlp-main.74,W18-6319,0,0.0249574,"Missing"
2020.emnlp-main.74,D19-1441,0,0.0744975,"Missing"
2020.emnlp-main.74,2020.acl-main.195,0,0.0527923,"prefer to keep all layers rather than skip them. KD has recently become popular in NMT but, to the best of our knowledge, all NMT models (Kim and Rush, 2016; Tan et al., 2019) are still trained using the original idea of KD (Hinton et al., 2015), which is referred to as Regular KD (RKD) throughout this paper. RKD only matches S and T outputs, regardless of their internal architecture. However, there exist techniques such as Patient KD (PKD) (Sun et al., 2019) proposed for other tasks that not only match final predictions but also focus on internal components and distill their information too (Sun et al., 2020). In this research, we borrowed those ideas and adapted them to NMT. This is the first contribution of the paper. PKD and other similar models suffer from the skip problem, which happens when T has more layers than S and some T layers have to be skipped in order to carry out layer-to-layer distillation. In this paper, we propose a model to distill from all teacher 1016 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1016–1021, c November 16–20, 2020. 2020 Association for Computational Linguistics layers so we do not have to skip any of them. This i"
2020.emnlp-main.74,W18-1819,0,0.0610558,"Missing"
2020.emnlp-main.74,N19-1192,0,0.135343,"Missing"
2020.findings-emnlp.207,N19-1423,0,0.218991,"ter called “in-entity” and “outentity”) of node “Bacterial pneumonia” are linked by various relation paths. These linked nodes and correlations can be seen as “graph contextualized information” of entity node “Bacterial pneumonia”. In this study, we will explore how to integrate graph contextualized knowledge into pre-trained language models. Pre-trained language models learn contextualized word representations on large-scale text corpus through self-supervised learning methods, and obtain new state-of-the-art (SOTA) results on most downstream tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). This gradually becomes a new paradigm for natural language processing research. Recently, several knowledgeenhanced pre-trained language models have been proposed, such as ERNIE-Baidu (Sun et al., 2019), ERNIE-Tsinghua (Zhang et al., 2019a), WKLM (Xiong et al., 2019) and K-ADAPTER (Wang et al., 2020). In this study, since we need to learn graph contextualized knowledge in a large-scale medical knowledge graph, ERNIE-Tsinghua (hereinafter called “ERNIE”) is chosen as our backbone model. In ERNIE, entity embeddings are learned by TransE (Bordes et al., 2013), which is a popular transitionbased"
2020.findings-emnlp.207,D18-2024,0,0.165269,"eed forward layer for further encoding. The aforementioned Transformer blocks are stacked by L times, and the output hidden states can be formalized as O XO = {xO 1 , . . . , xN }. (4) Then, the node position indexes P is utilized to restore triple representations: T = TripleRestoration(XO , P), (5) where Pk = (eks , rk , eko ) is the position index of a valid knowledge triple, and Tk = (xO , xO , xO ) eks rk eko is the representation of this triple. The subfigure in the upper right corner of Figure 3 shows the triple restoration process. In this study, the translation-based scoring function (Han et al., 2018) is adopted to measure the energy of a knowledge triple. The node embeddings are learned by minimizing a margin-based loss function on the training data: X L= max{d(t) − d(f (t)) + γ, 0}, (6) t∈T where t = (ts , tr , to ), d(t) = |ts + tr − to |, γ > 0 is a margin hyperparameter, f (t) is an entity replacement operation that the head entity or the tail entity in a triple is replaced and the replaced triple is an invalid triple in the KG. 2.2 # Entities 2,842,735 In-degree 5.05 Integrating Knowledge into the Language Model Given a comprehensive medical knowledge graph, graph contextualized know"
2020.findings-emnlp.207,W04-1213,0,0.566159,"d to split the sentences within a paragraph, and sentences having less than 5 words are discarded. As a result, a large corpus containing 9.9B tokens is achieved for language model pre-training. 2284 2 https://www.ncbi.nlm.nih.gov/pubmed/. https://www.ncbi.nlm.nih.gov/pmc/. 4 https://www.nltk.org/. 3 Table 2: Statistics of the datasets. Most of these datasets do not follow a standard train-valid-test set partition, and we adopt some traditional data partition ways to do model training and evaluation. Task Entity Typing Relation Classification Dataset 2010 i2b2/VA (Uzuner et al., 2011) JNLPBA (Kim et al., 2004) BC5CDR (Li et al., 2016) 2010 i2b2/VA (Uzuner et al., 2011) GAD (Bravo et al., 2015) EU-ADR (Van Mulligen et al., 2012) In our model, medical terms appearing in the corpus need to be aligned to the entities in the UMLS metathesaurus before pre-training. To make sure the coverage of identified entities in the metathesaurus, the forward maximum matching (FMM) algorithm is used to extract the term spans from the corpus aforementioned, and spans less than 5 characters are filtered. Then, BERT vocabulary is used to tokenize the input text into word pieces, and the medical entity is aligned with th"
2020.findings-emnlp.207,P19-1466,0,0.265169,"TransE (Bordes et al., 2013), which is a popular transitionbased method for knowledge representation learning (KRL). However, TransE cannot deal with the modeling of complex relations (Lin et al., 2281 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2281–2290 c November 16 - 20, 2020. 2020 Association for Computational Linguistics 2018), such as 1-to-n, n-to-1 and n-to-n relations. This shortcoming will be amplified in the medical knowledge graph, in which many entities have a large number of related neighbors. Inspired by previous work (Veliˇckovi´c et al., 2018; Nathani et al., 2019), we propose an approach to learn knowledge from subgraphs, and inject graph contextualized knowledge into the pretrained language model. We call this model BERTMK (a BERT-based language model integrated with Medical Knowledge), our contributions are as follows: • We propose a novel knowledge-enhanced pretrained language model BERT-MK for medical NLP tasks, which integrates graph contextualized knowledge learned from the medical KG. Algorithm 1: Subgraph generation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Input: Knowledge graph G = (E, R, T ), duplicate number M Output: Subgraph set S Initial S"
2020.findings-emnlp.207,N18-2053,0,0.0284057,"(Bhasuran and Natarajan, 2018), which achieves SOTA performance in the GAD dataset. CNN-M stands for CNN using multi-pooling (He et al., 2019), which is the SOTA model in the 2010 i2b2/VA dataset. Task Entity Typing Relation Classification Dataset 2010 i2b2/VA JNLPBA BC5CDR 2010 i2b2/VA GAD EU-ADR Metrics Acc Acc Acc P R F P R F P R F E-SVM 79.21 89.25 83.93 - CNN-M 73.1 66.7 69.7 - use OpenKE toolkit (Han et al., 2018) to learn entity and relation embeddings. Knowledge embedding dimension is set to 100, while training epoch number is set to 10000. Following the initialization method used in (Nguyen et al., 2018; Nathani et al., 2019), the embeddings produced by TransE are utilized to initialize knowledge representations of the GCKE module. We set the layer number to 4, and each layer contains 4 heads. Due to the median degree of entities in UMLS is 4 (shown in Table1), we set the count of in-entities and two out-entities to 4, so each subgraph contains four 1-hop and four 2-hop relations. The GCKE module runs 1200 epochs on a single NVIDIA Tesla V100 (32GB) GPU to learn graph contextualized knowledge. The batch size is set to 50000. 3.3.2 Pre-training In this study, two pre-trained language models a"
2020.findings-emnlp.207,N18-1202,0,0.038555,"d four outgoing neighboring nodes (hereinafter called “in-entity” and “outentity”) of node “Bacterial pneumonia” are linked by various relation paths. These linked nodes and correlations can be seen as “graph contextualized information” of entity node “Bacterial pneumonia”. In this study, we will explore how to integrate graph contextualized knowledge into pre-trained language models. Pre-trained language models learn contextualized word representations on large-scale text corpus through self-supervised learning methods, and obtain new state-of-the-art (SOTA) results on most downstream tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). This gradually becomes a new paradigm for natural language processing research. Recently, several knowledgeenhanced pre-trained language models have been proposed, such as ERNIE-Baidu (Sun et al., 2019), ERNIE-Tsinghua (Zhang et al., 2019a), WKLM (Xiong et al., 2019) and K-ADAPTER (Wang et al., 2020). In this study, since we need to learn graph contextualized knowledge in a large-scale medical knowledge graph, ERNIE-Tsinghua (hereinafter called “ERNIE”) is chosen as our backbone model. In ERNIE, entity embeddings are learned by TransE (Bordes et al"
2020.findings-emnlp.207,P19-1139,1,0.939823,"udy, we will explore how to integrate graph contextualized knowledge into pre-trained language models. Pre-trained language models learn contextualized word representations on large-scale text corpus through self-supervised learning methods, and obtain new state-of-the-art (SOTA) results on most downstream tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). This gradually becomes a new paradigm for natural language processing research. Recently, several knowledgeenhanced pre-trained language models have been proposed, such as ERNIE-Baidu (Sun et al., 2019), ERNIE-Tsinghua (Zhang et al., 2019a), WKLM (Xiong et al., 2019) and K-ADAPTER (Wang et al., 2020). In this study, since we need to learn graph contextualized knowledge in a large-scale medical knowledge graph, ERNIE-Tsinghua (hereinafter called “ERNIE”) is chosen as our backbone model. In ERNIE, entity embeddings are learned by TransE (Bordes et al., 2013), which is a popular transitionbased method for knowledge representation learning (KRL). However, TransE cannot deal with the modeling of complex relations (Lin et al., 2281 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2281–2290 c November 16 -"
2020.findings-emnlp.327,N18-1118,0,0.0227514,"le (3) in Figure 1). The former can be correctly interpreted by resorting to commonsense knowledge while the latter cannot be interpreted uniquely if no more context is given. The third rule that we conform to is to 1) create two contrastive source sentences for each lexical or syntactic ambiguity point, where each source sentence corresponds to one reasonable interpretation of the ambiguity point, and 2) to provide two contrastive translations for each created source sentence. This is similar to other linguistic evaluation by contrastive examples in the MT literature (Avramidis et al., 2019; Bawden et al., 2018; M¨uller et al., 2018; Sennrich, 2017). These two contrastive translations have similar wordings: one is correct and the other is not correct in that it translates the ambiguity part into the corresponding translation of the contrastive source sentence. This translation makes sense in the contrastive sentence but not in the sentence in question. Examples of contrastive source sentences and contrastive translations for each source sentence are shown in Figure 2, 3 and 4. 3664 z1 维修 桌子 的 桌脚 。 z1 主力 部队 已经 对 敌人的 建筑 展开 了 攻关 。 1 er1 Repair the legs of the table. The main force has already launched"
2020.findings-emnlp.327,D19-1255,0,0.0264059,"them on monolingual data, we provide a bilingual commonsense test suite for machine translation. Commonsense Reasoning in NLP In addition to common sense datasets, we have also witnessed that commonsense knowledge has been recently explored in different NLP tasks. Just to name a few, Trinh and Le (2018), He et al. (2019) and Klein and Nabi (2019) use language models trained on huge text corpora to do inference on the WSC dataset. Ding et al. (2019) use commonsense knowledge in Atomic (Sap et al., 2019a) and Event2mind (Rashkin et al., 2018) on downstream tasks such as script event prediction. Bi et al. (2019) exploit external commonsense knowledge from ConceptNet (Speer et al., 2016)) in machine reading comprehension. 3663 Commonsense Reasoning Evaluation With pre-trained language models, like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) being widely used in various NLP tasks, studies have been performed to examine the commonsense reasoning capability in pre-trained neural language models. Wang et al. (2019) and Zhou et al. (2020) propose to measure the success rate of the pretrained language models in commonsense inference by calculating LM probabilities. Two sentences which are used"
2020.findings-emnlp.327,W19-5351,0,0.0129188,"ntextual SA (e.g., Example (3) in Figure 1). The former can be correctly interpreted by resorting to commonsense knowledge while the latter cannot be interpreted uniquely if no more context is given. The third rule that we conform to is to 1) create two contrastive source sentences for each lexical or syntactic ambiguity point, where each source sentence corresponds to one reasonable interpretation of the ambiguity point, and 2) to provide two contrastive translations for each created source sentence. This is similar to other linguistic evaluation by contrastive examples in the MT literature (Avramidis et al., 2019; Bawden et al., 2018; M¨uller et al., 2018; Sennrich, 2017). These two contrastive translations have similar wordings: one is correct and the other is not correct in that it translates the ambiguity part into the corresponding translation of the contrastive source sentence. This translation makes sense in the contrastive sentence but not in the sentence in question. Examples of contrastive source sentences and contrastive translations for each source sentence are shown in Figure 2, 3 and 4. 3664 z1 维修 桌子 的 桌脚 。 z1 主力 部队 已经 对 敌人的 建筑 展开 了 攻关 。 1 er1 Repair the legs of the table. The main force"
2020.findings-emnlp.327,N19-1423,0,0.10551,"ge has been recently explored in different NLP tasks. Just to name a few, Trinh and Le (2018), He et al. (2019) and Klein and Nabi (2019) use language models trained on huge text corpora to do inference on the WSC dataset. Ding et al. (2019) use commonsense knowledge in Atomic (Sap et al., 2019a) and Event2mind (Rashkin et al., 2018) on downstream tasks such as script event prediction. Bi et al. (2019) exploit external commonsense knowledge from ConceptNet (Speer et al., 2016)) in machine reading comprehension. 3663 Commonsense Reasoning Evaluation With pre-trained language models, like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) being widely used in various NLP tasks, studies have been performed to examine the commonsense reasoning capability in pre-trained neural language models. Wang et al. (2019) and Zhou et al. (2020) propose to measure the success rate of the pretrained language models in commonsense inference by calculating LM probabilities. Two sentences which are used to test commonsense inference differ only in commonsense concepts. Feldman et al. (2019) further explore unsupervised methods to generate commonsense knowledge using the world knowledge of pre-trained language model"
2020.findings-emnlp.327,D19-1495,0,0.0124263,"llenge (WSC) test set (Levesque et al., 2012; Sakaguchi et al., 2020) focus on solving the commonsense problems in the form of coreference resolution. Different from them on monolingual data, we provide a bilingual commonsense test suite for machine translation. Commonsense Reasoning in NLP In addition to common sense datasets, we have also witnessed that commonsense knowledge has been recently explored in different NLP tasks. Just to name a few, Trinh and Le (2018), He et al. (2019) and Klein and Nabi (2019) use language models trained on huge text corpora to do inference on the WSC dataset. Ding et al. (2019) use commonsense knowledge in Atomic (Sap et al., 2019a) and Event2mind (Rashkin et al., 2018) on downstream tasks such as script event prediction. Bi et al. (2019) exploit external commonsense knowledge from ConceptNet (Speer et al., 2016)) in machine reading comprehension. 3663 Commonsense Reasoning Evaluation With pre-trained language models, like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) being widely used in various NLP tasks, studies have been performed to examine the commonsense reasoning capability in pre-trained neural language models. Wang et al. (2019) and Zhou et al."
2020.findings-emnlp.327,L16-1002,0,0.0264202,"anslation (Nirenburg, 1989). Large ontology that is constructed either manually or automatically to provide world knowledge is one of essential components in KBMT (Knight and Luk, 1994). As data-driven machine translation, such as statistical machine translation (SMT) and neural machine translation, becomes de facto standard in machine translation, world knowledge has been less explicitly explored. Only a few studies have indirectly and partially exploited world knowledge in SMT or NMT, by incorporating linked open data resources such as DBpedia and BabelNet into SMT with modest improvements (Du et al., 2016; Srivastava et al., 2017; Moussallem et al., 2018). 3 Commonsense Reasoning Test Suite for Machine Translation In this section, we discuss the design and construction of the test suite, including the rules and steps for building this test suite. 3.1 Test Suite Design Different from commonsense reasoning in Winogram Schema Challenge (Levesque et al., 2012) or sentence reasonability judgment (i.e., “He put a turkey into the fridge” vs. “He put an elephant into the fridge”) (Wang et al., 2019), where commonsense reasoning normally happens in one language, commonsense reasoning in NMT can be done"
2020.findings-emnlp.327,D19-1109,0,0.0274275,"l., 2016)) in machine reading comprehension. 3663 Commonsense Reasoning Evaluation With pre-trained language models, like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) being widely used in various NLP tasks, studies have been performed to examine the commonsense reasoning capability in pre-trained neural language models. Wang et al. (2019) and Zhou et al. (2020) propose to measure the success rate of the pretrained language models in commonsense inference by calculating LM probabilities. Two sentences which are used to test commonsense inference differ only in commonsense concepts. Feldman et al. (2019) further explore unsupervised methods to generate commonsense knowledge using the world knowledge of pre-trained language models. Our commonsense reasoning evaluation resonates with these evaluation efforts. Commonsense Knowledge and Reasoning in Machine Translation Commonsense knowledge has long been acknowledged as an indispensable knowledge source for disambiguation in machine translation (Bar-Hillel, 1960b; Davis and Marcus, 2015). Knowledgebased machine translation (KBMT), one of the popular machine translation paradigms in 1980s, lays much stress on extra-linguistic world knowledge in ma"
2020.findings-emnlp.327,D19-6002,0,0.0199909,"ct cloze tests. Bhagavatula et al. (2019) propose a dataset for abductive reasoning on events. The wellknown Winograd Schema Challenge (WSC) test set (Levesque et al., 2012; Sakaguchi et al., 2020) focus on solving the commonsense problems in the form of coreference resolution. Different from them on monolingual data, we provide a bilingual commonsense test suite for machine translation. Commonsense Reasoning in NLP In addition to common sense datasets, we have also witnessed that commonsense knowledge has been recently explored in different NLP tasks. Just to name a few, Trinh and Le (2018), He et al. (2019) and Klein and Nabi (2019) use language models trained on huge text corpora to do inference on the WSC dataset. Ding et al. (2019) use commonsense knowledge in Atomic (Sap et al., 2019a) and Event2mind (Rashkin et al., 2018) on downstream tasks such as script event prediction. Bi et al. (2019) exploit external commonsense knowledge from ConceptNet (Speer et al., 2016)) in machine reading comprehension. 3663 Commonsense Reasoning Evaluation With pre-trained language models, like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) being widely used in various NLP tasks, studies have been pe"
2020.findings-emnlp.327,D19-1243,0,0.0366772,"Missing"
2020.findings-emnlp.327,P19-1477,0,0.0229173,"vatula et al. (2019) propose a dataset for abductive reasoning on events. The wellknown Winograd Schema Challenge (WSC) test set (Levesque et al., 2012; Sakaguchi et al., 2020) focus on solving the commonsense problems in the form of coreference resolution. Different from them on monolingual data, we provide a bilingual commonsense test suite for machine translation. Commonsense Reasoning in NLP In addition to common sense datasets, we have also witnessed that commonsense knowledge has been recently explored in different NLP tasks. Just to name a few, Trinh and Le (2018), He et al. (2019) and Klein and Nabi (2019) use language models trained on huge text corpora to do inference on the WSC dataset. Ding et al. (2019) use commonsense knowledge in Atomic (Sap et al., 2019a) and Event2mind (Rashkin et al., 2018) on downstream tasks such as script event prediction. Bi et al. (2019) exploit external commonsense knowledge from ConceptNet (Speer et al., 2016)) in machine reading comprehension. 3663 Commonsense Reasoning Evaluation With pre-trained language models, like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) being widely used in various NLP tasks, studies have been performed to examine the com"
2020.findings-emnlp.327,W18-6307,0,0.0383687,"Missing"
2020.findings-emnlp.327,2001.mtsummit-papers.68,0,0.0627087,"and 512-dimension hidden states. We used Adam (Kingma and Ba, 2015) to train both NMT models. β1 and β2 of Adam were set to 0.9 and 0.999, the learning rate was set to 0.0005, and gradient norm was set to 5. To take full advantage of GPUs, we batched sentences of similar lengths. We trained both models on a single machine with 8 1080Ti cards. Each mini-batch contained 32,000 tokens. During decoding, we employed the beam search algorithm and set the beam size to 5. 5.2 Evaluation Metrics For translation performance evaluation, we used sacrebleu (Post, 2018) to calculate case-sensitive BLEU-4 (Papineni et al., 2001). To evaluate the commonsense reasoning accuracy of NMT on the test suite, we applied NMT models to score each pair (s, t) as follows: |t| Score(t|s) = 1 X logp(ti |t<i , s) |t| (1) i=0 In this section, we conducted extensive experiments to evaluate the commonsense reasoning capability of state-of-the-art neural machine translation on the built test suite. where p(ti |t<i , s) is the probabilty of the target word ti given the target history and source sentence. Given a triple (z, er , ec ), if an NMT model scores the reference translation higher than the contrastive translation (i.e., Score(er"
2020.findings-emnlp.327,W18-6319,0,0.0126514,"ployed neural architecture with 4 layers of LSTM and 512-dimension hidden states. We used Adam (Kingma and Ba, 2015) to train both NMT models. β1 and β2 of Adam were set to 0.9 and 0.999, the learning rate was set to 0.0005, and gradient norm was set to 5. To take full advantage of GPUs, we batched sentences of similar lengths. We trained both models on a single machine with 8 1080Ti cards. Each mini-batch contained 32,000 tokens. During decoding, we employed the beam search algorithm and set the beam size to 5. 5.2 Evaluation Metrics For translation performance evaluation, we used sacrebleu (Post, 2018) to calculate case-sensitive BLEU-4 (Papineni et al., 2001). To evaluate the commonsense reasoning accuracy of NMT on the test suite, we applied NMT models to score each pair (s, t) as follows: |t| Score(t|s) = 1 X logp(ti |t<i , s) |t| (1) i=0 In this section, we conducted extensive experiments to evaluate the commonsense reasoning capability of state-of-the-art neural machine translation on the built test suite. where p(ti |t<i , s) is the probabilty of the target word ti given the target history and source sentence. Given a triple (z, er , ec ), if an NMT model scores the reference translat"
2020.findings-emnlp.327,P18-1043,0,0.148031,". We refer readers to Storks et al. (2019)’s article for a thorough survey in this area. Commonsense Datasets According to Gunning (2018), commonsense knowledge normally consists of a general theory of how the physical world works and a basic understanding of human motives and behaviors. In recent years, a wide variety of datasets on the two kinds of commonsense knowledge have been proposed. Sap et al. (2019b) introduce Social IQA, containing 38k multiple choice questions for probing the commonsense reasoning about emotional and social in people’s daily life. Similarly, Event2mind and Atomic (Rashkin et al., 2018; Sap et al., 2019a) focus on inferred knowledge in the form of if-then to reason about people’s daily life behavior. For datasets on physical common sense, PIQA (Bisk et al., 2020) on commonsense phenomena in the physical world contains 21K QA pairs. SWAG and HellaSwag (Zellers et al., 2018, 2019) are datasets on commonsense NLI, where materials from video subtitles and wikihow articles are used to construct cloze tests. Bhagavatula et al. (2019) propose a dataset for abductive reasoning on events. The wellknown Winograd Schema Challenge (WSC) test set (Levesque et al., 2012; Sakaguchi et al."
2020.findings-emnlp.327,D19-1454,0,0.0302441,"Missing"
2020.findings-emnlp.327,W09-1701,0,0.112068,"can be done either in the encoding of the source language (i.e., encoding reasonable source representations) or in the decoding of the target language (i.e., producing reasonable target outputs). As it is difficult to detect whether reasonable senses are identified and encoded in the encoder, we check target outputs from the decoder to test the commonsense reasoning capability of NMT. This is the first rule that we follow to design the test suite. In the second rule for building the test suite, we manually create source sentences with ambiguity that requires commonsense reasoning. Inspired by Schwartz and Gomez (2009) and Ovchinnikova (2012), we ground the commonsense reasoning test on two types of ambiguity: lexical and syntactic ambiguity (LA and SA), which are common in machine translation. An example in LA is the “batter” in “she put the batter in the refrigerator” (food material vs. baseball player). SA relates to structures, for instance, “I saw a man swimming on the bridge” (I was standing on the bridge vs. The man was swimming on the bridge). We further refine SA into contextless (e.g., Example (2) in Figure 1) and contextual SA (e.g., Example (3) in Figure 1). The former can be correctly interpret"
2020.findings-emnlp.327,E17-2060,0,0.0231249,"ectly interpreted by resorting to commonsense knowledge while the latter cannot be interpreted uniquely if no more context is given. The third rule that we conform to is to 1) create two contrastive source sentences for each lexical or syntactic ambiguity point, where each source sentence corresponds to one reasonable interpretation of the ambiguity point, and 2) to provide two contrastive translations for each created source sentence. This is similar to other linguistic evaluation by contrastive examples in the MT literature (Avramidis et al., 2019; Bawden et al., 2018; M¨uller et al., 2018; Sennrich, 2017). These two contrastive translations have similar wordings: one is correct and the other is not correct in that it translates the ambiguity part into the corresponding translation of the contrastive source sentence. This translation makes sense in the contrastive sentence but not in the sentence in question. Examples of contrastive source sentences and contrastive translations for each source sentence are shown in Figure 2, 3 and 4. 3664 z1 维修 桌子 的 桌脚 。 z1 主力 部队 已经 对 敌人的 建筑 展开 了 攻关 。 1 er1 Repair the legs of the table. The main force has already launched an attack on the enemy’s building. 1 1"
2020.findings-emnlp.327,P16-1162,0,0.0552699,"MT model is believed to make a correct commonsense reasoning prediction. This is reasonable as er and ec are only different in words or structures related to the lexical or syntactical commonsense ambiguity point as described in Section 3.1. By scoring each triple with an NMT model, we can measure the commonsense reasoning accuracy of the model on our test suite. 5.1 5.3 5 Experiments Experimental setup corpus3 We adopted the CWMT Chinese-English of news domain as training data for NMT systems. This corpus contains 9M parallel sentences. We used byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and restricted merge operations to a maximum of 30k. We trained two neural machine translation models on the training data: RNNSearch (Bahdanau et al., 2015) and Transformer (Vaswani et al., 2017). 3 Results BLEU scores for the two NMT models are given in Table 4. Commonsense reasoning results on the test suite are provided in Table 5. From the table and figure, we can observe that Available at: http://nlp.nju.edu.cn/cwmt-wmt 3667 • Both BLEU and commonsense reasoning accuracy clearly show that Transformer is better than RNNSearch. • Both RNNSearch and Transformer pe"
2020.findings-emnlp.327,D18-1009,0,0.0250886,"recent years, a wide variety of datasets on the two kinds of commonsense knowledge have been proposed. Sap et al. (2019b) introduce Social IQA, containing 38k multiple choice questions for probing the commonsense reasoning about emotional and social in people’s daily life. Similarly, Event2mind and Atomic (Rashkin et al., 2018; Sap et al., 2019a) focus on inferred knowledge in the form of if-then to reason about people’s daily life behavior. For datasets on physical common sense, PIQA (Bisk et al., 2020) on commonsense phenomena in the physical world contains 21K QA pairs. SWAG and HellaSwag (Zellers et al., 2018, 2019) are datasets on commonsense NLI, where materials from video subtitles and wikihow articles are used to construct cloze tests. Bhagavatula et al. (2019) propose a dataset for abductive reasoning on events. The wellknown Winograd Schema Challenge (WSC) test set (Levesque et al., 2012; Sakaguchi et al., 2020) focus on solving the commonsense problems in the form of coreference resolution. Different from them on monolingual data, we provide a bilingual commonsense test suite for machine translation. Commonsense Reasoning in NLP In addition to common sense datasets, we have also witnessed t"
2020.findings-emnlp.327,P19-1472,0,0.0370488,"Missing"
2020.findings-emnlp.327,P19-1393,0,0.248639,"the WSC dataset. Ding et al. (2019) use commonsense knowledge in Atomic (Sap et al., 2019a) and Event2mind (Rashkin et al., 2018) on downstream tasks such as script event prediction. Bi et al. (2019) exploit external commonsense knowledge from ConceptNet (Speer et al., 2016)) in machine reading comprehension. 3663 Commonsense Reasoning Evaluation With pre-trained language models, like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) being widely used in various NLP tasks, studies have been performed to examine the commonsense reasoning capability in pre-trained neural language models. Wang et al. (2019) and Zhou et al. (2020) propose to measure the success rate of the pretrained language models in commonsense inference by calculating LM probabilities. Two sentences which are used to test commonsense inference differ only in commonsense concepts. Feldman et al. (2019) further explore unsupervised methods to generate commonsense knowledge using the world knowledge of pre-trained language models. Our commonsense reasoning evaluation resonates with these evaluation efforts. Commonsense Knowledge and Reasoning in Machine Translation Commonsense knowledge has long been acknowledged as an indispens"
2020.findings-emnlp.372,S17-2001,0,0.0294609,"orming the proposed Transformer distillation method at both the pre-training and fine-tuning stages, TinyBERT can achieve competitive performances in various NLP tasks. 4 Experiments In this section, we evaluate the effectiveness and efficiency of TinyBERT on a variety of tasks with different model settings. 4.1 Datasets We evaluate TinyBERT on the General Language Understanding Evaluation (GLUE) (Wang et al., 2018) benchmark, which consists of 2 singlesentence tasks: CoLA (Warstadt et al., 2019), SST2 (Socher et al., 2013), 3 sentence similarity tasks: MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017), QQP (Chen et al., 2018), and 4 natural language inference tasks: MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Bentivogli et al., 2009) and WNLI (Levesque et al., 2012). The metrics for these tasks can be found in the GLUE paper (Wang et al., 2018). 4.2 TinyBERT Settings We instantiate a tiny student model (the number of layers M =4, the hidden size d0 =312, the feedforward/filter size d0i =1200 and the head number h=12) that has a total of 14.5M parameters. This model is referred to as TinyBERT4 . The original 3 A word is tokenized into multiple word-pieces by the tokeni"
2020.findings-emnlp.372,W19-4828,0,0.28644,"s the difficulty of BERT distillation. Therefore, it is required to design an effective KD strategy for both training stages. To build a competitive TinyBERT, we firstly propose a new Transformer distillation method to distill the knowledge embedded in teacher BERT. Specifically, we design three types of loss functions to fit different representations from BERT layers: 1) the output of the embedding layer; 2) the hidden states and attention matrices derived from the Transformer layer; 3) the logits output by the prediction layer. The attention based fitting is inspired by the recent findings (Clark et al., 2019) that the attention weights learned by BERT can capture substantial linguistic knowledge, and it thus encourages the linguistic knowledge can be well transferred from teacher BERT to student TinyBERT. Then, we propose a novel two-stage learning framework including the general distillation and the task-specific distillation, as illustrated in Figure 1. At general distillation stage, the original BERT without fine-tuning acts as the teacher model. The student TinyBERT mimics the teacher’s behavior through the proposed Transformer distillation on general-domain corpus. After that, we obtain a gen"
2020.findings-emnlp.372,D19-1361,0,0.020829,"we actually 1) conduct embedding-layer distillation at the pre-training stage; 2) perform embedding-layer and prediction-layer distillation at finetuning stage. System MNLI-m MNLI-mm MRPC CoLA Avg Uniform 82.8 82.9 85.8 50.8 75.6 Top 81.7 82.3 83.6 35.9 70.9 Bottom 80.6 81.3 84.6 38.5 71.3 Table 4: Results (dev) of different mapping strategies for TinyBERT4 . et al., 2019; Lan et al., 2020), weight sharing (Dehghani et al., 2019; Lan et al., 2020), knowledge distillation (Tang et al., 2019; Sanh et al., 2019; Turc et al., 2019; Sun et al., 2020; Liu et al., 2020; Wang et al., 2020), pruning (Cui et al., 2019; McCarley, 2019; F. et al., 2020; Elbayad et al., 2020; Gordon et al., 2020; Hou et al., 2020) or quantization (Shen et al., 2019; Zafrir et al., 2019). In this paper, our focus is on knowledge distillation. Knowledge Distillation for PLMs There have been some works trying to distill pre-trained language models (PLMs) into smaller models. BiLSTMSOFT (Tang et al., 2019) distills taskspecific knowledge from BERT into a singlelayer BiLSTM. BERT-PKD (Sun et al., 2019) extracts knowledges not only from the last layer of the teacher, but also from intermediate layers at fine-tuning stage. DistilBER"
2020.findings-emnlp.372,D18-1232,0,0.0203184,"ro et al., 2014). In this paper, we focus on knowledge distillation, an idea originated from Hinton et al. (2015), in a teacher-student framework. KD aims to transfer the knowledge embedded in a large teacher network to a small student network where the student network is trained to reproduce the behaviors of the teacher network. Based on the framework, we propose a novel distillation method specifically for the Transformer-based models (Vaswani et al., 2017), and use BERT as an example to investigate the method for large-scale PLMs. KD has been extensively studied in NLP (Kim and Rush, 2016; Hu et al., 2018) as well as for pre-trained language models (Sanh et al., 2019; Sun et al., 2019, 2020; Wang et al., 2020). The pre-training-then-fine-tuning paradigm firstly pre4163 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163–4174 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Large-scale Text Corpus General Distillation General TinyBERT Data Augmentation Task Dataset Task-specific Distillation Fine-tuned TinyBERT Augmented Task Dataset Figure 1: The illustration of TinyBERT learning. trains BERT on a large-scale unsupervised text corpus, then f"
2020.findings-emnlp.372,D16-1139,0,0.0391255,"tillation (KD) (Romero et al., 2014). In this paper, we focus on knowledge distillation, an idea originated from Hinton et al. (2015), in a teacher-student framework. KD aims to transfer the knowledge embedded in a large teacher network to a small student network where the student network is trained to reproduce the behaviors of the teacher network. Based on the framework, we propose a novel distillation method specifically for the Transformer-based models (Vaswani et al., 2017), and use BERT as an example to investigate the method for large-scale PLMs. KD has been extensively studied in NLP (Kim and Rush, 2016; Hu et al., 2018) as well as for pre-trained language models (Sanh et al., 2019; Sun et al., 2019, 2020; Wang et al., 2020). The pre-training-then-fine-tuning paradigm firstly pre4163 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163–4174 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Large-scale Text Corpus General Distillation General TinyBERT Data Augmentation Task Dataset Task-specific Distillation Fine-tuned TinyBERT Augmented Task Dataset Figure 1: The illustration of TinyBERT learning. trains BERT on a large-scale unsupervised t"
2020.findings-emnlp.372,D19-1445,0,0.12292,"aster/TinyBERT † natural language processing (NLP). Pre-trained language models (PLMs), such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), T5 (Raffel et al., 2019) and ELECTRA (Clark et al., 2020), have achieved great success in many NLP tasks (e.g., the GLUE benchmark (Wang et al., 2018) and the challenging multi-hop reasoning task (Ding et al., 2019)). However, PLMs usually have a large number of parameters and take long inference time, which are difficult to be deployed on edge devices such as mobile phones. Recent studies (Kovaleva et al., 2019; Michel et al., 2019; Voita et al., 2019) demonstrate that there is redundancy in PLMs. Therefore, it is crucial and feasible to reduce the computational overhead and model storage of PLMs while retaining their performances. There have been many model compression techniques (Han et al., 2016) proposed to accelerate deep model inference and reduce model size while maintaining accuracy. The most commonly used techniques include quantization (Gong et al., 2014), weights pruning (Han et al., 2015), and knowledge distillation (KD) (Romero et al., 2014). In this paper, we focus on knowledge distill"
2020.findings-emnlp.372,2020.acl-main.537,0,0.0409414,"original train set. 11 Under “w/o Trm” setting, we actually 1) conduct embedding-layer distillation at the pre-training stage; 2) perform embedding-layer and prediction-layer distillation at finetuning stage. System MNLI-m MNLI-mm MRPC CoLA Avg Uniform 82.8 82.9 85.8 50.8 75.6 Top 81.7 82.3 83.6 35.9 70.9 Bottom 80.6 81.3 84.6 38.5 71.3 Table 4: Results (dev) of different mapping strategies for TinyBERT4 . et al., 2019; Lan et al., 2020), weight sharing (Dehghani et al., 2019; Lan et al., 2020), knowledge distillation (Tang et al., 2019; Sanh et al., 2019; Turc et al., 2019; Sun et al., 2020; Liu et al., 2020; Wang et al., 2020), pruning (Cui et al., 2019; McCarley, 2019; F. et al., 2020; Elbayad et al., 2020; Gordon et al., 2020; Hou et al., 2020) or quantization (Shen et al., 2019; Zafrir et al., 2019). In this paper, our focus is on knowledge distillation. Knowledge Distillation for PLMs There have been some works trying to distill pre-trained language models (PLMs) into smaller models. BiLSTMSOFT (Tang et al., 2019) distills taskspecific knowledge from BERT into a singlelayer BiLSTM. BERT-PKD (Sun et al., 2019) extracts knowledges not only from the last layer of the teacher, but also from inte"
2020.findings-emnlp.372,P19-1580,0,0.030542,"(NLP). Pre-trained language models (PLMs), such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), T5 (Raffel et al., 2019) and ELECTRA (Clark et al., 2020), have achieved great success in many NLP tasks (e.g., the GLUE benchmark (Wang et al., 2018) and the challenging multi-hop reasoning task (Ding et al., 2019)). However, PLMs usually have a large number of parameters and take long inference time, which are difficult to be deployed on edge devices such as mobile phones. Recent studies (Kovaleva et al., 2019; Michel et al., 2019; Voita et al., 2019) demonstrate that there is redundancy in PLMs. Therefore, it is crucial and feasible to reduce the computational overhead and model storage of PLMs while retaining their performances. There have been many model compression techniques (Han et al., 2016) proposed to accelerate deep model inference and reduce model size while maintaining accuracy. The most commonly used techniques include quantization (Gong et al., 2014), weights pruning (Han et al., 2015), and knowledge distillation (KD) (Romero et al., 2014). In this paper, we focus on knowledge distillation, an idea originated from Hinton et a"
2020.findings-emnlp.372,W18-5446,0,0.355915,"a new paradigm for ∗ Authors contribute equally. This work is done when Xiaoqi Jiao is an intern at Huawei Noah’s Ark Lab. ‡ Corresponding authors. 1 The code and models are publicly available at https: //github.com/huawei-noah/Pretrained-Language-Model/tree/ master/TinyBERT † natural language processing (NLP). Pre-trained language models (PLMs), such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), T5 (Raffel et al., 2019) and ELECTRA (Clark et al., 2020), have achieved great success in many NLP tasks (e.g., the GLUE benchmark (Wang et al., 2018) and the challenging multi-hop reasoning task (Ding et al., 2019)). However, PLMs usually have a large number of parameters and take long inference time, which are difficult to be deployed on edge devices such as mobile phones. Recent studies (Kovaleva et al., 2019; Michel et al., 2019; Voita et al., 2019) demonstrate that there is redundancy in PLMs. Therefore, it is crucial and feasible to reduce the computational overhead and model storage of PLMs while retaining their performances. There have been many model compression techniques (Han et al., 2016) proposed to accelerate deep model infere"
2020.findings-emnlp.372,Q19-1040,0,0.199969,"task-specific knowledge. Although there is a significant reduction of model size, with the data augmentation and by performing the proposed Transformer distillation method at both the pre-training and fine-tuning stages, TinyBERT can achieve competitive performances in various NLP tasks. 4 Experiments In this section, we evaluate the effectiveness and efficiency of TinyBERT on a variety of tasks with different model settings. 4.1 Datasets We evaluate TinyBERT on the General Language Understanding Evaluation (GLUE) (Wang et al., 2018) benchmark, which consists of 2 singlesentence tasks: CoLA (Warstadt et al., 2019), SST2 (Socher et al., 2013), 3 sentence similarity tasks: MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017), QQP (Chen et al., 2018), and 4 natural language inference tasks: MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Bentivogli et al., 2009) and WNLI (Levesque et al., 2012). The metrics for these tasks can be found in the GLUE paper (Wang et al., 2018). 4.2 TinyBERT Settings We instantiate a tiny student model (the number of layers M =4, the hidden size d0 =312, the feedforward/filter size d0i =1200 and the head number h=12) that has a total of 14.5M parameters."
2020.findings-emnlp.372,N18-1101,0,0.0603804,"ng stages, TinyBERT can achieve competitive performances in various NLP tasks. 4 Experiments In this section, we evaluate the effectiveness and efficiency of TinyBERT on a variety of tasks with different model settings. 4.1 Datasets We evaluate TinyBERT on the General Language Understanding Evaluation (GLUE) (Wang et al., 2018) benchmark, which consists of 2 singlesentence tasks: CoLA (Warstadt et al., 2019), SST2 (Socher et al., 2013), 3 sentence similarity tasks: MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017), QQP (Chen et al., 2018), and 4 natural language inference tasks: MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Bentivogli et al., 2009) and WNLI (Levesque et al., 2012). The metrics for these tasks can be found in the GLUE paper (Wang et al., 2018). 4.2 TinyBERT Settings We instantiate a tiny student model (the number of layers M =4, the hidden size d0 =312, the feedforward/filter size d0i =1200 and the head number h=12) that has a total of 14.5M parameters. This model is referred to as TinyBERT4 . The original 3 A word is tokenized into multiple word-pieces by the tokenizer of BERT. 4167 System #Params #FLOPs Speedup MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE"
2020.findings-emnlp.372,2020.emnlp-main.633,0,0.0588441,"Missing"
2020.wmt-1.93,W19-5403,0,0.297956,"Missing"
2020.wmt-1.93,N13-1073,0,0.0196253,"kenize subwords and remove undesirable spaces between special characters and numbers, i.e., converting “23 - 25” into “2325”. random search for the best performing candidate on the validation data. λ1 logP (y|x) + λ2 logP (y) + λ3 logP (x|y) (1) Due to time constraints, we did not implement the reranking approach on the system I. 3.3 Data Processing 4 A data processing pipeline is applied to enhance the quality of training data: • Data cleaning is implemented to filter out noisy data. An important step is to handle misalignment in the parallel corpus. An alignment model trained by fast-align (Dyer et al., 2013) 6 is applied to this end (Lu et al., 2018). In addition, we remove bitexts with a source and target sentence length ratio exceeding a certain threshold (i.e., 2.5). A language detection tool 7 is used to filter out bitexts with abnormal language patterns, i.e., sentences with undesirable langid. Other noisy data, such as those with HTML tags and extra spaces, are removed. • Scripts from Moses (Koehn et al., 2007) are used to perform punctuation normalization and tokenization. SentencePiece (Kudo and Richardson, 2018) segments words into subwords. • We extract “in-domain” data which are close"
2020.wmt-1.93,P19-1286,0,0.0234493,"20/biomedical-translationtask.html 2 https://ufal.mff.cuni.cz/ufal medical corpus 3 https://github.com/biomedical-translationcorpora/corpora use batch sizes of 6,144 and 8,000 tokens respectively in the finetuning process. 3.1 In-domain Dictionary Bilingual dictionaries have been studied in the machine translation community for various purposes. The lexicons are used to enhance the translation quality for rare and unknown words in the parallel corpus (Zhang and Zong, 2016). Research works in domain adaptation for NMT showed that incorporating domain-specific dictionaries is a viable solution (Hu et al., 2019; Thompson et al., 2019; Peng et al., 2020). Inspired by these studies, we apply domain-specific dictionaries derived from SNOMED-CT, 4 which is a collection of multilingual clinical terminology, to finetune general domain NMT models to boost cross-domain coverage. The dictionaries are treated as bitexts attached to the end of training data. 3.2 Reranking Apart from adopting a data-driven approach mentioned above, we also apply a transfer learning approach by reusing the publicly available pre-trained NMT models provided at fairseq (Ott et al., 2019). 5 After finetuning the selected pre-traine"
2020.wmt-1.93,P07-2045,0,0.00803028,"f training data: • Data cleaning is implemented to filter out noisy data. An important step is to handle misalignment in the parallel corpus. An alignment model trained by fast-align (Dyer et al., 2013) 6 is applied to this end (Lu et al., 2018). In addition, we remove bitexts with a source and target sentence length ratio exceeding a certain threshold (i.e., 2.5). A language detection tool 7 is used to filter out bitexts with abnormal language patterns, i.e., sentences with undesirable langid. Other noisy data, such as those with HTML tags and extra spaces, are removed. • Scripts from Moses (Koehn et al., 2007) are used to perform punctuation normalization and tokenization. SentencePiece (Kudo and Richardson, 2018) segments words into subwords. • We extract “in-domain” data which are close to Medline from general domain data by using TFIDF-based similarities. Similar data augmentation approaches can be identified in Wang et al. (2017) and Peng et al. (2020). 6 7 https://github.com/clab/fast align https://github.com/aboSamoor/polyglot Experimental Results The systems are trained with OOD data and finetuned using IND data to produce the submitted results. We benchmarked the submissions using WMT19 tes"
2020.wmt-1.93,D18-2012,0,0.0232588,"andle misalignment in the parallel corpus. An alignment model trained by fast-align (Dyer et al., 2013) 6 is applied to this end (Lu et al., 2018). In addition, we remove bitexts with a source and target sentence length ratio exceeding a certain threshold (i.e., 2.5). A language detection tool 7 is used to filter out bitexts with abnormal language patterns, i.e., sentences with undesirable langid. Other noisy data, such as those with HTML tags and extra spaces, are removed. • Scripts from Moses (Koehn et al., 2007) are used to perform punctuation normalization and tokenization. SentencePiece (Kudo and Richardson, 2018) segments words into subwords. • We extract “in-domain” data which are close to Medline from general domain data by using TFIDF-based similarities. Similar data augmentation approaches can be identified in Wang et al. (2017) and Peng et al. (2020). 6 7 https://github.com/clab/fast align https://github.com/aboSamoor/polyglot Experimental Results The systems are trained with OOD data and finetuned using IND data to produce the submitted results. We benchmarked the submissions using WMT19 test data. The BLEU scores are calculated using the MTEVAL script from Moses (Koehn et al., 2007). Results ar"
2020.wmt-1.93,W18-6482,0,0.0120996,"between special characters and numbers, i.e., converting “23 - 25” into “2325”. random search for the best performing candidate on the validation data. λ1 logP (y|x) + λ2 logP (y) + λ3 logP (x|y) (1) Due to time constraints, we did not implement the reranking approach on the system I. 3.3 Data Processing 4 A data processing pipeline is applied to enhance the quality of training data: • Data cleaning is implemented to filter out noisy data. An important step is to handle misalignment in the parallel corpus. An alignment model trained by fast-align (Dyer et al., 2013) 6 is applied to this end (Lu et al., 2018). In addition, we remove bitexts with a source and target sentence length ratio exceeding a certain threshold (i.e., 2.5). A language detection tool 7 is used to filter out bitexts with abnormal language patterns, i.e., sentences with undesirable langid. Other noisy data, such as those with HTML tags and extra spaces, are removed. • Scripts from Moses (Koehn et al., 2007) are used to perform punctuation normalization and tokenization. SentencePiece (Kudo and Richardson, 2018) segments words into subwords. • We extract “in-domain” data which are close to Medline from general domain data by usin"
2020.wmt-1.93,W19-5333,0,0.410461,"i’s submissions to the WMT20 biomedical translation task. We implement two NMT systems to maximize the performances of the shared task. The system I is an in-house NMT system built upon the transformer-big architecture (Vaswani et al., 2017) and trained using general domain data. We explore means to enhance cross-domain coverage of an NMT model by finetuning the NMT model with in-domain bitexts. We also investigate the effects of domain dictionaries in this domain adaptation process. Reusing pre-trained models has been regarded as an efficient way of transfer learning. Pre-trained NMT models (Ng et al., 2019) are adopted in the system II to this end. All NMT systems are evaluated against the test set released in the WMT19 biomedical shared task. We submitted translated results for a total of ten language directions between English (EN) and other five languages including French (FR), German (DE), Italian (IT), Russian (RU) and Chinese (ZH). Four of the submissions achieve the best BLEU scores according to the official automatic evaluation results. Substantial increases in BLEU scores are recorded in translation directions of DE→EN (+3.9 BLEU), ZH→EN (+3.5 BLEU), and EN→DE (+2.8 BLEU) compared to ou"
2020.wmt-1.93,N19-4009,0,0.0151347,"n-specific dictionaries is a viable solution (Hu et al., 2019; Thompson et al., 2019; Peng et al., 2020). Inspired by these studies, we apply domain-specific dictionaries derived from SNOMED-CT, 4 which is a collection of multilingual clinical terminology, to finetune general domain NMT models to boost cross-domain coverage. The dictionaries are treated as bitexts attached to the end of training data. 3.2 Reranking Apart from adopting a data-driven approach mentioned above, we also apply a transfer learning approach by reusing the publicly available pre-trained NMT models provided at fairseq (Ott et al., 2019). 5 After finetuning the selected pre-trained NMT models on the in-domain data, we apply a noisy channel model reranking method (Ng et al., 2019). The weights λ in Equation 1 are learned with a 4 5 858 https://www.nlm.nih.gov/healthit/snomedct/index.html https://github.com/pytorch/fairseq System I EN→FR FR→EN EN→IT IT→EN EN→ZH ZH→EN baseline + ft BS, IND + ft IND, IND-Dict. + ft BS,IND-Dict.,IND-Aug. WMT19 Submission 38.98 41.66 42.41 38.31 38.44 38.24 30.85 31.04 - 35.73 35.93 - 36.22 35.90 37.09 34.37 35.66 32.16 WMT20 Submission 43.51 44.45 42.57 49.74 45.46 35.28 WMT20 Best Official 43.51"
2020.wmt-1.93,W19-5420,1,0.748116,"m II to this end. All NMT systems are evaluated against the test set released in the WMT19 biomedical shared task. We submitted translated results for a total of ten language directions between English (EN) and other five languages including French (FR), German (DE), Italian (IT), Russian (RU) and Chinese (ZH). Four of the submissions achieve the best BLEU scores according to the official automatic evaluation results. Substantial increases in BLEU scores are recorded in translation directions of DE→EN (+3.9 BLEU), ZH→EN (+3.5 BLEU), and EN→DE (+2.8 BLEU) compared to our submissions last year (Peng et al., 2019). The improvements on EN⇔DE can be ascribed to strong pre-trained NMT baseline models and a series of optimization techniques, for example, in-domain data augmentation and a reranking method with strong language models. High-quality in-domain data and large-scale back-translation contribute to the improvements of the ZH→EN model. 2 The Data Table 1 captures the number of sentences pairs used in this shared task. The system I is trained using in-house general domain data (OOD) and finetuned on the in-domain data (IND) provided by 857 Proceedings of the 5th Conference on Machine Translation (WMT"
2020.wmt-1.93,D19-1142,0,0.0210356,"nslationtask.html 2 https://ufal.mff.cuni.cz/ufal medical corpus 3 https://github.com/biomedical-translationcorpora/corpora use batch sizes of 6,144 and 8,000 tokens respectively in the finetuning process. 3.1 In-domain Dictionary Bilingual dictionaries have been studied in the machine translation community for various purposes. The lexicons are used to enhance the translation quality for rare and unknown words in the parallel corpus (Zhang and Zong, 2016). Research works in domain adaptation for NMT showed that incorporating domain-specific dictionaries is a viable solution (Hu et al., 2019; Thompson et al., 2019; Peng et al., 2020). Inspired by these studies, we apply domain-specific dictionaries derived from SNOMED-CT, 4 which is a collection of multilingual clinical terminology, to finetune general domain NMT models to boost cross-domain coverage. The dictionaries are treated as bitexts attached to the end of training data. 3.2 Reranking Apart from adopting a data-driven approach mentioned above, we also apply a transfer learning approach by reusing the publicly available pre-trained NMT models provided at fairseq (Ott et al., 2019). 5 After finetuning the selected pre-trained NMT models on the in-"
2020.wmt-1.93,tiedemann-2012-parallel,0,0.0620321,"- 59K 59K 59K 59K EN→DE DE→EN EN→RU RU→EN - 40K 40K 54K 54K - Table 1: Data used for training and finetuning systems I and II. Note that “IND-Dict.” refers to the in-domain dictionary. “IND-Aug.” is the augmented data derived from processing IND data. For the system I, “IND-Aug.” is created from back-translating monolingual data. For the system II, “IND-Aug.” is the pre-processed IND data in combination with the data selected from some OOD data based on the similarity to the Medline data. M is for “million,” and K stands for “thousand”. WMT20.1 The in-domain data consist of bitexts from EMEA (Tiedemann, 2012), UFAL,2 Pubmed, and Medline.3 The data is processed by methods in the next section. The test data for the system I are from the WMT19 shared task. The data used for finetuning the system II are different from those for the system I. The system II only focuses on Medline as we discovered it is the most effective IND data for this shared task. The development (dev.) set for the system II is the OK-aligned test data from the WMT19 biomedical shared task. A batch of monolingual Medline data in English dated before July 2018 has been extracted to provide a basis for data augmentation and noisy cha"
2020.wmt-1.93,P17-2089,0,0.0212426,"ing a certain threshold (i.e., 2.5). A language detection tool 7 is used to filter out bitexts with abnormal language patterns, i.e., sentences with undesirable langid. Other noisy data, such as those with HTML tags and extra spaces, are removed. • Scripts from Moses (Koehn et al., 2007) are used to perform punctuation normalization and tokenization. SentencePiece (Kudo and Richardson, 2018) segments words into subwords. • We extract “in-domain” data which are close to Medline from general domain data by using TFIDF-based similarities. Similar data augmentation approaches can be identified in Wang et al. (2017) and Peng et al. (2020). 6 7 https://github.com/clab/fast align https://github.com/aboSamoor/polyglot Experimental Results The systems are trained with OOD data and finetuned using IND data to produce the submitted results. We benchmarked the submissions using WMT19 test data. The BLEU scores are calculated using the MTEVAL script from Moses (Koehn et al., 2007). Results are shown in Table 2 and Table 4. The final two rows demonstrate the scores of our submissions on this year’s test sets and the best official records released by the organizers. 4.1 English ⇔ French The system I is our in-hous"
2021.acl-long.318,D13-1160,0,0.0567264,"ed stabilizing training by reducing the variance of rewards and setting a small λ. 4.2 Multi-Mention Reading Comprehension Multi-mention reading comprehension is a natural feature of many QA tasks. Given a document d and a question q, a task-specific model is required to locate the answer text a which is usually mentioned many times in the document(s). A solution is defined as a document span. The solution set Z is computed by finding exact match of a: Z = {z = (s, e)d |[ds , ..., de ] = a} We experimented on two open domain QA datasets, i.e., Quasar-T (Dhingra et al., 2017) and WebQuestions (Berant et al., 2013). For Quasar-T, we retrieved 50 reference sentences from ClueWeb09 for each question; for WebQuestions, we used the 2016-12-21 dump of Wikipedia as the knowledge source and retrieved 50 reference paragraphs for each question using a Lucene index system. We used the same BERTbase (Devlin et al., 2019) reading comprehension model and data preprocessing from (Min et al., 2019). Quasar-T Dev Test EM F1 EM F1 First Only 36.0 43.9 35.6 42.8 MML 40.1 47.4 39.1 46.5 HardEM 41.5 49.1 40.7 47.7 HardEM-thres 42.8 50.2 41.9 49.4 Ours 44.7‡ 52.6‡ 44.0‡ 51.5‡ WebQuestions Test EM F1 16.7 22.6 18.4 25.0 18.0"
2021.acl-long.318,P19-1007,0,0.0187808,"use model confidence to filter out spurious solutions in a soft or hard way. They do not explicitly exploit the semantic correlations between a question and its solution. Most relevantly, Cheng and Lapata (2018) focused on text2SQL tasks; they modeled SQL queries as the latent variables for question generation, and maximized the evidence lower bound of log likelihood of questions. A few works treated solution prediction and question generation as dual tasks and introduced dual learning losses to regularize learning under the fully-supervised or the semi-supervised setting (Tang et al., 2017; Cao et al., 2019; Ye et al., 2019). In dual learning, a model generates intermediate outputs (e.g., the task-specific model predicts solutions from a question) while the dual model gives feedback signals (e.g., the question reconstructor computes the likelihood of the question conditioned on predicted solutions). This method is featured in three aspects. First, both models need training on fully-annotated data so that they can produce reasonable intermediate outputs. Second, the intermediate outputs can 4112 introduce noise during learning as they are sampled from models but not restricted to solutions with c"
2021.acl-long.318,K18-1035,0,0.140759,"Talmor and Berant, 2019), training on all possible solutions with MML (Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019; Wang et al., 2019), reinforcement learning (Liang et al., 2017, 2018), and hard EM (Min et al., 2019; Chen et al., 2020). All these approaches either use heuristics to select possibly reasonable solutions, rely on model architectures to bias towards correct solutions, or use model confidence to filter out spurious solutions in a soft or hard way. They do not explicitly exploit the semantic correlations between a question and its solution. Most relevantly, Cheng and Lapata (2018) focused on text2SQL tasks; they modeled SQL queries as the latent variables for question generation, and maximized the evidence lower bound of log likelihood of questions. A few works treated solution prediction and question generation as dual tasks and introduced dual learning losses to regularize learning under the fully-supervised or the semi-supervised setting (Tang et al., 2017; Cao et al., 2019; Ye et al., 2019). In dual learning, a model generates intermediate outputs (e.g., the task-specific model predicts solutions from a question) while the dual model gives feedback signals (e.g., t"
2021.acl-long.318,P18-1078,0,0.129234,"mprehension, many mentions of an answer in the document(s) are irrelevant to the question; for discrete reasoning tasks or text2SQL tasks, an answer can be produced by the equations or SQL queries that do not correctly match the question in logic. Some previous works heuristically selected one possible solution per question for training, e.g., the first answer span in the document (Joshi et al., 2017; Tay et al., 2018; Talmor and Berant, 2019); some treated all possible solutions equally and maximized the sum of their likelihood (maximum marginal likelihood, or MML) (Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019); many others selected solutions according to model confidence (Liang et al., 2018; Min et al., 2019), 4111 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4111–4124 August 1–6, 2021. ©2021 Association for Computational Linguistics i.e., the likelihood of the solutions being derived by the model. A drawback of these methods is that they do not explicitly consider the mutual semantic correlations between a question and its solution when selecting solutions"
2021.acl-long.318,N19-1423,0,0.0115289,"h is usually mentioned many times in the document(s). A solution is defined as a document span. The solution set Z is computed by finding exact match of a: Z = {z = (s, e)d |[ds , ..., de ] = a} We experimented on two open domain QA datasets, i.e., Quasar-T (Dhingra et al., 2017) and WebQuestions (Berant et al., 2013). For Quasar-T, we retrieved 50 reference sentences from ClueWeb09 for each question; for WebQuestions, we used the 2016-12-21 dump of Wikipedia as the knowledge source and retrieved 50 reference paragraphs for each question using a Lucene index system. We used the same BERTbase (Devlin et al., 2019) reading comprehension model and data preprocessing from (Min et al., 2019). Quasar-T Dev Test EM F1 EM F1 First Only 36.0 43.9 35.6 42.8 MML 40.1 47.4 39.1 46.5 HardEM 41.5 49.1 40.7 47.7 HardEM-thres 42.8 50.2 41.9 49.4 Ours 44.7‡ 52.6‡ 44.0‡ 51.5‡ WebQuestions Test EM F1 16.7 22.6 18.4 25.0 18.0 24.2 19.0 25.3 20.4‡ 27.2‡ Table 2: Evaluation on multi-mention reading comprehension datasets. Numbers marked with ‡ are significantly better than the others (t-test, p-value < 0.05). Results: Our method outperforms all baselines on both datasets (Table 2). The improvements can be attributed to the"
2021.acl-long.318,N19-1246,0,0.0274635,"Missing"
2021.acl-long.318,P17-1147,0,0.431756,"h spurious solutions can hurt model performance (e.g., misleading the model to produce unreasonable solutions or wrong answers). As shown in Fig 1, ∗ *Corresponding author: Minlie Huang. for multi-mention reading comprehension, many mentions of an answer in the document(s) are irrelevant to the question; for discrete reasoning tasks or text2SQL tasks, an answer can be produced by the equations or SQL queries that do not correctly match the question in logic. Some previous works heuristically selected one possible solution per question for training, e.g., the first answer span in the document (Joshi et al., 2017; Tay et al., 2018; Talmor and Berant, 2019); some treated all possible solutions equally and maximized the sum of their likelihood (maximum marginal likelihood, or MML) (Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019); many others selected solutions according to model confidence (Liang et al., 2018; Min et al., 2019), 4111 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4111–4124 August 1–6, 2021. ©2021 Association for Computational Linguistics i.e., t"
2021.acl-long.318,Q18-1023,0,0.0547158,"Missing"
2021.acl-long.318,P19-1612,0,0.0959051,"s of an answer in the document(s) are irrelevant to the question; for discrete reasoning tasks or text2SQL tasks, an answer can be produced by the equations or SQL queries that do not correctly match the question in logic. Some previous works heuristically selected one possible solution per question for training, e.g., the first answer span in the document (Joshi et al., 2017; Tay et al., 2018; Talmor and Berant, 2019); some treated all possible solutions equally and maximized the sum of their likelihood (maximum marginal likelihood, or MML) (Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019); many others selected solutions according to model confidence (Liang et al., 2018; Min et al., 2019), 4111 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4111–4124 August 1–6, 2021. ©2021 Association for Computational Linguistics i.e., the likelihood of the solutions being derived by the model. A drawback of these methods is that they do not explicitly consider the mutual semantic correlations between a question and its solution when selecting solutions for training. Intu"
2021.acl-long.318,2020.acl-main.703,0,0.0233164,"Missing"
2021.acl-long.318,P17-1003,0,0.11412,"an answer) are not provided. This setting is worth exploration as it simplifies annotation and makes it easier to collect large-scale corpora. However, this setting introduces the spurious solution problem, and thus complicates model learning. Most existing approaches for this learning challenge include heuristically selecting one possible solution per question for training (Joshi et al., 2017; Tay et al., 2018; Talmor and Berant, 2019), training on all possible solutions with MML (Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019; Wang et al., 2019), reinforcement learning (Liang et al., 2017, 2018), and hard EM (Min et al., 2019; Chen et al., 2020). All these approaches either use heuristics to select possibly reasonable solutions, rely on model architectures to bias towards correct solutions, or use model confidence to filter out spurious solutions in a soft or hard way. They do not explicitly exploit the semantic correlations between a question and its solution. Most relevantly, Cheng and Lapata (2018) focused on text2SQL tasks; they modeled SQL queries as the latent variables for question generation, and maximized the evidence lower bound of log likelihood of questions. A few"
2021.acl-long.318,D17-1090,0,0.0608387,"Missing"
2021.acl-long.318,D19-1284,0,0.225883,"t2SQL tasks, an answer can be produced by the equations or SQL queries that do not correctly match the question in logic. Some previous works heuristically selected one possible solution per question for training, e.g., the first answer span in the document (Joshi et al., 2017; Tay et al., 2018; Talmor and Berant, 2019); some treated all possible solutions equally and maximized the sum of their likelihood (maximum marginal likelihood, or MML) (Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019); many others selected solutions according to model confidence (Liang et al., 2018; Min et al., 2019), 4111 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4111–4124 August 1–6, 2021. ©2021 Association for Computational Linguistics i.e., the likelihood of the solutions being derived by the model. A drawback of these methods is that they do not explicitly consider the mutual semantic correlations between a question and its solution when selecting solutions for training. Intuitively speaking, a question often contains vital clues about how to derive the answer, and a wrong s"
2021.acl-long.318,P15-1142,0,0.0142296,"task performance and is more effective in training models to produce correct solutions. 2 Related Work Question answering has raised prevalent attention and has achieved great progress these years. A lot of challenging datasets have been constructed to advance models’ reasoning abilities, such as (1) reading comprehension datasets with extractive answer spans (Joshi et al., 2017; Dhingra et al., 2017), with free-form answers (Kocisk´y et al., 2018), for multi-hop reasoning (Yang et al., 2018), or for discrete reasoning over paragraphs (Dua et al., 2019), and (2) datasets for semantic parsing (Pasupat and Liang, 2015; Zhong et al., 2017; Yu et al., 2018). Under the weakly supervised setting, the specific solutions to derive the final answers (e.g., the correct location of an answer text, or the correct logic executing an answer) are not provided. This setting is worth exploration as it simplifies annotation and makes it easier to collect large-scale corpora. However, this setting introduces the spurious solution problem, and thus complicates model learning. Most existing approaches for this learning challenge include heuristically selecting one possible solution per question for training (Joshi et al., 20"
2021.acl-long.318,D19-1391,0,0.294103,"answer text, or the correct logic executing an answer) are not provided. This setting is worth exploration as it simplifies annotation and makes it easier to collect large-scale corpora. However, this setting introduces the spurious solution problem, and thus complicates model learning. Most existing approaches for this learning challenge include heuristically selecting one possible solution per question for training (Joshi et al., 2017; Tay et al., 2018; Talmor and Berant, 2019), training on all possible solutions with MML (Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019; Wang et al., 2019), reinforcement learning (Liang et al., 2017, 2018), and hard EM (Min et al., 2019; Chen et al., 2020). All these approaches either use heuristics to select possibly reasonable solutions, rely on model architectures to bias towards correct solutions, or use model confidence to filter out spurious solutions in a soft or hard way. They do not explicitly exploit the semantic correlations between a question and its solution. Most relevantly, Cheng and Lapata (2018) focused on text2SQL tasks; they modeled SQL queries as the latent variables for question generation, and maximized the evidence lower"
2021.acl-long.318,D18-1259,0,0.0250719,"conducted extensive experiments on four QA datasets. Our approach significantly outperforms strong baselines in terms of task performance and is more effective in training models to produce correct solutions. 2 Related Work Question answering has raised prevalent attention and has achieved great progress these years. A lot of challenging datasets have been constructed to advance models’ reasoning abilities, such as (1) reading comprehension datasets with extractive answer spans (Joshi et al., 2017; Dhingra et al., 2017), with free-form answers (Kocisk´y et al., 2018), for multi-hop reasoning (Yang et al., 2018), or for discrete reasoning over paragraphs (Dua et al., 2019), and (2) datasets for semantic parsing (Pasupat and Liang, 2015; Zhong et al., 2017; Yu et al., 2018). Under the weakly supervised setting, the specific solutions to derive the final answers (e.g., the correct location of an answer text, or the correct logic executing an answer) are not provided. This setting is worth exploration as it simplifies annotation and makes it easier to collect large-scale corpora. However, this setting introduces the spurious solution problem, and thus complicates model learning. Most existing approaches"
2021.acl-long.318,P19-1201,0,0.0236122,"nce to filter out spurious solutions in a soft or hard way. They do not explicitly exploit the semantic correlations between a question and its solution. Most relevantly, Cheng and Lapata (2018) focused on text2SQL tasks; they modeled SQL queries as the latent variables for question generation, and maximized the evidence lower bound of log likelihood of questions. A few works treated solution prediction and question generation as dual tasks and introduced dual learning losses to regularize learning under the fully-supervised or the semi-supervised setting (Tang et al., 2017; Cao et al., 2019; Ye et al., 2019). In dual learning, a model generates intermediate outputs (e.g., the task-specific model predicts solutions from a question) while the dual model gives feedback signals (e.g., the question reconstructor computes the likelihood of the question conditioned on predicted solutions). This method is featured in three aspects. First, both models need training on fully-annotated data so that they can produce reasonable intermediate outputs. Second, the intermediate outputs can 4112 introduce noise during learning as they are sampled from models but not restricted to solutions with correct answer or v"
2021.acl-long.318,P19-1485,0,0.0535664,"rformance (e.g., misleading the model to produce unreasonable solutions or wrong answers). As shown in Fig 1, ∗ *Corresponding author: Minlie Huang. for multi-mention reading comprehension, many mentions of an answer in the document(s) are irrelevant to the question; for discrete reasoning tasks or text2SQL tasks, an answer can be produced by the equations or SQL queries that do not correctly match the question in logic. Some previous works heuristically selected one possible solution per question for training, e.g., the first answer span in the document (Joshi et al., 2017; Tay et al., 2018; Talmor and Berant, 2019); some treated all possible solutions equally and maximized the sum of their likelihood (maximum marginal likelihood, or MML) (Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019); many others selected solutions according to model confidence (Liang et al., 2018; Min et al., 2019), 4111 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4111–4124 August 1–6, 2021. ©2021 Association for Computational Linguistics i.e., the likelihood of the solutions being derived"
2021.acl-long.318,D18-1425,0,0.0180048,", 2019), training on all possible solutions with MML (Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019; Wang et al., 2019), reinforcement learning (Liang et al., 2017, 2018), and hard EM (Min et al., 2019; Chen et al., 2020). All these approaches either use heuristics to select possibly reasonable solutions, rely on model architectures to bias towards correct solutions, or use model confidence to filter out spurious solutions in a soft or hard way. They do not explicitly exploit the semantic correlations between a question and its solution. Most relevantly, Cheng and Lapata (2018) focused on text2SQL tasks; they modeled SQL queries as the latent variables for question generation, and maximized the evidence lower bound of log likelihood of questions. A few works treated solution prediction and question generation as dual tasks and introduced dual learning losses to regularize learning under the fully-supervised or the semi-supervised setting (Tang et al., 2017; Cao et al., 2019; Ye et al., 2019). In dual learning, a model generates intermediate outputs (e.g., the task-specific model predicts solutions from a question) while the dual model gives feedback signals (e.g., t"
2021.acl-long.400,2020.findings-emnlp.372,1,0.849122,"ual to the dimensions of query/key/value vector dq|k|v and a quarter of the intermediate size df in feed-forward networks. 3 We empirically find that being at least 4x faster is a basic requirement in practical deployment environment. 2 5146 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5146–5157 August 1–6, 2021. ©2021 Association for Computational Linguistics Although, there have been quite a few work using knowledge distillation to build small PLMs (Sanh et al., 2019; Jiao et al., 2020b; Sun et al., 2019, 2020), all of them focus on the application of distillation techniques (Hinton et al., 2015; Romero et al., 2014) and do not study the effect of architecture hyper-parameter settings on model performance. Recently, neural architecture search and hyper-parameter optimization (Tan and Le, 2019; Han et al., 2020) have been widely explored in machine learning, mostly in computer vision, and have been proven to find better designs than heuristic ones. Inspired by this research, one problem that naturally arises is can we find better settings of hyper-parameters4 for efficient P"
2021.acl-long.400,2020.acl-main.250,0,0.0373827,"Missing"
2021.acl-long.400,2021.ccl-1.108,0,0.0767838,"Missing"
2021.acl-long.400,N19-1423,0,0.501554,"ation for Efficient Pre-trained Language Models Yichun Yin1 , Cheng Chen2* , Lifeng Shang1 , Xin Jiang1 , Xiao Chen1 , Qun Liu1 1 Huawei Noah’s Ark Lab 2 Department of Computer Science and Technology, Tsinghua University {yinyichun,shang.lifeng,jiang.xin,chen.xiao2,qun.liu}@huawei.com c-chen19@mails.tsinghua.edu.cn 1 78 Pre-trained language models (PLMs) have achieved great success in natural language processing. Most of PLMs follow the default setting of architecture hyper-parameters (e.g., the hidden dimension is a quarter of the intermediate dimension in feed-forward sub-networks) in BERT (Devlin et al., 2019). Few studies have been conducted to explore the design of architecture hyper-parameters in BERT, especially for the more efficient PLMs with tiny sizes, which are essential for practical deployment on resource-constrained devices. In this paper, we adopt the one-shot Neural Architecture Search (NAS) to automatically search architecture hyper-parameters. Specifically, we carefully design the techniques of one-shot learning and the search space to provide an adaptive and efficient development way of tiny PLMs for various latency constraints. We name our method AutoTinyBERT1 and evaluate its eff"
2021.acl-long.400,D16-1264,0,0.00994447,"ed-forward network to fit the data. For more details of evolutionary algorithm, please refer to Appendix C. Note that we can use different methods in search process, such as random search and more advanced search, which is left as future work. 3.6 Further Training The search process produces top several architectures, with which we extract these corresponding sub-models from SuperPLM and continue training them using the pre-training or KD objectives. 4 4.1 Experiment Experimental Setup Dataset and Fine tuning. We conduct the experiments on the GLUE benchmark (Wang et al., 2018) and SQuADv1.1 (Rajpurkar et al., 2016). For GLUE, we set the batch size to 32, choose the learning rate from {1e-5, 2e-5, 3e-5} and choose the epoch number from {4, 5, 10}. For SQuADv1.1, we set the batch size to 16, the learning rate to 3e5 and the epoch number to 4. The details for all datasets are displayed in Appendix D. AutoTinyBERT. Both the one-shot and further 5150 Model Speedup SQuAD SST-2 MNLI MRPC¶ CoLA QNLI QQP¶ STS-B RTE Score Avg. Dev results on GLUE and dev result on SQuAD AutoTinyBERT-KD-S1 BERT-KD-S1 MobileBERTTiny ‡(Sun et al., 2020) 4.6× 4.9× 3.6*× 87.6 86.2 88.6 91.4 89.7 91.6 82.3 81.1 82.0 88.5 87.9 86.7 47.3"
2021.acl-long.400,2020.emnlp-demos.6,0,0.0713911,"Missing"
2021.acl-long.400,D19-1441,0,0.0356708,"Missing"
2021.acl-long.400,2020.acl-main.195,0,0.356841,"ng pre-training process and makes the search process efficient. To make SuperPLM more effective, we propose practical techniques including the head sub-matrix extraction and efficient batch-wise training, and particularly limit the search space to the models with identical layer structure. Furthermore, by using SuperPLM, we leverage search algorithm (Xie and Yuille, 2017; Wang et al., 2020a) to find hyperparameters for various latency constraints. In the experiments, in addition to the pre-training setting (Devlin et al., 2019), we also consider the setting of task-agnostic BERT distillation (Sun et al., 2020) that pre-trains with the loss of knowledge distillation, to build efficient PLMs. Exten4 We abbreviate the phrase architecture hyper-parameter as hyper-parameter in the paper. sive results show that in pre-training setting, AutoTinyBERT not only consistently outperforms the BERT with conventional hyper-parameters under different latency constraints, but also outperforms NAS-BERT based on neural architecture search. In task-agnostic BERT distillation, AutoTinyBERT outperforms a series of existing SOTA methods of DistilBERT, TinyBERT and MobileBERT. Our contributions are three-fold: (1) we expl"
2021.acl-long.400,D19-1374,0,0.0406938,"Missing"
2021.acl-long.400,2020.emnlp-main.37,1,0.871459,"Missing"
2021.acl-long.400,W18-5446,0,0.0196972,"target devices, and then uses a feed-forward network to fit the data. For more details of evolutionary algorithm, please refer to Appendix C. Note that we can use different methods in search process, such as random search and more advanced search, which is left as future work. 3.6 Further Training The search process produces top several architectures, with which we extract these corresponding sub-models from SuperPLM and continue training them using the pre-training or KD objectives. 4 4.1 Experiment Experimental Setup Dataset and Fine tuning. We conduct the experiments on the GLUE benchmark (Wang et al., 2018) and SQuADv1.1 (Rajpurkar et al., 2016). For GLUE, we set the batch size to 32, choose the learning rate from {1e-5, 2e-5, 3e-5} and choose the epoch number from {4, 5, 10}. For SQuADv1.1, we set the batch size to 16, the learning rate to 3e5 and the epoch number to 4. The details for all datasets are displayed in Appendix D. AutoTinyBERT. Both the one-shot and further 5150 Model Speedup SQuAD SST-2 MNLI MRPC¶ CoLA QNLI QQP¶ STS-B RTE Score Avg. Dev results on GLUE and dev result on SQuAD AutoTinyBERT-KD-S1 BERT-KD-S1 MobileBERTTiny ‡(Sun et al., 2020) 4.6× 4.9× 3.6*× 87.6 86.2 88.6 91.4 89.7"
2021.acl-long.400,2020.acl-main.686,0,0.503017,"ial sub-architectures. Proxy means that when evaluating an architecture, we only need to extract the corresponding sub-model from the SuperPLM, instead of training the model from scratch. SuperPLM helps avoid the time-consuming pre-training process and makes the search process efficient. To make SuperPLM more effective, we propose practical techniques including the head sub-matrix extraction and efficient batch-wise training, and particularly limit the search space to the models with identical layer structure. Furthermore, by using SuperPLM, we leverage search algorithm (Xie and Yuille, 2017; Wang et al., 2020a) to find hyperparameters for various latency constraints. In the experiments, in addition to the pre-training setting (Devlin et al., 2019), we also consider the setting of task-agnostic BERT distillation (Sun et al., 2020) that pre-trains with the loss of knowledge distillation, to build efficient PLMs. Exten4 We abbreviate the phrase architecture hyper-parameter as hyper-parameter in the paper. sive results show that in pre-training setting, AutoTinyBERT not only consistently outperforms the BERT with conventional hyper-parameters under different latency constraints, but also outperforms N"
2021.acl-long.469,2020.lrec-1.3,0,0.0973937,"Missing"
2021.acl-long.469,2020.findings-emnlp.81,0,0.046775,"Missing"
2021.acl-long.469,D19-1060,0,0.06048,"Missing"
2021.acl-long.469,2020.acl-main.130,0,0.014664,"unt of data via self-supervised learning, have made remarkable progress on both natural language understanding (NLU) (Wang et al., 2018, 2019) and natural language generation (NLG) (Liu and Lapata, 2019; Weng et al., 2020; Cao et al., 2020). On several NLU datasets, PLM-based neural models have gradually achieved human-level performance in terms of automatic evaluation metrics (e.g., accuracy, F1 ) (He et al., 2020; Zhang et al., 2021). In order to deeply understand and analyze the capability of PLMs on NLU, a variety of more challenging NLU datasets have been proposed (Warstadt et al., 2020; Cui et al., 2020a; Jain et al., 2020; Talmor et al., 2020). These datasets can be used not only to obtain knowledge on how PLM-based models work and what they learn, but also to define new NLU tasks and to serve as a benchmark for future progress. For example, evaluating and analyzing PLM-based models on learning document structures with a carefully created benchmark test suite (Chen et al., 2019), helps to develop new methods to enhance the capability of these models on discourse modeling (Iter et al., 2020). Knowing the weakness of current PLM-based models in commonsense reasoning (Zhou et al., 2020) has in"
2021.acl-long.469,2020.findings-emnlp.58,0,0.0282809,"unt of data via self-supervised learning, have made remarkable progress on both natural language understanding (NLU) (Wang et al., 2018, 2019) and natural language generation (NLG) (Liu and Lapata, 2019; Weng et al., 2020; Cao et al., 2020). On several NLU datasets, PLM-based neural models have gradually achieved human-level performance in terms of automatic evaluation metrics (e.g., accuracy, F1 ) (He et al., 2020; Zhang et al., 2021). In order to deeply understand and analyze the capability of PLMs on NLU, a variety of more challenging NLU datasets have been proposed (Warstadt et al., 2020; Cui et al., 2020a; Jain et al., 2020; Talmor et al., 2020). These datasets can be used not only to obtain knowledge on how PLM-based models work and what they learn, but also to define new NLU tasks and to serve as a benchmark for future progress. For example, evaluating and analyzing PLM-based models on learning document structures with a carefully created benchmark test suite (Chen et al., 2019), helps to develop new methods to enhance the capability of these models on discourse modeling (Iter et al., 2020). Knowing the weakness of current PLM-based models in commonsense reasoning (Zhou et al., 2020) has in"
2021.acl-long.469,2020.emnlp-main.680,0,0.495208,"the best of our knowledge, is the first dataset built on machine-generated texts from state-of-the-art pretrained language models with rich annotations. The key interest of this dataset is detecting and annotating text generation errors from PLMs. Therefore it is different from conventional text generation datasets (e.g., Multi-News (Fabbri et al., 2019), TextCaps (Sidorov et al., 2020)) that are constructed to train models to learn text generation (e.g., generating texts from images or long documents). It is also different from grammatical error correction (GEC) datasets (Zhao et al., 2018; Flachs et al., 2020) that are built from human-written texts usually by second language learners. • TGEA provides rich semantic information for text generation errors, including error types, associated text spans, error corrections and rationals behind errors, as shown in Figure 1. Marking text spans that are closely related to erroneous words allows us to detect longdistance dependencies of errors or reasoning chains related to errors. Rationales behind errors directly explain why errors are annotated. All these error-centered manual annotations not only increase the interpretability of our dataset, but also fac"
2021.acl-long.469,2020.tacl-1.48,0,0.0340173,"ng, have made remarkable progress on both natural language understanding (NLU) (Wang et al., 2018, 2019) and natural language generation (NLG) (Liu and Lapata, 2019; Weng et al., 2020; Cao et al., 2020). On several NLU datasets, PLM-based neural models have gradually achieved human-level performance in terms of automatic evaluation metrics (e.g., accuracy, F1 ) (He et al., 2020; Zhang et al., 2021). In order to deeply understand and analyze the capability of PLMs on NLU, a variety of more challenging NLU datasets have been proposed (Warstadt et al., 2020; Cui et al., 2020a; Jain et al., 2020; Talmor et al., 2020). These datasets can be used not only to obtain knowledge on how PLM-based models work and what they learn, but also to define new NLU tasks and to serve as a benchmark for future progress. For example, evaluating and analyzing PLM-based models on learning document structures with a carefully created benchmark test suite (Chen et al., 2019), helps to develop new methods to enhance the capability of these models on discourse modeling (Iter et al., 2020). Knowing the weakness of current PLM-based models in commonsense reasoning (Zhou et al., 2020) has inspired people to develop various reasoning"
2021.acl-long.469,N19-1421,0,0.0296774,"ogrande, a larger version of WSC, is introduced by Sakaguchi et al. (2020), which contains ∼ 44, 000 examples. Winowhy (Zhang et al., 2020a) asks annotators to provide reasons for their decisions to WSC. In this aspect, the differences of our dataset from Winowhy are twofold. First, we provide reasons for errors rather than correct decisions to anaphora. Second, we provide reasons for all text generation errors, rather than only errors related to commonsense reasoning. In addition to COPA and WSC-style datasets, many large crowdsourced datasets have been also proposed recently. CommonsenseQA (Talmor et al., 2019), a commonsense question answering dataset, has been constructed from ConceptNet. HellaSwag (Zellers et al., 2019b) and Abductive NLI (Bhagavatula et al., 2020) evaluate commonsense reasoning in the form of natural language inference. CosmosQA (Huang et al., 2019) is a dataset with multi-choice questions that require commonsense reading comprehension. Beyond datasets for evaluating commonsense reasoning, there are other datasets providing commonsense knowledge. PIQA (Bisk et al., 2020) focuses on physical commonsense knowledge while SocialIQA (Sap et al., 2019) on social commonsense knowledge."
2021.acl-long.469,W18-5446,0,0.0823812,"Missing"
2021.acl-long.469,2020.aacl-main.20,0,0.0748908,"Missing"
2021.acl-long.469,P11-1019,0,0.0425583,"ble Inference Plausible Inference Reason Explanation Multiple tasks Commonsense Reasoning 6 6 6 6 6 6 3 3 3 3 3 3 3 Machine-Generated Texts 6 6 6 6 6 6 6 Rationales 6 6 6 6 6 6 6 6 6 6 6 6 3 3 6 6 6 6 3 Domain #Sentences Language Essay Journal articles TOFEL Exam Web doc/Essay Web doc Essay Open WikiHow articles Social situations Narratives Physical situations ROCStories Open Open 34K 1.2M 1,511 8K 13K 0.71M 273 70K 38K 35K 21K 200K 2,865 47K EN EN EN EN EN ZH EN EN EN EN EN EN EN ZH Table 1: Comparison between our dataset and other datasets. 2.1 Grammatical Error Correction Datasets 2.2 FCE (Yannakoudakis et al., 2011) is an early largescale English grammatical error correction dataset, where raw texts are produced by English learners taking the First Certificate in English exams. AESW (Daudaravicius et al., 2016) is a GEC dataset from a professional editing company. In addition to common grammatical errors, AESW covers style issues as it contains texts mainly from scholarly papers. JFLEG (Napoles et al., 2017) is a GEC dataset built from TOFEL Exams, which does not force annotators to make minimal edits, preferring holistic fluency rewrites. CMEG (Napoles et al., 2019) is different from general grammatical"
2021.acl-long.469,P19-1472,0,0.0217165,"nowhy (Zhang et al., 2020a) asks annotators to provide reasons for their decisions to WSC. In this aspect, the differences of our dataset from Winowhy are twofold. First, we provide reasons for errors rather than correct decisions to anaphora. Second, we provide reasons for all text generation errors, rather than only errors related to commonsense reasoning. In addition to COPA and WSC-style datasets, many large crowdsourced datasets have been also proposed recently. CommonsenseQA (Talmor et al., 2019), a commonsense question answering dataset, has been constructed from ConceptNet. HellaSwag (Zellers et al., 2019b) and Abductive NLI (Bhagavatula et al., 2020) evaluate commonsense reasoning in the form of natural language inference. CosmosQA (Huang et al., 2019) is a dataset with multi-choice questions that require commonsense reading comprehension. Beyond datasets for evaluating commonsense reasoning, there are other datasets providing commonsense knowledge. PIQA (Bisk et al., 2020) focuses on physical commonsense knowledge while SocialIQA (Sap et al., 2019) on social commonsense knowledge. Commonsense datasets in multiple languages or languages other than English have also been created recently. XCOP"
2021.acl-long.469,2020.acl-main.508,0,0.109801,"ot only to obtain knowledge on how PLM-based models work and what they learn, but also to define new NLU tasks and to serve as a benchmark for future progress. For example, evaluating and analyzing PLM-based models on learning document structures with a carefully created benchmark test suite (Chen et al., 2019), helps to develop new methods to enhance the capability of these models on discourse modeling (Iter et al., 2020). Knowing the weakness of current PLM-based models in commonsense reasoning (Zhou et al., 2020) has inspired people to develop various reasoning datasets (Cui et al., 2020a; Zhang et al., 2020b). On the other hand, state-of-the-art PLMs are able to generate texts that are even not distinguishable from human-written texts by human evaluators (Radford et al., 2019; Brown et al., 2020). This makes us curious about the capability of PLMs on text generation. Are they really reaching humanlevel performance on text generation? In contrast to the studies of PLMs on NLU, research on the 6012 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6012–6025 August 1–6, 2021. ©202"
2021.acl-long.469,2020.emnlp-main.374,0,0.0913543,"Missing"
2021.acl-long.509,2020.findings-emnlp.372,1,0.908416,"y of our proposed method. 1 ? ?= ?? ?= ? ?? ?= ? ?? ?= ?= ?= ? ?= ?? ?= ? ?= ?? ? ?? ?? ?? ? ?= ?? ? ?? ?= ?? ?? ?= ? ?? ?= ? ?? ? ?? ?= ? ?= ?? ?= ? ?= ?? ?= ?= ? ?? ?? ?? ?= ?? ?? ? ?? ? ?? Figure 1: Average GLUE development accuracy versus #params and FLOPs with the (pruned) BERT and our GhostBERT. m is the width multiplier of the model. Introduction Recently, there is a surge of research interests in compressing the transformer-based pre-trained language models like BERT into smaller ones using various compression methods, i.e., knowledge distillation (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), pruning (Michel et al., 2019; Fan et al., 2019), low-rank approximation (Lan et al., 2020), weight-sharing (Lan et al., 2020), dynamic networks with adaptive depth and/or width (Liu et al., 2020; Hou et al., 2020; Xin et al., 2020; Zhou et al., 2020), and quantization (Shen et al., 2020; Fan et al., 2020; Zhang et al., 2020; Bai et al., 2021). Previous works show that there are some redundant features in the BERT model, and unimportant attention heads or neurons can be pruned away without severe accuracy degradation (Michel et al., 2019; Hou et al., 2020). However, for computer vision (CV) t"
2021.acl-long.509,D19-1445,0,0.0862502,"0; Bai et al., 2021). Previous works show that there are some redundant features in the BERT model, and unimportant attention heads or neurons can be pruned away without severe accuracy degradation (Michel et al., 2019; Hou et al., 2020). However, for computer vision (CV) tasks, it is shown in (Han et al., 2020) that redundant features in convolutional neural networks also contribute positively to the performance, and using cheap linear operations to generate more ghost feature maps enhances the performance with few additional parameters. On the other hand, it is shown in (Voita et al., 2019; Kovaleva et al., 2019; Rogers et al., 2020) that many attention maps in pre-trained language models exhibit typical positional patterns, e.g., diagonal or vertical, which can be easily generated from other similar ones using operations like convolution. Based on the above two aspects, in this paper, we propose to use cheap ghost modules on top of the remaining important attention heads and neurons to generate more features, so as to compensate for the pruned ones. Considering that the convolution operation (1) encodes local context dependency, as a complement of the global self-attention in Transformer models (Wu"
2021.acl-long.509,2020.acl-main.537,0,0.0669879,"1: Average GLUE development accuracy versus #params and FLOPs with the (pruned) BERT and our GhostBERT. m is the width multiplier of the model. Introduction Recently, there is a surge of research interests in compressing the transformer-based pre-trained language models like BERT into smaller ones using various compression methods, i.e., knowledge distillation (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), pruning (Michel et al., 2019; Fan et al., 2019), low-rank approximation (Lan et al., 2020), weight-sharing (Lan et al., 2020), dynamic networks with adaptive depth and/or width (Liu et al., 2020; Hou et al., 2020; Xin et al., 2020; Zhou et al., 2020), and quantization (Shen et al., 2020; Fan et al., 2020; Zhang et al., 2020; Bai et al., 2021). Previous works show that there are some redundant features in the BERT model, and unimportant attention heads or neurons can be pruned away without severe accuracy degradation (Michel et al., 2019; Hou et al., 2020). However, for computer vision (CV) tasks, it is shown in (Han et al., 2020) that redundant features in convolutional neural networks also contribute positively to the performance, and using cheap linear operations to generate more g"
2021.acl-long.509,2020.emnlp-main.617,0,0.0517181,"Missing"
2021.acl-long.509,2020.tacl-1.54,0,0.0835865,"revious works show that there are some redundant features in the BERT model, and unimportant attention heads or neurons can be pruned away without severe accuracy degradation (Michel et al., 2019; Hou et al., 2020). However, for computer vision (CV) tasks, it is shown in (Han et al., 2020) that redundant features in convolutional neural networks also contribute positively to the performance, and using cheap linear operations to generate more ghost feature maps enhances the performance with few additional parameters. On the other hand, it is shown in (Voita et al., 2019; Kovaleva et al., 2019; Rogers et al., 2020) that many attention maps in pre-trained language models exhibit typical positional patterns, e.g., diagonal or vertical, which can be easily generated from other similar ones using operations like convolution. Based on the above two aspects, in this paper, we propose to use cheap ghost modules on top of the remaining important attention heads and neurons to generate more features, so as to compensate for the pruned ones. Considering that the convolution operation (1) encodes local context dependency, as a complement of the global self-attention in Transformer models (Wu et al., 2020); and (2)"
2021.acl-long.509,D19-1441,0,0.0266262,"verify the efficacy of our proposed method. 1 ? ?= ?? ?= ? ?? ?= ? ?? ?= ?= ?= ? ?= ?? ?= ? ?= ?? ? ?? ?? ?? ? ?= ?? ? ?? ?= ?? ?? ?= ? ?? ?= ? ?? ? ?? ?= ? ?= ?? ?= ? ?= ?? ?= ?= ? ?? ?? ?? ?= ?? ?? ? ?? ? ?? Figure 1: Average GLUE development accuracy versus #params and FLOPs with the (pruned) BERT and our GhostBERT. m is the width multiplier of the model. Introduction Recently, there is a surge of research interests in compressing the transformer-based pre-trained language models like BERT into smaller ones using various compression methods, i.e., knowledge distillation (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), pruning (Michel et al., 2019; Fan et al., 2019), low-rank approximation (Lan et al., 2020), weight-sharing (Lan et al., 2020), dynamic networks with adaptive depth and/or width (Liu et al., 2020; Hou et al., 2020; Xin et al., 2020; Zhou et al., 2020), and quantization (Shen et al., 2020; Fan et al., 2020; Zhang et al., 2020; Bai et al., 2021). Previous works show that there are some redundant features in the BERT model, and unimportant attention heads or neurons can be pruned away without severe accuracy degradation (Michel et al., 2019; Hou et al., 2020). However, for co"
2021.acl-long.509,2020.acl-main.195,0,0.0569002,"best development set performance is used for testing. For each method, we also report the number of parameters 6515 Model FLOPs(G) #params(M) MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B Avg. BERT-base (Devlin et al., 2019) RoBERTa-base (Liu et al., 2019) ELECTRA-small (Clark et al., 2020) 22.5 22.5 1.7 110 125 14 84.6 90.5 89.2 66.4 93.5 86.0 92.5 88.7 73.0 94.6 79.7 87.7 88.0 60.8 89.1 84.8 86.5 83.7 52.1 50.5 54.6 85.8 80.9 88.1 82.5 80.3 78.0 TinyBERT6 (Jiao et al., 2020) TinyBERT4 (Jiao et al., 2020) ConvBERT-medium (Jiang et al., 2020) ConvBERT-small (Jiang et al., 2020) MobileBERT w/o OPT (Sun et al., 2020) MobileBERT (Sun et al., 2020) MobileBERT-tiny (Sun et al., 2020) 11.3 1.2 4.7 2.0 5.7 5.7 3.1 67 15 17 14 25 25 15 84.6 82.5 82.1 81.5 84.3 83.3 81.5 90.4 87.7 88.7 88.5 91.6 90.6 89.5 89.1 89.2 88.4 88.0 88.3 - 70.0 66.6 65.3 62.2 70.4 66.2 65.1 93.1 92.6 89.2 89.2 92.6 92.8 91.7 87.3 86.4 84.6 83.3 84.5 - 51.1 44.1 56.4 54.8 51.1 50.5 46.7 83.7 80.4 82.9 83.4 84.8 84.4 80.1 81.2 78.7 79.7 78.9 81.0 - GhostBERT (m = 12/12) GhostBERT (m = 9/12) GhostBERT (m = 6/12) GhostBERT (m = 3/12) GhostBERT (m = 1/12) GhostRoBERTa (m = 12/12) GhostRoBERTa (m = 9/12) GhostRoBERTa (m = 6/12) GhostRoBERTa ("
2021.acl-long.509,P19-1580,0,0.146835,"0; Zhang et al., 2020; Bai et al., 2021). Previous works show that there are some redundant features in the BERT model, and unimportant attention heads or neurons can be pruned away without severe accuracy degradation (Michel et al., 2019; Hou et al., 2020). However, for computer vision (CV) tasks, it is shown in (Han et al., 2020) that redundant features in convolutional neural networks also contribute positively to the performance, and using cheap linear operations to generate more ghost feature maps enhances the performance with few additional parameters. On the other hand, it is shown in (Voita et al., 2019; Kovaleva et al., 2019; Rogers et al., 2020) that many attention maps in pre-trained language models exhibit typical positional patterns, e.g., diagonal or vertical, which can be easily generated from other similar ones using operations like convolution. Based on the above two aspects, in this paper, we propose to use cheap ghost modules on top of the remaining important attention heads and neurons to generate more features, so as to compensate for the pruned ones. Considering that the convolution operation (1) encodes local context dependency, as a complement of the global self-attention in"
2021.acl-long.509,2020.acl-main.204,0,0.0662721,"y versus #params and FLOPs with the (pruned) BERT and our GhostBERT. m is the width multiplier of the model. Introduction Recently, there is a surge of research interests in compressing the transformer-based pre-trained language models like BERT into smaller ones using various compression methods, i.e., knowledge distillation (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), pruning (Michel et al., 2019; Fan et al., 2019), low-rank approximation (Lan et al., 2020), weight-sharing (Lan et al., 2020), dynamic networks with adaptive depth and/or width (Liu et al., 2020; Hou et al., 2020; Xin et al., 2020; Zhou et al., 2020), and quantization (Shen et al., 2020; Fan et al., 2020; Zhang et al., 2020; Bai et al., 2021). Previous works show that there are some redundant features in the BERT model, and unimportant attention heads or neurons can be pruned away without severe accuracy degradation (Michel et al., 2019; Hou et al., 2020). However, for computer vision (CV) tasks, it is shown in (Han et al., 2020) that redundant features in convolutional neural networks also contribute positively to the performance, and using cheap linear operations to generate more ghost feature maps enhances the perfo"
2021.acl-long.509,2020.emnlp-main.37,1,0.817647,"r of the model. Introduction Recently, there is a surge of research interests in compressing the transformer-based pre-trained language models like BERT into smaller ones using various compression methods, i.e., knowledge distillation (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), pruning (Michel et al., 2019; Fan et al., 2019), low-rank approximation (Lan et al., 2020), weight-sharing (Lan et al., 2020), dynamic networks with adaptive depth and/or width (Liu et al., 2020; Hou et al., 2020; Xin et al., 2020; Zhou et al., 2020), and quantization (Shen et al., 2020; Fan et al., 2020; Zhang et al., 2020; Bai et al., 2021). Previous works show that there are some redundant features in the BERT model, and unimportant attention heads or neurons can be pruned away without severe accuracy degradation (Michel et al., 2019; Hou et al., 2020). However, for computer vision (CV) tasks, it is shown in (Han et al., 2020) that redundant features in convolutional neural networks also contribute positively to the performance, and using cheap linear operations to generate more ghost feature maps enhances the performance with few additional parameters. On the other hand, it is shown in (Voita et al., 2019; K"
2021.emnlp-main.211,C18-1139,0,0.0593857,"Missing"
2021.emnlp-main.211,W06-0116,0,0.0951731,"Missing"
2021.emnlp-main.211,Q16-1026,0,0.0640081,"Missing"
2021.emnlp-main.211,N19-1423,0,0.0365311,"ut matrices, respectively. Then self-attention can be formalized as: SelfAtt(X) = Att(XWQ , XWK , XWV ), (2) where WQ , WK , WV are parameter matrices to be learned. 2.2 The LexKg Extractor Matching Conventional methods normally learn additional word embeddings of lexicons to incorporate lexicon knowledge, thus it is required to retrain the entire model once the lexicons are updated. Our method is independent of the lexicon size and lexicon word content by designing a wordagnostic representation. Specifically, the Matching 2.1 BERT as Encoder module takes a word sequence as input, then uses a Devlin et al. (2019) introduces a new language rep- prefix tree-based fast matching algorithm (see algoresentation model called BERT, which has become rithm 1) to quickly retrieve the lexicons, and finally 2680 In this section, we will present how to incorporate large-scale lexicons into BERT. As illustrated in Figure 2, the proposed DyLex framework contains two parts, namely the BERT-based sequence tagger and Lexicon Knowledge extractor. The Lexicon Knowledge (LexKg) extractor has three submodules: Matching, Denoising and Fusing. Fusion O Bn In Bs Is O O Tagger Bn: B-name In: I-name SelfAttn Layer √ √ O Bn In O"
2021.emnlp-main.211,P19-1141,1,0.924095,"a Smart Speaker or “just a little while longer” is a famous song. In commercial systems, the lexicon is widely used as an effective way to store various domain knowledge. In practice, the size of a lexicon can range from ten to a few million, and we usually need to update the contents of lexicons frequently, which dramatically increases the difficulty of incorporating lexicons into deep models. In this work, we will study how to effectively incorporate large-scale dynamic lexicons into BERT-based sequence labeling models. Recent works on incorporating lexicon knowledge (Zhang and Yang, 2018; Ding et al., 2019; Mu et al., 2020; Li et al., 2020) can be summarized as follows. First, they match an input sentence with several lexicons to obtain all matched items. Second, leveraging the matched item information through modifying the structure of the transformer layer or the feature representation layer. However, 2679 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2679–2693 c November 7–11, 2021. 2021 Association for Computational Linguistics 1) current methods normally learn additional embeddings of the words in the lexicons, which bring us a challenge - if"
2021.emnlp-main.211,2020.coling-main.186,0,0.0799142,"Missing"
2021.emnlp-main.211,2020.coling-main.310,0,0.208503,"xploited. To make full use of word information, incorporating a lexicon is an effective method. Existing works on incorporating lexicon can be categorized as feature based, lattice based and graph based methods according to implementation complexity. Feature based Feature based method is a simpler way. Some works directly use lexical information with simple matching features and the others use auxiliary tasks to leverage the lexical information. Zhang et al. (2018) builds the template first and uses the template matching lexicon to build features, which help word segmentation tasks. Mu et al. (2020) uses a simple lexicon matching location information as features. Li et al. (2014) and Peters et al. (2017) adopt word-level language modeling objective and multi-task to use word information implicitly. Yang et al. (2017b) transfer crossdomain and cross-lingual knowledge via multi-task learning. Lattice based Lattice based method is to use With the advance of deep learning, sequence la- lattice structure. Zhang and Yang (2018) probelling tasks, such as segmentation and NER, have poses Lattice-LSTM for incorporating word lexachieved excellent performance. More and more icons into the character"
2021.emnlp-main.211,W06-0115,0,0.0604468,"Missing"
2021.emnlp-main.211,li-etal-2014-comparison,0,0.0182086,"ctive method. Existing works on incorporating lexicon can be categorized as feature based, lattice based and graph based methods according to implementation complexity. Feature based Feature based method is a simpler way. Some works directly use lexical information with simple matching features and the others use auxiliary tasks to leverage the lexical information. Zhang et al. (2018) builds the template first and uses the template matching lexicon to build features, which help word segmentation tasks. Mu et al. (2020) uses a simple lexicon matching location information as features. Li et al. (2014) and Peters et al. (2017) adopt word-level language modeling objective and multi-task to use word information implicitly. Yang et al. (2017b) transfer crossdomain and cross-lingual knowledge via multi-task learning. Lattice based Lattice based method is to use With the advance of deep learning, sequence la- lattice structure. Zhang and Yang (2018) probelling tasks, such as segmentation and NER, have poses Lattice-LSTM for incorporating word lexachieved excellent performance. More and more icons into the character-based NER model. Rather methods tend to be character-based(Chen et al., than heur"
2021.emnlp-main.211,2020.acl-main.611,0,0.169622,"hile longer” is a famous song. In commercial systems, the lexicon is widely used as an effective way to store various domain knowledge. In practice, the size of a lexicon can range from ten to a few million, and we usually need to update the contents of lexicons frequently, which dramatically increases the difficulty of incorporating lexicons into deep models. In this work, we will study how to effectively incorporate large-scale dynamic lexicons into BERT-based sequence labeling models. Recent works on incorporating lexicon knowledge (Zhang and Yang, 2018; Ding et al., 2019; Mu et al., 2020; Li et al., 2020) can be summarized as follows. First, they match an input sentence with several lexicons to obtain all matched items. Second, leveraging the matched item information through modifying the structure of the transformer layer or the feature representation layer. However, 2679 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2679–2693 c November 7–11, 2021. 2021 Association for Computational Linguistics 1) current methods normally learn additional embeddings of the words in the lexicons, which bring us a challenge - if the lexicons get updated, the mode"
2021.emnlp-main.211,P19-1524,0,0.131361,"espectively. Then self-attention can be formalized as: SelfAtt(X) = Att(XWQ , XWK , XWV ), (2) where WQ , WK , WV are parameter matrices to be learned. 2.2 The LexKg Extractor Matching Conventional methods normally learn additional word embeddings of lexicons to incorporate lexicon knowledge, thus it is required to retrain the entire model once the lexicons are updated. Our method is independent of the lexicon size and lexicon word content by designing a wordagnostic representation. Specifically, the Matching 2.1 BERT as Encoder module takes a word sequence as input, then uses a Devlin et al. (2019) introduces a new language rep- prefix tree-based fast matching algorithm (see algoresentation model called BERT, which has become rithm 1) to quickly retrieve the lexicons, and finally 2680 In this section, we will present how to incorporate large-scale lexicons into BERT. As illustrated in Figure 2, the proposed DyLex framework contains two parts, namely the BERT-based sequence tagger and Lexicon Knowledge extractor. The Lexicon Knowledge (LexKg) extractor has three submodules: Matching, Denoising and Fusing. Fusion O Bn In Bs Is O O Tagger Bn: B-name In: I-name SelfAttn Layer √ √ O Bn In O"
2021.emnlp-main.211,2020.acl-main.528,0,0.0657101,"Missing"
2021.emnlp-main.211,D15-1064,0,0.0777877,"Missing"
2021.emnlp-main.211,P17-1161,0,0.0230088,"d. Existing works on incorporating lexicon can be categorized as feature based, lattice based and graph based methods according to implementation complexity. Feature based Feature based method is a simpler way. Some works directly use lexical information with simple matching features and the others use auxiliary tasks to leverage the lexical information. Zhang et al. (2018) builds the template first and uses the template matching lexicon to build features, which help word segmentation tasks. Mu et al. (2020) uses a simple lexicon matching location information as features. Li et al. (2014) and Peters et al. (2017) adopt word-level language modeling objective and multi-task to use word information implicitly. Yang et al. (2017b) transfer crossdomain and cross-lingual knowledge via multi-task learning. Lattice based Lattice based method is to use With the advance of deep learning, sequence la- lattice structure. Zhang and Yang (2018) probelling tasks, such as segmentation and NER, have poses Lattice-LSTM for incorporating word lexachieved excellent performance. More and more icons into the character-based NER model. Rather methods tend to be character-based(Chen et al., than heuristically choosing a word"
2021.emnlp-main.211,W03-0419,0,0.198878,"Missing"
2021.emnlp-main.211,D17-1283,0,0.0222779,"Missing"
2021.emnlp-main.211,D19-1396,0,0.0138856,". Since the lattice structure is complex and dynamic, most existing lattice-based models cannot fully utilize GPUs’ parallel computation and usually have a low inference-speed. Li et al. (2020) propose a Transformer-based model for Chinese NER, which converts the lattice structure into a flat structure. Graph based Graph based method uses a directed graph structure to fuse lexiconal information. Gui et al. (2019b) uses a GNN-based method to explore multiple graph-based interactions among characters, potential words, and the whole-sentence semantics to effectively alleviate the word ambiguity. Sui et al. (2019) employ a collaborative graph network to assign both self-matched and the nearest contextual lexical terms. To automatically learn how to incorporate multiple gazetteers into a NER system, Ding et al. (2019) propose a novel approach based on graph neural networks with a multidigraph structure. The structure captures the information the gazetteers offer. 6 Conclusion and Future Work In this paper, we propose DyLex, a framework incorporating dynamic lexicon to improve BERT-like models’ performance in sequence labeling tasks. To alleviate the problems caused by large-scale dynamic lexicons, we in"
2021.emnlp-main.211,2020.emnlp-main.523,0,0.0719851,"Missing"
2021.emnlp-main.211,P17-1078,0,0.16528,"(Akbik et al., 2018) ✗ ✗ ✗ ✗ ✗ 91.03 91.33 91.62 92.40 92.72 86.28 88.43 86.28 89.13 89.71 88.65 89.88 88.95 90.76 91.40 SENNA (Collobert et al., 2011) JERL (Luo et al., 2015) ID-CNN (Strubell et al., 2017) GRN (Chen et al., 2019a) HSCRF (Liu et al., 2019a) LUKE (Yamada et al., 2020) ✓ ✓ ✓ ✓ ✓ ✓ 89.56 91.20 90.54 91.44 92.75 94.30 86.84 87.67 89.94 - 88.69 89.55 91.34 - DyLex ✓ 94.30 90.19 92.25 Table 3: F1 scores of different methods on English NER dataset. The setting is the same with Table2.Note that LUKE incorporate the entity information during the pre-training phase. Model LEX PKU CITYU Yang et al. (2017a) Ma et al. (2018) Huang et al. (2020a) BERT (Devlin et al., 2019) Glyce (Meng et al., 2019) ✗ ✗ ✗ ✗ ✓ 96.30 96.10 96.60 96.50 96.70 96.94 97.23 97.60 97.60 97.90 DyLex ✓ 97.14 98.60 Table 4: F1 Score on PKU and CITYU datasets. 3.3 Task1: Chinese Word Segmentation CWS aims to divide a sentence into meaningful chunks. It is a primary task for Chinese text processing. Using lexicons in CWS tasks is a commonly used operation. Brand new words and internet buzzwords emerge every day, and it is essential to add these words into lexicons for better performance. In this work, we experiment on two pop"
2021.emnlp-main.211,P18-1144,0,0.208786,"n Man” is the alias of a Smart Speaker or “just a little while longer” is a famous song. In commercial systems, the lexicon is widely used as an effective way to store various domain knowledge. In practice, the size of a lexicon can range from ten to a few million, and we usually need to update the contents of lexicons frequently, which dramatically increases the difficulty of incorporating lexicons into deep models. In this work, we will study how to effectively incorporate large-scale dynamic lexicons into BERT-based sequence labeling models. Recent works on incorporating lexicon knowledge (Zhang and Yang, 2018; Ding et al., 2019; Mu et al., 2020; Li et al., 2020) can be summarized as follows. First, they match an input sentence with several lexicons to obtain all matched items. Second, leveraging the matched item information through modifying the structure of the transformer layer or the feature representation layer. However, 2679 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2679–2693 c November 7–11, 2021. 2021 Association for Computational Linguistics 1) current methods normally learn additional embeddings of the words in the lexicons, which bring"
2021.emnlp-main.256,W14-4012,0,0.194004,"Missing"
2021.emnlp-main.256,2020.tacl-1.29,0,0.0924721,"Missing"
2021.emnlp-main.256,E14-1035,0,0.011227,"ber 7–11, 2021. 2021 Association for Computational Linguistics baselines. The effect of the proposed method is also verified on the English → French translation task. heterogeneous ways of explicitly integrating topic information into NMT, resulting in better performance. 2 3 Related Work Topic-enhanced Neural Machine Translation Many studies have focused on using topic inforFigure 1 illustrates the proposed topic-enhanced mation as explicit prior knowledge to help model NMT model with topic EN Cpre , EN Cpost , and learn sentence representations on NLP tasks, such as Zhang et al. (2017); Kim (2014); Kobus et al. DEC, built upon the Transformer architecture. (2017). Topic modeling has shown its effective- The topic knowledge in the figure is obtained from ness in statistical machine translation (SMT) mod- the topic embedding tables for source and target languages produced by ETM. els (Xiao et al., 2012; Xiong et al., 2015; Hasler et al., 2014). Incorporating topic information into 3.1 Pre-encoder Topic Embedding NMT has recently been explored by Chen et al. In the encoding phase, we convert the sequence (2016); Zhang et al. (2016); Wei et al. (2019); Chen et al. (2019). Zhang et al. (201"
2021.emnlp-main.256,D14-1181,0,0.0055507,"ovember 7–11, 2021. 2021 Association for Computational Linguistics baselines. The effect of the proposed method is also verified on the English → French translation task. heterogeneous ways of explicitly integrating topic information into NMT, resulting in better performance. 2 3 Related Work Topic-enhanced Neural Machine Translation Many studies have focused on using topic inforFigure 1 illustrates the proposed topic-enhanced mation as explicit prior knowledge to help model NMT model with topic EN Cpre , EN Cpost , and learn sentence representations on NLP tasks, such as Zhang et al. (2017); Kim (2014); Kobus et al. DEC, built upon the Transformer architecture. (2017). Topic modeling has shown its effective- The topic knowledge in the figure is obtained from ness in statistical machine translation (SMT) mod- the topic embedding tables for source and target languages produced by ETM. els (Xiao et al., 2012; Xiong et al., 2015; Hasler et al., 2014). Incorporating topic information into 3.1 Pre-encoder Topic Embedding NMT has recently been explored by Chen et al. In the encoding phase, we convert the sequence (2016); Zhang et al. (2016); Wei et al. (2019); Chen et al. (2019). Zhang et al. (201"
2021.emnlp-main.256,kobus-etal-2017-domain,0,0.0401118,"Missing"
2021.emnlp-main.256,P07-2045,0,0.0129589,".94 (+0.62) 42.89 (+0.57) 43.15 (+0.83) 43.35 (+1.03) Table 2: Case-sensitive BLEU (Papineni et al., 2002) scores evaluated on EN → DE translation task for topic NMT on newstest 2014, 2016, 2017, and 2019 and on EN → FR translation task on newstest 2014 with different settings. The numbers in parentheses represent the improvements of BLEU scores over the baseline BLEU score. tion of newstest 2012 and newstest 2013 is used for the development set. The training corpus contains 4.5M sentence pairs for DE, and 35.7M sentence pairs for FR. We use the truecasing model (Lita et al., 2003) and Moses (Koehn et al., (2007) to tokenize all the data. Besides, we use both source and target vocabularies with 32K most frequent words for DE and 44K words for FR. Training Details We preprocess the corpus for all experiments of ETM. We set the number of the topics to 50 and epoch number to 500, which are empirical values adopted from ETM. After preprocessing, we further remove one-word documents from the validation and test sets. For all NMT experiments, we train our models on one machine with 4 NVIDIA V100 GPUs and follow Vaswani et al. (2017) base model to set the hyper-parameters with model configurations. The numbe"
2021.emnlp-main.256,2020.acl-main.322,0,0.0233373,"into NMT The topic embedding table is pre-calculated as the intermediate product of ETM, and it is fixed durto improve translation performance. Both works were built upon gated recurrent units (GRU) archi- ing the NMT training process. Then we add up all the topic embedding in the sequence to produce tecture with limited coverage to the Transformer the topic information distribution of the whole senarchitecture. tence topics , added to each word embedding of Both studies adopted LDA to model topics of the input source words. Finally, we take the added source and target languages. Dieng et al. (2020) word embedding representation ei as the input empointed out that LDA is not an effective learner for bedding and feed it into the encoder with positional data with an extensive vocabulary because one has encoding results. to remove the most and least frequent words to fit good topic models. This pruning practice limits m X topics = ti (1) the scope of LDA models. The embedding topic i=1 model (ETM) (Dieng et al., 2020) was proposed to model each term as an embedding and each topic as ei = xi + topics (2) a point in that embedding space. The per-topic conditional probability of a term has a lo"
2021.emnlp-main.256,P03-1020,0,0.0688025,"42.60 (+0.28) 42.80 (+0.48) 42.94 (+0.62) 42.89 (+0.57) 43.15 (+0.83) 43.35 (+1.03) Table 2: Case-sensitive BLEU (Papineni et al., 2002) scores evaluated on EN → DE translation task for topic NMT on newstest 2014, 2016, 2017, and 2019 and on EN → FR translation task on newstest 2014 with different settings. The numbers in parentheses represent the improvements of BLEU scores over the baseline BLEU score. tion of newstest 2012 and newstest 2013 is used for the development set. The training corpus contains 4.5M sentence pairs for DE, and 35.7M sentence pairs for FR. We use the truecasing model (Lita et al., 2003) and Moses (Koehn et al., (2007) to tokenize all the data. Besides, we use both source and target vocabularies with 32K most frequent words for DE and 44K words for FR. Training Details We preprocess the corpus for all experiments of ETM. We set the number of the topics to 50 and epoch number to 500, which are empirical values adopted from ETM. After preprocessing, we further remove one-word documents from the validation and test sets. For all NMT experiments, we train our models on one machine with 4 NVIDIA V100 GPUs and follow Vaswani et al. (2017) base model to set the hyper-parameters with"
2021.emnlp-main.256,N19-4009,0,0.0328752,"topicj−2 + e(yj−1 ) (5) ding table and use the WMT14 German monolingual dataset to train the German embedding table. MODEL BLEU To verify the effect of the proposed method on the Transformer (base) (Vaswani et al., 2017) 27.3 English → French (EN → FR) task, we sample Transformer (big) (Vaswani et al., 2017) 28.4 Evolved Transformer (So et al., 2019) 28.4 only 10 million (M) sentences representing less DPE-NMT (Li et al., 2020) 27.61 than one third of the training data randomly from Transformer base + PR (Xu et al., 2020) 28.67 WMT14 French monolingual dataset to train the Fairseq (baseline) (Ott et al., 2019) 27.44 BLT-NMT (Wei et al., 2019) 27.93 French embedding table. The experiments are conLTR-NMT (Chen et al., 2019) 28.18 ducted on the standard WMT 14 English → GerTopic-enhanced NMT (ours) 29.01 man (EN → DE) and EN → FR training corpus as Table 1: Evaluation of the WMT14 EN → DE translaprevious work (Wu et al., 2016). We evaluate the tion using case-sensitive BLEU scores. models on the newstest 2014, while the concatena3199 MODEL Fairseq EN Cpre EN Cpost DEC EN Cpre + DEC EN Cpre + EN Cpost DEC + EN Cpost EN Cpre + DEC + EN Cpost newstest 2014 27.44 28.72 (+1.28) 28.96 (+1.52) 28.59 (+1.15)"
2021.emnlp-main.256,P02-1040,0,0.109164,"Missing"
2021.emnlp-main.256,P12-1079,1,0.642014,"hanced Neural Machine Translation Many studies have focused on using topic inforFigure 1 illustrates the proposed topic-enhanced mation as explicit prior knowledge to help model NMT model with topic EN Cpre , EN Cpost , and learn sentence representations on NLP tasks, such as Zhang et al. (2017); Kim (2014); Kobus et al. DEC, built upon the Transformer architecture. (2017). Topic modeling has shown its effective- The topic knowledge in the figure is obtained from ness in statistical machine translation (SMT) mod- the topic embedding tables for source and target languages produced by ETM. els (Xiao et al., 2012; Xiong et al., 2015; Hasler et al., 2014). Incorporating topic information into 3.1 Pre-encoder Topic Embedding NMT has recently been explored by Chen et al. In the encoding phase, we convert the sequence (2016); Zhang et al. (2016); Wei et al. (2019); Chen et al. (2019). Zhang et al. (2016) proposed a topic- of words into a sequence of word embedding xi and a sequence of topic embedding ti , as shown informed NMT model leveraging source-side and target-side topics, separately learned by two inde- in Figure 2(a). The word embedding is obtained pendent LDA models from training data. Wei et al."
2021.emnlp-main.256,2020.acl-main.37,0,0.0998683,"Missing"
2021.emnlp-main.256,P17-1139,0,0.0288183,", pages 3197–3202 c November 7–11, 2021. 2021 Association for Computational Linguistics baselines. The effect of the proposed method is also verified on the English → French translation task. heterogeneous ways of explicitly integrating topic information into NMT, resulting in better performance. 2 3 Related Work Topic-enhanced Neural Machine Translation Many studies have focused on using topic inforFigure 1 illustrates the proposed topic-enhanced mation as explicit prior knowledge to help model NMT model with topic EN Cpre , EN Cpost , and learn sentence representations on NLP tasks, such as Zhang et al. (2017); Kim (2014); Kobus et al. DEC, built upon the Transformer architecture. (2017). Topic modeling has shown its effective- The topic knowledge in the figure is obtained from ness in statistical machine translation (SMT) mod- the topic embedding tables for source and target languages produced by ETM. els (Xiao et al., 2012; Xiong et al., 2015; Hasler et al., 2014). Incorporating topic information into 3.1 Pre-encoder Topic Embedding NMT has recently been explored by Chen et al. In the encoding phase, we convert the sequence (2016); Zhang et al. (2016); Wei et al. (2019); Chen et al. (2019). Zhang"
2021.emnlp-main.256,C16-1170,1,0.739983,"ous ways of embedding topic information at the sentence level into an NMT model to improve translation performance. Specifically, the topic information can be incorporated as pre-encoder topic embedding, post-encoder topic embedding, and decoder topic embedding to increase the likelihood of selecting target words from the same topic of the source sentence. Experimental results show that NMT models with the proposed topic knowledge embedding outperform the baselines on the English → German and English → French translation tasks. 1 1 Incorporating topic information into NMT has been explored in Zhang et al. (2016) and Wei et al. (2019) with both studies adapting Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to model topics of source and target languages. Both works utilized the traditional encoder-decoder architecture with gated recurrent units (GRU) (Cho et al., 2014). Although Wei et al. (2019) showed that topic knowledge incorporation is also applicable to the Transformer architecture (Vaswani et al., 2017), it is argued that the joint learning of topic modeling and NMT is not an ideal way. The training of topic models can leverage a large volume of easily accessible monolingual data. Once a"
2021.emnlp-main.267,J09-1002,0,0.0616764,"ntroduction In recent years, neural approaches (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) have significantly improved the quality of machine translation (MT). Despite their apparent success, neural machine translation (NMT) systems still inevitably generate erroneous translations in real-world scenarios (Bentivogli et al., 2016; Castilho et al., 2017), especially for low-resource language pairs. Therefore, the evaluation of translation quality plays an important role in some applications of MT. For example, in computer-assisted translation (CAT) (Barrachina et al., 2009), the evaluation of translation quality can significantly reduce human efforts for postediting (Specia, 2011). ∗ Corresponding author Code can be found at https://github.com/ THUNLP-MT/SelfSupervisedQE. 1 Quality estimation (QE) of MT aims to evaluate the quality of the outputs of an MT system without references. Training QE models often requires massive parallel data, which are composed of authentic source sentences and machine-translated target sentences with quality annotations produced by manual evaluation or human post-editing (Moura et al., 2020; Hu et al., 2020; Ranasinghe et al., 2020)"
2021.emnlp-main.267,D16-1025,0,0.0268183,"ence- and word-level QE, which performs quality estimation by recovering the masked target words. Experimental results show that our method outperforms previous unsupervised methods on several QE tasks in different language pairs and domains.1 1 Introduction In recent years, neural approaches (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) have significantly improved the quality of machine translation (MT). Despite their apparent success, neural machine translation (NMT) systems still inevitably generate erroneous translations in real-world scenarios (Bentivogli et al., 2016; Castilho et al., 2017), especially for low-resource language pairs. Therefore, the evaluation of translation quality plays an important role in some applications of MT. For example, in computer-assisted translation (CAT) (Barrachina et al., 2009), the evaluation of translation quality can significantly reduce human efforts for postediting (Specia, 2011). ∗ Corresponding author Code can be found at https://github.com/ THUNLP-MT/SelfSupervisedQE. 1 Quality estimation (QE) of MT aims to evaluate the quality of the outputs of an MT system without references. Training QE models often requires mas"
2021.emnlp-main.267,2021.acl-long.369,1,0.705842,"process. Moreover, our work utilizes different techniques like WWM (Cui et al., 2019) and MC Dropout (Gal and Ghahramani, 2016) to further improve the performance. 5.2 Masked Language Models Recently, pretrained masked language models (MLMs) (Devlin et al., 2019) have been widely used in various NLP tasks including natural language understanding (Wang et al., 2019) and machine reading comprehension (Xu et al., 2019). The idea of MLM is also used in other complex NLP tasks. For example, Ghazvininejad et al. (2019) introduce a conditional masked language model (CMLM) for non-autoregressive NMT. Chen et al. (2021) and Zhang and van Genabith (2021) present MLM objectives to improve neural word alignment models. MLM objectives are also used in the training process of supervised QE (Kim et al., 2019; Rubino and Sumita, 2020; Cui et al., 2021). To 3329 the best of our knowledge, our work is the first to utilize MLM objectives for QE under unsupervised settings. Our work is also similar to translation language modeling (TLM) (Conneau and Lample, 2019). However, TLM is a multilingual pretraining schema designed for fine-tuning on various multilingual downstream tasks, while our work finetunes a multilingual"
2021.emnlp-main.267,2020.acl-main.747,0,0.03445,"ty Estimation for Machine Translation QE aims to evaluate the quality of machinetranslated sentences without references, which has been studied mainly under supervised settings. Specia et al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language models. Zhang et al. (2020) utilize lexical similarities based on word vectors. Zhou et al. (2020) propose an enhanced version of Zhang et al. (2020), which also utilizes explicit cross-lingual patter"
2021.emnlp-main.267,D19-1633,0,0.0211716,"ctly utilizing the conditional probabilities given by the model and does not require any further fine-tuning process. Moreover, our work utilizes different techniques like WWM (Cui et al., 2019) and MC Dropout (Gal and Ghahramani, 2016) to further improve the performance. 5.2 Masked Language Models Recently, pretrained masked language models (MLMs) (Devlin et al., 2019) have been widely used in various NLP tasks including natural language understanding (Wang et al., 2019) and machine reading comprehension (Xu et al., 2019). The idea of MLM is also used in other complex NLP tasks. For example, Ghazvininejad et al. (2019) introduce a conditional masked language model (CMLM) for non-autoregressive NMT. Chen et al. (2021) and Zhang and van Genabith (2021) present MLM objectives to improve neural word alignment models. MLM objectives are also used in the training process of supervised QE (Kim et al., 2019; Rubino and Sumita, 2020; Cui et al., 2021). To 3329 the best of our knowledge, our work is the first to utilize MLM objectives for QE under unsupervised settings. Our work is also similar to translation language modeling (TLM) (Conneau and Lample, 2019). However, TLM is a multilingual pretraining schema designe"
2021.emnlp-main.267,C18-1266,0,0.0214412,"t MC Dropout. 5 Related Work Our work is closely related to two lines of research: (1) quality estimation for machine translation, and (2) masked language models. 5.1 Quality Estimation for Machine Translation QE aims to evaluate the quality of machinetranslated sentences without references, which has been studied mainly under supervised settings. Specia et al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language models. Zhang et al. (2"
2021.emnlp-main.267,W19-5406,0,0.131787,"1 0.233 0.263 0.265 SyntheticQE-MLM 0.386 0.368 0.318 0.309 0.204 0.284 0.181 0.208 Ours 0.504 0.463 0.381 0.383 0.242 0.435 0.318 0.338 Results of Ensemble Unsupervised Models SyntheticQE-MT Ensemble 0.488 0.428 0.360 0.339 0.212 0.246 0.274 0.297 SyntheticQE-MLM Ensemble 0.407 0.379 0.318 0.307 0.210 0.299 0.185 0.216 0.508 0.460 0.373 0.362 0.247 0.317 0.262 0.286 SyntheticQE-MT+MLM Ours Ensemble 0.518 0.462 0.395 0.385 0.248 0.453 0.318 0.359 Method Table 3: Comparison with SyntheticQE (Tuan et al., 2021) on the WMT 2019 sentence- and word-level development and test sets. “*”: we followed Kepler et al. (2019) and implemented the supervised models by fine-tuning the multilingual BERT (Devlin et al., 2019). For the implementation details of the supervised models, please refer to Appendix A.4. Dataset SMT NMT Method SyntheticQE-MT SyntheticQE-MLM Ours SyntheticQE-MT SyntheticQE-MLM Ours Sent 0.469 0.416 0.560 0.526 0.424 0.590 Word 0.417 0.298 0.425 0.444 0.320 0.476 Table 4: Comparison with SyntheticQE (Tuan et al., 2021) on the WMT 2018 En-Lv test sets. En-Lv SMT NMT uMQE 0.385 0.550 0.176 0.221 BERTScore BERTScore++ 0.213 0.155 0.540 0.580 NMT-QE Ours 0.560 0.590 Method En-De En-Ru NMT NMT 0.375 0"
2021.emnlp-main.267,W17-4763,0,0.0200064,"counterpart without MC Dropout. 5 Related Work Our work is closely related to two lines of research: (1) quality estimation for machine translation, and (2) masked language models. 5.1 Quality Estimation for Machine Translation QE aims to evaluate the quality of machinetranslated sentences without references, which has been studied mainly under supervised settings. Specia et al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language model"
2021.emnlp-main.267,W19-5407,0,0.103634,"o evaluate the quality of machinetranslated sentences without references, which has been studied mainly under supervised settings. Specia et al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language models. Zhang et al. (2020) utilize lexical similarities based on word vectors. Zhou et al. (2020) propose an enhanced version of Zhang et al. (2020), which also utilizes explicit cross-lingual patterns obtained from word alignments and multi"
2021.emnlp-main.267,2020.wmt-1.122,0,0.151216,"Barrachina et al., 2009), the evaluation of translation quality can significantly reduce human efforts for postediting (Specia, 2011). ∗ Corresponding author Code can be found at https://github.com/ THUNLP-MT/SelfSupervisedQE. 1 Quality estimation (QE) of MT aims to evaluate the quality of the outputs of an MT system without references. Training QE models often requires massive parallel data, which are composed of authentic source sentences and machine-translated target sentences with quality annotations produced by manual evaluation or human post-editing (Moura et al., 2020; Hu et al., 2020; Ranasinghe et al., 2020). As obtaining such annotated data is time-consuming and labor-intensive in practice, unsupervised QE has received increasing attention (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020; Tuan et al., 2021). Most of the aforementioned methods use various features to conduct unsupervised QE (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020). These methods are simple and effective but limited to sentence-level tasks. Compared with sentencelevel QE, word-level QE can provide more finegrain"
2021.emnlp-main.267,2020.coling-main.385,0,0.10135,"been studied mainly under supervised settings. Specia et al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language models. Zhang et al. (2020) utilize lexical similarities based on word vectors. Zhou et al. (2020) propose an enhanced version of Zhang et al. (2020), which also utilizes explicit cross-lingual patterns obtained from word alignments and multilingual MLMs. Fomicheva et al. (2020) use different features extracted from NMT models. Tua"
2021.emnlp-main.267,P16-1162,0,0.110972,"Missing"
2021.emnlp-main.267,2011.eamt-1.12,0,0.0344057,"ni et al., 2017) have significantly improved the quality of machine translation (MT). Despite their apparent success, neural machine translation (NMT) systems still inevitably generate erroneous translations in real-world scenarios (Bentivogli et al., 2016; Castilho et al., 2017), especially for low-resource language pairs. Therefore, the evaluation of translation quality plays an important role in some applications of MT. For example, in computer-assisted translation (CAT) (Barrachina et al., 2009), the evaluation of translation quality can significantly reduce human efforts for postediting (Specia, 2011). ∗ Corresponding author Code can be found at https://github.com/ THUNLP-MT/SelfSupervisedQE. 1 Quality estimation (QE) of MT aims to evaluate the quality of the outputs of an MT system without references. Training QE models often requires massive parallel data, which are composed of authentic source sentences and machine-translated target sentences with quality annotations produced by manual evaluation or human post-editing (Moura et al., 2020; Hu et al., 2020; Ranasinghe et al., 2020). As obtaining such annotated data is time-consuming and labor-intensive in practice, unsupervised QE has rec"
2021.emnlp-main.267,P13-4014,0,0.0716555,"Missing"
2021.emnlp-main.267,P07-2045,0,0.00803237,"16 IT domain translation task, the WMT 2017 QE task, and the WMT 2018 APE task, as well as the Openoffice and KDE4 corpora available in OPUS3 (Tiedemann, 2012). For En-Ru, we used the in-domain parallel data collected by OPUS, including ada83, GNOME, KDE4, OpenOffice, PHP and Ubuntu. To further validate our method’s performance in different domains, we also conducted experiments on the WMT 2018 En-Lv QE task, which is in the biomedical domain. We used the EMEA corpus (which is also available in OPUS) as training data. Sentences were tokenized and truecased using the scripts provided by Moses (Koehn et al., 2007). We also deduplicated the sentences in the training datasets. Table 2 shows the statistics of these datasets. 2 Although some of the training data have quality annotations, we did not use these annotations in the experiments. 3 https://opus.nlpl.eu/ 3325 Year Language Pair Domain 2018 En-Lv Biomedical 2019 En-De En-Ru IT IT System SMT NMT NMT NMT Train 313K 365K 217K Dev 1.00K 1.00K 1.00K 1.00K Test 1.32K 1.45K 1.02K 1.02K Table 2: Statistics of the training, development and test datasets in our experiments. Baselines Implementation Details We mainly compared our method with SyntheticQE (Tuan"
2021.emnlp-main.267,2020.wmt-1.118,0,0.0365847,"t al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language models. Zhang et al. (2020) utilize lexical similarities based on word vectors. Zhou et al. (2020) propose an enhanced version of Zhang et al. (2020), which also utilizes explicit cross-lingual patterns obtained from word alignments and multilingual MLMs. Fomicheva et al. (2020) use different features extracted from NMT models. Tuan et al. (2021) train unsupervised QE mode"
2021.emnlp-main.267,D15-1166,0,0.0610192,"lop unsupervised QE methods. However, very few of them can be applied to both sentence- and word-level QE tasks, and they may suffer from noises in the synthetic data. To reduce the negative impact of noises, we propose a self-supervised method for both sentence- and word-level QE, which performs quality estimation by recovering the masked target words. Experimental results show that our method outperforms previous unsupervised methods on several QE tasks in different language pairs and domains.1 1 Introduction In recent years, neural approaches (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) have significantly improved the quality of machine translation (MT). Despite their apparent success, neural machine translation (NMT) systems still inevitably generate erroneous translations in real-world scenarios (Bentivogli et al., 2016; Castilho et al., 2017), especially for low-resource language pairs. Therefore, the evaluation of translation quality plays an important role in some applications of MT. For example, in computer-assisted translation (CAT) (Barrachina et al., 2009), the evaluation of translation quality can significantly reduce human efforts for posted"
2021.emnlp-main.267,2020.amta-research.11,1,0.822326,"Missing"
2021.emnlp-main.267,2020.wmt-1.119,0,0.0456977,"Missing"
2021.emnlp-main.267,tiedemann-2012-parallel,0,0.0352985,"nts Setup Data and Preprocessing We mainly conducted experiments on the WMT 2019 QE tasks, which consist of tasks in two different language pairs (En-De and En-Ru). Both tasks are in the IT domain. Since our experiments were conducted in an unsupervised setting, we used parallel corpora without quality annotations as training data2 . Specifically, for En-De, we used indomain parallel data from various sources, including the training data from the WMT 2016 IT domain translation task, the WMT 2017 QE task, and the WMT 2018 APE task, as well as the Openoffice and KDE4 corpora available in OPUS3 (Tiedemann, 2012). For En-Ru, we used the in-domain parallel data collected by OPUS, including ada83, GNOME, KDE4, OpenOffice, PHP and Ubuntu. To further validate our method’s performance in different domains, we also conducted experiments on the WMT 2018 En-Lv QE task, which is in the biomedical domain. We used the EMEA corpus (which is also available in OPUS) as training data. Sentences were tokenized and truecased using the scripts provided by Moses (Koehn et al., 2007). We also deduplicated the sentences in the training datasets. Table 2 shows the statistics of these datasets. 2 Although some of the traini"
2021.emnlp-main.267,2021.eacl-main.50,0,0.253522,"evaluate the quality of the outputs of an MT system without references. Training QE models often requires massive parallel data, which are composed of authentic source sentences and machine-translated target sentences with quality annotations produced by manual evaluation or human post-editing (Moura et al., 2020; Hu et al., 2020; Ranasinghe et al., 2020). As obtaining such annotated data is time-consuming and labor-intensive in practice, unsupervised QE has received increasing attention (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020; Tuan et al., 2021). Most of the aforementioned methods use various features to conduct unsupervised QE (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020). These methods are simple and effective but limited to sentence-level tasks. Compared with sentencelevel QE, word-level QE can provide more finegrained quality information (Fan et al., 2019), and thus it can better assist post-editing in CAT when combined with sentence-level QE. Recently, Tuan et al. (2021) use synthetic data to train unsupervised QE models, which can be applied for both sentence- and word"
2021.emnlp-main.267,2020.wmt-1.125,0,0.573067,"QE. 1 Quality estimation (QE) of MT aims to evaluate the quality of the outputs of an MT system without references. Training QE models often requires massive parallel data, which are composed of authentic source sentences and machine-translated target sentences with quality annotations produced by manual evaluation or human post-editing (Moura et al., 2020; Hu et al., 2020; Ranasinghe et al., 2020). As obtaining such annotated data is time-consuming and labor-intensive in practice, unsupervised QE has received increasing attention (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020; Tuan et al., 2021). Most of the aforementioned methods use various features to conduct unsupervised QE (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020). These methods are simple and effective but limited to sentence-level tasks. Compared with sentencelevel QE, word-level QE can provide more finegrained quality information (Fan et al., 2019), and thus it can better assist post-editing in CAT when combined with sentence-level QE. Recently, Tuan et al. (2021) use synthetic data to train unsupervised QE models, whic"
2021.emnlp-main.267,2020.emnlp-demos.6,0,0.0767023,"Missing"
2021.emnlp-main.267,N19-1242,0,0.0271217,"o be finetuned on labeled training data, while our work conducts unsupervised QE by directly utilizing the conditional probabilities given by the model and does not require any further fine-tuning process. Moreover, our work utilizes different techniques like WWM (Cui et al., 2019) and MC Dropout (Gal and Ghahramani, 2016) to further improve the performance. 5.2 Masked Language Models Recently, pretrained masked language models (MLMs) (Devlin et al., 2019) have been widely used in various NLP tasks including natural language understanding (Wang et al., 2019) and machine reading comprehension (Xu et al., 2019). The idea of MLM is also used in other complex NLP tasks. For example, Ghazvininejad et al. (2019) introduce a conditional masked language model (CMLM) for non-autoregressive NMT. Chen et al. (2021) and Zhang and van Genabith (2021) present MLM objectives to improve neural word alignment models. MLM objectives are also used in the training process of supervised QE (Kim et al., 2019; Rubino and Sumita, 2020; Cui et al., 2021). To 3329 the best of our knowledge, our work is the first to utilize MLM objectives for QE under unsupervised settings. Our work is also similar to translation language m"
2021.emnlp-main.267,2020.emnlp-main.205,0,0.0577838,"Missing"
2021.emnlp-main.267,2021.acl-long.24,0,0.0846356,"Missing"
2021.emnlp-main.306,L18-1431,0,0.0752281,"Missing"
2021.emnlp-main.306,C16-1167,0,0.0292639,"candidate choices for models. LAMBADA (Paperno et al., 2016) masks the last word in a target sentence and evaluates the ability of models in predicting the masked target words with broader context beyond target sentences in novels. Winograd Schema Challenge (WSC) (Levesque et al., 2012) and WinoGrande (Sakaguchi et al., 2020) defines a word selection task that focuses on solving commonsense problems in the form of coreference resolution. Details on the differences of Chinese WPLC from previous related datasets are shown in Table 1. In Chinese, People Daily (PD) & Children’s Fairy Tale (CFT) (Cui et al., 2016) corpus is the first cloze-style reading comprehension dataset in chinese. ChID (Zheng et al., 2019) offers an interesting task where words to be predicted are all idioms. CLUEWSC2020 (Xu et al., 2020), a Chinese version of WSC dataset, aims to test the ability of coreference resolution via word prediction. Significantly different from such Chinese datasets, our dataset is specifically developed for evaluating word prediction from long-range context. 3 3.1 Dataset Creation Passage Collection Book Topics Romance Fantasy Urban Supernatural Comprehension Rebirth Science Fiction Horror Suspense Hi"
2021.emnlp-main.306,N19-1423,0,0.0325943,"Missing"
2021.emnlp-main.306,D19-1249,1,0.898753,"Missing"
2021.emnlp-main.306,D16-1241,0,0.0709712,"Missing"
2021.emnlp-main.306,P16-1144,0,0.038703,"EN EN EN EN EN ZH Table 1: Comparison between our dataset and other datasets. AC: Automatically Chosen. RNNF: Filtering by RNN, MC: manual check. PLMF: Filtering by pretrained language models. performance, indicating a large space for further research. 2 Related Work CNN/Daily Mail (Hermann et al., 2015) uses an automatic method to create a large amount of instances of replacing entities with placeholders in news. Children’s Book Test (CBT) (Felix et al., 2016) removes four types of words that are expected to be predicted by evaluated models and provides candidate choices for models. LAMBADA (Paperno et al., 2016) masks the last word in a target sentence and evaluates the ability of models in predicting the masked target words with broader context beyond target sentences in novels. Winograd Schema Challenge (WSC) (Levesque et al., 2012) and WinoGrande (Sakaguchi et al., 2020) defines a word selection task that focuses on solving commonsense problems in the form of coreference resolution. Details on the differences of Chinese WPLC from previous related datasets are shown in Table 1. In Chinese, People Daily (PD) & Children’s Fairy Tale (CFT) (Cui et al., 2016) corpus is the first cloze-style reading com"
2021.emnlp-main.306,P19-1075,0,0.015818,"ence and evaluates the ability of models in predicting the masked target words with broader context beyond target sentences in novels. Winograd Schema Challenge (WSC) (Levesque et al., 2012) and WinoGrande (Sakaguchi et al., 2020) defines a word selection task that focuses on solving commonsense problems in the form of coreference resolution. Details on the differences of Chinese WPLC from previous related datasets are shown in Table 1. In Chinese, People Daily (PD) & Children’s Fairy Tale (CFT) (Cui et al., 2016) corpus is the first cloze-style reading comprehension dataset in chinese. ChID (Zheng et al., 2019) offers an interesting task where words to be predicted are all idioms. CLUEWSC2020 (Xu et al., 2020), a Chinese version of WSC dataset, aims to test the ability of coreference resolution via word prediction. Significantly different from such Chinese datasets, our dataset is specifically developed for evaluating word prediction from long-range context. 3 3.1 Dataset Creation Passage Collection Book Topics Romance Fantasy Urban Supernatural Comprehension Rebirth Science Fiction Horror Suspense Historical Military Detective Mystery Modern Others Total Nums 22,292 12,190 5,277 4,624 3,067 3,023 2"
2021.emnlp-main.340,P19-1620,0,0.018061,"ound-truth label for the outputs of the Decoder. Unsupervised QA In unsupervised QA, the QA model is trained using synthetic data based on a QG model instead of an existing QA dataset. Instead of resorting to existing QA datasets, unsupervised QG methods have been employed, such as Unsupervised Neural Machine Translation (Lewis et al., 2019). Fabbri et al. (2020) and Li et al. (2020) propose template/rule-based methods for generating questions and employ retrieved paragraphs and cited passages as source passages to alleviate the problems of lexical similarities between passages and questions. Alberti et al. (2019); Puri et al. (2020); Shakeri et al. (2020) additionally employ existing QA datasets to train a QG model. Although related, this work falls outside the scope of unsupervised QA. 3 Methodology Diverging from supervised neural question generation models trained on existing QA datasets, the approach we propose employs synthetic QG data, that we create from summary data using a number of heuristics, to train a QG model. We provide an overview of the proposed method is shown in Figure 2. We then employ the trained QG model to generate synthetic QA data that is further employed to train an unsupervi"
2021.emnlp-main.340,D18-1549,0,0.0258651,"). Document length and stride length are 364 and 128 respectively, the learning rate is set to 1 × 10−5 . Evaluation metrics for unsupervised QA are Exact Match (EM) and F-1 score. 4.2 Results We use the 20k generated synthetic QA pairs to train a BERT QA model and first validate its performance on the development sets of three benchmark QA datasets based on Wikipedia – SQuAD1.1, Natural Questions and TriviaQA. The results of our method are shown in Tables 1 and 2. The unsupervised baselines we compare with are as follows: 1. Lewis et al. (2019) employ unsupervised neural machine translation (Artetxe et al., 2018) to train a QG model; 4M synthetic QA examples were generated to train a QA model; 2. Li et al. (2020) employ dependency trees to generate questions and employed cited documents as passages. 5 For comparison, we also show the results of some supervised models fine-tuned on the correspond4138 Models S UPERVISED M ODELS BERT-base BERT-large U NSUPERVISED M ODELS Lewis et al. (2019) Li et al. (2020) Our Method NQ TriviaQA NewsQA BioASQ DuoRC EM F-1 EM F-1 EM F-1 EM F-1 EM F-1 Lewis et al. (2019) 19.6 28.5 18.9 27.0 26.0 32.6 Li et al. (2020) 33.6 46.3 30.3 38.7 32.7 41.1 Our Method 37.5 50.1 32.0"
2021.emnlp-main.340,2021.emnlp-main.693,1,0.844914,"Missing"
2021.emnlp-main.340,2020.acl-main.413,0,0.0353091,"2020). 4135 Figure 2: An overview of our approach where Answer and Question are generated based on Summary by the Question Generation Heuristics, the Answer is combined with the Article to form the input to the Encoder, the Question is employed as the ground-truth label for the outputs of the Decoder. Unsupervised QA In unsupervised QA, the QA model is trained using synthetic data based on a QG model instead of an existing QA dataset. Instead of resorting to existing QA datasets, unsupervised QG methods have been employed, such as Unsupervised Neural Machine Translation (Lewis et al., 2019). Fabbri et al. (2020) and Li et al. (2020) propose template/rule-based methods for generating questions and employ retrieved paragraphs and cited passages as source passages to alleviate the problems of lexical similarities between passages and questions. Alberti et al. (2019); Puri et al. (2020); Shakeri et al. (2020) additionally employ existing QA datasets to train a QG model. Although related, this work falls outside the scope of unsupervised QA. 3 Methodology Diverging from supervised neural question generation models trained on existing QA datasets, the approach we propose employs synthetic QG data, that we"
2021.emnlp-main.340,D19-5801,0,0.0143966,"rain the QG model on the QG data for 3 epochs with a learning rate of 3 × 10−5 , using the AdamW optimizer (Loshchilov and Hutter, 2019). 4.1.2 Unsupervised QA Datasets We carry out experiments on six extractive QA datasets, namely, SQuAD1.1 (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015) and DuoRC (Saha et al., 2018). We employ the official data of SQuAD1.1, NewsQA and TriviaQA and for Natural Questions, BioASQ and DuoRC, we employ the pre-processed data released by MRQA (Fisch et al., 2019). Unsupervised QA Training Details To generate synthetic QA training data, we make use of Wikidumps 4 by firstly removing all HTML tags and reference links, then extracting paragraphs that are longer than 500 characters, resulting in 60k paragraphs sampled from all paragraphs of Wikidumps. We employ the NER toolkits of Spacy5 (Honnibal et al., 2020) and AllenNLP6 (Gardner et al., 2017) to extract entity mentions in the paragraphs. We then remove paragraph, answer pairs that meet one or more of the following three conditions: 1) paragraphs with less than 20 words and more than 480 words; 2) par"
2021.emnlp-main.340,E06-1032,0,0.0223319,"Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrowed from text generation tasks to evalglish QG model using news summary data. We uate QG. Even with respect to original text generemploy our QG model to generate synthetic QA ation tasks, however, the use of such metrics has data to train a QA model in an unsupervised set- been questioned (Callison-Burch et al., 2006; Reting and test the approach with six English QA iter, 2018). Such metrics are particularly probdatasets: SQuAD1.1, Natural Questions, TriviaQA, lematic for QG evaluation since multiple plausiNewsQA, BioASQ and DuoRC (Rajpurkar et al., ble questions exist for a given passage and answer. 2016; Kwiatkowski et al., 2019; Joshi et al., 2017; Consequently, there has been a shift in focus to Trischler et al., 2017; Tsatsaronis et al., 2015; Saha evaluating QG using an extrinsic evaluation that et al., 2018). Experiment results show that our generates synthetic QA pairs for the purpose of approach"
2021.emnlp-main.340,N10-1086,0,0.0648963,"Missing"
2021.emnlp-main.340,2020.coling-main.306,0,0.0346789,"tically correct questions, the resulting questions often lack variety and incur high lexical overlap with corresponding declarative sentences. For example, the question generated from the sentence Stephen Hawking announced the party in the morning, with Stephen Hawking as the candidate answer span, could be Who announced the party in the morning?, with a high level of lexi1 Introduction cal overlap between the generated question and the The aim of Question Generation (QG) is the pro- declarative sentence. This is undesirable in a QA duction of meaningful questions given a set of input system (Hong et al., 2020) since the strong lexical passages and corresponding answers, a task with clues in the question would make it a poor test of many applications including dialogue systems as real comprehension. 4134 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4134–4148 c November 7–11, 2021. 2021 Association for Computational Linguistics Neural seq2seq models (Sutskever et al., 2014) have come to dominate QG (Du et al., 2017), and are commonly trained with <passage, answer, question> triples taken from human-created QA datasets (Dzendzik et al., 2021) and this l"
2021.emnlp-main.340,P17-1147,0,0.175328,"ish QG model using news summary data. We uate QG. Even with respect to original text generemploy our QG model to generate synthetic QA ation tasks, however, the use of such metrics has data to train a QA model in an unsupervised set- been questioned (Callison-Burch et al., 2006; Reting and test the approach with six English QA iter, 2018). Such metrics are particularly probdatasets: SQuAD1.1, Natural Questions, TriviaQA, lematic for QG evaluation since multiple plausiNewsQA, BioASQ and DuoRC (Rajpurkar et al., ble questions exist for a given passage and answer. 2016; Kwiatkowski et al., 2019; Joshi et al., 2017; Consequently, there has been a shift in focus to Trischler et al., 2017; Tsatsaronis et al., 2015; Saha evaluating QG using an extrinsic evaluation that et al., 2018). Experiment results show that our generates synthetic QA pairs for the purpose of approach substantially improves over previous un- evaluating their effectiveness as a data augmentasupervised QA models even when trained on sub- tion or unsupervised QA approach (Alberti et al., stantially fewer synthetic QA examples. 2019; Puri et al., 2020; Shakeri et al., 2020). 4135 Figure 2: An overview of our approach where Answer and Quest"
2021.emnlp-main.340,Q19-1026,0,0.0572483,"generation tasks to evalglish QG model using news summary data. We uate QG. Even with respect to original text generemploy our QG model to generate synthetic QA ation tasks, however, the use of such metrics has data to train a QA model in an unsupervised set- been questioned (Callison-Burch et al., 2006; Reting and test the approach with six English QA iter, 2018). Such metrics are particularly probdatasets: SQuAD1.1, Natural Questions, TriviaQA, lematic for QG evaluation since multiple plausiNewsQA, BioASQ and DuoRC (Rajpurkar et al., ble questions exist for a given passage and answer. 2016; Kwiatkowski et al., 2019; Joshi et al., 2017; Consequently, there has been a shift in focus to Trischler et al., 2017; Tsatsaronis et al., 2015; Saha evaluating QG using an extrinsic evaluation that et al., 2018). Experiment results show that our generates synthetic QA pairs for the purpose of approach substantially improves over previous un- evaluating their effectiveness as a data augmentasupervised QA models even when trained on sub- tion or unsupervised QA approach (Alberti et al., stantially fewer synthetic QA examples. 2019; Puri et al., 2020; Shakeri et al., 2020). 4135 Figure 2: An overview of our approach wh"
2021.emnlp-main.340,2020.acl-main.703,0,0.140325,"n and Smith, 2009). Recently, Dhole and Manning (2020) showed that, with the help of advanced neural syntactic parsers, template-based methods are capable of generating high-quality questions from texts. Neural seq2seq generation models have additionally been widely employed in QG, with QG data usually borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrowed from text generation tasks to evalglish QG model using news summary data. We uate QG. Even with respect to original text generemploy our QG mod"
2021.emnlp-main.340,P19-1484,0,0.320563,"A) and three out-of-domain datasets (NewsQA, BioASQ, DuoRC), demonstrating the transferability of the approach. Figure 1: Example questions generated via heuristics informed by semantic role labeling of summary sentences using different candidate answer spans well as education (Graesser et al., 2005). Additionally, QG can be applied to Question Answering (QA) for the purpose of data augmentation (Puri et al., 2020) where labeled <passage, answer, question> triples are combined with synthetic <passage, answer, question> triples produced by a QG system to train a QA system, and unsupervised QA (Lewis et al., 2019), in which only the QG system output is used to train the QA system. Early work on QG focused on template or rulebased approaches, employing syntactic knowledge to manipulate constituents in declarative sentences to form interrogatives (Heilman and Smith, 2009, 2010). Although template-based methods are capable of generating linguistically correct questions, the resulting questions often lack variety and incur high lexical overlap with corresponding declarative sentences. For example, the question generated from the sentence Stephen Hawking announced the party in the morning, with Stephen Hawk"
2021.emnlp-main.340,2020.acl-main.600,0,0.369141,"overview of our approach where Answer and Question are generated based on Summary by the Question Generation Heuristics, the Answer is combined with the Article to form the input to the Encoder, the Question is employed as the ground-truth label for the outputs of the Decoder. Unsupervised QA In unsupervised QA, the QA model is trained using synthetic data based on a QG model instead of an existing QA dataset. Instead of resorting to existing QA datasets, unsupervised QG methods have been employed, such as Unsupervised Neural Machine Translation (Lewis et al., 2019). Fabbri et al. (2020) and Li et al. (2020) propose template/rule-based methods for generating questions and employ retrieved paragraphs and cited passages as source passages to alleviate the problems of lexical similarities between passages and questions. Alberti et al. (2019); Puri et al. (2020); Shakeri et al. (2020) additionally employ existing QA datasets to train a QG model. Although related, this work falls outside the scope of unsupervised QA. 3 Methodology Diverging from supervised neural question generation models trained on existing QA datasets, the approach we propose employs synthetic QG data, that we create from summary d"
2021.emnlp-main.340,W04-1013,0,0.0213914,"lly borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrowed from text generation tasks to evalglish QG model using news summary data. We uate QG. Even with respect to original text generemploy our QG model to generate synthetic QA ation tasks, however, the use of such metrics has data to train a QA model in an unsupervised set- been questioned (Callison-Burch et al., 2006; Reting and test the approach with six English QA iter, 2018). Such metrics are particularly probdatasets: SQuAD1.1, Na"
2021.emnlp-main.340,D18-1206,0,0.173048,"(who undergoes the action), and a set of modifier arguments such as a temporal ARGTMP or locative argument ARG-LOC. Questions are then generated from the arguments according to argument type and NER tags, which means that wh-words can be determined jointly. Returning to the example in Figure 1: given the SRL analysis [U2’s lead singer Bono ARG-0] has [had VERB] [emergency spinal surgery ARG-1] [after suffering an injury while preparing for tour Question Generation 1 Data we employ in experiments is news summary data In order to avoid generating trivial questions that originally from BBC News (Narayan et al., 2018) and the are highly similar to corresponding declarative news articles are typically a few hundred words in length. 4136 dates ARG-TMP]., the three questions shown in Figure 1 can be generated based on these three arguments. The pseudocode for our algorithm to generate questions is shown in Algorithm 1. We first obAlgorithm 1: Question Generation Heuristics S = summary srl_f rames = SRL(S) ners = N ER(S) dps = DP (S) examples = [] for frame in srl_frames do root_verb = dpsroot verb = f rameverb if root_verb equal to verb then for arg in frame do wh∗ = identif y_wh_word(arg, ners) base_verb, au"
2021.emnlp-main.340,P02-1040,0,0.110129,"o interrogatives (Heilman and Smith, 2009). Recently, Dhole and Manning (2020) showed that, with the help of advanced neural syntactic parsers, template-based methods are capable of generating high-quality questions from texts. Neural seq2seq generation models have additionally been widely employed in QG, with QG data usually borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrowed from text generation tasks to evalglish QG model using news summary data. We uate QG. Even with respect to original text g"
2021.emnlp-main.340,2020.emnlp-main.468,0,0.0807813,"trained with only 20k English Wikipedia-based synthetic QA pairs, the QA model substantially outperforms previous unsupervised models on three in-domain datasets (SQuAD1.1, Natural Questions, TriviaQA) and three out-of-domain datasets (NewsQA, BioASQ, DuoRC), demonstrating the transferability of the approach. Figure 1: Example questions generated via heuristics informed by semantic role labeling of summary sentences using different candidate answer spans well as education (Graesser et al., 2005). Additionally, QG can be applied to Question Answering (QA) for the purpose of data augmentation (Puri et al., 2020) where labeled <passage, answer, question> triples are combined with synthetic <passage, answer, question> triples produced by a QG system to train a QA system, and unsupervised QA (Lewis et al., 2019), in which only the QG system output is used to train the QA system. Early work on QG focused on template or rulebased approaches, employing syntactic knowledge to manipulate constituents in declarative sentences to form interrogatives (Heilman and Smith, 2009, 2010). Although template-based methods are capable of generating linguistically correct questions, the resulting questions often lack var"
2021.emnlp-main.340,D16-1264,0,0.0511569,"ter than 5 tokens (very short questions are likely to have removed too much information) For the dataset in question, this process resulted in a total of 14,830 <passage-answer-question> triples. For training the QG model, we employ implementations of BART (Lewis et al., 2020) from Huggingface (Wolf et al., 2019). The QG model we employ is BART-base. We train the QG model on the QG data for 3 epochs with a learning rate of 3 × 10−5 , using the AdamW optimizer (Loshchilov and Hutter, 2019). 4.1.2 Unsupervised QA Datasets We carry out experiments on six extractive QA datasets, namely, SQuAD1.1 (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015) and DuoRC (Saha et al., 2018). We employ the official data of SQuAD1.1, NewsQA and TriviaQA and for Natural Questions, BioASQ and DuoRC, we employ the pre-processed data released by MRQA (Fisch et al., 2019). Unsupervised QA Training Details To generate synthetic QA training data, we make use of Wikidumps 4 by firstly removing all HTML tags and reference links, then extracting paragraphs that are longer than 500 characters, resulting in 60k paragraph"
2021.emnlp-main.340,J18-3002,0,0.0226063,"Missing"
2021.emnlp-main.340,P18-1156,0,0.0202575,"stion> triples. For training the QG model, we employ implementations of BART (Lewis et al., 2020) from Huggingface (Wolf et al., 2019). The QG model we employ is BART-base. We train the QG model on the QG data for 3 epochs with a learning rate of 3 × 10−5 , using the AdamW optimizer (Loshchilov and Hutter, 2019). 4.1.2 Unsupervised QA Datasets We carry out experiments on six extractive QA datasets, namely, SQuAD1.1 (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015) and DuoRC (Saha et al., 2018). We employ the official data of SQuAD1.1, NewsQA and TriviaQA and for Natural Questions, BioASQ and DuoRC, we employ the pre-processed data released by MRQA (Fisch et al., 2019). Unsupervised QA Training Details To generate synthetic QA training data, we make use of Wikidumps 4 by firstly removing all HTML tags and reference links, then extracting paragraphs that are longer than 500 characters, resulting in 60k paragraphs sampled from all paragraphs of Wikidumps. We employ the NER toolkits of Spacy5 (Honnibal et al., 2020) and AllenNLP6 (Gardner et al., 2017) to extract entity mentions in the"
2021.emnlp-main.340,2020.emnlp-main.439,0,0.0339364,"exist for a given passage and answer. 2016; Kwiatkowski et al., 2019; Joshi et al., 2017; Consequently, there has been a shift in focus to Trischler et al., 2017; Tsatsaronis et al., 2015; Saha evaluating QG using an extrinsic evaluation that et al., 2018). Experiment results show that our generates synthetic QA pairs for the purpose of approach substantially improves over previous un- evaluating their effectiveness as a data augmentasupervised QA models even when trained on sub- tion or unsupervised QA approach (Alberti et al., stantially fewer synthetic QA examples. 2019; Puri et al., 2020; Shakeri et al., 2020). 4135 Figure 2: An overview of our approach where Answer and Question are generated based on Summary by the Question Generation Heuristics, the Answer is combined with the Article to form the input to the Encoder, the Question is employed as the ground-truth label for the outputs of the Decoder. Unsupervised QA In unsupervised QA, the QA model is trained using synthetic data based on a QG model instead of an existing QA dataset. Instead of resorting to existing QA datasets, unsupervised QG methods have been employed, such as Unsupervised Neural Machine Translation (Lewis et al., 2019). Fabbri"
2021.emnlp-main.340,D19-1253,0,0.0207924,"on Generation Traditional approaches to QG mostly employ linguistic templates and rules to transform declarative sentences into interrogatives (Heilman and Smith, 2009). Recently, Dhole and Manning (2020) showed that, with the help of advanced neural syntactic parsers, template-based methods are capable of generating high-quality questions from texts. Neural seq2seq generation models have additionally been widely employed in QG, with QG data usually borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrow"
2021.emnlp-main.340,D18-1427,0,0.0182907,"-of-the-art performance even at low volumes of synthetic training data. 2 Related Work Question Generation Traditional approaches to QG mostly employ linguistic templates and rules to transform declarative sentences into interrogatives (Heilman and Smith, 2009). Recently, Dhole and Manning (2020) showed that, with the help of advanced neural syntactic parsers, template-based methods are capable of generating high-quality questions from texts. Neural seq2seq generation models have additionally been widely employed in QG, with QG data usually borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Ban"
2021.emnlp-main.340,W17-2623,0,0.158077,"o original text generemploy our QG model to generate synthetic QA ation tasks, however, the use of such metrics has data to train a QA model in an unsupervised set- been questioned (Callison-Burch et al., 2006; Reting and test the approach with six English QA iter, 2018). Such metrics are particularly probdatasets: SQuAD1.1, Natural Questions, TriviaQA, lematic for QG evaluation since multiple plausiNewsQA, BioASQ and DuoRC (Rajpurkar et al., ble questions exist for a given passage and answer. 2016; Kwiatkowski et al., 2019; Joshi et al., 2017; Consequently, there has been a shift in focus to Trischler et al., 2017; Tsatsaronis et al., 2015; Saha evaluating QG using an extrinsic evaluation that et al., 2018). Experiment results show that our generates synthetic QA pairs for the purpose of approach substantially improves over previous un- evaluating their effectiveness as a data augmentasupervised QA models even when trained on sub- tion or unsupervised QA approach (Alberti et al., stantially fewer synthetic QA examples. 2019; Puri et al., 2020; Shakeri et al., 2020). 4135 Figure 2: An overview of our approach where Answer and Question are generated based on Summary by the Question Generation Heuristics,"
2021.emnlp-main.340,2020.coling-main.228,0,0.0416481,"stly employ linguistic templates and rules to transform declarative sentences into interrogatives (Heilman and Smith, 2009). Recently, Dhole and Manning (2020) showed that, with the help of advanced neural syntactic parsers, template-based methods are capable of generating high-quality questions from texts. Neural seq2seq generation models have additionally been widely employed in QG, with QG data usually borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrowed from text generation tasks to evalgl"
2021.emnlp-main.580,2021.findings-acl.420,1,0.812354,"Missing"
2021.emnlp-main.580,2020.acl-main.747,0,0.248181,"both heterogeneous (different corpora reveal difso that the learning process is balanced and ferent linguistic properties) and imbalance (the aclow-resource cases can benefit from the highcessibility of training data varies across corpora). resource ones. However, automatic balancing The standard practice to address this issue is to admethods usually depend on the intra- and interjust the training data distribution heuristically by dataset characteristics, which is usually agnosup-sampling the training data from LRLs/LRDs tic or requires human priors. In this work, (Arivazhagan et al., 2019; Conneau et al., 2020). we propose an approach, M ULTI UAT, that dynamically adjusts the training data usage based Arivazhagan et al. (2019) rescale the training on the model’s uncertainty on a small set of data distribution with a heuristic temperature term trusted clean data for multi-corpus machine and demonstrate that the ideal temperature can subtranslation. We experiment with two classes of stantially improve the overall performance. Howuncertainty measures on multilingual (16 lanever, the optimal value for such heuristics is both guages with 4 settings) and multi-domain sethard to find and varies from one ex"
2021.emnlp-main.580,P07-1033,0,0.341337,"Missing"
2021.emnlp-main.580,2020.findings-emnlp.377,0,0.195595,"in different languages, raising the tween a small set of trusted clean data and training problem of learning a NLP system from the hetero- data. They instantiate this framework on multilingeneous corpora, such as multilingual models (Wu gual NMT, known as M ULTI DDS, to dynamically and Dredze, 2019; Arivazhagan et al., 2019; Aha- weigh the importance of language pairs. Both the roni et al., 2019; Freitag and Firat, 2020; Arthur hypothesis and the proposed approach rely on the et al., 2021) and multi-domain models (Daumé III, assumption that knowledge learned from one cor2007; Li et al., 2019; Deng et al., 2020; Jiang et al., pus can always be beneficial to the other corpora. 2020). A strong demand is to deploy a unified However, their assumption does not always hold. model for all the languages and domains, because If the knowledge learned from one corpus is not ∗ Work done during the internship at Huawei Noah’s Ark able to be transferred easily or is useless to the other Lab. corpora, this approach fails. Unlike cosine similar1 Code available at https://github.com/ ity, model uncertainty is free from the aforemenhuawei-noah/noah-research/tree/master/ noahnmt/multiuat tioned assumption on cross-cor"
2021.emnlp-main.580,P15-1166,0,0.0208238,"ty over KDE of M ULTI DDS-S and M ULTI UAT with different temperature priors. TI UAT with different prior sampling distributions in Figure 4. The learned sampling distribution by M ULTI UAT always converges to uniform distribution, regardless of the change of prior sampling distribution. However, the change of priors significantly affects the learned sampling distribution of M ULTI DDS-S. 7 Related Work Multi-corpus NLP Multilingual training has been particularly prominent in recent advances driven by the demand of training a unified model 6.4 Effects of Sampling Priors for all the languages (Dong et al., 2015; Plank et al., Both M ULTI DDS-S and M ULTI UAT initialize the 2016; Johnson et al., 2017; Arivazhagan et al., sampling probability distribution to proportional 2019). Freitag and Firat (2020) extend current distribution (line 1 in Algorithm 1). We investi- English-centric training to a many-to-many setup gate how the prior sampling distribution affects without sacrificing the performance on Englishthe performance and present the results in Table 3. centric language pairs. Wang et al. (2021) imWe can observe that the prior sampling distribu- prove the multilingual training by adjusting gratio"
2021.emnlp-main.580,P18-1069,0,0.0189446,"on et al., 2007; von Stackelberg et al., 2011). 3 Algorithm 1: Training with M ULTI UAT Methodology In this work, we leverage the idea of DDS under the multi-corpus scenarios. We utilize a differentiable domain/language scorer ψ to weigh the training corpora. To learn ψ , we exploit the model uncertainty to measure the model’s ability over the target corpus. Below, we elaborate on the details of our method. 1 2 3 4 5 6 3.1 Model Uncertainty 7 Model uncertainty can be a measure that indicates whether the model parameters θ are able to describe the data distribution well (Kendall and Gal, 2017; Dong et al., 2018; Xiao and Wang, 2019). Bayesian neural networks can be used for quantifying the model uncertainty (Buntine and Weigend, 1991), which models the θ as a probabilistic distribution with constant input and output. From the Bayesian point of view, θ is interpreted as a random variable with the prior p(θθ ). Given a dataset D, the posterior p(θθ |D) can be obtained via Bayes’ rule. However, the exact Bayesian inference is intractable for neural networks, so that it is common to place the approximation q(θθ ) to the true posterior p(θθ |D). Several variational inference methods have been proposed (G"
2021.emnlp-main.580,D17-1063,0,0.0202276,"dient descent between two updates 3 We parameterize the scorer ψ following Wang et al. (2020b). 7293 of ψ , like common gradient-based optimization, and hence the objective is formulated as follows: • Variance of Translation Probability (VARTP): The variance of the distribution of maximal position-wise translation probability, ψ )) ψ = argmin L(Ddev ; θ (ψ ψ ψ) = θ (ψ n argmin En∼pψ (n) [L(Dtrn ; θ )] . θ A considerable problem here is Equation 6 is not directly differentiable w.r.t. the scorer ψ . To tackle this problem, reinforcement learning (RL) with suitable reward functions is required (Fang et al., 2017; Wang et al., 2020a): ψ ←ψ− N X R(n) · ∇ψ log pψ (n) . (9) n=1 Details for the reward functions R(n) are depicted at Section 3.3 and the update of ψ follows the REINFORCE algorithm (Williams, 1992). 3.3 xn , y n,&lt;t ; θ k )] . RVARTP (n; θ k ) = Var[p(ˆ yn,t |x (8) Uncertainty Measures We explore the utility of two groups of model uncertainty measures: probability-based and entropybased measures at the sentence level (Wang et al., 2019; Fomicheva et al., 2020; Malinin and Gales, 2021). Probability-Based Measures We explore four probability-based uncertainty measures following the definition of"
2021.emnlp-main.580,2020.wmt-1.66,0,0.175795,"S) that automati1 Introduction cally adjusts the importance of data points, whose Text corpora are commonly collected from several reward is the cosine similarity of the gradients bedifferent sources in different languages, raising the tween a small set of trusted clean data and training problem of learning a NLP system from the hetero- data. They instantiate this framework on multilingeneous corpora, such as multilingual models (Wu gual NMT, known as M ULTI DDS, to dynamically and Dredze, 2019; Arivazhagan et al., 2019; Aha- weigh the importance of language pairs. Both the roni et al., 2019; Freitag and Firat, 2020; Arthur hypothesis and the proposed approach rely on the et al., 2021) and multi-domain models (Daumé III, assumption that knowledge learned from one cor2007; Li et al., 2019; Deng et al., 2020; Jiang et al., pus can always be beneficial to the other corpora. 2020). A strong demand is to deploy a unified However, their assumption does not always hold. model for all the languages and domains, because If the knowledge learned from one corpus is not ∗ Work done during the internship at Huawei Noah’s Ark able to be transferred easily or is useless to the other Lab. corpora, this approach fails. U"
2021.emnlp-main.580,2020.aacl-main.73,0,0.0293311,"multilingual training by adjusting gration can affect the overall performance. For both dient directions based on gradient similarity. ExM ULTI DDS-S and M ULTI UAT, the overall results isting works on multi-domain training commonly on both in-domain and out-of-domain evaluation attempt to leverage architectural domain-specific are negatively correlated with the prior τ . components or auxiliary loss (Sajjad et al., 2017; We also visualize the change of sampling prob- Tars and Fishel, 2018; Zeng et al., 2018; Li et al., ability of KDE given by M ULTI DDS-S and M UL - 2018; Deng et al., 2020; Jiang et al., 2020). These 7298 approaches commonly do not explore much on the training proportion across domains and are limited to in-domain prediction and less generalizable to unseen domains. Zaremoodi and Haffari (2019) dynamically balance the importance of tasks in multitask NMT to improve the low-resource NMT performance. Vu et al. (2021) leverage a pre-trained language model to select useful monolingual data from either source language or target language to perform unsupervised domain adaptation for NMT models. Our work is directly related to Wang et al. (2020a) and Wang et al. (2020b) that leverage cosi"
2021.emnlp-main.580,W04-3250,0,0.6712,"nvolved in this work, including the baseline approaches and M ULTI UAT, are model-agnostic. To validate this idea, we experiment two variants of transformer (Vaswani et al., 2017). For multilingual NMT, the model architecture is a transformer with 4 attention heads and 6 layers.10 And for multi-domain NMT models, we use the standard transformer-base with 8 attention heads and 6 layers.11 All the models in this work are implemented by fairseq (Ott et al., 2019). 4.5 Evaluation We report detokenized BLEU (Papineni et al., 2002) using SacreBLEU (Post, 2018) with statistical significance given by Koehn (2004).12 µBLEU is the macro average of BLEU scores within the same setting, with the assumption that all the language pairs/domains are equally important. 8 https://opus.nlpl.eu/QED.php https://opus.nlpl.eu/TED2013.php 10 Signature: multilingual_transformer_iwslt_de_en 11 Signature: transformer 12 Signature: BLEU+case.mixed+numrefs.1 +smooth.exp+tok.13a+version.1.4.14 7295 9 Multilingual Related Multi-Domain Diverse ID OOD 16.79 17.94 16.86 18.24 35.69 36.92 36.85 36.42 25.15 23.46 22.22 22.74 19.57† 19.59† 19.62† 19.67† 19.68† 19.76† 37.50† 37.29 35.49 37.25 37.27 37.44† 23.77 23.89 23.75 23.36 23"
2021.emnlp-main.580,P07-2045,0,0.00742773,"les. 4 Refer to Wang et al. (2020b) for dataset statistics. https://opus.nlpl.eu/Tanzil.php 6 https://opus.nlpl.eu/EMEA.php 7 https://opus.nlpl.eu/KDE4.php 5 ID WMT Tanzil EMEA KDE OOD QED TED Train Valid Test 3, 950K 449K 277K 135K 11K 3K 3K 3K 3K 3K 3K 3K - - 3K 3K Table 1: Dataset statistics of multi-domain corpora. Out-Of-Domain (OOD) (i) QED,8 a collection of subtitles for educational videos and lectures (Abdelali et al., 2014); (ii) TED,9 a parallel corpus of TED talk subtitles. These two domains are only used for out-of-domain evaluation. All these corpora are first tokenized by Moses (Koehn et al., 2007) and processed into sub-word units by BPE (Sennrich et al., 2016) with 32K merge operations. Sentence pairs that are duplicated and violates source-target ratio of 1.5 are removed. The validation sets and test sets are randomly sampled, except for WMT. The dataset statistics are listed in Table 1. 4.4 Model Architecture We believe all the approaches involved in this work, including the baseline approaches and M ULTI UAT, are model-agnostic. To validate this idea, we experiment two variants of transformer (Vaswani et al., 2017). For multilingual NMT, the model architecture is a transformer with"
2021.emnlp-main.580,P16-2067,0,0.0541322,"Missing"
2021.emnlp-main.580,P19-1583,0,0.0181778,"ta usage based Arivazhagan et al. (2019) rescale the training on the model’s uncertainty on a small set of data distribution with a heuristic temperature term trusted clean data for multi-corpus machine and demonstrate that the ideal temperature can subtranslation. We experiment with two classes of stantially improve the overall performance. Howuncertainty measures on multilingual (16 lanever, the optimal value for such heuristics is both guages with 4 settings) and multi-domain sethard to find and varies from one experimental settings (4 for in-domain and 2 for out-of-domain ting to another (Wang and Neubig, 2019; Wang on English-German translation) and demonstrate our approach M ULTI UAT substantially et al., 2020a,b). Wang et al. (2020a) and Wang et al. outperforms its baselines, including both static (2020b) hypothesize that the training data instances and dynamic strategies. We analyze the crossthat are similar to the validation set can be more domain transfer and show the deficiency of beneficial to the evaluation performance and prostatic and similarity based methods.1 pose a general reinforcement-learning framework Differentiable Data Selection (DDS) that automati1 Introduction cally adjusts th"
2021.emnlp-main.580,W18-6319,0,0.0116562,"Model Architecture We believe all the approaches involved in this work, including the baseline approaches and M ULTI UAT, are model-agnostic. To validate this idea, we experiment two variants of transformer (Vaswani et al., 2017). For multilingual NMT, the model architecture is a transformer with 4 attention heads and 6 layers.10 And for multi-domain NMT models, we use the standard transformer-base with 8 attention heads and 6 layers.11 All the models in this work are implemented by fairseq (Ott et al., 2019). 4.5 Evaluation We report detokenized BLEU (Papineni et al., 2002) using SacreBLEU (Post, 2018) with statistical significance given by Koehn (2004).12 µBLEU is the macro average of BLEU scores within the same setting, with the assumption that all the language pairs/domains are equally important. 8 https://opus.nlpl.eu/QED.php https://opus.nlpl.eu/TED2013.php 10 Signature: multilingual_transformer_iwslt_de_en 11 Signature: transformer 12 Signature: BLEU+case.mixed+numrefs.1 +smooth.exp+tok.13a+version.1.4.14 7295 9 Multilingual Related Multi-Domain Diverse ID OOD 16.79 17.94 16.86 18.24 35.69 36.92 36.85 36.42 25.15 23.46 22.22 22.74 19.57† 19.59† 19.62† 19.67† 19.68† 19.76† 37.50† 37.29"
2021.emnlp-main.580,E17-2045,0,0.0289594,"sacrificing the performance on Englishthe performance and present the results in Table 3. centric language pairs. Wang et al. (2021) imWe can observe that the prior sampling distribu- prove the multilingual training by adjusting gration can affect the overall performance. For both dient directions based on gradient similarity. ExM ULTI DDS-S and M ULTI UAT, the overall results isting works on multi-domain training commonly on both in-domain and out-of-domain evaluation attempt to leverage architectural domain-specific are negatively correlated with the prior τ . components or auxiliary loss (Sajjad et al., 2017; We also visualize the change of sampling prob- Tars and Fishel, 2018; Zeng et al., 2018; Li et al., ability of KDE given by M ULTI DDS-S and M UL - 2018; Deng et al., 2020; Jiang et al., 2020). These 7298 approaches commonly do not explore much on the training proportion across domains and are limited to in-domain prediction and less generalizable to unseen domains. Zaremoodi and Haffari (2019) dynamically balance the importance of tasks in multitask NMT to improve the low-resource NMT performance. Vu et al. (2021) leverage a pre-trained language model to select useful monolingual data from"
2021.emnlp-main.580,P16-1162,0,0.0160701,"tps://opus.nlpl.eu/Tanzil.php 6 https://opus.nlpl.eu/EMEA.php 7 https://opus.nlpl.eu/KDE4.php 5 ID WMT Tanzil EMEA KDE OOD QED TED Train Valid Test 3, 950K 449K 277K 135K 11K 3K 3K 3K 3K 3K 3K 3K - - 3K 3K Table 1: Dataset statistics of multi-domain corpora. Out-Of-Domain (OOD) (i) QED,8 a collection of subtitles for educational videos and lectures (Abdelali et al., 2014); (ii) TED,9 a parallel corpus of TED talk subtitles. These two domains are only used for out-of-domain evaluation. All these corpora are first tokenized by Moses (Koehn et al., 2007) and processed into sub-word units by BPE (Sennrich et al., 2016) with 32K merge operations. Sentence pairs that are duplicated and violates source-target ratio of 1.5 are removed. The validation sets and test sets are randomly sampled, except for WMT. The dataset statistics are listed in Table 1. 4.4 Model Architecture We believe all the approaches involved in this work, including the baseline approaches and M ULTI UAT, are model-agnostic. To validate this idea, we experiment two variants of transformer (Vaswani et al., 2017). For multilingual NMT, the model architecture is a transformer with 4 attention heads and 6 layers.10 And for multi-domain NMT model"
2021.emnlp-main.580,2020.acl-main.754,0,0.136071,"heuristic temperature term trusted clean data for multi-corpus machine and demonstrate that the ideal temperature can subtranslation. We experiment with two classes of stantially improve the overall performance. Howuncertainty measures on multilingual (16 lanever, the optimal value for such heuristics is both guages with 4 settings) and multi-domain sethard to find and varies from one experimental settings (4 for in-domain and 2 for out-of-domain ting to another (Wang and Neubig, 2019; Wang on English-German translation) and demonstrate our approach M ULTI UAT substantially et al., 2020a,b). Wang et al. (2020a) and Wang et al. outperforms its baselines, including both static (2020b) hypothesize that the training data instances and dynamic strategies. We analyze the crossthat are similar to the validation set can be more domain transfer and show the deficiency of beneficial to the evaluation performance and prostatic and similarity based methods.1 pose a general reinforcement-learning framework Differentiable Data Selection (DDS) that automati1 Introduction cally adjusts the importance of data points, whose Text corpora are commonly collected from several reward is the cosine similarity of the grad"
2021.emnlp-main.663,2020.emnlp-main.175,0,0.0446963,"Missing"
2021.emnlp-main.663,D17-1159,0,0.033541,"tained without resorting to external resources and has been proven beneficial to sentence modeling (Yang et al., 2018; Xu et al., 2019). For each word xm i , we add m } and (xm , xm }. This two edges (xm , x i i+1 i i−1 means we add links from the current word to its adjacent words. • Dependency directly models syntactic and semantic relations between two words in a sentence. Dependency relations not only provide linguistic meanings but also allow connections between words with a longer distance. Previous practices have shown that dependency relations enhance representation learning of words (Marcheggiani and Titov, 2017; Strubell et al., 2018; Lin et al., 2019). Given a dependency tree of the sentence and m m a word xm i , we add a graph edge (xi , xj ) if m xm i is a headword of xj . Graph Rep. Hg0, . . . , Hgm, . . . , Hgm Sentence Emb. E 0, . . . , E m, . . . , E M S0 Sentence Rep. GCN S 0 … … Sm S m 其实 他的 名字 SM SM 我 我 ⽶格尔 L× … … 看着 说 ⽶格尔 ⽶格尔 … … Input Figure 2: Illustration of the proposed document graph encoder. L in this paper is set to 2. It helps understand the logic and structure of the document and resolve the ambiguities. In n this paper we add a graph edge (xm i , xj ) if m n xi is a referent of"
2021.emnlp-main.663,P18-1118,0,0.0729723,"ang et al., 2018; Yang et al., 2019). Different from representation-based approaches, Tu et al. (2018) and Kuang et al. (2018) propose using a cache to memorize context information, which can be either history hidden states or lexicons. To keep tracking of most recent contexts, the cache is updated when new translations are generated. Therefore, long-distance contexts would likely be erased. How to use long-distance contexts is drawing attention in recent years. Approaches, like treating the whole document as a long sentence (JunczysDowmunt, 2019) and using memory and hierarchical structures (Maruf and Haffari, 2018; Maruf et al., 2019; Tan et al., 2019), are proposed to take global contexts into consideration. However, Kim et al. (2019) point out that not all the words in a document are beneficial to context integration, suggesting that it is essential for each word to focus on its own relevant context. 1 Dependency and coreference relations are from Stanford 8435 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8435–8448 c November 7–11, 2021. 2021 Association for Computational Linguistics To address this problem, we suppose to build a document graph for a d"
2021.emnlp-main.663,N19-1313,0,0.278191,"al., 2019). Different from representation-based approaches, Tu et al. (2018) and Kuang et al. (2018) propose using a cache to memorize context information, which can be either history hidden states or lexicons. To keep tracking of most recent contexts, the cache is updated when new translations are generated. Therefore, long-distance contexts would likely be erased. How to use long-distance contexts is drawing attention in recent years. Approaches, like treating the whole document as a long sentence (JunczysDowmunt, 2019) and using memory and hierarchical structures (Maruf and Haffari, 2018; Maruf et al., 2019; Tan et al., 2019), are proposed to take global contexts into consideration. However, Kim et al. (2019) point out that not all the words in a document are beneficial to context integration, suggesting that it is essential for each word to focus on its own relevant context. 1 Dependency and coreference relations are from Stanford 8435 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8435–8448 c November 7–11, 2021. 2021 Association for Computational Linguistics To address this problem, we suppose to build a document graph for a document, where each"
2021.emnlp-main.663,D18-1325,0,0.320702,"… … Figure 1: The structure of graph. Solid lines in blue depict adjacency relations. Dash lines in green denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicich et al., 2018; Wang et al., 2017; Tan et al."
2021.emnlp-main.663,N19-4009,0,0.0419119,"Missing"
2021.emnlp-main.663,P02-1040,0,0.111876,"ntences. (3) We adopted the WMT19 document-level corpus published by Scherrer et al. (2019) for the En-De translation task. This data contains 2.9M parallel sentences with document boundaries and 10.3M back-translated sentence pairs. All data are tokenized and segmented into subword units using the byte-pair encoding (Sennrich et al., 2016). We apply 32k merge steps for each language on En-Fr, En-Ru, En-De tasks, and 30k for Zh-En task. As a node in a document graph represents a word rather than its subwords, we average embeddings of the subwords as the embedding of the node. The 4-gram BLEU (Papineni et al., 2002) is used as the evaluation metric. only train additional parameters introduced by our methods. We set the layers of the document graph encoder to 2 and share their parameters3 . To compare our graph-based method with prior works, we reimplement several document-level baselines on the T RANSFORMER architecture and replace their context modules with ours (Please refer to Supplementary on details): • C TX (Zhang et al., 2018) employs an additional encoder to learn context representations, which are then integrated by cross-attention mechanisms. • H AN (Miculicich et al., 2018) uses a hierarchical"
2021.emnlp-main.663,D19-6506,0,0.0176526,"marks with different corpus size: (1) IWSLT En–Fr and Zh–En translation tasks (Cettolo et al., 2012) with around 200K sentence pairs for training. Following convention (Wang et al., 2017; Miculicich et al., 2018; Zhang et al., 2018), both language pairs take dev2010 as the development set. tst2010 is used for testing on En–Fr and tst2010∼tst2013 on Zh–En. (2) Opensubtitle2018 En–Ru translation corpus released by Voita et al. (2018), which contains 6M sentence pairs for training, among which 1.5M sentence pairs have context sentences. (3) We adopted the WMT19 document-level corpus published by Scherrer et al. (2019) for the En-De translation task. This data contains 2.9M parallel sentences with document boundaries and 10.3M back-translated sentence pairs. All data are tokenized and segmented into subword units using the byte-pair encoding (Sennrich et al., 2016). We apply 32k merge steps for each language on En-Fr, En-Ru, En-De tasks, and 30k for Zh-En task. As a node in a document graph represents a word rather than its subwords, we average embeddings of the subwords as the embedding of the node. The 4-gram BLEU (Papineni et al., 2002) is used as the evaluation metric. only train additional parameters i"
2021.emnlp-main.663,W04-3250,0,0.149933,"+0.61 19.56↑ +0.27 41.54 +1.07 20.02⇑ +0.62 41.89↑ Pre-integration − 19.01 − 41.35 +0.99 20.00⇑ +0.52 41.87 +1.45 20.46⇑ +0.98 42.33⇑ 4 − 4 − En-Ru Test 31.98 Para. 4 Speed - 24.9k 31.27 +0.92 +0.93 31.95 32.87⇑ 32.88⇑ 22.06M 21.01M 21.01M 16.3k 17.7K 17.0K − − − −0.07 +0.01 32.07 32.36 32.54 32.47 32.55 7.30 7.36 8.39 6.27 6.27 M M M M M 19.9k 14.4k 7.7 k 19.7K 18.9K − +0.45 +0.47 32.44 32.89↑ 32.91↑ 0.01 M 5.27 M 6.27 M 19.6K 19.7K 18.5K Table 1: Main results (BLEU) on IWSLT Zh–En and EN–FR, WMT19 En–De, and Opensubtitle2018 En–Ru translation tasks. “↑ / ⇑” denotes significant improvement (Koehn, 2004) over the best baseline model with context on each task at p < 0.05/0.01, respectively. The models in bold are selected to merge with our document graph methods. “Para.” and “Speed” indicate the model size (M = million) and training speed (tokens/second), respectively. ∗ denotes that the model considers the target context. Ablation Relations Comp. Model BASE +A DJACENCY +D EPENDENCY +L EXICAL +C OREFERENCE +I NTRA +I NTER +A LL Dev 29.75 30.50 30.75 30.68 30.49 30.95 30.89 31.79 Test 36.93 37.69 37.81 37.78 37.54 38.04 37.97 38.94 Table 2: Ablation study of source graph variants on IWSLT En-Fr"
2021.emnlp-main.663,P16-1162,0,0.0583485,"rs take dev2010 as the development set. tst2010 is used for testing on En–Fr and tst2010∼tst2013 on Zh–En. (2) Opensubtitle2018 En–Ru translation corpus released by Voita et al. (2018), which contains 6M sentence pairs for training, among which 1.5M sentence pairs have context sentences. (3) We adopted the WMT19 document-level corpus published by Scherrer et al. (2019) for the En-De translation task. This data contains 2.9M parallel sentences with document boundaries and 10.3M back-translated sentence pairs. All data are tokenized and segmented into subword units using the byte-pair encoding (Sennrich et al., 2016). We apply 32k merge steps for each language on En-Fr, En-Ru, En-De tasks, and 30k for Zh-En task. As a node in a document graph represents a word rather than its subwords, we average embeddings of the subwords as the embedding of the node. The 4-gram BLEU (Papineni et al., 2002) is used as the evaluation metric. only train additional parameters introduced by our methods. We set the layers of the document graph encoder to 2 and share their parameters3 . To compare our graph-based method with prior works, we reimplement several document-level baselines on the T RANSFORMER architecture and repla"
2021.emnlp-main.663,N19-1238,0,0.0553695,"Missing"
2021.emnlp-main.663,D18-1548,0,0.0200062,"ternal resources and has been proven beneficial to sentence modeling (Yang et al., 2018; Xu et al., 2019). For each word xm i , we add m } and (xm , xm }. This two edges (xm , x i i+1 i i−1 means we add links from the current word to its adjacent words. • Dependency directly models syntactic and semantic relations between two words in a sentence. Dependency relations not only provide linguistic meanings but also allow connections between words with a longer distance. Previous practices have shown that dependency relations enhance representation learning of words (Marcheggiani and Titov, 2017; Strubell et al., 2018; Lin et al., 2019). Given a dependency tree of the sentence and m m a word xm i , we add a graph edge (xi , xj ) if m xm i is a headword of xj . Graph Rep. Hg0, . . . , Hgm, . . . , Hgm Sentence Emb. E 0, . . . , E m, . . . , E M S0 Sentence Rep. GCN S 0 … … Sm S m 其实 他的 名字 SM SM 我 我 ⽶格尔 L× … … 看着 说 ⽶格尔 ⽶格尔 … … Input Figure 2: Illustration of the proposed document graph encoder. L in this paper is set to 2. It helps understand the logic and structure of the document and resolve the ambiguities. In n this paper we add a graph edge (xm i , xj ) if m n xi is a referent of xj given by coreference"
2021.emnlp-main.663,C18-1050,0,0.0465172,"re of graph. Solid lines in blue depict adjacency relations. Dash lines in green denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicich et al., 2018; Wang et al., 2017; Tan et al., 2019) or extra con"
2021.emnlp-main.663,D18-1512,0,0.0276142,"adjacency relations. Dash lines in green denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicich et al., 2018; Wang et al., 2017; Tan et al., 2019) or extra context encoders (Jean et al., 2015; Zhang e"
2021.emnlp-main.663,D19-1168,0,0.269716,"al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicich et al., 2018; Wang et al., 2017; Tan et al., 2019) or extra context encoders (Jean et al., 2015; Zhang et al., 2018; Yang et al., 2019). Different from representation-based approaches, Tu et al. (2018) and Kuang et al. (2018) propose using a cache to memorize context information, which can be either history hidden states or lexicons. To keep tracking of most recent contexts, the cache is updated when new translations are generated. Therefore, long-distance contexts would likely be erased. How to use long-distance contexts is drawing attention in recent years. Approaches, like treating the whole document as a long sentence (JunczysDowmunt, 201"
2021.emnlp-main.663,W17-4811,0,0.147345,"look at ⽶格尔 Miguel S 我 I 说 say ⽶格尔 Miguel 名字 name … … Figure 1: The structure of graph. Solid lines in blue depict adjacency relations. Dash lines in green denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicic"
2021.emnlp-main.663,2020.acl-main.322,0,0.0188567,"iterative phrases. 3) Ell. tests whether models correctly predict ellipsis verb phrases or the morphology of words. The Discourse test set consists of two probing tasks on En–Fr: 1) Coref. aims to test whether the gender of an anaphoric pronoun (it or they) is coherent with the previous sentence. 2) Cohe. is a set of ambiguous examples whose correct translations rely on the context. Result on Discourse Phenomena As shown in Table 5, all the context-aware models comprehensively improve the performance on discourse phenomena over the context-agnostic BASE model. Results on the the N OISE model (Li et al., 2020) indicate that the improvement is not merely because of robust training. Compared to prior contextaware models, our model achieves the best accuracy on all tasks. Especially on the Lex., Coref. and Cohe. tasks, our model outperforms others over two points. Note that on the ellipsis task graph edges are usually missing for elided verb phrases. For example, given the following source sentence and its context (Voita et al., 2019b), the verbs “told” and “did” are not directly connected in our graph but indirectly connected via the coreference relation of their neighbors “Nick” and “he”. Hence, our"
2021.emnlp-main.663,Q18-1029,0,0.123816,"Dash lines in green denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicich et al., 2018; Wang et al., 2017; Tan et al., 2019) or extra context encoders (Jean et al., 2015; Zhang et al., 2018; Yang"
2021.emnlp-main.663,2020.acl-main.321,0,0.0865574,"enote both the attention and gated residual mechanisms. In this paper, the Context-Attn sublayer is used in three different ways, as shown in Figure 3: • Hyb-integration: integrates the contextual information with an additional Context-Attn layer inside each encoder layer (Zhang et al., 2018). • Post-integration: aggregates the contextual information by adding a Context-Attn layer after the encoder (Tan et al., 2019; Miculicich et al., 2018; Maruf et al., 2019). • Pre-integration: interpolates the context representation before the encoder, which can be considered as the hierarchical embedded (Ma et al., 2020). 3 Experiments Data We evaluate our approach on translation benchmarks with different corpus size: (1) IWSLT En–Fr and Zh–En translation tasks (Cettolo et al., 2012) with around 200K sentence pairs for training. Following convention (Wang et al., 2017; Miculicich et al., 2018; Zhang et al., 2018), both language pairs take dev2010 as the development set. tst2010 is used for testing on En–Fr and tst2010∼tst2013 on Zh–En. (2) Opensubtitle2018 En–Ru translation corpus released by Voita et al. (2018), which contains 6M sentence pairs for training, among which 1.5M sentence pairs have context sente"
2021.emnlp-main.663,D19-1081,0,0.119533,"en denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicich et al., 2018; Wang et al., 2017; Tan et al., 2019) or extra context encoders (Jean et al., 2015; Zhang et al., 2018; Yang et al., 2019). Diff"
2021.emnlp-main.663,P19-1116,0,0.123822,"en denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicich et al., 2018; Wang et al., 2017; Tan et al., 2019) or extra context encoders (Jean et al., 2015; Zhang et al., 2018; Yang et al., 2019). Diff"
2021.emnlp-main.663,P18-1117,0,0.0800035,"ines in blue depict adjacency relations. Dash lines in green denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicich et al., 2018; Wang et al., 2017; Tan et al., 2019) or extra context encoders (Jean"
2021.emnlp-main.663,D17-1301,1,0.892888,"ly 他的 his S 我 I 看着 look at ⽶格尔 Miguel S 我 I 说 say ⽶格尔 Miguel 名字 name … … Figure 1: The structure of graph. Solid lines in blue depict adjacency relations. Dash lines in green denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hi"
2021.findings-acl.137,N19-1423,0,0.0622221,"Missing"
2021.findings-acl.137,2020.emnlp-main.498,0,0.188847,"Missing"
2021.findings-acl.137,D19-1419,0,0.0442042,"Missing"
2021.findings-acl.137,D19-1423,0,0.13923,"Missing"
2021.findings-acl.137,P19-1561,0,0.0161835,"atus quo. However, recent works have shown the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,"
2021.findings-acl.137,P19-1103,0,0.365226,"ts at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 August 1–6, 2021. ©2021 Association for Computational Linguistics Li et al., 2020; Tan et al., 2020; Yin et al., 2020; Zheng et al., 2020; Zou et al., 2020; Wang et al., 2020b). ADA is generally applicable to any type of adversarial attacks but is not very effective in improving model performance under attacks. In this work, we aim to improve ADA and devise a general defense strategy to effectively improve model robustness during finetuning.1 ADA has two majo"
2021.findings-acl.137,2021.findings-acl.56,1,0.33783,"sents the resultant decision boundary. AMDA helps achieve a more robust decision boundary. Pretrained language models (PLMs) have established state-of-the-art results on various NLP tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020) and the pretraining-then-finetuning paradigm has become the status quo. However, recent works have shown the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmen"
2021.findings-acl.137,2020.acl-main.245,0,0.341757,"cent works have shown the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 Aug"
2021.findings-acl.137,D13-1170,0,0.048696,"Missing"
2021.findings-acl.137,2020.acl-main.263,0,0.275487,"et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 August 1–6, 2021. ©2021 Association for Computational Linguistics Li et al., 2020; Tan et al., 2020; Yin et al., 2020; Zheng et al., 2020; Zou et al., 2020; Wang et al., 2020b). ADA is generally applicable to any type of adversarial attacks but is not very effective in improving model performance under attacks. In this work, we aim to improve ADA and devise a general defense strategy to effectively improve model robustness during finetuning.1 ADA has two major limitations for NLP models. Firstly, unlike images, it is harder to create new augmented textual data due to their discrete nature. Moreover, for textual adversarial attacks, the attack search space is prohibitively large. For example"
2021.findings-acl.137,2020.emnlp-main.500,0,0.399236,"nt decision boundary. AMDA helps achieve a more robust decision boundary. Pretrained language models (PLMs) have established state-of-the-art results on various NLP tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020) and the pretraining-then-finetuning paradigm has become the status quo. However, recent works have shown the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), whi"
2021.findings-acl.137,2020.emnlp-main.495,0,0.0772852,"e robust decision boundary. Pretrained language models (PLMs) have established state-of-the-art results on various NLP tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020) and the pretraining-then-finetuning paradigm has become the status quo. However, recent works have shown the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial"
2021.findings-acl.137,2020.emnlp-main.417,0,0.0523825,"Missing"
2021.findings-acl.137,2020.acl-main.310,0,0.0965234,"et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 August 1–6, 2021. ©2021 Association for Computational Linguistics Li et al., 2020; Tan et al., 2020; Yin et al., 2020; Zheng et al., 2020; Zou et al., 2020; Wang et al., 2020b). ADA is generally applicable to any type of adversarial attacks but is not very effective in improving model performance under attacks. In this work, we aim to improve ADA and devise a general defense strategy to effectively improve model robustness during finetuning.1 ADA has two major limitations for NLP models. Firstly, unlike images, it is harder to create new augmented textual data due to their discrete nature. Moreover, for textual adversarial attacks, the attack search space is prohibitively large. For example, the search space"
2021.findings-acl.137,2020.coling-main.4,1,0.778555,"n the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 August 1–6, 2021. ©20"
2021.findings-acl.137,2020.acl-main.540,1,0.905788,"Missing"
2021.findings-acl.137,P11-1015,0,0.0143693,"Missing"
2021.findings-acl.137,2020.acl-main.590,0,0.0456194,"Missing"
2021.findings-acl.137,2020.acl-main.319,0,0.170701,"ted in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 August 1–6, 2021. ©2021 Association for Computational Linguistics Li et al., 2020; Tan et al., 2020; Yin et al., 2020; Zheng et al., 2020; Zou et al., 2020; Wang et al., 2020b). ADA is generally applicable to any type of adversarial attacks but is not very effective in improving model performance under attacks. In this work, we aim to improve ADA and devise a general defense strategy to effectively improve model robustness during finetuning.1 ADA has two major limitations for NLP models. Firstly, unlike images, it is harder to create new augmented textual data due to their discrete nature. Moreover, for textual adversarial attacks, the attack search space is prohibitively large. For example, the search space of word-substitution attacks consists"
2021.findings-acl.139,P17-1120,0,0.0210758,"ialog turns (Williams et al., 2013; Henderson et al., 2014). The states are converted into a representation of constraints based on different schemes to query the databases (El Asri et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020; Zhu et al., 2020). The entry matching results are then used to generate the system response. With the development of intelligent assistants, the system should have a good command of massive external knowledge to better accomplish complicated user goals and improve user satisfaction. To realize this, some researchers (Zhao et al., 2017; Yu et al., 2017; Akasaki and Kaji, 2017) equip the system with chatting capability to address both task and non-task content in TODs. Other studies apply knowledge graph (Liao et al., 2019; Yang et al., 2020) or tables via SQL (Yu et al., 2019) to enrich the knowledge of TOD systems. However, all these studies are still limited in dialog modeling grounded on structured knowledge. There are a couple of studies to integrate unstructured knowledge into TOD modeling recently. Kim et al. (2020) introduce knowledge snippets to answer follow-up questions out of the coverage of databases. Feng et al. (2020) formulate document-grounded dialo"
2021.findings-acl.139,W05-0909,0,0.188612,"LABES-S2S TRADE + BDA UniConv + BDA LABES-S2S + BDA HyKnow (Single) - w/o Joint Optim HyKnow (Multiple) - w/o Joint Optim TripPy SimpleTOD TripPy + BDA SimpleTOD + BDA Model Inform Success BLEU Combined UniConv 84.2 71.8 19.0 97.3 LABES-S2S 83.6 74.2 18.3 97.2 UniConv + BDA 85.8 73.9 19.3 99.4 LABES-S2S + BDA 85.0 75.3 18.9 99.1 HyKnow 87.2 76.5 19.5 101.4 SimpleTOD 87.5 76.4 16.3 98.3 SimpleTOD + BDA 89.0 77.2 17.0 100.1 Table 2: Context-to-response generation results on modified MultiWOZ 2.1. All symbols and markings have the same meaning as in Table 1. BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-L (Lin, 2004). Moreover, we use Combined score computed by (Inf orm + Success) × 0.5 + BLEU for overall evaluation, as suggested by Eric et al. (2020). We find that HyKnow has better task completion rate than the light-weight E2E TOD models, which is comparable with SimpleTOD who uses large-scale pretrained GPT-2. It also generates responses with better language quality compared to all the E2E models. This is because our extended belief state can distinguish whether a dialog turn is grounded on structured or unstructured knowledge, which avoids the confusion between handling the two"
2021.findings-acl.139,D18-1547,0,0.24845,"jointly optimize dialog modeling grounded on the two kinds of knowledge. • Experimental results show that HyKnow has strong performance in dialog modeling grounded on hybrid knowledge.1 2 Related Work TOD systems usually use belief tracking, i.e. dialog state tracking (DST) to trace the user goals, i.e. be1 The code is available at https://github.com/ truthless11/HyKnow lief states, through multiple dialog turns (Williams et al., 2013; Henderson et al., 2014). The states are converted into a representation of constraints based on different schemes to query the databases (El Asri et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020; Zhu et al., 2020). The entry matching results are then used to generate the system response. With the development of intelligent assistants, the system should have a good command of massive external knowledge to better accomplish complicated user goals and improve user satisfaction. To realize this, some researchers (Zhao et al., 2017; Yu et al., 2017; Akasaki and Kaji, 2017) equip the system with chatting capability to address both task and non-task content in TODs. Other studies apply knowledge graph (Liao et al., 2019; Yang et al., 2020) or tables via SQL (Yu et al.,"
2021.findings-acl.139,N19-1423,0,0.00482968,"d from GPT-2 (Radford et al., 2019). All three E2E models only manage structured knowledge (database) in their TOD modeling. In addition to E2E TOD models, we also compare HyKnow with existing DST models in the belief tracking evaluation. Specifically, we use TRADE (Wu et al., 2019) and TripPy (Heck et al., 2020) as two DST baselines, which are representative BERT-free and BERT-based DST models, respectively. Unstructured Knowledge Management Models. We first compare our system with Beyond Domain APIs (BDA) (Kim et al., 2020). This baseline model uses two classification modules based on BERT (Devlin et al., 2019) to detect unstructured knowledge-grounded dialog turns and retrieve relevant documents, respectively. Moreover, we use standard information retrieval (IR) systems TFIDF (Manning et al., 2008) and BM25 (Robertson and Zaragoza, 2009) as the other two baseline models. Combinations. We combine the unstructured knowledge management model BDA with every DST or E2E TOD model. Specifically, BDA detects dialog turns that are grounded on unstructured knowledge, and uses a fine-tuned GPT-2 to generate responses in these turns, based on the dialog context and retrieved documents. While the DST or E2E TOD"
2021.findings-acl.139,W17-5526,0,0.0534814,"Missing"
2021.findings-acl.139,2020.emnlp-main.652,0,0.0592158,"Missing"
2021.findings-acl.139,P16-1154,0,0.01249,"e the belief state enet and coder and the document encoder to encode B et B Dt Dt into hidden states henc and henc , respectively. Based on the hidden states of all the encoders and (3) Dt t Rt = Seq2Seq(r) (Ct |hB enc , henc , mt ), e where Encoder(b) and Encoder(d) denote the belief state encoder and the document encoder. Following previous TOD systems with Seq2Seq architectures (Lei et al., 2018; Liang et al., 2020; Zhang et al., 2020a,b), we use one-layer, bidirectional GRU as encoders and standard GRU as decoders. We also apply global attention (Bahdanau et al., 2015) and copy mechanism (Gu et al., 2016) in all the Seq2Seq processes, to improve the et and Rt . context-awareness of decoding B 4.4 (2) 4.2 4.3 the vector mt , we use the response decoder to generate the system response Rt , formulated as: Model Training HyKnow is optimized through supervised training. Specifically, each dialog turn in the training data is initially labeled with the original belief state and the relevant document. We extend the belief state label based on the domain, entity and extracted topic of the relevant document. Then the extended belief state label and the reference response are used to calculate the cross-"
2021.findings-acl.139,2020.emnlp-main.146,0,0.0442862,"erformance compared to existing TOD systems. It also outperforms the pipeline knowledge management schemes, with higher unstructured knowledge retrieval accuracy. 1 Figure 1: Illustration of task-oriented dialog modeling with hybrid knowledge management. Words in red and blue illustrate the new domain-slot-value triple and the topic of user utterance that we introduce into the belief state, respectively. Words in yellow illustrate the topics of documents that we extract through preprocessing. Introduction Recently, Task-Oriented Dialog (TOD) systems (Mehri et al., 2019; Zhang et al., 2020a,b; Le et al., 2020; Hosseini-Asl et al., 2020; Peng et al., 2020; Li et al., 2021) have achieved promising performance on accomplishing user goals. Most systems typically query structured knowledge such as tables and databases based on the user goals, and use the query results to guide the generation of system responses, as shown in the first dialog turn in Fig. 1. However, real-world task-oriented conversations often step into dialog turns which are grounded on unstructured knowledge (Feng et al., 2020), such as passages and documents. For example, as the * Equal † contribution. Corresponding author. second di"
2021.findings-acl.139,P18-1133,0,0.140656,"trieve relevant references for generating the response. To address our defined task, we propose a taskoriented dialog system with Hybrid Knowledge management (HyKnow). This model extends the belief state to handle TODs grounded on hybrid knowledge, and further uses the extended belief state to perform both database query and document retrieval, whose outputs are thereby used to generate the final response. We consider two implementations of our system, with different schemes of extended belief state decoding. Both implementations are in an end-to-end multi-stage sequence-tosequence (Seq2Seq) (Lei et al., 2018; Liang et al., 2020; Zhang et al., 2020a,b) framework, where dialog modeling grounded on the two kinds of knowledge can be jointly optimized. We evaluate our system on the modified version of MultiWOZ 2.1 (Kim et al., 2020) dataset, where dialogs are grounded on hybrid knowledge. Experimental results show that HyKnow outperforms existing TOD systems which do not leverage large pretrained language models, no matter whether they add extra unstructured knowledge management or not. It also has a higher accuracy in unstructured knowledge retrieval, compared to the pipeline knowledge management sch"
2021.findings-acl.139,2020.sigdial-1.4,0,0.020553,"Missing"
2021.findings-acl.139,W14-4337,0,0.162364,"ems. • We propose a TOD system HyKnow to address our proposed task. It extends the belief state to manage hybrid knowledge, and is the first end-toend model to jointly optimize dialog modeling grounded on the two kinds of knowledge. • Experimental results show that HyKnow has strong performance in dialog modeling grounded on hybrid knowledge.1 2 Related Work TOD systems usually use belief tracking, i.e. dialog state tracking (DST) to trace the user goals, i.e. be1 The code is available at https://github.com/ truthless11/HyKnow lief states, through multiple dialog turns (Williams et al., 2013; Henderson et al., 2014). The states are converted into a representation of constraints based on different schemes to query the databases (El Asri et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020; Zhu et al., 2020). The entry matching results are then used to generate the system response. With the development of intelligent assistants, the system should have a good command of massive external knowledge to better accomplish complicated user goals and improve user satisfaction. To realize this, some researchers (Zhao et al., 2017; Yu et al., 2017; Akasaki and Kaji, 2017) equip the system with chatting capa"
2021.findings-acl.139,W19-5921,0,0.070558,"ts show that HyKnow has strong end-to-end performance compared to existing TOD systems. It also outperforms the pipeline knowledge management schemes, with higher unstructured knowledge retrieval accuracy. 1 Figure 1: Illustration of task-oriented dialog modeling with hybrid knowledge management. Words in red and blue illustrate the new domain-slot-value triple and the topic of user utterance that we introduce into the belief state, respectively. Words in yellow illustrate the topics of documents that we extract through preprocessing. Introduction Recently, Task-Oriented Dialog (TOD) systems (Mehri et al., 2019; Zhang et al., 2020a,b; Le et al., 2020; Hosseini-Asl et al., 2020; Peng et al., 2020; Li et al., 2021) have achieved promising performance on accomplishing user goals. Most systems typically query structured knowledge such as tables and databases based on the user goals, and use the query results to guide the generation of system responses, as shown in the first dialog turn in Fig. 1. However, real-world task-oriented conversations often step into dialog turns which are grounded on unstructured knowledge (Feng et al., 2020), such as passages and documents. For example, as the * Equal † contr"
2021.findings-acl.139,P02-1040,0,0.111671,"etrics 1595 Model TRADE UniConv LABES-S2S TRADE + BDA UniConv + BDA LABES-S2S + BDA HyKnow (Single) - w/o Joint Optim HyKnow (Multiple) - w/o Joint Optim TripPy SimpleTOD TripPy + BDA SimpleTOD + BDA Model Inform Success BLEU Combined UniConv 84.2 71.8 19.0 97.3 LABES-S2S 83.6 74.2 18.3 97.2 UniConv + BDA 85.8 73.9 19.3 99.4 LABES-S2S + BDA 85.0 75.3 18.9 99.1 HyKnow 87.2 76.5 19.5 101.4 SimpleTOD 87.5 76.4 16.3 98.3 SimpleTOD + BDA 89.0 77.2 17.0 100.1 Table 2: Context-to-response generation results on modified MultiWOZ 2.1. All symbols and markings have the same meaning as in Table 1. BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-L (Lin, 2004). Moreover, we use Combined score computed by (Inf orm + Success) × 0.5 + BLEU for overall evaluation, as suggested by Eric et al. (2020). We find that HyKnow has better task completion rate than the light-weight E2E TOD models, which is comparable with SimpleTOD who uses large-scale pretrained GPT-2. It also generates responses with better language quality compared to all the E2E models. This is because our extended belief state can distinguish whether a dialog turn is grounded on structured or unstructured knowledge, which avoids the"
2021.findings-acl.139,2020.sigdial-1.35,0,0.444767,"nded on hybrid knowledge, and further uses the extended belief state to perform both database query and document retrieval, whose outputs are thereby used to generate the final response. We consider two implementations of our system, with different schemes of extended belief state decoding. Both implementations are in an end-to-end multi-stage sequence-tosequence (Seq2Seq) (Lei et al., 2018; Liang et al., 2020; Zhang et al., 2020a,b) framework, where dialog modeling grounded on the two kinds of knowledge can be jointly optimized. We evaluate our system on the modified version of MultiWOZ 2.1 (Kim et al., 2020) dataset, where dialogs are grounded on hybrid knowledge. Experimental results show that HyKnow outperforms existing TOD systems which do not leverage large pretrained language models, no matter whether they add extra unstructured knowledge management or not. It also has a higher accuracy in unstructured knowledge retrieval, compared to the pipeline knowledge management schemes. Our contributions are summarized as below: • We formulate a task of modeling TOD grounded on both structured and unstructured knowledge, to incorporate more domain knowledge into the TOD systems. • We propose a TOD sys"
2021.findings-acl.139,D14-1162,0,0.0843713,"Missing"
2021.findings-acl.139,W17-5505,0,0.0117876,"Know lief states, through multiple dialog turns (Williams et al., 2013; Henderson et al., 2014). The states are converted into a representation of constraints based on different schemes to query the databases (El Asri et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020; Zhu et al., 2020). The entry matching results are then used to generate the system response. With the development of intelligent assistants, the system should have a good command of massive external knowledge to better accomplish complicated user goals and improve user satisfaction. To realize this, some researchers (Zhao et al., 2017; Yu et al., 2017; Akasaki and Kaji, 2017) equip the system with chatting capability to address both task and non-task content in TODs. Other studies apply knowledge graph (Liao et al., 2019; Yang et al., 2020) or tables via SQL (Yu et al., 2019) to enrich the knowledge of TOD systems. However, all these studies are still limited in dialog modeling grounded on structured knowledge. There are a couple of studies to integrate unstructured knowledge into TOD modeling recently. Kim et al. (2020) introduce knowledge snippets to answer follow-up questions out of the coverage of databases. Feng et al"
2021.findings-acl.139,2020.tacl-1.19,1,0.797213,"two kinds of knowledge. • Experimental results show that HyKnow has strong performance in dialog modeling grounded on hybrid knowledge.1 2 Related Work TOD systems usually use belief tracking, i.e. dialog state tracking (DST) to trace the user goals, i.e. be1 The code is available at https://github.com/ truthless11/HyKnow lief states, through multiple dialog turns (Williams et al., 2013; Henderson et al., 2014). The states are converted into a representation of constraints based on different schemes to query the databases (El Asri et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020; Zhu et al., 2020). The entry matching results are then used to generate the system response. With the development of intelligent assistants, the system should have a good command of massive external knowledge to better accomplish complicated user goals and improve user satisfaction. To realize this, some researchers (Zhao et al., 2017; Yu et al., 2017; Akasaki and Kaji, 2017) equip the system with chatting capability to address both task and non-task content in TODs. Other studies apply knowledge graph (Liao et al., 2019; Yang et al., 2020) or tables via SQL (Yu et al., 2019) to enrich the knowledge of TOD sys"
2021.findings-acl.139,W13-4065,0,0.0186943,"ledge into the TOD systems. • We propose a TOD system HyKnow to address our proposed task. It extends the belief state to manage hybrid knowledge, and is the first end-toend model to jointly optimize dialog modeling grounded on the two kinds of knowledge. • Experimental results show that HyKnow has strong performance in dialog modeling grounded on hybrid knowledge.1 2 Related Work TOD systems usually use belief tracking, i.e. dialog state tracking (DST) to trace the user goals, i.e. be1 The code is available at https://github.com/ truthless11/HyKnow lief states, through multiple dialog turns (Williams et al., 2013; Henderson et al., 2014). The states are converted into a representation of constraints based on different schemes to query the databases (El Asri et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020; Zhu et al., 2020). The entry matching results are then used to generate the system response. With the development of intelligent assistants, the system should have a good command of massive external knowledge to better accomplish complicated user goals and improve user satisfaction. To realize this, some researchers (Zhao et al., 2017; Yu et al., 2017; Akasaki and Kaji, 2017) equip the"
2021.findings-acl.139,P19-1078,0,0.0119481,"seline E2E TOD models with different types of structures: UniConv (Le et al., 2020) uses a structured fusion (Mehri et al., 2019) design, LABES-S2S (Zhang et al., 2020a) uses a multistage Seq2Seq (Lei et al., 2018) architecture, and SimpleTOD (Hosseini-Asl et al., 2020) is based on a single auto-regressive language model initialized from GPT-2 (Radford et al., 2019). All three E2E models only manage structured knowledge (database) in their TOD modeling. In addition to E2E TOD models, we also compare HyKnow with existing DST models in the belief tracking evaluation. Specifically, we use TRADE (Wu et al., 2019) and TripPy (Heck et al., 2020) as two DST baselines, which are representative BERT-free and BERT-based DST models, respectively. Unstructured Knowledge Management Models. We first compare our system with Beyond Domain APIs (BDA) (Kim et al., 2020). This baseline model uses two classification modules based on BERT (Devlin et al., 2019) to detect unstructured knowledge-grounded dialog turns and retrieve relevant documents, respectively. Moreover, we use standard information retrieval (IR) systems TFIDF (Manning et al., 2008) and BM25 (Robertson and Zaragoza, 2009) as the other two baseline mode"
2021.findings-acl.139,2020.emnlp-main.147,0,0.0399803,"Missing"
2021.findings-acl.139,D19-1204,0,0.0250975,"al., 2018; Rastogi et al., 2020; Zhu et al., 2020). The entry matching results are then used to generate the system response. With the development of intelligent assistants, the system should have a good command of massive external knowledge to better accomplish complicated user goals and improve user satisfaction. To realize this, some researchers (Zhao et al., 2017; Yu et al., 2017; Akasaki and Kaji, 2017) equip the system with chatting capability to address both task and non-task content in TODs. Other studies apply knowledge graph (Liao et al., 2019; Yang et al., 2020) or tables via SQL (Yu et al., 2019) to enrich the knowledge of TOD systems. However, all these studies are still limited in dialog modeling grounded on structured knowledge. There are a couple of studies to integrate unstructured knowledge into TOD modeling recently. Kim et al. (2020) introduce knowledge snippets to answer follow-up questions out of the coverage of databases. Feng et al. (2020) formulate document-grounded dialog for information seeking tasks. However, they only focus on dialog turns grounded on unstructured knowledge instead. In this paper, we aims to fill the gap of managing domain-specific knowledge with vari"
2021.findings-acl.139,2020.emnlp-main.740,0,0.560606,"has strong end-to-end performance compared to existing TOD systems. It also outperforms the pipeline knowledge management schemes, with higher unstructured knowledge retrieval accuracy. 1 Figure 1: Illustration of task-oriented dialog modeling with hybrid knowledge management. Words in red and blue illustrate the new domain-slot-value triple and the topic of user utterance that we introduce into the belief state, respectively. Words in yellow illustrate the topics of documents that we extract through preprocessing. Introduction Recently, Task-Oriented Dialog (TOD) systems (Mehri et al., 2019; Zhang et al., 2020a,b; Le et al., 2020; Hosseini-Asl et al., 2020; Peng et al., 2020; Li et al., 2021) have achieved promising performance on accomplishing user goals. Most systems typically query structured knowledge such as tables and databases based on the user goals, and use the query results to guide the generation of system responses, as shown in the first dialog turn in Fig. 1. However, real-world task-oriented conversations often step into dialog turns which are grounded on unstructured knowledge (Feng et al., 2020), such as passages and documents. For example, as the * Equal † contribution. Correspondi"
2021.findings-acl.218,J82-2005,0,0.70061,"Missing"
2021.findings-acl.218,P19-1126,0,0.0179979,"). To bridge the length gap, Zhang et al. (2020) propose adaptive feature selection, while Dong et al. (2020a) and Liu et al. (2020) exploit the CTC-based (Graves et al., 2006) shrinking mechanism. Nevertheless, they do not explore in simultaneous scenarios, where encoding quality inevitably suffers because of lacking future information in unidirectional encoders. Simultaneous Translation. Previous studies on simultaneous translation focus on text-to-text scenarios (STT) (Cho and Esipova, 2016; Gu et al., 2017; Dalvi et al., 2018), where fixed policies (Ma et al., 2019) and adaptive policies (Arivazhagan et al., 2019; Zheng et al., 2019a,b) are proposed to decide when to read and write tokens. (Ma et al., 2019) proposed a simple yet effective strategy, Wait-K, based on a prefix-to-prefix framework. It first waits for the first k tokens, and then start to generate target tokens concurrently with the source stream. It achieves competitive performance in simultaneous translation (Zheng et al., 2019a, 2020). Traditional simultaneous speech-to-text translation (SST) mainly depends on the ASR segmentation and then performs NMT based on the stream2462 ST Encoder Acoustic Encoder Block 1 Weighted-Shrinking Operat"
2021.findings-acl.218,E17-1099,0,0.141232,"RealTranS avoids such errors and translates accurately. Introduction Simultaneous speech translation (SST) (F¨ugen et al., 2007; Oda et al., 2014; Ren et al., 2020) aims to translate speech in one language into text in another language concurrently. It is useful in many scenarios, like synchronous interpretation in international conferences, automatic caption for live videos, etc. However, prior studies either focus on full sentence speech translation (ST) (Berard et al., 2016; Weiss et al., 2017; Liu et al., 2019) or simultaneous text-to-text machine translation (STT) (Cho and Esipova, 2016; Gu et al., 2017; Dalvi et al., 2018) which takes a segmented output from an automatic speech recognition (ASR) system as input. Such two-stage models (i.e., cascaded models) inevitably introduce error propagation and also increase translation latency (see Figure 1). Ren et al. (2020) propose an end-to-end SST system called SimulSpeech, but they ignore the modality gap between speech and text, which is important for improving translation quality (Liu et al., 2020). In this paper, we propose RealTranS model for SST. To relieve the burden of our encoder (Wang et al., 2020c; Liu et al., 2020), we decouple it int"
2021.findings-acl.218,2020.acl-demos.34,0,0.0415772,"Missing"
2021.findings-acl.218,2020.emnlp-main.206,0,0.0461683,"Missing"
2021.findings-acl.218,2020.aacl-main.58,0,0.281733,"Layers Transformer Layers Conv1d Conv1d Conv2d+LN+ReLU Conv1d+LN+ReLU Conv2d+LN+ReLU Conv1d+LN+ReLU Blank-limited CTC Module Softmax ×2 (weighted average) ?? blank non-blank MultiLayer Perceptron Conv-Transformer Figure 2: Overall structure of the proposed RealTranS model. ing segmented chunks (Oda et al., 2014; IranzoS´anchez et al., 2020). There is little attention on end-to-end SST. Ren et al. (2020), to our knowledge, first propose an end-to-end model called SimulSpeech with multi-task learning and knowledge distillation, and apply the Wait-K strategy to perform simultaneous translation. Ma et al. (2020a) explore how to define a “token” in source speech, and then adapt methods from STT to SST. And Ma et al. (2020b) introduce a memory-augmented Transformer to tackle the streaming speech input. However, none of them investigate the modality gap between speech and text. 3 The RealTranS Model Our RealTranS follows the sequence-to-sequence architecture, which consists of an ST encoder and an ST decoder. The ST encoder is decoupled into three parts, an acoustic encoder, a weighted shrinking operation, and a semantic encoder, to gradually map speech inputs into semantic representation space of text"
2021.findings-acl.218,P14-2090,0,0.0993131,"ith the Wait-KStride-N strategy outperforms prior end-to-end models as well as cascaded models in diverse latency settings. ST Encoder ASR … 1 My mom was the …. I’m almost the …. NMT ST Decoder Soy casi el …. Mi madre era la …. (a) Cascaded Model (b) RealTranS Figure 1: An example for a cascaded model and our RealTranS. The ASR part in the cascaded model wrongly recognizes “My mom was” as “I’m almost”. The error is propagated to NMT and leads to a wrong translation. RealTranS avoids such errors and translates accurately. Introduction Simultaneous speech translation (SST) (F¨ugen et al., 2007; Oda et al., 2014; Ren et al., 2020) aims to translate speech in one language into text in another language concurrently. It is useful in many scenarios, like synchronous interpretation in international conferences, automatic caption for live videos, etc. However, prior studies either focus on full sentence speech translation (ST) (Berard et al., 2016; Weiss et al., 2017; Liu et al., 2019) or simultaneous text-to-text machine translation (STT) (Cho and Esipova, 2016; Gu et al., 2017; Dalvi et al., 2018) which takes a segmented output from an automatic speech recognition (ASR) system as input. Such two-stage mo"
2021.findings-acl.218,2020.acl-main.217,0,0.0369003,"k learning (Anastasopoulos and Chiang, 2018), knowledge distillation (Liu et al., 2019; Ren et al., 2020), data synthesis (Jia et al., 2019), self-supervised learning (Chen et al., 2020) and speech augmentation techniques like SpecAugment (Bahar et al., 2019) or speed perturbation (Stoian et al., 2020). Some studies focus on how to bridge the gap between different modalities (speech and text) or different modules (acoustic and semantic modeling). Wang et al. (2020b,c) propose a TCEN model and a curriculum pre-training technique to make sure the modules learn desired information, respectively. Salesky and Black (2020) explore phone features as intermediate representations to improve performance, while Dong et al. (2020b) use pre-trained BERT to guide the model to learn semantic knowledge. Modality Agnostic Meta-Leaning is also exploited for ST in Indurthi et al. (2019). To bridge the length gap, Zhang et al. (2020) propose adaptive feature selection, while Dong et al. (2020a) and Liu et al. (2020) exploit the CTC-based (Graves et al., 2006) shrinking mechanism. Nevertheless, they do not explore in simultaneous scenarios, where encoding quality inevitably suffers because of lacking future information in uni"
2021.findings-acl.218,D17-1145,0,0.0234792,"gradual downsampling and weighted shrinking. • We introduce a blank penalty and the Wait-KStride-N strategy to improve the performance in simultaneous translation scenarios. • Extensive experiments on public and widelyused datasets show the superiority of our RealTranS model and our Wait-K-Stride-N strategy in diverse latency settings. 2 Related Work Speech Translation. Speech translation (ST) has recently attracted intensive attention from the AI community. Earlier works are mostly based on cascaded models, which perform NMT on the outputs of ASR systems (Ney, 1999; Mathias and Byrne, 2006; Sperber et al., 2017; Bahar et al., 2021). Cascaded models inevitably introduce error propagation from ASR (Weiss et al., 2017). To avoid this problem and for better efficiency, end-to-end ST models are proposed and become popular in recent years (Berard et al., 2016, 2018; Bansal et al., 2018; Gangi et al., 2019). To alleviate the data scarcity problem of end-to-end ST models, various techniques are utilized, including pre-training (Bansal et al., 2019), multi-task learning (Anastasopoulos and Chiang, 2018), knowledge distillation (Liu et al., 2019; Ren et al., 2020), data synthesis (Jia et al., 2019), self-supe"
2021.findings-acl.218,2020.findings-emnlp.230,0,0.0292469,"e studies focus on how to bridge the gap between different modalities (speech and text) or different modules (acoustic and semantic modeling). Wang et al. (2020b,c) propose a TCEN model and a curriculum pre-training technique to make sure the modules learn desired information, respectively. Salesky and Black (2020) explore phone features as intermediate representations to improve performance, while Dong et al. (2020b) use pre-trained BERT to guide the model to learn semantic knowledge. Modality Agnostic Meta-Leaning is also exploited for ST in Indurthi et al. (2019). To bridge the length gap, Zhang et al. (2020) propose adaptive feature selection, while Dong et al. (2020a) and Liu et al. (2020) exploit the CTC-based (Graves et al., 2006) shrinking mechanism. Nevertheless, they do not explore in simultaneous scenarios, where encoding quality inevitably suffers because of lacking future information in unidirectional encoders. Simultaneous Translation. Previous studies on simultaneous translation focus on text-to-text scenarios (STT) (Cho and Esipova, 2016; Gu et al., 2017; Dalvi et al., 2018), where fixed policies (Ma et al., 2019) and adaptive policies (Arivazhagan et al., 2019; Zheng et al., 2019a,b)"
2021.findings-acl.218,2020.acl-main.254,0,0.0285799,"Missing"
2021.findings-acl.218,D19-1137,0,0.112411,"e a blank-limited CTC loss, which adds a blank penalty to the traditional CTC loss to encourage the model to produce non-blank labels, given the observation that CTC tends to produce peaky distribution as a kind of overfitting (Liu et al., 2018) by over predicting blank labels. Accordingly, the shrinking quality can be improved. Furthermore, we propose a new simultaneous strategy Wait-K-Stride-N which allows local reranking during decoding. This strategy can resolve the inherent drawback of the conventional Wait-K strategy (Ma et al., 2019), which cannot apply vanilla beam search efficiently (Zheng et al., 2019c). Experiments on Augmented LibriSpeech En–Fr, MUST-C En–Es and En–De datasets demonstrate the effectiveness of the Wait-K-Stride-N strategy, and show that RealTranS achieves better performance than the prior end-to-end model SimulSpeech (Ren et al., 2020) as well as the cascaded models. Further analysis and ablation study reveal the effects of our proposed modules in RealTranS. We also compare RealTranS with other methods on full sentence ST. Results show that RealTranS achieves competitive or even better results, indicating its superiority. In summary, the contributions of this work include"
2021.findings-acl.218,P19-1582,0,0.318636,"e a blank-limited CTC loss, which adds a blank penalty to the traditional CTC loss to encourage the model to produce non-blank labels, given the observation that CTC tends to produce peaky distribution as a kind of overfitting (Liu et al., 2018) by over predicting blank labels. Accordingly, the shrinking quality can be improved. Furthermore, we propose a new simultaneous strategy Wait-K-Stride-N which allows local reranking during decoding. This strategy can resolve the inherent drawback of the conventional Wait-K strategy (Ma et al., 2019), which cannot apply vanilla beam search efficiently (Zheng et al., 2019c). Experiments on Augmented LibriSpeech En–Fr, MUST-C En–Es and En–De datasets demonstrate the effectiveness of the Wait-K-Stride-N strategy, and show that RealTranS achieves better performance than the prior end-to-end model SimulSpeech (Ren et al., 2020) as well as the cascaded models. Further analysis and ablation study reveal the effects of our proposed modules in RealTranS. We also compare RealTranS with other methods on full sentence ST. Results show that RealTranS achieves competitive or even better results, indicating its superiority. In summary, the contributions of this work include"
2021.findings-acl.218,2020.aacl-demo.6,0,0.196336,"translation (STT) (Cho and Esipova, 2016; Gu et al., 2017; Dalvi et al., 2018) which takes a segmented output from an automatic speech recognition (ASR) system as input. Such two-stage models (i.e., cascaded models) inevitably introduce error propagation and also increase translation latency (see Figure 1). Ren et al. (2020) propose an end-to-end SST system called SimulSpeech, but they ignore the modality gap between speech and text, which is important for improving translation quality (Liu et al., 2020). In this paper, we propose RealTranS model for SST. To relieve the burden of our encoder (Wang et al., 2020c; Liu et al., 2020), we decouple it into three parts: acoustic encoder, weighted-shrinking operation, and semantic encoder. We apply ConvTransformer (Huang et al., 2020) as our acoustic encoder, which gradually downsamples the input speech and learns acoustic information with interleaved convolution and Transformer layers. The weighted-shrinking operation bridges the length gap between speech and text, by weighted summing up the frames in one detected segment based on the posterior probabilities generated by a CTC module (Graves et al., 2006). Finally, we use a semantic encoder to extract sem"
2021.findings-acl.218,D19-1144,0,0.160791,"e a blank-limited CTC loss, which adds a blank penalty to the traditional CTC loss to encourage the model to produce non-blank labels, given the observation that CTC tends to produce peaky distribution as a kind of overfitting (Liu et al., 2018) by over predicting blank labels. Accordingly, the shrinking quality can be improved. Furthermore, we propose a new simultaneous strategy Wait-K-Stride-N which allows local reranking during decoding. This strategy can resolve the inherent drawback of the conventional Wait-K strategy (Ma et al., 2019), which cannot apply vanilla beam search efficiently (Zheng et al., 2019c). Experiments on Augmented LibriSpeech En–Fr, MUST-C En–Es and En–De datasets demonstrate the effectiveness of the Wait-K-Stride-N strategy, and show that RealTranS achieves better performance than the prior end-to-end model SimulSpeech (Ren et al., 2020) as well as the cascaded models. Further analysis and ablation study reveal the effects of our proposed modules in RealTranS. We also compare RealTranS with other methods on full sentence ST. Results show that RealTranS achieves competitive or even better results, indicating its superiority. In summary, the contributions of this work include"
2021.findings-acl.218,2020.acl-main.344,0,0.112486,"translation (STT) (Cho and Esipova, 2016; Gu et al., 2017; Dalvi et al., 2018) which takes a segmented output from an automatic speech recognition (ASR) system as input. Such two-stage models (i.e., cascaded models) inevitably introduce error propagation and also increase translation latency (see Figure 1). Ren et al. (2020) propose an end-to-end SST system called SimulSpeech, but they ignore the modality gap between speech and text, which is important for improving translation quality (Liu et al., 2020). In this paper, we propose RealTranS model for SST. To relieve the burden of our encoder (Wang et al., 2020c; Liu et al., 2020), we decouple it into three parts: acoustic encoder, weighted-shrinking operation, and semantic encoder. We apply ConvTransformer (Huang et al., 2020) as our acoustic encoder, which gradually downsamples the input speech and learns acoustic information with interleaved convolution and Transformer layers. The weighted-shrinking operation bridges the length gap between speech and text, by weighted summing up the frames in one detected segment based on the posterior probabilities generated by a CTC module (Graves et al., 2006). Finally, we use a semantic encoder to extract sem"
2021.findings-acl.241,D18-1549,0,0.0222504,"ame number of merge operations as the source domain on target domain monolingual data. Table 8 shows that our approach can surpass the baselines, especially with the child (medical domain) vocabulary. 6 Related Work Low-resource NMT has been researched from many perspectives. Exploiting auxiliary data has been verified to be helpful by various approaches, including data augmentation like back-translation (Sennrich et al., 2016a; Xia et al., 2019), transfer learning as focused in our work, meta-learning (Gu et al., 2018), semi-supervised learning (Cheng et al., 2016), or even unsupervised NMT (Artetxe et al., 2018; Lample et al., 2018b; Chronopoulou et al., 2020). Transfer learning usually utilizes a single source of knowledge. When multiple sources are available, transfer learning may be applied in a cascaded fashion (Lakew et al., 2018), but catastrophic forgetting may need to be addressed. Maimaiti et al. (2019) proposed multi-round transfer by performing transfer learning for several rounds on multiple high-resource language pairs. Multilingual NMT (Johnson et al., 2017; Dabre et al., 2019) aims to perform translation for multiple translation pairs in a single model, and positive transfer towards l"
2021.findings-acl.241,2020.acl-main.421,0,0.0343262,"ion (Lakew et al., 2018), but catastrophic forgetting may need to be addressed. Maimaiti et al. (2019) proposed multi-round transfer by performing transfer learning for several rounds on multiple high-resource language pairs. Multilingual NMT (Johnson et al., 2017; Dabre et al., 2019) aims to perform translation for multiple translation pairs in a single model, and positive transfer towards low-resource language pairs 2733 typically occurs. In our experiment, we have considered a variant that solely focuses on the lowresource pair (Kocmi and Bojar, 2018; Nguyen and Chiang, 2017). Outside NMT, Artetxe et al. (2020) proposed a similar partial freezing approach to transferring BERT cross-lingually. As they worked on BERT (Transformer encoder) for natural language understanding tasks, several differences from our work arise. First, we need to consider the initialization of decoder for NMT, and for the shared source case, we need to deal with vocabulary mismatch on the decoder side. Second, we find that additionally transferring position embeddings is not helpful in our experiments. Third, our approach can outperform BBERT transfer, whereas they observe slightly lower performance in their experiments. 7 the"
2021.findings-acl.241,W19-5206,0,0.0150485,"omain adaptation results. The transfer parameters are word embeddings for dual transfer. “Parent” indicates using source domain (news) vocabulary, and “child” indicates using target domain (medical) vocabulary. We then use the same synthetic parallel data for en→et, turning to the case of BT. The upper rows in Table 7 show that BT is highly beneficial for both the baseline and our approach. Encouraged by this, we further try using all 130m et monolingual data with the maximum of 80 tokens and 100 subwords per line. We upsample authentic data to have a 1:4 ratio with synthetic data, following (Caswell et al., 2019). The lower rows in Table 7 show that more BT data can further improve the “no transfer” baseline, though the small improvement appears unattractive considering the cost. As for our approach, going from 4m to 130m yields no gain. Besides, our approach with 4m BT still surpasses no transfer with 130m BT. We conjecture that our approach can work complementarily with a manageable amount of BT data, reducing the need to decode and train on a huge data size. Finally, note that we use the “no transfer” NMTet→en to generate all synthetic parallel data in our experiments. In practice, the model produc"
2021.findings-acl.241,P16-1185,0,0.0105857,"t domain. In this case, we learn BPE with the same number of merge operations as the source domain on target domain monolingual data. Table 8 shows that our approach can surpass the baselines, especially with the child (medical domain) vocabulary. 6 Related Work Low-resource NMT has been researched from many perspectives. Exploiting auxiliary data has been verified to be helpful by various approaches, including data augmentation like back-translation (Sennrich et al., 2016a; Xia et al., 2019), transfer learning as focused in our work, meta-learning (Gu et al., 2018), semi-supervised learning (Cheng et al., 2016), or even unsupervised NMT (Artetxe et al., 2018; Lample et al., 2018b; Chronopoulou et al., 2020). Transfer learning usually utilizes a single source of knowledge. When multiple sources are available, transfer learning may be applied in a cascaded fashion (Lakew et al., 2018), but catastrophic forgetting may need to be addressed. Maimaiti et al. (2019) proposed multi-round transfer by performing transfer learning for several rounds on multiple high-resource language pairs. Multilingual NMT (Johnson et al., 2017; Dabre et al., 2019) aims to perform translation for multiple translation pairs in"
2021.findings-acl.241,2020.emnlp-main.214,0,0.040306,"domain on target domain monolingual data. Table 8 shows that our approach can surpass the baselines, especially with the child (medical domain) vocabulary. 6 Related Work Low-resource NMT has been researched from many perspectives. Exploiting auxiliary data has been verified to be helpful by various approaches, including data augmentation like back-translation (Sennrich et al., 2016a; Xia et al., 2019), transfer learning as focused in our work, meta-learning (Gu et al., 2018), semi-supervised learning (Cheng et al., 2016), or even unsupervised NMT (Artetxe et al., 2018; Lample et al., 2018b; Chronopoulou et al., 2020). Transfer learning usually utilizes a single source of knowledge. When multiple sources are available, transfer learning may be applied in a cascaded fashion (Lakew et al., 2018), but catastrophic forgetting may need to be addressed. Maimaiti et al. (2019) proposed multi-round transfer by performing transfer learning for several rounds on multiple high-resource language pairs. Multilingual NMT (Johnson et al., 2017; Dabre et al., 2019) aims to perform translation for multiple translation pairs in a single model, and positive transfer towards low-resource language pairs 2733 typically occurs."
2021.findings-acl.241,Q17-1024,0,0.0696731,"Missing"
2021.findings-acl.241,D19-5611,0,0.0196342,"el data for several highresource languages is readily available. These corpora have been used in various methods to help training low-resource NMT. The most relevant method to our work is transfer learning. Transfer learning starts with training a source task and then initializes the target task with the parameters. Recent advances in pretrained language models (PLM) like BERT (Devlin et al., 2019) can be seen as transfer learning, where language modeling is the source task for downstream target tasks. In low-resource NMT, pretrained language models have also provided noticeable improvements (Clinchant et al., 2019; Imamura and Sumita, 2019). As another source of transfer, high-resource NMT models have also been used for transfer learning low-resource NMT. Zoph et al. (2016) pioneered this direction with NMT based on recurrent neural networks, and coined the high-resource and low-resource models as parent and child models, respectively. However, it is non-trivial to transfer from both PLMs and NMT models. This limitation constrains most existing transfer-learning-based low-resource NMT to a single source of auxiliary data, either monolingual or parallel. In this paper, we propose a framework for transfe"
2021.findings-acl.241,P19-1120,0,0.16327,"iary data, including both high-resource parallel data and monolingual data of involved languages. Knowledge in those sources is respectively encoded in a high-resource translation model and pretrained language models, and dually transferred to the low-resource translation model by our approach. Extensive experiments show that our approach yields consistent improvements over strong competitors for multiple translation directions. Furthermore, our approach still exhibits benefit on top of back-translation, making it a useful addition to practitioners’ toolbox. 1 no transfer (Zoph et al., 2016) (Kim et al., 2019) BERT 2 RND BERT 2 BERT (Kocmi and Bojar, 2018) BBERT 2 BBERT BBERT transfer dual transfer (ours) H M L P ! ! M ! ! ! ! ! ! ! ! ! ! ! P ! ! ! ! ! ! ! ! ! Table 1: An overview of data usage by approaches considered in this work (Section 4.3). H/L: high/lowresource language pair; M: monolingual; P: parallel. BBERT transfer checks all the boxes but uses data in a different way from ours. Introduction Neural machine translation (NMT) has achieved remarkable success in recent years, but its quality critically hinges on large-scale parallel data. In the low-resource scenarios for most world language"
2021.findings-acl.241,D19-1146,0,0.0218848,"k, meta-learning (Gu et al., 2018), semi-supervised learning (Cheng et al., 2016), or even unsupervised NMT (Artetxe et al., 2018; Lample et al., 2018b; Chronopoulou et al., 2020). Transfer learning usually utilizes a single source of knowledge. When multiple sources are available, transfer learning may be applied in a cascaded fashion (Lakew et al., 2018), but catastrophic forgetting may need to be addressed. Maimaiti et al. (2019) proposed multi-round transfer by performing transfer learning for several rounds on multiple high-resource language pairs. Multilingual NMT (Johnson et al., 2017; Dabre et al., 2019) aims to perform translation for multiple translation pairs in a single model, and positive transfer towards low-resource language pairs 2733 typically occurs. In our experiment, we have considered a variant that solely focuses on the lowresource pair (Kocmi and Bojar, 2018; Nguyen and Chiang, 2017). Outside NMT, Artetxe et al. (2020) proposed a similar partial freezing approach to transferring BERT cross-lingually. As they worked on BERT (Transformer encoder) for natural language understanding tasks, several differences from our work arise. First, we need to consider the initialization of dec"
2021.findings-acl.241,N19-1423,0,0.0194129,"ormance usually deteriorates dramatically. Although parallel data for some translation tasks may be difficult to obtain, monolingual data is usually within reach, and often comes in much larger quantity. Besides, parallel data for several highresource languages is readily available. These corpora have been used in various methods to help training low-resource NMT. The most relevant method to our work is transfer learning. Transfer learning starts with training a source task and then initializes the target task with the parameters. Recent advances in pretrained language models (PLM) like BERT (Devlin et al., 2019) can be seen as transfer learning, where language modeling is the source task for downstream target tasks. In low-resource NMT, pretrained language models have also provided noticeable improvements (Clinchant et al., 2019; Imamura and Sumita, 2019). As another source of transfer, high-resource NMT models have also been used for transfer learning low-resource NMT. Zoph et al. (2016) pioneered this direction with NMT based on recurrent neural networks, and coined the high-resource and low-resource models as parent and child models, respectively. However, it is non-trivial to transfer from both P"
2021.findings-acl.241,D18-1398,0,0.017181,"s using a dedicated vocabulary for the target domain. In this case, we learn BPE with the same number of merge operations as the source domain on target domain monolingual data. Table 8 shows that our approach can surpass the baselines, especially with the child (medical domain) vocabulary. 6 Related Work Low-resource NMT has been researched from many perspectives. Exploiting auxiliary data has been verified to be helpful by various approaches, including data augmentation like back-translation (Sennrich et al., 2016a; Xia et al., 2019), transfer learning as focused in our work, meta-learning (Gu et al., 2018), semi-supervised learning (Cheng et al., 2016), or even unsupervised NMT (Artetxe et al., 2018; Lample et al., 2018b; Chronopoulou et al., 2020). Transfer learning usually utilizes a single source of knowledge. When multiple sources are available, transfer learning may be applied in a cascaded fashion (Lakew et al., 2018), but catastrophic forgetting may need to be addressed. Maimaiti et al. (2019) proposed multi-round transfer by performing transfer learning for several rounds on multiple high-resource language pairs. Multilingual NMT (Johnson et al., 2017; Dabre et al., 2019) aims to perfor"
2021.findings-acl.241,D19-5603,0,0.020705,"resource languages is readily available. These corpora have been used in various methods to help training low-resource NMT. The most relevant method to our work is transfer learning. Transfer learning starts with training a source task and then initializes the target task with the parameters. Recent advances in pretrained language models (PLM) like BERT (Devlin et al., 2019) can be seen as transfer learning, where language modeling is the source task for downstream target tasks. In low-resource NMT, pretrained language models have also provided noticeable improvements (Clinchant et al., 2019; Imamura and Sumita, 2019). As another source of transfer, high-resource NMT models have also been used for transfer learning low-resource NMT. Zoph et al. (2016) pioneered this direction with NMT based on recurrent neural networks, and coined the high-resource and low-resource models as parent and child models, respectively. However, it is non-trivial to transfer from both PLMs and NMT models. This limitation constrains most existing transfer-learning-based low-resource NMT to a single source of auxiliary data, either monolingual or parallel. In this paper, we propose a framework for transfer learning low-resource NMT"
2021.findings-acl.241,W18-6325,0,0.245939,"rallel data and monolingual data of involved languages. Knowledge in those sources is respectively encoded in a high-resource translation model and pretrained language models, and dually transferred to the low-resource translation model by our approach. Extensive experiments show that our approach yields consistent improvements over strong competitors for multiple translation directions. Furthermore, our approach still exhibits benefit on top of back-translation, making it a useful addition to practitioners’ toolbox. 1 no transfer (Zoph et al., 2016) (Kim et al., 2019) BERT 2 RND BERT 2 BERT (Kocmi and Bojar, 2018) BBERT 2 BBERT BBERT transfer dual transfer (ours) H M L P ! ! M ! ! ! ! ! ! ! ! ! ! ! P ! ! ! ! ! ! ! ! ! Table 1: An overview of data usage by approaches considered in this work (Section 4.3). H/L: high/lowresource language pair; M: monolingual; P: parallel. BBERT transfer checks all the boxes but uses data in a different way from ours. Introduction Neural machine translation (NMT) has achieved remarkable success in recent years, but its quality critically hinges on large-scale parallel data. In the low-resource scenarios for most world languages and many domains, its performance usually det"
2021.findings-acl.241,2020.eamt-1.3,0,0.0252869,"owever, one problem still persists. Because the high-resource languages have different vocabularies from the low-resource ones, directly transferring the word embedding layer is not possible. One way to circumvent this issue is to prepare a joint vocabulary of the involved languages that is shared between the parent and child NMT models (Kocmi and Bojar, 2018). Known as warm-start transfer (Neubig and Hu, 2018), this type of methods need to prepare a new joint vocabulary whenever a new low-resource model is on demand, and retrain both parent and child models. In contrast, cold-start transfer (Kocmi and Bojar, 2020) trains a universal parent NMT model that does not depend on child languages. Kim et al. (2019) addressed the vocabulary mismatch for cold-start transfer by matching word embeddings across languages. They first learn monolingual word embeddings of the child language with e.g. skip-gram (Mikolov et al., 2013), and then learn a cross-lingual linear mapping to connect child monolingual word embeddings and pretrained parent NMT word embeddings. The child monolingual word embeddings can then be mapped to the parent word embedding space, and be used to initialize the child NMT word embeddings. The c"
2021.findings-acl.241,2020.acl-main.703,0,0.0297794,"the encoder-decoder architecture (Sutskever et al., 2014; Bahdanau et al., 2015), the direct application of the “pretraining-finetuning” paradigm would be initializing the encoder with PLM and treating the decoder as task-specific layers. However, it is also possible to initialize the compatible modules in the decoder, leaving the cross attention module randomly initialized. Although initializing the decoder does not appear as useful, especially for high-resource language pairs (Rothe et al., 2020), it is not harmful either. 1 Examples of such models include MASS (Song et al., 2019) and BART (Lewis et al., 2020). If desired, pretrained encoders in these models can be used in our approach. 2.2 Transfer from High-Resource Translation Models Even though the Transformer model (Vaswani et al., 2017) has become more popular than recurrent neural networks for NMT, the transfer procedure proposed by Zoph et al. (2016) still applies as long as the parent model and the child model share the same architecture, which is typically the case. However, one problem still persists. Because the high-resource languages have different vocabularies from the low-resource ones, directly transferring the word embedding layer"
2021.findings-acl.241,2021.ccl-1.108,0,0.0432308,"Missing"
2021.findings-acl.241,2015.iwslt-evaluation.11,0,0.0190404,"hat our approach can work complementarily with a manageable amount of BT data, reducing the need to decode and train on a huge data size. Finally, note that we use the “no transfer” NMTet→en to generate all synthetic parallel data in our experiments. In practice, the model produced by our approach can be used for decoding, which should result in higher-quality synthetic data. This might also be the reason that ST hurts our approach more than the “no transfer” baseline. Domain Adaptation A simple and effective approach to domain adaptation is finetuning source domain NMT on target domain data (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). This approach is possible because directly inheriting parent NMT vocabulary is acceptable for domain adaptation. In other words, this is a special case of (Kocmi and Bojar, 2018) where child vocabulary largely overlaps with parent vocabulary. However, our approach allows using a dedicated vocabulary for the target domain. In this case, we learn BPE with the same number of merge operations as the source domain on target domain monolingual data. Table 8 shows that our approach can surpass the baselines, especially with the child (medical domain) vocabulary. 6 Rel"
2021.findings-acl.241,I17-2050,0,0.0147279,"rning may be applied in a cascaded fashion (Lakew et al., 2018), but catastrophic forgetting may need to be addressed. Maimaiti et al. (2019) proposed multi-round transfer by performing transfer learning for several rounds on multiple high-resource language pairs. Multilingual NMT (Johnson et al., 2017; Dabre et al., 2019) aims to perform translation for multiple translation pairs in a single model, and positive transfer towards low-resource language pairs 2733 typically occurs. In our experiment, we have considered a variant that solely focuses on the lowresource pair (Kocmi and Bojar, 2018; Nguyen and Chiang, 2017). Outside NMT, Artetxe et al. (2020) proposed a similar partial freezing approach to transferring BERT cross-lingually. As they worked on BERT (Transformer encoder) for natural language understanding tasks, several differences from our work arise. First, we need to consider the initialization of decoder for NMT, and for the shared source case, we need to deal with vocabulary mismatch on the decoder side. Second, we find that additionally transferring position embeddings is not helpful in our experiments. Third, our approach can outperform BBERT transfer, whereas they observe slightly lower per"
2021.findings-acl.241,W18-6319,0,0.0134492,". Previous works # sentence (pair) 5.9m 1.9m 207k 10k 347k 94m 147m 139m 100m 4.1m 4.2m 4.0m 3.6m Table 2: Training data statistics. mainly consider shared target transfer (Dabre et al., 2020), and we make extensive comparison in the experiment that transfers from de→en to et→en. We then verify on other translation directions, including shared source transfer, as well as general transfer, in which we consider an artificial setting of transferring from de→en to French→Spanish (fr→es). For domain adaptation we work on de→en, transferring from news domain to medical domain. We report SacreBLEU3 (Post, 2018). Further details about data and hyperparameters can be found in Appendices B and C, respectively. 4.1 Data We mainly use data from WMT 20184 . We use preprocessed parallel data for training NMT models. The provided development data includes multiparallel data for several languages, which we use for fr→es. We collect monolingual data for the involved languages and follow the same preprocessing pipeline. Training data statistics is provided in Table 2. Each language is encoded with byte pair encoding (BPE) (Sennrich et al., 2016b). The BPE codes and vocabularies are learned on each language’s m"
2021.findings-acl.241,E17-2025,0,0.0297721,"ed language model). In Step (3), NMTA→B needs to learn translation based on the frozen A and B word embeddings space. With P and Q word embeddings swapped in place in Step (4), the body and embedding parameters can cooperate in a close semantic space, allowing finetuning to proceed smoothly. Like (Kim et al., 2019), our approach solves the vocabulary mismatch issue by manipulation in the embedding space, allowing transfer between arbitrary languages, even with different scripts2 . Each language now manages its own independent vocabulary. We also tie input and output embeddings of the decoder (Press and Wolf, 2017), so a single decoder embedding block is shown in Figure 1. We can further generalize our approach by defining transfer parameters as those responsible for transforming input into continuous representations shared across languages. In Figure 1, the transfer parameters are simply word embeddings, but we may also use other sets of transfer parameters, e.g. word and position embeddings, or even lower layers of the body. In Step (2), only transfer parameters are trainable, while in Step (3), only nontransfer parameters are trainable, and initialization changes accordingly. 2 We verified the effect"
2021.findings-acl.241,2020.tacl-1.18,0,0.506785,"to simplify discussion1 . Common pretrained language models include BERT and GPT (Brown et al., 2020). In NMT with the encoder-decoder architecture (Sutskever et al., 2014; Bahdanau et al., 2015), the direct application of the “pretraining-finetuning” paradigm would be initializing the encoder with PLM and treating the decoder as task-specific layers. However, it is also possible to initialize the compatible modules in the decoder, leaving the cross attention module randomly initialized. Although initializing the decoder does not appear as useful, especially for high-resource language pairs (Rothe et al., 2020), it is not harmful either. 1 Examples of such models include MASS (Song et al., 2019) and BART (Lewis et al., 2020). If desired, pretrained encoders in these models can be used in our approach. 2.2 Transfer from High-Resource Translation Models Even though the Transformer model (Vaswani et al., 2017) has become more popular than recurrent neural networks for NMT, the transfer procedure proposed by Zoph et al. (2016) still applies as long as the parent model and the child model share the same architecture, which is typically the case. However, one problem still persists. Because the high-resou"
2021.findings-acl.241,P16-1009,0,0.410717,"e→en, transferring from news domain to medical domain. We report SacreBLEU3 (Post, 2018). Further details about data and hyperparameters can be found in Appendices B and C, respectively. 4.1 Data We mainly use data from WMT 20184 . We use preprocessed parallel data for training NMT models. The provided development data includes multiparallel data for several languages, which we use for fr→es. We collect monolingual data for the involved languages and follow the same preprocessing pipeline. Training data statistics is provided in Table 2. Each language is encoded with byte pair encoding (BPE) (Sennrich et al., 2016b). The BPE codes and vocabularies are learned on each language’s monolingual data, and then used to segment parallel data. Following (Kim et al., 2019), we use 50k merge operations for English, and 20k for other languages. Sentences with more than 150 subwords are removed from NMT training. 3 SacreBLEU signature: BLEU+case.mixed+numrefs.1+ smooth.exp+tok.13a+version.1.4.12. 4 http://statmt.org/wmt18/ translation-task.html 2729 4.2 Hyperparameters We use Transformer base as our NMT model, but with slight modifications that follow the implementation of BERT5 . The absolute position embeddings a"
2021.findings-acl.241,P16-1162,0,0.706306,"e→en, transferring from news domain to medical domain. We report SacreBLEU3 (Post, 2018). Further details about data and hyperparameters can be found in Appendices B and C, respectively. 4.1 Data We mainly use data from WMT 20184 . We use preprocessed parallel data for training NMT models. The provided development data includes multiparallel data for several languages, which we use for fr→es. We collect monolingual data for the involved languages and follow the same preprocessing pipeline. Training data statistics is provided in Table 2. Each language is encoded with byte pair encoding (BPE) (Sennrich et al., 2016b). The BPE codes and vocabularies are learned on each language’s monolingual data, and then used to segment parallel data. Following (Kim et al., 2019), we use 50k merge operations for English, and 20k for other languages. Sentences with more than 150 subwords are removed from NMT training. 3 SacreBLEU signature: BLEU+case.mixed+numrefs.1+ smooth.exp+tok.13a+version.1.4.12. 4 http://statmt.org/wmt18/ translation-task.html 2729 4.2 Hyperparameters We use Transformer base as our NMT model, but with slight modifications that follow the implementation of BERT5 . The absolute position embeddings a"
2021.findings-acl.241,P19-1579,0,0.0131181,"abulary largely overlaps with parent vocabulary. However, our approach allows using a dedicated vocabulary for the target domain. In this case, we learn BPE with the same number of merge operations as the source domain on target domain monolingual data. Table 8 shows that our approach can surpass the baselines, especially with the child (medical domain) vocabulary. 6 Related Work Low-resource NMT has been researched from many perspectives. Exploiting auxiliary data has been verified to be helpful by various approaches, including data augmentation like back-translation (Sennrich et al., 2016a; Xia et al., 2019), transfer learning as focused in our work, meta-learning (Gu et al., 2018), semi-supervised learning (Cheng et al., 2016), or even unsupervised NMT (Artetxe et al., 2018; Lample et al., 2018b; Chronopoulou et al., 2020). Transfer learning usually utilizes a single source of knowledge. When multiple sources are available, transfer learning may be applied in a cascaded fashion (Lakew et al., 2018), but catastrophic forgetting may need to be addressed. Maimaiti et al. (2019) proposed multi-round transfer by performing transfer learning for several rounds on multiple high-resource language pairs."
2021.findings-acl.241,D16-1160,0,0.0193436,"Missing"
2021.findings-acl.241,D16-1163,0,0.270847,"ple sources of auxiliary data, including both high-resource parallel data and monolingual data of involved languages. Knowledge in those sources is respectively encoded in a high-resource translation model and pretrained language models, and dually transferred to the low-resource translation model by our approach. Extensive experiments show that our approach yields consistent improvements over strong competitors for multiple translation directions. Furthermore, our approach still exhibits benefit on top of back-translation, making it a useful addition to practitioners’ toolbox. 1 no transfer (Zoph et al., 2016) (Kim et al., 2019) BERT 2 RND BERT 2 BERT (Kocmi and Bojar, 2018) BBERT 2 BBERT BBERT transfer dual transfer (ours) H M L P ! ! M ! ! ! ! ! ! ! ! ! ! ! P ! ! ! ! ! ! ! ! ! Table 1: An overview of data usage by approaches considered in this work (Section 4.3). H/L: high/lowresource language pair; M: monolingual; P: parallel. BBERT transfer checks all the boxes but uses data in a different way from ours. Introduction Neural machine translation (NMT) has achieved remarkable success in recent years, but its quality critically hinges on large-scale parallel data. In the low-resource scenarios for"
2021.findings-emnlp.195,2021.naacl-main.211,0,0.0525007,"Missing"
2021.findings-emnlp.195,N19-1423,0,0.00980117,"d scope of problems. 5.2 Pre-trained Language Model Pre-trained language models have obtained stateof-the-art results in many NLP benchmarks (Wang et al., 2018a, 2019a). These models are usually based on Transformer layers (Vaswani et al., 2017) and trained on large corpus with self-supervised tasks. According to their architectures, pre-trained Parsing-based methods. Later on, researchers language models can be categorized into three use statistical methods to solve MWP and achieve types: encoder-only, decoder-only and encodera great performance improvement. One line of decoder models. BERT (Devlin et al., 2019) is an research focuses on semantic parsing, which leverencoder-only model which firstly proposes masked ages traditional machine learning techniques to token prediction and next sentence prediction to identify entities, quantities, and operators from the train a language representation model. Followproblem text. Roy et al. (2015) proposes three ing this, many other models are proposed like types of classifiers to identify different elements of RoBERTa (Liu et al., 2019b) and SpanBERT (Joshi problems. ARIS (Hosseini et al., 2014) splits the et al., 2020). Decoder-only models are typically prob"
2021.findings-emnlp.195,D14-1058,0,0.0195048,"a great performance improvement. One line of decoder models. BERT (Devlin et al., 2019) is an research focuses on semantic parsing, which leverencoder-only model which firstly proposes masked ages traditional machine learning techniques to token prediction and next sentence prediction to identify entities, quantities, and operators from the train a language representation model. Followproblem text. Roy et al. (2015) proposes three ing this, many other models are proposed like types of classifiers to identify different elements of RoBERTa (Liu et al., 2019b) and SpanBERT (Joshi problems. ARIS (Hosseini et al., 2014) splits the et al., 2020). Decoder-only models are typically problem into fragments and updates a logic temauto-regressive models trained to estimate the probplate named state by verb categorization. Other ability distribution of a text corpus, including works (Sundaram and Khemani, 2015; Mitra and GPT2 (Radford et al., 2019), GPT3 (Brown et al., Baral, 2016; Liang et al., 2016) follow a similar 2020) and XLNet (Yang et al., 2019). Encoderprocess with different templates and annotations. decoder models like BART (Lewis et al., 2020) and Two-stage methods. Another research line first T5 (Raffel"
2021.findings-emnlp.195,2020.tacl-1.5,0,0.0397632,"Missing"
2021.findings-emnlp.195,Q15-1042,0,0.0510579,"Missing"
2021.findings-emnlp.195,N16-1136,0,0.0166782,"3.4 Model Inference We perform a two-stage model inference, namely generation and ranking. Specifically, given a new problem text sequence P , we first pass it to the encoder to get the problem representation R. Then we perform the beam search to generate top-K expressions. These generated expressions are used as candidate solutions for the ranker. All expressions are passed to the ranker and that with the highest score is selected as the final result. 4 Experiment 4.1 Experimental Setup Datasets. We conduct the experiments on two commonly-used datasets: Math23K (Wang et al., 2017) and MAWPS (Koncel-Kedziorski et al., 2016). Math23K is a large-scale Chinese dataset that contains 23,162 math word problems and their corresponding expression solutions. MAWPS is a English dataset containing 2,373 problems. All the problems are one-unknown-variable linear problems and can be solved with a single expression. Baselines. We compare our model with the following baselines including the state-of-the-art models: DNS (Wang et al., 2017) uses a vanilla Seq2Seq model to generate expressions. Math-EN (Wang et al., 2018b) uses the equation normalization to avoid equation duplication problem. T-RNN (Wang et al., 2019b) applies re"
2021.findings-emnlp.195,P14-1026,0,0.0316315,"named state by verb categorization. Other ability distribution of a text corpus, including works (Sundaram and Khemani, 2015; Mitra and GPT2 (Radford et al., 2019), GPT3 (Brown et al., Baral, 2016; Liang et al., 2016) follow a similar 2020) and XLNet (Yang et al., 2019). Encoderprocess with different templates and annotations. decoder models like BART (Lewis et al., 2020) and Two-stage methods. Another research line first T5 (Raffel et al., 2020) use the encoder-decoder arobtains an expression template then maps numbers chitecture and are trained on sequence-to-sequence to the template slots. Kushman et al. (2014) train tasks such as text denoising and translation. 2276 6 Conclusion and Future Work We propose Generate & Rank, a new multi-task framework for math word problems. Specifically, our model has a generator and a ranker which enhance each other with joint training. We also use tree-based disturbance and online update to further improve the performance. The experimental results on the benchmark show that our work consistently outperforms baselines in all datasets. In future work, we will explore the generation and ranking framework to other tasks like summarization and translation. Acknowledgeme"
2021.findings-emnlp.195,2020.acl-main.703,0,0.0566424,"rn at Huawei Noah’s Ark Lab a generator and a ranker. The former is designed *Corresponding author 1 to generate candidate expressions given a probCode could be found at https://github.com/ huawei-noah/noah-research lem text and the latter aims to rank the candidate 2269 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2269–2279 November 7–11, 2021. ©2021 Association for Computational Linguistics expressions. They are built based on an encoderdecoder model and are jointly trained with generation loss and ranking loss. In this work, we build our model based on BART (Lewis et al., 2020), a widely used pre-trained language model that achieves SOTA performance on various sequenceto-sequence tasks (Ahmad et al., 2021; Liu et al., 2020). During multi-task training, expressions produced by the generator are used to construct an expression bank and train the ranker, in which way the model can learn from its own mistakes. To construct more informative candidates for the ranker, we specially design tree-based disturbance for MWP. We also introduce an online update mechanism to generate a new set of candidate expressions at each training epoch. The overall training procedure is in an"
2021.findings-emnlp.195,P19-1619,0,0.0598477,"ll the problems are one-unknown-variable linear problems and can be solved with a single expression. Baselines. We compare our model with the following baselines including the state-of-the-art models: DNS (Wang et al., 2017) uses a vanilla Seq2Seq model to generate expressions. Math-EN (Wang et al., 2018b) uses the equation normalization to avoid equation duplication problem. T-RNN (Wang et al., 2019b) applies recursive neural networks to model the tree structures of expressions. SAligned (Chiang and Chen, 2019) tracks the semantic meanings of operands with a stack during decoding. Group-ATT (Li et al., 2019) leverages the attention mechanism to enrich problem representation. Both AST-Dec (Liu et al., 2019a) and GTS (Xie and Sun, 2019) develop a tree-based decoder to generate expressions. Graph2Tree (Zhang et al., 2020) proposes to build a quantity cell graph and a comparison graph to better capture the quantity relationships of the problem. Multi-E/D (Shen and Jin, 2020) is an ensemble model which combines multiple encoders and decoders. Implementation Details. We use the PyTorch2 implementations and pre-trained language models provided by the Transformers library3 . Since the Math23K dataset is"
2021.findings-emnlp.195,N16-3014,0,0.0157238,"llowproblem text. Roy et al. (2015) proposes three ing this, many other models are proposed like types of classifiers to identify different elements of RoBERTa (Liu et al., 2019b) and SpanBERT (Joshi problems. ARIS (Hosseini et al., 2014) splits the et al., 2020). Decoder-only models are typically problem into fragments and updates a logic temauto-regressive models trained to estimate the probplate named state by verb categorization. Other ability distribution of a text corpus, including works (Sundaram and Khemani, 2015; Mitra and GPT2 (Radford et al., 2019), GPT3 (Brown et al., Baral, 2016; Liang et al., 2016) follow a similar 2020) and XLNet (Yang et al., 2019). Encoderprocess with different templates and annotations. decoder models like BART (Lewis et al., 2020) and Two-stage methods. Another research line first T5 (Raffel et al., 2020) use the encoder-decoder arobtains an expression template then maps numbers chitecture and are trained on sequence-to-sequence to the template slots. Kushman et al. (2014) train tasks such as text denoising and translation. 2276 6 Conclusion and Future Work We propose Generate & Rank, a new multi-task framework for math word problems. Specifically, our model has a"
2021.findings-emnlp.195,D19-1241,0,0.0867527,"tural language is more robust to such minor mistakes. The objective function of the gen1 Introduction eration task is to maximize generation likelihood Solving math word problems (MWP) (Bobrow, on ground-truth expressions, which does not have 1964) is an important and fundamental task in nat- an explicit strategy to make the model learn to ural language processing (NLP), which requires to distinguish between ground-truth and expressions provide a solution expression given a mathematical that have minor mistakes. In addition, previous problem description, as illustrated in Table 1. Many works (Liu et al., 2019a; Xie and Sun, 2019; Zhang recent studies formalize MWP as a generation task et al., 2020) find that the performance of generation and commonly adopt LSTM-based sequence-to- models degrades fast as the expression gets longer. sequence (Seq2Seq) models (Wang et al., 2017, To handle the above problems, we propose Gen2018b; Xie and Sun, 2019), where problem texts erate & Rank, a multi-task framework for MWP, are source sequences, mathematical expressions are which introduces a new ranker to explicitly distintarget sequences and the model learns the mapping guish between correct and incorrect exp"
2021.findings-emnlp.195,2020.tacl-1.47,0,0.109591,"could be found at https://github.com/ huawei-noah/noah-research lem text and the latter aims to rank the candidate 2269 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2269–2279 November 7–11, 2021. ©2021 Association for Computational Linguistics expressions. They are built based on an encoderdecoder model and are jointly trained with generation loss and ranking loss. In this work, we build our model based on BART (Lewis et al., 2020), a widely used pre-trained language model that achieves SOTA performance on various sequenceto-sequence tasks (Ahmad et al., 2021; Liu et al., 2020). During multi-task training, expressions produced by the generator are used to construct an expression bank and train the ranker, in which way the model can learn from its own mistakes. To construct more informative candidates for the ranker, we specially design tree-based disturbance for MWP. We also introduce an online update mechanism to generate a new set of candidate expressions at each training epoch. The overall training procedure is in an iterative manner, in which the ranker and generator continue to enhance each other. To evaluate the effectiveness of the proposed model, we conduct"
2021.findings-emnlp.195,2021.ccl-1.108,0,0.0645969,"Missing"
2021.findings-emnlp.195,P16-1202,0,0.0389718,"Missing"
2021.findings-emnlp.195,D15-1202,0,0.0602398,"Missing"
2021.findings-emnlp.195,2020.coling-main.262,0,0.10523,"on problem. T-RNN (Wang et al., 2019b) applies recursive neural networks to model the tree structures of expressions. SAligned (Chiang and Chen, 2019) tracks the semantic meanings of operands with a stack during decoding. Group-ATT (Li et al., 2019) leverages the attention mechanism to enrich problem representation. Both AST-Dec (Liu et al., 2019a) and GTS (Xie and Sun, 2019) develop a tree-based decoder to generate expressions. Graph2Tree (Zhang et al., 2020) proposes to build a quantity cell graph and a comparison graph to better capture the quantity relationships of the problem. Multi-E/D (Shen and Jin, 2020) is an ensemble model which combines multiple encoders and decoders. Implementation Details. We use the PyTorch2 implementations and pre-trained language models provided by the Transformers library3 . Since the Math23K dataset is a Chinese dataset and officially released BART is only for English, we switch to 2 https://pytorch.org/ https://github.com/huggingface/ transformers 2273 3 mBART25 (Liu et al., 2020), which is a multilingual BART for 25 languages including Chinese. For the MAWPS dataset, we also use mBART25. We optimize our model with AdamW (Loshchilov and Hutter, 2019). The training"
2021.findings-emnlp.195,W15-5955,0,0.019832,"n to identify entities, quantities, and operators from the train a language representation model. Followproblem text. Roy et al. (2015) proposes three ing this, many other models are proposed like types of classifiers to identify different elements of RoBERTa (Liu et al., 2019b) and SpanBERT (Joshi problems. ARIS (Hosseini et al., 2014) splits the et al., 2020). Decoder-only models are typically problem into fragments and updates a logic temauto-regressive models trained to estimate the probplate named state by verb categorization. Other ability distribution of a text corpus, including works (Sundaram and Khemani, 2015; Mitra and GPT2 (Radford et al., 2019), GPT3 (Brown et al., Baral, 2016; Liang et al., 2016) follow a similar 2020) and XLNet (Yang et al., 2019). Encoderprocess with different templates and annotations. decoder models like BART (Lewis et al., 2020) and Two-stage methods. Another research line first T5 (Raffel et al., 2020) use the encoder-decoder arobtains an expression template then maps numbers chitecture and are trained on sequence-to-sequence to the template slots. Kushman et al. (2014) train tasks such as text denoising and translation. 2276 6 Conclusion and Future Work We propose Gener"
2021.findings-emnlp.195,W18-5446,0,0.0841453,"Missing"
2021.findings-emnlp.195,D18-1132,0,0.0910842,"We conduct the experiments on two commonly-used datasets: Math23K (Wang et al., 2017) and MAWPS (Koncel-Kedziorski et al., 2016). Math23K is a large-scale Chinese dataset that contains 23,162 math word problems and their corresponding expression solutions. MAWPS is a English dataset containing 2,373 problems. All the problems are one-unknown-variable linear problems and can be solved with a single expression. Baselines. We compare our model with the following baselines including the state-of-the-art models: DNS (Wang et al., 2017) uses a vanilla Seq2Seq model to generate expressions. Math-EN (Wang et al., 2018b) uses the equation normalization to avoid equation duplication problem. T-RNN (Wang et al., 2019b) applies recursive neural networks to model the tree structures of expressions. SAligned (Chiang and Chen, 2019) tracks the semantic meanings of operands with a stack during decoding. Group-ATT (Li et al., 2019) leverages the attention mechanism to enrich problem representation. Both AST-Dec (Liu et al., 2019a) and GTS (Xie and Sun, 2019) develop a tree-based decoder to generate expressions. Graph2Tree (Zhang et al., 2020) proposes to build a quantity cell graph and a comparison graph to better"
2021.findings-emnlp.195,D17-1088,0,0.0432245,"Missing"
2021.findings-emnlp.195,2020.acl-main.362,0,0.0768486,"t al., 2017) uses a vanilla Seq2Seq model to generate expressions. Math-EN (Wang et al., 2018b) uses the equation normalization to avoid equation duplication problem. T-RNN (Wang et al., 2019b) applies recursive neural networks to model the tree structures of expressions. SAligned (Chiang and Chen, 2019) tracks the semantic meanings of operands with a stack during decoding. Group-ATT (Li et al., 2019) leverages the attention mechanism to enrich problem representation. Both AST-Dec (Liu et al., 2019a) and GTS (Xie and Sun, 2019) develop a tree-based decoder to generate expressions. Graph2Tree (Zhang et al., 2020) proposes to build a quantity cell graph and a comparison graph to better capture the quantity relationships of the problem. Multi-E/D (Shen and Jin, 2020) is an ensemble model which combines multiple encoders and decoders. Implementation Details. We use the PyTorch2 implementations and pre-trained language models provided by the Transformers library3 . Since the Math23K dataset is a Chinese dataset and officially released BART is only for English, we switch to 2 https://pytorch.org/ https://github.com/huggingface/ transformers 2273 3 mBART25 (Liu et al., 2020), which is a multilingual BART fo"
2021.findings-emnlp.323,W19-5361,0,0.0126041,"ues can play a role. Sennrich et al. (2016) and Michel and Neubig (2018) showed that subwords are better alternatives than surface forms (words) to handle perturbations and outof-vocabulary words. Belinkov and Bisk (2018) comprehensively studied this by using different character- and subword-based representations in different architectures. Sakaguchi et al. (2017) also carried out a similar investigation where they proposed a new encoding that is invariant to the order of characters. Fine-tuning (FT) is one of the most straightforward and reliable techniques to protect NMT systems from noise. Berard et al. (2019), Dabre and Sumita (2019), and Helcl et al. (2019) studied its impact and showed how it needs to be utilized to boost NMT quality. Adversarial training is another common solution to build noise-robust models. Cheng et al. (2019) proposed a gradient-based method to construct adversarial examples for both source and target samBesides these approaches, translating noisy inples. Source-side inputs are supposed to attack the puts can be viewed as a two-pass process performed model while adversarial target inputs help defend via two connected neural networks. The first one the translation model. In"
2021.findings-emnlp.323,P19-1425,0,0.0202736,"ensively studied this by using different character- and subword-based representations in different architectures. Sakaguchi et al. (2017) also carried out a similar investigation where they proposed a new encoding that is invariant to the order of characters. Fine-tuning (FT) is one of the most straightforward and reliable techniques to protect NMT systems from noise. Berard et al. (2019), Dabre and Sumita (2019), and Helcl et al. (2019) studied its impact and showed how it needs to be utilized to boost NMT quality. Adversarial training is another common solution to build noise-robust models. Cheng et al. (2019) proposed a gradient-based method to construct adversarial examples for both source and target samBesides these approaches, translating noisy inples. Source-side inputs are supposed to attack the puts can be viewed as a two-pass process performed model while adversarial target inputs help defend via two connected neural networks. The first one the translation model. In their model, a candidate acts as a monolingual translator to correct noisy inword is replaced with its semantically-close peer to puts and the second one is an engine that consumes introduce noise. This way, the neural engine vi"
2021.findings-emnlp.323,P18-1163,0,0.0524493,"adversarial examples using synthetic noise. Their proposed architecture relies on Transformers but the encoder is equipped with a character-based convolutional model (Kim et al., 2016). This work is one of the few attempts that studied Transformers’ behaviour in the presence of noise. However, their results are based on relatively small datasets. We know that NMT models’ performance could change proportionally with a change in the size of training sets. Therefore, we used larger datasets in our experiments. The application of adversarial training is not limited to the aforementioned examples. Cheng et al. (2018) defined additional loss functions which force the encoder and decoder to ignore perturbations and generate clean outputs. This idea is similar to our CD approach, but the underlying architecture is different. Cheng et al. (2018) only reported results on recurrent NMT models. Providing better representations is as important as designing tailored training strategies for noiserobust models. A group of researchers focused on how different segmentation schemes and encoding techniques can play a role. Sennrich et al. (2016) and Michel and Neubig (2018) showed that subwords are better alternatives t"
2021.findings-emnlp.323,W14-4012,0,0.141943,"Missing"
2021.findings-emnlp.323,D14-1179,0,0.0530562,"Missing"
2021.findings-emnlp.323,W19-5362,0,0.0194951,"ennrich et al. (2016) and Michel and Neubig (2018) showed that subwords are better alternatives than surface forms (words) to handle perturbations and outof-vocabulary words. Belinkov and Bisk (2018) comprehensively studied this by using different character- and subword-based representations in different architectures. Sakaguchi et al. (2017) also carried out a similar investigation where they proposed a new encoding that is invariant to the order of characters. Fine-tuning (FT) is one of the most straightforward and reliable techniques to protect NMT systems from noise. Berard et al. (2019), Dabre and Sumita (2019), and Helcl et al. (2019) studied its impact and showed how it needs to be utilized to boost NMT quality. Adversarial training is another common solution to build noise-robust models. Cheng et al. (2019) proposed a gradient-based method to construct adversarial examples for both source and target samBesides these approaches, translating noisy inples. Source-side inputs are supposed to attack the puts can be viewed as a two-pass process performed model while adversarial target inputs help defend via two connected neural networks. The first one the translation model. In their model, a candidate"
2021.findings-emnlp.323,N19-1423,0,0.0302614,"(auxiliary encoder’s outputs). This architecture is illustrated in Figure 1. the noisy sequence could vary from the clean one, e.g. one token can be added/dropped or the noisy token can be decomposed into multiple units via bpe (as shown in Table 1). In such cases, the shape of encoders’ outputs does not even match and a vector-to-vector comparison is impossible. To handle these issues, we learn a dedicated representation for the entire input sequence, so comparing outputs would be straightforward. We do this form of sequence modelling by following the same idea proposed for the CLS token in Devlin et al. (2019). We refer to this sequence-level representation as REP in our setting. In Figure 1, the inputs to the main and auxiliary encoders are [S1i , N2i , S3i , S4i ] and [S1i , S2i , S3i , S4i ], respectively, and their sequence-level representations are REPmn and REPax . If our Transformer encoder is fed with s tokens it returns s + 1 vectors with the last one being REP. This token is only used for comparison purposes between encoders and is not sent to the decoder. To train our models with CD, we extend the original translation loss (Ltr ) with an additional term, LCD , as defined in Equation 1: L"
2021.findings-emnlp.323,P17-1012,0,0.0193192,"t al., 2014a,b; Sutskever et al., 2014), in which an encoder maps a given input sequence to an intermediate representation and a decoder then uses this representation to generate candidate translations. Both encoder and decoder are neural networks that ∗ † Equal contribution. Work done while Peyman Passban was at Huawei. are trained jointly. Due to the sequential nature of the NMT task, early models usually relied on recurrent architectures (Yang et al., 2020), or benefited from the sliding feature of convolutional kernels to encode/decode variable-length sequences (Kalchbrenner et al., 2014; Gehring et al., 2017). Recently, Transformers (Vaswani et al., 2017) have shown promising results for NMT and become the new standard in the field. They follow the same concept of encoding and decoding but in a relatively different fashion. A Transformer is fundamentally a feed-forward model with its unique neural components (self-attention, layer norm, etc) that altered the traditional translation pipeline accordingly. It is expected if such a new architecture would behave differently than its recurrent or convolutional counterparts, and our goal in this research is to study this aspect in the presence of noise."
2021.findings-emnlp.323,W19-5364,0,0.0197639,"hel and Neubig (2018) showed that subwords are better alternatives than surface forms (words) to handle perturbations and outof-vocabulary words. Belinkov and Bisk (2018) comprehensively studied this by using different character- and subword-based representations in different architectures. Sakaguchi et al. (2017) also carried out a similar investigation where they proposed a new encoding that is invariant to the order of characters. Fine-tuning (FT) is one of the most straightforward and reliable techniques to protect NMT systems from noise. Berard et al. (2019), Dabre and Sumita (2019), and Helcl et al. (2019) studied its impact and showed how it needs to be utilized to boost NMT quality. Adversarial training is another common solution to build noise-robust models. Cheng et al. (2019) proposed a gradient-based method to construct adversarial examples for both source and target samBesides these approaches, translating noisy inples. Source-side inputs are supposed to attack the puts can be viewed as a two-pass process performed model while adversarial target inputs help defend via two connected neural networks. The first one the translation model. In their model, a candidate acts as a monolingual tra"
2021.findings-emnlp.323,P14-1062,0,0.00731576,"decoder architecture (Cho et al., 2014a,b; Sutskever et al., 2014), in which an encoder maps a given input sequence to an intermediate representation and a decoder then uses this representation to generate candidate translations. Both encoder and decoder are neural networks that ∗ † Equal contribution. Work done while Peyman Passban was at Huawei. are trained jointly. Due to the sequential nature of the NMT task, early models usually relied on recurrent architectures (Yang et al., 2020), or benefited from the sliding feature of convolutional kernels to encode/decode variable-length sequences (Kalchbrenner et al., 2014; Gehring et al., 2017). Recently, Transformers (Vaswani et al., 2017) have shown promising results for NMT and become the new standard in the field. They follow the same concept of encoding and decoding but in a relatively different fashion. A Transformer is fundamentally a feed-forward model with its unique neural components (self-attention, layer norm, etc) that altered the traditional translation pipeline accordingly. It is expected if such a new architecture would behave differently than its recurrent or convolutional counterparts, and our goal in this research is to study this aspect in"
2021.findings-emnlp.323,D19-5506,0,0.110782,"d them in Section 3. To validate our methods, we report experimental results in Section 4. Finally, we conclude the paper and discuss possible future directions in Section 5. 2 Related Work its generalization. In other words, the network is trained to deliver the same, consistent functionality even though it is fed with different forms of a sample. Although this strategy showed promising results, in our setting we replace input words with real noisy candidates instead of synonyms or semantically-related peers. We find this way of adding noise more realistic and closer to real-world scenarios. Karpukhin et al. (2019) experimented another idea by generating adversarial examples using synthetic noise. Their proposed architecture relies on Transformers but the encoder is equipped with a character-based convolutional model (Kim et al., 2016). This work is one of the few attempts that studied Transformers’ behaviour in the presence of noise. However, their results are based on relatively small datasets. We know that NMT models’ performance could change proportionally with a change in the size of training sets. Therefore, we used larger datasets in our experiments. The application of adversarial training is not"
2021.findings-emnlp.323,W18-2709,0,0.0260887,"Missing"
2021.findings-emnlp.323,W19-5368,0,0.0174801,"nslating noisy inples. Source-side inputs are supposed to attack the puts can be viewed as a two-pass process performed model while adversarial target inputs help defend via two connected neural networks. The first one the translation model. In their model, a candidate acts as a monolingual translator to correct noisy inword is replaced with its semantically-close peer to puts and the second one is an engine that consumes introduce noise. This way, the neural engine visits denoised sequences to generate clean translations different forms of the same sample, which extends (Sun and Jiang, 2019; Zhou et al., 2019). This 3832 idea can be implemented as an end-to-end, differentiable solution or as a pipeline, but it should be noted that such a mechanism could be hard to deploy or slow(er) to run in practice. 3 Methodology This section covers details of our proposed methods. FT is a well-known technique so we skip its details and only focus on TAFT, our own extension of it (Section 3.1). Besides FT and TAFT that leverage data, we introduce CD (Section 3.2) and DCD (Section 3.3), which modify the training procedure as well as the neural architecture of Transformers. 3.1 Fine-Tuning Transformers Original al"
2021.findings-emnlp.323,D18-1050,0,0.282243,"f encoding and decoding but in a relatively different fashion. A Transformer is fundamentally a feed-forward model with its unique neural components (self-attention, layer norm, etc) that altered the traditional translation pipeline accordingly. It is expected if such a new architecture would behave differently than its recurrent or convolutional counterparts, and our goal in this research is to study this aspect in the presence of noise. NMT engines trained on clean samples provide high-quality results when tested on similarly clean texts, but they break easily if noise appears in the input (Michel and Neubig, 2018). They are not designed to handle noise by default and Transformers are no exception. Many previous works have focused on this issue and studied different architectures (Li et al., 2019). However, in this work, we particularly focus on Transformers1 as they are relatively new and to some extent understudied. A common approach to make NMT models immune to noise is fine-tuning (FT), where a noisy version of input tokens is intentionally introduced during training and the decoder is forced to generate correct translations despite deformed inputs. FT is quite useful for almost all situations but i"
2021.findings-emnlp.323,P02-1040,0,0.111087,"Missing"
2021.findings-emnlp.323,W18-6319,0,0.029096,"Missing"
2021.findings-emnlp.323,P16-1162,0,0.767296,"ication of adversarial training is not limited to the aforementioned examples. Cheng et al. (2018) defined additional loss functions which force the encoder and decoder to ignore perturbations and generate clean outputs. This idea is similar to our CD approach, but the underlying architecture is different. Cheng et al. (2018) only reported results on recurrent NMT models. Providing better representations is as important as designing tailored training strategies for noiserobust models. A group of researchers focused on how different segmentation schemes and encoding techniques can play a role. Sennrich et al. (2016) and Michel and Neubig (2018) showed that subwords are better alternatives than surface forms (words) to handle perturbations and outof-vocabulary words. Belinkov and Bisk (2018) comprehensively studied this by using different character- and subword-based representations in different architectures. Sakaguchi et al. (2017) also carried out a similar investigation where they proposed a new encoding that is invariant to the order of characters. Fine-tuning (FT) is one of the most straightforward and reliable techniques to protect NMT systems from noise. Berard et al. (2019), Dabre and Sumita (201"
2021.findings-emnlp.323,D19-5537,0,0.0215364,"these approaches, translating noisy inples. Source-side inputs are supposed to attack the puts can be viewed as a two-pass process performed model while adversarial target inputs help defend via two connected neural networks. The first one the translation model. In their model, a candidate acts as a monolingual translator to correct noisy inword is replaced with its semantically-close peer to puts and the second one is an engine that consumes introduce noise. This way, the neural engine visits denoised sequences to generate clean translations different forms of the same sample, which extends (Sun and Jiang, 2019; Zhou et al., 2019). This 3832 idea can be implemented as an end-to-end, differentiable solution or as a pipeline, but it should be noted that such a mechanism could be hard to deploy or slow(er) to run in practice. 3 Methodology This section covers details of our proposed methods. FT is a well-known technique so we skip its details and only focus on TAFT, our own extension of it (Section 3.1). Besides FT and TAFT that leverage data, we introduce CD (Section 3.2) and DCD (Section 3.3), which modify the training procedure as well as the neural architecture of Transformers. 3.1 Fine-Tuning Tran"
2021.iwslt-1.17,D18-2012,0,0.0314923,"k. Therefore, only the data from the Multilingual TEDx (Salesky et al., 2021) is available. It contains speech and transcripts from four languages (Spanish, French, Portuguese, and Italian), and some of them are translated into other languages of the five (English and the four mentioned above). The data statistics are shown in Table 1. We use 80-dimensional log-mel filterbanks as acoustic features, which are calculated with 25ms window size and 10ms step size and normalized by utterance-level Cepstral Mean and Variance Normalization (CMVN). For transcriptions and translations, SentencePiece2 (Kudo and Richardson, 2018) is used to generate a joint subword vocabulary with the size of 10k. We share the weights for input and output embeddings, as well as the output projection in CTC module. Our model is trained with 8 NVIDIA Tesla V100 GPUs, each with a batch size of 32. We use Adam optimizer (Kingma and Ba, 2015) during model training with learning rates selected in {2e−3 , 1e−3 , 8e−4 , 5e−4 , 3e−4 } and warm-up steps selected in {2000, 6000, 10000}, followed by the inverse square root scheduler. Dropout rate is selected in {0.1, 0.2, 0.3}. We save checkpoints every epoch and average the last 10 checkpoints f"
2021.iwslt-1.17,2020.aacl-demo.6,0,0.0345926,"share the weights for input and output embeddings, as well as the output projection in CTC module. Our model is trained with 8 NVIDIA Tesla V100 GPUs, each with a batch size of 32. We use Adam optimizer (Kingma and Ba, 2015) during model training with learning rates selected in {2e−3 , 1e−3 , 8e−4 , 5e−4 , 3e−4 } and warm-up steps selected in {2000, 6000, 10000}, followed by the inverse square root scheduler. Dropout rate is selected in {0.1, 0.2, 0.3}. We save checkpoints every epoch and average the last 10 checkpoints for evaluation with a beam size of 5. Our code is based on fairseq S2T3 (Wang et al., 2020). 3.4 4.2 Time Stretch. Time stretching is a kind of augmentation method applied in extracted acoustic features like filterbanks to simulate conventional speed perturbation technique (Ko et al., 2015). Specifically, given a consecutive feature vectors of speech input, it stretches every window of w feature vectors by a factor of s obtained from an uniform distribution of range [low, high]. In this way, some frames are dropped (if s &gt; 1) or repeated (if s &lt; 1) to simulate audio speeding up or down. We only apply Time Stretch in the first two training steps, as we found it does not help much in"
C08-1041,J96-1002,0,0.0197612,"both terminals and nonterminals, which causes a reordering of phrases. The hierarchical model uses the maximum likelihood method to estimate translation probabilities for a phrase pair α, γ, independent of any other context information. To perform translation, Chiang uses a log-linear model (Och and Ney, 2002) to combine various features. The weight of a derivation D is computed by: (7) w(D) =  Lexicalized Rule Selection The rule selection task can be considered as a multi-class classification task. For a source-side, each corresponding target-side is a label. The maximum entropy approach (Berger et al., 1996) is known to be well suited to solve the classification problem. Therefore, we build a maximum entropy based rule selection (MaxEnt RS) model for each ambiguous hierarchical LHS. In this section, we will describe how to build the MaxEnt RS models and how to integrate them into the hierarchical SMT model. 3.1 The hierarchical model (Chiang, 2005; Chiang, 2007) is built on a weighted synchronous contextfree grammar (SCFG) . A SCFG rule has the following form: (4) 3 The MaxEnt RS Model Following (Chiang, 2005), we use α, γ to represent a SCFG rule extracted from the training corpus, where α and"
C08-1041,2007.tmi-papers.6,0,0.0243291,"kinds of reorderings between two adjacent phrases: monotone or swap. However, our method is more generic, we perform lexicalized rule selection for syntax-based SMT models. In these models, the rules with hierarchical structures can handle reorderings of non-adjacent phrases. Furthermore, the rule selection can be considered as a multiclass classification task, while the phrase reordering between two adjacent phrases is a two-class classification task. Recently, word sense disambiguation (WSD) techniques improved the performance of SMT systems by helping the decoder perform lexical selection. Carpuat and Wu (2007b) integrated a WSD system into a phrase-based SMT system, Pharaoh (Koehn, 2004a). Furthermore, they extended WSD to phrase sense disambiguation (PSD) (Carpuat and Wu, 2007a). Either the WSD or PSD system combines rich context information to solve the ambiguity problem for words or phrases. Their experiments showed stable improvements of translation quality. These are different from our work. On one hand, they focus on solving the lexical ambiguity problem, and use a WSD or PSD system to predict translations for phrases which only consist of words. However, we put emphasis on rule selection, a"
C08-1041,D07-1007,0,0.139468,"kinds of reorderings between two adjacent phrases: monotone or swap. However, our method is more generic, we perform lexicalized rule selection for syntax-based SMT models. In these models, the rules with hierarchical structures can handle reorderings of non-adjacent phrases. Furthermore, the rule selection can be considered as a multiclass classification task, while the phrase reordering between two adjacent phrases is a two-class classification task. Recently, word sense disambiguation (WSD) techniques improved the performance of SMT systems by helping the decoder perform lexical selection. Carpuat and Wu (2007b) integrated a WSD system into a phrase-based SMT system, Pharaoh (Koehn, 2004a). Furthermore, they extended WSD to phrase sense disambiguation (PSD) (Carpuat and Wu, 2007a). Either the WSD or PSD system combines rich context information to solve the ambiguity problem for words or phrases. Their experiments showed stable improvements of translation quality. These are different from our work. On one hand, they focus on solving the lexical ambiguity problem, and use a WSD or PSD system to predict translations for phrases which only consist of words. However, we put emphasis on rule selection, a"
C08-1041,P07-1005,0,0.187355,"work. On one hand, they focus on solving the lexical ambiguity problem, and use a WSD or PSD system to predict translations for phrases which only consist of words. However, we put emphasis on rule selection, and predict translations for hierarchical LHS’s which consist of both words and nonterminals. On the other hand, they incorporated a WSD or PSD system into a phrase-based SMT system with a weak distortion model for phrase reordering. While we incorporate MaxEnt RS models into the state-of-the-art syntax-based SMT system, which captures phrase reordering by using a hierarchical model. 322 Chan et al. (2007) incorporated a WSD system into the hierarchical SMT system, Hiero (Chiang, 2005), and reported statistically significant improvement. But they only focused on solving ambiguity for terminals of translation rules, and limited the length of terminals up to 2. Different from their work, we consider a translation rule as a whole, which contains both terminals and nonterminals. Moreover, they explored features for the WSD system only on the source-side. While we define context features for the MaxEnt RS models on both the source-side and target-side. 2.2 The Hierarchical Model X → α, γ, ∼ where"
C08-1041,P05-1033,0,0.158974,"ion using Lexicalized Rule Selection Zhongjun He1,2 and Qun Liu1 and Shouxun Lin1 Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences Beijing, 100190, China 2 Graduate University of Chinese Academy of Sciences Beijing, 100049, China {zjhe,liuqun,sxlin}@ict.ac.cn 1 Abstract are estimated from the training corpus, as well as a n-gram language model which is trained on the target language. The limitation of this method is that it ignores context information (especially on the source-side) during decoding. Take the hierarchical model (Chiang, 2005) as an example. Consider the following rules for Chinese-to-English translation 2 : This paper proposes a novel lexicalized approach for rule selection for syntax-based statistical machine translation (SMT). We build maximum entropy (MaxEnt) models which combine rich context information for selecting translation rules during decoding. We successfully integrate the MaxEnt-based rule selection models into the state-of-the-art syntax-based SMT model. Experiments show that our lexicalized approach for rule selection achieves statistically significant improvements over the state-of-the-art SMT syst"
C08-1041,J07-2003,0,0.706857,"n D is computed by: (7) w(D) =  Lexicalized Rule Selection The rule selection task can be considered as a multi-class classification task. For a source-side, each corresponding target-side is a label. The maximum entropy approach (Berger et al., 1996) is known to be well suited to solve the classification problem. Therefore, we build a maximum entropy based rule selection (MaxEnt RS) model for each ambiguous hierarchical LHS. In this section, we will describe how to build the MaxEnt RS models and how to integrate them into the hierarchical SMT model. 3.1 The hierarchical model (Chiang, 2005; Chiang, 2007) is built on a weighted synchronous contextfree grammar (SCFG) . A SCFG rule has the following form: (4) 3 The MaxEnt RS Model Following (Chiang, 2005), we use α, γ to represent a SCFG rule extracted from the training corpus, where α and γ are source and target strings, respectively. The nonterminals in α and γ are represented by Xk , where k is an index indicating one-one correspondence between nonterminals in source and target sides. Let us use f (Xk ) to represent the source text covered by Xk , and e(Xk ) to represent the translation of f (Xk ). Let C(α) be the context information of sou"
C08-1041,P06-1121,0,0.0907301,"(MaxEnt) models which combine rich context information for selecting translation rules during decoding. We successfully integrate the MaxEnt-based rule selection models into the state-of-the-art syntax-based SMT model. Experiments show that our lexicalized approach for rule selection achieves statistically significant improvements over the state-of-the-art SMT system. 1 (1) X →  ᄝ X 1  X 2 , X 2 in X 1  (2) X →  ᄝ X 1  X 2 , at X 1 ’s X 2  (3) X →  ᄝ X 1  X 2 , with X 2 of X 1  Introduction The syntax-based statistical machine translation (SMT) models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006) use rules with hierarchical structures as translation knowledge, which can capture long-distance reorderings. Generally, a translation rule consists of a left-handside (LHS) 1 and a right-hand-side (RHS). The LHS and RHS can be words, phrases, or even syntactic trees, depending on SMT models. Translation rules can be learned automatically from parallel corpus. Usually, an LHS may correspond to multiple RHS’s in multiple rules. Therefore, in statistical machine translation, the rule selection task is to select the correct RHS for an LHS during decoding. The conventional ap"
C08-1041,W05-1506,0,0.0170594,"uous source-side. During decoding, if an LHS has multiple translations, this feature is set to exp(1), otherwise it is set to exp(0). The advantage of our integration is that we need not change the main decoding algorithm of a SMT system. Furthermore, the weights of the new features can be trained together with other features of the translation model. Chiang (2007) uses the CKY algorithm with a cube pruning method for decoding. This method can significantly reduce the search space by efficiently computing the top-n items rather than all possible items at a node, using the k-best Algorithms of Huang and Chiang (2005) to speed up the computation. In cube pruning, the translation model is treated as the monotonic backbone of the search space, while the language model score is a non-monotonic cost that distorts the search space (see (Huang and Chiang, 2005) for definition of monotonicity). Similarly, in the MaxEnt RS model, source-side features form a monotonic score while target-side features constitute a nonmonotonic cost that can be seen as part of the language model. For translating a source sentence FIJ , the decoder adopts a bottom-up strategy. All derivations are stored in a chart structure. Each cell"
C08-1041,2006.amta-papers.8,0,0.187095,"combine rich context information for selecting translation rules during decoding. We successfully integrate the MaxEnt-based rule selection models into the state-of-the-art syntax-based SMT model. Experiments show that our lexicalized approach for rule selection achieves statistically significant improvements over the state-of-the-art SMT system. 1 (1) X →  ᄝ X 1  X 2 , X 2 in X 1  (2) X →  ᄝ X 1  X 2 , at X 1 ’s X 2  (3) X →  ᄝ X 1  X 2 , with X 2 of X 1  Introduction The syntax-based statistical machine translation (SMT) models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006) use rules with hierarchical structures as translation knowledge, which can capture long-distance reorderings. Generally, a translation rule consists of a left-handside (LHS) 1 and a right-hand-side (RHS). The LHS and RHS can be words, phrases, or even syntactic trees, depending on SMT models. Translation rules can be learned automatically from parallel corpus. Usually, an LHS may correspond to multiple RHS’s in multiple rules. Therefore, in statistical machine translation, the rule selection task is to select the correct RHS for an LHS during decoding. The conventional approach for rule selec"
C08-1041,N03-1017,0,0.0706337,"Missing"
C08-1041,koen-2004-pharaoh,0,0.0542471,"is more generic, we perform lexicalized rule selection for syntax-based SMT models. In these models, the rules with hierarchical structures can handle reorderings of non-adjacent phrases. Furthermore, the rule selection can be considered as a multiclass classification task, while the phrase reordering between two adjacent phrases is a two-class classification task. Recently, word sense disambiguation (WSD) techniques improved the performance of SMT systems by helping the decoder perform lexical selection. Carpuat and Wu (2007b) integrated a WSD system into a phrase-based SMT system, Pharaoh (Koehn, 2004a). Furthermore, they extended WSD to phrase sense disambiguation (PSD) (Carpuat and Wu, 2007a). Either the WSD or PSD system combines rich context information to solve the ambiguity problem for words or phrases. Their experiments showed stable improvements of translation quality. These are different from our work. On one hand, they focus on solving the lexical ambiguity problem, and use a WSD or PSD system to predict translations for phrases which only consist of words. However, we put emphasis on rule selection, and predict translations for hierarchical LHS’s which consist of both words and"
C08-1041,W04-3250,0,0.687916,"is more generic, we perform lexicalized rule selection for syntax-based SMT models. In these models, the rules with hierarchical structures can handle reorderings of non-adjacent phrases. Furthermore, the rule selection can be considered as a multiclass classification task, while the phrase reordering between two adjacent phrases is a two-class classification task. Recently, word sense disambiguation (WSD) techniques improved the performance of SMT systems by helping the decoder perform lexical selection. Carpuat and Wu (2007b) integrated a WSD system into a phrase-based SMT system, Pharaoh (Koehn, 2004a). Furthermore, they extended WSD to phrase sense disambiguation (PSD) (Carpuat and Wu, 2007a). Either the WSD or PSD system combines rich context information to solve the ambiguity problem for words or phrases. Their experiments showed stable improvements of translation quality. These are different from our work. On one hand, they focus on solving the lexical ambiguity problem, and use a WSD or PSD system to predict translations for phrases which only consist of words. However, we put emphasis on rule selection, and predict translations for hierarchical LHS’s which consist of both words and"
C08-1041,P06-1077,1,0.890075,"d maximum entropy (MaxEnt) models which combine rich context information for selecting translation rules during decoding. We successfully integrate the MaxEnt-based rule selection models into the state-of-the-art syntax-based SMT model. Experiments show that our lexicalized approach for rule selection achieves statistically significant improvements over the state-of-the-art SMT system. 1 (1) X →  ᄝ X 1  X 2 , X 2 in X 1  (2) X →  ᄝ X 1  X 2 , at X 1 ’s X 2  (3) X →  ᄝ X 1  X 2 , with X 2 of X 1  Introduction The syntax-based statistical machine translation (SMT) models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006) use rules with hierarchical structures as translation knowledge, which can capture long-distance reorderings. Generally, a translation rule consists of a left-handside (LHS) 1 and a right-hand-side (RHS). The LHS and RHS can be words, phrases, or even syntactic trees, depending on SMT models. Translation rules can be learned automatically from parallel corpus. Usually, an LHS may correspond to multiple RHS’s in multiple rules. Therefore, in statistical machine translation, the rule selection task is to select the correct RHS for an LHS during decoding"
C08-1041,P00-1056,0,0.121169,"English part is used to train a trigram language model. We use IWSLT-04 test set as the development set and IWSLT-05 test set as the test set. • NIST-03: We use the FBIS corpus as the training corpus, which contains 239k sentence pairs with 6.9M Chinese words and 8.9M English words. For this task, we train two trigram language models on the English part of the training corpus and the Xinhua portion of the Gigaword corpus, respectively. NIST-02 test set is used as the development set and NIST-03 test set is used as the test set. 4.2 Training To train the translation model, we first run GIZA++ (Och and Ney, 2000) to obtain word alignment in both translation directions. Then the word alignment is refined by performing “growdiag-final” method (Koehn et al., 2003). We use the same method suggested in (Chiang, 2005) to extract SCFG grammar rules. Meanwhile, we gather context features for training the MaxEnt RS models. The maximum initial phrase length is set to 10 and the maximum rule length of the sourceside is set to 5. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train language models for both tasks. We use minimum error rate training (Och, 2003) to tune the feature weights for the log-linea"
C08-1041,P02-1038,0,0.350244,"nslation of α, ∼ defines a one-one correspondence between nonterminals in α and γ. For example, (5) X →  ࣜ࠶ ؿᅚ, economic development (6) X →  X 1  X 2 đ the X 2 of X 1  Rule (5) contains only terminals, which is similar to phrase-to-phrase translation in phrase-based SMT models. Rule (6) contains both terminals and nonterminals, which causes a reordering of phrases. The hierarchical model uses the maximum likelihood method to estimate translation probabilities for a phrase pair α, γ, independent of any other context information. To perform translation, Chiang uses a log-linear model (Och and Ney, 2002) to combine various features. The weight of a derivation D is computed by: (7) w(D) =  Lexicalized Rule Selection The rule selection task can be considered as a multi-class classification task. For a source-side, each corresponding target-side is a label. The maximum entropy approach (Berger et al., 1996) is known to be well suited to solve the classification problem. Therefore, we build a maximum entropy based rule selection (MaxEnt RS) model for each ambiguous hierarchical LHS. In this section, we will describe how to build the MaxEnt RS models and how to integrate them into the hierarchica"
C08-1041,P03-1021,0,0.0370658,"anslation model, we first run GIZA++ (Och and Ney, 2000) to obtain word alignment in both translation directions. Then the word alignment is refined by performing “growdiag-final” method (Koehn et al., 2003). We use the same method suggested in (Chiang, 2005) to extract SCFG grammar rules. Meanwhile, we gather context features for training the MaxEnt RS models. The maximum initial phrase length is set to 10 and the maximum rule length of the sourceside is set to 5. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train language models for both tasks. We use minimum error rate training (Och, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by BLEU metric (Papineni et al., 2002), as calculated by mteval-v11b.pl with case-insensitive matching of n-grams, where n = 4. 4.3 Baseline We reimplement the decoder of Hiero (Chiang, 2007) in C++, which is the state-of-the-art SMT 325 System Baseline + MaxEnt RS SLex PF SLex+PF SLex+PF+SLen SLex+PF +SLen+TF IWSLT-05 56.20 NIST-03 28.05 56.51 56.95 56.99 57.10 57.20 28.26 28.78 28.89 28.96 29.02 Table 3: BLEU-4 scores (case-insensitive) on IWSLT-05 task and NIST MT-03 task. SLex = Source-side Lexical"
C08-1041,P02-1040,0,0.102349,"the word alignment is refined by performing “growdiag-final” method (Koehn et al., 2003). We use the same method suggested in (Chiang, 2005) to extract SCFG grammar rules. Meanwhile, we gather context features for training the MaxEnt RS models. The maximum initial phrase length is set to 10 and the maximum rule length of the sourceside is set to 5. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train language models for both tasks. We use minimum error rate training (Och, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by BLEU metric (Papineni et al., 2002), as calculated by mteval-v11b.pl with case-insensitive matching of n-grams, where n = 4. 4.3 Baseline We reimplement the decoder of Hiero (Chiang, 2007) in C++, which is the state-of-the-art SMT 325 System Baseline + MaxEnt RS SLex PF SLex+PF SLex+PF+SLen SLex+PF +SLen+TF IWSLT-05 56.20 NIST-03 28.05 56.51 56.95 56.99 57.10 57.20 28.26 28.78 28.89 28.96 29.02 Table 3: BLEU-4 scores (case-insensitive) on IWSLT-05 task and NIST MT-03 task. SLex = Source-side Lexical Features, PF = POS Features, SLen = Source-side Length Feature, TF = Target-side features. system. During decoding, we set b = 100"
C08-1041,P06-1066,1,0.725075,"t hierarchical SMT system (Chiang, 2005). Experiments show that the lexicalized rule selection approach improves translation quality of the state-of-the-art SMT system, and the improvements are statistically significant. 2 Previous Work 2.1 The Selection Problem in SMT Statistical machine translation systems usually face the selection problem because of the one-tomany correspondence between the source and target language. Recent researches showed that rich context information can help SMT systems perform selection and improves translation quality. The discriminative phrasal reordering models (Xiong et al., 2006; Zens and Ney, 2006) provided a lexicalized method for phrase reordering. In these models, LHS and RHS can be considered as phrases and reordering types, respectively. Therefore the selection task is to select a reordering type for phrases. They use a MaxEnt model to combine context features and distinguished two kinds of reorderings between two adjacent phrases: monotone or swap. However, our method is more generic, we perform lexicalized rule selection for syntax-based SMT models. In these models, the rules with hierarchical structures can handle reorderings of non-adjacent phrases. Further"
C08-1041,W06-3108,0,0.0233893,"ystem (Chiang, 2005). Experiments show that the lexicalized rule selection approach improves translation quality of the state-of-the-art SMT system, and the improvements are statistically significant. 2 Previous Work 2.1 The Selection Problem in SMT Statistical machine translation systems usually face the selection problem because of the one-tomany correspondence between the source and target language. Recent researches showed that rich context information can help SMT systems perform selection and improves translation quality. The discriminative phrasal reordering models (Xiong et al., 2006; Zens and Ney, 2006) provided a lexicalized method for phrase reordering. In these models, LHS and RHS can be considered as phrases and reordering types, respectively. Therefore the selection task is to select a reordering type for phrases. They use a MaxEnt model to combine context features and distinguished two kinds of reorderings between two adjacent phrases: monotone or swap. However, our method is more generic, we perform lexicalized rule selection for syntax-based SMT models. In these models, the rules with hierarchical structures can handle reorderings of non-adjacent phrases. Furthermore, the rule select"
C08-1041,zhang-etal-2004-interpreting,0,0.0188581,"Missing"
C08-1049,P08-1102,1,0.516716,"rt equals to t. For example, a tag sequence b N N m N N e N N represents a three-character word with POS tag N N . 4 The features we use to build the classifier are generated from the templates of Ng and Low (2004). For convenience of comparing with other, they didn’t adopt the ones containing external knowledge, such as punctuation information. All their templates are shown in Table 2. C denotes a character, while its subscript indicates its position relative to the current considering character(it has the subscript 0). Baseline Perceptron Classifier 4.1 Joint S&T as Classification Following Jiang et al. (2008), we describe segmentation and Joint S&T as below: For a given Chinese sentence appearing as a character sequence: C1:n = C1 C2 .. Cn the goal of segmentation is splitting the sequence into several subsequences: C1:e1 Ce1 +1:e2 .. Cem−1 +1:em While in Joint S&T, each of these subsequences is labelled a POS tag: C1:e1 /t1 Ce1 +1:e2 /t2 .. Cem−1 +1:em /tm Where Ci (i = 1..n) denotes a character, Cl:r (l ≤ r) denotes the subsequence ranging from Cl to Cr , and ti (i = 1..m, m ≤ n) denotes the POS tag of Cei−1 +1:ei . If we label each character a positional tag indicating its relative position in"
C08-1049,W04-3236,0,0.483609,"presenting features, and usually obtains almost perfect accuracy in two tasks. Originated by Xue and Shen (2003), the typical approach of discriminative models conducts c 2008. Licensed to the Coling 2008 Organizing Com mittee for publication in Coling 2008 and for re-publishing in any form or medium. segmentation in a classification style, by assigning each character a positional tag indicating its relative position in the word. If we extend these positional tags to include POS information, segmentation and POS tagging can be performed by a single pass under a unify classification framework (Ng and Low, 2004). In the rest of the paper, we call this operation mode Joint S&T. Experiments of Ng and Low (2004) shown that, compared with performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging. Besides the usual local features such as the character-based ones (Xue and Shen, 2003; Ng and Low, 2004), many non-local features related to POSs or words can also be employed to improve performance. However, as such features are generated dynamically during the decoding procedure, incorporating these features directly into the c"
C08-1049,W03-1728,0,0.011118,"perceptron baseline and the n-best list reranking. 1 Introduction Recent work for Chinese word segmentation and POS tagging pays much attention to discriminative methods, such as Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), Conditional Random Fields (CRFs) (Lafferty et al., 2001), perceptron training algorithm (Collins, 2002), etc. Compared to generative ones such as Hidden Markov Model (HMM) (Rabiner, 1989; Fine et al., 1998), discriminative models have the advantage of flexibility in representing features, and usually obtains almost perfect accuracy in two tasks. Originated by Xue and Shen (2003), the typical approach of discriminative models conducts c 2008. Licensed to the Coling 2008 Organizing Com mittee for publication in Coling 2008 and for re-publishing in any form or medium. segmentation in a classification style, by assigning each character a positional tag indicating its relative position in the word. If we extend these positional tags to include POS information, segmentation and POS tagging can be performed by a single pass under a unify classification framework (Ng and Low, 2004). In the rest of the paper, we call this operation mode Joint S&T. Experiments of Ng and Low (2"
C08-1049,W02-1001,0,0.828167,"l features as the baseline, word lattice reranking performs reranking with non-local features that can’t be easily incorporated into the perceptron baseline. Experimental results show that, this strategy achieves improvement on both segmentation and POS tagging, above the perceptron baseline and the n-best list reranking. 1 Introduction Recent work for Chinese word segmentation and POS tagging pays much attention to discriminative methods, such as Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), Conditional Random Fields (CRFs) (Lafferty et al., 2001), perceptron training algorithm (Collins, 2002), etc. Compared to generative ones such as Hidden Markov Model (HMM) (Rabiner, 1989; Fine et al., 1998), discriminative models have the advantage of flexibility in representing features, and usually obtains almost perfect accuracy in two tasks. Originated by Xue and Shen (2003), the typical approach of discriminative models conducts c 2008. Licensed to the Coling 2008 Organizing Com mittee for publication in Coling 2008 and for re-publishing in any form or medium. segmentation in a classification style, by assigning each character a positional tag indicating its relative position in the word."
C08-1049,P08-1067,0,0.477087,"ttice Reranking for Chinese Word Segmentation and Part-of-Speech Tagging Wenbin Jiang † ‡ Haitao Mi † ‡ Qun Liu † † Key Lab. of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China ‡ Graduate University of Chinese Academy of Sciences Beijing, 100049, China {jiangwenbin,htmi,liuqun}@ict.ac.cn Abstract In this paper, we describe a new reranking strategy named word lattice reranking, for the task of joint Chinese word segmentation and part-of-speech (POS) tagging. As a derivation of the forest reranking for parsing (Huang, 2008), this strategy reranks on the pruned word lattice, which potentially contains much more candidates while using less storage, compared with the traditional n-best list reranking. With a perceptron classifier trained with local features as the baseline, word lattice reranking performs reranking with non-local features that can’t be easily incorporated into the perceptron baseline. Experimental results show that, this strategy achieves improvement on both segmentation and POS tagging, above the perceptron baseline and the n-best list reranking. 1 Introduction Recent work for Chinese word segment"
C08-1049,W96-0213,0,\N,Missing
C08-1049,W05-1506,0,\N,Missing
C08-1049,P07-1019,0,\N,Missing
C08-1049,J07-2003,0,\N,Missing
C10-1080,D08-1023,0,0.0148063,"al parser instead of a syntax-based language model. More importantly, we integrate translation models and parsing models in a discriminative framework where they can interact with each other directly. Our work also has connections to joint parsing (Smith and Smith, 2004; Burkett and Klein, 2008) and bilingually-constrained monolingual parsing (Huang et al., 2009a) because we use another language to resolve ambiguity for one language. However, while both joint parsing and bilinguallyconstrained monolingual parsing rely on the target sentence, our approach only takes a source sentence as input. Blunsom and Osborne (2008) incorporate the source-side parse trees into their probabilistic SCFG framework and treat every source-parse PCFG rule as an individual feature. The difference is that they parse the test set before decoding so as to exploit the source syntactic information to guide translation. More recently, Chiang (2010) has shown that (“exact”) tree-to-tree translation as parsing achieves comparable performance with Hiero (Chiang, 2007) using much fewer rules. Xiao et al. (2010) integrate tokenization and translation into a single step and improve the performance of tokenization and translation significan"
C10-1080,P04-1083,0,0.0797349,"Missing"
C10-1080,P08-1023,1,0.938919,"1: Tree-based decoding: (a) separate parsing and translation versus (b) joint parsing and translation. 1 Introduction Recent several years have witnessed the rapid development of syntax-based translation models (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Eisner, 2003; Zhang et al., 2008; Chiang, 2010), which incorporate formal or linguistic syntax into translation process. Depending on whether modeling the linguistic syntax of source language or not, we divide them into two categories: string-based and tree-based models. 1 1 Mi et al. (2008) also distinguish between string-based and tree-based models but depending on the type of input. String-based models include string-to-string (Chiang, 2007) and string-to-tree (Galley et al., 2006; Shen et al., 2008). Regardless of the syntactic information on the source side, they treat decoding as a parsing problem: the decoder parses a source-language sentence using the source projection of a synchronous grammar while building the target sub-translations in parallel. Tree-based models include tree-to-string (Liu et al., 2006; Huang et al., 2006) and tree-to-tree (Quirk et al., 2005; Eisner,"
C10-1080,D08-1092,0,0.0134633,"ine translation. Recently, Shen et al. (2008) have shown that dependency language model is beneficial for capturing long-distance relations between target words. As our approach adds parsing models to the source side where the source sentence is fixed during decoding, our decoder does parse the source sentence like a monolingual parser instead of a syntax-based language model. More importantly, we integrate translation models and parsing models in a discriminative framework where they can interact with each other directly. Our work also has connections to joint parsing (Smith and Smith, 2004; Burkett and Klein, 2008) and bilingually-constrained monolingual parsing (Huang et al., 2009a) because we use another language to resolve ambiguity for one language. However, while both joint parsing and bilinguallyconstrained monolingual parsing rely on the target sentence, our approach only takes a source sentence as input. Blunsom and Osborne (2008) incorporate the source-side parse trees into their probabilistic SCFG framework and treat every source-parse PCFG rule as an individual feature. The difference is that they parse the test set before decoding so as to exploit the source syntactic information to guide tr"
C10-1080,J03-1002,0,0.00572434,"Missing"
C10-1080,2003.mtsummit-papers.6,0,0.0515247,"enote only using translation features. Table 5 shows the results. Translation features were used for all configurations. Without parsing models, the F1 score is 62.7. Adding Collins’ Model 1 results in much larger gains than adding PCFG. With all parsing models integrated, our joint decoder achieves an F1 score of 80.6 on the test set. Although lower than the F1 score of the in-house parser that produces the noisy training data, this result is still very promising because the tree-to-string rules that construct trees in the decoding process are learned from noisy training data. 4 Related Work Charniak et al. (2003) firstly associate lexicalized parsing model with syntax-based translation. They first run a string-to-tree decoder (Yamada and Knight, 2001) to produce an English parse forest and then use a lexicalized parsing model to select the best translation from the forest. As the parsing model operates on the target side, it actually serves as a syntax-based language model for machine translation. Recently, Shen et al. (2008) have shown that dependency language model is beneficial for capturing long-distance relations between target words. As our approach adds parsing models to the source side where t"
C10-1080,P03-1021,0,0.0354318,"probabilities of a natural rule unchanged and set those of a virtual rule to 1. 4 After binarizing tree-to-string rules into SCFG rules that have at most two non-terminals, we can use the CKY algorithm to parse a source sentence and produce its translation simultaneously as described in (Chiang, 2007; Galley et al., 2006). 2.2 Adding Parsing Models As our decoder produces “genuine” parse trees during decoding, we can integrate parsing models as features together with translation features such as the tree-to-string model, n-gram language model, and word penalty into a discriminative framework (Och, 2003). We expect that parsing and translation could interact with each other: parsing offers linguistically motivated reordering to translation and translation helps parsing resolve ambiguity. IP(x1 :NPB VP(x2 :PP x3 :VPB))→x1 x3 x2 2.2.1 PCFG We use the probabilistic context-free grammar (PCFG) as the first parsing feature in our decoder. Given a PCFG, the probability for a tree is the IP → hT 1 , T 1 i (1) product of probabilities for the rules that it contains. That is, if a tree π is a context-free derivaT → hNPB 1 PP 2 VPB 3 , NPB 1 VPB 3 PP 2 i (2) tion that involves K rules of the form αk →"
C10-1080,J07-2003,0,0.685036,"two-step SCFG derivation. For example, consider the first rule in Table 1: We call rules the tree roots of which are virtual non-terminals virtual rules and others natural rules. For example, the rule (1) is a natural rule and the rules (3) and (4) are virtual rules. We follow Huang et al. (2009b) to keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1. 4 After binarizing tree-to-string rules into SCFG rules that have at most two non-terminals, we can use the CKY algorithm to parse a source sentence and produce its translation simultaneously as described in (Chiang, 2007; Galley et al., 2006). 2.2 Adding Parsing Models As our decoder produces “genuine” parse trees during decoding, we can integrate parsing models as features together with translation features such as the tree-to-string model, n-gram language model, and word penalty into a discriminative framework (Och, 2003). We expect that parsing and translation could interact with each other: parsing offers linguistically motivated reordering to translation and translation helps parsing resolve ambiguity. IP(x1 :NPB VP(x2 :PP x3 :VPB))→x1 x3 x2 2.2.1 PCFG We use the probabilistic context-free grammar (PCFG)"
C10-1080,N07-1051,0,0.0884195,"Missing"
C10-1080,P10-1146,0,0.0342491,"ed monolingual parsing (Huang et al., 2009a) because we use another language to resolve ambiguity for one language. However, while both joint parsing and bilinguallyconstrained monolingual parsing rely on the target sentence, our approach only takes a source sentence as input. Blunsom and Osborne (2008) incorporate the source-side parse trees into their probabilistic SCFG framework and treat every source-parse PCFG rule as an individual feature. The difference is that they parse the test set before decoding so as to exploit the source syntactic information to guide translation. More recently, Chiang (2010) has shown that (“exact”) tree-to-tree translation as parsing achieves comparable performance with Hiero (Chiang, 2007) using much fewer rules. Xiao et al. (2010) integrate tokenization and translation into a single step and improve the performance of tokenization and translation significantly. 5 Conclusion We have presented a framework for joint parsing and translation by casting tree-to-string translation as a parsing problem. While tree-to-string rules construct parse trees on the source side and translations on the target side simultaneously, parsing models can be integrated to improve bot"
C10-1080,W06-1608,0,0.0239566,", the output on the target side can be taken as a translation. Compared with its string-based counterparts, tree-based decoding is simpler and faster: there is no need for synchronous binarization (Huang et al., 2009b; Zhang et al., 2006) and tree parsing generally runs in linear time (Huang et al., 2006). While separating parsing and translation makes tree-based decoding simple and efficient, its search space is limited by the number of parse trees offered by parser. Studies reveal that treebased systems are prone to produce degenerate translations due to the propagation of parsing mistakes (Quirk and Corston-Oliver, 2006). This problem can be alleviated by offering more alternatives to the pipeline. An elegant solution is to replace 1-best trees with packed forests that encode exponentially many trees (Mi et al., 2008; Liu et al., 2009). Mi et al. (2008) present an efficient algorithm to match tree-to-string rules against packed forests that encode millions of trees. They prove that offering more alternatives to tree parsing improves translation performance substantially. In this paper, we take a further step towards the direction of offering multiple parses to translation by proposing joint parsing and transl"
C10-1080,J03-4003,0,0.366819,"the original tree from virtual rules. We first construct the tree on the left by substituting the trees of the rules (1), (3), and (4) and then restore the original tree on the right via T. Now, we can calculate the PCFG probability of the original tree. 6 In practice, we pre-compute this PCFG probability and store it in the rule (1) to reduce computational overhead. 2.2.2 Although widely used in natural language processing, PCFGs are often criticized for the lack of lexicalization, which is very important to capture the lexical dependencies between words. Therefore, we use Collins’ Model 1 (Collins, 2003), a simple and effective lexicalized parsing model, as the second parsing feature in our decoder. Following Collins (2003), we first lexicalize a tree by associating a headword h with each nonterminal. Figure 4 gives the lexicalized tree corresponding to Figure 2. The left-hand side of a rule in a lexicalized PCFG is P (h) and the right-hand side has the form: Ln (ln ) . . . L1 (l1 )H(h)R1 (τ1 ) . . . Rm (τm ) (11) where H is the head-child that inherits the headword h from its parent P , L1 . . . Ln and R1 . . . Rm are left and right modifiers of H, and l1 . . . ln and τ1 . . . τm are the cor"
C10-1080,P05-1034,0,0.105504,"Missing"
C10-1080,P03-2041,0,0.171565,"xing) AS(le) x1 :NPB)→held a x1 NPB(NN(huitan))→meeting Table 1: Tree-to-string rules extracted from Figure 2. ¬! huitan Sharon Figure 2: A training example that consists of a Chinese parse, an English sentence, and the word alignment between them. a set of tree-to-string rules obtained from Figure 2. The source side of a rule is a tree fragment and the target side is a string. We use x to denote non-terminals and the associated subscripts indicate the correspondence between non-terminals on both sides. Conventionally, decoding for tree-to-string translation is cast as a tree parsing problem (Eisner, 2003). The tree parsing algorithm visits each node in the input source tree in a top-down order and tries to match each translation rule against the local sub-tree rooted at the node. For example, the first rule in Table 1 matches a sub-tree rooted at IP0,6 in Figure 2. The descendent nodes of IP0,6 (i.e., NPB0,1 , PP1,3 , and VPB3,6 ) can be further matched by other rules in Table 1. The matching procedure runs recursively until the entire tree is covered. Finally, the output on the target side can be taken as a translation. Compared with its string-based counterparts, tree-based decoding is simpl"
C10-1080,P08-1066,0,0.286792,"we cast tree-to-string decoding as a monolingual parsing problem (Melamed, 2004; Chiang, 2007; Galley et al., 2006): the decoder takes a source-language string as input and parses it using the source-projection of SCFG while building the corresponding sub-translations simultaneously. 708 For example, given the Chinese sentence bushi yu sha long juxing le huitan in Figure 2, the derivation in Table 1 explains how a Chinese tree, an English string, and the word alignment between them are generated synchronously. Unlike the string-based systems as described in (Chiang, 2007; Galley et al., 2006; Shen et al., 2008), we exploit the linguistic syntax on the source side explicitly. Therefore, the source parse trees produced by our decoder are meaningful from a linguistic point of view. As tree-to-string rules usually have multiple non-terminals that make decoding complexity generally exponential, synchronous binarization (Huang et al., 2009b; Zhang et al., 2006) is a key technique for applying the CKY algorithm to parsing with tree-to-string rules. 2 Huang et al. (2009b) factor each tree-to-string rule into two SCFG rules: one from the root nonterminal to the subtree, and the other from the subtree to the"
C10-1080,N04-1035,0,0.286019,"Missing"
C10-1080,P06-1121,0,0.053641,"derivation. For example, consider the first rule in Table 1: We call rules the tree roots of which are virtual non-terminals virtual rules and others natural rules. For example, the rule (1) is a natural rule and the rules (3) and (4) are virtual rules. We follow Huang et al. (2009b) to keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1. 4 After binarizing tree-to-string rules into SCFG rules that have at most two non-terminals, we can use the CKY algorithm to parse a source sentence and produce its translation simultaneously as described in (Chiang, 2007; Galley et al., 2006). 2.2 Adding Parsing Models As our decoder produces “genuine” parse trees during decoding, we can integrate parsing models as features together with translation features such as the tree-to-string model, n-gram language model, and word penalty into a discriminative framework (Och, 2003). We expect that parsing and translation could interact with each other: parsing offers linguistically motivated reordering to translation and translation helps parsing resolve ambiguity. IP(x1 :NPB VP(x2 :PP x3 :VPB))→x1 x3 x2 2.2.1 PCFG We use the probabilistic context-free grammar (PCFG) as the first parsing"
C10-1080,2006.amta-papers.8,0,0.223162,"Missing"
C10-1080,D09-1127,1,0.886349,"Missing"
C10-1080,J09-4009,0,0.209095,"rce tree in a top-down order and tries to match each translation rule against the local sub-tree rooted at the node. For example, the first rule in Table 1 matches a sub-tree rooted at IP0,6 in Figure 2. The descendent nodes of IP0,6 (i.e., NPB0,1 , PP1,3 , and VPB3,6 ) can be further matched by other rules in Table 1. The matching procedure runs recursively until the entire tree is covered. Finally, the output on the target side can be taken as a translation. Compared with its string-based counterparts, tree-based decoding is simpler and faster: there is no need for synchronous binarization (Huang et al., 2009b; Zhang et al., 2006) and tree parsing generally runs in linear time (Huang et al., 2006). While separating parsing and translation makes tree-based decoding simple and efficient, its search space is limited by the number of parse trees offered by parser. Studies reveal that treebased systems are prone to produce degenerate translations due to the propagation of parsing mistakes (Quirk and Corston-Oliver, 2006). This problem can be alleviated by offering more alternatives to the pipeline. An elegant solution is to replace 1-best trees with packed forests that encode exponentially many trees ("
C10-1080,P08-1067,0,0.0662005,"Missing"
C10-1080,P06-1077,1,0.949632,"Missing"
C10-1080,P09-1063,1,0.874797,"Missing"
C10-1080,W04-3207,0,0.0208115,"language model for machine translation. Recently, Shen et al. (2008) have shown that dependency language model is beneficial for capturing long-distance relations between target words. As our approach adds parsing models to the source side where the source sentence is fixed during decoding, our decoder does parse the source sentence like a monolingual parser instead of a syntax-based language model. More importantly, we integrate translation models and parsing models in a discriminative framework where they can interact with each other directly. Our work also has connections to joint parsing (Smith and Smith, 2004; Burkett and Klein, 2008) and bilingually-constrained monolingual parsing (Huang et al., 2009a) because we use another language to resolve ambiguity for one language. However, while both joint parsing and bilinguallyconstrained monolingual parsing rely on the target sentence, our approach only takes a source sentence as input. Blunsom and Osborne (2008) incorporate the source-side parse trees into their probabilistic SCFG framework and treat every source-parse PCFG rule as an individual feature. The difference is that they parse the test set before decoding so as to exploit the source syntact"
C10-1080,C10-1135,1,0.716482,"inguallyconstrained monolingual parsing rely on the target sentence, our approach only takes a source sentence as input. Blunsom and Osborne (2008) incorporate the source-side parse trees into their probabilistic SCFG framework and treat every source-parse PCFG rule as an individual feature. The difference is that they parse the test set before decoding so as to exploit the source syntactic information to guide translation. More recently, Chiang (2010) has shown that (“exact”) tree-to-tree translation as parsing achieves comparable performance with Hiero (Chiang, 2007) using much fewer rules. Xiao et al. (2010) integrate tokenization and translation into a single step and improve the performance of tokenization and translation significantly. 5 Conclusion We have presented a framework for joint parsing and translation by casting tree-to-string translation as a parsing problem. While tree-to-string rules construct parse trees on the source side and translations on the target side simultaneously, parsing models can be integrated to improve both translation and parsing quality. This work can be considered as a final step towards the continuum of tree-to-string translation: from single tree to forest and"
C10-1080,I05-1007,1,0.854995,"Missing"
C10-1080,P01-1067,0,0.116813,"dels, the F1 score is 62.7. Adding Collins’ Model 1 results in much larger gains than adding PCFG. With all parsing models integrated, our joint decoder achieves an F1 score of 80.6 on the test set. Although lower than the F1 score of the in-house parser that produces the noisy training data, this result is still very promising because the tree-to-string rules that construct trees in the decoding process are learned from noisy training data. 4 Related Work Charniak et al. (2003) firstly associate lexicalized parsing model with syntax-based translation. They first run a string-to-tree decoder (Yamada and Knight, 2001) to produce an English parse forest and then use a lexicalized parsing model to select the best translation from the forest. As the parsing model operates on the target side, it actually serves as a syntax-based language model for machine translation. Recently, Shen et al. (2008) have shown that dependency language model is beneficial for capturing long-distance relations between target words. As our approach adds parsing models to the source side where the source sentence is fixed during decoding, our decoder does parse the source sentence like a monolingual parser instead of a syntax-based l"
C10-1080,N06-1033,0,0.170021,"n order and tries to match each translation rule against the local sub-tree rooted at the node. For example, the first rule in Table 1 matches a sub-tree rooted at IP0,6 in Figure 2. The descendent nodes of IP0,6 (i.e., NPB0,1 , PP1,3 , and VPB3,6 ) can be further matched by other rules in Table 1. The matching procedure runs recursively until the entire tree is covered. Finally, the output on the target side can be taken as a translation. Compared with its string-based counterparts, tree-based decoding is simpler and faster: there is no need for synchronous binarization (Huang et al., 2009b; Zhang et al., 2006) and tree parsing generally runs in linear time (Huang et al., 2006). While separating parsing and translation makes tree-based decoding simple and efficient, its search space is limited by the number of parse trees offered by parser. Studies reveal that treebased systems are prone to produce degenerate translations due to the propagation of parsing mistakes (Quirk and Corston-Oliver, 2006). This problem can be alleviated by offering more alternatives to the pipeline. An elegant solution is to replace 1-best trees with packed forests that encode exponentially many trees (Mi et al., 2008; Liu e"
C10-1080,P08-1064,0,0.101116,"Missing"
C10-1080,D08-1060,0,0.0349263,"(5) Then, the rule (2) can be further binarized into k=1...K two rules that have at most two non-terminals: For example, the probability for the tree in FigT → hNPB 1 PP-VPB 2 , NPB 1 PP-VPB 2 i (3) ure 2 is PP-VPB → hPP 1 VPB 2 , VPB 2 PP 1 i (4) P(π) = Ppcf g (IP → NPB VP) × where PP-VPB is an intermediate virtual nonPpcf g (NPB → NR) × terminal. Ppcf g (NR → bushi) × 2 But CKY is not the only choice. The Earley algorithm We use a specific non-terminal, say, T, to uniquely identify the left-hand side subtree and produce two SCFG rules: 3 can also be used to parse with tree-to-string rules (Zhao and Al-Onaizan, 2008). As the Earley algorithm binarizes multinonterminal rules implicitly, there is no need for synchronous binarization. 3 It might look strange that the node VP disappears. This node is actually stored in the monolithic node T. Please refer to page 573 of (Huang et al., 2009b) for more details about how to convert tree-to-string rules to SCFG rules. ... (6) 4 This makes the scores of hypotheses in the same chart cell hardly comparable because some hypotheses are covered by a natural non-terminal and others covered by a virtual non-terminal. To alleviate this problem, we follow Huang et al. (2009"
C10-1080,D08-1022,0,\N,Missing
C10-1123,P05-1067,0,0.0626954,"string-to-dependency system obtains significant improvements ranging from 1.36 to 1.46 BLEU points over the tree-based baseline on the NIST 2004/2005/2006 Chinese-English test sets. 1 Introduction Dependency grammars have become increasingly popular in syntax-based statistical machine translation (SMT). One important advantage of dependency grammars is that they directly capture the dependencies between words, which are key to resolving most parsing ambiguities. As a result, incorporating dependency trees proves to be effective in improving statistical machine translation (Quirk et al., 2005; Ding and Palmer, 2005; Shen et al., 2008). However, most dependency-based translation systems suffer from a major drawback: they only use 1-best dependency trees for rule extraction, dependency language model training, and decoding, which potentially introduces translation mistakes due to the propagation of parsing errors (Quirk and Corston-Oliver, 2006). While the treelet system (Quirk et al., 2005) takes a dependency tree as input, the string-to-dependency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits to using degenerate rul"
C10-1123,P08-1115,0,0.0433796,"endency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits to using degenerate rules and dependency language models learned from noisy 1-best trees. HILab Convergence Technology Center C&I Business SKTelecom yshwang@sktelecom.com To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang, 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al., 2009). Along the same direction, we propose a structure called dependency forest, which encodes exponentially many dependency trees compactly, for dependency-based translation systems. In this paper, we develop two new algorithms for extracting string-to-dependency rules and for training dependency language models, respectively. We show that using the rules and dependency language models learned from dependency forests leads to consistent and significant improvements over that of using 1-best trees on the NIST 2004/200"
C10-1123,N04-1035,0,0.0493122,".append(r) keep k-best dependency structures for v It is difficult to assign a probability to each hyperedge. The current method is arbitrary, and we will improve it in the future. 4 Forest-based Rule Extraction In tree-based rule extraction, one just needs to first enumerate all bilingual phrases that are consistent with word alignment and then check whether the dependency structures over the target phrases are well-formed. However, this algorithm fails to work in the forest scenario because there are usually exponentially many well-formed structures over a target phrase. The GHKM algorithm (Galley et al., 2004), which is originally developed for extracting treeto-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). The algorithm distinguishes between minimal and composed rules. Although there are exponentially many composed rules, the number of minimal rules extracted from each node is rather limited (e.g., one or zero). Therefore, one can obtain promising composed rules by combining minimal rules. Unfortunately, the GHKM algorithm cannot be applied to extracting string-to-dependency rules from dependency forests. This is because the GHKM al"
C10-1123,W05-1506,0,0.411678,"i denotes that he0,1 , boy2,4 , and with4,7 are dependants (from left to right) of saw0,7 . More formally, a dependency forest is a pair hV, Ei, where V is a set of nodes, and E is a set of hyperedges. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form of wi,j , which denotes that w dominates the substring from positions i through j (i.e., wi+1 . . . wj ). Each hyperedge e ∈ E is a pair htails(e), head(e)i, where head(e) ∈ V is the head and tails(e) ∈ V are its dependants. A dependency forest has a structure of a hypergraph such as packed forest (Klein and Manning, 2001; Huang and Chiang, 2005). However, while each hyperedge in a packed forest naturally treats the corresponding PCFG rule probability as its weight, it is challenging to make dependency forest to be a weighted hypergraph because dependency parsers usually only output a score, which can be either positive or negative, for each edge in a dependency tree rather than a hyperedge in a 1094 saw0,7 e1 he0,1 Algorithm 1 Forest-based Initial Phrase Extraction e2 boy2,4 e3 boy2,7 e4 a2,3 with4,7 e5 he ta saw a boy kandao yige dai with 1: 2: 3: telescope5,7 4: e6 5: 6: a5,6 7: 8: a telescope 9: 10: 11: wangyuanjing de nanhai Figu"
C10-1123,D09-1127,1,0.908687,"Missing"
C10-1123,W01-1812,0,0.060847,"y2,4 , with4,7 ), saw0,7 i denotes that he0,1 , boy2,4 , and with4,7 are dependants (from left to right) of saw0,7 . More formally, a dependency forest is a pair hV, Ei, where V is a set of nodes, and E is a set of hyperedges. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form of wi,j , which denotes that w dominates the substring from positions i through j (i.e., wi+1 . . . wj ). Each hyperedge e ∈ E is a pair htails(e), head(e)i, where head(e) ∈ V is the head and tails(e) ∈ V are its dependants. A dependency forest has a structure of a hypergraph such as packed forest (Klein and Manning, 2001; Huang and Chiang, 2005). However, while each hyperedge in a packed forest naturally treats the corresponding PCFG rule probability as its weight, it is challenging to make dependency forest to be a weighted hypergraph because dependency parsers usually only output a score, which can be either positive or negative, for each edge in a dependency tree rather than a hyperedge in a 1094 saw0,7 e1 he0,1 Algorithm 1 Forest-based Initial Phrase Extraction e2 boy2,4 e3 boy2,7 e4 a2,3 with4,7 e5 he ta saw a boy kandao yige dai with 1: 2: 3: telescope5,7 4: e6 5: 6: a5,6 7: 8: a telescope 9: 10: 11: wa"
C10-1123,D09-1106,1,0.801307,"we will show, the string-to-dependency system still commits to using degenerate rules and dependency language models learned from noisy 1-best trees. HILab Convergence Technology Center C&I Business SKTelecom yshwang@sktelecom.com To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang, 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al., 2009). Along the same direction, we propose a structure called dependency forest, which encodes exponentially many dependency trees compactly, for dependency-based translation systems. In this paper, we develop two new algorithms for extracting string-to-dependency rules and for training dependency language models, respectively. We show that using the rules and dependency language models learned from dependency forests leads to consistent and significant improvements over that of using 1-best trees on the NIST 2004/2005/2006 Chinese-English test sets. 2 Background Figure 1 shows a dependency tree o"
C10-1123,D08-1022,0,0.607088,"rk et al., 2005) takes a dependency tree as input, the string-to-dependency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits to using degenerate rules and dependency language models learned from noisy 1-best trees. HILab Convergence Technology Center C&I Business SKTelecom yshwang@sktelecom.com To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang, 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al., 2009). Along the same direction, we propose a structure called dependency forest, which encodes exponentially many dependency trees compactly, for dependency-based translation systems. In this paper, we develop two new algorithms for extracting string-to-dependency rules and for training dependency language models, respectively. We show that using the rules and dependency language models learned from dependency forests leads to consistent and significan"
C10-1123,P00-1056,0,0.135647,"Each n-gram (e.g., “boy-as-head a”) is assigned the same fractional count of the hyperedge it belongs to. We also tried training dependency language model as in (Shen et al., 2008), which means all hyperedges were on equal footing without regarding probabilities. However, the performance is about 0.8 point lower in BLEU. One possbile reason is that hyperedges with probabilities could distinguish high quality structures better. 6 Experiments 6.1 Results on the Chinese-English Task We used the FBIS corpus (6.9M Chinese words + 8.9M English words) as our bilingual training corpus. We ran GIZA++ (Och and Ney, 2000) to obtain word alignments. We trained a 4-gram language model on the Xinhua portion of GIGAWORD corpus using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We optimized feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) on the NIST 2004/2005/2006 test sets. To obtain dependency trees and forests, we parsed the English sentences of the FBIS corpus using a shift-reduce dependency"
C10-1123,P02-1038,0,0.220964,"Missing"
C10-1123,J04-4002,0,0.0457485,"dency structure di..j is fixed on head h, where h ∈ / [i, j], or fixed for short, if and only if it meets the following conditions telescope a (b) (c) and the word alignments between them. To facilitate identifying the correspondence between the English and Chinese words, we also gives the English sentence. Extracting string-to-dependency rules from aligned string-dependency pairs is similar to extracting SCFG (Chiang, 2007) except that the target side of a rule is a well-formed structure. For example, we can first extract a string-todependency rule that is consistent with the word alignment (Och and Ney, 2004): with ((a) telescope) → dai wangyuanjing de Then a smaller rule (a) telescope → wangyuanjing can be subtracted to obtain a rule with one nonterminal: • dh ∈ / [i, j] with (X1 ) → dai X1 de • ∀k ∈ [i, j] and k 6= h, dk ∈ [i, j] • ∀k ∈ / [i, j], dk = h or dk ∈ / [i, j] Definition 2. A dependency structure di..j is floating with children C, for a non-empty set C ⊆ {i, ..., j}, or floating for short, if and only if it meets the following conditions • ∃h ∈ / [i, j], s.t.∀k ∈ C, dk = h • ∀k ∈ [i, j] and k ∈ / C, dk ∈ [i, j] • ∀k ∈ / [i, j], dk ∈ / [i, j] A dependency structure is well-formed if and"
C10-1123,P02-1040,0,0.0903191,"Missing"
C10-1123,W06-1608,0,0.0253466,"age of dependency grammars is that they directly capture the dependencies between words, which are key to resolving most parsing ambiguities. As a result, incorporating dependency trees proves to be effective in improving statistical machine translation (Quirk et al., 2005; Ding and Palmer, 2005; Shen et al., 2008). However, most dependency-based translation systems suffer from a major drawback: they only use 1-best dependency trees for rule extraction, dependency language model training, and decoding, which potentially introduces translation mistakes due to the propagation of parsing errors (Quirk and Corston-Oliver, 2006). While the treelet system (Quirk et al., 2005) takes a dependency tree as input, the string-to-dependency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits to using degenerate rules and dependency language models learned from noisy 1-best trees. HILab Convergence Technology Center C&I Business SKTelecom yshwang@sktelecom.com To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed fo"
C10-1123,P05-1034,0,0.308079,"s. Our forest-based string-to-dependency system obtains significant improvements ranging from 1.36 to 1.46 BLEU points over the tree-based baseline on the NIST 2004/2005/2006 Chinese-English test sets. 1 Introduction Dependency grammars have become increasingly popular in syntax-based statistical machine translation (SMT). One important advantage of dependency grammars is that they directly capture the dependencies between words, which are key to resolving most parsing ambiguities. As a result, incorporating dependency trees proves to be effective in improving statistical machine translation (Quirk et al., 2005; Ding and Palmer, 2005; Shen et al., 2008). However, most dependency-based translation systems suffer from a major drawback: they only use 1-best dependency trees for rule extraction, dependency language model training, and decoding, which potentially introduces translation mistakes due to the propagation of parsing errors (Quirk and Corston-Oliver, 2006). While the treelet system (Quirk et al., 2005) takes a dependency tree as input, the string-to-dependency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits"
C10-1123,P08-1066,0,0.574261,"stem obtains significant improvements ranging from 1.36 to 1.46 BLEU points over the tree-based baseline on the NIST 2004/2005/2006 Chinese-English test sets. 1 Introduction Dependency grammars have become increasingly popular in syntax-based statistical machine translation (SMT). One important advantage of dependency grammars is that they directly capture the dependencies between words, which are key to resolving most parsing ambiguities. As a result, incorporating dependency trees proves to be effective in improving statistical machine translation (Quirk et al., 2005; Ding and Palmer, 2005; Shen et al., 2008). However, most dependency-based translation systems suffer from a major drawback: they only use 1-best dependency trees for rule extraction, dependency language model training, and decoding, which potentially introduces translation mistakes due to the propagation of parsing errors (Quirk and Corston-Oliver, 2006). While the treelet system (Quirk et al., 2005) takes a dependency tree as input, the string-to-dependency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits to using degenerate rules and dependency la"
C10-1123,D07-1078,0,0.013782,"ructures of a head can be constructed from those of its dependants. For example, in Figure 4, as the fixed structure rooted at telescope 5,7 is (a) telescope we can obtain a fixed structure rooted for the node with4,7 by attaching the fixed structure of its dependant to the node (EnumFixed in line 4). Figure 2(b) shows the resulting fixed structure. Similarly, the floating structure for the node saw0,7 can be obtained by concatenating the fixed structures of its dependants boy2,4 and with4,7 (EnumFloating in line 5). Figure 2(c) shows the resulting fixed structure. The algorithm is similar to Wang et al. (2007), which binarize each constituent node to create some intermediate nodes that correspond to the floating structures. Therefore, we can find k-best fixed and floating structures for a node in a dependency forest by manipulating the fixed structures of its dependants. Then we can extract string-to-dependency rules if the dependency structures are consistent with the word alignment. How to judge a well-formed structure extracted from a node is better than others? We follow Mi and Huang (2008) to assign a fractional count to each well-formed structure. Given a tree fragment t, we use the inside-ou"
C10-1123,P08-1023,1,\N,Missing
C10-1123,P03-1041,0,\N,Missing
C10-1123,P08-1067,0,\N,Missing
C10-1123,W06-1606,0,\N,Missing
C10-1123,P05-1033,0,\N,Missing
C10-1123,P08-1010,0,\N,Missing
C10-1123,J07-2003,0,\N,Missing
C10-1123,2008.amta-papers.18,0,\N,Missing
C10-1135,W08-0336,0,0.0683447,"nput sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially introduce translation mistakes. As some languages such as Chinese have no spaces in their writing systems, how to segment sentences into appropriate words has a direct impact on translation performance (Xu et al., 2005; Chang et al., 2008; Zhang et al., 2008). In addition, although agglutinative languages such as Korean incorporate spaces between “words”, which consist of multiple morphemes, the granularity is too coarse and makes the training data 1200 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1200–1208, Beijing, August 2010 considerably sparse. Studies reveal that segmenting “words” into morphemes effectively improves translating morphologically rich languages (Oflazer, 2008). More importantly, a tokenization close to a gold standard does not necessarily leads to bette"
C10-1135,J07-2003,0,0.702362,"enizer, our joint decoder achieves significant improvements over monolingual Chinese tokenizers. translation (a) source string tokenization tokenize+translate target translation (b) Figure 1: (a) Separate tokenization and translation and (b) joint tokenization and translation. 1 Introduction Tokenization plays an important role in statistical machine translation (SMT) because tokenizing a source-language sentence is always the first step in SMT systems. Based on the type of input, Mi and Huang (2008) distinguish between two categories of SMT systems : string-based systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Shen et al., 2008) that take a string as input and tree-based systems (Liu et al., 2006; Mi et al., 2008) that take a tree as input. Note that a treebased system still needs to first tokenize the input sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially"
C10-1135,D09-1075,0,0.119698,"is no gold standard data for tokenization, we do not use ME and LM tokenization features here. However, our joint method can still significantly (p < 0.05) improve the performance by about +0.3 points. This also reflects the importance of optimizing granularity for morphological complex languages. Related Work Methods have been proposed to optimize tokenization for word alignment. For example, word alignment can be simplified by packing (Ma et al., 2007) several consecutive words together. Word alignment and tokenization can also be optimized by maximizing the likelihood of bilingual corpus (Chung and Gildea, 2009; Xu et al., 2008). In fact, these work are orthogonal to our joint method, since they focus on training step while we are concerned of decoding. We believe we can further the performance by combining these two kinds of work. Our work also has connections to multilingual tokenization (Snyder and Barzilay, 2008). While they have verified that tokenization can be improved by multilingual learning, our work shows that we can also improve tokenization by collaborating with translation task in a supervised way. More recently, Liu and Liu (2010) also shows the effect of joint method. They integrate"
C10-1135,P08-1115,0,0.42896,"t 2010 considerably sparse. Studies reveal that segmenting “words” into morphemes effectively improves translating morphologically rich languages (Oflazer, 2008). More importantly, a tokenization close to a gold standard does not necessarily leads to better translation quality (Chang et al., 2008; Zhang et al., 2008). Therefore, it is necessary to offer more tokenizations to SMT systems to alleviate the tokenization error propagation problem. Recently, many researchers have shown that replacing 1-best tokenizations with lattices improves translation performance significantly (Xu et al., 2005; Dyer et al., 2008; Dyer, 2009). We take a next step towards the direction of offering more tokenizations to SMT systems by proposing joint tokenization and translation. As shown in Figure 1(b), our approach tokenizes and translates jointly to find a tokenization and a translation for a source-language string simultaneously. We integrate translation and tokenization models into a discriminative framework (Och and Ney, 2002), within which tokenization and translation models interact with each other. Experiments show that joint tokenization and translation outperforms its separate counterparts (1best tokenization"
C10-1135,N09-1046,0,0.0470927,"sparse. Studies reveal that segmenting “words” into morphemes effectively improves translating morphologically rich languages (Oflazer, 2008). More importantly, a tokenization close to a gold standard does not necessarily leads to better translation quality (Chang et al., 2008; Zhang et al., 2008). Therefore, it is necessary to offer more tokenizations to SMT systems to alleviate the tokenization error propagation problem. Recently, many researchers have shown that replacing 1-best tokenizations with lattices improves translation performance significantly (Xu et al., 2005; Dyer et al., 2008; Dyer, 2009). We take a next step towards the direction of offering more tokenizations to SMT systems by proposing joint tokenization and translation. As shown in Figure 1(b), our approach tokenizes and translates jointly to find a tokenization and a translation for a source-language string simultaneously. We integrate translation and tokenization models into a discriminative framework (Och and Ney, 2002), within which tokenization and translation models interact with each other. Experiments show that joint tokenization and translation outperforms its separate counterparts (1best tokenizations and lattice"
C10-1135,P06-1121,0,0.0317509,"int decoder achieves significant improvements over monolingual Chinese tokenizers. translation (a) source string tokenization tokenize+translate target translation (b) Figure 1: (a) Separate tokenization and translation and (b) joint tokenization and translation. 1 Introduction Tokenization plays an important role in statistical machine translation (SMT) because tokenizing a source-language sentence is always the first step in SMT systems. Based on the type of input, Mi and Huang (2008) distinguish between two categories of SMT systems : string-based systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Shen et al., 2008) that take a string as input and tree-based systems (Liu et al., 2006; Mi et al., 2008) that take a tree as input. Note that a treebased system still needs to first tokenize the input sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially introduce translation"
C10-1135,P08-1102,1,0.87833,"Missing"
C10-1135,N03-1017,0,0.0193365,"erestingly, as a tokenizer, our joint decoder achieves significant improvements over monolingual Chinese tokenizers. translation (a) source string tokenization tokenize+translate target translation (b) Figure 1: (a) Separate tokenization and translation and (b) joint tokenization and translation. 1 Introduction Tokenization plays an important role in statistical machine translation (SMT) because tokenizing a source-language sentence is always the first step in SMT systems. Based on the type of input, Mi and Huang (2008) distinguish between two categories of SMT systems : string-based systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Shen et al., 2008) that take a string as input and tree-based systems (Liu et al., 2006; Mi et al., 2008) that take a tree as input. Note that a treebased system still needs to first tokenize the input sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization error"
C10-1135,W04-3250,0,0.0218819,"05 33.06 33.22 30.91 33.95 33.76 34.69 34.56 34.17 34.88** Speed 2.48 2.55 2.34 3.83 6.79 17.66 17.37 17.23 17.52 Table 2: Comparison of Separate and Joint methods in terms of BLEU and speed (second per sentence). Columns Train and Test represents the tokenization methods for training and testing respectively. Column TFs stands for whether the 8 √ tokenization features is used ( ) or not (×). ICT, SF and ME are segmenter names for preprocessing. All means combined corpus processed by the three segmenters. Lattice represent the system implemented as Dyer et al., (2008). ** means significantly (Koehn, 2004) better than Lattice (p < 0.01). 4 Experiments In this section, we try to answer the following questions: 1. Does the joint method outperform conventional methods that separate tokenization from decoding. (Section 4.1) 2. How about the tokenization performance of the joint decoder? (Section 4.2) 4.1 Translation Evaluation We use the SCFG model (Chiang, 2007) for our experiments. We firstly work on the ChineseEnglish translation task. The bilingual training data contains 1.5M sentence pairs coming from LDC data.1 The monolingual data for training English language model includes Xinhua portion o"
C10-1135,C10-1080,1,0.716304,"by maximizing the likelihood of bilingual corpus (Chung and Gildea, 2009; Xu et al., 2008). In fact, these work are orthogonal to our joint method, since they focus on training step while we are concerned of decoding. We believe we can further the performance by combining these two kinds of work. Our work also has connections to multilingual tokenization (Snyder and Barzilay, 2008). While they have verified that tokenization can be improved by multilingual learning, our work shows that we can also improve tokenization by collaborating with translation task in a supervised way. More recently, Liu and Liu (2010) also shows the effect of joint method. They integrate parsing and translation into a single step and improve the performance of translation significantly. 6 Conclusion We have presented a novel method for joint tokenization and translation which directly combines the tokenization model into the decoding phase. Allowing tokenization and translation to collaborate with each other, tokenization can be optimized for translation, while translation also makes contribution to tokenization performance under a supervised way. We believe that our approach can be applied to other string-based model such"
C10-1135,P06-1077,1,0.83937,"on (a) source string tokenization tokenize+translate target translation (b) Figure 1: (a) Separate tokenization and translation and (b) joint tokenization and translation. 1 Introduction Tokenization plays an important role in statistical machine translation (SMT) because tokenizing a source-language sentence is always the first step in SMT systems. Based on the type of input, Mi and Huang (2008) distinguish between two categories of SMT systems : string-based systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Shen et al., 2008) that take a string as input and tree-based systems (Liu et al., 2006; Mi et al., 2008) that take a tree as input. Note that a treebased system still needs to first tokenize the input sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially introduce translation mistakes. As some languages such as Chinese have no spaces in their writing systems, how"
C10-1135,P07-1039,0,0.0879531,"odel includes Xinhua portion of the GIGAWORD corpus, which contains 238M English words. We use the NIST evaluation sets of 2002 (MT02) as our development data set, and sets of 2004(MT04) and 2005(MT05) as test sets. We use the corpus derived from the People’s Daily (Renmin Ribao) in Feb. to Jun. 1998 containing 6M words for training LM and ME tokenization models. Translation Part. We used GIZA++ (Och and Ney, 2003) to perform word alignment in both directions, and grow-diag-final-and (Koehn et al., 2003) to generate symmetric word alignment. We extracted the SCFG rules as describing in Chiang (2007). The language model were trained by the 1 including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 SRILM toolkit (Stolcke, 2002).2 Case insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. Tokenization Part. We used the toolkit implemented by Zhang (2004) to train the ME model. Three Chinese word segmenters were used for comparing: ICTCLAS (ICT) developed by institute of Computing Technology Chinese Academy of Sciences (Zhang et al., 2003); SF developed at Stanford University (Huihsin et al., 2005) and ME whi"
C10-1135,P08-1023,1,0.879426,"ng tokenization tokenize+translate target translation (b) Figure 1: (a) Separate tokenization and translation and (b) joint tokenization and translation. 1 Introduction Tokenization plays an important role in statistical machine translation (SMT) because tokenizing a source-language sentence is always the first step in SMT systems. Based on the type of input, Mi and Huang (2008) distinguish between two categories of SMT systems : string-based systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Shen et al., 2008) that take a string as input and tree-based systems (Liu et al., 2006; Mi et al., 2008) that take a tree as input. Note that a treebased system still needs to first tokenize the input sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially introduce translation mistakes. As some languages such as Chinese have no spaces in their writing systems, how to segment senten"
C10-1135,W04-3236,0,0.0287871,"okenization and translation could collaborate with each other. Tokenization offers translation with good tokenized results, while translation helps tokenization to eliminate ambiguity. Formally, the probability of a derivation D is represented as P (D) ∝ Y λi φi (D) (1) i Since our model is still a string-based model, the CKY algorithm and cube pruning are still applicable for our model to find the derivation with max score. 3.2 Adding Tokenization Features Maximum Entropy model (ME). We first introduce ME model feature for tokenization by casting it as a labeling problem (Xue and Shen, 2003; Ng and Low, 2004). We label a character with the following 4 types: • b: the begin of a word • m: the middle of a word • e: the end of a word • s: a single-character word Taking the tokenization you-wang of the string you wang for example, we first create a label sequence b e for the tokenization you-wang and then calculate the probability of tokenization by P (you-wang |you wang) = P (b e |you wang) = P (b |you, you wang) × P (e |wang, you wang) Given a tokenization w1L with L words for a character sequence cn1 , we firstly create labels l1n for every characters and then calculate the probability by where φi"
C10-1135,P02-1038,0,0.0512261,"okenization error propagation problem. Recently, many researchers have shown that replacing 1-best tokenizations with lattices improves translation performance significantly (Xu et al., 2005; Dyer et al., 2008; Dyer, 2009). We take a next step towards the direction of offering more tokenizations to SMT systems by proposing joint tokenization and translation. As shown in Figure 1(b), our approach tokenizes and translates jointly to find a tokenization and a translation for a source-language string simultaneously. We integrate translation and tokenization models into a discriminative framework (Och and Ney, 2002), within which tokenization and translation models interact with each other. Experiments show that joint tokenization and translation outperforms its separate counterparts (1best tokenizations and lattices) significantly on the NIST 2004 and 2005 Chinese-English test sets. Our joint decoder also reports positive results on Korean-Chinese translation. As a tokenizer, our joint decoder achieves significantly better tokenization accuracy than three monolingual Chinese tokenizers. 2 Separate Tokenization and Translation Tokenization is to split a string of characters into meaningful elements, whic"
C10-1135,J03-1002,0,0.00401364,"xperiments. We firstly work on the ChineseEnglish translation task. The bilingual training data contains 1.5M sentence pairs coming from LDC data.1 The monolingual data for training English language model includes Xinhua portion of the GIGAWORD corpus, which contains 238M English words. We use the NIST evaluation sets of 2002 (MT02) as our development data set, and sets of 2004(MT04) and 2005(MT05) as test sets. We use the corpus derived from the People’s Daily (Renmin Ribao) in Feb. to Jun. 1998 containing 6M words for training LM and ME tokenization models. Translation Part. We used GIZA++ (Och and Ney, 2003) to perform word alignment in both directions, and grow-diag-final-and (Koehn et al., 2003) to generate symmetric word alignment. We extracted the SCFG rules as describing in Chiang (2007). The language model were trained by the 1 including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 SRILM toolkit (Stolcke, 2002).2 Case insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. Tokenization Part. We used the toolkit implemented by Zhang (2004) to train the ME model. Three Chinese word segmenters were used for com"
C10-1135,P03-1021,0,0.0602288,"Missing"
C10-1135,P02-1040,0,0.079414,"Missing"
C10-1135,P08-1066,0,0.0259864,"significant improvements over monolingual Chinese tokenizers. translation (a) source string tokenization tokenize+translate target translation (b) Figure 1: (a) Separate tokenization and translation and (b) joint tokenization and translation. 1 Introduction Tokenization plays an important role in statistical machine translation (SMT) because tokenizing a source-language sentence is always the first step in SMT systems. Based on the type of input, Mi and Huang (2008) distinguish between two categories of SMT systems : string-based systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Shen et al., 2008) that take a string as input and tree-based systems (Liu et al., 2006; Mi et al., 2008) that take a tree as input. Note that a treebased system still needs to first tokenize the input sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially introduce translation mistakes. As some l"
C10-1135,P08-1084,0,0.0280376,"Work Methods have been proposed to optimize tokenization for word alignment. For example, word alignment can be simplified by packing (Ma et al., 2007) several consecutive words together. Word alignment and tokenization can also be optimized by maximizing the likelihood of bilingual corpus (Chung and Gildea, 2009; Xu et al., 2008). In fact, these work are orthogonal to our joint method, since they focus on training step while we are concerned of decoding. We believe we can further the performance by combining these two kinds of work. Our work also has connections to multilingual tokenization (Snyder and Barzilay, 2008). While they have verified that tokenization can be improved by multilingual learning, our work shows that we can also improve tokenization by collaborating with translation task in a supervised way. More recently, Liu and Liu (2010) also shows the effect of joint method. They integrate parsing and translation into a single step and improve the performance of translation significantly. 6 Conclusion We have presented a novel method for joint tokenization and translation which directly combines the tokenization model into the decoding phase. Allowing tokenization and translation to collaborate w"
C10-1135,2005.iwslt-1.18,0,0.0338026,"st tokenize the input sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially introduce translation mistakes. As some languages such as Chinese have no spaces in their writing systems, how to segment sentences into appropriate words has a direct impact on translation performance (Xu et al., 2005; Chang et al., 2008; Zhang et al., 2008). In addition, although agglutinative languages such as Korean incorporate spaces between “words”, which consist of multiple morphemes, the granularity is too coarse and makes the training data 1200 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1200–1208, Beijing, August 2010 considerably sparse. Studies reveal that segmenting “words” into morphemes effectively improves translating morphologically rich languages (Oflazer, 2008). More importantly, a tokenization close to a gold standard does not necess"
C10-1135,C08-1128,0,0.0174115,"a for tokenization, we do not use ME and LM tokenization features here. However, our joint method can still significantly (p < 0.05) improve the performance by about +0.3 points. This also reflects the importance of optimizing granularity for morphological complex languages. Related Work Methods have been proposed to optimize tokenization for word alignment. For example, word alignment can be simplified by packing (Ma et al., 2007) several consecutive words together. Word alignment and tokenization can also be optimized by maximizing the likelihood of bilingual corpus (Chung and Gildea, 2009; Xu et al., 2008). In fact, these work are orthogonal to our joint method, since they focus on training step while we are concerned of decoding. We believe we can further the performance by combining these two kinds of work. Our work also has connections to multilingual tokenization (Snyder and Barzilay, 2008). While they have verified that tokenization can be improved by multilingual learning, our work shows that we can also improve tokenization by collaborating with translation task in a supervised way. More recently, Liu and Liu (2010) also shows the effect of joint method. They integrate parsing and transl"
C10-1135,W03-1728,0,0.0308134,"amework. We expect tokenization and translation could collaborate with each other. Tokenization offers translation with good tokenized results, while translation helps tokenization to eliminate ambiguity. Formally, the probability of a derivation D is represented as P (D) ∝ Y λi φi (D) (1) i Since our model is still a string-based model, the CKY algorithm and cube pruning are still applicable for our model to find the derivation with max score. 3.2 Adding Tokenization Features Maximum Entropy model (ME). We first introduce ME model feature for tokenization by casting it as a labeling problem (Xue and Shen, 2003; Ng and Low, 2004). We label a character with the following 4 types: • b: the begin of a word • m: the middle of a word • e: the end of a word • s: a single-character word Taking the tokenization you-wang of the string you wang for example, we first create a label sequence b e for the tokenization you-wang and then calculate the probability of tokenization by P (you-wang |you wang) = P (b e |you wang) = P (b |you, you wang) × P (e |wang, you wang) Given a tokenization w1L with L words for a character sequence cn1 , we firstly create labels l1n for every characters and then calculate the proba"
C10-1135,W03-1730,1,0.832219,"Missing"
C10-1135,W08-0335,0,0.180017,"en obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially introduce translation mistakes. As some languages such as Chinese have no spaces in their writing systems, how to segment sentences into appropriate words has a direct impact on translation performance (Xu et al., 2005; Chang et al., 2008; Zhang et al., 2008). In addition, although agglutinative languages such as Korean incorporate spaces between “words”, which consist of multiple morphemes, the granularity is too coarse and makes the training data 1200 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1200–1208, Beijing, August 2010 considerably sparse. Studies reveal that segmenting “words” into morphemes effectively improves translating morphologically rich languages (Oflazer, 2008). More importantly, a tokenization close to a gold standard does not necessarily leads to better translation quality"
C10-1135,D08-1022,0,\N,Missing
C10-1135,I05-3027,0,\N,Missing
C10-2033,P05-1066,0,0.0967175,"s, including beam size, phrase table limit and so on, were the same, so the performance was compared fairly. First, we will show the performance of shiftreduce algorithm on three data sets with large training data sets (Section 4.1). Then, we will analyze the performance elaborately in terms of accuracy, speed and search ability with a smaller 289 training data set (Section 4.2). All experiments were done on Chinese-to-English translation tasks and all results are reported with case insensitive BLEU score. Statistical significance were computed using the sign-test described in Collins et al. (Collins et al., 2005). 4.1 Performance Evaluation NIST06 30.24 30.27 30.35 —— 30.47 NIST08 25.08 23.80 26.23** 25.09 26.67** speed 4.827 1.501 4.335 3.856 4.126 NIST05 35.80 35.03 36.56** 35.84 36.42** speed 7.142 1.811 6.276 5.008 5.432 (a) We did three experiments to compare the performance of the shift-reduce decoder, Moses and the decoder with ITG constraints using cover vector (denoted as CV). 2 The shift-reduce decoder decoded with two sets of parameters: one was tuned by itself (denoted as SR) and the other was tuned by Moses (denoted as SR-same), using MERT (Och, 2003). Two searching algorithms of Moses ar"
C10-2033,D08-1089,0,0.198885,"ransitions LShift and RShift push [i, j] into St , they check whether [i, j] is adjacent to the top block of St . If so, they change the top block into the merged block directly. In practical implementation, in order to further restrict search space, distortion limit is applied besides ITG constraints: a source phrase can be covered next only when it is ITG-legal and its distortion does not exceed distortion limit. The distortion d is calculated by d = |starti − endi−1 − 1|, where starti is the start position of the current phrase and endi−1 is the last position of the last translated phrase. Galley and Manning (2008) present a hierarchical phrase reordering model aimed at improving non-local reorderings. Via the hierarchical mergence of two blocks, the orientation of long distance words can be computed. Their shift-reduce algorithm does not import ITG constraints and admits the translation violating ITG constraints. Zens et al. (2004) introduce a left-toright decoding algorithm with ITG constraints on the alignment template system (Och et al., 1999). Their algorithm processes candidate source phrases one by one through the whole search space and checks if the candidate phrase complies with ITG constraints"
C10-2033,W05-1506,0,0.0420627,"models easily. Xiong et al. (2006) propose a BTG-based model, which uses the context to determine the orientation of two adjacent spans. It employs the cube-time CYK algorithm. 4 3 Related Work St [0] [0][5] [0][5][1] [0][5][1][3] [0][5][1][3][4] [0][5][1][3, 4] [0][5][1][3, 4][2] [0][5][1][2, 4] [0][5][1, 4] [0][1, 5] [0, 5] [0, 5][6] [0, 6] Experiments We compare the shift-reduce decoder with the state-of-the-art decoder Moses (Koehn et al., 2007). The shift-reduce decoder was implemented by modifying the normal search algorithm of Moses to our shift-reduce algorithm, without cube pruning (Huang and Chiang, 2005). We retained the features of Moses: four translation features, three lexical reordering features (straight, inverted and discontinuous), linear distortion, phrase penalty, word penalty and language model, without importing any new feature. The decoding configurations used by all the decoders, including beam size, phrase table limit and so on, were the same, so the performance was compared fairly. First, we will show the performance of shiftreduce algorithm on three data sets with large training data sets (Section 4.1). Then, we will analyze the performance elaborately in terms of accuracy, sp"
C10-2033,J99-4005,0,0.793313,"resent a shift-reduce decoding algorithm that can generate ITG-legal translation from left to right in linear time. This algorithm runs in a reduce-eager style and is suited to phrase-based models. Using the state-ofthe-art decoder Moses as the baseline, experiment results show that the shift-reduce algorithm can significantly improve both the accuracy and the speed on different test sets. 1 Introduction In statistical machine translation, for the diversity of natural languages, the word order of source and target language may differ and searching through all possible translations is NP-hard (Knight, 1999). So some measures have to be taken to reduce search space: either using a search algorithm with pruning technique or restricting possible reorderings. Currently, beam search is widely used (Tillmann and Ney, 2003; Koehn, 2004) to reduce search space. However, the pruning technique adopted by this algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the"
C10-2033,P07-2045,0,0.011696,"ple in Figure 2. The top nine transitions correspond to Figure 3 (a), ... , Figure 3 (i), respectively. the help of ITG structure, it can be extended to syntax-based models easily. Xiong et al. (2006) propose a BTG-based model, which uses the context to determine the orientation of two adjacent spans. It employs the cube-time CYK algorithm. 4 3 Related Work St [0] [0][5] [0][5][1] [0][5][1][3] [0][5][1][3][4] [0][5][1][3, 4] [0][5][1][3, 4][2] [0][5][1][2, 4] [0][5][1, 4] [0][1, 5] [0, 5] [0, 5][6] [0, 6] Experiments We compare the shift-reduce decoder with the state-of-the-art decoder Moses (Koehn et al., 2007). The shift-reduce decoder was implemented by modifying the normal search algorithm of Moses to our shift-reduce algorithm, without cube pruning (Huang and Chiang, 2005). We retained the features of Moses: four translation features, three lexical reordering features (straight, inverted and discontinuous), linear distortion, phrase penalty, word penalty and language model, without importing any new feature. The decoding configurations used by all the decoders, including beam size, phrase table limit and so on, were the same, so the performance was compared fairly. First, we will show the perfor"
C10-2033,koen-2004-pharaoh,0,0.0590687,"der Moses as the baseline, experiment results show that the shift-reduce algorithm can significantly improve both the accuracy and the speed on different test sets. 1 Introduction In statistical machine translation, for the diversity of natural languages, the word order of source and target language may differ and searching through all possible translations is NP-hard (Knight, 1999). So some measures have to be taken to reduce search space: either using a search algorithm with pruning technique or restricting possible reorderings. Currently, beam search is widely used (Tillmann and Ney, 2003; Koehn, 2004) to reduce search space. However, the pruning technique adopted by this algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly"
C10-2033,W99-0604,0,0.0555584,"by d = |starti − endi−1 − 1|, where starti is the start position of the current phrase and endi−1 is the last position of the last translated phrase. Galley and Manning (2008) present a hierarchical phrase reordering model aimed at improving non-local reorderings. Via the hierarchical mergence of two blocks, the orientation of long distance words can be computed. Their shift-reduce algorithm does not import ITG constraints and admits the translation violating ITG constraints. Zens et al. (2004) introduce a left-toright decoding algorithm with ITG constraints on the alignment template system (Och et al., 1999). Their algorithm processes candidate source phrases one by one through the whole search space and checks if the candidate phrase complies with ITG constraints. Besides, their algorithm checks validity via cover vector and does not formalize ITG structure. The shift-reduce decoding algorithm holds ITG structure via three stacks. As a result, it can offer ITG-legal spans directly and decode faster. Furthermore, with Sl ∅ [1, 4] ∅ [2] [2] [2] ∅ ∅ ∅ ∅ ∅ ∅ ∅ Sr [1, 6] [6] [2, 4][6] [4][6] [6] [6] [6] [6] [6] [6] [6] ∅ ∅ Figure 5: Transition sequence for the example in Figure 2. The top nine transi"
C10-2033,P03-1021,0,0.0707834,"scribed in Collins et al. (Collins et al., 2005). 4.1 Performance Evaluation NIST06 30.24 30.27 30.35 —— 30.47 NIST08 25.08 23.80 26.23** 25.09 26.67** speed 4.827 1.501 4.335 3.856 4.126 NIST05 35.80 35.03 36.56** 35.84 36.42** speed 7.142 1.811 6.276 5.008 5.432 (a) We did three experiments to compare the performance of the shift-reduce decoder, Moses and the decoder with ITG constraints using cover vector (denoted as CV). 2 The shift-reduce decoder decoded with two sets of parameters: one was tuned by itself (denoted as SR) and the other was tuned by Moses (denoted as SR-same), using MERT (Och, 2003). Two searching algorithms of Moses are considered: one is the normal search algorithm without cubing pruning (denoted as Moses), the other is the search algorithm with cube pruning (denoted as Moses-cb). For all the decoders, the distortion limit was set to 6, the nbest size was set to 100 and the phrase table limit was 50. In the first experiment, the development set is part of NIST MT06 data set including 862 sentences, the test set is NIST MT08 data set and the training data set contains 5 million sentence pairs. We used a 5-gram language model which were trained on the Xinhua and AFP port"
C10-2033,J03-1005,0,0.0606595,"the state-ofthe-art decoder Moses as the baseline, experiment results show that the shift-reduce algorithm can significantly improve both the accuracy and the speed on different test sets. 1 Introduction In statistical machine translation, for the diversity of natural languages, the word order of source and target language may differ and searching through all possible translations is NP-hard (Knight, 1999). So some measures have to be taken to reduce search space: either using a search algorithm with pruning technique or restricting possible reorderings. Currently, beam search is widely used (Tillmann and Ney, 2003; Koehn, 2004) to reduce search space. However, the pruning technique adopted by this algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are"
C10-2033,P96-1021,0,0.0604209,"pted by this algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly well suited to modeling ordering shifts between languages (Wu, 1996; Wu, 1997). They can well balance the needed flexibility against complexity constraints. Recently, ITG has been successfully applied to statistical machine translation (Zens and Ney, 2003; Zens et al., 2004; Xiong et al., 2006). However, ITG generally employs the expensive CYK parsing algorithm which runs in cube time. In addition, the CYK algorithm can not calculate language model exactly in the process of decoding, as it can not catch the full history context of the left words in a hypothesis. In this paper, we introduce a shift-reduce decoding algorithm with ITG constraints which runs in a"
C10-2033,J97-3002,0,0.701493,"is algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly well suited to modeling ordering shifts between languages (Wu, 1996; Wu, 1997). They can well balance the needed flexibility against complexity constraints. Recently, ITG has been successfully applied to statistical machine translation (Zens and Ney, 2003; Zens et al., 2004; Xiong et al., 2006). However, ITG generally employs the expensive CYK parsing algorithm which runs in cube time. In addition, the CYK algorithm can not calculate language model exactly in the process of decoding, as it can not catch the full history context of the left words in a hypothesis. In this paper, we introduce a shift-reduce decoding algorithm with ITG constraints which runs in a left-to-ri"
C10-2033,P06-1066,1,0.934889,"radeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly well suited to modeling ordering shifts between languages (Wu, 1996; Wu, 1997). They can well balance the needed flexibility against complexity constraints. Recently, ITG has been successfully applied to statistical machine translation (Zens and Ney, 2003; Zens et al., 2004; Xiong et al., 2006). However, ITG generally employs the expensive CYK parsing algorithm which runs in cube time. In addition, the CYK algorithm can not calculate language model exactly in the process of decoding, as it can not catch the full history context of the left words in a hypothesis. In this paper, we introduce a shift-reduce decoding algorithm with ITG constraints which runs in a left-to-right manner. This algorithm parses source words in the order of their corresponding translations on the target side. In the meantime, it gives all candidate ITG-legal reorderings. The shift-reduce algorithm is differen"
C10-2033,P03-1019,0,0.245335,"ranslation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly well suited to modeling ordering shifts between languages (Wu, 1996; Wu, 1997). They can well balance the needed flexibility against complexity constraints. Recently, ITG has been successfully applied to statistical machine translation (Zens and Ney, 2003; Zens et al., 2004; Xiong et al., 2006). However, ITG generally employs the expensive CYK parsing algorithm which runs in cube time. In addition, the CYK algorithm can not calculate language model exactly in the process of decoding, as it can not catch the full history context of the left words in a hypothesis. In this paper, we introduce a shift-reduce decoding algorithm with ITG constraints which runs in a left-to-right manner. This algorithm parses source words in the order of their corresponding translations on the target side. In the meantime, it gives all candidate ITG-legal reorderings"
C10-2033,C04-1030,0,\N,Missing
C10-2059,P06-1109,0,0.0640572,", the projected parser leads to translation performance comparable with using a supervised parser trained on thousands of annotated trees. 1 Introduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise but totally automatic strategy, which performs constituent projection on the basis of dependency projection, and then use a constraint EM optimization algorithm to optimized the initially projected trees. Given a word-aligned bilingual corpus with source sentences parsed, we first projec"
C10-2059,W08-2102,0,0.020283,"target language than a supervised-trained parser dose. 3.1 Boost an Traditional Parser We first establish a unified framework for the enhanced parser where a projected parser is adopted to guide the parsing procedure of the baseline parser. For a given target sentence S, the enhanced parser selected the best parse T˜ among the set of candidates Ω(S) according to two evaluation functions, given by the baseline parser B and the projected guide parser G, respectively. T˜ = argmax P (T |B) × P (T |G)λ (4) T ∈Ω(S) These two evaluation functions can be integrated deeply into the decoding procedure (Carreras et al., 2008; Zhang and Clark, 2008; Huang, 2008), or can be integrated at a shallow level in a reranking manner (Collins, 2000; Charniak and Johnson, 2005). For simplicity and generability, we adopt the reranking strategy. In k-best reranking, Ω(S) is simply a set of candidate parses, denoted as {T1 , T2 , ..., Tk }, and we use the single parse of the guide parser, TG , to re-evaluate these candidates. Formula 4 can be redefined as T˜(TG ) = argmax w · f (T, TG ) (5) T ∈Ω(S) Here, f (T, TG ) and w represent a high dimensional feature representation and a corresponding weight vector, respectively. The fir"
C10-2059,P06-1043,0,0.0541073,"troduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise but totally automatic strategy, which performs constituent projection on the basis of dependency projection, and then use a constraint EM optimization algorithm to optimized the initially projected trees. Given a word-aligned bilingual corpus with source sentences parsed, we first project the dependency structures of these constituent trees to the target sentences using a dynamic programming algorithm, then we generate a set of candidate co"
C10-2059,A00-2018,0,0.0555991,"PCFG-style parsing procedure is then used to search for the most probable projected constituent tree. Experiments show that, the parser trained on the projected treebank can significantly boost a state-of-the-art supervised parser. When integrated into a tree-based machine translation system, the projected parser leads to translation performance comparable with using a supervised parser trained on thousands of annotated trees. 1 Introduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise"
C10-2059,H05-1066,0,0.0407709,"tituent projection which is conducted on projected dependent structures, and finally show the constraint EM procedure for constituent optimization. 2.1 Algorithm 1 Dependency projection. (1) Pe can then be obtained by simple accumulation across all possible situations of correspondence Pe (x y y|DE , A) X Ax,x′ × Ay,y′ × δ(x′ , y ′ |DE ) = 1≤x′ ,y ′ ≤|E| where δ(x′ , y ′ |DE ) is a 0-1 function that equals 1 only if the dependent relation x′ y y ′ holds in DE . The search procedure needed by the argmax operation in equation 1 can be effectively solved by the Chu-Liu-Edmonds algorithm used in (McDonald et al., 2005). In this work, however, we adopt a more general and simple dynamic programming algorithm as shown in Algorithm 1, in order to facilitate the possible expansions. In practice, the cube-pruning strategy (Huang and Chiang, 2005) is used to speed up the enumeration of derivations (loops started by line 4 and 5). 2.2 Constituent Projection The PCFG-style parsing procedure searches for the most probable projected constituent tree in a shrunken search space determined by the projected dependency structure and the target constituent tree. The shrunken search space can be built as following. First, we"
C10-2059,W02-1001,0,0.100295,"able to cover this span, and attach a asterisk at the tail of this non-terminal. Here is an example of the guide features f100 (T, TG ) = V P ∈ T ◦ P P ∗ ∈ TG It represents that a V P in the candidate parse corresponds to a segment of a P P in the projected parse. The quantity of its weight w100 indicates how probably a span can be predicated as V P if the span corresponds to a partial P P in the projected parse. We adopt the perceptron algorithm to train the reranker. To reduce overfitting and produce a more stable weight vector, we also use a refinement strategy called averaged parameters (Collins, 2002). 3.2 Using in Machine Translation Researchers have achieved promising improvements in tree-based machine translation (Liu et al., 2006; Huang et al., 2006). Such models use a parsed tree as input and converts it into a target tree or string. Given a source language sentence, first we use a traditional source language parser to parse the sentence to obtain the syntax tree T , and then use the translation decoder to search for ˜ where a derivation d is a sethe best derivation d, quence of transformations that converts the source tree into the target language string d˜ = argmax P (d|T ) (6) d∈D"
C10-2059,P09-1042,0,0.204465,"Missing"
C10-2059,W05-1506,0,0.0283388,"accumulation across all possible situations of correspondence Pe (x y y|DE , A) X Ax,x′ × Ay,y′ × δ(x′ , y ′ |DE ) = 1≤x′ ,y ′ ≤|E| where δ(x′ , y ′ |DE ) is a 0-1 function that equals 1 only if the dependent relation x′ y y ′ holds in DE . The search procedure needed by the argmax operation in equation 1 can be effectively solved by the Chu-Liu-Edmonds algorithm used in (McDonald et al., 2005). In this work, however, we adopt a more general and simple dynamic programming algorithm as shown in Algorithm 1, in order to facilitate the possible expansions. In practice, the cube-pruning strategy (Huang and Chiang, 2005) is used to speed up the enumeration of derivations (loops started by line 4 and 5). 2.2 Constituent Projection The PCFG-style parsing procedure searches for the most probable projected constituent tree in a shrunken search space determined by the projected dependency structure and the target constituent tree. The shrunken search space can be built as following. First, we generates the candidate constituents of the source tree and the candidate spans of the target sentence, so as to enumerate the candidate constituents of the target sentence. Then we compute the consistent degree for 517 has L"
C10-2059,2006.amta-papers.8,0,0.0222814,"P ∗ ∈ TG It represents that a V P in the candidate parse corresponds to a segment of a P P in the projected parse. The quantity of its weight w100 indicates how probably a span can be predicated as V P if the span corresponds to a partial P P in the projected parse. We adopt the perceptron algorithm to train the reranker. To reduce overfitting and produce a more stable weight vector, we also use a refinement strategy called averaged parameters (Collins, 2002). 3.2 Using in Machine Translation Researchers have achieved promising improvements in tree-based machine translation (Liu et al., 2006; Huang et al., 2006). Such models use a parsed tree as input and converts it into a target tree or string. Given a source language sentence, first we use a traditional source language parser to parse the sentence to obtain the syntax tree T , and then use the translation decoder to search for ˜ where a derivation d is a sethe best derivation d, quence of transformations that converts the source tree into the target language string d˜ = argmax P (d|T ) (6) d∈D 1 Using non-terminals as features brings no improvement in the reranking experiments, so as to examine the impact of the projected parser. 520 Here D is the"
C10-2059,P08-1067,0,0.0165313,"er dose. 3.1 Boost an Traditional Parser We first establish a unified framework for the enhanced parser where a projected parser is adopted to guide the parsing procedure of the baseline parser. For a given target sentence S, the enhanced parser selected the best parse T˜ among the set of candidates Ω(S) according to two evaluation functions, given by the baseline parser B and the projected guide parser G, respectively. T˜ = argmax P (T |B) × P (T |G)λ (4) T ∈Ω(S) These two evaluation functions can be integrated deeply into the decoding procedure (Carreras et al., 2008; Zhang and Clark, 2008; Huang, 2008), or can be integrated at a shallow level in a reranking manner (Collins, 2000; Charniak and Johnson, 2005). For simplicity and generability, we adopt the reranking strategy. In k-best reranking, Ω(S) is simply a set of candidate parses, denoted as {T1 , T2 , ..., Tk }, and we use the single parse of the guide parser, TG , to re-evaluate these candidates. Formula 4 can be redefined as T˜(TG ) = argmax w · f (T, TG ) (5) T ∈Ω(S) Here, f (T, TG ) and w represent a high dimensional feature representation and a corresponding weight vector, respectively. The first feature f1 (T, TG ) = logP (T |B)"
C10-2059,P02-1050,0,0.0558792,"Missing"
C10-2059,P02-1017,0,0.0419431,"rated into a tree-based machine translation system, the projected parser leads to translation performance comparable with using a supervised parser trained on thousands of annotated trees. 1 Introduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise but totally automatic strategy, which performs constituent projection on the basis of dependency projection, and then use a constraint EM optimization algorithm to optimized the initially projected trees. Given a word-aligned bilingual corpus with sour"
C10-2059,P04-1061,0,0.0442519,"achine translation system, the projected parser leads to translation performance comparable with using a supervised parser trained on thousands of annotated trees. 1 Introduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise but totally automatic strategy, which performs constituent projection on the basis of dependency projection, and then use a constraint EM optimization algorithm to optimized the initially projected trees. Given a word-aligned bilingual corpus with source sentences parsed, we f"
C10-2059,P06-1077,1,0.947086,"lts validate the effectiveness of our approach. On the Chinese-English FBIS corpus, we project the English parses produced by the Charniak parser across to the Chinese sen516 Coling 2010: Poster Volume, pages 516–524, Beijing, August 2010 tences. A berkeley parser trained on this projected treebank can effectively boost the supervised parsers trained on bunches of CTB trees. Especially, the supervised parser trained on the smaller CTB 1.0 benefits a significant F-measure increment of more than 1 point from the projected parser. When using the projected parser in a treebased translation model (Liu et al., 2006), we achieve translation performance comparable with using a state-of-the-art supervised parser trained on thousands of CTB trees. This surprising result gives us an inspiration that better translation would be achieved by combining both projected parsing and supervised parsing into a hybrid parsing schema. 2 Stepwise Constituent Projection Dependency Projection For dependency projection we adopt a dynamic programming algorithm, which searches the most probable projected target dependency structure according to the source dependency structure and the word alignment. In order to mitigate the ef"
C10-2059,D09-1106,1,0.833412,"n 40 words, and evaluate the translation quality by the case-sensitive BLEU-4 metric (Papineni et al., 2002) with 4 references. 4.1 Constituent Projection We perform constituent projection from English to Chinese on the FBIS corpus, which contains 239K sentence pairs with about 6.9M/8.9M words in Chinese/English. The English sentences are parsed by the Charniak Parser and the dependency structures are extracted from these parses according to the head-finding rules of (Yamada and Matsumoto, 2003). The word alignment matrixes are obtained by combining the 10-best results of GIZA++ according to (Liu et al., 2009). We first project the dependency structures from English to Chinese according to section 2.1, and then project the constituent structures according to section 2.2. We define an assessment criteria to evaluate the confidence of the final projected constituent tree p c = n P (DF |DE , A) × P (TF |TE , A) where n is the word count of a Chinese sentence in our experiments. A series of projected ChiThres c 0.5 0.4 0.3 0.2 0.1 #Resrv 12.6K 17.8K 27.2K 45.1K 87.0K Cons-F1 23.9 23.9 25.4 26.6 27.8 Span-F1 32.7 33.4 35.7 38.0 40.4 Table 1: Performances of the projected parsers on the CTB test set. #Re"
C10-2059,P00-1056,0,0.0564898,"with using a state-of-the-art supervised parser trained on thousands of CTB trees. This surprising result gives us an inspiration that better translation would be achieved by combining both projected parsing and supervised parsing into a hybrid parsing schema. 2 Stepwise Constituent Projection Dependency Projection For dependency projection we adopt a dynamic programming algorithm, which searches the most probable projected target dependency structure according to the source dependency structure and the word alignment. In order to mitigate the effect of word alignment errors, multiple GIZA++ (Och and Ney, 2000) results are combined into a compact representation called alignment matrix. Given a source sentence with m words, represented as E1:m , and a target sentence with n words, represented as F1:n , their word alignment matrix A is an m × n matrix, where each element Ai,j denotes the probability of the source word Ei aligned to the target word Fj . Using P (DF |DE , A) to denote the probability of the projected target dependency structure DF conditioned on the source dependency structure DE and the alignment matrix A, the projection algorithm aims to find D˜F = argmax P (DF |DE , A) DF 1: 2: 3: 4:"
C10-2059,P03-1021,0,0.0185498,"Missing"
C10-2059,P02-1040,0,0.080704,"cted parser to parse the input sentence for the subsequent translation decoding procedure. 4 Experiments In this section, we first invalidate the effect of constituent projection by evaluating a parser trained on the projected treebank. Then we investigate two applications of the projected parser: boosting an traditional supervised-trained parser, and integration in a tree-based machine translation system. Following the previous works, we depict the parsing performance by F-score on sentences with no more than 40 words, and evaluate the translation quality by the case-sensitive BLEU-4 metric (Papineni et al., 2002) with 4 references. 4.1 Constituent Projection We perform constituent projection from English to Chinese on the FBIS corpus, which contains 239K sentence pairs with about 6.9M/8.9M words in Chinese/English. The English sentences are parsed by the Charniak Parser and the dependency structures are extracted from these parses according to the head-finding rules of (Yamada and Matsumoto, 2003). The word alignment matrixes are obtained by combining the 10-best results of GIZA++ according to (Liu et al., 2009). We first project the dependency structures from English to Chinese according to section 2"
C10-2059,P06-1055,0,0.075058,"ing procedure is then used to search for the most probable projected constituent tree. Experiments show that, the parser trained on the projected treebank can significantly boost a state-of-the-art supervised parser. When integrated into a tree-based machine translation system, the projected parser leads to translation performance comparable with using a supervised parser trained on thousands of annotated trees. 1 Introduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise but totally automatic"
C10-2059,N01-1023,0,0.160907,"on thousands of annotated trees. 1 Introduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise but totally automatic strategy, which performs constituent projection on the basis of dependency projection, and then use a constraint EM optimization algorithm to optimized the initially projected trees. Given a word-aligned bilingual corpus with source sentences parsed, we first project the dependency structures of these constituent trees to the target sentences using a dynamic programming al"
C10-2059,P07-1049,0,0.0499131,"cted parser leads to translation performance comparable with using a supervised parser trained on thousands of annotated trees. 1 Introduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise but totally automatic strategy, which performs constituent projection on the basis of dependency projection, and then use a constraint EM optimization algorithm to optimized the initially projected trees. Given a word-aligned bilingual corpus with source sentences parsed, we first project the dependency"
C10-2059,D09-1086,0,0.0410601,"fective Constituent Projection across Languages Wenbin Jiang and Yajuan Lu¨ and Yang Liu and Qun Liu Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences {jiangwenbin, lvyajuan, yliu, liuqun}@ict.ac.cn Abstract the need of reliable priori knowledge in semisupervised methods, it seems promising to project the syntax structures from a resource-rich language to a resource-scarce one across a bilingual corpus. Lots of researches have so far been devoted to dependency projection (Hwa et al., 2002; Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009). While for constituent projection there is few progress. This is due to the fact that the constituent syntax describes the language structure in a more detailed way, and the degree of isomorphism between constituent structures appears much lower. We describe an effective constituent projection strategy, where constituent projection is performed on the basis of dependency projection. Especially, a novel measurement is proposed to evaluate the candidate projected constituents for a target language sentence, and a PCFG-style parsing procedure is then used to search for the most probable projecte"
C10-2059,E03-1008,0,0.0901041,"f annotated trees. 1 Introduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise but totally automatic strategy, which performs constituent projection on the basis of dependency projection, and then use a constraint EM optimization algorithm to optimized the initially projected trees. Given a word-aligned bilingual corpus with source sentences parsed, we first project the dependency structures of these constituent trees to the target sentences using a dynamic programming algorithm, then we genera"
C10-2059,I05-1007,1,0.916773,"the CTB annotation standard, but it needs to be validated by the following experiments. 4.2 CTB 1.0 baseline boosted parser 10000 Scale of treebank (log) Figure 1: Log-likelihood of the 87K-projected treebank after each EM interation. Cons-F1 27.8 22.8 CTB 5.0 1000 EM iteration Train Set Original 87K Optimized 87K 88 86 84 82 80 78 76 74 72 70 Boost an Traditional Parser The projected parser is used to help the reranking of the k-best parses produced by another state-ofthe-art parser, which is called the baseline parser for convenience. In our experiments we choose the revised Chinese parser (Xiong et al., 2005) Figure 2: Boosting performance of the projected parser on a series of baseline parsers that are trained on treebanks of different scales. based on Collins model 2 (Collins, 1999) as the baseline parser.2 The baseline parser is respectively trained on CTB 1.0 and CTB 5.0. For both corpora we follow the traditional corpus splitting: chapters 271-300 for testing, chapters 301-325 for development, and else for training. Experimental results are shown in Table 3. We find that both projected parsers bring significant improvement to the baseline parsers. Especially the later, although performs worse"
C10-2059,W03-3023,0,0.0392877,"ne translation system. Following the previous works, we depict the parsing performance by F-score on sentences with no more than 40 words, and evaluate the translation quality by the case-sensitive BLEU-4 metric (Papineni et al., 2002) with 4 references. 4.1 Constituent Projection We perform constituent projection from English to Chinese on the FBIS corpus, which contains 239K sentence pairs with about 6.9M/8.9M words in Chinese/English. The English sentences are parsed by the Charniak Parser and the dependency structures are extracted from these parses according to the head-finding rules of (Yamada and Matsumoto, 2003). The word alignment matrixes are obtained by combining the 10-best results of GIZA++ according to (Liu et al., 2009). We first project the dependency structures from English to Chinese according to section 2.1, and then project the constituent structures according to section 2.2. We define an assessment criteria to evaluate the confidence of the final projected constituent tree p c = n P (DF |DE , A) × P (TF |TE , A) where n is the word count of a Chinese sentence in our experiments. A series of projected ChiThres c 0.5 0.4 0.3 0.2 0.1 #Resrv 12.6K 17.8K 27.2K 45.1K 87.0K Cons-F1 23.9 23.9 25"
C10-2059,D08-1059,0,0.0220778,"supervised-trained parser dose. 3.1 Boost an Traditional Parser We first establish a unified framework for the enhanced parser where a projected parser is adopted to guide the parsing procedure of the baseline parser. For a given target sentence S, the enhanced parser selected the best parse T˜ among the set of candidates Ω(S) according to two evaluation functions, given by the baseline parser B and the projected guide parser G, respectively. T˜ = argmax P (T |B) × P (T |G)λ (4) T ∈Ω(S) These two evaluation functions can be integrated deeply into the decoding procedure (Carreras et al., 2008; Zhang and Clark, 2008; Huang, 2008), or can be integrated at a shallow level in a reranking manner (Collins, 2000; Charniak and Johnson, 2005). For simplicity and generability, we adopt the reranking strategy. In k-best reranking, Ω(S) is simply a set of candidate parses, denoted as {T1 , T2 , ..., Tk }, and we use the single parse of the guide parser, TG , to re-evaluate these candidates. Formula 4 can be redefined as T˜(TG ) = argmax w · f (T, TG ) (5) T ∈Ω(S) Here, f (T, TG ) and w represent a high dimensional feature representation and a corresponding weight vector, respectively. The first feature f1 (T, TG )"
C10-2059,J03-4003,0,\N,Missing
C10-2059,P05-1022,0,\N,Missing
C10-2096,J07-2003,0,0.0215215,"s: P (r) = P (r|lhs(r))λ4 · P (r|rhs(r))λ5 · P (r|root(lhs(r))λ6 (7) · Plex (lhs(r)|rhs(r))λ7 · Plex (rhs(r)|lhs(r))λ8 , where the last two are the lexical probabilities between the terminals of lhs(r) and rhs(r). All the weights of those features are tuned by using Minimal Error Rate Training (Och, 2003). Following Mi et al. (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al. (2008), and then the decoder finds the best derivation on the lattice translation forest. For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) to incrementally compute the second, third, through the kth best alternatives. For more detail, we refer to the algorithms of Mi et al. (2008). 6 Experiments 6.1 Data Preparation Our experiments are on Chinese-to-English translation. Our training corpus is FBIS corpus with about 6.9M/8.9M words in Chinese/English respectively. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram l"
C10-2096,P05-1066,0,0.0822847,"Missing"
C10-2096,P09-1063,1,0.884078,"Missing"
C10-2096,P09-1064,0,0.0274916,"ove 1-best system by 3 BLEU points. Zhang et al. (2009) use forest in 844 tree-sequence-to-string model and also achieve a promising improvement. Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009). Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system. 8 Conclusion and Future Work In this paper, we have proposed a lattice-forest based model to alleviate the problem of error propagation in traditional single-best pipeline framework. Unlike previous works, which only focus on one module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-tostring system. We have explored the algorithms of lattice parsing, rule extraction and decoding. Ou"
C10-2096,D08-1076,0,0.0294601,"09) use forest in treeto-tree model, and improve 1-best system by 3 BLEU points. Zhang et al. (2009) use forest in 844 tree-sequence-to-string model and also achieve a promising improvement. Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009). Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system. 8 Conclusion and Future Work In this paper, we have proposed a lattice-forest based model to alleviate the problem of error propagation in traditional single-best pipeline framework. Unlike previous works, which only focus on one module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-tostring system. We have explored the algorithms of lattic"
C10-2096,P08-1115,0,0.33427,"alignments and parses in the training phase. However, with limited scope and too many redundancies, it is inefficient to search separately on each of these similar lists (Huang, 2008). Another efficient method is to use compact data structures instead of k-best lists. A lattice or forest, compactly encoded exponentially many derivations, have proven to be a promising technique. For example, Mi and Huang (2008), Mi et al. (2008), Liu et al. (2009) and Zhang et al. (2009) use forests in rule extraction and decoding phases to extract more general rules and weaken the influence of parsing errors; Dyer et al. (2008) use word lattice in Chinese word segmentation and Arabic morphological variation phases to weaken the influence of segmentation errors; Huang (2008) and 837 Coling 2010: Poster Volume, pages 837–845, Beijing, August 2010 (0, 2, NR) 0 c0 :B`u 1 c1 :sh´ı (2, 3, CC) 2 c2 :yˇu (3, 5, NR) 3 c3 :Sh¯a (5, 6, VV) 4 c4 :l´ong 5 (6, 8, NN) c5 :jˇu 6 c6 :x´ıng 7 (5, 7, VV) (2, 3, P) (8, 9, NN) c7 :tˇao 8 c8 :l`un (7, 9, NN) 9 Figure 2: The lattice of the example:“ B`u sh´ı yˇu Sh¯a l´ong jˇu x´ıng tˇao l`un.” The solid lines show the 1-best result, which is wrong. Jiang et al. (2008b) stress the problem"
C10-2096,D08-1022,1,0.953642,"oxes. As a result, the search space is reduced without losing any derivations. Theoretically, the parsing over fs will speed up in O((n/nr )3 ). And our experiments in Section 6 show the efficiency of our new approach. It turns out that the parsing algorithm developed in lattice-parsing Section 3.2 can be used here without any change. The non-terminals inducted are also shown in Figure 3(c) in italic style. 4 Rule Extraction with Lattice & Forest We now explore the extraction algorithm from aligned source lattice-forest and target string2 , which is a tuple F, τ, a in Figure 5(b). Following Mi and Huang (2008), we extract minimal rules from a lattice-forest also in two steps: (1) frontier set computation (2) fragmentation Following the algorithms developed by Mi and Huang (2008) in Algorithm 1, all the nodes in frontier set (fs) are highlighted with gray in Figure 5(b). Our process of fragmentation (lines 1- 13) is to visit each frontier node v and initial a queue (open) of growing fragments with a pair of empty fragment and node v (line 3). Each fragment is associated with a list of expansion sites (front) being 2 For simplicity and consistency, we use character-based lattice-forest for the runnin"
C10-2096,N09-1046,0,0.0192013,"evious works on SMT. To alleviate the problem of parsing error in 1-best tree-to-string translation model, Mi et al. (2008) first use forest to direct translation. Then Mi and Huang (2008) use forest in rule extraction step. Following the same direction, Liu et al. (2009) use forest in treeto-tree model, and improve 1-best system by 3 BLEU points. Zhang et al. (2009) use forest in 844 tree-sequence-to-string model and also achieve a promising improvement. Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009). Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system. 8 Conclusion and Future Work In this paper, we have proposed a lattice-forest based model to alleviate th"
C10-2096,P06-1121,0,0.0508816,".pop()  expand frontier 11: for each e ∈ IN (u) do 12: f ← front ∪ (tails(e)  fs) 13: open.append(frag ∪ {e}, f ) (line 7) . Otherwise we pop one expansion node u to grow and spin-off new fragments by IN (u), adding new expansion sites (lines 11- 13), until all active fragments are complete and open queue is empty. The extra minimal rules extracted on latticeforest are listed at the right bottom of Figure 5(c). Compared with the forest-only approach, we can extract smaller and more general rules. After we get all the minimal rules, we compose two or more minimal rules into composed rules (Galley et al., 2006), which will be used in our experiments. For each rule r extracted, we also assign a fractional count which is computed by using insideoutside probabilities: c(r) = α(root(r)) · P (lhs(r)) · Q v∈yield(root(r)) β(TOP) β(v) , (1) where root(r) is the root of the rule, lhs(r) is the left-hand-side of rule, rhs(r) is the righthand-side of rule, P (lhs(r)) is the product of all probabilities of hyperedges involved in lhs(r), yield(root(r)) is the leave nodes, TOP is the root node of the forest, α(v) and β(v) are outside and inside probabilities, respectively. Then we compute three conditional proba"
C10-2096,W05-1506,1,0.746831,"nals of lhs(r) and rhs(r). All the weights of those features are tuned by using Minimal Error Rate Training (Och, 2003). Following Mi et al. (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al. (2008), and then the decoder finds the best derivation on the lattice translation forest. For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) to incrementally compute the second, third, through the kth best alternatives. For more detail, we refer to the algorithms of Mi et al. (2008). 6 Experiments 6.1 Data Preparation Our experiments are on Chinese-to-English translation. Our training corpus is FBIS corpus with about 6.9M/8.9M words in Chinese/English respectively. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Gigaword corpus. We use the 2002 NIST MT Evaluation test set as development set and the 2005 NIST MT Evaluation test"
C10-2096,P07-1019,1,0.838862,"r|lhs(r))λ4 · P (r|rhs(r))λ5 · P (r|root(lhs(r))λ6 (7) · Plex (lhs(r)|rhs(r))λ7 · Plex (rhs(r)|lhs(r))λ8 , where the last two are the lexical probabilities between the terminals of lhs(r) and rhs(r). All the weights of those features are tuned by using Minimal Error Rate Training (Och, 2003). Following Mi et al. (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al. (2008), and then the decoder finds the best derivation on the lattice translation forest. For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) to incrementally compute the second, third, through the kth best alternatives. For more detail, we refer to the algorithms of Mi et al. (2008). 6 Experiments 6.1 Data Preparation Our experiments are on Chinese-to-English translation. Our training corpus is FBIS corpus with about 6.9M/8.9M words in Chinese/English respectively. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser"
C10-2096,P08-1067,1,0.865734,"s a result, the search space is reduced without losing any derivations. Theoretically, the parsing over fs will speed up in O((n/nr )3 ). And our experiments in Section 6 show the efficiency of our new approach. It turns out that the parsing algorithm developed in lattice-parsing Section 3.2 can be used here without any change. The non-terminals inducted are also shown in Figure 3(c) in italic style. 4 Rule Extraction with Lattice & Forest We now explore the extraction algorithm from aligned source lattice-forest and target string2 , which is a tuple F, τ, a in Figure 5(b). Following Mi and Huang (2008), we extract minimal rules from a lattice-forest also in two steps: (1) frontier set computation (2) fragmentation Following the algorithms developed by Mi and Huang (2008) in Algorithm 1, all the nodes in frontier set (fs) are highlighted with gray in Figure 5(b). Our process of fragmentation (lines 1- 13) is to visit each frontier node v and initial a queue (open) of growing fragments with a pair of empty fragment and node v (line 3). Each fragment is associated with a list of expansion sites (front) being 2 For simplicity and consistency, we use character-based lattice-forest for the runnin"
C10-2096,P08-1102,1,0.871066,"Missing"
C10-2096,C08-1049,1,0.920892,"e of parsing errors; Dyer et al. (2008) use word lattice in Chinese word segmentation and Arabic morphological variation phases to weaken the influence of segmentation errors; Huang (2008) and 837 Coling 2010: Poster Volume, pages 837–845, Beijing, August 2010 (0, 2, NR) 0 c0 :B`u 1 c1 :sh´ı (2, 3, CC) 2 c2 :yˇu (3, 5, NR) 3 c3 :Sh¯a (5, 6, VV) 4 c4 :l´ong 5 (6, 8, NN) c5 :jˇu 6 c6 :x´ıng 7 (5, 7, VV) (2, 3, P) (8, 9, NN) c7 :tˇao 8 c8 :l`un (7, 9, NN) 9 Figure 2: The lattice of the example:“ B`u sh´ı yˇu Sh¯a l´ong jˇu x´ıng tˇao l`un.” The solid lines show the 1-best result, which is wrong. Jiang et al. (2008b) stress the problems in reranking phase. Both lattices and forests have become popular in machine translation literature. However, to the best of our knowledge, previous work only focused on one module at a time. In this paper, we investigate the combination of lattice and forest (Section 2), as shown in Figure 1(b). We explore the algorithms of lattice parsing (Section 3.2), rule extraction (Section 4) and decoding (Section 5). More importantly, in the decoding step, our model can search among not only more parse-trees but also more segmentations encoded in the lattice-forests and can take"
C10-2096,P08-1023,1,0.95054,"ee separate phases. To alleviate this problem, an obvious solution is to widen the pipeline with k-best lists rather than 1-best results. For example Venugopal et al. (2008) use k-best alignments and parses in the training phase. However, with limited scope and too many redundancies, it is inefficient to search separately on each of these similar lists (Huang, 2008). Another efficient method is to use compact data structures instead of k-best lists. A lattice or forest, compactly encoded exponentially many derivations, have proven to be a promising technique. For example, Mi and Huang (2008), Mi et al. (2008), Liu et al. (2009) and Zhang et al. (2009) use forests in rule extraction and decoding phases to extract more general rules and weaken the influence of parsing errors; Dyer et al. (2008) use word lattice in Chinese word segmentation and Arabic morphological variation phases to weaken the influence of segmentation errors; Huang (2008) and 837 Coling 2010: Poster Volume, pages 837–845, Beijing, August 2010 (0, 2, NR) 0 c0 :B`u 1 c1 :sh´ı (2, 3, CC) 2 c2 :yˇu (3, 5, NR) 3 c3 :Sh¯a (5, 6, VV) 4 c4 :l´ong 5 (6, 8, NN) c5 :jˇu 6 c6 :x´ıng 7 (5, 7, VV) (2, 3, P) (8, 9, NN) c7 :tˇao 8 c8 :l`un (7, 9,"
C10-2096,P00-1056,0,0.0903879,"take as input the POS tagging results. Then we parse the segmentation results into forest by using the parser of Xiong et al. (2005). Actually, the parser will assign multiple POS tags to each word rather than one. As a result, our baseline system has already postponed the POS tagging disambiguition to the decoding step. Forest is pruned by using a marginal probabilitybased pruning algorithm similar to Huang (2008). The pruning threshold are pf = 5 and pf = 10 at rule extraction and decoding steps respectively. We word-align the strings of 1-best segmentations and target strings with GIZA++ (Och and Ney, 2000) and apply the refinement method “grow-diag-final-and” (Koehn et al., 2003) to get the final alignments. Following Mi and Huang 843 (2008) and Mi et al. (2008), we also extract rules from forest-string pairs and translate forest to string. 6.1.2 Lattice-forest System We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al., 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm. Then, as current GIZA++ (Och and Ney, 2000) can only handle alignment between string-string pairs, and word-alingme"
C10-2096,P03-1021,0,0.0918259,"guage model and eλ3 |τ (d) |is the length penalty term on target translation. The P (d|T ) decomposes into the product of rule probabilities P (r), each of which is decomposed further into P (d|T ) =  P (r). (6) r∈d Each P (r) in Equation 6 is decomposed further into the production of five probabilities: P (r) = P (r|lhs(r))λ4 · P (r|rhs(r))λ5 · P (r|root(lhs(r))λ6 (7) · Plex (lhs(r)|rhs(r))λ7 · Plex (rhs(r)|lhs(r))λ8 , where the last two are the lexical probabilities between the terminals of lhs(r) and rhs(r). All the weights of those features are tuned by using Minimal Error Rate Training (Och, 2003). Following Mi et al. (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al. (2008), and then the decoder finds the best derivation on the lattice translation forest. For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) to incrementally compute the second, third, through the kth best alternatives. For more detail,"
C10-2096,P02-1040,0,0.0803448,"we refer to the algorithms of Mi et al. (2008). 6 Experiments 6.1 Data Preparation Our experiments are on Chinese-to-English translation. Our training corpus is FBIS corpus with about 6.9M/8.9M words in Chinese/English respectively. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Gigaword corpus. We use the 2002 NIST MT Evaluation test set as development set and the 2005 NIST MT Evaluation test set as test set. We evaluate the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). We use the standard MERT (Och, 2003) to tune the weights. 6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al., 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results. Then we parse the segmentation results into forest by using the parser of Xiong et al. (2005). Actually, the parser will assign multiple POS tags to each word rather than one. As a result, our baseline system has already postponed the POS tagging disambiguition to the decoding step. For"
C10-2096,D08-1065,0,0.0217086,"o-tree model, and improve 1-best system by 3 BLEU points. Zhang et al. (2009) use forest in 844 tree-sequence-to-string model and also achieve a promising improvement. Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009). Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system. 8 Conclusion and Future Work In this paper, we have proposed a lattice-forest based model to alleviate the problem of error propagation in traditional single-best pipeline framework. Unlike previous works, which only focus on one module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-tostring system. We have explored the algorithms of lattice parsing, rule extrac"
C10-2096,2008.amta-papers.18,0,0.0310293,"Missing"
C10-2096,N03-1017,0,0.00419903,"ults into forest by using the parser of Xiong et al. (2005). Actually, the parser will assign multiple POS tags to each word rather than one. As a result, our baseline system has already postponed the POS tagging disambiguition to the decoding step. Forest is pruned by using a marginal probabilitybased pruning algorithm similar to Huang (2008). The pruning threshold are pf = 5 and pf = 10 at rule extraction and decoding steps respectively. We word-align the strings of 1-best segmentations and target strings with GIZA++ (Och and Ney, 2000) and apply the refinement method “grow-diag-final-and” (Koehn et al., 2003) to get the final alignments. Following Mi and Huang 843 (2008) and Mi et al. (2008), we also extract rules from forest-string pairs and translate forest to string. 6.1.2 Lattice-forest System We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al., 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm. Then, as current GIZA++ (Och and Ney, 2000) can only handle alignment between string-string pairs, and word-alingment with the pairs of Chinese characters and target-string will obviously re"
C10-2096,I05-1007,1,0.88287,"se the 2002 NIST MT Evaluation test set as development set and the 2005 NIST MT Evaluation test set as test set. We evaluate the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). We use the standard MERT (Och, 2003) to tune the weights. 6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al., 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results. Then we parse the segmentation results into forest by using the parser of Xiong et al. (2005). Actually, the parser will assign multiple POS tags to each word rather than one. As a result, our baseline system has already postponed the POS tagging disambiguition to the decoding step. Forest is pruned by using a marginal probabilitybased pruning algorithm similar to Huang (2008). The pruning threshold are pf = 5 and pf = 10 at rule extraction and decoding steps respectively. We word-align the strings of 1-best segmentations and target strings with GIZA++ (Och and Ney, 2000) and apply the refinement method “grow-diag-final-and” (Koehn et al., 2003) to get the final alignments. Following"
C10-2096,P09-1019,0,0.0266168,"3 BLEU points. Zhang et al. (2009) use forest in 844 tree-sequence-to-string model and also achieve a promising improvement. Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009). Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system. 8 Conclusion and Future Work In this paper, we have proposed a lattice-forest based model to alleviate the problem of error propagation in traditional single-best pipeline framework. Unlike previous works, which only focus on one module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-tostring system. We have explored the algorithms of lattice parsing, rule extraction and decoding. Our model postpones th"
C10-2096,P09-1020,0,0.0592981,"oblem, an obvious solution is to widen the pipeline with k-best lists rather than 1-best results. For example Venugopal et al. (2008) use k-best alignments and parses in the training phase. However, with limited scope and too many redundancies, it is inefficient to search separately on each of these similar lists (Huang, 2008). Another efficient method is to use compact data structures instead of k-best lists. A lattice or forest, compactly encoded exponentially many derivations, have proven to be a promising technique. For example, Mi and Huang (2008), Mi et al. (2008), Liu et al. (2009) and Zhang et al. (2009) use forests in rule extraction and decoding phases to extract more general rules and weaken the influence of parsing errors; Dyer et al. (2008) use word lattice in Chinese word segmentation and Arabic morphological variation phases to weaken the influence of segmentation errors; Huang (2008) and 837 Coling 2010: Poster Volume, pages 837–845, Beijing, August 2010 (0, 2, NR) 0 c0 :B`u 1 c1 :sh´ı (2, 3, CC) 2 c2 :yˇu (3, 5, NR) 3 c3 :Sh¯a (5, 6, VV) 4 c4 :l´ong 5 (6, 8, NN) c5 :jˇu 6 c6 :x´ıng 7 (5, 7, VV) (2, 3, P) (8, 9, NN) c7 :tˇao 8 c8 :l`un (7, 9, NN) 9 Figure 2: The lattice of the example"
C10-2096,D09-1005,0,\N,Missing
C10-2096,P08-1000,0,\N,Missing
C10-2136,P05-1067,0,0.108695,"ing of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over"
C10-2136,P03-2041,0,0.165157,"while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achie"
C10-2136,W02-1039,0,0.0891427,"cently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over the hierarchical phrase-based system (Chiang, 2007). So 1 A block is a bilingual phrase without maximum length limitation. 1185 Coling 2010: Poster Volume, pages 1185–1193, Beijing, August 2010 we think it will be a promising way to integrate the target-side dependency str"
C10-2136,P06-1121,0,0.0348927,"9). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a"
C10-2136,P07-1090,0,0.0199902,"cted increasing attention in statistical machine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be d"
C10-2136,P08-1066,0,0.446303,"influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over the hierarchical phrase-based system (C"
C10-2136,C10-1123,1,0.783262,"Missing"
C10-2136,P96-1021,0,0.0727476,"w that our system achieves significant improvements over the baseline system on various test sets even with fewer phrases. 1 Introduction Bracketing transduction grammar (BTG) (Wu, 1995) is an important subclass of synchronous context free grammar, which employs a special synchronous rewriting mechanism to parse parallel sentence of both languages. Due to the prominent advantages such as the simplicity of grammar and the good coverage of syntactic diversities in different language pairs, BTG has attracted increasing attention in statistical machine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these met"
C10-2136,P06-1066,1,0.905951,"writing mechanism to parse parallel sentence of both languages. Due to the prominent advantages such as the simplicity of grammar and the good coverage of syntactic diversities in different language pairs, BTG has attracted increasing attention in statistical machine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue th"
C10-2136,C08-1127,0,0.0148475,"chine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constitu"
C10-2136,D09-1127,1,0.839344,"DIK Figure 5: Dependency combination of the illformed dependency structure dl with the right well-formed dependency structure dr . G denotes gap and the dotted line denotes the substitution of the gap G with dr . P (wL |wh -as-head) ... · P (wln |wln−1 , wln−2 ) JE Setup The training corpus1 comes from LDC with 1.54M bilingual sentences (41M Chinese words and 48M English words). We run GIZA++ (Och and Ney, 2000) to obtain word alignments with the heuristic method “grow-diag-final-and”. Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al., 2009). From this corpus, we extract bilingual phrases with dependency structure. Here, the maximum length of the source phrase is set to 7. For the n-gram LM, we use SRILM Toolkits (Stolcke, 2002) to train a 4-gram LM on the Xinhua portion of the Gigaword corpus. For the dependency LM, we train different 3-gram dependency LMs at word level and POS level separately on the English side of the training corpus. During the process of bilingual phrase extraction, we collect the neighboring blocks without 1 The training corpus consists of six LDC corpora: LDC2002E18, LDC2003E07, LDC2003E14, Hansards part"
C10-2136,D09-1073,0,0.0134639,"MT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Ga"
C10-2136,C04-1090,0,0.414998,"g the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a signi"
C10-2136,zhang-etal-2004-interpreting,0,0.068922,"Missing"
C10-2136,P09-1063,1,0.918036,"Missing"
C10-2136,P00-1056,0,0.0838278,"s-head) · P (wl2 |wl1 , wh -as-head) 6 where ‘-as-head’ is used to distinguish the head word from child word in the language model. In like manner, P (wR |wh -as-head) has a similar calculation method. KDIK Figure 5: Dependency combination of the illformed dependency structure dl with the right well-formed dependency structure dr . G denotes gap and the dotted line denotes the substitution of the gap G with dr . P (wL |wh -as-head) ... · P (wln |wln−1 , wln−2 ) JE Setup The training corpus1 comes from LDC with 1.54M bilingual sentences (41M Chinese words and 48M English words). We run GIZA++ (Och and Ney, 2000) to obtain word alignments with the heuristic method “grow-diag-final-and”. Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al., 2009). From this corpus, we extract bilingual phrases with dependency structure. Here, the maximum length of the source phrase is set to 7. For the n-gram LM, we use SRILM Toolkits (Stolcke, 2002) to train a 4-gram LM on the Xinhua portion of the Gigaword corpus. For the dependency LM, we train different 3-gram dependency LMs at word level and POS level separately on the English side of the trainin"
C10-2136,D07-1056,0,0.0175994,"on in statistical machine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two cate"
C10-2136,P03-1021,0,0.041434,"Missing"
C10-2136,P08-1064,0,0.0141864,"thods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language"
C10-2136,P02-1040,0,0.0785539,"Missing"
C10-2136,P05-1034,0,0.0710455,"syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over the hierarchical ph"
C10-2136,N03-1017,0,\N,Missing
C10-2136,J07-2003,0,\N,Missing
C12-1176,P09-1088,0,0.136299,"r (Galley et al., 2006; Liu et al., 2006; Chiang, 2007). Most systems induce synchronous grammars (including phrases) from parallel corpora using a heuristic two-step pipeline. This pipeline first aligns a parallel corpus at the word level with heuristic word alignment combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step"
C12-1176,N10-1015,0,0.0540957,"hat our improvement comes from the theoretical advantage of our approach, and that such advantage does exist even on large-scale data. 7 Related Work Because of the great importance of synchronous grammars to SMT systems, researchers have proposed various methods in order to improve the quality of grammars. In addition to the generative model introduced in Section 1, researchers also have made efforts on word alignment and grammar rescoring. The first effort is to improve word alignment by considering phrase/syntax information (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such approaches also use discriminative framework to combine word alignment and syntactic alignment information. In this way, they prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. 2895 Researchers also try to rescore the weights of translation rules. They rescore the weights of rules extracted from the two-step pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporating various features us"
C12-1176,W07-0403,0,0.245791,"nt combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative fo"
C12-1176,P11-2031,0,0.116176,"Missing"
C12-1176,D11-1005,0,0.0253585,"ted from the two-step pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporating various features using labeled data (Huang and Xiang, 2010). Such methods still need to run the heuristic two-step pipeline to extract the grammar, while our method can directly learn the grammar and correspondent weights. Our work also has a connection to the research direction that exploits resources-rich languages to construct similar tools for resource-poor languages. This can be done with parallel data (Pauls et al., 2010) or without parallel data (Cohen et al., 2011). Saers et al. (2009) also propose a cubic biparsing algorithm based on beam pruning. They apply this algorithm for generative model-based ITG grammar induction. Conclusion and Future Work We have presented a global log-linear model for synchronous grammar induction, and have also proposed efficient training and inference algorithms. In addition to the theoretical advantage, we also achieve significant improvements over the traditional heuristic two-stage pipeline on both medium-scale and large-scale training data. In the future, we hope to find efficient algorithms that are capable of incorpo"
C12-1176,D09-1037,0,0.376558,"extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative for synchronous grammar induction is unsupervised discriminative model. Unsupervised disc"
C12-1176,J97-3002,0,0.532366,"Missing"
C12-1176,D08-1033,0,0.164223,"-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative for synchronous grammar induction is unsupe"
C12-1176,P10-1147,0,0.0800555,"; Liu et al., 2006; Chiang, 2007). Most systems induce synchronous grammars (including phrases) from parallel corpora using a heuristic two-step pipeline. This pipeline first aligns a parallel corpus at the word level with heuristic word alignment combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, de"
C12-1176,N10-1033,0,0.0163797,"sorted candidate lists for every substructure, and create cubes that represent the potential combinations of these candidates (see Fig. 2(a)). In this way, we are able to create k-best hyperedges by cube pruning. More specifically, we maintain charts for source words and source spans respectively: • s-chart for each source word. The cell char t[s, i] in s-chart stores a list of candidate alignments for the i-th source word. A candidate alignment is represented by a list of target index. The vertical dimension in Figure 2(a) is an instance of char t[s, 3]. 1 http://leon.bottou.org/projects/sgd Dyer (2010) has shown that two monolingual parses can be more efficient than one synchronous parse, due to the sparsity of pre-fixed translation rules. Such rules are extracted by the heuristic two-step pipeline. In contrast, there are no pre-fixed translation rules in our case. 2 2888 5 ,3 , 5 0, 2 ,2 , 0, 2 zhiyi3 [0,3] [0,5] r3 1 2 [0,2] zhiyi 3 [2,5] zhiyi 3 4 1 2 Figure 2: Construct hyperedge from a cube. (a) Cube X 0,2 zhiyi3 for source span (0,3). The vertical direction represents the candidate list of zhiyi3 , while the horizontal direction is a list of nodes that share the same source span (0,2)"
C12-1176,P11-1042,0,0.215487,"ules discovered in the training corpus by our algorithm. sr c() denotes the source side of a rule. G() is the set of rules in a hypergraph. The neighbor grammar contains those rules that are discovered during training (rather than all potential rules), and whose source sides occur in the synchronous hypergraph. Sine the size of NG is typical fairly small, the parsing of our source hypergraph becomes tractable in practice. The definition of neighbor source hypergraph is inspired by contrastive estimation (Smith and Eisner, 2005). Similar shrinkage of discriminative neighborhood is also used in Dyer et al. (2011a). Notably, our approximation is consistent with the purpose of synchronous grammar induction for SMT. In SMT, the goal of grammar induction is for translation rather than synchronous parsing. Our source hypergraph corresponds to the potential translation space during SMT decoding. Thus, we expect such approximation to be suitable for SMT. 3.3 Training Algorithm Based on the synchronous hypergraph and source hypergraph introduced above, we optimize L in an online style as shown in Algorithm 1. For each sentence pair, we first use cube-pruning based biparsing (Sec. 4) to construct a synchronou"
C12-1176,W11-2139,0,0.468141,"ules discovered in the training corpus by our algorithm. sr c() denotes the source side of a rule. G() is the set of rules in a hypergraph. The neighbor grammar contains those rules that are discovered during training (rather than all potential rules), and whose source sides occur in the synchronous hypergraph. Sine the size of NG is typical fairly small, the parsing of our source hypergraph becomes tractable in practice. The definition of neighbor source hypergraph is inspired by contrastive estimation (Smith and Eisner, 2005). Similar shrinkage of discriminative neighborhood is also used in Dyer et al. (2011a). Notably, our approximation is consistent with the purpose of synchronous grammar induction for SMT. In SMT, the goal of grammar induction is for translation rather than synchronous parsing. Our source hypergraph corresponds to the potential translation space during SMT decoding. Thus, we expect such approximation to be suitable for SMT. 3.3 Training Algorithm Based on the synchronous hypergraph and source hypergraph introduced above, we optimize L in an online style as shown in Algorithm 1. For each sentence pair, we first use cube-pruning based biparsing (Sec. 4) to construct a synchronou"
C12-1176,P06-1121,0,0.117782,"Missing"
C12-1176,C10-1056,0,0.0569973,"11). Such approaches also use discriminative framework to combine word alignment and syntactic alignment information. In this way, they prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. 2895 Researchers also try to rescore the weights of translation rules. They rescore the weights of rules extracted from the two-step pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporating various features using labeled data (Huang and Xiang, 2010). Such methods still need to run the heuristic two-step pipeline to extract the grammar, while our method can directly learn the grammar and correspondent weights. Our work also has a connection to the research direction that exploits resources-rich languages to construct similar tools for resource-poor languages. This can be done with parallel data (Pauls et al., 2010) or without parallel data (Cohen et al., 2011). Saers et al. (2009) also propose a cubic biparsing algorithm based on beam pruning. They apply this algorithm for generative model-based ITG grammar induction. Conclusion and Futur"
C12-1176,P08-1067,0,0.116367,"Missing"
C12-1176,D09-1107,0,0.449941,"nto t, and d is one such derivation. Given a source sentence s, the conditional probability of a derivation d and the corresponding translation t is: P exp i λi H i (d, t, s) p(d, t|s) = (2) Z(s) P where H i (d, t, s) = r∈d hi (r, s) is feature function. We assume H i decomposes with derivation d in terms of local feature function hi , which is related to a rule r and a source sentence s. λi is the correspondent feature weight. Z(s) is the partition function: X X X Z(s) = exp λi H i (d, t, s) (3) t d∈△(t,s) i Such a discriminative latent variable model is not new to SMT (Blunsom et al., 2008; Kääriäinen, 2009; Xiao et al., 2011). However, we are distinguished from previous work by applying this model to synchronous grammar induction. The purpose of the latent variable model in such previous work is to do max-translation decoding and training (Blunsom et al., 2008), or to eliminate the gap between heuristic extraction and decoding (Kääriäinen, 2009), instead of grammar induction as synchronous rules are still extracted by the heuristic two-step pipeline. In contrast, our interest lies in using latent variable model to learn synchronous grammar directly from sentence pairs. Overall, to the best of o"
C12-1176,W01-1812,0,0.0529357,"tion E p(d|t,s) [H i ] of a parameter given a sentence pair, and the second one E p(d,t|s) [H i ] is the expectation of a parameter given the source sentence. In the following sections, we first introduce how to use hypergraph to compute these expectations by synchronous hypergraph and source hypergraph respectively (Sec. 3.1). Then, we discuss the intractability of the exact training, and achieve tractable training by approximation (Sec. 3.2). Finally, we describe the training algorithm in detail to explain how the rules is induced (Sec. 3.3). 3.1 Inference with Hypergraph We use hypergraph (Klein and Manning, 2001) to compactly represent the space of derivations. Based on hypergraphs, it’s straightforward to calculate the two expectations in Eq. (5). The first expectation E p(d|t,s) [H i ] is the expected value when observing both source sentence s and target sentence t. The second expectation E p(d,t|s) [H i ] is a similar function, but only the source sentence is observed. Thus, in order to calculate the first expectation, we construct a synchronous hypergraph to represent all derivations of a sentence pair. Similarly, for the second expectation, we use a source hypergraph to represent all derivations"
C12-1176,W04-3250,0,0.12891,"reports the average score on the three test set. |G′ | MT03 MT04 MT05 Avg. Moses-Chart 45.0M 32.93 34.73 31.24 32.97 Baseline Baseline-expand 13.2M 46.2M 32.36 33.04 34.51 35.13 31.86 32.08 32.91 33.42 UDSGI Dense UDSGI Dense+Sparse 13.1M 13.1M 33.46 33.58 35.43 36.27 32.74 33.05 33.87 34.30 System Table 3: Evaluation of translation quality in terms of BLEU. Moses-Chart is the running of hierarchical phrased-model in Moses. Baseline-expand uses a similar extraction constraint to Moses-Chart and the same decoder as Baseline. The improvement of UDSGI over Baseline is statistically significant (Koehn, 2004) (p < 0.01). We can not directly evaluate the quality of grammar, since there is not a golden grammar. However, as grammar is used to generate target translations, it’s reasonable to decide the quality of a grammar by testing what best translations it can produce. Therefore, we compare the oracle translation result of the grammar in baseline and the grammar in UDSGI, with four reference translations given.4 As shown in Table 2, our grammar achieves a much higher oracle BLEU (+8.3 BLEU points) than the baseline grammar. This suggests that the grammar induced by our method is able to generate be"
C12-1176,P07-2045,0,0.00773561,"four reference translations given.4 As shown in Table 2, our grammar achieves a much higher oracle BLEU (+8.3 BLEU points) than the baseline grammar. This suggests that the grammar induced by our method is able to generate better translations than the grammar extracted by the traditional two-step pipeline. 6.3 Translation Results Table 3 compares the translation performance of our approach and the baseline on the test sets. When extracting rules as Chiang (2007), the baseline produces an average BLEU of 32.91. We also run Moses-Chart, the implementation of hierarchical phrased-model in Moses (Koehn et al., 2007), on our data with its default settings. As shown in the table, our Baseline is comparable with Moses-Chart. However, Moses-Chart extracts much more rules, because it uses a different extraction constraint from Chiang (2007). The differences mainly include edges of initial phrases can be unaligned and minimum size of source part of sub-phrases is 2. We also relax the Baseline by applying these two options, and call such setting as Baselineexpand. Baseline-expand outperforms Baseline by +0.51 BLEU with 3.5 times of rules. As our UDSGI method learns a similar size of rules with the Baseline’s, w"
C12-1176,N03-1017,0,0.512013,"with millions of features that contain rule level, word level and translation boundary information, we significantly outperform a competitive hierarchical phrased-based baseline system by +1.4 BLEU on average on three NIST test sets. KEYWORDS: synchronous grammar induction, discriminative model, unsupervised learning, machine translation. Proceedings of COLING 2012: Technical Papers, pages 2883–2898, COLING 2012, Mumbai, December 2012. 2883 1 Introduction In the last decade, statistical machine translation (SMT) has been advanced by expanding the basic unit of translation from word to phrase (Koehn et al., 2003) and grammar (Galley et al., 2006; Liu et al., 2006; Chiang, 2007). Most systems induce synchronous grammars (including phrases) from parallel corpora using a heuristic two-step pipeline. This pipeline first aligns a parallel corpus at the word level with heuristic word alignment combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignm"
C12-1176,D12-1021,0,0.600119,"sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative for synchronous grammar induction is unsupervised discriminative model. Unsupervised discriminative model can directly learn synchronou"
C12-1176,P09-1067,0,0.0479859,"is the set of hyperedges. Each hyperedge e ∈ E connects a set of antecedent nodes to a single consequent node. And each hyperedge corresponds to an SCFG rule r. In a synchronous hypergraph, a node v ∈ V is in the form X i, j,k,l , which denotes the nonterminal X spanning from i to j (that is si+1 ...s j ) in the source sentence, and from k to l in the target sentence. In a source hypergraph, each node v ∈ V is in the form X i, j , which spans from i to j in the source sentence. Based on these hypergraphs, we compute the two expectations by applying the inside-outside algorithm as described in Li et al. (2009). The computation complexity is linear to the size of hypergraph O(|E|). More exactly, O(|E|) denotes O(|s|3 |t|3 ) for synchronous hypergraph, and O(|G||s|3 ) for source hypergraph. Here, G denotes all potential synchronous grammars. 3.2 Tractable Estimation by Approximation However, the size of potential SCFGs G is extremely large given a vocabulary Ω, resulting in a large number of hyperedges in source hypergraph. See the rule r2 in Figure 1. In reality, there are many potential translation rules that share the same source side “shaoshu X ” as r2 , but with different target side. Suppose n"
C12-1176,P06-1077,1,0.815726,"d level and translation boundary information, we significantly outperform a competitive hierarchical phrased-based baseline system by +1.4 BLEU on average on three NIST test sets. KEYWORDS: synchronous grammar induction, discriminative model, unsupervised learning, machine translation. Proceedings of COLING 2012: Technical Papers, pages 2883–2898, COLING 2012, Mumbai, December 2012. 2883 1 Introduction In the last decade, statistical machine translation (SMT) has been advanced by expanding the basic unit of translation from word to phrase (Koehn et al., 2003) and grammar (Galley et al., 2006; Liu et al., 2006; Chiang, 2007). Most systems induce synchronous grammars (including phrases) from parallel corpora using a heuristic two-step pipeline. This pipeline first aligns a parallel corpus at the word level with heuristic word alignment combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein,"
C12-1176,W02-1018,0,0.0903456,"heuristic word alignment combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet"
C12-1176,D07-1038,0,0.220266,"Missing"
C12-1176,P11-1064,0,0.359975,"es from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative for synchronous grammar induction is unsupervised discriminative model. Unsupervised discriminative model can"
C12-1176,P03-1021,0,0.0658986,"for MERT, and test the translation performance on the NIST 2003-2005 (MT03-05) evaluation sets. Case-insensitive NIST BLEU-4 (Papineni et al., 2002) is used to measure translation performance. We used a bilingual corpus that contains 200K sentence pairs of up to length 40 from the LDC data. 3 There are 8.8M words in the 200K data. We trained a 5-grams language model by the SRILM toolkit (Stolcke, 2002). The monolingual data for language model training includes the Xinhua portion of the GIGAWORD corpus and the English side of the entire LDC data, which contains 432 million words. We used MERT (Och, 2003) to optimize the feature weights for decoding by maximizing BLEU. Since the instability of MERT has a substantial impact on results, we follow Clark et al. (2011) to report the average scores of 3 independent runs. 6.2 Grammar Analysis and Oracle Results The synchronous grammar generated by UDSGI has 13.1 millions rules. The number of these rules is comparable with that of grammar extracted by the traditional pipeline, which has 13.2 millions rules. However, the two grammars are quite different as shown in Table 1. Our grammar is more reusable than the baseline’s, because it contains less sour"
C12-1176,J03-1002,0,0.00589028,"The experiments are aimed at measuring the quality and effectiveness of grammar induced by our method. We present the performance of our unsupervised discriminate synchronous grammar induction (UDSGI) using two groups of features during decoding. We test the translation performance of UDSGI and the baseline on the same decoder. Baseline The baseline system is an in-house implementation of hierarchical phrase-based translation system(Chiang, 2007). The grammar is extracted from word-aligned corpus by traditional two-step pipeline. Symmetric word alignments were created by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-finaland” (Koehn et al., 2003). The system uses 8 dense features including: forward and backward translation probabilities; forward and backward lexical weights; language model; 3 penalties for word count, extracted rule count, and glue rule count. UDSGI Dense This configuration uses the same feature set as the baseline. Our log-linear model for grammar induction does not contain the forward and backward translation probabilities. However, we still compute the forward and backward translation probabilities for UDSGI by normalizin"
C12-1176,P02-1040,0,0.0874883,"ize these sparse features with the dense features by minimum error rate training (MERT), we group features of the same type into one coarse ""summary feature"", and get three such features including: rule, word-pair and phraseboundary features. In this way, we rescale the weights of the three ""summary features"" with the 8 dense features by MERT. Such approach is similar to Dyer et al. (2011b). 6.1 Data We used the NIST evaluation set of 2002 (MT02) as our development set for MERT, and test the translation performance on the NIST 2003-2005 (MT03-05) evaluation sets. Case-insensitive NIST BLEU-4 (Papineni et al., 2002) is used to measure translation performance. We used a bilingual corpus that contains 200K sentence pairs of up to length 40 from the LDC data. 3 There are 8.8M words in the 200K data. We trained a 5-grams language model by the SRILM toolkit (Stolcke, 2002). The monolingual data for language model training includes the Xinhua portion of the GIGAWORD corpus and the English side of the entire LDC data, which contains 432 million words. We used MERT (Och, 2003) to optimize the feature weights for decoding by maximizing BLEU. Since the instability of MERT has a substantial impact on results, we fo"
C12-1176,N10-1014,0,0.0886082,"refore, we believe that our improvement comes from the theoretical advantage of our approach, and that such advantage does exist even on large-scale data. 7 Related Work Because of the great importance of synchronous grammars to SMT systems, researchers have proposed various methods in order to improve the quality of grammars. In addition to the generative model introduced in Section 1, researchers also have made efforts on word alignment and grammar rescoring. The first effort is to improve word alignment by considering phrase/syntax information (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such approaches also use discriminative framework to combine word alignment and syntactic alignment information. In this way, they prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. 2895 Researchers also try to rescore the weights of translation rules. They rescore the weights of rules extracted from the two-step pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporati"
C12-1176,D11-1046,0,0.0596119,"mes from the theoretical advantage of our approach, and that such advantage does exist even on large-scale data. 7 Related Work Because of the great importance of synchronous grammars to SMT systems, researchers have proposed various methods in order to improve the quality of grammars. In addition to the generative model introduced in Section 1, researchers also have made efforts on word alignment and grammar rescoring. The first effort is to improve word alignment by considering phrase/syntax information (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such approaches also use discriminative framework to combine word alignment and syntactic alignment information. In this way, they prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. 2895 Researchers also try to rescore the weights of translation rules. They rescore the weights of rules extracted from the two-step pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporating various features using labeled data (Hua"
C12-1176,W09-3804,0,0.0647723,"pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporating various features using labeled data (Huang and Xiang, 2010). Such methods still need to run the heuristic two-step pipeline to extract the grammar, while our method can directly learn the grammar and correspondent weights. Our work also has a connection to the research direction that exploits resources-rich languages to construct similar tools for resource-poor languages. This can be done with parallel data (Pauls et al., 2010) or without parallel data (Cohen et al., 2011). Saers et al. (2009) also propose a cubic biparsing algorithm based on beam pruning. They apply this algorithm for generative model-based ITG grammar induction. Conclusion and Future Work We have presented a global log-linear model for synchronous grammar induction, and have also proposed efficient training and inference algorithms. In addition to the theoretical advantage, we also achieve significant improvements over the traditional heuristic two-stage pipeline on both medium-scale and large-scale training data. In the future, we hope to find efficient algorithms that are capable of incorporating contextual fea"
C12-1176,P05-1044,0,0.289551,"nerative model. However, the advantage over generative model is that it is able to easily incorporate word alignment information which has been proved useful in the two-step pipeline. In this paper, we propose a global log-linear model (Sec. 2) for the induction of synchronous context free grammar (SCFG) (Chiang, 2007). The log-linear model is able to incorporate arbitrary features. Furthermore, it is trained from sentence pairs without word alignments in an unsupervised fashion. In particular: • We approximate the exact conditional log-likelihood objective inspired by contrastive estimation (Smith and Eisner, 2005) as the optimization of the exact objective is very expensive. The key idea is to estimate parameters via synchronous hypergraphs of sentence pairs and neighbor source hypergraphs of source sentences (Sec. 3). • Synchronous parsing is often impractical in large-scale learning applications due to its high complexity O(n6 ). We address this challenge by proposing a novel and efficient O(n3 ) cube pruning based synchronous parsing algorithm (Sec. 4). • Aiming to enhance the ability to predict whether a translation derivation is good or not, we incorporate a variety of fine-grained features into o"
C12-1176,P09-1054,0,0.0315396,"SE(NG, s) ⊲ generate neighbor source hypergraph λ ← λ + η × ∂∂ λL (H1 , H2 ) ⊲ η is learning rate return G′ , λ and store them in G′ . After that, we create the neighbor source hypergraph H2 by exhaustive bottom-up chart parsing using the neighbor grammar NG (line 6). Finally, we calculate the gradient by these two hypergraphs and update the feature weights (line 8). When the algorithm is complete, we learn a grammar G′ and also the feature weights λ of the model. We implement an stochastic gradient descent (SGD) recommended by Bottou.1 We schedule the learning rate η by an exponential decay (Tsuruoka et al., 2009). We set the regularization strength, initial learning rate and the base of exponential decay as 1.0, 0.2, 0.9 respectively. We choose these values by maximizing the translation performance measured by BLEU on the NIST 2002 development set with a subset of our training data including 20k sentence pairs. 4 Cube Pruning-based Synchronous Parsing The approximation makes the training algorithm tractable. However, there is still one problem: how to efficiently construct the synchronous hypergraph? Exhaustive synchronous parsing requires O(|s|3 |t|3 ) time.2 To overcome this challenge, we propose a"
C12-1176,D11-1081,1,0.868474,"e such derivation. Given a source sentence s, the conditional probability of a derivation d and the corresponding translation t is: P exp i λi H i (d, t, s) p(d, t|s) = (2) Z(s) P where H i (d, t, s) = r∈d hi (r, s) is feature function. We assume H i decomposes with derivation d in terms of local feature function hi , which is related to a rule r and a source sentence s. λi is the correspondent feature weight. Z(s) is the partition function: X X X Z(s) = exp λi H i (d, t, s) (3) t d∈△(t,s) i Such a discriminative latent variable model is not new to SMT (Blunsom et al., 2008; Kääriäinen, 2009; Xiao et al., 2011). However, we are distinguished from previous work by applying this model to synchronous grammar induction. The purpose of the latent variable model in such previous work is to do max-translation decoding and training (Blunsom et al., 2008), or to eliminate the gap between heuristic extraction and decoding (Kääriäinen, 2009), instead of grammar induction as synchronous rules are still extracted by the heuristic two-step pipeline. In contrast, our interest lies in using latent variable model to learn synchronous grammar directly from sentence pairs. Overall, to the best of our knowledge, this i"
C12-1176,N10-1016,1,0.945845,"parameters via synchronous hypergraphs of sentence pairs and neighbor source hypergraphs of source sentences (Sec. 3). • Synchronous parsing is often impractical in large-scale learning applications due to its high complexity O(n6 ). We address this challenge by proposing a novel and efficient O(n3 ) cube pruning based synchronous parsing algorithm (Sec. 4). • Aiming to enhance the ability to predict whether a translation derivation is good or not, we incorporate a variety of fine-grained features into our model, including rule level features, word level features and phrase boundary features (Xiong et al., 2010) (Sec. 5). We evaluate our approach on the NIST Chinese-English translation task. According to the analysis of grammar (Sec. 6.2), our induced grammar is more reusable, and is able to generate better (+8.3 BLEU points) oracle translations than the grammar of the baseline. Meanwhile, in the end-to-end machine translation experiments, our approach outperforms the two-step pipeline by +1.4 BLEU points (Sec. 6.3). 2884 2 Global Log-linear Model We propose a log-linear model to induce SCFG rules for hierarchical phrase-based translation (Chiang, 2007) which transforms a source sentence s into a tar"
C12-1176,P08-1012,0,0.0560722,"ies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative for synchronous gramma"
C12-1176,2009.eamt-smart.4,0,\N,Missing
C12-1176,J07-2003,0,\N,Missing
C12-1176,P08-1024,0,\N,Missing
C12-2080,P04-1015,0,0.052779,"rd segmentation, dictionary maximum matching. Proceedings of COLING 2012: Posters, pages 819–828, COLING 2012, Mumbai, December 2012. 819 1 Introduction Word segmentation is a basic and important task for information processing of Chinese. Most effective approaches (Xue and Shen, 2003; Ng and Low, 2004) to CWS treat it as a character tagging task, in which the model used to make tagging decisions can be trained by discriminative methods, such as Maximum Entropy (ME) (Ratnaparkhi and Adwait, 1996), Conditional Random Fields (Lafferty et al., 2001), perceptron training algorithm (Collins, 2002; Collins and Roark, 2004), etc. These methods have achieved good results, but rely on large scale high quality annotated corpora, which are rare in resource-poor languages and domains. Besides, directly adapting a classifier trained on one domain to another domain leads to poorer performance 1 . Given a dictionary, dictionary maximum matching (DMM) is an alternative in the case of no available annotated corpora, but its performance is not satisfying due to the poor ability on OOV words recognition. Dict Dictionary Matching Feature Extraction Training with ME Classifier Raw Text Figure 1: The pipeline of our method. In"
C12-2080,P11-2095,0,0.0356264,"Missing"
C12-2080,P09-1059,1,0.854383,"dictionary matching based feature instances extraction, we give a brief introduction of gap tagging classification strategy for segmentation. Formally, a Chinese sentence can be represented as a character sequence: C1:n = C1 C2 · · · Cn , where Ci (i = 1, · · · , n) is a character. We explicitly add the gap Gi (i = 1, · · · , n − 1) of character Ci and Ci+1 to the sentence C1:n , denoted as C1:n |G1:n−1 = C1 G1 C2 G2 · · · Gn−1 Cn . Then the segmented results with gaps represented as follows: 1 C1:e1 |G1:e1 −1 , Ge1 , Ce1 +1:e2 |Ge1 +1:e2 −1 , Ge2 , · · · , Gem−1 , Cem−1 +1:em |Gem−1 +1:em −1 Jiang et al. (2009) describe a similar situation. We also tried to directly adapt a classifier trained with news corpus to Chinese medicine patent corpus, which led to dramatic decrease in accuracy. 820 where Ci: j |Gi: j−1 denotes the character-gap sequence with gaps Gi: j−1 . As is shown above, there are two kinds of gaps, one occurs inside a word, such as G1 · · · Ge1 −1 ; the other occurs between two words, such as Ge1 . Word Segmentation can be treated as a gap tagging problem. 2.1 From Character Tagging to Gap Tagging Ng and Low (2004) give a boundary tag to each character denoting its relative position in"
C12-2080,D12-1038,1,0.780811,"Missing"
C12-2080,N09-1036,0,0.0434464,"Missing"
C12-2080,P09-1058,0,0.0458646,"Missing"
C12-2080,P11-1141,0,0.0231981,"Missing"
C12-2080,J09-4006,0,0.0361854,"Missing"
C12-2080,P09-1012,0,0.0726165,"Missing"
C12-2080,W04-3236,0,0.0199792,":e2 |Ge1 +1:e2 −1 , Ge2 , · · · , Gem−1 , Cem−1 +1:em |Gem−1 +1:em −1 Jiang et al. (2009) describe a similar situation. We also tried to directly adapt a classifier trained with news corpus to Chinese medicine patent corpus, which led to dramatic decrease in accuracy. 820 where Ci: j |Gi: j−1 denotes the character-gap sequence with gaps Gi: j−1 . As is shown above, there are two kinds of gaps, one occurs inside a word, such as G1 · · · Ge1 −1 ; the other occurs between two words, such as Ge1 . Word Segmentation can be treated as a gap tagging problem. 2.1 From Character Tagging to Gap Tagging Ng and Low (2004) give a boundary tag to each character denoting its relative position in a word. There are four boundary tags: &quot;b&quot; for a character that begins a word, &quot;m&quot; for a character that occurs in the middle of a word, &quot;e&quot; for a character that ends a word, and &quot;s&quot; for a character that occurs as a single-character word. Following Ng and Low (2004), the feature templates and the corresponding instances are listed in Table 1. Feature Template Ci (i = −2, · · · , 2) Ci Ci+1 (i = −2, · · · , 1) C−1 C1 Pu(C0 ) T (C−2 )T (C−1 )T (C0 )T (C1 )T (C2 ) Instances C−2 = , C−1 = , C0 = , C1 = , C2 = C−2 C−1 = , C−1 C0"
C12-2080,W96-0213,0,0.579617,"Missing"
C12-2080,P11-1139,0,0.0233332,"Missing"
C12-2080,P12-1027,0,0.0602497,"Missing"
C12-2080,J11-3001,0,0.0503322,"Missing"
C12-2080,C10-1132,0,0.0490416,"Missing"
C12-2080,W03-1728,0,0.08618,"Missing"
C12-2080,P07-1106,0,0.0647758,"Missing"
C12-2080,P08-1101,0,0.0425805,"Missing"
C12-2080,D10-1082,0,0.0260372,"Missing"
C12-2122,P06-1002,0,0.0193855,"nother alignment by Vigne (a3 ). where S and P are sets of sure and possible links in a hand-aligned reference alignment respectively, and A is a candidate alignment. Note that S is a subset of P: S ⊆ P. As there is no reference alignment that is hand-aligned by human experts in our work, we cannot distinguish sure links from possible links. Therefore, we regard all links to be sure links: S = P. With this, the AER score is calculated by: AER(ai , a j ) = 1 − (2 × |ai ∩ a j |)/(|ai |+ |a j |) (3) CPER Although widely used, AER is criticized for correlating poorly with translation performance (Ayan and Dorr, 2006; Fraser and Marcu, 2007). Therefore, Ayan and Dorr (2006) have proposed constituent phrase error rate (CPER) for evaluating word alignments at the phrase level instead of the alignment level. CPER can be computed as: C P ER(ai , a j ) = 1 − (2 × |Pai ∩ Pa j |)/(|Pai |+ |Pa j |) (4) where Pa denotes the set of phrases that are consistent with a given alignment a. Compared with AER, CPER penalizes dissimilar alignment links more heavily. As a dissimilar link reduces the number of intersected links of two alignments by 1 in AER, it might lead to more than one different phrase pair added to or re"
C12-2122,H05-1009,0,0.0216,"stem combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (D"
C12-2122,P06-1009,0,0.0532677,"Missing"
C12-2122,J93-2003,0,0.0478199,"Missing"
C12-2122,P05-1033,0,0.441175,"robabilities. Instead of extracting phrase pairs that respect the word alignment, Tu et al. (2011) enumerate all potential phrase pairs and calculate their fractional counts. As they soften the alignment consistency constraint, there exists a massive number of phrase pairs extracted from the training corpus. To maintain a reasonable phrase table size, they discard any phrase pair that has a fractional count lower than a threshold t. For further details, see (Tu et al., 2011). 4 Experiments 4.1 Setup We carry out our experiments using a reimplementation of the hierarchical phrase-based system (Chiang, 2005) on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs from LDC dataset.4 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments"
C12-2122,P05-1066,0,0.166329,"Missing"
C12-2122,P11-1043,0,0.0377862,"he process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weigh"
C12-2122,C10-1036,0,0.01903,"2 3.1.2 Loss Functions The loss function L (ai , a j ) is used to measure the quality of alignments. Here we introduce a set of metrics for the evaluation of alignments at both alignment and phrase levels. AER Alignment error rate (Och and Ney, 2003) has been used as the official evaluation criterion in most alignment shared tasks (Liu et al., 2009). AER scores are given by: 2 AER(S, P, A) = 1 − (|A ∩ S |+ |A ∩ P|)/(|A |+ |S|) (2) Alignment probabilities can be set empirically based on (expected overall) performance (Fossum and Knight, 2009), or uniformly without any bias (Xiao et al., 2010; Duan et al., 2010). We tried a few other settings and found them to be less effective. 1251 the development of China ’s economy (a) zhongguo de jingji fazhan zhongguo de jingji fazhan zhongguo de jingji fazhan the development of China ’s economy the development of China ’s economy (b) (c) Figure 1: (a) Alignment of a sentence pair generated by GIZA++ (a1 ), (b) alignment of the same sentence by Berkeley aligner (a2 ), (c) another alignment by Vigne (a3 ). where S and P are sets of sure and possible links in a hand-aligned reference alignment respectively, and A is a candidate alignment. Note that S is a subset"
C12-2122,P08-1115,0,0.0280219,"5; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment selection refers to selecting one alignment from multiple alignments using minim"
C12-2122,D09-1115,1,0.846211,"). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering mo"
C12-2122,N09-2064,0,0.13425,"Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary nu"
C12-2122,J07-3002,0,0.0988242,"igne (a3 ). where S and P are sets of sure and possible links in a hand-aligned reference alignment respectively, and A is a candidate alignment. Note that S is a subset of P: S ⊆ P. As there is no reference alignment that is hand-aligned by human experts in our work, we cannot distinguish sure links from possible links. Therefore, we regard all links to be sure links: S = P. With this, the AER score is calculated by: AER(ai , a j ) = 1 − (2 × |ai ∩ a j |)/(|ai |+ |a j |) (3) CPER Although widely used, AER is criticized for correlating poorly with translation performance (Ayan and Dorr, 2006; Fraser and Marcu, 2007). Therefore, Ayan and Dorr (2006) have proposed constituent phrase error rate (CPER) for evaluating word alignments at the phrase level instead of the alignment level. CPER can be computed as: C P ER(ai , a j ) = 1 − (2 × |Pai ∩ Pa j |)/(|Pai |+ |Pa j |) (4) where Pa denotes the set of phrases that are consistent with a given alignment a. Compared with AER, CPER penalizes dissimilar alignment links more heavily. As a dissimilar link reduces the number of intersected links of two alignments by 1 in AER, it might lead to more than one different phrase pair added to or removed from the set of phr"
C12-2122,P06-1121,0,0.10519,"Missing"
C12-2122,P11-1127,0,0.0185151,"s high quality alignment choices, that can be exploited by alignment compaction. 2 Related Work Our research builds on previous work in the field of minimum Bayes risk (MBR) decision, system combination and model compaction. MBR decision aims to find the candidate hypothesis that has the least expected loss under a probability model when the true reference is not known (Brickel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; F"
C12-2122,D08-1011,0,0.0397351,"Missing"
C12-2122,W99-0623,0,0.0194602,"eference is not known (Brickel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alig"
C12-2122,2006.amta-papers.8,0,0.0607293,"Missing"
C12-2122,N03-1017,0,0.0694144,"nd phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-be"
C12-2122,N04-1022,0,0.0650294,"ell together: alignment refinement e.g. offers high quality alignment choices, that can be exploited by alignment compaction. 2 Related Work Our research builds on previous work in the field of minimum Bayes risk (MBR) decision, system combination and model compaction. MBR decision aims to find the candidate hypothesis that has the least expected loss under a probability model when the true reference is not known (Brickel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to"
C12-2122,N06-1014,0,0.0889621,"ce pairs from LDC dataset.4 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments with default settings: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and the linear modeling alignment Vigne (Liu et al., 2010). We use the three baseline alignments to select MBR alignments and to generate a refined alignment. We use all three baseline alignments, as well as all of the MBR and refined alignments in the WAM-based compaction approach. When extracting rules from WAM, we follow (Tu et al., 2011) to set the pruning threshold t=0.5. 3 In practice, alignment compaction encodes both baseline alignments and the new alignments in Section 3.1 and 3.2. 4 The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 a"
C12-2122,P06-1077,1,0.905513,"Missing"
C12-2122,J10-3002,1,0.857717,"n the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments with default settings: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and the linear modeling alignment Vigne (Liu et al., 2010). We use the three baseline alignments to select MBR alignments and to generate a refined alignment. We use all three baseline alignments, as well as all of the MBR and refined alignments in the WAM-based compaction approach. When extracting rules from WAM, we follow (Tu et al., 2011) to set the pruning threshold t=0.5. 3 In practice, alignment compaction encodes both baseline alignments and the new alignments in Section 3.1 and 3.2. 4 The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 1254 Alignments GIZA++ Berkeley Vigne Selecti"
C12-2122,D09-1106,1,0.959517,"t al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment selection refers to selecting one alignment from multiple alignments using minimum Bayes risk. If the reference alignment a was known, we could measure each alig"
C12-2122,D08-1022,0,0.0291746,"en explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment s"
C12-2122,P06-1065,0,0.0455271,"Missing"
C12-2122,P03-1021,0,0.0759237,"size, they discard any phrase pair that has a fractional count lower than a threshold t. For further details, see (Tu et al., 2011). 4 Experiments 4.1 Setup We carry out our experiments using a reimplementation of the hierarchical phrase-based system (Chiang, 2005) on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs from LDC dataset.4 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments with default settings: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and the linear modeling alignment Vigne (Liu et al., 2010). We use the three baseline alignments to select MBR alignments and to generate a refined alignment. We use all three baseline alignments, as well as all of the MBR and refined al"
C12-2122,J03-1002,0,0.167522,"at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), wor"
C12-2122,J04-4002,0,0.308906,"Missing"
C12-2122,P02-1040,0,0.0852613,"ts 4.1 Setup We carry out our experiments using a reimplementation of the hierarchical phrase-based system (Chiang, 2005) on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs from LDC dataset.4 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments with default settings: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and the linear modeling alignment Vigne (Liu et al., 2010). We use the three baseline alignments to select MBR alignments and to generate a refined alignment. We use all three baseline alignments, as well as all of the MBR and refined alignments in the WAM-based compaction approach. When extracting rules from WAM, we follow (Tu et al., 2011) to set the pruning threshold t=0.5. 3 In practice,"
C12-2122,N07-1029,0,0.0897098,"s calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, wh"
C12-2122,N06-2033,0,0.0331853,"kel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approa"
C12-2122,P08-1066,0,0.102941,"Missing"
C12-2122,P12-1025,0,0.0250082,"using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into ac"
C12-2122,H05-1010,0,0.073942,"Missing"
C12-2122,D08-1065,0,0.016802,"refinement e.g. offers high quality alignment choices, that can be exploited by alignment compaction. 2 Related Work Our research builds on previous work in the field of minimum Bayes risk (MBR) decision, system combination and model compaction. MBR decision aims to find the candidate hypothesis that has the least expected loss under a probability model when the true reference is not known (Brickel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard"
C12-2122,C10-1123,1,0.860759,"ly (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment selection refers to"
C12-2122,I11-1145,1,0.883984,"ur technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment selection refers to selecting one alignment from multiple alignments using minimum Bayes risk. If the reference alignment a was known, we could measure each alignment ai using the"
C12-2122,C96-2141,0,0.530719,"Missing"
C12-2122,C10-2154,0,0.0147392,"ave the same value. 2 3.1.2 Loss Functions The loss function L (ai , a j ) is used to measure the quality of alignments. Here we introduce a set of metrics for the evaluation of alignments at both alignment and phrase levels. AER Alignment error rate (Och and Ney, 2003) has been used as the official evaluation criterion in most alignment shared tasks (Liu et al., 2009). AER scores are given by: 2 AER(S, P, A) = 1 − (|A ∩ S |+ |A ∩ P|)/(|A |+ |S|) (2) Alignment probabilities can be set empirically based on (expected overall) performance (Fossum and Knight, 2009), or uniformly without any bias (Xiao et al., 2010; Duan et al., 2010). We tried a few other settings and found them to be less effective. 1251 the development of China ’s economy (a) zhongguo de jingji fazhan zhongguo de jingji fazhan zhongguo de jingji fazhan the development of China ’s economy the development of China ’s economy (b) (c) Figure 1: (a) Alignment of a sentence pair generated by GIZA++ (a1 ), (b) alignment of the same sentence by Berkeley aligner (a2 ), (c) another alignment by Vigne (a3 ). where S and P are sets of sure and possible links in a hand-aligned reference alignment respectively, and A is a candidate alignment. Note"
C12-2122,P06-1066,1,0.854268,"Missing"
C14-1104,J93-2003,0,0.0392884,"eled independently in order to break the synchronous constraint, resorting to dependency structures with dependency edges as atomic manipulating units. Large-scale experiments on Chinese to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most"
C14-1104,W09-2307,0,0.0218502,"translation model that achieves the state-of-the-art performance. 2 Dependency Edge-based Transfer Model 2.1 Edges in Dependency Trees Given a sentence, its dependency tree is a directed acyclic graph with words in the sentence as nodes. An example dependency tree is shown in Figure 1 (a). An edge in the tree represents a dependency relationship between a pair of words, a head and a dependent. When a nominal dependent acts as a subject and modifies a verbal head, they usually have a fixed relative position. In Figure 1 (a), “`aob¯am˘a” modifies “f¯ab`u”. The grammatical relation label nsubj (Chang et al., 2009) between them denotes that a noun phrase acts as the subject of a clause. “`aob¯am˘a” is on the left of “f¯ab`u”. Based on the above observations, we take the edge as the elementary structure of a dependency tree and regard a dependency tree to be a set of edges. Definition 1. An source side edge is a 4-tuple e = ⟨H, D, P, R⟩, where H is the head, D is the dependent, P denotes the relative position between H and D, left or right, R is the grammatical relation label . In Figure 1 (b), the upper sides of transfer rules are source side edges extracted from the dependency tree. 1104 àobāmǎ jīntiān"
C14-1104,P05-1033,0,0.294445,"Large-scale experiments on Chinese to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspo"
C14-1104,P05-1066,0,0.140302,"Missing"
C14-1104,W04-1513,0,0.0163552,"hibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspondence between two languages. In contrast, the analysis-transfer-generation methodology i"
C14-1104,D09-1023,0,0.0382526,"Missing"
C14-1104,J14-2005,0,0.0313161,"Missing"
C14-1104,N04-1014,0,0.0296779,"omic manipulating units. Large-scale experiments on Chinese to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the struc"
C14-1104,2006.amta-papers.8,0,0.028994,"nslation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspondence between two languages. In contrast, the analysis"
C14-1104,N03-1017,0,0.0760436,"traint, resorting to dependency structures with dependency edges as atomic manipulating units. Large-scale experiments on Chinese to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some"
C14-1104,W02-1610,0,0.105786,"Missing"
C14-1104,C04-1090,0,0.0308064,"ur model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspondence between two languages. In contrast, the analysis-transfer-g"
C14-1104,P06-1077,1,0.84498,"periments on Chinese to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspondence between two"
C14-1104,W02-1018,0,0.0298914,"k the synchronous constraint, resorting to dependency structures with dependency edges as atomic manipulating units. Large-scale experiments on Chinese to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin"
C14-1104,D13-1108,1,0.743092,"Missing"
C14-1104,P08-1023,1,0.818582,"se to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspondence between two languages. In co"
C14-1104,P02-1038,0,0.10659,"for the loss of word information. The single node translations of the generalized words are also extracted. The unaligned words of the target side is handled by extending nsp(head) and tsp(dependent) on both left and right directions. We do this process similar with the method of Och and Ney (2004). We might obtain m(m ≥ 1) extended rules from an acceptable edge. The frequency of each rule is divided by m. We take the extracted rule set as observed data and make use of relative frequency estimator to obtain the translation probabilities P (t|s) and P (s|t). 4 Decoding and Generation We follow Och and Ney (2002), using a general log-linear model to score the sentence generated by each concatenation of the target edges. Let c be concatenations that concatenate the target edges to generate the target sentence e. The probability of e is defined as： P (c) ∝ ∏ ϕi (c)λi (1) i where ϕi (c) are features defined on concatenations and λi are feature weights. In our experiments of this paper, thirteen features are used as follows: • Transfer rules translation probabilities P (t|s) and P (s|t), and lexical translation probabilities Plex (t|s) and Plex (s|t); • Bilingual phrases probabilities Pbp (t|s) and Pbp (s"
C14-1104,J03-1002,0,0.00640586,"tion limit to our model. We take open source phrase-based system Moses (with default configuration)1 as our baseline system. 5.1 Experimental Setting Our training corpus consists of 1.25M sentence pairs from LDC data, including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. To obtain the dependency trees of the source side, we parse the source sentences with Stanford Parser (Klein and Manning, 2003) into projective dependency structures with nodes annotated by POS tags and edges by dependency labels. To obtain the word alignments, we run GIZA++ (Och and Ney, 2003) on the corpus in both directions and apply “grow-diag-and” refinement (Koehn et al., 2003). We extract the phrases covering no more than 10 nodes of the fixed structures. We use SRILM (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We use NIST MT Evaluation test set 2002 as our development set, 2003-2005 NIST datasets as testsets. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric2 . We make use of the minimum error rate training algorithm (Och, 2003) in order to maximize the BLE"
C14-1104,J04-4002,0,0.115754,"“f¯ab`u” modifying any noun, respectively. The generalized rules are also called 1107 *:VV nsubj Generalize head àobāmǎ * fābù t cen adja nsubj non obama ĸ àobāmǎ issue non fābù nsubj ent ac -adj Generalize dependent obama ķ *:NN t issue en adjac non* Ĺ Figure 4: Generalization of transfer rule. un-lexicalized rules for the loss of word information. The single node translations of the generalized words are also extracted. The unaligned words of the target side is handled by extending nsp(head) and tsp(dependent) on both left and right directions. We do this process similar with the method of Och and Ney (2004). We might obtain m(m ≥ 1) extended rules from an acceptable edge. The frequency of each rule is divided by m. We take the extracted rule set as observed data and make use of relative frequency estimator to obtain the translation probabilities P (t|s) and P (s|t). 4 Decoding and Generation We follow Och and Ney (2002), using a general log-linear model to score the sentence generated by each concatenation of the target edges. Let c be concatenations that concatenate the target edges to generate the target sentence e. The probability of e is defined as： P (c) ∝ ∏ ϕi (c)λi (1) i where ϕi (c) are"
C14-1104,P03-1021,0,0.00786932,"ents, we run GIZA++ (Och and Ney, 2003) on the corpus in both directions and apply “grow-diag-and” refinement (Koehn et al., 2003). We extract the phrases covering no more than 10 nodes of the fixed structures. We use SRILM (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We use NIST MT Evaluation test set 2002 as our development set, 2003-2005 NIST datasets as testsets. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric2 . We make use of the minimum error rate training algorithm (Och, 2003) in order to maximize the BLEU score of the development set. The statistical significance test is performed by sign-test (Collins et al., 2005). 5.2 Influence of Maximum Distortion Limit Figure 6 gives the performance of our system with different maximum distortion limits in terms of uncased BLEU of three NIST test sets. The performance of different distortion limit are consistent on both development set and three test sets. Maximum distortion limit 2 gets the best performances. A low distortion limit may cause the target sentence been translated more close to the sequence of the source, espec"
C14-1104,P05-1034,0,0.665237,"performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspondence between two languages. In contrast, the analysis-transfer-generation methodology in rule-based transla"
C14-1104,2001.mtsummit-papers.53,0,0.114607,"Missing"
C14-1104,P08-1066,0,0.175114,"ificantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspondence between two languages. In contrast, the analysis-transfer-generation methodology in rule-based translation solves the mac"
C14-1104,D11-1020,1,0.945069,"ming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspondence between two languages. In contrast, the analysis-transfer-generation methodology in rule-based translation solves the machine translation p"
C14-1104,W07-0706,1,0.820474,"el and some of those previous works acquired transfer rules automatically from word-aligned corpus (Richardson et al., 2001; Carbonell et al., 2002; Lavoie et al., 2002; Lin, 2004). Gimpel and Smith (2009) and Gimpel and Smith (2014) used quasi-synchronous dependency grammar for MT and they are similar to our idea of doing transfer of dependency syntax in a non-synchronous setting. They do the translation as monolingual lattice parsing. As dependency-based system, Lin (2004) used path as the transfer unit and regarded the translation problem with minimal path covering. Quirk et al. (2005) and Xiong et al. (2007) used treelets to model the source dependency tree using synchronous grammars. Quirk et al. (2005) projected the source dependency structure into target side by word alignment and faced the problem of non-isomorphism between languages. Xiong et al. (2007) directly modeled the treelet to the corresponding target string to alleviate the problem. Xie et al. (2011) directly specified the ordering information in head-dependents rules that represent the source side as head-dependents relations and the target side as string. Differently, our model uses a much simpler elementary structure, edge, which"
C14-1104,P01-1067,0,0.164083,"th dependency edges as atomic manipulating units. Large-scale experiments on Chinese to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which"
C14-1104,carbonell-etal-2002-automatic,0,\N,Missing
C14-1107,P81-1022,0,0.553007,"meeting S 4 : Bush held a with held Bush meeting Sharon S 8 : Bush held a meeting with meeting sh with Sharon S 9 : Bush held a meeting with Sharon rey held with S 10 : Bush held a meeting with Sharon Bush meeting Sharon rey S 11 : Bush held a meeting with Sharon held Bush meeting with Figure 1: Linear-time left-to-right dependency parsing. A shift-reduce parser performs a left-to-right scan of the input sentence, and at each parsing step, chooses one of two parsing actions: either shift (sh) the current word onto the stack, or reduce (re) the top two (or more) items at the end of the stack (Aho and Ullman, 1972). In the dependency parsing scenario, the reduce action is further divided into two cases: left-reduce (rex ) and right-reduce (rey ), depending on which one of the two items becomes the head after reduction. Each parsing derivation can be represented by a sequence of parsing actions. 1134 2.1 Shift-reduce Dependency Parsing We will use the following sentence as the running example: Bush held a meeting with Sharon Given an input sentence e, where ei is the ith token, ei ...e j is the substring of e from i to j, a shift-reduce parser searches for a dependency tree with a sequence of shift-reduc"
C14-1107,2003.mtsummit-papers.6,0,0.067579,"rget syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). However, tree-to-tree approaches still significantly under-perform than tree-to-string systems due to the poor rule coverage (Liu et al., 2009) and bi-parsing failures (Liu et al., 2009; Mi and Liu, 2010). Another potential solution is to use structured language models (Slm) (Chelba and Jelinek, 2000; Charniak et al., 2003; Post and Gildea, 2008; Post and Gildea, 2009), as the monolingual Slm has achieved better perplexity than the traditional n-gram word sequence model. More importantly, the Slm is independent of any translation model. Thus, integrating a Slm into a tree-to-string model will not face the problems that tree-to-tree models have. However, integration is not easy, as the following two questions arise. First, the search space grows significantly, as a partial translation has a lot of syntax structures. Second, hypotheses in the same bin may not be comparable, since their syntactic structures may no"
C14-1107,P04-1015,0,0.0412563,"window, consisting of current partial tree and first word to be processed. All Feature functions are listed in Table 1(b), which is a conjunction of atomic 1135 IP NP VP VP PP B`ush´ı NP P VV yˇu Sh¯al´ong jˇux´ıng AS NP le hu`ıt´an Figure 2: A parse tree features in Table 1(a). To decide which action is the best of the current structure, we perform a three-way classification based on f, and conjoin these feature instances with each action: [f ◦ (action=sh /rex /rey )] We extract all the feature templates from training data, and use the average perceptron algorithm and early-update strategy (Collins and Roark, 2004; Huang et al., 2012) to train the model. 3 Incremental Tree-to-string Translation with Slm The incremental tree-to-string decoding (Huang and Mi, 2010) performs translation in two separate steps: parsing and decoding. A parser first parses the source language input into a 1-best tree in Figure 2, and the linear incremental decoder then searches for the best derivation that generates a target-language string in strictly left-to-right manner. Figure 3 works out the full running example, and we describe it in the following section. 3.1 Decoding with Slm Since the incremental tree-to-string model"
C14-1107,P09-1087,0,0.0145008,"ontributes more, the larger beam size or the Slm. In contrast, our approach gains significant improvements over a state-of-theart tree-to-string baseline at a reasonable speed, about 6 times slower. And we answer some questions beyond their work. Hassan et al. (2009) incorporate a linear-time CCG parser into a DTM system, and achieve a significant improvement. Different from their work, we pay more attention to the dependency parser, and we also test this approach in our experiments. As they only keep 1-best parsing states during the decoding, they are suffering from the local parsing errors. Galley and Manning (2009) adapt the maximum spanning tree (MST) parser of McDonald et al. (2005) to an incremental dependency parsing, and incorporate it into a phrase-based system. But this incremental parser remains in quadratic time. Besides, there are also some other efforts that are less closely related to ours. Shen et al. (2008) and Mi and Liu (2010) develop a generative dependency language model for string-to-dependency and tree-to-tree models. But they need parse the target side first, and encode target syntactic structures in translation rules. Both papers integrate dependency structures into translation mod"
C14-1107,N04-1035,0,0.0601723,"parser, w s is the weight of Slm(e), fi are the features in the baseline model and wi are the weights. 1139 4 Experiments 4.1 Data Preparation The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. We use the NIST evaluation sets of MT06 as our development set, and MT03, 04, 05, and 08 (newswire portion) as our test sets. We word-aligned the training data using GIZA++ with refinement option “grow-diag-and” (Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). we applied the algorithm of Galley et al. (2004) to extract tree-to-string translation rules. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the same feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-rate training (Och, 2003) to maxim"
C14-1107,P06-1121,0,0.0289536,"to-right shift-reduce parser in specific, into an incremental tree-to-string model, and introduce an efficient grouping and pruning mechanism for this integration. Large-scale experiments on various Chinese-English test sets show that with a reasonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a state-of-the-art tree-to-string system. 1 Introduction Tree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gained significant popularity in recent years, as they run faster than string-to-tree counterparts (e.g. (Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). Ho"
C14-1107,D09-1123,0,0.0497972,"Missing"
C14-1107,D10-1027,1,0.850995,"tring model, and introduce an efficient grouping and pruning mechanism for this integration. Large-scale experiments on various Chinese-English test sets show that with a reasonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a state-of-the-art tree-to-string system. 1 Introduction Tree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gained significant popularity in recent years, as they run faster than string-to-tree counterparts (e.g. (Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). However, tree-to-tree approaches still significantly under-perform than"
C14-1107,P10-1110,1,0.918487,"ad. (a) (b) unigram bigram trigram feature templates s0 .w s0 .t s1 .w s1 .t q0 .w q0 .t s0 .w ◦ s1 .w s0 .t ◦ q0 .t s0 .w ◦ s1 .w ◦ s1 .t s0 .w ◦ s0 .t ◦ s1 .w s0 .t ◦ s1 .t ◦ q0 .t s1 .t ◦ s0 .t ◦ q0 .t (c) ... atomic features s0 .w s0 .t s1 .w s1 .t s0 .lc.t s0 .rc.t q0 .w q0 .t s1 s0 .w ◦ s0 .t s1 .w ◦ s1 .t q0 .w ◦ q0 .t s0 .t ◦ s1 .t s0 .w ◦ s0 .t ◦ s1 .t s0 .t ◦ s1 .w ◦ s1 .t s1 .t ◦ s0 .t ◦ s0 .lc.t s1 .t ◦ s0 .t ◦ s0 .rc.t ←− parsing stack s0 s0 .lc · · · parsing queue −→ q0 s0 .rc Table 1: (a) atomic features, used for parsing signatures. (b): parsing feature templates, adapted from Huang and Sagae (2010). x.w and x.t denotes the root word and POS tag of the partial dependency tree, x.lc and x.rc denote x’s leftmost and rightmost child respectively. (c) the feature window. 2.2 Features We view features as “abstractions” or (partial) observations of the current structure. Feature templates f are functions that draw information from the feature window, consisting of current partial tree and first word to be processed. All Feature functions are listed in Table 1(b), which is a conjunction of atomic 1135 IP NP VP VP PP B`ush´ı NP P VV yˇu Sh¯al´ong jˇux´ıng AS NP le hu`ıt´an Figure 2: A parse tree"
C14-1107,2006.amta-papers.8,1,0.780582,"he target syntax to guarantee the grammaticality of the output. Instead of using complex tree-to-tree models, we integrate a structured language model, a left-to-right shift-reduce parser in specific, into an incremental tree-to-string model, and introduce an efficient grouping and pruning mechanism for this integration. Large-scale experiments on various Chinese-English test sets show that with a reasonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a state-of-the-art tree-to-string system. 1 Introduction Tree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gained significant popularity in recent years, as they run faster than string-to-tree counterparts (e.g. (Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approac"
C14-1107,N12-1015,1,0.848658,"rrent partial tree and first word to be processed. All Feature functions are listed in Table 1(b), which is a conjunction of atomic 1135 IP NP VP VP PP B`ush´ı NP P VV yˇu Sh¯al´ong jˇux´ıng AS NP le hu`ıt´an Figure 2: A parse tree features in Table 1(a). To decide which action is the best of the current structure, we perform a three-way classification based on f, and conjoin these feature instances with each action: [f ◦ (action=sh /rex /rey )] We extract all the feature templates from training data, and use the average perceptron algorithm and early-update strategy (Collins and Roark, 2004; Huang et al., 2012) to train the model. 3 Incremental Tree-to-string Translation with Slm The incremental tree-to-string decoding (Huang and Mi, 2010) performs translation in two separate steps: parsing and decoding. A parser first parses the source language input into a 1-best tree in Figure 2, and the linear incremental decoder then searches for the best derivation that generates a target-language string in strictly left-to-right manner. Figure 3 works out the full running example, and we describe it in the following section. 3.1 Decoding with Slm Since the incremental tree-to-string model generates translatio"
C14-1107,N03-1017,0,0.05875,"he following equation: X fi · wi ) (1) e∗ = argmax exp(Slm(e) · w s + e∈E i where Slm(e) is the dependency parsing score calculated by our parser, w s is the weight of Slm(e), fi are the features in the baseline model and wi are the weights. 1139 4 Experiments 4.1 Data Preparation The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. We use the NIST evaluation sets of MT06 as our development set, and MT03, 04, 05, and 08 (newswire portion) as our test sets. We word-aligned the training data using GIZA++ with refinement option “grow-diag-and” (Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). we applied the algorithm of Galley et al. (2004) to extract tree-to-string translation rules. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010"
C14-1107,W04-3250,0,0.0210447,"m baseline +Slm MT03 Bleu (T-B)/2 19.94 10.73 21.49 9.44 MT04 Bleu (T-B)/2 22.03 18.63 22.33 18.38 MT05 Bleu (T-B)/2 19.92 11.45 20.51 10.71 MT08 Bleu (T-B)/2 21.06 10.37 21.64 9.88 Avg. (T-B)/2 12.80 12.10 Table 3: Results on all test sets. Bold scores are significantly better than the baseline system (p &lt; 0.5). 4.3 Final Results on All Test Sets Table 3 shows our main results on all test sets. Our method gains an average improvement of 0.7 points in terms of (T-B)/2. Results on NIST MT 03, 05, and 08 are statistically significant with p &lt; 0.05, using bootstrap re-sampling with 1000 samples (Koehn, 2004). The average decoding speed is about 10 times slower than the baseline. 5 Related Work The work of Schwartz et al. (2011) is similar in spirit to ours. We are different in the following ways. First, they integrate an Slm into a phrase-based system (Koehn et al., 2003), we pay more attention to a syntax-based system. Second, their approach slowdowns the speed at near 2000 times, thus, they can only tune their system on short sentences less than 20 words. Furthermore, their results are from a much bigger beam (10 times larger than their baseline), so it is not clear which factor contributes mor"
C14-1107,P06-1077,1,0.793255,"but they lack in the target syntax to guarantee the grammaticality of the output. Instead of using complex tree-to-tree models, we integrate a structured language model, a left-to-right shift-reduce parser in specific, into an incremental tree-to-string model, and introduce an efficient grouping and pruning mechanism for this integration. Large-scale experiments on various Chinese-English test sets show that with a reasonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a state-of-the-art tree-to-string system. 1 Introduction Tree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gained significant popularity in recent years, as they run faster than string-to-tree counterparts (e.g. (Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target sid"
C14-1107,P09-1063,1,0.891121,"Missing"
C14-1107,H05-1066,0,0.0363915,"h gains significant improvements over a state-of-theart tree-to-string baseline at a reasonable speed, about 6 times slower. And we answer some questions beyond their work. Hassan et al. (2009) incorporate a linear-time CCG parser into a DTM system, and achieve a significant improvement. Different from their work, we pay more attention to the dependency parser, and we also test this approach in our experiments. As they only keep 1-best parsing states during the decoding, they are suffering from the local parsing errors. Galley and Manning (2009) adapt the maximum spanning tree (MST) parser of McDonald et al. (2005) to an incremental dependency parsing, and incorporate it into a phrase-based system. But this incremental parser remains in quadratic time. Besides, there are also some other efforts that are less closely related to ours. Shen et al. (2008) and Mi and Liu (2010) develop a generative dependency language model for string-to-dependency and tree-to-tree models. But they need parse the target side first, and encode target syntactic structures in translation rules. Both papers integrate dependency structures into translation model, we instead model the dependency structures with a monolingual parsi"
C14-1107,P10-1145,1,0.907751,"(Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). However, tree-to-tree approaches still significantly under-perform than tree-to-string systems due to the poor rule coverage (Liu et al., 2009) and bi-parsing failures (Liu et al., 2009; Mi and Liu, 2010). Another potential solution is to use structured language models (Slm) (Chelba and Jelinek, 2000; Charniak et al., 2003; Post and Gildea, 2008; Post and Gildea, 2009), as the monolingual Slm has achieved better perplexity than the traditional n-gram word sequence model. More importantly, the Slm is independent of any translation model. Thus, integrating a Slm into a tree-to-string model wi"
C14-1107,P08-1023,1,0.824443,"rd-aligned the training data using GIZA++ with refinement option “grow-diag-and” (Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). we applied the algorithm of Galley et al. (2004) to extract tree-to-string translation rules. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the same feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development set. Our dependency parser is an implementation of the “arc-standard” shift-reduce parser (Nivre, 2004), and it is trained on the standard split of English Penn Tree-bank (PTB): Sections 02-21 as the training set, Section 22 as the held-out set, and Section 23 as the test set. Using the same features as Huang and Sagae (2010), our depe"
C14-1107,W04-0308,0,0.0529218,"us using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the same feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development set. Our dependency parser is an implementation of the “arc-standard” shift-reduce parser (Nivre, 2004), and it is trained on the standard split of English Penn Tree-bank (PTB): Sections 02-21 as the training set, Section 22 as the held-out set, and Section 23 as the test set. Using the same features as Huang and Sagae (2010), our dependency parser achieves a similar performance as Huang and Sagae (2010). We add the structured language model as an additional feature into the baseline system. We evaluate translation quality using case-insensitive IBM Bleu-4, calculated by the script mteval-v13a.pl. We also report the Ter scores. 4.2 Complete Comparisons on MT08 To explore the soundness of our ap"
C14-1107,P03-1021,0,0.0375581,"alley et al. (2004) to extract tree-to-string translation rules. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the same feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development set. Our dependency parser is an implementation of the “arc-standard” shift-reduce parser (Nivre, 2004), and it is trained on the standard split of English Penn Tree-bank (PTB): Sections 02-21 as the training set, Section 22 as the held-out set, and Section 23 as the test set. Using the same features as Huang and Sagae (2010), our dependency parser achieves a similar performance as Huang and Sagae (2010). We add the structured language model as an additional feature into the baseline system. We evaluate translation quality using case-insensitive I"
C14-1107,N07-1051,0,0.0353772,"e) is the dependency parsing score calculated by our parser, w s is the weight of Slm(e), fi are the features in the baseline model and wi are the weights. 1139 4 Experiments 4.1 Data Preparation The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. We use the NIST evaluation sets of MT06 as our development set, and MT03, 04, 05, and 08 (newswire portion) as our test sets. We word-aligned the training data using GIZA++ with refinement option “grow-diag-and” (Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). we applied the algorithm of Galley et al. (2004) to extract tree-to-string translation rules. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the same feature set shown in Huang and Mi (2010), and tune all the weights usin"
C14-1107,P05-1034,0,0.0501425,"than string-to-tree counterparts (e.g. (Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). However, tree-to-tree approaches still significantly under-perform than tree-to-string systems due to the poor rule coverage (Liu et al., 2009) and bi-parsing failures (Liu et al., 2009; Mi and Liu, 2010). Another potential solution is to use structured language models (Slm) (Chelba and Jelinek, 2000; Charniak et al., 2003; Post and Gildea, 2008; Post and Gildea, 2009), as the monolingual Slm has achieved better perplexity than the traditional n-gram word sequence model. More importantly, the Slm is independent of any translation model. Thus, integrating"
C14-1107,P11-1063,0,0.0169543,"B)/2 19.92 11.45 20.51 10.71 MT08 Bleu (T-B)/2 21.06 10.37 21.64 9.88 Avg. (T-B)/2 12.80 12.10 Table 3: Results on all test sets. Bold scores are significantly better than the baseline system (p &lt; 0.5). 4.3 Final Results on All Test Sets Table 3 shows our main results on all test sets. Our method gains an average improvement of 0.7 points in terms of (T-B)/2. Results on NIST MT 03, 05, and 08 are statistically significant with p &lt; 0.05, using bootstrap re-sampling with 1000 samples (Koehn, 2004). The average decoding speed is about 10 times slower than the baseline. 5 Related Work The work of Schwartz et al. (2011) is similar in spirit to ours. We are different in the following ways. First, they integrate an Slm into a phrase-based system (Koehn et al., 2003), we pay more attention to a syntax-based system. Second, their approach slowdowns the speed at near 2000 times, thus, they can only tune their system on short sentences less than 20 words. Furthermore, their results are from a much bigger beam (10 times larger than their baseline), so it is not clear which factor contributes more, the larger beam size or the Slm. In contrast, our approach gains significant improvements over a state-of-theart tree-t"
C14-1107,P08-1066,0,0.101418,"tem, and achieve a significant improvement. Different from their work, we pay more attention to the dependency parser, and we also test this approach in our experiments. As they only keep 1-best parsing states during the decoding, they are suffering from the local parsing errors. Galley and Manning (2009) adapt the maximum spanning tree (MST) parser of McDonald et al. (2005) to an incremental dependency parsing, and incorporate it into a phrase-based system. But this incremental parser remains in quadratic time. Besides, there are also some other efforts that are less closely related to ours. Shen et al. (2008) and Mi and Liu (2010) develop a generative dependency language model for string-to-dependency and tree-to-tree models. But they need parse the target side first, and encode target syntactic structures in translation rules. Both papers integrate dependency structures into translation model, we instead model the dependency structures with a monolingual parsing model over translation strings. 6 Conclusion In this paper, we presented an efficient algorithm to integrate a structured language model (an incremental shift-reduce parser in specific) into an incremental tree-to-string system. We calcul"
C14-1193,W11-2105,0,0.0149825,"nce level on WMT 2012 and WMT 2013. 1 Introduction Automatic machine translation (MT) evaluation plays an important role in the evolution of MT. It not only evaluates the performance of MT systems, but also makes the development of MT systems rapider (Och, 2003). According to the type of the employed information, the automatic MT evaluation metrics can be classified into three categories: lexicon-based metrics, syntax-based metrics and semantic-based metrics. The lexicon-based metrics, such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Lavie and Agarwal, 2007) and AMBER (Chen and Kuhn, 2011; Chen et al., 2012), are good at capturing the lexicon or phrase level information, e.g. fixed phrases or idioms. But they cannot adequately reflect the syntax similarity. Current efforts in syntax-based metrics, such as the headword chain based metric (HWCM) (Liu and Gildea, 2005), the LFG dependency tree based metric (Owczarzak et al., 2007) and syntactic/semantic-role overlap (Gim´enez and M`arquez, 2007) , suffer from the parsing of the potentially noisy machine translations, so the improvement of their performance is restricted due to the serious parsing errors. Semantic-based metrics, s"
C14-1193,W12-3104,0,0.0146484,"and WMT 2013. 1 Introduction Automatic machine translation (MT) evaluation plays an important role in the evolution of MT. It not only evaluates the performance of MT systems, but also makes the development of MT systems rapider (Och, 2003). According to the type of the employed information, the automatic MT evaluation metrics can be classified into three categories: lexicon-based metrics, syntax-based metrics and semantic-based metrics. The lexicon-based metrics, such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Lavie and Agarwal, 2007) and AMBER (Chen and Kuhn, 2011; Chen et al., 2012), are good at capturing the lexicon or phrase level information, e.g. fixed phrases or idioms. But they cannot adequately reflect the syntax similarity. Current efforts in syntax-based metrics, such as the headword chain based metric (HWCM) (Liu and Gildea, 2005), the LFG dependency tree based metric (Owczarzak et al., 2007) and syntactic/semantic-role overlap (Gim´enez and M`arquez, 2007) , suffer from the parsing of the potentially noisy machine translations, so the improvement of their performance is restricted due to the serious parsing errors. Semantic-based metrics, such as MEANT (Lo et"
C14-1193,W11-2107,0,0.052937,"resources to expand the reference information. We also introduce some extra resources to RED, such as stem, synonym and paraphrase. The words within a sentence can be classified into content words and function words. The effects of the two kinds of words are different and they shouldn’t have the same matching score, so we introduce a parameter to distinguish them. The methods of applying these resources are introduced as follows. • Stem and Synonym Stem(Porter, 2001) and synonym (WordNet1 ) are introduced to RED in the following three steps. First, we obtain the alignment with Meteor Aligner (Denkowski and Lavie, 2011) in which not only exact match but also stem and synonym are considered. We use stem and synonym together with exact match as three match modules. Second, the alignment is used to match for a dep-ngram. We think the dep-ngram can match with the translation if the following conditions are satisfied. 1) Each of the words in the dep-ngram has a matched word in the translation according to the alignment; 2) The words in dep-ngram and the matched words in translation appear in the same order; 3) The matched words in translation must be continuous if the dep-ngram is a fixed-floating ngram. At last,"
C14-1193,W07-0738,0,0.0769068,"Missing"
C14-1193,W07-0734,0,0.15872,"d is comparable with METEOR on sentence level on WMT 2012 and WMT 2013. 1 Introduction Automatic machine translation (MT) evaluation plays an important role in the evolution of MT. It not only evaluates the performance of MT systems, but also makes the development of MT systems rapider (Och, 2003). According to the type of the employed information, the automatic MT evaluation metrics can be classified into three categories: lexicon-based metrics, syntax-based metrics and semantic-based metrics. The lexicon-based metrics, such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Lavie and Agarwal, 2007) and AMBER (Chen and Kuhn, 2011; Chen et al., 2012), are good at capturing the lexicon or phrase level information, e.g. fixed phrases or idioms. But they cannot adequately reflect the syntax similarity. Current efforts in syntax-based metrics, such as the headword chain based metric (HWCM) (Liu and Gildea, 2005), the LFG dependency tree based metric (Owczarzak et al., 2007) and syntactic/semantic-role overlap (Gim´enez and M`arquez, 2007) , suffer from the parsing of the potentially noisy machine translations, so the improvement of their performance is restricted due to the serious parsing er"
C14-1193,W05-0904,0,0.547747,"type of the employed information, the automatic MT evaluation metrics can be classified into three categories: lexicon-based metrics, syntax-based metrics and semantic-based metrics. The lexicon-based metrics, such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Lavie and Agarwal, 2007) and AMBER (Chen and Kuhn, 2011; Chen et al., 2012), are good at capturing the lexicon or phrase level information, e.g. fixed phrases or idioms. But they cannot adequately reflect the syntax similarity. Current efforts in syntax-based metrics, such as the headword chain based metric (HWCM) (Liu and Gildea, 2005), the LFG dependency tree based metric (Owczarzak et al., 2007) and syntactic/semantic-role overlap (Gim´enez and M`arquez, 2007) , suffer from the parsing of the potentially noisy machine translations, so the improvement of their performance is restricted due to the serious parsing errors. Semantic-based metrics, such as MEANT (Lo et al., 2012; Lo and Wu, 2013), have the similar problem that the accuracy of semantic role labeling (SRL) can also drop due to the errors in translations. To avoid the parsing of potentially noisy translations, the CCG based metric (Mehay and Brew, 2007) only uses"
C14-1193,W13-2254,0,0.0126479,"at capturing the lexicon or phrase level information, e.g. fixed phrases or idioms. But they cannot adequately reflect the syntax similarity. Current efforts in syntax-based metrics, such as the headword chain based metric (HWCM) (Liu and Gildea, 2005), the LFG dependency tree based metric (Owczarzak et al., 2007) and syntactic/semantic-role overlap (Gim´enez and M`arquez, 2007) , suffer from the parsing of the potentially noisy machine translations, so the improvement of their performance is restricted due to the serious parsing errors. Semantic-based metrics, such as MEANT (Lo et al., 2012; Lo and Wu, 2013), have the similar problem that the accuracy of semantic role labeling (SRL) can also drop due to the errors in translations. To avoid the parsing of potentially noisy translations, the CCG based metric (Mehay and Brew, 2007) only uses the parsing result of reference and employs 2-gram dependents, but it did not achieve the state-of-the-art performance. In this paper, we propose a novel dependency tree based MT evaluation metric. The new metric only employs the reference dependency tree, leaving the translation unparsed to avoid the error propagation. We use two kinds of reference dependency s"
C14-1193,W12-3129,0,0.0586829,"2012), are good at capturing the lexicon or phrase level information, e.g. fixed phrases or idioms. But they cannot adequately reflect the syntax similarity. Current efforts in syntax-based metrics, such as the headword chain based metric (HWCM) (Liu and Gildea, 2005), the LFG dependency tree based metric (Owczarzak et al., 2007) and syntactic/semantic-role overlap (Gim´enez and M`arquez, 2007) , suffer from the parsing of the potentially noisy machine translations, so the improvement of their performance is restricted due to the serious parsing errors. Semantic-based metrics, such as MEANT (Lo et al., 2012; Lo and Wu, 2013), have the similar problem that the accuracy of semantic role labeling (SRL) can also drop due to the errors in translations. To avoid the parsing of potentially noisy translations, the CCG based metric (Mehay and Brew, 2007) only uses the parsing result of reference and employs 2-gram dependents, but it did not achieve the state-of-the-art performance. In this paper, we propose a novel dependency tree based MT evaluation metric. The new metric only employs the reference dependency tree, leaving the translation unparsed to avoid the error propagation. We use two kinds of refe"
C14-1193,W11-2108,0,0.0163079,"T 2012 and WMT 2013. We parsed the reference into constituent tree by Berkeley parser2 and then converted the constituent tree into dependency tree by Penn2Malt3 . Presumably, the performance of the new metric will be better if the dependency trees are labeled by human. Reference dependency trees are labeled only once and can be used forever so it will not increase costs. 5.2 Baselines In the experiments, we compare the performance of our metric with the widely-used lexicon-based metrics such as BLEU4 , TER5 and METEOR6 , dependency-based metric HWCM and semantic-based metric SEMPOS (Mach´acˇ ek and Bojar, 2011) which has the best performance on system level according to the published results of WMT 2012. The results of BLEU are obtained using 4-gram with smoothing option. The version of TER is 0.7.25. The results of METEOR are obtained by Version 1.4 with task option ‘rank’. We re-implement HWCM which employs an epsilon value of 10−3 to replace zero for smoothing purpose. The correlations of SEMPOS are obtained from the published results of WMT 2012 and WMT 2013. 5.3 Experiment Results The experiments on both system level and sentence level are carried out. On system level, the correlations are calc"
C14-1193,2007.tmi-papers.15,0,0.234481,"ic (HWCM) (Liu and Gildea, 2005), the LFG dependency tree based metric (Owczarzak et al., 2007) and syntactic/semantic-role overlap (Gim´enez and M`arquez, 2007) , suffer from the parsing of the potentially noisy machine translations, so the improvement of their performance is restricted due to the serious parsing errors. Semantic-based metrics, such as MEANT (Lo et al., 2012; Lo and Wu, 2013), have the similar problem that the accuracy of semantic role labeling (SRL) can also drop due to the errors in translations. To avoid the parsing of potentially noisy translations, the CCG based metric (Mehay and Brew, 2007) only uses the parsing result of reference and employs 2-gram dependents, but it did not achieve the state-of-the-art performance. In this paper, we propose a novel dependency tree based MT evaluation metric. The new metric only employs the reference dependency tree, leaving the translation unparsed to avoid the error propagation. We use two kinds of reference dependency structures in our metric. One is the headword chain (Liu and Gildea, 2005) which can capture long distance dependency information. The other is fixed and floating structure (Shen et al., 2010) which can capture local continuou"
C14-1193,P03-1021,0,0.0177625,"riment results show that our metric achieves higher correlations with human judgments than BLEU, TER and HWCM on WMT 2012 and WMT 2013. By introducing extra linguistic resources and tuning parameters, the new metric gets the state-of-the-art performance which is better than METEOR and SEMPOS on system level, and is comparable with METEOR on sentence level on WMT 2012 and WMT 2013. 1 Introduction Automatic machine translation (MT) evaluation plays an important role in the evolution of MT. It not only evaluates the performance of MT systems, but also makes the development of MT systems rapider (Och, 2003). According to the type of the employed information, the automatic MT evaluation metrics can be classified into three categories: lexicon-based metrics, syntax-based metrics and semantic-based metrics. The lexicon-based metrics, such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Lavie and Agarwal, 2007) and AMBER (Chen and Kuhn, 2011; Chen et al., 2012), are good at capturing the lexicon or phrase level information, e.g. fixed phrases or idioms. But they cannot adequately reflect the syntax similarity. Current efforts in syntax-based metrics, such as the headword chain ba"
C14-1193,W07-0411,0,0.111412,"Missing"
C14-1193,P02-1040,0,0.0900579,"which is better than METEOR and SEMPOS on system level, and is comparable with METEOR on sentence level on WMT 2012 and WMT 2013. 1 Introduction Automatic machine translation (MT) evaluation plays an important role in the evolution of MT. It not only evaluates the performance of MT systems, but also makes the development of MT systems rapider (Och, 2003). According to the type of the employed information, the automatic MT evaluation metrics can be classified into three categories: lexicon-based metrics, syntax-based metrics and semantic-based metrics. The lexicon-based metrics, such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Lavie and Agarwal, 2007) and AMBER (Chen and Kuhn, 2011; Chen et al., 2012), are good at capturing the lexicon or phrase level information, e.g. fixed phrases or idioms. But they cannot adequately reflect the syntax similarity. Current efforts in syntax-based metrics, such as the headword chain based metric (HWCM) (Liu and Gildea, 2005), the LFG dependency tree based metric (Owczarzak et al., 2007) and syntactic/semantic-role overlap (Gim´enez and M`arquez, 2007) , suffer from the parsing of the potentially noisy machine translations, so the improvement of"
C14-1193,J10-4005,0,0.0607183,"lations, the CCG based metric (Mehay and Brew, 2007) only uses the parsing result of reference and employs 2-gram dependents, but it did not achieve the state-of-the-art performance. In this paper, we propose a novel dependency tree based MT evaluation metric. The new metric only employs the reference dependency tree, leaving the translation unparsed to avoid the error propagation. We use two kinds of reference dependency structures in our metric. One is the headword chain (Liu and Gildea, 2005) which can capture long distance dependency information. The other is fixed and floating structure (Shen et al., 2010) which can capture local continuous ngram. When calculating the matching score between the headword chain and the translation, we use a distance-based similarity. Experiment This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2042 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2042–2051, Dublin, Ireland, August 23-29 2014. results show that our metric achieves higher c"
C14-1193,2006.amta-papers.25,0,0.101955,"and SEMPOS on system level, and is comparable with METEOR on sentence level on WMT 2012 and WMT 2013. 1 Introduction Automatic machine translation (MT) evaluation plays an important role in the evolution of MT. It not only evaluates the performance of MT systems, but also makes the development of MT systems rapider (Och, 2003). According to the type of the employed information, the automatic MT evaluation metrics can be classified into three categories: lexicon-based metrics, syntax-based metrics and semantic-based metrics. The lexicon-based metrics, such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Lavie and Agarwal, 2007) and AMBER (Chen and Kuhn, 2011; Chen et al., 2012), are good at capturing the lexicon or phrase level information, e.g. fixed phrases or idioms. But they cannot adequately reflect the syntax similarity. Current efforts in syntax-based metrics, such as the headword chain based metric (HWCM) (Liu and Gildea, 2005), the LFG dependency tree based metric (Owczarzak et al., 2007) and syntactic/semantic-role overlap (Gim´enez and M`arquez, 2007) , suffer from the parsing of the potentially noisy machine translations, so the improvement of their performance is restri"
C14-1193,W09-0441,0,0.0610376,"s with the length of n in the reference. 2045 The final score of RED is achieved using Formula (6), in which a weighted sum of the dep-ngrams’ Fscore is calculated. wngram (0 ≤ wngram ≤ 1) is the weight of dep-ngram with the length of n. Fscoren is the Fscore for the dep-ngrams with the length of n. RED = N X (wngram × F scoren ) (6) n=1 3 Introducing Extra Resources Many automatic evaluation metrics can only find the exact match between the reference and the translation, and the information provided by the limited number of references is not sufficient. Some evaluation metrics, such as TERp (Snover et al., 2009) and METOER, introduce extra resources to expand the reference information. We also introduce some extra resources to RED, such as stem, synonym and paraphrase. The words within a sentence can be classified into content words and function words. The effects of the two kinds of words are different and they shouldn’t have the same matching score, so we introduce a parameter to distinguish them. The methods of applying these resources are introduced as follows. • Stem and Synonym Stem(Porter, 2001) and synonym (WordNet1 ) are introduced to RED in the following three steps. First, we obtain the al"
C14-1209,P05-1067,0,0.513699,"o-English translation show that our augmented dependency-to-string model gains significant improvement of averaged +0.85 BLEU scores on three test sets over the dependencyto-string model. 1 Introduction As a representation holding both syntactic and semantic information, dependency grammar has been attracting more and more attention in statistical machine translation. Lin (2004) took paths as the elementary structures and proposed a path-based transfer model. Quirk et al. (2005) extended path to treelets (connected subgraphs of dependency trees) and put forward dependency treelet translation. Ding and Palmer (2005) proposed a model on the basis of dependency insertion grammar. Shen et al. (2008) employed the fixed and floating structures as elementary structures and proposed a string-to-dependency model with state-of-the-art performance. Xie et al. (2011) employs head-dependents relations as elementary structures and proposed a dependency-to-string model with good long distance reordering property. A head-dependents relation (HDR) is composed of a head and all its dependents, which can be viewed as an instance of a sentence pattern or phrase pattern. However, since dependency trees are much flatter than"
C14-1209,N04-1035,0,0.0648016,"ed a string-to-dependency model with state-of-the-art performance. Xie et al. (2011) employs head-dependents relations as elementary structures and proposed a dependency-to-string model with good long distance reordering property. A head-dependents relation (HDR) is composed of a head and all its dependents, which can be viewed as an instance of a sentence pattern or phrase pattern. However, since dependency trees are much flatter than constituency trees, the dependency-to-string model suffers more severe non-syntactic phrase coverage problem (Meng et al., 2013) than constituencybased models (Galley et al., 2004; Liu et al., 2006; Huang et al., 2006). Non-syntactic phrases are those phrases that can not be covered by whole subtrees. To address this problem, Meng et al. (2013) proposed to translate with both constituency and dependency trees, which can incorporate non-syntactic phrases covered by the constituents of the constituency trees. This model requires both constituency and dependency trees, thus may suffer from both constituency and dependency parse errors. Additionally, there are only few languages that have both constituency and dependency parsers, which limits its practical use. In this pap"
C14-1209,P07-1019,0,0.0515678,"d and floating structures of (a). There will be three combinations of the labeled fixed and floating structures as shown in the middle of Figure 4. For each combination, the decoder builds a new translation rule by turning variable sequences “X2:NT X3:给*” and/or “X4:VV X5:NN” into new variables “X23:NT P*” and/or “X45:VV NN”. And we will obtain three new translation rules (d)-(f) that can incorporate nonsyntactic phrases into translations. If there are no matched rules, the decoder builds a pseudo translation rule with monotonic reordering. The decoder then employs cube pruning (Chiang, 2007; Huang and Chiang, 2007) to generate k-best hypothesis with integrated language model for node n. Repeat the above process till the root of T is accessed. The hypothesis with the highest score is output as translation. 4 Experiments We evaluated our augmented model by comparison with dependency-to-string model and hierarchical phrase-based model on Chinese-to-English translation in terms of BLEU (Papineni et al., 2002). 4.1 Experimental Setup The parallel training corpus include 1.25M Chinese-English sentence pairs.1 We parse the Chinese sentences with Stanford Parser (Klein and Manning, 2003) into projective depende"
C14-1209,2006.amta-papers.8,0,0.0328677,"ate-of-the-art performance. Xie et al. (2011) employs head-dependents relations as elementary structures and proposed a dependency-to-string model with good long distance reordering property. A head-dependents relation (HDR) is composed of a head and all its dependents, which can be viewed as an instance of a sentence pattern or phrase pattern. However, since dependency trees are much flatter than constituency trees, the dependency-to-string model suffers more severe non-syntactic phrase coverage problem (Meng et al., 2013) than constituencybased models (Galley et al., 2004; Liu et al., 2006; Huang et al., 2006). Non-syntactic phrases are those phrases that can not be covered by whole subtrees. To address this problem, Meng et al. (2013) proposed to translate with both constituency and dependency trees, which can incorporate non-syntactic phrases covered by the constituents of the constituency trees. This model requires both constituency and dependency trees, thus may suffer from both constituency and dependency parse errors. Additionally, there are only few languages that have both constituency and dependency parsers, which limits its practical use. In this paper, we propose to address non-syntactic"
C14-1209,N03-1017,0,0.0286771,"e hypothesis with the highest score is output as translation. 4 Experiments We evaluated our augmented model by comparison with dependency-to-string model and hierarchical phrase-based model on Chinese-to-English translation in terms of BLEU (Papineni et al., 2002). 4.1 Experimental Setup The parallel training corpus include 1.25M Chinese-English sentence pairs.1 We parse the Chinese sentences with Stanford Parser (Klein and Manning, 2003) into projective dependency trees, obtain word alignment by running GIZA++ (Och and Ney, 2003) in both directions and applying “grow-diag-final” refinement (Koehn et al., 2003), and train a 4-gram language model by SRI Language Modeling Toolkit (Stolcke, 2002) with Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We take NIST MT Evaluation test set 2002 as our development set, 2003 (MT03), 2004 (MT04) and 2005 (MT05) as our test sets, evaluate the quality of translations by case insensitive NIST BLEU4 metric2 , tune the feature weights by Max-BLEU strategy with MERT (Och, 2003), and check the statistical difference between the systems with significance test (Collins et al., 2005). 4.2 Systems We take “Moses-Chart” of Moses3 (Koehn et al., 2007) as"
C14-1209,P07-2045,0,0.0201326,"nt (Koehn et al., 2003), and train a 4-gram language model by SRI Language Modeling Toolkit (Stolcke, 2002) with Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We take NIST MT Evaluation test set 2002 as our development set, 2003 (MT03), 2004 (MT04) and 2005 (MT05) as our test sets, evaluate the quality of translations by case insensitive NIST BLEU4 metric2 , tune the feature weights by Max-BLEU strategy with MERT (Och, 2003), and check the statistical difference between the systems with significance test (Collins et al., 2005). 4.2 Systems We take “Moses-Chart” of Moses3 (Koehn et al., 2007) as hierarchical phrase-based model baseline. In our experiments, we use the default settings. Both the dependency-to-string baseline and our augmented model employ the same settings as those of Xie et al. (2011), with the beam threshold, beam size and rule size are set to 10−3 , 200 and 100 respectively. And both systems employ bilingual phrases with length ≤ 7 extracted by Moses. 4.3 Experiment results Table 1 shows the results of the BLEU scores of the three systems. Where “dep2str” and “dep2str-aug” denote dependency-to-string model baseline and our augmented dependency-to-string model, re"
C14-1209,C04-1090,0,0.719989,"translation rules that can incorporate non-syntactic phrases into translations, thus alleviate the non-syntactic phrase coverage problem of dependency-to-string translation (Xie et al., 2011). Large-scale experiments on Chinese-to-English translation show that our augmented dependency-to-string model gains significant improvement of averaged +0.85 BLEU scores on three test sets over the dependencyto-string model. 1 Introduction As a representation holding both syntactic and semantic information, dependency grammar has been attracting more and more attention in statistical machine translation. Lin (2004) took paths as the elementary structures and proposed a path-based transfer model. Quirk et al. (2005) extended path to treelets (connected subgraphs of dependency trees) and put forward dependency treelet translation. Ding and Palmer (2005) proposed a model on the basis of dependency insertion grammar. Shen et al. (2008) employed the fixed and floating structures as elementary structures and proposed a string-to-dependency model with state-of-the-art performance. Xie et al. (2011) employs head-dependents relations as elementary structures and proposed a dependency-to-string model with good lo"
C14-1209,P06-1077,1,0.762924,"ency model with state-of-the-art performance. Xie et al. (2011) employs head-dependents relations as elementary structures and proposed a dependency-to-string model with good long distance reordering property. A head-dependents relation (HDR) is composed of a head and all its dependents, which can be viewed as an instance of a sentence pattern or phrase pattern. However, since dependency trees are much flatter than constituency trees, the dependency-to-string model suffers more severe non-syntactic phrase coverage problem (Meng et al., 2013) than constituencybased models (Galley et al., 2004; Liu et al., 2006; Huang et al., 2006). Non-syntactic phrases are those phrases that can not be covered by whole subtrees. To address this problem, Meng et al. (2013) proposed to translate with both constituency and dependency trees, which can incorporate non-syntactic phrases covered by the constituents of the constituency trees. This model requires both constituency and dependency trees, thus may suffer from both constituency and dependency parse errors. Additionally, there are only few languages that have both constituency and dependency parsers, which limits its practical use. In this paper, we propose to"
C14-1209,D13-1108,1,0.694863,"Missing"
C14-1209,P02-1038,0,0.612189,"Missing"
C14-1209,J03-1002,0,0.00530475,"age model for node n. Repeat the above process till the root of T is accessed. The hypothesis with the highest score is output as translation. 4 Experiments We evaluated our augmented model by comparison with dependency-to-string model and hierarchical phrase-based model on Chinese-to-English translation in terms of BLEU (Papineni et al., 2002). 4.1 Experimental Setup The parallel training corpus include 1.25M Chinese-English sentence pairs.1 We parse the Chinese sentences with Stanford Parser (Klein and Manning, 2003) into projective dependency trees, obtain word alignment by running GIZA++ (Och and Ney, 2003) in both directions and applying “grow-diag-final” refinement (Koehn et al., 2003), and train a 4-gram language model by SRI Language Modeling Toolkit (Stolcke, 2002) with Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We take NIST MT Evaluation test set 2002 as our development set, 2003 (MT03), 2004 (MT04) and 2005 (MT05) as our test sets, evaluate the quality of translations by case insensitive NIST BLEU4 metric2 , tune the feature weights by Max-BLEU strategy with MERT (Och, 2003), and check the statistical difference between the systems with significance test (Collins e"
C14-1209,P03-1021,0,0.103132,"g, 2003) into projective dependency trees, obtain word alignment by running GIZA++ (Och and Ney, 2003) in both directions and applying “grow-diag-final” refinement (Koehn et al., 2003), and train a 4-gram language model by SRI Language Modeling Toolkit (Stolcke, 2002) with Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We take NIST MT Evaluation test set 2002 as our development set, 2003 (MT03), 2004 (MT04) and 2005 (MT05) as our test sets, evaluate the quality of translations by case insensitive NIST BLEU4 metric2 , tune the feature weights by Max-BLEU strategy with MERT (Och, 2003), and check the statistical difference between the systems with significance test (Collins et al., 2005). 4.2 Systems We take “Moses-Chart” of Moses3 (Koehn et al., 2007) as hierarchical phrase-based model baseline. In our experiments, we use the default settings. Both the dependency-to-string baseline and our augmented model employ the same settings as those of Xie et al. (2011), with the beam threshold, beam size and rule size are set to 10−3 , 200 and 100 respectively. And both systems employ bilingual phrases with length ≤ 7 extracted by Moses. 4.3 Experiment results Table 1 shows the resu"
C14-1209,P05-1034,0,0.738563,"e the non-syntactic phrase coverage problem of dependency-to-string translation (Xie et al., 2011). Large-scale experiments on Chinese-to-English translation show that our augmented dependency-to-string model gains significant improvement of averaged +0.85 BLEU scores on three test sets over the dependencyto-string model. 1 Introduction As a representation holding both syntactic and semantic information, dependency grammar has been attracting more and more attention in statistical machine translation. Lin (2004) took paths as the elementary structures and proposed a path-based transfer model. Quirk et al. (2005) extended path to treelets (connected subgraphs of dependency trees) and put forward dependency treelet translation. Ding and Palmer (2005) proposed a model on the basis of dependency insertion grammar. Shen et al. (2008) employed the fixed and floating structures as elementary structures and proposed a string-to-dependency model with state-of-the-art performance. Xie et al. (2011) employs head-dependents relations as elementary structures and proposed a dependency-to-string model with good long distance reordering property. A head-dependents relation (HDR) is composed of a head and all its de"
C14-1209,P08-1066,0,0.487198,"cant improvement of averaged +0.85 BLEU scores on three test sets over the dependencyto-string model. 1 Introduction As a representation holding both syntactic and semantic information, dependency grammar has been attracting more and more attention in statistical machine translation. Lin (2004) took paths as the elementary structures and proposed a path-based transfer model. Quirk et al. (2005) extended path to treelets (connected subgraphs of dependency trees) and put forward dependency treelet translation. Ding and Palmer (2005) proposed a model on the basis of dependency insertion grammar. Shen et al. (2008) employed the fixed and floating structures as elementary structures and proposed a string-to-dependency model with state-of-the-art performance. Xie et al. (2011) employs head-dependents relations as elementary structures and proposed a dependency-to-string model with good long distance reordering property. A head-dependents relation (HDR) is composed of a head and all its dependents, which can be viewed as an instance of a sentence pattern or phrase pattern. However, since dependency trees are much flatter than constituency trees, the dependency-to-string model suffers more severe non-syntac"
C14-1209,D11-1020,1,0.723102,"augmented dependency-to-string model to combine the merits of both the head-dependents relations at handling long distance reordering and the fixed and floating structures at handling local reordering. For this purpose, we first compactly represent both the head-dependent relation and the fixed and floating structures into translation rules; second, in decoding we build “on-the-fly” new translation rules from the compact translation rules that can incorporate non-syntactic phrases into translations, thus alleviate the non-syntactic phrase coverage problem of dependency-to-string translation (Xie et al., 2011). Large-scale experiments on Chinese-to-English translation show that our augmented dependency-to-string model gains significant improvement of averaged +0.85 BLEU scores on three test sets over the dependencyto-string model. 1 Introduction As a representation holding both syntactic and semantic information, dependency grammar has been attracting more and more attention in statistical machine translation. Lin (2004) took paths as the elementary structures and proposed a path-based transfer model. Quirk et al. (2005) extended path to treelets (connected subgraphs of dependency trees) and put fo"
C14-1209,P02-1040,0,\N,Missing
C14-1209,P05-1066,0,\N,Missing
C14-1209,J07-2003,0,\N,Missing
C16-1057,P05-1022,0,0.786049,"ween long-distance projections and localized dependencies, which can be characterized by word categories of their lexical heads. The long-distance projections can be captured by a coarse constituent parser, which only sees peripheral and head words of such long-distance projections. So as to reduce error propagation, we transform k-best constituent parsing outputs to ’carved’ sequences for the following dependency parsing, which could be overlapped. Therefore, a third stage is required to search for the optimal subset of all candidate subtrees to combine. Given previous work on parsing, e.g. (Charniak and Johnson, 2005),(McDonald et al., 2005),(Martins et al., 2013) and so on, reliable constituent-based parsers and dependency parsers are available. Our main implementation challenge is to select an subset of those subtrees over overlapped carved sequences of the original input to cover the original sentence. We propose to formulate this as an Integer Linear Programming (ILP) problem, which is similar to a set cover problem. Note that, since the search space at this stage is highly constrained by the input of the previous stages, an exact decoding is efficient enough. This work is licensed under a Creative Com"
C16-1057,D14-1082,0,0.310367,"0.06) 93.24 (-0.19) 93.57 combination by subtree scores of DCNN 92.79 92.45 92.57 92.88 92.93 93.26 Table 3: Performance (UAS) of parsing by the combination of subtrees. The order-1/2 perceptron-based subtree classifier and DCNN are three alternative combination strategies. The highest increase of UAS for each baseline model is given in parentheses. A merge of the subtrees from all baseline models can be used to improve the second dependency parsing stage. We could pick any dependency model for subtree dependency parsing, especially the following: • An NN-based transition-based model, NNDep, (Chen and Manning, 2014). • A first- (second-) order graph-based model, MST-1(2), (McDonald and Pereira, 2006). • A third-order(and beyond) approximate model, Turbo-standard(full), (Martins et al., 2011). For scoring subtrees, we experiment with a feature-based perceptron classifier of first/second- order MST features, as well as the DCNN model described in Section 3.4. Given the well-known over-fitting problem for re-ranking models, we adopt the same mixture strategies as both (Le and Zuidema, 2014) 594 Figure 4: The range (min, max, average) of the number of subtrees (primary y-axis) and the averaged number of long"
C16-1057,W02-1001,0,0.035223,"axis). and (Zhu et al., 2015) with arc scores by a standard Turbo parser. The classifiers are trained with merged dependency parsing outputs, since to do 10-fold cross-validation training for every baseline model is overloading. For a given set of carved sequences transformed from constituent parsing outputs, the merged dependency parsing model covers the subtrees from all baseline models, thus increasing the oracle-combination score by 0.95 even compared to Turbo-full, the best baseline model. More specifically, the feature-based classifiers are trained with online perceptron-based learning (Collins, 2002). DCNN is trained off-line with the same settings as (Ma et al., 2015). And oracle combination as described in Section 3.4 is used to guide training. As shown in Table 3, with the merged dependency parsing model, our parsing system performs better than Turbo-full, by 0.14 (UAS). However, this is not what we push for in this work. We are more encouraged by the results that, with no algorithmic change to transition-based or first-order models, an increase of 1.28/1.21 (UAS) can be achieved solely due to a factorization of input. It is worth to notice that, on full dependency parsing, Turbo-full"
C16-1057,W06-2929,0,0.0390995,"more accurate to describe our pipeline as imposing constituent-based structural constraints to dependency parsing. Even though imposing constraints practically performs similarly as pruning, it provokes interesting work, e.g. recent work on approximate Linear Programming decoders for parsing, (Koo et al., 2010; Martins et al., 2011). In this work, we also formulate the search of optimal combination of subtrees as an ILP problem, and due to our efforts on constraining the search space in previous stages, it can be efficiently solved by exact decoding. Furthermore, works on Vine parsing, e.g. (Dreyer et al., 2006; Rush and Petrov, 2012), especially relate to ours. This line of work pays special attention to lengths of arcs and consider it as an import factor to constrain the search. Instead of pruning arcs by distance like Vine parsing, we decompose parsing of long-distance projections and localized dependencies, which is characterized by pattern in word categories but not absolute distance. There is also a more recent related work, (Fernandez-Gonzalez et al., 2016), that also pays attention to length of arcs. In their work, they use simple criteria based on the length and position of dependency arcs"
C16-1057,C96-1058,0,0.0320196,"search for optimal subset to combine is formulated as an ILP problem. This framework especially benefits the models poor on long sentences, generally improving baselines by 0.75-1.28 (UAS) on English, achieving comparable performance with highorder models but faster. For Chinese, the most notable increase is as high as 3.63 (UAS) when the proposed framework is applied to first-order parsing models. 1 Introduction Incorporating ’non-local’ features into syntactic parsing has been well-studied in literature. For exact parsing, we have seen cubic-time decoders for first and second-order models (Eisner, 1996; McDonald and Pereira, 2006), quadratic-time decoders for third-order models (Koo and Collins, 2010) and etc. In transition-based parsers, e.g. (Nivre et al., 2006), so-called non-local features can be easily extracted from parsing history. With a global inference framework, e.g. approximate Linear Programming (Martins et al., 2011; Koo et al., 2010), arbitrary structural constraints can be imposed. There are two main research goals underlying these works, one is to localize long-distance dependencies, which can be achieved by assuming the optimal substructure property or introducing parsing"
C16-1057,P07-1050,0,0.027077,"ctural factorization of dependency trees, such as arcs or RCNN units as defined in (Zhu et al., 2015). 4 Related work Klein and Manning (2003) proposed to factor parsing model into a phrase-structure tree and a dependency tree, even before dependency parsing was well-studied later in literature. The idea of combining the merits of constituents and dependency parsing is not new, but our proposed factorization is novel. Since k-best constituent parsing outputs are used, this work resembles the re-ranking works. However, we do not perform k-best list re-ranking, e.g. (Charniak and Johnson, 2005; Hall, 2007; Qian and Liu, 2015). Neither forest reranking (cube pruning), e.g. (Huang, 2008; Zhang and McDonald, 2012; Hayashi et al., 2011). Our search space is factored into pieces of subtrees, thus represents more combinatory candidates than a k-best list of whole dependency trees. This decomposition distinguishes our work from (Le and Zuidema, 2014) and (Zhu et al., 2015) which use NN-based models to re-rank whole dependency trees. These models could also be evaluated in our parsing pipeline. Furthermore, our work concerns very different aspects with (Ren et al., 2013), which uses dependency model f"
C16-1057,D11-1137,0,0.0231798,"lein and Manning (2003) proposed to factor parsing model into a phrase-structure tree and a dependency tree, even before dependency parsing was well-studied later in literature. The idea of combining the merits of constituents and dependency parsing is not new, but our proposed factorization is novel. Since k-best constituent parsing outputs are used, this work resembles the re-ranking works. However, we do not perform k-best list re-ranking, e.g. (Charniak and Johnson, 2005; Hall, 2007; Qian and Liu, 2015). Neither forest reranking (cube pruning), e.g. (Huang, 2008; Zhang and McDonald, 2012; Hayashi et al., 2011). Our search space is factored into pieces of subtrees, thus represents more combinatory candidates than a k-best list of whole dependency trees. This decomposition distinguishes our work from (Le and Zuidema, 2014) and (Zhu et al., 2015) which use NN-based models to re-rank whole dependency trees. These models could also be evaluated in our parsing pipeline. Furthermore, our work concerns very different aspects with (Ren et al., 2013), which uses dependency model for forest reranking of constituent-based parsing. Our dependency parsing process deals with local dependencies only, a complementa"
C16-1057,P08-1067,0,0.0350795,"in (Zhu et al., 2015). 4 Related work Klein and Manning (2003) proposed to factor parsing model into a phrase-structure tree and a dependency tree, even before dependency parsing was well-studied later in literature. The idea of combining the merits of constituents and dependency parsing is not new, but our proposed factorization is novel. Since k-best constituent parsing outputs are used, this work resembles the re-ranking works. However, we do not perform k-best list re-ranking, e.g. (Charniak and Johnson, 2005; Hall, 2007; Qian and Liu, 2015). Neither forest reranking (cube pruning), e.g. (Huang, 2008; Zhang and McDonald, 2012; Hayashi et al., 2011). Our search space is factored into pieces of subtrees, thus represents more combinatory candidates than a k-best list of whole dependency trees. This decomposition distinguishes our work from (Le and Zuidema, 2014) and (Zhu et al., 2015) which use NN-based models to re-rank whole dependency trees. These models could also be evaluated in our parsing pipeline. Furthermore, our work concerns very different aspects with (Ren et al., 2013), which uses dependency model for forest reranking of constituent-based parsing. Our dependency parsing process"
C16-1057,P10-1001,0,0.0934829,"ially benefits the models poor on long sentences, generally improving baselines by 0.75-1.28 (UAS) on English, achieving comparable performance with highorder models but faster. For Chinese, the most notable increase is as high as 3.63 (UAS) when the proposed framework is applied to first-order parsing models. 1 Introduction Incorporating ’non-local’ features into syntactic parsing has been well-studied in literature. For exact parsing, we have seen cubic-time decoders for first and second-order models (Eisner, 1996; McDonald and Pereira, 2006), quadratic-time decoders for third-order models (Koo and Collins, 2010) and etc. In transition-based parsers, e.g. (Nivre et al., 2006), so-called non-local features can be easily extracted from parsing history. With a global inference framework, e.g. approximate Linear Programming (Martins et al., 2011; Koo et al., 2010), arbitrary structural constraints can be imposed. There are two main research goals underlying these works, one is to localize long-distance dependencies, which can be achieved by assuming the optimal substructure property or introducing parsing actions like reductions. The other goal is to appropriately factor tree score, over parts like arcs a"
C16-1057,D10-1125,0,0.200078,"ed framework is applied to first-order parsing models. 1 Introduction Incorporating ’non-local’ features into syntactic parsing has been well-studied in literature. For exact parsing, we have seen cubic-time decoders for first and second-order models (Eisner, 1996; McDonald and Pereira, 2006), quadratic-time decoders for third-order models (Koo and Collins, 2010) and etc. In transition-based parsers, e.g. (Nivre et al., 2006), so-called non-local features can be easily extracted from parsing history. With a global inference framework, e.g. approximate Linear Programming (Martins et al., 2011; Koo et al., 2010), arbitrary structural constraints can be imposed. There are two main research goals underlying these works, one is to localize long-distance dependencies, which can be achieved by assuming the optimal substructure property or introducing parsing actions like reductions. The other goal is to appropriately factor tree score, over parts like arcs as well as over parsing actions. In this work, we propose a dependency parsing pipeline, so that long-distance dependencies are explicitly localized at the input level. With the proposed factorization, dependency tree score sums over its subtrees. More"
C16-1057,D14-1081,0,0.205664,"1: Example 2.2 demonstrates ’carved sequences’. Example 2.3 demonstrates the coarse constituent parsing. tions headed by each POS category, as computed from Penn WSJ treebank. We experiment with two alternative approaches for scoring subtrees. A feature-based perceptron classifier, and Dependency-based Convolutional Neural Networks (DCNN) (Ma et al., 2015). Previous use of continuous representation in parsing mainly models parsing actions, phrases, words, or features. When dependency chains are modeled, they were only used for reranking of k-best lists of whole dependency trees, e.g. (Le and Zuidema, 2014; Zhu et al., 2015). Since transition-based parsers are known to act worse on long-distance dependencies, they benefit from this pipeline the most. With no algorithmic change to transition-based or first-order models, an increase of 1.28/1.21 (UAS) can be achieved solely due to the proposed factorization of input, thus achieving comparable performance with high-order models but faster. For Chinese, the most notable increase is as high as 3.63 (UAS), when the proposed framework is applied to first-order parsing models. 2 System Design 2.1 Motivation Consider a sentence as follows: Example 2.1."
C16-1057,P15-2029,0,0.305896,"vecomons.org/licenses/by/4.0/. Licence details: 589 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 589–598, Osaka, Japan, December 11-17 2016. Figure 2: The averaged length of projecFigure 1: Example 2.2 demonstrates ’carved sequences’. Example 2.3 demonstrates the coarse constituent parsing. tions headed by each POS category, as computed from Penn WSJ treebank. We experiment with two alternative approaches for scoring subtrees. A feature-based perceptron classifier, and Dependency-based Convolutional Neural Networks (DCNN) (Ma et al., 2015). Previous use of continuous representation in parsing mainly models parsing actions, phrases, words, or features. When dependency chains are modeled, they were only used for reranking of k-best lists of whole dependency trees, e.g. (Le and Zuidema, 2014; Zhu et al., 2015). Since transition-based parsers are known to act worse on long-distance dependencies, they benefit from this pipeline the most. With no algorithmic change to transition-based or first-order models, an increase of 1.28/1.21 (UAS) can be achieved solely due to the proposed factorization of input, thus achieving comparable perf"
C16-1057,J93-2004,0,0.0572198,"e parsing, we decompose parsing of long-distance projections and localized dependencies, which is characterized by pattern in word categories but not absolute distance. There is also a more recent related work, (Fernandez-Gonzalez et al., 2016), that also pays attention to length of arcs. In their work, they use simple criteria based on the length and position of dependency arcs to determine how to combine the outputs of an left-to right transition-based parser and its ”mirrored” version. 5 Experiments Our main experiments are performed on dependency trees extracted from English WSJ Treebank (Marcus et al., 1993). We use Yamada and Matsumoto (2003)’s head rules to convert phrase structures to dependency structures, considering the productivity assumption. Following the conventional split, we use sections 02-21 for training, section 22 for development and section 23 for testing. Dependency parsing is evaluated by unlabeled attachment score (UAS), which is the percentage of words that correctly identified their heads. Chinese experiments are performed on CTB5 with the conventional splits described in (Zhu et al., 2015). For both English and Chinese experiments, the BLLIP parser (Charniak and Johnson, 20"
C16-1057,D11-1022,0,0.317518,"(UAS) when the proposed framework is applied to first-order parsing models. 1 Introduction Incorporating ’non-local’ features into syntactic parsing has been well-studied in literature. For exact parsing, we have seen cubic-time decoders for first and second-order models (Eisner, 1996; McDonald and Pereira, 2006), quadratic-time decoders for third-order models (Koo and Collins, 2010) and etc. In transition-based parsers, e.g. (Nivre et al., 2006), so-called non-local features can be easily extracted from parsing history. With a global inference framework, e.g. approximate Linear Programming (Martins et al., 2011; Koo et al., 2010), arbitrary structural constraints can be imposed. There are two main research goals underlying these works, one is to localize long-distance dependencies, which can be achieved by assuming the optimal substructure property or introducing parsing actions like reductions. The other goal is to appropriately factor tree score, over parts like arcs as well as over parsing actions. In this work, we propose a dependency parsing pipeline, so that long-distance dependencies are explicitly localized at the input level. With the proposed factorization, dependency tree score sums over"
C16-1057,P13-2109,0,0.075375,"cies, which can be characterized by word categories of their lexical heads. The long-distance projections can be captured by a coarse constituent parser, which only sees peripheral and head words of such long-distance projections. So as to reduce error propagation, we transform k-best constituent parsing outputs to ’carved’ sequences for the following dependency parsing, which could be overlapped. Therefore, a third stage is required to search for the optimal subset of all candidate subtrees to combine. Given previous work on parsing, e.g. (Charniak and Johnson, 2005),(McDonald et al., 2005),(Martins et al., 2013) and so on, reliable constituent-based parsers and dependency parsers are available. Our main implementation challenge is to select an subset of those subtrees over overlapped carved sequences of the original input to cover the original sentence. We propose to formulate this as an Integer Linear Programming (ILP) problem, which is similar to a set cover problem. Note that, since the search space at this stage is highly constrained by the input of the previous stages, an exact decoding is efficient enough. This work is licensed under a Creative Commons Attribution 4.0 International Licence. htt"
C16-1057,E06-1011,0,0.640181,"timal subset to combine is formulated as an ILP problem. This framework especially benefits the models poor on long sentences, generally improving baselines by 0.75-1.28 (UAS) on English, achieving comparable performance with highorder models but faster. For Chinese, the most notable increase is as high as 3.63 (UAS) when the proposed framework is applied to first-order parsing models. 1 Introduction Incorporating ’non-local’ features into syntactic parsing has been well-studied in literature. For exact parsing, we have seen cubic-time decoders for first and second-order models (Eisner, 1996; McDonald and Pereira, 2006), quadratic-time decoders for third-order models (Koo and Collins, 2010) and etc. In transition-based parsers, e.g. (Nivre et al., 2006), so-called non-local features can be easily extracted from parsing history. With a global inference framework, e.g. approximate Linear Programming (Martins et al., 2011; Koo et al., 2010), arbitrary structural constraints can be imposed. There are two main research goals underlying these works, one is to localize long-distance dependencies, which can be achieved by assuming the optimal substructure property or introducing parsing actions like reductions. The"
C16-1057,P05-1012,0,0.538727,"s and localized dependencies, which can be characterized by word categories of their lexical heads. The long-distance projections can be captured by a coarse constituent parser, which only sees peripheral and head words of such long-distance projections. So as to reduce error propagation, we transform k-best constituent parsing outputs to ’carved’ sequences for the following dependency parsing, which could be overlapped. Therefore, a third stage is required to search for the optimal subset of all candidate subtrees to combine. Given previous work on parsing, e.g. (Charniak and Johnson, 2005),(McDonald et al., 2005),(Martins et al., 2013) and so on, reliable constituent-based parsers and dependency parsers are available. Our main implementation challenge is to select an subset of those subtrees over overlapped carved sequences of the original input to cover the original sentence. We propose to formulate this as an Integer Linear Programming (ILP) problem, which is similar to a set cover problem. Note that, since the search space at this stage is highly constrained by the input of the previous stages, an exact decoding is efficient enough. This work is licensed under a Creative Commons Attribution 4.0 Int"
C16-1057,W06-2933,0,0.105794,"Missing"
C16-1057,P15-1114,0,0.0125639,"rization of dependency trees, such as arcs or RCNN units as defined in (Zhu et al., 2015). 4 Related work Klein and Manning (2003) proposed to factor parsing model into a phrase-structure tree and a dependency tree, even before dependency parsing was well-studied later in literature. The idea of combining the merits of constituents and dependency parsing is not new, but our proposed factorization is novel. Since k-best constituent parsing outputs are used, this work resembles the re-ranking works. However, we do not perform k-best list re-ranking, e.g. (Charniak and Johnson, 2005; Hall, 2007; Qian and Liu, 2015). Neither forest reranking (cube pruning), e.g. (Huang, 2008; Zhang and McDonald, 2012; Hayashi et al., 2011). Our search space is factored into pieces of subtrees, thus represents more combinatory candidates than a k-best list of whole dependency trees. This decomposition distinguishes our work from (Le and Zuidema, 2014) and (Zhu et al., 2015) which use NN-based models to re-rank whole dependency trees. These models could also be evaluated in our parsing pipeline. Furthermore, our work concerns very different aspects with (Ren et al., 2013), which uses dependency model for forest reranking o"
C16-1057,N12-1054,0,0.0207091,"cribe our pipeline as imposing constituent-based structural constraints to dependency parsing. Even though imposing constraints practically performs similarly as pruning, it provokes interesting work, e.g. recent work on approximate Linear Programming decoders for parsing, (Koo et al., 2010; Martins et al., 2011). In this work, we also formulate the search of optimal combination of subtrees as an ILP problem, and due to our efforts on constraining the search space in previous stages, it can be efficiently solved by exact decoding. Furthermore, works on Vine parsing, e.g. (Dreyer et al., 2006; Rush and Petrov, 2012), especially relate to ours. This line of work pays special attention to lengths of arcs and consider it as an import factor to constrain the search. Instead of pruning arcs by distance like Vine parsing, we decompose parsing of long-distance projections and localized dependencies, which is characterized by pattern in word categories but not absolute distance. There is also a more recent related work, (Fernandez-Gonzalez et al., 2016), that also pays attention to length of arcs. In their work, they use simple criteria based on the length and position of dependency arcs to determine how to comb"
C16-1057,xia-etal-2000-developing,0,0.116355,"red as longdistance projecting by tuning a threshold on the development set. As shown in Table 1, a proper setting of this threshold matters for all parsing stages. For following experiments on English (also Chinese and German), we consider a POS category as long-distance projecting, if the averaged distance of its projections is more than or equal to 5. For English, this setting gives us all verbal categories, whholders, and prepositions. For Chinese, it gives long-distance projecting categories of ’VA’, ’DEC’, ’P’, ’CS’, ’SP’, ’VV’, ’VE’, ’LB’, ’BA’ and ’VC’, whose meanings are referred to (Xia et al., 2000). The choice of k defines k-best constituent parsing outputs that will be transformed to carved sequences in the following dependency parsing stage. As shown in Table 2, a larger k doesn’t introduces sharply more subtrees per sentence, because most brackets in the top k-best constituent parsing outputs are the same. This observation shows a great advantage of our factorization and distinguishes our pipeline from k-best list reranking of whole trees. We can make use of a relatively large k to achieve higher recall, which is set to be 500 in the following experiments on English and Chinese. 5.2"
C16-1057,W03-3023,0,0.0779147,"ng of long-distance projections and localized dependencies, which is characterized by pattern in word categories but not absolute distance. There is also a more recent related work, (Fernandez-Gonzalez et al., 2016), that also pays attention to length of arcs. In their work, they use simple criteria based on the length and position of dependency arcs to determine how to combine the outputs of an left-to right transition-based parser and its ”mirrored” version. 5 Experiments Our main experiments are performed on dependency trees extracted from English WSJ Treebank (Marcus et al., 1993). We use Yamada and Matsumoto (2003)’s head rules to convert phrase structures to dependency structures, considering the productivity assumption. Following the conventional split, we use sections 02-21 for training, section 22 for development and section 23 for testing. Dependency parsing is evaluated by unlabeled attachment score (UAS), which is the percentage of words that correctly identified their heads. Chinese experiments are performed on CTB5 with the conventional splits described in (Zhu et al., 2015). For both English and Chinese experiments, the BLLIP parser (Charniak and Johnson, 2005) is used for the first-stage cons"
C16-1057,D12-1030,0,0.0194032,"., 2015). 4 Related work Klein and Manning (2003) proposed to factor parsing model into a phrase-structure tree and a dependency tree, even before dependency parsing was well-studied later in literature. The idea of combining the merits of constituents and dependency parsing is not new, but our proposed factorization is novel. Since k-best constituent parsing outputs are used, this work resembles the re-ranking works. However, we do not perform k-best list re-ranking, e.g. (Charniak and Johnson, 2005; Hall, 2007; Qian and Liu, 2015). Neither forest reranking (cube pruning), e.g. (Huang, 2008; Zhang and McDonald, 2012; Hayashi et al., 2011). Our search space is factored into pieces of subtrees, thus represents more combinatory candidates than a k-best list of whole dependency trees. This decomposition distinguishes our work from (Le and Zuidema, 2014) and (Zhu et al., 2015) which use NN-based models to re-rank whole dependency trees. These models could also be evaluated in our parsing pipeline. Furthermore, our work concerns very different aspects with (Ren et al., 2013), which uses dependency model for forest reranking of constituent-based parsing. Our dependency parsing process deals with local dependenc"
C16-1057,P15-1112,0,0.285371,"demonstrates ’carved sequences’. Example 2.3 demonstrates the coarse constituent parsing. tions headed by each POS category, as computed from Penn WSJ treebank. We experiment with two alternative approaches for scoring subtrees. A feature-based perceptron classifier, and Dependency-based Convolutional Neural Networks (DCNN) (Ma et al., 2015). Previous use of continuous representation in parsing mainly models parsing actions, phrases, words, or features. When dependency chains are modeled, they were only used for reranking of k-best lists of whole dependency trees, e.g. (Le and Zuidema, 2014; Zhu et al., 2015). Since transition-based parsers are known to act worse on long-distance dependencies, they benefit from this pipeline the most. With no algorithmic change to transition-based or first-order models, an increase of 1.28/1.21 (UAS) can be achieved solely due to the proposed factorization of input, thus achieving comparable performance with high-order models but faster. For Chinese, the most notable increase is as high as 3.63 (UAS), when the proposed framework is applied to first-order parsing models. 2 System Design 2.1 Motivation Consider a sentence as follows: Example 2.1. The SEC will probab"
C16-1131,D11-1033,0,0.0297315,"edding = 50 setting. For the baseline systems with embedding 7 https://catalog.ldc.upenn.edu/LDC2011T07 1394 size 50 and 100, we observe a sudden explosion in the evaluation perplexities with the decay factor setting described in Section 4.1. We experimentally set the learning rate to a decay factor of 0.5 after 8 iterations. In Table 6, the DAGRU approach can produce better perplexity results in all settings. 5 Related Work Domain adaptation for n-gram LMs is a well-studied research field. In general, there are approaches to select data which are similar to ID from GD (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Toral, 2013). There are also model mixture approaches (Bellegarda, 2004; Hsu and Glass, 2006; Allauzen and Riley, 2011), which try to find a weight to combine the ID LM and GD LM. In neural LM work, one approach to perform domain adaptation is to use an additional adaptation layer to combine the GD neural LM into the ID neural LM (Park et al., 2010; Ter-Sarkisov et al., 2014). However, an LM trained on all GD data is required. Curriculum learning (Bengio et al., 2009), which rearranges the training data in a particular order to improve generalization, is also applied on dom"
C16-1131,J81-4005,0,0.754477,"Missing"
C16-1131,P13-2119,0,0.019503,"or the baseline systems with embedding 7 https://catalog.ldc.upenn.edu/LDC2011T07 1394 size 50 and 100, we observe a sudden explosion in the evaluation perplexities with the decay factor setting described in Section 4.1. We experimentally set the learning rate to a decay factor of 0.5 after 8 iterations. In Table 6, the DAGRU approach can produce better perplexity results in all settings. 5 Related Work Domain adaptation for n-gram LMs is a well-studied research field. In general, there are approaches to select data which are similar to ID from GD (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Toral, 2013). There are also model mixture approaches (Bellegarda, 2004; Hsu and Glass, 2006; Allauzen and Riley, 2011), which try to find a weight to combine the ID LM and GD LM. In neural LM work, one approach to perform domain adaptation is to use an additional adaptation layer to combine the GD neural LM into the ID neural LM (Park et al., 2010; Ter-Sarkisov et al., 2014). However, an LM trained on all GD data is required. Curriculum learning (Bengio et al., 2009), which rearranges the training data in a particular order to improve generalization, is also applied on domain adaptation on"
C16-1131,D08-1089,0,0.0834468,"Missing"
C16-1131,W11-2123,0,0.120853,"Missing"
C16-1131,W06-1644,0,0.0270689,"50 and 100, we observe a sudden explosion in the evaluation perplexities with the decay factor setting described in Section 4.1. We experimentally set the learning rate to a decay factor of 0.5 after 8 iterations. In Table 6, the DAGRU approach can produce better perplexity results in all settings. 5 Related Work Domain adaptation for n-gram LMs is a well-studied research field. In general, there are approaches to select data which are similar to ID from GD (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Toral, 2013). There are also model mixture approaches (Bellegarda, 2004; Hsu and Glass, 2006; Allauzen and Riley, 2011), which try to find a weight to combine the ID LM and GD LM. In neural LM work, one approach to perform domain adaptation is to use an additional adaptation layer to combine the GD neural LM into the ID neural LM (Park et al., 2010; Ter-Sarkisov et al., 2014). However, an LM trained on all GD data is required. Curriculum learning (Bengio et al., 2009), which rearranges the training data in a particular order to improve generalization, is also applied on domain adaptation on a neural LM (Shi et al., 2013). In the work of Mikolov and Zweig (2012), word predictions are"
C16-1131,W04-3250,0,0.186791,"Missing"
C16-1131,J93-2004,0,0.0533767,"obtained from a very large GD data set. The word vectors in such a model are not domain-specific. By keeping it static, we interpret it as a “knowledge database”, and the knowledge should be consistent. Another practical reason for not updating the pre-trained GD word vector model is that fewer parameters need to be optimized in the network. 4 Experiments 4.1 Adaptation on Penn Treebank and News Corpus The typical setting of domain adaptation is small amount of ID training data and large amount of DG training data. Accordingly, we choose to use the availability of widely known Penn Treebank (Marcus et al., 1993) portion of the Wall Street Journal corpus in our LM adaptation experiment.5 The words outside the 10K vocabulary frequency list are mapped to the special unk token; sections 0-20 are used for training, and sections 21-22 are used for validation. We report the perplexity on data from sections 2324. More detailed data statistics are summarized in Table 1. We use the pre-trained word vector Google word2vec6 (Mikolov et al., 2013a) as the GD “look-up table”. It is trained on about 100 billion words, and consists of 3 million words and phrases. The word vectors are 300-dimensional in the word2vec"
C16-1131,N13-1090,0,0.352866,"on approach which has the ability to learn knowledge from huge corpora at speed. The question that arises here is how to make use of large amounts of GD data but avoiding long training times. In neural network training, words are represented as distributed representations, so-called “word vectors”, which can be pre-trained or trained with a specific task in mind. Although a pre-trained word vector model is also learned with a neural network, the training can be very fast. Recent optimized work shows learning word vectors can process more than 100 billion tokens in one day on a single machine (Mikolov et al., 2013c). Another advantage of a pre-trained word vector model is its flexibility, as it can be used later for different task-specific models. Furthermore, the pre-trained and the task-specific word vector models have no functional difference. Accordingly, we think it is very natural to use them This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1386 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1386–1397, Osaka, Japan, December 11-17"
C16-1131,P10-2041,0,0.58879,"are dissimilarities between the training and the testing environments. In research on domain adaptation, training data with the same style and topic (van der Wees et al., 2015) as the test data is often defined as in-domain (ID) data, with everything else called general-domain (GD) data. The ID data is often scarce and expensive to obtain, whereas the GD is plentiful and easy to access. One approach to address the situation of scarce ID training data is through data selection. For example, using the perplexity difference between ID and GD can select data that is close to ID and away from GD (Moore and Lewis, 2010). The selected data can then be concatenated with ID data for training. However, the drawback of using data selection is a threshold needs to be set, which often requires many models to be trained and evaluated. The situation will worsen if neural networks are used since the computational cost is immense in neural network training, where models often require days to converge even with the help of GPU-accelerated computing. Thus, simply adapting previous domain adaptation techniques into the neural network framework may not be efficient or effective. Ideally, we want to have an adaptation appro"
C16-1131,D14-1162,0,0.0800812,"both LMs. Already after training iteration 2, the DAGRU LM starts to outperform the baseline LM in terms of perplexity at every iteration. The plots flatten after 18 iterations, and the learning begins to converge for both the baseline LM and DAGRU LM. To demonstrate the scalability of the DAGRU adaptation approach, we also train LMs adapting from other freely available word vector models. SENNA (Semantic/syntactic Extraction using a Neural Network Architecture) is the word vector model received after a LM training (Collobert et al., 2011). The training data is obtained from Wikipedia. GloVe (Pennington et al., 2014) – Global Vectors for Word Representation – provides several versions of word vector models. The glove 6b model is trained on Wikipedia data and the English Gigaword Fifth Edition corpus;7 the glove 42b model is trained on the Common Crawl data; and the glove 840b model is trained on the the Common Crawl and additional web data. Table 6 presents the experimental results of DAGRU adaptation using different word vector models as GD data. The baseline models are trained on the Penn Treebank only. The word embedding numbers in Table 6 indicate the word vector size of the adapting word vector model"
C16-1131,W13-2803,0,0.0127122,"stems with embedding 7 https://catalog.ldc.upenn.edu/LDC2011T07 1394 size 50 and 100, we observe a sudden explosion in the evaluation perplexities with the decay factor setting described in Section 4.1. We experimentally set the learning rate to a decay factor of 0.5 after 8 iterations. In Table 6, the DAGRU approach can produce better perplexity results in all settings. 5 Related Work Domain adaptation for n-gram LMs is a well-studied research field. In general, there are approaches to select data which are similar to ID from GD (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Toral, 2013). There are also model mixture approaches (Bellegarda, 2004; Hsu and Glass, 2006; Allauzen and Riley, 2011), which try to find a weight to combine the ID LM and GD LM. In neural LM work, one approach to perform domain adaptation is to use an additional adaptation layer to combine the GD neural LM into the ID neural LM (Park et al., 2010; Ter-Sarkisov et al., 2014). However, an LM trained on all GD data is required. Curriculum learning (Bengio et al., 2009), which rearranges the training data in a particular order to improve generalization, is also applied on domain adaptation on a neural LM (S"
C16-1131,P15-2092,0,0.0359037,"Missing"
C16-1131,P07-2045,0,\N,Missing
C16-1170,P11-2080,0,0.0260277,"k Topic modelling has been applied successfully in many SMT works, especially in the domain adaptation literature. The motivation for introducing topic-model information in MT is that the translation performance decreases when there are dissimilarities between the training and the testing domains. A better approach is to make the use of the topic knowledge learned during training. Such knowledge can yield a better word or phrase choice in translation. Early work shows that the lexical translation table conditioned on the provenance of each domain can significantly improve translation quality (Chiang et al., 2011). Eidelman (2012) extends the provenance idea by including topic-dependent lexical weighting probabilities on the source side. Hasler (2012) successfully combines sparse word-pair and phrase-pair features with topic models. Later, Hasler (2014) also reports that translation performance is improved by using similarity features, which are computed from training phrase-pair and test-context vectors generated from the phrase-pair topic model. However, these ideas cannot be directly applied in NMT given the different training algorithms used in SMT and NMT. Despite the fact that neural network trai"
C16-1170,P05-1033,0,0.17334,"ailable (e.g. the word bank in Figure 1), it is clear that the translation in the Financial domain is more likely to be selected because many of the source words are from the same topic. Topic models have been applied successfully in many SMT works. For example, similar “topic consistent” behaviour is also observed by Su (2015). In his work, a context-aware topic model is integrated into SMT for better lexical selection. Xiao et al. (2012) and Zhang et al. (2014) focus on document translations and propose a topic-similarity model and a topic-sensitivity model for hierarchical phrasebased SMT (Chiang, 2005) on the document level. The topic-similarity model is used to encourage or penalize topic-sensitive rules, and the topic-sensitivity model is applied to balance topic-insensitive rules. However, these approaches cannot be directly used in NMT. In this work, we propose our novel topic-informed NMT model. In topic models, word-topic distributions can be viewed as a vector. In NMT, words are represented as vector-space representations. Thus, it is very natural to use them together. Word expressions in neural models are derived by its near context words while the topic vectors represent the docume"
C16-1170,D14-1179,0,0.0285064,"Missing"
C16-1170,P12-2023,0,0.0569223,"Missing"
C16-1170,D08-1089,0,0.0775974,"Missing"
C16-1170,2012.iwslt-papers.17,0,0.0411233,"Missing"
C16-1170,W14-3358,0,0.059939,"Missing"
C16-1170,W11-2123,0,0.0301983,"Missing"
C16-1170,2005.iwslt-1.8,0,0.120064,"Missing"
C16-1170,W04-3250,0,0.383993,"Missing"
C16-1170,P15-1002,0,0.0689874,"Missing"
C16-1170,W02-1018,0,0.126041,"Missing"
C16-1170,P00-1056,0,0.0398101,"Missing"
C16-1170,P03-1021,0,0.0206403,"Missing"
C16-1170,P02-1040,0,0.0951818,"oehn, 2004) improvements upon the baseline NMT using bootstrapping method at the level p = 0.01 and p = 0.05 level, respectively (with 1000 iterations). 36.5 36.5 36 36 35.5 35.5 Source Topic Number 150 100 80 50 40 30 20 150 33.5 100 33.5 80 34 50 34 40 34.5 30 34.5 20 35 10 35 10 BLEU 37 BLEU 37 Target Topic Number Figure 3: Topic numbers vs. translation BLEU scores on the NIST 2002 development dataset. and the models are saved at each 1,000 updates. The training takes approximately 3 days on an NVIDIA GeForce GTX TITAN X GM200 GPU machine. We then choose the final model based on the BLEU4 (Papineni et al., 2002) score on the development data. 5.2 Results Table 2 presents the experiment results on the development and test data. In Table 2, the number next to each topic-informed NMT system indicates the number of topics used in the system, i.e. we use 40 source topics in the source topic-informed NMT system, and 10 target topics in the target topic-informed NMT model. The source and target topic numbers are experimentally chosen from {10, 20, 30, 40, 50, 80, 100, 150} according to the development BLEU scores, as seen in Figure 3. We then leverage topic information on both source (with 40 topics) and ta"
C16-1170,W16-2209,0,0.0275087,"has been shown to be advantageous in many natural language processing tasks, little work has been proposed on using additional knowledge in NMT. Gulcehre et al. (2015) leverage monolingual corpora for NMT and propose two approaches to integrate a neural language model into the encoder-decoder architecture. He et al. (2016) integrate SMT features into NMT in order to solve the out-of-vocabulary problem. Furthermore, they use a n-gram language model trained on large monolingual data to enhance the local fluency of translation outputs. Linguistic information can also be used during NMT training (Sennrich and Haddow, 2016; Garc´ıa-Mart´ınez et al., 2016). There are also works that include topic modelling in neural language model training. Mikolov and Zweig (2012) use a contextual real-valued input vector associated with each word in a recurrent neural network (RNN) language model. 3 3.1 Neural Machine Translation Encoder-Decoder Architecture In a nutshell, the fundamental job of the encoder-decoder architecture (Cho et al., 2014) in NMT is to probabilistically decode a target sequence given the encoded source sequence, where the two sequences can be of different lengths. Given a sentence pair (S, T ), S is the"
C16-1170,P15-1023,0,0.0359834,"Missing"
C16-1170,I05-3027,0,0.0638,"Missing"
C16-1170,P12-1079,1,0.862252,"c in the translations during the decoding phase, and consequently better translations can be produced. Specifically, when a source word has more than one translation option available (e.g. the word bank in Figure 1), it is clear that the translation in the Financial domain is more likely to be selected because many of the source words are from the same topic. Topic models have been applied successfully in many SMT works. For example, similar “topic consistent” behaviour is also observed by Su (2015). In his work, a context-aware topic model is integrated into SMT for better lexical selection. Xiao et al. (2012) and Zhang et al. (2014) focus on document translations and propose a topic-similarity model and a topic-sensitivity model for hierarchical phrasebased SMT (Chiang, 2005) on the document level. The topic-similarity model is used to encourage or penalize topic-sensitive rules, and the topic-sensitivity model is applied to balance topic-insensitive rules. However, these approaches cannot be directly used in NMT. In this work, we propose our novel topic-informed NMT model. In topic models, word-topic distributions can be viewed as a vector. In NMT, words are represented as vector-space representa"
C16-1170,P07-2045,0,\N,Missing
C16-1205,D14-1179,0,0.0570165,"Missing"
C16-1205,N16-1102,0,0.0151591,"ce representation than just history of attention, and is therefore a more powerful model for machine translation. 2181 We also provide some actual translation examples (see Appendix) to show that our I NTERACTIVE ATTENTION can get better performance then baselines, especially on solving under-translation problem. We think the interactive mechanism of NMTIA is helpful for the decoder to automatically distinguish which parts have been translated and which parts are under-translated. 5 Related Work Our work is related to recent works that focus on improving attention models (Luong et al., 2015a; Cohn et al., 2016; Feng et al., 2016). Luong et al. (2015a) proposed to use global and local attention models to improve translation performance. They use a global one to attend to all source words and a local one to look at a subset of source words at a time. Cohn et al. (2016) extended the attention-based NMT to include structural biases from word-based alignment models, which achieved improvements across several language pairs. Feng et al. (2016) added implicit distortion and fertility models to attentionbased NMT to achieve translation improvements. These works are different with our I NTERACTIVE ATTENTION"
C16-1205,P05-1066,0,0.21663,"Missing"
C16-1205,P15-1001,0,0.0579261,"can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with"
C16-1205,N03-1017,0,0.0445318,"T baselines: Groundhog and RNNsearch? (our implementation of improved attention model as described in Section 2.2), and our I NTERACTIVE ATTENTION model (NMTIA ). The “*” indicates that the results are significantly (p&lt;0.01) better than those of all the baseline systems. 4.3 Comparison Systems We compare our NMTIA with four systems: • Moses (Koehn et al., 2007): an open source phrase-based translation system5 with default configuration. The word alignments are obtained with GIZA++ (Och and Ney, 2003) on the training corpora in both directions, using the “grow-diag-final-and” balance strategy (Koehn et al., 2003). The 4-gram language model with modified Kneser-Ney smoothing is trained on the target portion of training data with the SRILM toolkit (Stolcke and others, 2002), • Groundhog: an open source NMT system6 implemented with the conventional attention model (Bahdanau et al., 2015). • RNNsearch? : our in-house implementation of NMT system with the improved conventional attention model as described in Section 2.2. • Coverage Model: state-of-the-art variants of attention-based NMT model (Tu et al., 2016) which improve the attention mechanism through modeling a soft coverage on the source representati"
C16-1205,D15-1166,0,0.701706,"NTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comp"
C16-1205,P15-1002,0,0.384963,"NTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comp"
C16-1205,J03-1002,0,0.0114395,"Missing"
C16-1205,P16-1159,0,0.0609123,"rt variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with far less parameters and training instances (Jean et al., 2015). This superiority in efficiency comes mainly from"
C16-1205,P16-1008,1,0.724987,"paper, we propose a new attention mechanism, called I NTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. I NTERACTIVE ATTENTION can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that I NTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an in"
C16-1205,D16-1027,1,0.848213,"attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with far less parameters and training instances (Jean et al., 20"
C16-1205,Q16-1027,0,0.0182714,"ntion-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with far less parameters and training instances (Jean et al., 2015). This superiority in efficiency comes mainly from the mechanism of dy"
C16-1205,P07-2045,0,\N,Missing
C16-1243,D14-1179,0,0.0386727,"Missing"
C16-1243,P14-1129,0,0.0815892,"Missing"
C16-1243,P12-1092,0,0.0382067,"re: the phrase translation probability φ(e|f ), inverse phrase translation probability φ(f |e), lexical weighting lex(e|f ) and inverse lexical weighting lex(f |e). These scores are computed based on the co-occurrence of phrase pairs in training corpora and do not indicate any other information about phrases, their relation or context. Our goal in this research is to extend this set of features by incorporating semantic information of phrase pairs. Word embeddings are numerical representations of words which preserve semantic and syntactic information about words themselves and their context (Huang et al., 2012; Luong et al., 2013; Mikolov et al., 2013a; Mikolov et al., 2013b). They also preserve information about word order. These types of contextual, syntactic and semantic information can be quite useful for machine translation (MT), but being by its very nature a bilingual application, it requires bilingual (cross-lingual) information. Accordingly, we need methods to train bilingual embeddings via which we can access syntactic and semantic information about the source side as well as the target. Several papers have explored the training of bilingual embeddings (Mikolov et al., 2013b; Zou et al.,"
C16-1243,P07-2045,0,0.00688535,"he main set of bilingual features. Our goal is to enrich that set of features, as a better feature set should yield better translations. We propose new scores generated by a Convolutional Neural Network (CNN) which indicate the semantic relatedness of phrase pairs. We evaluate our model in different experimental settings with different language pairs. We observe significant improvements when the proposed features are incorporated into the PBSMT pipeline. 1 Introduction PBSMT models sentence-level translation with a phrase-based setting in which sentences are decomposed into different phrases (Koehn et al., 2007; Koehn, 2009). At each step, for a given source phrase the best candidate among the target phrases is selected as its translation. Phrasal translations are combined together to produce the sentence-level translation. This is a high-level view of PBSMT and there are many other processes involved in the main pipeline. Different bilingual and monolingual features are taken into account to make the final translation as adequate and fluent as possible. In this paper we only focus on the phrase-pairing process and try to enrich that part. The standard baseline bilingual features in the PBSMT pipeli"
C16-1243,W04-3250,0,0.126027,"K parallel sentences. All sentences are randomly selected from the En–Fr part of the Europarl (Koehn, 2005) collection. In our models we use 5-gram language models trained using the IRSTLM toolkit (Stolcke, 2002) and we tune models via MERT (Och, 2003). The extended systems those which include new scores within their phrase tables. In the extended systems we keep everything unchanged. only adding two new scores to the phrase table and the re-tune the PBSMT engine. The bold-faced scores indicate improvements are statistically significant according to paired bootstrap re-sampling with p = 0.05 (Koehn, 2004). As Table 1 shows, adding new features considerably boosts the PBSMT model, especially for smallsize datasets. For example, the BLEU score reported for the En→Fr system trained on the 500K dataset is 35.2, while a better performance (35.5) is achievable on the smaller 200K dataset in the presence of the new features. Usually as the size of the phrase table grows, the impact of such models attenuate, as large(r) phrase tables are rich enough to cover different cases and do not need to be enriched. Accordingly, such models are more suitable for small/medium-size datasets or low-resource languag"
C16-1243,2005.mtsummit-papers.11,0,0.00906448,"k 34.5 35.2 +0.7 1M 35.4 35.5 +0.1 Table 1: Experimental results on the En–Fr pair. The numbers indicate the BLEU scores. The bold-faced scores indicate improvements are statistically significant according to paired bootstrap re-sampling with p = 0.05. BLEU (Papineni et al., 2002) is used as the evaluation metric. We trained 3 baseline systems over datasets of 200K, 500K and 1 million (1M) parallel sentences. As the test set we use a corpus of 1.5K parallel sentences and the validation set includes 2K parallel sentences. All sentences are randomly selected from the En–Fr part of the Europarl (Koehn, 2005) collection. In our models we use 5-gram language models trained using the IRSTLM toolkit (Stolcke, 2002) and we tune models via MERT (Och, 2003). The extended systems those which include new scores within their phrase tables. In the extended systems we keep everything unchanged. only adding two new scores to the phrase table and the re-tune the PBSMT engine. The bold-faced scores indicate improvements are statistically significant according to paired bootstrap re-sampling with p = 0.05 (Koehn, 2004). As Table 1 shows, adding new features considerably boosts the PBSMT model, especially for sma"
C16-1243,P09-5002,0,0.0148636,"gual features. Our goal is to enrich that set of features, as a better feature set should yield better translations. We propose new scores generated by a Convolutional Neural Network (CNN) which indicate the semantic relatedness of phrase pairs. We evaluate our model in different experimental settings with different language pairs. We observe significant improvements when the proposed features are incorporated into the PBSMT pipeline. 1 Introduction PBSMT models sentence-level translation with a phrase-based setting in which sentences are decomposed into different phrases (Koehn et al., 2007; Koehn, 2009). At each step, for a given source phrase the best candidate among the target phrases is selected as its translation. Phrasal translations are combined together to produce the sentence-level translation. This is a high-level view of PBSMT and there are many other processes involved in the main pipeline. Different bilingual and monolingual features are taken into account to make the final translation as adequate and fluent as possible. In this paper we only focus on the phrase-pairing process and try to enrich that part. The standard baseline bilingual features in the PBSMT pipeline by default"
C16-1243,W13-3512,0,0.0327847,"lation probability φ(e|f ), inverse phrase translation probability φ(f |e), lexical weighting lex(e|f ) and inverse lexical weighting lex(f |e). These scores are computed based on the co-occurrence of phrase pairs in training corpora and do not indicate any other information about phrases, their relation or context. Our goal in this research is to extend this set of features by incorporating semantic information of phrase pairs. Word embeddings are numerical representations of words which preserve semantic and syntactic information about words themselves and their context (Huang et al., 2012; Luong et al., 2013; Mikolov et al., 2013a; Mikolov et al., 2013b). They also preserve information about word order. These types of contextual, syntactic and semantic information can be quite useful for machine translation (MT), but being by its very nature a bilingual application, it requires bilingual (cross-lingual) information. Accordingly, we need methods to train bilingual embeddings via which we can access syntactic and semantic information about the source side as well as the target. Several papers have explored the training of bilingual embeddings (Mikolov et al., 2013b; Zou et al., 2013; Cho et al., 20"
C16-1243,P03-1021,0,0.0131858,"icate improvements are statistically significant according to paired bootstrap re-sampling with p = 0.05. BLEU (Papineni et al., 2002) is used as the evaluation metric. We trained 3 baseline systems over datasets of 200K, 500K and 1 million (1M) parallel sentences. As the test set we use a corpus of 1.5K parallel sentences and the validation set includes 2K parallel sentences. All sentences are randomly selected from the En–Fr part of the Europarl (Koehn, 2005) collection. In our models we use 5-gram language models trained using the IRSTLM toolkit (Stolcke, 2002) and we tune models via MERT (Och, 2003). The extended systems those which include new scores within their phrase tables. In the extended systems we keep everything unchanged. only adding two new scores to the phrase table and the re-tune the PBSMT engine. The bold-faced scores indicate improvements are statistically significant according to paired bootstrap re-sampling with p = 0.05 (Koehn, 2004). As Table 1 shows, adding new features considerably boosts the PBSMT model, especially for smallsize datasets. For example, the BLEU score reported for the En→Fr system trained on the 500K dataset is 35.2, while a better performance (35.5)"
C16-1243,P02-1040,0,0.0987152,"Experimental Results To show the impact of the proposed scores we perform several experiments. In the first experiment we evaluate the model on the English–French (En–Fr) pair. Results are reported in Table 1. System Baseline Extended Improvement En→Fr 200K 34.4 35.5 +1.1 500k 35.2 36.0 +0.8 Fr→En 1M 35.7 36.0 +0.3 200k 33.8 35.1 +1.3 500k 34.5 35.2 +0.7 1M 35.4 35.5 +0.1 Table 1: Experimental results on the En–Fr pair. The numbers indicate the BLEU scores. The bold-faced scores indicate improvements are statistically significant according to paired bootstrap re-sampling with p = 0.05. BLEU (Papineni et al., 2002) is used as the evaluation metric. We trained 3 baseline systems over datasets of 200K, 500K and 1 million (1M) parallel sentences. As the test set we use a corpus of 1.5K parallel sentences and the validation set includes 2K parallel sentences. All sentences are randomly selected from the En–Fr part of the Europarl (Koehn, 2005) collection. In our models we use 5-gram language models trained using the IRSTLM toolkit (Stolcke, 2002) and we tune models via MERT (Och, 2003). The extended systems those which include new scores within their phrase tables. In the extended systems we keep everything"
C16-1243,W15-4911,1,0.892229,"Mikolov et al., 2013b). They also preserve information about word order. These types of contextual, syntactic and semantic information can be quite useful for machine translation (MT), but being by its very nature a bilingual application, it requires bilingual (cross-lingual) information. Accordingly, we need methods to train bilingual embeddings via which we can access syntactic and semantic information about the source side as well as the target. Several papers have explored the training of bilingual embeddings (Mikolov et al., 2013b; Zou et al., 2013; Cho et al., 2014; Gouws et al., 2015; Passban et al., 2015a; Zhao et al., 2015; Passban et al., 2016) with different architectures for different tasks such as MT and document classification. In our work we also try to follow the same research line. We propose a multi-plane data structure and a CNN to train This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2582 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2582–2591, Osaka, Japan, December 11-17 2016. mixed embeddings. Using the propose"
C16-1243,W16-3403,1,0.859545,"e information about word order. These types of contextual, syntactic and semantic information can be quite useful for machine translation (MT), but being by its very nature a bilingual application, it requires bilingual (cross-lingual) information. Accordingly, we need methods to train bilingual embeddings via which we can access syntactic and semantic information about the source side as well as the target. Several papers have explored the training of bilingual embeddings (Mikolov et al., 2013b; Zou et al., 2013; Cho et al., 2014; Gouws et al., 2015; Passban et al., 2015a; Zhao et al., 2015; Passban et al., 2016) with different architectures for different tasks such as MT and document classification. In our work we also try to follow the same research line. We propose a multi-plane data structure and a CNN to train This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2582 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2582–2591, Osaka, Japan, December 11-17 2016. mixed embeddings. Using the proposed data structure, source and target words a"
C16-1243,D14-1162,0,0.0870383,"ine. We propose a multi-plane data structure and a CNN to train This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2582 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2582–2591, Osaka, Japan, December 11-17 2016. mixed embeddings. Using the proposed data structure, source and target words are linked together, so we call our embeddings mixed. Our model is a bilingual extension to well-known embedding models (Mikolov et al., 2013a; Pennington et al., 2014) with a quite different architecture which is fine-tuned for MT tasks. The reminder of the paper is structured as follows. In Section 2 we briefly review some similar models which train bilingual embeddings. Section 3 discusses our neural model and how we incorporate results from our model into the PBSMT pipeline. Section 4 explains the results from several experiments to show the impact of the proposed model. Finally, Section 5 concludes the paper with some avenues for future work. 2 Background One of the most successful neural models proposed for training word embeddings is Word2Vec (Mikolov"
C16-1243,N15-1176,0,0.0840114,". They also preserve information about word order. These types of contextual, syntactic and semantic information can be quite useful for machine translation (MT), but being by its very nature a bilingual application, it requires bilingual (cross-lingual) information. Accordingly, we need methods to train bilingual embeddings via which we can access syntactic and semantic information about the source side as well as the target. Several papers have explored the training of bilingual embeddings (Mikolov et al., 2013b; Zou et al., 2013; Cho et al., 2014; Gouws et al., 2015; Passban et al., 2015a; Zhao et al., 2015; Passban et al., 2016) with different architectures for different tasks such as MT and document classification. In our work we also try to follow the same research line. We propose a multi-plane data structure and a CNN to train This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2582 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2582–2591, Osaka, Japan, December 11-17 2016. mixed embeddings. Using the proposed data structure, so"
C16-1243,D13-1141,0,0.430929,"et al., 2012; Luong et al., 2013; Mikolov et al., 2013a; Mikolov et al., 2013b). They also preserve information about word order. These types of contextual, syntactic and semantic information can be quite useful for machine translation (MT), but being by its very nature a bilingual application, it requires bilingual (cross-lingual) information. Accordingly, we need methods to train bilingual embeddings via which we can access syntactic and semantic information about the source side as well as the target. Several papers have explored the training of bilingual embeddings (Mikolov et al., 2013b; Zou et al., 2013; Cho et al., 2014; Gouws et al., 2015; Passban et al., 2015a; Zhao et al., 2015; Passban et al., 2016) with different architectures for different tasks such as MT and document classification. In our work we also try to follow the same research line. We propose a multi-plane data structure and a CNN to train This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2582 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2582–2591, Osaka, Jap"
C16-1243,2015.eamt-1.12,1,\N,Missing
C18-1110,P17-2021,0,0.0118123,"h1 h2 → − h1 ← − h1 → − h2 ← − h2 ··· x1 x2 ··· ··· h ls → − h ls ← − h ls x ls Figure 1: The architecture of attention-based NMT NMT is able to capture certain syntactic phenomena (e.g. subject-verb agreement) without external syntactic information (Linzen et al., 2016; Shi et al., 2016), there are some other works which has shown their superior performance by modeling word relationship explicitly. However, these works usually need to introduce external syntactic knowledge or connect words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017). In this paper, we present a method to refine the NMT based on the above two points. The main idea is to learn relationship between the source word pairs. Corresponding to the first point, our method employs Convolutional Neural Networks (CNNs) to collect local information around one word and relates each word with its neighbors, which ensures the subsequent operations are performed in the unit of multiple words. As for the second point, Relation Network (RN) (Santoro et al., 2017) is introduced to establish pairwise relationship between words, meanwhile, there’s no need to"
C18-1110,D17-1209,0,0.130141,"i=1 αij hi αj2 αjls ··· h1 h2 → − h1 ← − h1 → − h2 ← − h2 ··· x1 x2 ··· ··· h ls → − h ls ← − h ls x ls Figure 1: The architecture of attention-based NMT NMT is able to capture certain syntactic phenomena (e.g. subject-verb agreement) without external syntactic information (Linzen et al., 2016; Shi et al., 2016), there are some other works which has shown their superior performance by modeling word relationship explicitly. However, these works usually need to introduce external syntactic knowledge or connect words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017). In this paper, we present a method to refine the NMT based on the above two points. The main idea is to learn relationship between the source word pairs. Corresponding to the first point, our method employs Convolutional Neural Networks (CNNs) to collect local information around one word and relates each word with its neighbors, which ensures the subsequent operations are performed in the unit of multiple words. As for the second point, Relation Network (RN) (Santoro et al., 2017) is introduced to establish pairwise relationship between words, me"
C18-1110,D14-1179,0,0.023952,"Missing"
C18-1110,P05-1066,0,0.167644,"Missing"
C18-1110,P17-1012,0,0.0173668,"between source and target words, with the main architecture remaining the RNN encoder-decoder framework. Sennrich et al. (2016) enriched source representations with POS tags, dependency labels and other linguistic features. Bastings et al. (2017) employed graph convolutional networks to model relations of words in dependency trees for the source embeddings to include these relations. These two models both require extra supervised syntax input while our method does not need external knowledge and learn the relationship by its own. Another line is to change the structure of the neural network. Gehring et al. (2017a) and Gehring et al. (2017b) proposed to substitute the conventional RNN encoder with the CNN encoder in order to train faster. They employed stacked CNNs to capture relationships between source words which can be calculated simultaneously, not like RNNs, the computation of which is constrained by temporal dependencies. The attention scores are also computed based on the output of the CNNs and the decoder is still the RNN-based decoder. Vaswani et al. (2017) is another work to eschew the recurrence. It instead 1296 relied entirely on the attention mechanism to draw the global dependencies bet"
C18-1110,D13-1176,0,0.0841323,"ization capability of recurrent neural network via associating source words with each other, this would also help retain their relationships. Then the source representations and all the relations are fed into the attention component together while decoding, with the main encoderdecoder framework unchanged. Experiments on several datasets show that our method can improve the translation performance significantly over the conventional encoder-decoder model and even outperform the approach involving supervised syntactic knowledge. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has achieved great success in some language pairs, rivalling the state-ofthe-art Statistical Machine Translation (SMT). The Recurrent Neural Network (RNN) encoder-decoder architecture is widely used framework for NMT, the principle behind which is that: encoding the meaning of the input bidirectionally into a concept space via RNNs and decoding into target words with RNNs based on this encoding (Sutskever et al., 2014; Bahdanau et al., 2015). This means that encoding principle leads to a deeper understanding and learning of the translation rules"
C18-1110,N03-1017,0,0.0290452,"h makes the decoding process only focus on the most related source words, the RNN encoder-decoder framework is expected to be able to handle long sequences and consider the globally related information. However, the practical situation is that RNNs tend to forget old history information, especially the far older one. Sometimes the older information is indispensable for generating proper translation, e.g., for the source sentence “take the heavy box away”, when translating “away”, “take” should be considered together. In addition, it has been proven that using phrases rather than words in SMT (Koehn et al., 2003) brings performance improvement, while in NMT the attention is only modeled in the unit of words. In the same sense, improvement is expected if attention is operated on more words rather than one. Moreover, NMT produces the representation for the source by running through the source words sequentially with a bidirectional RNN (Schuster and Paliwal, 1997), so it only employs word order information and ignores the relation between words. Although some researchers have demonstrated that ∗ Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. c"
C18-1110,P17-1064,0,0.0124943,"← − h2 ··· x1 x2 ··· ··· h ls → − h ls ← − h ls x ls Figure 1: The architecture of attention-based NMT NMT is able to capture certain syntactic phenomena (e.g. subject-verb agreement) without external syntactic information (Linzen et al., 2016; Shi et al., 2016), there are some other works which has shown their superior performance by modeling word relationship explicitly. However, these works usually need to introduce external syntactic knowledge or connect words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017). In this paper, we present a method to refine the NMT based on the above two points. The main idea is to learn relationship between the source word pairs. Corresponding to the first point, our method employs Convolutional Neural Networks (CNNs) to collect local information around one word and relates each word with its neighbors, which ensures the subsequent operations are performed in the unit of multiple words. As for the second point, Relation Network (RN) (Santoro et al., 2017) is introduced to establish pairwise relationship between words, meanwhile, there’s no need to attain external in"
C18-1110,Q16-1037,0,0.0225492,"e Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1292 Proceedings of the 27th International Conference on Computational Linguistics, pages 1292–1303 Santa Fe, New Mexico, USA, August 20-26, 2018. yj−1 Decoder ··· Attention Layer αj1 Encoder Wv y j sj−1 ! ls aj = i=1 αij hi αj2 αjls ··· h1 h2 → − h1 ← − h1 → − h2 ← − h2 ··· x1 x2 ··· ··· h ls → − h ls ← − h ls x ls Figure 1: The architecture of attention-based NMT NMT is able to capture certain syntactic phenomena (e.g. subject-verb agreement) without external syntactic information (Linzen et al., 2016; Shi et al., 2016), there are some other works which has shown their superior performance by modeling word relationship explicitly. However, these works usually need to introduce external syntactic knowledge or connect words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017). In this paper, we present a method to refine the NMT based on the above two points. The main idea is to learn relationship between the source word pairs. Corresponding to the first point, our method employs Convolutional Neur"
C18-1110,P02-1040,0,0.100729,"ning data. WMT16 We conducted experiments on WMT16 dataset, the same dataset as the work of Bastings et al. (2017) for comparison. We kept the same settings as those in Bastings et al. (2017): The original dataset consists of 4500966 sentence pairs, with 4173550 left after filtering pairs which contains more than 50 tokens on either side after tokenization. newstest2015 and newstest2016 were used as the validation set and test dataset, respectively. 16k BPE merging operations were conducted on the target side of the bilingual training data. For WMT16 dataset, case-sensitive 4-gram BLEU score (Papineni et al., 2002) was reported by using the multi-bleu.pl script. The results on the other two datasets were evaluated with case-insensitive 4-gram BLEU score. 5.2 Systems Results of five systems on different datasets were reported: RNNsearch We implemented the attention-based NMT of Bahdanau et al. (2015) by PyTorch framework3 with the following settings: the length of the sentences on both sides was limited up to 50 tokens with 30K vocabulary, and the source and target word embedding sizes were both set to 512, the size of all hidden units in both encoder and decoder RNNs was also set to 512, and all paramet"
C18-1110,W16-2209,0,0.0358996,"Missing"
C18-1110,P16-1162,0,0.68146,"Wv y j sj−1 ! ls aj = i=1 αij hi αj2 αjls ··· h1 h2 → − h1 ← − h1 → − h2 ← − h2 ··· x1 x2 ··· ··· h ls → − h ls ← − h ls x ls Figure 1: The architecture of attention-based NMT NMT is able to capture certain syntactic phenomena (e.g. subject-verb agreement) without external syntactic information (Linzen et al., 2016; Shi et al., 2016), there are some other works which has shown their superior performance by modeling word relationship explicitly. However, these works usually need to introduce external syntactic knowledge or connect words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017). In this paper, we present a method to refine the NMT based on the above two points. The main idea is to learn relationship between the source word pairs. Corresponding to the first point, our method employs Convolutional Neural Networks (CNNs) to collect local information around one word and relates each word with its neighbors, which ensures the subsequent operations are performed in the unit of multiple words. As for the second point, Relation Network (RN) (Santoro et al., 2017) is introduced to establish pairwise relatio"
C18-1110,D16-1159,0,0.0184928,"4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1292 Proceedings of the 27th International Conference on Computational Linguistics, pages 1292–1303 Santa Fe, New Mexico, USA, August 20-26, 2018. yj−1 Decoder ··· Attention Layer αj1 Encoder Wv y j sj−1 ! ls aj = i=1 αij hi αj2 αjls ··· h1 h2 → − h1 ← − h1 → − h2 ← − h2 ··· x1 x2 ··· ··· h ls → − h ls ← − h ls x ls Figure 1: The architecture of attention-based NMT NMT is able to capture certain syntactic phenomena (e.g. subject-verb agreement) without external syntactic information (Linzen et al., 2016; Shi et al., 2016), there are some other works which has shown their superior performance by modeling word relationship explicitly. However, these works usually need to introduce external syntactic knowledge or connect words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017). In this paper, we present a method to refine the NMT based on the above two points. The main idea is to learn relationship between the source word pairs. Corresponding to the first point, our method employs Convolutional Neural Networks (CNNs)"
C18-1110,P03-1021,0,0.187267,"only memorize history information but capture the relationship between words, both of which are beneficial to translate long sentences. 5.5 Word Alignment Systems RNNsearch? R NMT BLEU 22.40 24.12 AER 46.76 45.66 Table 4: Comparison of alignment quality on NIST Zh-En translation task. In this section, we will verify the translation performance of our model from another perspective. Intuitively, the better translation should have better alignment to the source sentence, so we evaluated the quality of the alignments derived from the attention module of the NMT using Alignment Error Rate (AER) (Och, 2003). We did this experiment on the artificially aligned dataset from Liu and Sun (2015) which contains 900 Zh-En sentence pairs. The alignments were got in this way for both RNNsearch? 1299 (a) RNNsearch? (b) R NMT Figure 4: Word alignment comparison. The green boxes show the manual golden alignments. system and our system. When one target word was generated, we retained the alignment link with the highest probability αij in Equation 3. The comparison results are shown in Table 4. It illustrates that our system R NMT can produce better translations than the baseline RNNsearch? , a difference of 1"
C18-1265,W17-4703,0,0.0312819,"Missing"
C18-1265,D14-1179,0,0.0310498,"Missing"
C18-1265,P16-1160,0,0.133621,"t et al. (2017) investigated the impact of different word representation models in the context of factored NMT. Our work is also an example of models which try to provide richer information when the source side is an MRL. Models reviewed so far address the problem of morphology on the source side. In contrast, there is a group of models which study the same problem for the target side. Huck et al. (2017) compared different word-segmentation models, including linguistically motivated as well as statistical techniques, to find the most appropriate segmentation scheme when translating into MRLs. Chung et al. (2016) tried to design a suitable architecture when the target language is an MRL. They benefit from using a character-based decoder which partially resolves the problem. Passban et al. (2018) proposed a similar approach in which they equipped the character-based decoder with an additional morphology table to inform the decoder with the target language’s morphological structures. 1 Comparing translation results generated by different word-, subword-, and character-based architectures on different language pairs shows that there is no character-based model which is able to outperform its word- and/or"
C18-1265,P16-2058,0,0.0395449,"Missing"
C18-1265,I17-1015,0,0.0203217,"rent language pairs shows that there is no character-based model which is able to outperform its word- and/or subword-based counterparts. Please see results at http://matrix.statmt.org/?mode=all 3136 Apart from these models, there are others that do not directly address the problem of morphology but their solutions could be quite useful to translate MRLs. Jean et al. (2015) proposed a model to handle very large vocabularies. Luong et al. (2015) addressed the problem of rare and unseen words with the help of a post-translation phase to exchange unknown tokens with their potential translations. Dalvi et al. (2017) did not propose a new model but studied the impact of morphological information in NMT. They evaluated the behaviour of an encoder-decoder model to see what sort of morphological information is learned via the model and how the model deals with complex structures. Passban (2018) extensively discussed the problem of morphology at the word and sequence level and proposed solutions for modeling and translating sequences in monolingual and bilingual settings. 3 Proposed Approach We propose an NMT architecture with a double-source encoder and double-attentive decoder for translating from MRLs. Our"
C18-1265,N16-1101,0,0.310235,"morphological information in NMT. They evaluated the behaviour of an encoder-decoder model to see what sort of morphological information is learned via the model and how the model deals with complex structures. Passban (2018) extensively discussed the problem of morphology at the word and sequence level and proposed solutions for modeling and translating sequences in monolingual and bilingual settings. 3 Proposed Approach We propose an NMT architecture with a double-source encoder and double-attentive decoder for translating from MRLs. Our neural architecture is inspired by models proposed in Firat et al. (2016) and Zoph and Knight (2016). It takes inputs from two different channels: one channel which is referred to as the main channel sends stem information (main input), and the other one (the auxiliary channel) sends affix information. If the input is w0 , w1 , ..., wn for the (original) encoder-decoder model, our proposed architecture takes two sequences of ς0 , ς1 , ..., ςn and τ0 , τ1 , ..., τn through the main and auxiliary channels, respectively, where wi shows the surface form of a word whose stem is ςi , and affix information associated with wi is given by τi . Our new neural architecture is"
C18-1265,W17-4706,0,0.0194826,"cter- and word-based encoder to try to solve the out-of-vocabulary problem. Vylomova et al. (2016) tackled the problem by comparing the impact of different representation schemes on the encoder. Similarly, Burlot et al. (2017) investigated the impact of different word representation models in the context of factored NMT. Our work is also an example of models which try to provide richer information when the source side is an MRL. Models reviewed so far address the problem of morphology on the source side. In contrast, there is a group of models which study the same problem for the target side. Huck et al. (2017) compared different word-segmentation models, including linguistically motivated as well as statistical techniques, to find the most appropriate segmentation scheme when translating into MRLs. Chung et al. (2016) tried to design a suitable architecture when the target language is an MRL. They benefit from using a character-based decoder which partially resolves the problem. Passban et al. (2018) proposed a similar approach in which they equipped the character-based decoder with an additional morphology table to inform the decoder with the target language’s morphological structures. 1 Comparing"
C18-1265,P15-1001,0,0.0596598,"er-based decoder with an additional morphology table to inform the decoder with the target language’s morphological structures. 1 Comparing translation results generated by different word-, subword-, and character-based architectures on different language pairs shows that there is no character-based model which is able to outperform its word- and/or subword-based counterparts. Please see results at http://matrix.statmt.org/?mode=all 3136 Apart from these models, there are others that do not directly address the problem of morphology but their solutions could be quite useful to translate MRLs. Jean et al. (2015) proposed a model to handle very large vocabularies. Luong et al. (2015) addressed the problem of rare and unseen words with the help of a post-translation phase to exchange unknown tokens with their potential translations. Dalvi et al. (2017) did not propose a new model but studied the impact of morphological information in NMT. They evaluated the behaviour of an encoder-decoder model to see what sort of morphological information is learned via the model and how the model deals with complex structures. Passban (2018) extensively discussed the problem of morphology at the word and sequence lev"
C18-1265,W04-3250,0,0.0645315,"[ς]1 [pre+suf]2 [ς]1 [τ ]2 [ς]1 [τ ]2 char char char 26.11 26.74 26.29 22.95 23.44 23.40 23.62 23.81 23.74 Costa-juss`a and Fonollosa (2016) Firat et al. (2016) Lee et al. (2017) Sennrich and Haddow (2016)? Our model pre+suf affix-cς cτ affix-cτ Table 2: Source and Target indicate the data type for the encoder and decoder, respectively. ς is the stem, pre is the prefix and suf is the suffix. τ is the affix token. The bold-faced score is the best score for the direction and the score with * shows the best performance reported by other existing models. According to paired bootstrap re-sampling (Koehn, 2004) with p = 0.05, the bold-faced number is significantly better than the score with *. Brackets show different channels and the + mark indicates the summation, e.g. [ς]1 [pre+suf]2 means the first channel takes a stem at each step and the second channel takes the summation of the prefix and suffix of the associated stem. • is the concatenation operation and ? indicates that results were produced in our experimental setting using our re-implementation of the original model. The second row shows the model proposed by Costa-juss`a and Fonollosa (2016), in which a complicated convolutional module is"
C18-1265,Q17-1026,0,0.489613,"rds are consumed character by character. Usually, there is a convolutional module after the input layer to read characters, connect them to each other, and extract inter-character relations. This approach is helpful as it requires no preprocessing step, but there are two main problems with that. Similar to the previous group, the length of the input sequences could be an issue as by segmenting words into characters an input sequence with a limited number of words is changed to a long sequence of characters. Furthermore, the convolutional computation over characters could be quite costly, e.g. Lee et al. (2017) use 200 convolutional filters of width 1 along with 200 filters of width 2, 250 filters of width 3, and continue up to a filter size of 300 with width 8 to extract useful information on the source side. This amount of computation is carried out only in one layer (for one word), so in addition there are max-pooling This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 3135 License details: http:// layers followed by 4 highway layers (Srivastava et al., 2015), and then recurrent layers on top. This is a complex architecture wh"
C18-1265,L16-1147,0,0.0255189,"ic gradient descent with Adam (Kingma and Ba, 2015). The mini-batch size is 80, the beam search width is 20, and the norm of the gradient is clipped with the threshold 1.0. 4.1 Data Preparation Our models are trained to translate from German (De), Russian (Ru), and Turkish (Tr) into English (En). For German and Russian, we used WMT-15 datasets,2 where the De–En corpus includes 4.5M parallel sentences and the size of the Ru–En corpus is 2.1M. The newstest-2013 and newstest-2015 datasets are used as the development and test sets, respectively. For Tr–En, we used the OpenSubtitle2016 collection (Lison and Tiedemann, 2016).3 We randomly selected 3K sentences for each of the development and test sets, and 4M for training. 2 3 http://www.statmt.org/wmt15/translation-task.html http://opus.nlpl.eu/OpenSubtitles2016.php 3139 English–German En Sentence Token Type Stem Affix Prefix Suffix Char De 4.2M 103, 692, 553 96, 235, 845 103, 574 143, 329 21, 223 26, 301 13, 410 24, 054 3, 104 5, 208 3, 285 4, 974 355 302 English–Russian En Ru 2.1M 43, 944, 989 39, 694, 475 70, 376 119, 258 15, 964 19, 557 9, 320 17, 542 2, 959 3, 324 2, 219 3, 460 360 323 English–Turkish En Tr 4.0M 24, 875, 286 17, 915, 076 108, 699 178, 672 1"
C18-1265,P16-1100,0,0.033724,"portant issue in the field of sequence modeling as it directly affects the model’s architecture and its performance. There is a fair amount of research carried out to address this problem, which can be discussed in two main categories. One group of models tries to cope with the problem on the encoder side where the goal is to understand the rich morphology of the source language. Kim et al. (2016) proposed a convolutional module to process complex inputs for the problem of language modeling. Costa-juss`a and Fonollosa (2016) and Lee et al. (2017) adapted the same convolutional encoder to NMT. Luong and Manning (2016) designed a hybrid character- and word-based encoder to try to solve the out-of-vocabulary problem. Vylomova et al. (2016) tackled the problem by comparing the impact of different representation schemes on the encoder. Similarly, Burlot et al. (2017) investigated the impact of different word representation models in the context of factored NMT. Our work is also an example of models which try to provide richer information when the source side is an MRL. Models reviewed so far address the problem of morphology on the source side. In contrast, there is a group of models which study the same probl"
C18-1265,P15-1002,0,0.0498344,"der with the target language’s morphological structures. 1 Comparing translation results generated by different word-, subword-, and character-based architectures on different language pairs shows that there is no character-based model which is able to outperform its word- and/or subword-based counterparts. Please see results at http://matrix.statmt.org/?mode=all 3136 Apart from these models, there are others that do not directly address the problem of morphology but their solutions could be quite useful to translate MRLs. Jean et al. (2015) proposed a model to handle very large vocabularies. Luong et al. (2015) addressed the problem of rare and unseen words with the help of a post-translation phase to exchange unknown tokens with their potential translations. Dalvi et al. (2017) did not propose a new model but studied the impact of morphological information in NMT. They evaluated the behaviour of an encoder-decoder model to see what sort of morphological information is learned via the model and how the model deals with complex structures. Passban (2018) extensively discussed the problem of morphology at the word and sequence level and proposed solutions for modeling and translating sequences in mono"
C18-1265,P02-1040,0,0.104755,"cond extension we can expect the neural network to learn our desired information. Basically, by defining the affix token we try to introduce an additional cell of memory, in which the NMT model and the affix encoder store the useful information it has learned 4 In these two extensions everything is the same but the attention mechanism. Equations (5) and (6) explain the attention mechanisms for affix-cς cτ and affix-cτ , respectively. 3140 (in addition to stem). 4.2 Experimental Results The results obtained from our experiments are reported in Table 2. The numbers in the table are BLEU scores (Papineni et al., 2002) of different neural models. We compare our models to all existing models which translate from MRLs or reported experimental results on our datasets. The first row of the table shows an encoder-decoder model where the decoder works at the character level and uses the architecture proposed in Chung et al. (2016). The first row can be considered as a baseline for all other models reported in the table, as it does not use any complicated neural architecture on the source side. For each word it simply sums stem, prefix, and suffix embeddings together and sends the summed vector as the word-level r"
C18-1265,N18-1006,1,0.442297,"ation when the source side is an MRL. Models reviewed so far address the problem of morphology on the source side. In contrast, there is a group of models which study the same problem for the target side. Huck et al. (2017) compared different word-segmentation models, including linguistically motivated as well as statistical techniques, to find the most appropriate segmentation scheme when translating into MRLs. Chung et al. (2016) tried to design a suitable architecture when the target language is an MRL. They benefit from using a character-based decoder which partially resolves the problem. Passban et al. (2018) proposed a similar approach in which they equipped the character-based decoder with an additional morphology table to inform the decoder with the target language’s morphological structures. 1 Comparing translation results generated by different word-, subword-, and character-based architectures on different language pairs shows that there is no character-based model which is able to outperform its word- and/or subword-based counterparts. Please see results at http://matrix.statmt.org/?mode=all 3136 Apart from these models, there are others that do not directly address the problem of morpholog"
C18-1265,W16-2209,0,0.0436963,"Missing"
C18-1265,P16-1162,0,0.0855822,"e an architectural design (manipulation) inspired by the nature of this particular problem, which is able to decompose MCWs or process them in decomposed forms. Subword-level NMT models are the most preferred alternatives to translate from MRLs, which can be discussed in two main categories. One group of models does not change the neural architecture but manipulates data by decomposing words into their subunits. In this approach either linguistically motivated morphological analyzers such as Morfessor (Smit et al., 2014) or purely statistical models such as the byte-pair encoding (bpe) model (Sennrich et al., 2016) are applied to process words. This approach, by its very nature, seems to be a promising solution as it changes the sparse surface formbased vocabulary set into a much smaller set of fundamental subunits. The size of a set including atomic subunits, especially for MRLs, is considerably smaller than that of the vocabulary set. Moreover, this solution partially solves the out-of-vocabulary word problem, as the chance of facing an unknown surface form is much higher than the chance of facing an unknown subunit. The aforementioned approach could help generate better translations, but there is a p"
C18-1265,E14-2006,0,0.036409,"Missing"
C18-1265,N16-1004,0,0.0288903,"n in NMT. They evaluated the behaviour of an encoder-decoder model to see what sort of morphological information is learned via the model and how the model deals with complex structures. Passban (2018) extensively discussed the problem of morphology at the word and sequence level and proposed solutions for modeling and translating sequences in monolingual and bilingual settings. 3 Proposed Approach We propose an NMT architecture with a double-source encoder and double-attentive decoder for translating from MRLs. Our neural architecture is inspired by models proposed in Firat et al. (2016) and Zoph and Knight (2016). It takes inputs from two different channels: one channel which is referred to as the main channel sends stem information (main input), and the other one (the auxiliary channel) sends affix information. If the input is w0 , w1 , ..., wn for the (original) encoder-decoder model, our proposed architecture takes two sequences of ς0 , ς1 , ..., ςn and τ0 , τ1 , ..., τn through the main and auxiliary channels, respectively, where wi shows the surface form of a word whose stem is ςi , and affix information associated with wi is given by τi . Our new neural architecture is based on a hypothesis whic"
calixto-liu-2017-sentence,N15-1016,0,\N,Missing
calixto-liu-2017-sentence,P02-1040,0,\N,Missing
calixto-liu-2017-sentence,W14-3348,0,\N,Missing
calixto-liu-2017-sentence,P11-2031,0,\N,Missing
D07-1036,2005.eamt-1.19,0,0.531942,"the effectiveness of our online model optimization method. 5 Related work Most previous research on SMT training data is focused on parallel data collection. Some work tries to acquire parallel sentences from web (Nie et al. 1999; Resnik and Smith 2003; Chen et al. 2004). Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006). These work aims to collect more 349 parallel training corpora, while our work aims to make better use of existing parallel corpora. Some research has been conducted on parallel data selection and adaptation. Eck et al. (2005) propose a method to select more informative sentences based on n-gram coverage. They use ngrams to estimate the importance of a sentence. The more previously unseen n-grams in the sentence the more important the sentence is. TF-IDF weighting scheme is also tried in their method, but didn’t show improvements over n-grams. This method is independent of test data. Their goal is to decrease the amount of training data to make SMT system adaptable to small devices. Similar to our work, Hildebrand et al. (2005) also use information retrieval method for translation model adaptation. They select sent"
D07-1036,N03-1017,0,0.0144145,"Missing"
D07-1036,2006.iwslt-evaluation.15,0,0.100104,"Missing"
D07-1036,P02-1040,0,0.096074,". We random select 200,000 sentence pairs from each corpus and combine them together as the baseline corpus, which includes 16M Chinese words and 19M English words in total. This is the usual case when we train a SMT system, i.e. we simply combine all corpora from different origins to get a larger training corpus. We use the 2002 NIST MT evaluation test data as our development set, and the 2005 NIST MT test data as the test set in offline data optimization experiments. In both data, each sentence has four 4 human translations as references. The translation quality is evaluated by BLEU metric (Papineni et al., 2002), as calculated by mteval-v11b.pl 6 with case-sensitive matching of n-grams. HK_News LDC2004T08 Hong Kong News Text 200000 Baseline - All above data 600000 Table 1. Training corpora 4.2 Baseline experiments We first train translation models on each sub training corpus and the baseline corpus. The development set is used to tune the feature weights. The results on test set are shown in Table 2. System BLEU on dev set BLEU on test set FBIS 0.2614 0.2331 HK_Hansards 0.1679 0.1624 HK_News 0.1748 0.1608 Baseline 0.2565 0.2363 Table 2. Baseline results From the results we can see that although the s"
D07-1036,2005.mtsummit-papers.30,0,\N,Missing
D07-1036,C04-1059,0,\N,Missing
D07-1036,W06-1626,0,\N,Missing
D07-1036,J03-3002,0,\N,Missing
D07-1036,J05-4003,0,\N,Missing
D07-1036,P06-1011,0,\N,Missing
D07-1036,eck-etal-2004-language,0,\N,Missing
D07-1036,P03-1021,0,\N,Missing
D08-1010,2007.tmi-papers.6,0,0.021171,"work (He et al., 2008), we reported improvements by integrating a MERS model into a formally syntax-based SMT model, the hierarchical phrase-based model (Chiang, 2005). In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al., 2006). The basic differences are: NPB DEG X 2 :NN NN overall standard of the match Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased (Koehn, 2004) and a hierarchical phrasebased (Chiang, 2005) SMT system, respectively. Similar to WSD, Carpuat and Wu (2007a) used contextual information to solve the ambiguity problem for phrases. They integrated a phrase-sensedisambiguation (PSD) model into a phrase-based SMT system and achieved improvements. In this paper, we propose a novel solution for rule selection for syntax-based SMT. We use the maximum entropy approach to combine rich contextual information around a rule"
D08-1010,D07-1007,0,0.118445,"work (He et al., 2008), we reported improvements by integrating a MERS model into a formally syntax-based SMT model, the hierarchical phrase-based model (Chiang, 2005). In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al., 2006). The basic differences are: NPB DEG X 2 :NN NN overall standard of the match Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased (Koehn, 2004) and a hierarchical phrasebased (Chiang, 2005) SMT system, respectively. Similar to WSD, Carpuat and Wu (2007a) used contextual information to solve the ambiguity problem for phrases. They integrated a phrase-sensedisambiguation (PSD) model into a phrase-based SMT system and achieved improvements. In this paper, we propose a novel solution for rule selection for syntax-based SMT. We use the maximum entropy approach to combine rich contextual information around a rule"
D08-1010,P07-1005,0,0.196541,"reported improvements by integrating a MERS model into a formally syntax-based SMT model, the hierarchical phrase-based model (Chiang, 2005). In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al., 2006). The basic differences are: NPB DEG X 2 :NN NN overall standard of the match Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased (Koehn, 2004) and a hierarchical phrasebased (Chiang, 2005) SMT system, respectively. Similar to WSD, Carpuat and Wu (2007a) used contextual information to solve the ambiguity problem for phrases. They integrated a phrase-sensedisambiguation (PSD) model into a phrase-based SMT system and achieved improvements. In this paper, we propose a novel solution for rule selection for syntax-based SMT. We use the maximum entropy approach to combine rich contextual information around a rule and the information of s"
D08-1010,P05-1033,0,0.0940463,"he decoder hardly distinguish the two rules. Intuitively, information of sub-trees covered by nonterminals as well as contextual information of rules are believed 89 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89–97, c Honolulu, October 2008. 2008 Association for Computational Linguistics NP DNP X 1 :NP NP NPB DEG X 2 :NN DNP NN industrial products manufacturing levels X 1 :NP In our previous work (He et al., 2008), we reported improvements by integrating a MERS model into a formally syntax-based SMT model, the hierarchical phrase-based model (Chiang, 2005). In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al., 2006). The basic differences are: NPB DEG X 2 :NN NN overall standard of the match Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased (Koehn, 2004) and a hierarchica"
D08-1010,P05-1066,0,0.0340478,"to Liu et al. (2006). We perform minimum error rate training (Och, 2003) to tune the feature weights for the log-linear model to maximize the systems’s BLEU score on the development set. The weights are shown in Table 2. These weights are then used to run Lynx and Lynx+MERS on the test sets. Table 3 shows the results. Lynx obtains BLEU scores of 26.15 on NIST03 and 26.09 on NIST05. Using all features described in Section 3.2, Lynx+MERS finally obtains BLEU scores of 27.05 on NIST03 and 27.28 on NIST05. The absolute improvements is 0.90 and 1.19, respectively. Using the sign-test described by Collins et al. (2005), both improvements are statistically significant at p &lt; 0.01. Moreover, Lynx+MERS also achieves higher n-gram precisions than Lynx. Test Set NIST03 NIST05 System BLEU-4 Lynx +MERS Lynx +MERS 26.15 27.05 26.09 27.28 Individual n-gram precisions 1 2 3 4 71.62 35.64 18.64 9.82 72.00 36.72 19.51 10.37 70.39 35.12 18.53 10.11 71.16 36.19 19.62 10.95 Table 3: BLEU-4 scores (case-insensitive) on the test sets. 5.4 Analysis The baseline system only uses four features for rule selection: the translation probabilities P (e!|T!) and P (T!|e!); and the lexical weights Pw (e!|T!) and Pw (T!|e!). These fea"
D08-1010,P06-1121,0,0.0809525,"ion of sub-trees covered by variables in rules. Therefore, our model allows the decoder to perform context-dependent rule selection during decoding. We incorporate the MERS model into a state-of-the-art linguistically syntax-based SMT model, the treeto-string alignment template model. Experiments show that our approach achieves significant improvements over the baseline system. 1 NP X1 DEG NP NPB NN X2 X 1 X 2 levels NN DNP NP X1 DEG NPB NN NN X2 X 2 standard of X 1 Figure 1: Example of translation rules Introduction Syntax-based statistical machine translation (SMT) models (Liu et al., 2006; Galley et al., 2006; Huang et al., 2006) capture long distance reorderings by using rules with structural and linguistical information as translation knowledge. Typically, a translation rule consists of a source-side and a target-side. However, the source-side of a rule usually corresponds to multiple target-sides in multiple rules. Therefore, during decoding, the decoder should select a correct target-side for a source-side. We call this rule selection. Rule selection is of great importance to syntaxbased SMT systems. Comparing with word selection in word-based SMT and phrase selection in phrase-based SMT, rule"
D08-1010,C08-1041,1,0.381762,"or example, the information of X 1 and X 2 is not recorded when the rules in Figure 1 extracted from the training examples in Figure 2. This makes the decoder hardly distinguish the two rules. Intuitively, information of sub-trees covered by nonterminals as well as contextual information of rules are believed 89 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89–97, c Honolulu, October 2008. 2008 Association for Computational Linguistics NP DNP X 1 :NP NP NPB DEG X 2 :NN DNP NN industrial products manufacturing levels X 1 :NP In our previous work (He et al., 2008), we reported improvements by integrating a MERS model into a formally syntax-based SMT model, the hierarchical phrase-based model (Chiang, 2005). In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al., 2006). The basic differences are: NPB DEG X 2 :NN NN overall standard of the match Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b)"
D08-1010,2006.amta-papers.8,0,0.0215417,"red by variables in rules. Therefore, our model allows the decoder to perform context-dependent rule selection during decoding. We incorporate the MERS model into a state-of-the-art linguistically syntax-based SMT model, the treeto-string alignment template model. Experiments show that our approach achieves significant improvements over the baseline system. 1 NP X1 DEG NP NPB NN X2 X 1 X 2 levels NN DNP NP X1 DEG NPB NN NN X2 X 2 standard of X 1 Figure 1: Example of translation rules Introduction Syntax-based statistical machine translation (SMT) models (Liu et al., 2006; Galley et al., 2006; Huang et al., 2006) capture long distance reorderings by using rules with structural and linguistical information as translation knowledge. Typically, a translation rule consists of a source-side and a target-side. However, the source-side of a rule usually corresponds to multiple target-sides in multiple rules. Therefore, during decoding, the decoder should select a correct target-side for a source-side. We call this rule selection. Rule selection is of great importance to syntaxbased SMT systems. Comparing with word selection in word-based SMT and phrase selection in phrase-based SMT, rule selection is more ge"
D08-1010,N03-1017,0,0.083958,"Missing"
D08-1010,koen-2004-pharaoh,0,0.0449247,"ase-based model (Chiang, 2005). In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al., 2006). The basic differences are: NPB DEG X 2 :NN NN overall standard of the match Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased (Koehn, 2004) and a hierarchical phrasebased (Chiang, 2005) SMT system, respectively. Similar to WSD, Carpuat and Wu (2007a) used contextual information to solve the ambiguity problem for phrases. They integrated a phrase-sensedisambiguation (PSD) model into a phrase-based SMT system and achieved improvements. In this paper, we propose a novel solution for rule selection for syntax-based SMT. We use the maximum entropy approach to combine rich contextual information around a rule and the information of sub-trees covered by nonterminals in a rule. For each ambiguous source-side of translation rules, a maxim"
D08-1010,P06-1077,1,0.945521,"rules and information of sub-trees covered by variables in rules. Therefore, our model allows the decoder to perform context-dependent rule selection during decoding. We incorporate the MERS model into a state-of-the-art linguistically syntax-based SMT model, the treeto-string alignment template model. Experiments show that our approach achieves significant improvements over the baseline system. 1 NP X1 DEG NP NPB NN X2 X 1 X 2 levels NN DNP NP X1 DEG NPB NN NN X2 X 2 standard of X 1 Figure 1: Example of translation rules Introduction Syntax-based statistical machine translation (SMT) models (Liu et al., 2006; Galley et al., 2006; Huang et al., 2006) capture long distance reorderings by using rules with structural and linguistical information as translation knowledge. Typically, a translation rule consists of a source-side and a target-side. However, the source-side of a rule usually corresponds to multiple target-sides in multiple rules. Therefore, during decoding, the decoder should select a correct target-side for a source-side. We call this rule selection. Rule selection is of great importance to syntaxbased SMT systems. Comparing with word selection in word-based SMT and phrase selection in p"
D08-1010,P00-1056,0,0.236409,"Missing"
D08-1010,P02-1038,0,0.175438,"racted from the training example in Figure 3: lexicalized (the left), partially lexicalized (the middle), unlexicalized (the right). Lexicalized TAT contains only terminals, which is similar to phrase-to-phrase translation in phrase-based model except that it is constrained by a syntactic tree on the source-side. Partially lexicalized TAT contains both terminals and non-terminals, which can be used for both lexical translation and phrase reordering. Unlexicalized TAT contains only nonterminals and can only be used for phrase reordering. Lynx builds translation model in a log-linear framework (Och and Ney, 2002): (2) P (eI1 |T (f1J )) = &quot; exp[ m λm hm (eI1 , T (f1J ))] &quot; &quot; I J e! exp[ m λm hm (e1 , T (f1 ))] The incomes of city and village resident continued to grow Figure 3: Word-aligned, source-parsed training example. NN • Lexical weights: Pw (e!|T!) and Pw (T!|e!); • TAT penalty: exp(1), which is analogous to phrase penalty in phrase-based model; • Language model Plm (eI1 ); • Word penalty I. In Lynx, rule selection mainly depends on translation probabilities and lexical weights. These four scores describe how well a source tree links to a target string, which are estimated on the training corpus"
D08-1010,P03-1021,0,0.275965,"Missing"
D08-1010,P02-1040,0,0.0755056,"res lm1 lm2 0.171 0.013 0.152 0.014 TP -0.055 0.027 WP 0.403 0.270 Prs 0.194 AP 0.207 Table 2: Feature weights obtained by minimum error rate training on the development set. The first 8 features are used by Lynx. TP=TAT penalty, WP=word penalty, AP=ambiguous TAT penalty. Note that in fact, the positive weight for WP and AP indicate a reward. the training corpus and the Xinhua portion of the Gigaword corpus, respectively. NIST MT 2002 test set is used as the development set. NIST MT 2003 and NIST MT 2005 test sets are used as the test sets. The translation quality is evaluated by BLEU metric (Papineni et al., 2002), as calculated by mtevalv11b.pl with case-insensitive matching of n-grams, where n = 4. 5.2 Training To train the translation model, we first run GIZA++ (Och and Ney, 2000) to obtain word alignment in both translation directions. Then the word alignment is refined by performing “grow-diag-final” method (Koehn et al., 2003). We use a Chinese parser developed by Deyi Xiong (Xiong et al., 2005) to parse the Chinese sentences of the training corpus. Our TAT extraction algorithm is similar to Liu et al. (2006), except that we make some tiny modifications to extract contextual features for MERS mod"
D08-1010,I05-1007,1,0.881304,"Missing"
D09-1106,P06-1009,0,0.113398,"Missing"
D09-1106,J93-2003,0,0.0572883,"Missing"
D09-1106,C04-1032,0,0.0330266,"GIZA++ usually produces high quality 1-best alignments for closelyrelated European language pairs, especially when trained on millions of sentences. 6 Related Work Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al., 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and n-best alignments instead of 1-best alignments (Venugopal et al., 2008). We propose a compact representation of multiple word alignments that enables SMT systems to make a better use of noisy alignments. Matusov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We follow Och and Ney (2004) to develop a new phrase extraction algorithm for weighted alignment matrices. The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al., 2003) are also adapted for the weighted matrix case. Many researchers (e.g., (Venugopal et al."
D09-1106,J07-2003,0,0.033434,"tiple language pairs show that using weighted matrices achieves consistent improvements over using n-best lists in significant less extraction time. 1 Introduction Statistical machine translation (SMT) relies heavily on annotated bilingual corpora. Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2"
D09-1106,P08-1023,1,0.123632,"-insensitive BLEU scores of 1-best, 10-best, and m(10) on the Europarl data. Using weighted packed matrices continued to show advantage over using 1-best alignments on multiple language pairs. However, these improvements were very small and not significant. We attribute this to the fact that GIZA++ usually produces high quality 1-best alignments for closelyrelated European language pairs, especially when trained on millions of sentences. 6 Related Work Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al., 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and n-best alignments instead of 1-best alignments (Venugopal et al., 2008). We propose a compact representation of multiple word alignments that enables SMT systems to make a better use of noisy alignments. Matusov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We foll"
D09-1106,P05-1066,0,0.0301148,"Missing"
D09-1106,P06-1065,0,0.119801,"Missing"
D09-1106,P08-1010,0,0.346784,"Missing"
D09-1106,W08-0303,0,0.0176702,"weights (Koehn et al., 2003) are also adapted for the weighted matrix case. Many researchers (e.g., (Venugopal et al., 2003; Deng et al., 2008)) observe that softening the alignment consistency constraint help improve translation quality. For example, Deng et al. (2008) define a feature named “within phrase pair consistency ratio” to measure the degree of consistency. As each link is associated with a probability in a weighted matrix, we use these probabilities to evaluate the validity of a phrase pair. We estimate the link probabilities by calculating relative frequencies over n-best lists. Niehues and Vogel (2008) propose a discriminative approach to modeling the alignment matrix directly. The difference is that they assign a boolean value instead of a probability to each word pair. 7 Conclusion and Future Work We have presented a new structure called weighted alignment matrix that encodes the alignment distribution for a sentence pair. Accordingly, we develop new methods for extracting phrase pairs and estimating their probabilities. Our experiments show that the proposed approach achieves better translation quality over using n-best lists in less extraction time. An interesting finding is that our ap"
D09-1106,P08-1115,0,0.0558642,"oparl data. Using weighted packed matrices continued to show advantage over using 1-best alignments on multiple language pairs. However, these improvements were very small and not significant. We attribute this to the fact that GIZA++ usually produces high quality 1-best alignments for closelyrelated European language pairs, especially when trained on millions of sentences. 6 Related Work Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al., 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and n-best alignments instead of 1-best alignments (Venugopal et al., 2008). We propose a compact representation of multiple word alignments that enables SMT systems to make a better use of noisy alignments. Matusov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We follow Och and Ney (2004) to develop a new phrase extraction algorithm"
D09-1106,J07-3002,0,0.0312434,"Missing"
D09-1106,P06-1121,0,0.0474764,"pairs show that using weighted matrices achieves consistent improvements over using n-best lists in significant less extraction time. 1 Introduction Statistical machine translation (SMT) relies heavily on annotated bilingual corpora. Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2005; Liu et al., 2005"
D09-1106,N03-1017,0,0.0408612,"Missing"
D09-1106,P07-2045,0,0.00353568,"in the training data. We Then, we adapt Eq. (4) to calculate lexical obtained n-best lists by selecting the top n alignweight: ments from the 550-best lists. The probability of  |˜ e| each alignment in the n-best list was re-estimated Y 1 e|f˜, m) = pw (˜ × by re-normalization (Venugopal et al., 2008). Fi{j|pm (j, i) &gt; 0} i=1 nally, these n-best alignments served as samples  X p(ei |fj ) × pm (j, i) + for constructing weighted alignment matrices. After extracting phrase pairs from n-best lists ∀j:pm (j,i)&gt;0 and weighted alignment matrices, we ran Moses ! |f˜| Y p(ei |f0 ) × p¯m (j, i) (16) (Koehn et al., 2007) to translate the development and test sets. We used the simple distance-based j=1 reordering model to remove the dependency of For example, for the target word “of” in Figure lexicalization on word alignments for Moses. 4, the sum of aligned and unaligned probabilities is 5.2 Effect of Pruning Threshold 1 × (p(of|de) × 0.6 + p(of|fazhan) × 0.4) + Our first experiment investigated the effect of 2 p(of|NULL) × 0.24 pruning threshold on translation quality (BLEU scores on the test set) and the phrase table size (filNote that we take link probabilities into account tered for the test set), as sho"
D09-1106,D07-1005,0,0.0209405,"language pairs, especially when trained on millions of sentences. 6 Related Work Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al., 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and n-best alignments instead of 1-best alignments (Venugopal et al., 2008). We propose a compact representation of multiple word alignments that enables SMT systems to make a better use of noisy alignments. Matusov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We follow Och and Ney (2004) to develop a new phrase extraction algorithm for weighted alignment matrices. The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al., 2003) are also adapted for the weighted matrix case. Many researchers (e.g., (Venugopal et al., 2003; Deng et al., 2008)) observe that softening the alignment consistency con"
D09-1106,P05-1057,1,0.854198,"Missing"
D09-1106,P06-1077,1,0.338522,"achieves consistent improvements over using n-best lists in significant less extraction time. 1 Introduction Statistical machine translation (SMT) relies heavily on annotated bilingual corpora. Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2005; Liu et al., 2005; Blunsom and Cohn, 2006)) have the fr"
D09-1106,J03-1002,0,0.0181765,"Missing"
D09-1106,J04-4002,0,0.735347,"ted alignment matrices and estimating their probabilities. Our experiments on multiple language pairs show that using weighted matrices achieves consistent improvements over using n-best lists in significant less extraction time. 1 Introduction Statistical machine translation (SMT) relies heavily on annotated bilingual corpora. Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In c"
D09-1106,P02-1040,0,0.110018,"+ 8.9M words) as the training data. For language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD cor4.3 Calculating Lexical Weights pus. We used the NIST 2002 MT evaluation test Recall that we need to obtain two translation probset as our development set, and used the NIST ability tables w(e|f ) and w(f |e) before calculat2005 test set as our test set. We evaluated the transing lexical weights (see Section 2). Following lation quality using case-insensitive BLEU metric Koehn et al. (2003), we estimate the two distribu(Papineni et al., 2002). tions by relative frequencies from the training corTo obtain weighted alignment matrices, we folpus annotated with weighted alignment matrices. lowed Venugopal et al. (2008) to produce nIn other words, we still use Eq. (3) but the way of best lists via GIZA++. We first ran GIZA++ calculating fractional counts is different now. to produce 50-best lists in two translation direcGiven a source word fj , a target word ei , and tions. Then, we used the refinement technique a weighted alignment matrix, the fractional count “grow-diag-final-and” (Koehn et al., 2003) to all count(fj , ei ) is pm (j,"
D09-1106,P08-1066,0,0.0232111,"Missing"
D09-1106,H05-1010,0,0.123168,"Missing"
D09-1106,P03-1041,0,0.4864,"ov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We follow Och and Ney (2004) to develop a new phrase extraction algorithm for weighted alignment matrices. The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al., 2003) are also adapted for the weighted matrix case. Many researchers (e.g., (Venugopal et al., 2003; Deng et al., 2008)) observe that softening the alignment consistency constraint help improve translation quality. For example, Deng et al. (2008) define a feature named “within phrase pair consistency ratio” to measure the degree of consistency. As each link is associated with a probability in a weighted matrix, we use these probabilities to evaluate the validity of a phrase pair. We estimate the link probabilities by calculating relative frequencies over n-best lists. Niehues and Vogel (2008) propose a discriminative approach to modeling the alignment matrix directly. The difference is that"
D09-1106,2008.amta-papers.18,0,0.505656,"ical weight can be calculated as pw (˜ e|f˜, a ˜) = |˜ e| Y X 1 w(ei |fj ) (4) |{j|(j, i) ∈ a ˜}| i=1 If there are multiple alignments a ˜ for a phrase ˜ pair (f , e˜), Koehn et al. (2003) choose the one with the highest lexical weight: n o pw (˜ e|f˜) = max pw (˜ e|f˜, a ˜) a ˜ (5) Simple and effective, relative frequencies and lexical weights have become the standard features in modern discriminative SMT systems. 3 Weighted Alignment Matrix We believe that offering more candidate alignments to extracting translation rules might help improve translation quality. Instead of using nbest lists (Venugopal et al., 2008), we propose a new structure called weighted alignment matrix. We use an example to illustrate our idea. Figure 2(a) and Figure 2(b) show two alignments of a Chinese-English sentence pair. We observe that some links (e.g., (1,4) corresponding to the word 1018 (b) fazhan fazhan (a) zhongguo de jingji of development the fazhan of development the economy 0 0 1.0 0 ’s 0 0.4 0.4 0 China 1.0 0 0 0 of 0 0.6 0 0.4 development 0 0 0 1.0 the 0 0 0 0 zhongguo de jingji economy ’s China zhongguo de jingji economy ’s China (c) Figure 2: (a) One alignment of a sentence pair; (b) another alignment of the sam"
D09-1106,C96-2141,0,0.611347,"Missing"
D09-1115,P08-1115,0,0.0751179,"eels like apples He prefer lations. Note that the phrase “is fond of” is attached to an edge. Now, it is unlikely to obtain a translation like “He is like of apples”. A lattice G = hV, Ei is a directed acyclic graph, formally a weighted finite state automation (FSA), where V is the set of nodes and E is the set of edges. The nodes in a lattice are usually labeled according to an appropriate numbering to reflect how to produce a translation. Each edge in a lattice is attached with a sequence of words as well as the associated probability. As lattice is a more general form of confusion network (Dyer et al., 2008), we expect that replacing confusion networks with lattices will further improve system combination. He feels like apples apples He prefer apples He feels like apples He feels like apples He is fond of apples He is fond of apples (a) unidirectional alignments (b) bidirectional alignments He ε feels prefer like of ε apples is fond (c) confusion network he ε feels prefer like apples is fond of (d) lattice 2.2 Figure 1: Comparison of a confusion network and a lattice. 2 Background 2.1 Confusion Network and Lattice We use an example shown in Figure 1 to illustrate our idea. Suppose that there are"
D09-1115,A94-1016,0,0.202214,"in which several words are connected to another several words. Instead, we propose a lattice-based system combination model that allows for such phrase alignments and uses lattices to encode all candidate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way t"
D09-1115,D08-1011,0,0.510095,"s significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more common that several words are connected to another several words. For example, “be capable of” and “"
D09-1115,W07-0711,0,0.0521771,"s were collected as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s method to maximum BLEU score. The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996). In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, ρ = 3; the controlling factor for the distortion model, K = 2. 6.1 Comparison with Confusion-network-based model In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008). In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS data set whi"
D09-1115,W05-1506,0,0.0415363,"hich store more than one non-null word. The above two numbers are gotten by counting both backbone arcs and hypothesis arcs. α and β are the corresponding weights of the numbers, respectively. Nword (e) is the non-null word number and γ is its weight. Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps (arc). ps (arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008). Cube pruning algorithm with beam search is employed to search for the consensus output (Huang and Chiang, 2005). The nodes in the lattice are searched in a topological order and each node retains a list of N best candidate partial translations. 6 Experiments The candidate systems participating in the system combination are as listed in Table 1: System A is a BTG-based system using a MaxEnt-based reordering model; System B is a hierarchical phrase-based system; System C is the Moses decoder (Koehn et al., 2007); System D is a syntax-based system. 10best hypotheses from each candidate system on the dev and test sets were collected as the input of the system combination. In our experiments, the weights we"
D09-1115,N03-1017,0,0.027572,"he hypothesis with the minimum cost of edits against all hypotheses is selected. The backbone is significant for it influences not only the word order, but also the following alignments. The backbone is selected as follows: EB = argmin E ′ ∈E X T ER(E ′ , E) (8) E∈E (3) Get the alignments of the backbone and hypothesis pairs. First, each pair is aligned in both directions using the IHMM-based alignment method. In the IHMM alignment model, bilingual dictionaries in both directions are indispensable. Then, we apply a grow-diag-final algorithm which is widely used in bilingual phrase extraction (Koehn et al., 2003) to monolingual alignments. The bidirectional alignments are combined to one resorting to the grow-diag-final algorithm, allowing n-to-n mappings. (4)Normalize the alignment pairs. The word order of the backbone determines the word order of consensus outputs, so the word order of hypotheses must be consistent with that of the backbone. All words of a hypotheses are reordered according to the alignment to the backbone. For a word aligned to null, an actual null word may be inserted to the proper position. The alignment units are extracted first and then the hypothesis words in each unit are shi"
D09-1115,P07-2045,0,0.00971279,"1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008). Cube pruning algorithm with beam search is employed to search for the consensus output (Huang and Chiang, 2005). The nodes in the lattice are searched in a topological order and each node retains a list of N best candidate partial translations. 6 Experiments The candidate systems participating in the system combination are as listed in Table 1: System A is a BTG-based system using a MaxEnt-based reordering model; System B is a hierarchical phrase-based system; System C is the Moses decoder (Koehn et al., 2007); System D is a syntax-based system. 10best hypotheses from each candidate system on the dev and test sets were collected as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s meth"
D09-1115,E06-1005,0,0.230719,"nts and uses lattices to encode all candidate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more co"
D09-1115,P07-1040,0,0.376587,"idate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more common that several words are connected"
D09-1115,N07-1029,0,0.250842,"ere collected as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s method to maximum BLEU score. The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996). In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, ρ = 3; the controlling factor for the distortion model, K = 2. 6.1 Comparison with Confusion-network-based model In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008). In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS data set whi"
D09-1115,W08-0329,0,0.156101,"our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more common that several words are connected to another several words. For example, “be"
D09-1115,C96-2141,0,0.491588,"ed as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s method to maximum BLEU score. The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996). In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, ρ = 3; the controlling factor for the distortion model, K = 2. 6.1 Comparison with Confusion-network-based model In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008). In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS data set which included 289K sentenc"
D09-1127,D08-1092,0,0.679778,"not all languages are ambiguous in the same way. For example, prepositional phrase (PP) attachment is (notoriously) ambiguous in English (and related European languages), but is strictly unambiguous in Chinese and largely unambiguous Japanese; see Figure 1 for an example.1 It is thus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1 Chinese uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” attachment ambiguities like in"
D09-1127,P04-1015,0,0.11193,"da V do 7: for act ∈ {shift, reduceL , reduceR } do 8: if act is applicable to config then 9: next ← apply act to config 10: insert next into buffer BUF 11: V ← top k configurations of BUF 12: Output: the tree of the best config in V c(st−1 , st ) =+; reduce is correct (b) saw Bill ::::::::: with a telescope . c(st−1 , st ) =−; reduce is wrong I saw ::::::::::: Bill with:::a:::::::::: telescope: . wo kandao le na wangyuanjin de Bi’er. cR (st , wi ) =+; shift is correct (d) We use the popular online learning algorithm of structured perceptron with parameter averaging (Collins, 2002). Following Collins and Roark (2004) we also use the “early-update” strategy, where an update happens whenever the goldstandard action-sequence falls off the beam, with the rest of the sequence neglected. As a special case, for the deterministic mode, updates always co-occur with the first mistake made. The intuition behind this strategy is that future mistakes are often caused by previous ones, so with the parser on the wrong track, future actions become irrelevant for learning. See Section 5.3 for more discussions. I wo kandao le na wangyuanjin de Bi’er. into the agenda for the next step. The complexity of this algorithm is O("
D09-1127,P05-1066,0,0.0290422,"Missing"
D09-1127,W02-1001,0,0.00516533,"6: for each config in agenda V do 7: for act ∈ {shift, reduceL , reduceR } do 8: if act is applicable to config then 9: next ← apply act to config 10: insert next into buffer BUF 11: V ← top k configurations of BUF 12: Output: the tree of the best config in V c(st−1 , st ) =+; reduce is correct (b) saw Bill ::::::::: with a telescope . c(st−1 , st ) =−; reduce is wrong I saw ::::::::::: Bill with:::a:::::::::: telescope: . wo kandao le na wangyuanjin de Bi’er. cR (st , wi ) =+; shift is correct (d) We use the popular online learning algorithm of structured perceptron with parameter averaging (Collins, 2002). Following Collins and Roark (2004) we also use the “early-update” strategy, where an update happens whenever the goldstandard action-sequence falls off the beam, with the rest of the sequence neglected. As a special case, for the deterministic mode, updates always co-occur with the first mistake made. The intuition behind this strategy is that future mistakes are often caused by previous ones, so with the parser on the wrong track, future actions become irrelevant for learning. See Section 5.3 for more discussions. I wo kandao le na wangyuanjin de Bi’er. into the agenda for the next step. Th"
D09-1127,P03-2041,0,0.0527296,"ch Marcus, p.c.). 1222 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP forcing existing approaches to employ complicated modeling and crude approximations. Joint parsing with a simplest synchronous context-free grammar (Wu, 1997) is O(n6 ) as opposed to the monolingual O(n3 ) time. To make things worse, languages are non-isomorphic, i.e., there is no 1to-1 mapping between tree nodes, thus in practice one has to use more expressive formalisms such as synchronous tree-substitution grammars (Eisner, 2003; Galley et al., 2004). In fact, rather than joint parsing per se, Burkett and Klein (2008) resort to separate monolingual parsing and bilingual reranking over k 2 tree pairs, which covers a tiny fraction of the whole space (Huang, 2008). We instead propose a much simpler alternative, bilingually-constrained monolingual parsing, where a source-language parser is extended to exploit the reorderings between languages as additional observation, but not bothering to build a tree for the target side simultaneously. To illustrate the idea, suppose we are parsing the sentence straightforward to imple"
D09-1127,2008.amta-srw.2,0,0.188831,"e at hand.” Figure 1: PP-attachment is unambiguous in Chinese, which can help English parsing. Introduction Ambiguity resolution is a central task in Natural Language Processing. Interestingly, not all languages are ambiguous in the same way. For example, prepositional phrase (PP) attachment is (notoriously) ambiguous in English (and related European languages), but is strictly unambiguous in Chinese and largely unambiguous Japanese; see Figure 1 for an example.1 It is thus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1 Chinese"
D09-1127,N04-1035,0,0.00750622,"hus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1 Chinese uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” attachment ambiguities like in Figure 1 (see (Schwartz et al., 2003)) but not for the “N1 or N2 ” case (Mitch Marcus, p.c.). 1222 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP forcing existing approa"
D09-1127,P05-1012,0,0.0881665,"Missing"
D09-1127,D08-1022,1,0.400432,"two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1 Chinese uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” attachment ambiguities like in Figure 1 (see (Schwartz et al., 2003)) but not for the “N1 or N2 ” case (Mitch Marcus, p.c.). 1222 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP forcing existing approaches to employ compli"
D09-1127,C04-1010,0,0.023255,"shapes with crossing brackets (for example, [saw Bill] vs. [Bill with a telescope]). We will see in Section 5.3 that this is indeed the case in practice, thus suggesting us to focus on shift-reduce resolution, which we will return to with the help of bilingual constraints in Section 3. 3. the “arc-standard” scan always succeeds, since at the end we can always reduce with empty queue, whereas the “arc-eager” style sometimes goes into deadends where no action can perform (prevented by preconditions, otherwise the result will not be a wellformed tree). This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack. 2.3 Comparison with Arc-Eager The three action system was originally described by Yamada and Matsumoto (2003) (although their methods require multiple passes over the input), and then appeared as “arc-standard” in Nivre (2004), but was argued against in comparison to the four-action “arc-eager” variant. Most subsequent works on shift-reduce or “transition-based” dependency parsing followed “arc-eager” (Nivre and Scholz, 2004; Zhang and Clark, 2008), which now becomes the dominant style. But we argue that “arc-standard” is preferable because: 1. in th"
D09-1127,W04-0308,0,0.162902,"ependency Parsing with Three Actions The basic idea of classical shift-reduce parsing from compiler theory (Aho and Ullman, 1972) is to perform a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items on the stack, replacing them with their combination. This idea has been applied to constituency parsing, for example in Sagae and Lavie (2006), and we describe below a simple variant for dependency parsing similar to Yamada and Matsumoto (2003) and the “arc-standard” version of Nivre (2004). 2.1 The Three Actions Basically, we just need to split the reduce action into two symmetric (sub-)actions, reduceL and reduceR , depending on which one of the two 1223 previous shift previous reduceL reduceR stack S S|wi S|st−1 |st S|st S|st−1 queue wi |Q Q Q Q Q 0 1 2 3 arcs A A A A ∪ {(st , st−1 )} A ∪ {(st−1 , st )} 4 2. reduceL : combine the top two items on the stack, st and st−1 (t ≥ 2), and replace them with st (as the head), and add a left arc (st , st−1 ) to A; 3. reduceR : combine the top two items on the stack, st and st−1 (t ≥ 2), and replace them with st−1 (as the head), and add"
D09-1127,J08-4003,0,0.114273,"Missing"
D09-1127,N07-1051,0,0.0178172,"Missing"
D09-1127,P06-2089,0,0.0462942,"s translated into a Chinese relative clause, but nevertheless all phrasal modifiers attach to the immediate right in Mandarin Chinese. 2 Simpler Shift-Reduce Dependency Parsing with Three Actions The basic idea of classical shift-reduce parsing from compiler theory (Aho and Ullman, 1972) is to perform a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items on the stack, replacing them with their combination. This idea has been applied to constituency parsing, for example in Sagae and Lavie (2006), and we describe below a simple variant for dependency parsing similar to Yamada and Matsumoto (2003) and the “arc-standard” version of Nivre (2004). 2.1 The Three Actions Basically, we just need to split the reduce action into two symmetric (sub-)actions, reduceL and reduceR , depending on which one of the two 1223 previous shift previous reduceL reduceR stack S S|wi S|st−1 |st S|st S|st−1 queue wi |Q Q Q Q Q 0 1 2 3 arcs A A A A ∪ {(st , st−1 )} A ∪ {(st−1 , st )} 4 2. reduceL : combine the top two items on the stack, st and st−1 (t ≥ 2), and replace them with st (as the head), and add a le"
D09-1127,2003.mtsummit-papers.44,0,0.0549386,"attachment is unambiguous in Chinese, which can help English parsing. Introduction Ambiguity resolution is a central task in Natural Language Processing. Interestingly, not all languages are ambiguous in the same way. For example, prepositional phrase (PP) attachment is (notoriously) ambiguous in English (and related European languages), but is strictly unambiguous in Chinese and largely unambiguous Japanese; see Figure 1 for an example.1 It is thus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1 Chinese uses word-order to disam"
D09-1127,D09-1086,0,0.0420661,". This is because the extra work, with just 3 bilingual features, is not the bottleneck in practice, since the extraction of the vast amount of other features in Table 2 dominates the computation. 4 Related Work in Grammar Induction Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing). For example, Hwa et al. (2005) use simple heuristics to project English trees to Spanish and Chinese, but get discouraging accuracy results learned from those projected trees. Following this idea, Ganchev et al. (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. Our work, by constrast, never uses bilingual tree pairs not tree projections, and only uses word alignment alone to enhance a monolingual grammar, which learns to prefer target-side contiguity. 5 Experiments 5.1 Baseline Parser We implement our baseline monolingual parser (in C++) based on the shift-reduce algorithm in Section 2, with feature templates from Table 2. We evaluate its performance on the standard Penn English Treebank (PTB) dependency par"
D09-1127,W04-3207,0,0.147563,"Processing. Interestingly, not all languages are ambiguous in the same way. For example, prepositional phrase (PP) attachment is (notoriously) ambiguous in English (and related European languages), but is strictly unambiguous in Chinese and largely unambiguous Japanese; see Figure 1 for an example.1 It is thus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1 Chinese uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” at"
D09-1127,J97-3002,0,0.204352,"se uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” attachment ambiguities like in Figure 1 (see (Schwartz et al., 2003)) but not for the “N1 or N2 ” case (Mitch Marcus, p.c.). 1222 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP forcing existing approaches to employ complicated modeling and crude approximations. Joint parsing with a simplest synchronous context-free grammar (Wu, 1997) is O(n6 ) as opposed to the monolingual O(n3 ) time. To make things worse, languages are non-isomorphic, i.e., there is no 1to-1 mapping between tree nodes, thus in practice one has to use more expressive formalisms such as synchronous tree-substitution grammars (Eisner, 2003; Galley et al., 2004). In fact, rather than joint parsing per se, Burkett and Klein (2008) resort to separate monolingual parsing and bilingual reranking over k 2 tree pairs, which covers a tiny fraction of the whole space (Huang, 2008). We instead propose a much simpler alternative, bilingually-constrained monolingual p"
D09-1127,P09-1042,0,0.030299,"the new bilingual features. This is because the extra work, with just 3 bilingual features, is not the bottleneck in practice, since the extraction of the vast amount of other features in Table 2 dominates the computation. 4 Related Work in Grammar Induction Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing). For example, Hwa et al. (2005) use simple heuristics to project English trees to Spanish and Chinese, but get discouraging accuracy results learned from those projected trees. Following this idea, Ganchev et al. (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. Our work, by constrast, never uses bilingual tree pairs not tree projections, and only uses word alignment alone to enhance a monolingual grammar, which learns to prefer target-side contiguity. 5 Experiments 5.1 Baseline Parser We implement our baseline monolingual parser (in C++) based on the shift-reduce algorithm in Section 2, with feature templates from Table 2. We evaluate its performance on the standard Penn English T"
D09-1127,C02-1145,0,0.00973013,"and Clark (2008) our baseline at k=1 our baseline at k=16 accuracy 90.7 91.4 90.2 91.3 secs/sent 0.150 0.195 0.009 0.125 Table 4: Baseline parser performance on standard Penn English Treebank dependency parsing task. The speed numbers are not exactly comparable since they are reported on different machines. CTB Articles Bilingual Paris Training 1-270 2745 Dev 301-325 273 Test 271-300 290 Table 5: Training, dev, and test sets from bilingual Chinese Treebank a` la Burkett and Klein (2008). 5.2 Bilingual Data The bilingual data we use is the translated portion of the Penn Chinese Treebank (CTB) (Xue et al., 2002), corresponding to articles 1-325 of PTB, which have English translations with goldstandard parse trees (Bies et al., 2007). Table 5 shows the split of this data into training, development, and test subsets according to Burkett and Klein (2008). Note that not all sentence pairs could be included, since many of them are not oneto-one aligned at the sentence level. Our wordalignments are generated from the HMM aligner of Liang et al. (2006) trained on approximately 1.7M sentence pairs (provided to us by David Burkett, p.c.). This aligner outputs “soft alignments”, i.e., posterior probabilities f"
D09-1127,P08-1067,1,0.203752,"guages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1 Chinese uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” attachment ambiguities like in Figure 1 (see (Schwartz et al., 2003)) but not for the “N1 or N2 ” case (Mitch Marcus, p.c.). 1222 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP forcing existing approaches to employ compli"
D09-1127,W03-3023,0,0.377869,"immediate right in Mandarin Chinese. 2 Simpler Shift-Reduce Dependency Parsing with Three Actions The basic idea of classical shift-reduce parsing from compiler theory (Aho and Ullman, 1972) is to perform a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items on the stack, replacing them with their combination. This idea has been applied to constituency parsing, for example in Sagae and Lavie (2006), and we describe below a simple variant for dependency parsing similar to Yamada and Matsumoto (2003) and the “arc-standard” version of Nivre (2004). 2.1 The Three Actions Basically, we just need to split the reduce action into two symmetric (sub-)actions, reduceL and reduceR , depending on which one of the two 1223 previous shift previous reduceL reduceR stack S S|wi S|st−1 |st S|st S|st−1 queue wi |Q Q Q Q Q 0 1 2 3 arcs A A A A ∪ {(st , st−1 )} A ∪ {(st−1 , st )} 4 2. reduceL : combine the top two items on the stack, st and st−1 (t ≥ 2), and replace them with st (as the head), and add a left arc (st , st−1 ) to A; 3. reduceR : combine the top two items on the stack, st and st−1 (t ≥ 2), an"
D09-1127,D08-1059,0,0.578158,"guration is hwj , ∅, Ai where wj is recognized as the root of the whole sentence, and A encodes a spanning tree rooted at wj . For a sentence of n words, there are exactly 2n − 1 actions: n shifts and n − 1 reductions, since every word must be pushed onto stack once, and every word except the root will eventually be popped in a reduction. The time complexity, as other shift-reduce instances, is clearly O(n). 2.2 Example of Shift-Reduce Conflict Figure 2 shows the trace of this paradigm on the example sentence. For the first two configurations 3 a “configuration” is sometimes called a “state” (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different. saw saw saw saw Bill Bill Bill Bill with with with with a ... a ... a ... a ... shift saw Bill with a ... with a ... with a ... I 5a reduceR saw I 5b 1. shift: move the head of (a non-empty) queue Q onto stack S; I I I I Table 1: Formal description of the three actions. Note that shift requires non-empty queue while reduce requires at least two elements on the stack. items becomes the head after reduction. More formally, we describe a parser configuration by a tuple hS, Q, Ai where S is the s"
D09-1127,P06-1096,0,0.0276256,"Missing"
D09-1127,J03-4003,0,\N,Missing
D11-1020,P05-1033,0,0.925102,"nd has the best inter-lingual phrasal cohesion properties (Fox, 2002). Those attractive characteristics make it possible to improve translation quality by using dependency structures. Some researchers pay more attention to use dependency structure on the target side. (Shen et al., 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristi"
D11-1020,J07-2003,0,0.700046,"the head-dependents relation rooted at n, and checks the rule set for matched translation rules. If there is no matched rule, we construct a pseudo translation rule according to the word order of the head-dependents relation. For example, suppose that we can not find any translation rule about to “(2010年) (FIFA) 世 界杯”, we will construct a pseudo translation rule “(x1 :2010年) (x2 :FIFA) x3 :世界杯 → x1 x2 x3 ”. A larger translation is generated by substituting the variables in the target side of a translation rule with the translations of the corresponding dependents. We make use of cube pruning (Chiang, 2007; Huang and Chiang, 2007) to find the k-best items with integrated language model for each node. To balance performance and speed, we prune the search space in several ways. First, beam threshold β , items with a score worse than β times of the best score in the same cell will be discarded; second, beam size b, items with a score worse than the bth best item in the same cell will be discarded. The item consist of the necessary information used in decoding. Each cell contains all the items standing for the subtree rooted at it. For our experiments, we set β = 10−3 and b = 300. Additionally, we"
D11-1020,P05-1067,0,0.56847,"the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristics (Lin, 2004; Xiong et al., 2007) or separate ordering models (Quirk et al., 2005; Ding and Palmer, 2005) to control the word order of translations. In this paper, we handle this problem by directly specifying the ordering information in headdependents rules that represent the source side as head-dependents relations and the target side as string."
D11-1020,W02-1039,0,0.14595,"g distance reordering, and outperforms the stateof-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models. 1 Introduction Dependency structure represents the grammatical relations that hold between the words in a sentence. It encodes semantic relations directly, and has the best inter-lingual phrasal cohesion properties (Fox, 2002). Those attractive characteristics make it possible to improve translation quality by using dependency structures. Some researchers pay more attention to use dependency structure on the target side. (Shen et al., 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cub"
D11-1020,P06-1121,0,0.45011,"., 2002).1 We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maximize the system’s BLEU score on the development set. 1 System cons2str hiero-re dep2str ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 223 We take a replication of Hiero (Chiang, 2007) as the hierarchical phrase-based model baseline. In our experiments of this paper, we set the beam size b = 200 and the beam threshold β = 0. The maximum initial phrase length is 10. We use constituency-to-string model (Liu et al., 2006) as the syntax-based model baseline which make use of composed rules (Galley et al., 2006) without handling the unaligned words. In our experiments of this paper, we set the tatTable-limit=20, tatTable-threshold=10−1 , stack-limit=100, stackthreshold=10−1 ,hight-limit=3, and length-limit=7. 7.3 Results We display the results of our experiments in Table 1. Our dependency-to-string model dep2str significantly outperforms its constituency structure-based counterpart (cons2str) with +1.27 and +1.68 BLEU on MT04 and MT05 respectively. Moreover, without resort to phrases or parse forest, dep2str surpasses the hierarchical phrase-based model (hierore) over +0.53 and +0.4 BLEU on MT04 and"
D11-1020,P07-1019,0,0.0860775,"dents relation rooted at n, and checks the rule set for matched translation rules. If there is no matched rule, we construct a pseudo translation rule according to the word order of the head-dependents relation. For example, suppose that we can not find any translation rule about to “(2010年) (FIFA) 世 界杯”, we will construct a pseudo translation rule “(x1 :2010年) (x2 :FIFA) x3 :世界杯 → x1 x2 x3 ”. A larger translation is generated by substituting the variables in the target side of a translation rule with the translations of the corresponding dependents. We make use of cube pruning (Chiang, 2007; Huang and Chiang, 2007) to find the k-best items with integrated language model for each node. To balance performance and speed, we prune the search space in several ways. First, beam threshold β , items with a score worse than β times of the best score in the same cell will be discarded; second, beam size b, items with a score worse than the bth best item in the same cell will be discarded. The item consist of the necessary information used in decoding. Each cell contains all the items standing for the subtree rooted at it. For our experiments, we set β = 10−3 and b = 300. Additionally, we also prune rules that hav"
D11-1020,W06-3601,0,0.136216,"ttractive characteristics make it possible to improve translation quality by using dependency structures. Some researchers pay more attention to use dependency structure on the target side. (Shen et al., 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristics (Lin, 2004; Xiong et al., 2007) or separate ordering models (Quirk et al., 2005;"
D11-1020,N03-1017,0,0.0607523,"Missing"
D11-1020,C04-1090,0,0.850261,"endency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristics (Lin, 2004; Xiong et al., 2007) or separate ordering models (Quirk et al., 2005; Ding and Palmer, 2005) to control the word order of translations. In this paper, we handle this problem by directly specifying the ordering information in headdependents rules that represent the source side as head-dependent"
D11-1020,P06-1077,1,0.876701,"of translations is evaluated by the case insensitive NIST BLEU-4 metric (Papineni et al., 2002).1 We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maximize the system’s BLEU score on the development set. 1 System cons2str hiero-re dep2str ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 223 We take a replication of Hiero (Chiang, 2007) as the hierarchical phrase-based model baseline. In our experiments of this paper, we set the beam size b = 200 and the beam threshold β = 0. The maximum initial phrase length is 10. We use constituency-to-string model (Liu et al., 2006) as the syntax-based model baseline which make use of composed rules (Galley et al., 2006) without handling the unaligned words. In our experiments of this paper, we set the tatTable-limit=20, tatTable-threshold=10−1 , stack-limit=100, stackthreshold=10−1 ,hight-limit=3, and length-limit=7. 7.3 Results We display the results of our experiments in Table 1. Our dependency-to-string model dep2str significantly outperforms its constituency structure-based counterpart (cons2str) with +1.27 and +1.68 BLEU on MT04 and MT05 respectively. Moreover, without resort to phrases or parse forest, dep2str sur"
D11-1020,P02-1038,0,0.26238,"lized and unlexicalized head-dependents rules from f 10 append the induced rules to R 11 end 12 end spans simultaneously. In this process, we might obtain m(m ≥ 1) head-dependents rules from a headdependent fragment in handling unaligned words. Each of these rules is assigned with a fractional count 1/m. 4.4 Algorithm for Rule Acquisition The rule acquisition is a three-step process, which is summarized in Algorithm 1. We take the extracted rule set as observed data and make use of relative frequency estimator to obtain the translation probabilities P (t|s) and P (s|t). 5 The model Following (Och and Ney, 2002), we adopt a general log-linear model. Let d be a derivation that convert a source dependency structure T into a target string e. The probability of d is defined as: P (d) ∝ ∏ ϕi (d)λi (1) i where ϕi are features defined on derivations and λi are feature weights. In our experiments of this paper, we used seven features as follows: - translation probabilities P (t|s) and P (s|t); - lexical translation probabilities Plex (t|s) and Plex (s|t); - rule penalty exp(−1); 222 - language model Plm (e); - word penalty exp(|e|). 6 Decoding Our decoder is based on bottom up chart parsing. It finds the bes"
D11-1020,J03-1002,0,0.00889922,"of the hierarchical phrase-based model and the tree-tostring models on Chinese-English translation. 7.1 Data preparation Our training corpus consists of 1.5M sentence pairs from LDC data, including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. We parse the source sentences with Stanford Parser (Klein and Manning, 2003) into projective dependency structures, whose nodes are annotated by POS tags and edges by typed dependencies. In our implementation of this paper, we make use of the POS tags only. We obtain the word alignments by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and applying “grow-diag-and” refinement(Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We use NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test set 2004 (MT04) and 2005 (MT05) as our test set. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric (Papineni et al., 2002).1 We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maxi"
D11-1020,J04-4002,0,0.852881,"licity, we use {1-5} to denotes the contiguous span {1, 2, 3, 4, 5}. ′ Definition 4. Given a subtree T rooted at n, the dependency span dsp(n) of n is defined as: ∪ dsp(n) = cloz( hsp(n′ )). ′ n′ ∈T hsp(n′ ) is consistent ′ If the head spans of all the nodes of T is not consistent, dsp(n) = ∅. For example, since hsp(在) is not consistent, dsp(在)=dsp(南非)={9, 10}, which corresponds to the target words “South” and “Africa”. The tree annotation can be accomplished by a single postorder transversal of T . The extraction of head rules from each node can be readily achieved with the same criteria as (Och and Ney, 2004). In the following, we focus on head-dependents rules acquisition. Ѯ㹼/VV {6}{2-10} (a) 4.2 Head-Dependents Fragments Identification ᡀ/AD /P ц⭼ᶟ/NR /P {3,4}{2-4} {5,8}{9,10} {5,8}{9,10} {7}{7}} We then identify the head-dependents fragments that are suitable for rule induction from the annotated dependency structure. To facilitate the identification process, we first define two sets of dependency structure related to head spans and dependency spans. Definition 5. A acceptable head set ahs(T) of a dependency structure T is a set of nodes, each of which has a consistent head span. For example,"
D11-1020,P03-1021,0,0.0927096,"e word alignments by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and applying “grow-diag-and” refinement(Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We use NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test set 2004 (MT04) and 2005 (MT05) as our test set. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric (Papineni et al., 2002).1 We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maximize the system’s BLEU score on the development set. 1 System cons2str hiero-re dep2str ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 223 We take a replication of Hiero (Chiang, 2007) as the hierarchical phrase-based model baseline. In our experiments of this paper, we set the beam size b = 200 and the beam threshold β = 0. The maximum initial phrase length is 10. We use constituency-to-string model (Liu et al., 2006) as the syntax-based model baseline which make use of composed rules (Galley et al., 2006) without handling the unaligned wor"
D11-1020,P02-1040,0,0.104432,"this paper, we make use of the POS tags only. We obtain the word alignments by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and applying “grow-diag-and” refinement(Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We use NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test set 2004 (MT04) and 2005 (MT05) as our test set. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric (Papineni et al., 2002).1 We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maximize the system’s BLEU score on the development set. 1 System cons2str hiero-re dep2str ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 223 We take a replication of Hiero (Chiang, 2007) as the hierarchical phrase-based model baseline. In our experiments of this paper, we set the beam size b = 200 and the beam threshold β = 0. The maximum initial phrase length is 10. We use constituency-to-string model (Liu et al., 2006) as the syntax-based model baseline which make use of composed rules (Galley e"
D11-1020,P05-1034,0,0.867324,"el, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristics (Lin, 2004; Xiong et al., 2007) or separate ordering models (Quirk et al., 2005; Ding and Palmer, 2005) to control the word order of translations. In this paper, we handle this problem by directly specifying the ordering information in headdependents rules that represent the source side as head-dependents relations and the"
D11-1020,P08-1066,0,0.434244,"ithout resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models. 1 Introduction Dependency structure represents the grammatical relations that hold between the words in a sentence. It encodes semantic relations directly, and has the best inter-lingual phrasal cohesion properties (Fox, 2002). Those attractive characteristics make it possible to improve translation quality by using dependency structures. Some researchers pay more attention to use dependency structure on the target side. (Shen et al., 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency"
D11-1020,W07-0706,1,0.871506,"hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristics (Lin, 2004; Xiong et al., 2007) or separate ordering models (Quirk et al., 2005; Ding and Palmer, 2005) to control the word order of translations. In this paper, we handle this problem by directly specifying the ordering information in headdependents rules that represent the source side as head-dependents relations and the target side as string. The head-dependents r"
D11-1081,D08-1023,0,0.040359,"(the ration between them is 1.5 to 1). Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus 887 Minimum error rate training (Och, 2003) is perhaps the most popular discriminative training for SMT. However, it fails to scale to large number of features. Researchers have propose many learning algorithms to train many features: perceptron (Shen et al., 2004; Liang et al., 2006), minimum risk (Smith and Eisner, 2006; Li et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008). The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data. For efficiency, we only use neighboring derivations for training. Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution. Furthermore, we focus on how to fast generate translation forest for training. The local operators lexicalize/generali"
D11-1081,P08-1024,0,0.575199,"derivation tree with maximum score by the help of word alignment, and then traverse the tree to generate the subset forest in linear time. Besides the efficiency improvement, such a forest allows us to train the model without resortDiscriminative model (Och and Ney, 2002) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade. Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). However, the training of the large number of features was always restricted in fairly small data sets. Some systems limit the number of training examples, while others use short sentences to maintain efficiency. 1 Exactly, there are no reference derivations, since derivation Overfitting problem often comes when training is a latent variable in SMT. We call them reference derivation many features on a small data (Watanabe et al., just for convenience. 880 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 880–888, c Edinburgh, S"
D11-1081,P09-1088,0,0.271938,"ince rule table is fixed, the second step is a process of decoding. Therefore, sometimes we may fail to create a new reference derivation (like r2 may not exist in the rule table). In such case, we keep the origin derivation unchanged. The changes made by the two operators are local. Considering the change of rules, the lexicalize operator deletes two rules and adds one new rule, while the generalize operator deletes one rule and adds two new rules. Such local changes provide us with a way to incrementally calculate the scores of new derivations. We use this method motivated by Gibbs Sampler (Blunsom et al., 2009) which has been used for efficiently learning rules. The different lies in that we use the operator for decoding where the rule table is fixing. 4.2 Initialize a Reference Derivation The generation starts from an initial reference derivation with max score. This requires bi-parsing (Dyer, 2010) over the source sentence f and the reference translation e. In practice, we may face three problems. First is efficiency problem. Exhaustive search over the space under SCFG requires O(|f |3 |e|3 ). To parse quickly, we only visit the tight consistent (Zhang et al., 2008) bi-spans with the help of word"
D11-1081,P07-1005,0,0.21909,"h is called degenerate solution (DeNero et al., 2006). That is because the translation rules with rare source/target sides always receive a very high translation probability. We add a prior score log(#rule) for each rule, where #rule is the number of occurrence of a rule, to reward frequent reusable rules and derivations with more rules. Finally, we may fail to create reference derivations due to the limitation in rule extraction. We create minimum trees for (f , e, a) using shift-reduce (Zhang et al., 2008). Some minimum rules in the trees may be illegal according to the definition of Chiang (2007). We also add these rules to the rule table, so as to make sure every sentence is reachable given the rule table. A source sentence is reachable given a rule table if reference derivations exists. We refer these rules as added rules. However, this may introduce rules with more than two variables and increase the complexity of bi-parsing. To tackle this problem, we initialize the chart with minimum parallel tree from the Zhang et al. (2008) algorithm, ensuring that the bi-parsing has at least one path to create a reference derivation. Then we only need to consider the traditional rules during b"
D11-1081,N09-1025,0,0.449562,"unt of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 B LEU over the baseline system on the NIST Chinese-English test sets. 1 Introduction 2007; Chiang et al., 2009). Obviously, using much more data can alleviate such problem. Furthermore, large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT. Despite these advantages, to the best of our knowledge, no previous discriminative training paradigms scale up to use a large amount of training data. The main obstacle comes from the complexity of packed forests or n-best lists generation which requires to search through all possible translations of each training example, which is computationally prohibitive in practice for SMT. To make normalization eff"
D11-1081,J07-2003,0,0.839921,"Overall, both the generation of forests and the training algorithm are scalable, enabling us to train millions of features on large-scale data. To show the effect of our framework, we globally train millions of word level context features motivated by word sense disambiguation (Chan et al., 2007) together with the features used in traditional SMT system (Section 6). Training on 519K sentence pairs in 0.03 seconds per sentence, we achieve significantly improvement over the traditional pipeline by 0.84 B LEU. 2 Synchronous Context Free Grammar We work on synchronous context free grammar (SCFG) (Chiang, 2007) based translation. The elementary structures in an SCFG are rewrite rules of the form: X ⇒ ⟨γ, α⟩ where γ and α are strings of terminals and nonterminals. We call γ and α as the source side and the target side of rule respectively. Here a rule means a phrase translation (Koehn et al., 2003) or a translation pair that contains nonterminals. We call a sequence of translation steps as a derivation. In context of SCFG, a derivation is a se881 quence of SCFG rules {ri }. Translation forest (Mi et al., 2008; Li and Eisner, 2009) is a compact representation of all the derivations for a given sentenc"
D11-1081,D09-1037,0,0.140873,"005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution. Furthermore, we focus on how to fast generate translation forest for training. The local operators lexicalize/generalize are use for greedy decoding. The idea is related to “pegging” algorithm (Brown et al., 1993) and greedy decoding (Germann et al., 2001). Such types of local operators are also used in Gibbs sampler for synchronous grammar induction (Blunsom et al., 2009; Cohn and Blunsom, 2009). 8 Conclusion and Future Work We have presented a fast generation algorithm for translation forest which contains both reference derivations and neighboring non-reference derivations for large-scale SMT discriminative training. We have achieved significantly improvement of 0.84 B LEU by incorporate 13.9M feature trained on 519K data in 0.03 second per sentence. In this paper, we define the forest based on competing derivations which only differ in one rule. There may be better classes of forest that can produce a better performance. It’s interesting to modify the definition of forest, and use"
D11-1081,W02-1001,0,0.0671022,"s. We first create an initial reference derivation for every training examples using bi-parsing (lines 4-5), and then online learn the parameters using SGD (lines 6-12). We use the G ENERATE function to calculate the gradient. In practice, instead of storing all the derivations in a list, we traverse the tree twice. The first time is calculating the partition function, and the second time calculates the gradient normalized by partition function. During training, we also change the derivations (line 10). When training is finished after M epochs, the algorithm returns an averaged weight vector (Collins, 2002) to avoid overfitting (line 13). We use a development set to select total epoch m, which is set as M = 5 in our experiments. 6 Experiments Our method is able to train a large number of features on large data. We use a set of word context features motivated by word sense disambiguation (Chan et al., 2007) to test scalability. A word level context feature is a triple (f, e, f+1 ), which counts the number of time that f is aligned to e and f+1 occurs to the right of f . Triple (f, e, f−1 ) is similar except that f−1 locates to the left of f . We retain word alignment information in the extracted"
D11-1081,W06-3105,0,0.0168007,"). To parse quickly, we only visit the tight consistent (Zhang et al., 2008) bi-spans with the help of word alignment a. Only visiting tight consistent spans greatly speeds up bi-parsing. Besides efficiency, adoption of this constraint receives support from the fact that heuristic SCFG rule extraction only extracts tight consistent initial phrases (Chiang, 2007). Second is degenerate problem. If we only use the features as traditional SCFG systems, the biparsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al., 2006). That is because the translation rules with rare source/target sides always receive a very high translation probability. We add a prior score log(#rule) for each rule, where #rule is the number of occurrence of a rule, to reward frequent reusable rules and derivations with more rules. Finally, we may fail to create reference derivations due to the limitation in rule extraction. We create minimum trees for (f , e, a) using shift-reduce (Zhang et al., 2008). Some minimum rules in the trees may be illegal according to the definition of Chiang (2007). We also add these rules to the rule table, so"
D11-1081,N10-1033,0,0.161422,"ge of rules, the lexicalize operator deletes two rules and adds one new rule, while the generalize operator deletes one rule and adds two new rules. Such local changes provide us with a way to incrementally calculate the scores of new derivations. We use this method motivated by Gibbs Sampler (Blunsom et al., 2009) which has been used for efficiently learning rules. The different lies in that we use the operator for decoding where the rule table is fixing. 4.2 Initialize a Reference Derivation The generation starts from an initial reference derivation with max score. This requires bi-parsing (Dyer, 2010) over the source sentence f and the reference translation e. In practice, we may face three problems. First is efficiency problem. Exhaustive search over the space under SCFG requires O(|f |3 |e|3 ). To parse quickly, we only visit the tight consistent (Zhang et al., 2008) bi-spans with the help of word alignment a. Only visiting tight consistent spans greatly speeds up bi-parsing. Besides efficiency, adoption of this constraint receives support from the fact that heuristic SCFG rule extraction only extracts tight consistent initial phrases (Chiang, 2007). Second is degenerate problem. If we o"
D11-1081,P01-1030,0,0.0518157,"ata. For efficiency, we only use neighboring derivations for training. Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution. Furthermore, we focus on how to fast generate translation forest for training. The local operators lexicalize/generalize are use for greedy decoding. The idea is related to “pegging” algorithm (Brown et al., 1993) and greedy decoding (Germann et al., 2001). Such types of local operators are also used in Gibbs sampler for synchronous grammar induction (Blunsom et al., 2009; Cohn and Blunsom, 2009). 8 Conclusion and Future Work We have presented a fast generation algorithm for translation forest which contains both reference derivations and neighboring non-reference derivations for large-scale SMT discriminative training. We have achieved significantly improvement of 0.84 B LEU by incorporate 13.9M feature trained on 519K data in 0.03 second per sentence. In this paper, we define the forest based on competing derivations which only differ in one"
D11-1081,N03-1017,0,0.124393,"al., 2007) together with the features used in traditional SMT system (Section 6). Training on 519K sentence pairs in 0.03 seconds per sentence, we achieve significantly improvement over the traditional pipeline by 0.84 B LEU. 2 Synchronous Context Free Grammar We work on synchronous context free grammar (SCFG) (Chiang, 2007) based translation. The elementary structures in an SCFG are rewrite rules of the form: X ⇒ ⟨γ, α⟩ where γ and α are strings of terminals and nonterminals. We call γ and α as the source side and the target side of rule respectively. Here a rule means a phrase translation (Koehn et al., 2003) or a translation pair that contains nonterminals. We call a sequence of translation steps as a derivation. In context of SCFG, a derivation is a se881 quence of SCFG rules {ri }. Translation forest (Mi et al., 2008; Li and Eisner, 2009) is a compact representation of all the derivations for a given sentence under an SCFG (see Figure 1). A tree t in the forest corresponds to a derivation. In our paper, tree means the same as derivation. More formally, a forest is a pair ⟨V, E⟩, where V is the set of nodes, E is the set of hyperedge. For a given source sentence f = f1n , Each node v ∈ V is in t"
D11-1081,W04-3250,0,0.397179,"Missing"
D11-1081,D09-1005,0,0.0590317,"nous Context Free Grammar We work on synchronous context free grammar (SCFG) (Chiang, 2007) based translation. The elementary structures in an SCFG are rewrite rules of the form: X ⇒ ⟨γ, α⟩ where γ and α are strings of terminals and nonterminals. We call γ and α as the source side and the target side of rule respectively. Here a rule means a phrase translation (Koehn et al., 2003) or a translation pair that contains nonterminals. We call a sequence of translation steps as a derivation. In context of SCFG, a derivation is a se881 quence of SCFG rules {ri }. Translation forest (Mi et al., 2008; Li and Eisner, 2009) is a compact representation of all the derivations for a given sentence under an SCFG (see Figure 1). A tree t in the forest corresponds to a derivation. In our paper, tree means the same as derivation. More formally, a forest is a pair ⟨V, E⟩, where V is the set of nodes, E is the set of hyperedge. For a given source sentence f = f1n , Each node v ∈ V is in the form Xi,j , which denotes the recognition of nonterminal X spanning the substring from the i through j (that is fi+1 ...fj ). Each hyperedge e ∈ E connects a set of antecedent to a single consequent node and corresponds to an SCFG rul"
D11-1081,P09-1067,0,0.187943,"Missing"
D11-1081,P06-1096,0,0.26191,"Missing"
D11-1081,P08-1023,1,0.817451,"B LEU. 2 Synchronous Context Free Grammar We work on synchronous context free grammar (SCFG) (Chiang, 2007) based translation. The elementary structures in an SCFG are rewrite rules of the form: X ⇒ ⟨γ, α⟩ where γ and α are strings of terminals and nonterminals. We call γ and α as the source side and the target side of rule respectively. Here a rule means a phrase translation (Koehn et al., 2003) or a translation pair that contains nonterminals. We call a sequence of translation steps as a derivation. In context of SCFG, a derivation is a se881 quence of SCFG rules {ri }. Translation forest (Mi et al., 2008; Li and Eisner, 2009) is a compact representation of all the derivations for a given sentence under an SCFG (see Figure 1). A tree t in the forest corresponds to a derivation. In our paper, tree means the same as derivation. More formally, a forest is a pair ⟨V, E⟩, where V is the set of nodes, E is the set of hyperedge. For a given source sentence f = f1n , Each node v ∈ V is in the form Xi,j , which denotes the recognition of nonterminal X spanning the substring from the i through j (that is fi+1 ...fj ). Each hyperedge e ∈ E connects a set of antecedent to a single consequent node and corr"
D11-1081,P02-1038,0,0.342674,"erence” derivations that fail to produce the reference translation.1 However, the complexity of generating this translation forest is up to O(n6 ), because we still need biparsing to create the reference derivations. Consequently, we propose a method to fast generate a subset of the forest. The key idea (Section 4) is to initialize a reference derivation tree with maximum score by the help of word alignment, and then traverse the tree to generate the subset forest in linear time. Besides the efficiency improvement, such a forest allows us to train the model without resortDiscriminative model (Och and Ney, 2002) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade. Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). However, the training of the large number of features was always restricted in fairly small data sets. Some systems limit the number of training examples, while others use short sentences to maintain efficiency. 1 Exactly, there"
D11-1081,J03-1002,0,0.00856682,"Missing"
D11-1081,P03-1021,0,0.412179,"otivated by word sense disambiguation (Chan et al., 2007) to test scalability. A word level context feature is a triple (f, e, f+1 ), which counts the number of time that f is aligned to e and f+1 occurs to the right of f . Triple (f, e, f−1 ) is similar except that f−1 locates to the left of f . We retain word alignment information in the extracted rules to exploit such features. To demonstrate the importance of scaling up the size of training data and the effect of our method, we compare three types of training configurations which differ in the size of features and data. MERT. We use MERT (Och, 2003) to training 8 features on a small data. The 8 features is the same as Chiang (2007) including 4 rule scores (direct and reverse translation scores; direct and reverse lexical translation scores); 1 target side language model score; 3 penalties for word counts, extracted rules and glue rule. Actually, traditional pipeline often uses such configuration. Perceptron. We also learn thousands of context word features together with the 8 traditional features on a small data using perceptron. Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature. This setting u"
D11-1081,P02-1040,0,0.081006,"Missing"
D11-1081,N09-1024,0,0.184056,"blem. Furthermore, large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT. Despite these advantages, to the best of our knowledge, no previous discriminative training paradigms scale up to use a large amount of training data. The main obstacle comes from the complexity of packed forests or n-best lists generation which requires to search through all possible translations of each training example, which is computationally prohibitive in practice for SMT. To make normalization efficient, contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009) introduce neighborhood for unsupervised log-linear model, and has presented positive results in various tasks. Motivated by these work, we use a translation forest (Section 3) which contains both “reference” derivations that potentially yield the reference translation and also neighboring “non-reference” derivations that fail to produce the reference translation.1 However, the complexity of generating this translation forest is up to O(n6 ), because we still need biparsing to create the reference derivations. Consequently, we propose a method to fast generate a subset of the forest. The key i"
D11-1081,N04-1023,0,0.0899572,"Missing"
D11-1081,P05-1044,0,0.416688,"a can alleviate such problem. Furthermore, large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT. Despite these advantages, to the best of our knowledge, no previous discriminative training paradigms scale up to use a large amount of training data. The main obstacle comes from the complexity of packed forests or n-best lists generation which requires to search through all possible translations of each training example, which is computationally prohibitive in practice for SMT. To make normalization efficient, contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009) introduce neighborhood for unsupervised log-linear model, and has presented positive results in various tasks. Motivated by these work, we use a translation forest (Section 3) which contains both “reference” derivations that potentially yield the reference translation and also neighboring “non-reference” derivations that fail to produce the reference translation.1 However, the complexity of generating this translation forest is up to O(n6 ), because we still need biparsing to create the reference derivations. Consequently, we propose a method to fast generate a subset of t"
D11-1081,P06-2101,0,0.1775,"on average, where 6.1 of them are reference derivations. Furthermore, we use lexicalize operator more often than generalize operator (the ration between them is 1.5 to 1). Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus 887 Minimum error rate training (Och, 2003) is perhaps the most popular discriminative training for SMT. However, it fails to scale to large number of features. Researchers have propose many learning algorithms to train many features: perceptron (Shen et al., 2004; Liang et al., 2006), minimum risk (Smith and Eisner, 2006; Li et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008). The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data. For efficiency, we only use neighboring derivations for training. Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marg"
D11-1081,P06-1091,0,0.0476138,"ey idea (Section 4) is to initialize a reference derivation tree with maximum score by the help of word alignment, and then traverse the tree to generate the subset forest in linear time. Besides the efficiency improvement, such a forest allows us to train the model without resortDiscriminative model (Och and Ney, 2002) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade. Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). However, the training of the large number of features was always restricted in fairly small data sets. Some systems limit the number of training examples, while others use short sentences to maintain efficiency. 1 Exactly, there are no reference derivations, since derivation Overfitting problem often comes when training is a latent variable in SMT. We call them reference derivation many features on a small data (Watanabe et al., just for convenience. 880 Proceedings of the 2011 Conference on Empirical Methods in Natural Langu"
D11-1081,D07-1080,0,0.779874,"initialize a reference derivation tree with maximum score by the help of word alignment, and then traverse the tree to generate the subset forest in linear time. Besides the efficiency improvement, such a forest allows us to train the model without resortDiscriminative model (Och and Ney, 2002) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade. Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). However, the training of the large number of features was always restricted in fairly small data sets. Some systems limit the number of training examples, while others use short sentences to maintain efficiency. 1 Exactly, there are no reference derivations, since derivation Overfitting problem often comes when training is a latent variable in SMT. We call them reference derivation many features on a small data (Watanabe et al., just for convenience. 880 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 8"
D11-1081,C08-1136,0,0.111034,"d motivated by Gibbs Sampler (Blunsom et al., 2009) which has been used for efficiently learning rules. The different lies in that we use the operator for decoding where the rule table is fixing. 4.2 Initialize a Reference Derivation The generation starts from an initial reference derivation with max score. This requires bi-parsing (Dyer, 2010) over the source sentence f and the reference translation e. In practice, we may face three problems. First is efficiency problem. Exhaustive search over the space under SCFG requires O(|f |3 |e|3 ). To parse quickly, we only visit the tight consistent (Zhang et al., 2008) bi-spans with the help of word alignment a. Only visiting tight consistent spans greatly speeds up bi-parsing. Besides efficiency, adoption of this constraint receives support from the fact that heuristic SCFG rule extraction only extracts tight consistent initial phrases (Chiang, 2007). Second is degenerate problem. If we only use the features as traditional SCFG systems, the biparsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al., 2006). That is because the translation rules with rare source/"
D11-1081,J93-2003,0,\N,Missing
D11-1110,P06-1109,0,0.186239,"ed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims t"
D11-1110,D08-1092,0,0.020044,"nd Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship between words in the pars"
D11-1110,P10-1003,0,0.0157995,"tins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship between words in the parsed source sentences can be directly projected across the wo"
D11-1110,N09-1009,0,0.0146374,"iments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from"
D11-1110,W02-1001,0,0.0348219,"that returns 1 if d appears in D and returns 0 otherwise; λ is a weight coefficient that needs to be tuned to maximize the quality of the projected treebank. 4 Experiments Our work focuses on the constituency projection from English to Chinese. The FBIS Chinese-English parallel corpus is used to obtain a projected constituent treebank. It contains 239 thousand sentence pairs, with about 6.9/8.9 million Chinese/English words. We parse the English sentences with the Charniak Parser (Charniak and Johnson, 2005), and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins, 2002) and trained on the Penn Chinese Treebank 5.0 (Xue et al., 2005). We perform word alignment by runing GIZA++ (Och and Ney, 2000), and then use the alignment results for constituency projection. Following the previous works of unsupervised constituent parsing, we evaluate the projected parser on the subsets of CTB 1.0 and CTB 5.0, which contain no more than 10 or 40 words after the removal of punctuation. The gold-standard POS tags are directly used for testing. The evaluation for unsupervised parsing differs slightly from the standard 1197 4.1 Projected PCFG Grammar An initial projected PCFG g"
D11-1110,J03-4003,0,0.0146149,"y projected from the source language syntax. Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with"
D11-1110,P09-1042,0,0.172725,"Missing"
D11-1110,D09-1127,1,0.84911,"d Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship between words in the parsed source sentences can be directly pro"
D11-1110,C10-2059,1,0.865041,"Missing"
D11-1110,P04-1061,0,0.394805,"bank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projec"
D11-1110,P10-1001,0,0.0119223,"specific syntactic idiosyncrasy rather than a strained grammar directly projected from the source language syntax. Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2"
D11-1110,P08-1068,0,0.0307007,"ervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel"
D11-1110,P04-1060,0,0.0281608,"ction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship between words in the parsed source sentences can be directly projected across the word alignment to words in the target sentences, following the direct correspondence assumption (DCA) (Hwa et al., 2005)."
D11-1110,D10-1004,0,0.0223133,"syncrasy rather than a strained grammar directly projected from the source language syntax. Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010)"
D11-1110,P06-1043,0,0.0288794,"ous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingu"
D11-1110,E06-1011,0,0.0407574,"es, and enables us to learn the target-language-specific syntactic idiosyncrasy rather than a strained grammar directly projected from the source language syntax. Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klei"
D11-1110,W06-2933,0,0.0265737,"the target-language-specific syntactic idiosyncrasy rather than a strained grammar directly projected from the source language syntax. Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al."
D11-1110,P00-1056,0,0.200675,"uality of the projected treebank. 4 Experiments Our work focuses on the constituency projection from English to Chinese. The FBIS Chinese-English parallel corpus is used to obtain a projected constituent treebank. It contains 239 thousand sentence pairs, with about 6.9/8.9 million Chinese/English words. We parse the English sentences with the Charniak Parser (Charniak and Johnson, 2005), and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins, 2002) and trained on the Penn Chinese Treebank 5.0 (Xue et al., 2005). We perform word alignment by runing GIZA++ (Och and Ney, 2000), and then use the alignment results for constituency projection. Following the previous works of unsupervised constituent parsing, we evaluate the projected parser on the subsets of CTB 1.0 and CTB 5.0, which contain no more than 10 or 40 words after the removal of punctuation. The gold-standard POS tags are directly used for testing. The evaluation for unsupervised parsing differs slightly from the standard 1197 4.1 Projected PCFG Grammar An initial projected PCFG grammar can be induced from the word-aligned and source-parsed parallel corpus according to section 3.1. Such an initial grammar"
D11-1110,P06-1055,0,0.0430272,". Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar"
D11-1110,N01-1023,0,0.048848,"eebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially"
D11-1110,P07-1049,0,0.144985,"l corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the s"
D11-1110,D09-1086,0,0.0886377,"Previous works mainly focus on dependency projection. The dependency relationship between words in the parsed source sentences can be directly projected across the word alignment to words in the target sentences, following the direct correspondence assumption (DCA) (Hwa et al., 2005). Due to the syntactic nonisomorphism between languages, DCA assumption usually leads to conflicting or incomplete projection. Researchers have to adopt strategies to tackle this problem, such as designing rules to handle language non-isomorphism (Hwa et al., 2005), and resorting to the quasi-synchronous grammar (Smith and Eisner, 2009). For constituency projection, however, the lack of isomorphism becomes much more serious, since a constituent grammar describes a language in a more detailed way. In this paper we propose a relaxed correspondence assumption (RCA) for constituency 1192 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1192–1201, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics TOP TOP 1S 2 PP IN 1 [S] 3 VP NP 4 NP . . 5 NP PRP VBD NP PP DT JJ [S-PP-*-VP-*] 2 NN 3 [VP] [PP] the previous hypothesis DT NN IN NP a series of NNS"
D11-1110,W04-3207,0,0.017728,"ncy parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship"
D11-1110,P09-1009,0,0.0166364,"of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship between words in the parsed source sentences can be directly projected across the word alignment to words in the target sentences, following the direct correspondence assumption (DCA) (Hwa et al., 2005). Due to the syntactic nonisomorphism between"
D11-1110,E03-1008,0,0.0294063,"cally outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the lang"
D11-1110,J97-3002,0,0.0802881,"the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship between words in the parsed source sentences can be directly projected across the word alignment to words in the target sentences, following the direct correspondence assumption (DCA) (Hwa et"
D11-1110,P09-1007,0,0.115634,"t al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship between words in the parsed source sentences"
D11-1110,P05-1022,0,\N,Missing
D12-1038,W06-1615,0,0.0614391,"r: S+ (y|Ms→t , Mt→s , Φ, x) = (1 − λ) × S(y|Ms→t , Φ, x) (2) + λ × S(x|Mt→s , Φ, y) The weight parameter λ is tuned on the developing set. To integrating the predict-self reestimation into the iterative transformation training, a reversed transformation model is introduced and the enhanced scoring function above is used when the function T RANS A NNOTATE invokes the function D ECODE . 4 Related Works Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum´e III, 2007), and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011). There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the util"
D12-1038,P09-2014,0,0.0169834,"ed when the function T RANS A NNOTATE invokes the function D ECODE . 4 Related Works Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum´e III, 2007), and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011). There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the utilization rate of human-annotated knowledge. The iterative training procedure proposed in this work shares some similarity with the co-training algorithm in parsing (Sarkar, 2001), where the training procedure lets two different models learn from each other during parsing the raw text. The key idea of co-training is utilize"
D12-1038,P04-1015,0,0.22417,"adopt the 4 boundary tags of Ng and Low (2004), b, m, e and s, where b, m and e mean the beginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm∗ e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Jiang et al., 2008; Zhang and Clark, 2010). The training procedure learns a discriminative model mapping from the inputs x ∈ X to the outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function Φ to map a training example (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd . Given the character sequence x, the decoder finds the output F (x) that maximizes the score function: F (x) = argmax S(y|~ α,"
D12-1038,W02-1001,0,0.854722,"+ Φ(xi , yi ) − Φ(xi , zi ) 8: Output: Parameters α ~ solved in a character classification approach by extending the boundary tags to include POS information. For word segmentation we adopt the 4 boundary tags of Ng and Low (2004), b, m, e and s, where b, m and e mean the beginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm∗ e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Jiang et al., 2008; Zhang and Clark, 2010). The training procedure learns a discriminative model mapping from the inputs x ∈ X to the outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function Φ to map a"
D12-1038,P07-1033,0,0.114075,"Missing"
D12-1038,P11-2095,0,0.119802,"rio of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as the annotation adaptation work (Jiang et al., 2009) for convenience of comparison. The two corpora are segmented following different segmentation guidelines and differ largel"
D12-1038,J07-3004,0,0.0432111,"ced scoring function above is used when the function T RANS A NNOTATE invokes the function D ECODE . 4 Related Works Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum´e III, 2007), and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011). There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the utilization rate of human-annotated knowledge. The iterative training procedure proposed in this work shares some similarity with the co-training algorithm in parsing (Sarkar, 2001), where the training procedure lets two different models learn from each other during parsing the raw text. The key idea"
D12-1038,P08-1102,1,0.9146,", m and e mean the beginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm∗ e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Jiang et al., 2008; Zhang and Clark, 2010). The training procedure learns a discriminative model mapping from the inputs x ∈ X to the outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function Φ to map a training example (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd . Given the character sequence x, the decoder finds the output F (x) that maximizes the score function: F (x) = argmax S(y|~ α, Φ, x) 2 Classification-Based Chinese Word Segmentation y∈GEN(x)"
D12-1038,P09-1059,1,0.943817,"iang and Fandong Meng and Qun Liu and Yajuan Lu¨ Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences {jiangwenbin, mengfandong, liuqun, lvyajuan}@ict.ac.cn Abstract nese word segmentation. However, the need of cascaded classification decisions makes it less practical for tasks of high computational complexity such as parsing, and less efficient to incorporate more than two annotated corpora. In this paper we first describe the technology of automatic annotation transformation, which is based on the annotation adaptation algorithm (Jiang et al., 2009). It can automatically transform a human-annotated corpus from one annotation guideline to another. We then propose two optimization strategies, iterative training and predict-self reestimation, to further improve the accuracy of annotation guideline transformation. Experiments on Chinese word segmentation show that, the iterative training strategy together with predictself reestimation brings significant improvement over the simple annotation transformation baseline, and leads to classifiers with significantly higher accuracy and several times faster processing than annotation adaptation does"
D12-1038,N09-1036,0,0.0850306,"ed dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as the annotation adaptation work (Jiang et al., 2009) for convenience of comparison. The two corpora are segmented following"
D12-1038,P09-1058,0,0.228495,". The predictself methodology is implicit in many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the"
D12-1038,P11-1141,0,0.189156,"Trans. Baseline Table 3: Data partitioning for CTB and PD. to train the transformation models. The predictself methodology is implicit in many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from"
D12-1038,P09-1012,0,0.107899,"pt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as the annotation adaptation work (Jiang et al., 2009) for convenience of comparison. The two corpora are segmented following different segmentation gu"
D12-1038,P07-2055,0,0.0621717,"ain the transformation models. The predictself methodology is implicit in many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et"
D12-1038,W04-3236,0,0.841665,"ation algorithm and the two optimization methods. After the introduction of related works in section 4, we give the experimental results on Chinese word segmentation in section 5. Algorithm 1 Perceptron training algorithm. 1: Input: Training examples (xi , yi ) 2: α ~ ←0 3: for t ← 1 .. T do 4: for i ← 1 .. N do 5: zi ← argmaxz∈GEN(xi ) Φ(xi , z) · α ~ 6: if zi 6= yi then 7: α ~ ←α ~ + Φ(xi , yi ) − Φ(xi , zi ) 8: Output: Parameters α ~ solved in a character classification approach by extending the boundary tags to include POS information. For word segmentation we adopt the 4 boundary tags of Ng and Low (2004), b, m, e and s, where b, m and e mean the beginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm∗ e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation"
D12-1038,N01-1023,0,0.154489,"Missing"
D12-1038,P11-1139,0,0.3586,"many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as t"
D12-1038,C10-1132,0,0.212957,"logy is implicit in many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental"
D12-1038,W03-1728,0,0.219269,"utputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function Φ to map a training example (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd . Given the character sequence x, the decoder finds the output F (x) that maximizes the score function: F (x) = argmax S(y|~ α, Φ, x) 2 Classification-Based Chinese Word Segmentation y∈GEN(x) = argmax Φ(x, y) · α ~ (1) y∈GEN(x) Chinese word segmentation can be formalized as the problem of sequence labeling (Xue and Shen, 2003), where each character in the sentence is given a boundary tag denoting its position in a word. Following Ng and Low (2004), joint word segmentation and part-of-speech (POS) tagging can also be 413 Where α ~ ∈ Rd is the parameter vector (that is, the discriminative model) and Φ(x, y) · α ~ is the inner product of Φ(x, y) and α ~. Algorithm 1 shows the perceptron algorithm for tuning the parameter α ~ . The “averaged parameters” Type Unigram Bigram Property Feature Templates C−2 C−1 C0 C1 C2 C−2 C−1 C−1 C0 C0 C1 C1 C2 C−1 C1 P u(C0 ) T (C−2 )T (C−1 )T (C0 )T (C1 )T (C2 ) Type Baseline Guiding T"
D12-1038,P07-1106,0,0.674995,"b, m, e and s, where b, m and e mean the beginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm∗ e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Jiang et al., 2008; Zhang and Clark, 2010). The training procedure learns a discriminative model mapping from the inputs x ∈ X to the outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function Φ to map a training example (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd . Given the character sequence x, the decoder finds the output F (x) that maximizes the score function: F (x) = argmax S(y|~ α, Φ, x) 2 Classification-Based Chinese Word Se"
D12-1038,D10-1082,0,0.847117,"eginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm∗ e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Jiang et al., 2008; Zhang and Clark, 2010). The training procedure learns a discriminative model mapping from the inputs x ∈ X to the outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function Φ to map a training example (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd . Given the character sequence x, the decoder finds the output F (x) that maximizes the score function: F (x) = argmax S(y|~ α, Φ, x) 2 Classification-Based Chinese Word Segmentation y∈GEN(x) = argmax Φ(x, y) · α ~ ("
D12-1038,I08-4017,0,0.0624271,"2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as the annotation adaptation work (Jiang et al., 2009) for convenience of comparison. The two cor"
D12-1038,P11-2126,0,0.146239,"on the developing set. To integrating the predict-self reestimation into the iterative transformation training, a reversed transformation model is introduced and the enhanced scoring function above is used when the function T RANS A NNOTATE invokes the function D ECODE . 4 Related Works Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum´e III, 2007), and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011). There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the utilization rate of human-annotated knowledge. The iterative training procedure proposed in this work shares some simi"
D12-1038,P04-1059,0,\N,Missing
D12-1109,P05-1022,0,0.0171017,"the vote) = f (r4 ) · 1 · lm(of the vote) NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnson, 2005; Huang, 2008; Mi et al., 2008) to compute the merit. In our algorithm, the merit of a derivation is just the Viterbi inside cost β of the root node calculated with the derivations continuing from the current derivation. Given a partial derivation, we calculate its future cost by searching through the translation forest defined by all applicable CFG rules. Figure 4 shows the translation forest for the derivation of step 3. We calculate the future cost for each node as follows: given a node v, we define its cost function f (v) as   v is completed 1 f (v) = lm(v) v is a terminal string  ∏"
D12-1109,P05-1033,0,0.199485,"ompete actions until the next action is grow. The predict and grow actions decide which rules can be used to expand hypotheses next, so we update the applicable rule set during these two actions. Given a source sentence with n words, we maintain n beams, and let each beam hold b hypotheses 1197 at most. Besides, we prune viable prefixes of each node up to u, so each hypothesis can expand to u new hypotheses at most, so the time complexity of beam search is O(nub). 4 Related Work Watanabe et al. (2006) present a novel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based mod"
D12-1109,J07-2003,0,0.371901,"ure consistent syntactic transformations between the source and target languages, e.g., from subject-verb-object to subject-object-verb word orderings. Decoding algorithms for grammar-based translation seek to find the best string in the intersection between a weighted context free grammar (the translation mode, given a source string/tree) and a weighted finite state acceptor (an n-gram language model). This intersection is problematic, as it results in an intractably large grammar, and makes exact search impossible. Most researchers have resorted to approximate search, typically beam search (Chiang, 2007). The decoder parses the source sentence, recording the target translations for each span.1 As the partial translation hypothesis grows, its component ngrams are scored and the hypothesis score is updated. This decoding method though is inefficient as it requires recording the language model context (n − 1 words) on the left and right edges of each chart cell. These contexts allow for boundary ngrams to be evaluated when the cell is used in another grammar production. In contrast, if the target string is generated in left-to-right order, then only one language model context is required, and th"
D12-1109,P05-1066,0,0.0607385,"5.2 Performance Comparison Our bottom-up left-to-right decoder employs the same features as the traditional decoder: rule probability, lexical probability, language model probability, rule count and word count. In order to compare them fairly, we used the same beam size which is 20 and employed cube pruning technique (Huang and Chiang, 2005). We show the results in Table 3. From the results, we can see that the bottom-up decoder outperforms top-down decoder and traditional decoder by 1.1 and 0.8 BLEU points respectively and the improvements are statistically significant using the sign-test of Collins et al. (2005) (p < 0.01). The improvement may result from dynamically searching for a whole derivation which leads to more accurate estimation of a partial derivation. The additional time consumption of the bottom-up decoder against the top-down decoder comes from dynamic future cost computation. Next we compare decoding speed versus translation quality using various beam sizes. The results are shown in Figure 5. We can see that our bottomup decoder can produce better BLEU score at the same decoding speed. At small beams (decoding time around 0.5 second), the improvement of translation quality is much bigg"
D12-1109,N10-1128,0,0.0532922,"e set during these two actions. Given a source sentence with n words, we maintain n beams, and let each beam hold b hypotheses 1197 at most. Besides, we prune viable prefixes of each node up to u, so each hypothesis can expand to u new hypotheses at most, so the time complexity of beam search is O(nub). 4 Related Work Watanabe et al. (2006) present a novel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tre"
D12-1109,P81-1022,0,0.741421,"nt Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1191–1200, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics nodes, it is necessary to consider the cost of uncovered nodes, i.e., the future cost. We show that a good future cost estimate is essential for accurate and efficient search, leading to high quality translation output. Other researchers have also considered the leftto-right decoding algorithm for tree-to-string models. Huang and Mi (2010) developed an Earleystyle parsing algorithm (Earley, 1970). In their approach, hypotheses covering the same number of tree nodes were binned together. Their method uses a top-down depth-first search, with a mechanism for early elimation of some rules which lead to deadends in the search. Huang and Mi (2010)’s method was shown to outperform the traditional post-ordertraversal decoding algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our approach. Our experiments show that compared with the Earle"
D12-1109,C10-2033,1,0.88656,"ovel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tree. Galley and Manning (2008) use the shift-reduce algorithm to conduct hierarchical phrase reordering so as to capture long-distance reordering. This algorithm shows good performance on phrase-based models, but can not be applied to syntax-based models directly. 5 Experiments In the experiments, we use two baseline systems: our in-house tree-to-str"
D12-1109,D08-1089,0,0.0379223,"scribe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tree. Galley and Manning (2008) use the shift-reduce algorithm to conduct hierarchical phrase reordering so as to capture long-distance reordering. This algorithm shows good performance on phrase-based models, but can not be applied to syntax-based models directly. 5 Experiments In the experiments, we use two baseline systems: our in-house tree-to-string decoder implemented according to Liu et al. (2006) (denoted as traditional) and the Earley-style top-down decoder implemented according to Huang and Mi (2010) (denoted as topdown), respectively. We compare our bottom-up left-to-right decoder (denoted as bottom-up) with the"
D12-1109,N04-1035,0,0.0554355,"t decoding (Huang and Mi, 2010) and the traditional post-order-traversal decoding (Liu et al., 2006) algorithms, our algorithm achieves a significant improvement on search capacity and better translation performance at the same level of speed. 2 Background A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) searches through a 1-best source parse tree for the best derivation. It transduces the source tree into a target-language string using a Synchronous Tree Substitution Grammar (STSG). The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We will briefly review the traditional decoding algorithm (Liu et al., 2006) and the Earley-style topdown decoding algorithm (Huang and Mi, 2010) for the tree-to-string model. 2.1 Traditional Decoding The traditional decoding algorithm processes source tree nodes one by one according to a post-order traversal. For each node, it applies matched STSG rules by substituting each non-terminal with its cor1192 traditional top-down bottom-up in theory O(nc˙|V |4(g−1) ) O(c(cr)d |V |g−1 ) O((cr)d |V |g−1 ) beam search O(ncb2 ) O(ncb) O(nub) Table 1: Time complexity of different algorithms. tradition"
D12-1109,W05-1506,0,0.043939,"Missing"
D12-1109,D10-1027,0,0.664834,"rding to a postorder traversal. 1191 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1191–1200, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics nodes, it is necessary to consider the cost of uncovered nodes, i.e., the future cost. We show that a good future cost estimate is essential for accurate and efficient search, leading to high quality translation output. Other researchers have also considered the leftto-right decoding algorithm for tree-to-string models. Huang and Mi (2010) developed an Earleystyle parsing algorithm (Earley, 1970). In their approach, hypotheses covering the same number of tree nodes were binned together. Their method uses a top-down depth-first search, with a mechanism for early elimation of some rules which lead to deadends in the search. Huang and Mi (2010)’s method was shown to outperform the traditional post-ordertraversal decoding algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our a"
D12-1109,2006.amta-papers.8,0,0.277257,"algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our approach. Our experiments show that compared with the Earley-style left-to-right decoding (Huang and Mi, 2010) and the traditional post-order-traversal decoding (Liu et al., 2006) algorithms, our algorithm achieves a significant improvement on search capacity and better translation performance at the same level of speed. 2 Background A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) searches through a 1-best source parse tree for the best derivation. It transduces the source tree into a target-language string using a Synchronous Tree Substitution Grammar (STSG). The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We will briefly review the traditional decoding algorithm (Liu et al., 2006) and the Earley-style topdown decoding algorithm (Huang and Mi, 2010) for the tree-to-string model. 2.1 Traditional Decoding The traditional decoding algorithm processes source tree nodes one by one according to a post-order trav"
D12-1109,P08-1067,0,0.0222632,"(of the vote) NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnson, 2005; Huang, 2008; Mi et al., 2008) to compute the merit. In our algorithm, the merit of a derivation is just the Viterbi inside cost β of the root node calculated with the derivations continuing from the current derivation. Given a partial derivation, we calculate its future cost by searching through the translation forest defined by all applicable CFG rules. Figure 4 shows the translation forest for the derivation of step 3. We calculate the future cost for each node as follows: given a node v, we define its cost function f (v) as   v is completed 1 f (v) = lm(v) v is a terminal string  ∏  maxr∈Rv f (r)"
D12-1109,J99-4005,0,0.212094,"e: 4 We bundle the successive terminals in one rule into a symbol 1195 grow [ IP] [NN2 ] −→ [ IP] [NN2  of the vote] From the above definition, we can find that there may be an ambiguity about whether to use a complete action or a grow action. Similarly, predict actions must select a viable prefix form the set for a node. For example in step 5, although we select to perform complete with r4 in the example, r7 is applicable, too. In our implementation, if both r4 and r7 are applicable, we apply them both to generate two seperate hypotheses. To limit the exponential explosion of hypotheses (Knight, 1999), we use beam search over bins of similar partial hypotheses (Koehn, 2004). IP r6 r4 , then r7 f (N P ) = f (r4 ) · f (N N2 ) · lm(of the vote) = f (r4 ) · 1 · lm(of the vote) NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a d"
D12-1109,koen-2004-pharaoh,0,0.674331,"[ IP] [NN2 ] −→ [ IP] [NN2  of the vote] From the above definition, we can find that there may be an ambiguity about whether to use a complete action or a grow action. Similarly, predict actions must select a viable prefix form the set for a node. For example in step 5, although we select to perform complete with r4 in the example, r7 is applicable, too. In our implementation, if both r4 and r7 are applicable, we apply them both to generate two seperate hypotheses. To limit the exponential explosion of hypotheses (Knight, 1999), we use beam search over bins of similar partial hypotheses (Koehn, 2004). IP r6 r4 , then r7 f (N P ) = f (r4 ) · f (N N2 ) · lm(of the vote) = f (r4 ) · 1 · lm(of the vote) NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the futu"
D12-1109,P06-1077,1,0.969992,"od uses a top-down depth-first search, with a mechanism for early elimation of some rules which lead to deadends in the search. Huang and Mi (2010)’s method was shown to outperform the traditional post-ordertraversal decoding algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our approach. Our experiments show that compared with the Earley-style left-to-right decoding (Huang and Mi, 2010) and the traditional post-order-traversal decoding (Liu et al., 2006) algorithms, our algorithm achieves a significant improvement on search capacity and better translation performance at the same level of speed. 2 Background A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) searches through a 1-best source parse tree for the best derivation. It transduces the source tree into a target-language string using a Synchronous Tree Substitution Grammar (STSG). The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We will briefly review the traditional decoding algorithm (Liu et al., 2006) a"
D12-1109,P08-1023,1,0.858066,"NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnson, 2005; Huang, 2008; Mi et al., 2008) to compute the merit. In our algorithm, the merit of a derivation is just the Viterbi inside cost β of the root node calculated with the derivations continuing from the current derivation. Given a partial derivation, we calculate its future cost by searching through the translation forest defined by all applicable CFG rules. Figure 4 shows the translation forest for the derivation of step 3. We calculate the future cost for each node as follows: given a node v, we define its cost function f (v) as   v is completed 1 f (v) = lm(v) v is a terminal string  ∏  maxr∈Rv f (r) π∈rhs(r) f (π) ot"
D12-1109,J03-1002,0,0.00491025,"topdown), respectively. We compare our bottom-up left-to-right decoder (denoted as bottom-up) with the baseline in terms of performance, translation quality and decoding speed with different beam sizes, and search capacity. Lastly, we show the influence of future cost. All systems are implemented in C++. System Traditional Top-down Bottom-up 5.1 Data Setup Time (s) 0.84 0.41 0.81 Table 3: Performance comparison. 30.8 30.6 30.4 BLEU Score We used the FBIS corpus consisting of about 250K Chinese-English sentence pairs as the training set. We aligned the sentence pairs using the GIZA++ toolkit (Och and Ney, 2003) and extracted tree-tostring rules according to the GHKM algorithm (Galley et al., 2004). We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus. We used the 2002 NIST MT Chinese-English test set (571 sentences) as the development set and the 2005 NIST MT Chinese-English test set (1082 sentences) as the test set. We evaluated translation quality using BLEU-metric (Papineni et al., 2002) with case-insensitive n-gram matching up to n = 4. We used the standard minimum error rate training (Och, 2003) to tune feature weights to maximi"
D12-1109,P03-1021,0,0.0478665,"Missing"
D12-1109,P02-1040,0,0.0894745,"Missing"
D12-1109,P06-1098,0,0.566743,"as “closure” actions. That is to say, once there are some complete actions after a scan action, we finish all the compete actions until the next action is grow. The predict and grow actions decide which rules can be used to expand hypotheses next, so we update the applicable rule set during these two actions. Given a source sentence with n words, we maintain n beams, and let each beam hold b hypotheses 1197 at most. Besides, we prune viable prefixes of each node up to u, so each hypothesis can expand to u new hypotheses at most, so the time complexity of beam search is O(nub). 4 Related Work Watanabe et al. (2006) present a novel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CK"
D13-1051,N10-1141,0,0.0373541,"Missing"
D13-1051,P12-1001,0,0.023394,"m our method just in hypothesis alignment algorithm, and the second combines the first and second module of the system combination pipeline. Because our method utilizes bidirectional information, we also provide another two alternative systems for comparison, which are GIZA++ based alignment and the posterior probability based alignment (Liang et al., 2006). Finally, we also provide an N-best alignment IHMM system, which combines an N-best alignment list to simulate the Pareto optimal solutions in our method. The method that linearly combines all objectives is not listed as our baseline like (Duh et al., 2012) does, because their algorithm finds the best weighted solution in a fixed and small solution set, while in our problem, the solution space is a trellis-style structure consisting of an exponential number of solutions, and no efficient algorithms apply here. The IHMM based alignment utilizes typical settings (He et al., 2008; Feng et al., 2009). The 541 smoothing factor for the surface similarity model, and ρ = 3 the controlling factor for the distortion model, K = 2. The bilingual probabilistic dictionary is trained in the FBIS corpus which includes about 230k parallel sentence pairs. GIZA++"
D13-1051,D09-1115,1,0.767088,"timization Tian Xia+ , Zongcheng Ji∗ , Shaodan Zhai+ , Yidong Chen++ , Qun Liu∗ , Shaojun Wang+ ++ Xiamen University, Xiamen 361005, P.R. China + Wright State University, 3640 Colonel Glenn Hwy, Dayton, OH 45435, USA ∗ Institute of Computing Technology, Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {jizongcheng, liuqun}@ict.ac.cn and ydchen@xmu.edu.cn {xia.7, zhai.6, shaojun.wang}@wright.edu Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC systems have been using IHMM or variants in their alignment algorithms more or less (Li et al., 2009; F"
D13-1051,D09-1125,0,0.18129,"m Combination by Using Multi-objective Optimization Tian Xia+ , Zongcheng Ji∗ , Shaodan Zhai+ , Yidong Chen++ , Qun Liu∗ , Shaojun Wang+ ++ Xiamen University, Xiamen 361005, P.R. China + Wright State University, 3640 Colonel Glenn Hwy, Dayton, OH 45435, USA ∗ Institute of Computing Technology, Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {jizongcheng, liuqun}@ict.ac.cn and ydchen@xmu.edu.cn {xia.7, zhai.6, shaojun.wang}@wright.edu Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC systems have been using IHMM or variants in their alignment algorit"
D13-1051,D08-1011,0,0.10981,"te University, 3640 Colonel Glenn Hwy, Dayton, OH 45435, USA ∗ Institute of Computing Technology, Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {jizongcheng, liuqun}@ict.ac.cn and ydchen@xmu.edu.cn {xia.7, zhai.6, shaojun.wang}@wright.edu Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC systems have been using IHMM or variants in their alignment algorithms more or less (Li et al., 2009; Feng et al., 2009). Abstract This paper proposes a multi-objective optimization framework which supports heterogeneous information sources to improve alignm"
D13-1051,W05-1506,0,0.0455054,"tion of edges traversed, on which 4 categories of features are defined. Training and Decoding Our work does not change the classic pipeline, thus the model and features are nearly identical to the 540 2. logarithm of language model score, L(h). 3. number of null edge, N umnull . 4. number of words, N umw . P P log(h) = span log( sys λsys p(w|sys, span)) + w0 L(h) + w1 N umnull + w2 N umw (8) Decoding a confusion network is straightforward, traversing each node from left to right, and the beam search algorithm will retain for each node an Nbest list. The final N-best can be acquired following (Huang and Chiang, 2005). The training process follows minimum error rate training (MERT) described in (Och, 2003; Koehn et al., 2003). In each iteration, the Powell algorithm would attempt to predict the optimal parameters on the cumulative N-best list. 5 Experiments We evaluate our method in two datasets in the Chinese-to-English task. In the first one, NIST MT 2002 and 2005 are used for tuning and testing respectively, and in the second, the newswire part of MT 2006 and 2008 are for tuning and testing. A 5gram language model is trained on the Xinhua portion of the Gigaword corpus. We report the casesensitive NIST-"
D13-1051,D07-1029,0,0.0537282,"Missing"
D13-1051,N03-1017,0,0.254144,"on systems. 1 Introduction System combination (SC) techniques power of boosting translation quality in several percent over the best among all chine translation systems (Bangalore et have the BLEU by input maal., 2001; Our motivation derives from an observation that in an ideal alignment of a pair of sentences, many-tomany alignments often exist. For instance, “be about to” has the same meaning with “be on the point of”. Because Hidden Markov Model based alignment algorithms, e.g. IHMM for system combination, HMM in GIZA++ software for statistical machine translation (SMT) (Och and Ney, 2000; Koehn et al., 2003), are designed for one-to-many alignment, and running GIZA++ from two directions to gain better performance turns into a standard operation in SMT, therefore we are seeking a way to empower IHMM by introducing bi-directional information. However, it appears to be intractable in an IHMM model to search the optimal solution by simply defining a new goal as a product of probabilities 535 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 535–544, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics from two direc"
D13-1051,P07-2045,0,0.00680497,"Missing"
D13-1051,P09-1019,0,0.0237271,"egrates neighboring modules to avoid propagated errors to gain improved performance. For example, Rosti et al. (2008), and Li et al. (2009) combine the first and the second module, and He and Toutanova (2009) combine all modules into one directly. Nevertheless, the classic structure also owns its merits. Because of the independence between modules, a system is relatively 536 simple to maintain, and improvements on each module might contribute to final performance additively. Based on our work, lattice-based minimum error rate training (lattice-MERT) and minimum bayes risk training techniques (Kumar et al., 2009) could be adopted on the third module. And Feng et al. (2009) in the second module adopts a different data structure called lattice which could directly use our better many-to-many alignment for construction. Experiments on the Chinese-to-English task on two datasets use four objectives, IHMM probability (Section 3.2.1), and alignment probability from GIZA++ (Section 3.2.2) from two directions. Results show multi-objective optimization framework efficiently integrates different information to gain approximately 1 BLEU point improvement over a strong baseline. 2 Background We briefly give an in"
D13-1051,P09-1107,0,0.200815,"ulti-objective Optimization Tian Xia+ , Zongcheng Ji∗ , Shaodan Zhai+ , Yidong Chen++ , Qun Liu∗ , Shaojun Wang+ ++ Xiamen University, Xiamen 361005, P.R. China + Wright State University, 3640 Colonel Glenn Hwy, Dayton, OH 45435, USA ∗ Institute of Computing Technology, Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {jizongcheng, liuqun}@ict.ac.cn and ydchen@xmu.edu.cn {xia.7, zhai.6, shaojun.wang}@wright.edu Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC systems have been using IHMM or variants in their alignment algorithms more or less"
D13-1051,N06-1014,0,0.605349,"alignment, and running GIZA++ from two directions to gain better performance turns into a standard operation in SMT, therefore we are seeking a way to empower IHMM by introducing bi-directional information. However, it appears to be intractable in an IHMM model to search the optimal solution by simply defining a new goal as a product of probabilities 535 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 535–544, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics from two directions. To bypass this problem, Liang et al. (2006) adopts a simple and effective variational inference algorithm. Further, different alignment algorithms capture different information and linguistic phenomena for a pair of sentences, hence more information would be expected to benefit the final alignment. Liang’s method may not be suitable for this expected outcome. We propose to adopt multi-objective optimization framework to support heterogeneous information sources which may induce difficulties in a conventional search algorithm. In this framework, there exist a variety of matured multi-objective optimization algorithms, e.g. evolutionary"
D13-1051,E06-1005,0,0.0628495,"Missing"
D13-1051,P08-1023,1,0.902145,"Missing"
D13-1051,P00-1056,0,0.541878,"e machine translation systems. 1 Introduction System combination (SC) techniques power of boosting translation quality in several percent over the best among all chine translation systems (Bangalore et have the BLEU by input maal., 2001; Our motivation derives from an observation that in an ideal alignment of a pair of sentences, many-tomany alignments often exist. For instance, “be about to” has the same meaning with “be on the point of”. Because Hidden Markov Model based alignment algorithms, e.g. IHMM for system combination, HMM in GIZA++ software for statistical machine translation (SMT) (Och and Ney, 2000; Koehn et al., 2003), are designed for one-to-many alignment, and running GIZA++ from two directions to gain better performance turns into a standard operation in SMT, therefore we are seeking a way to empower IHMM by introducing bi-directional information. However, it appears to be intractable in an IHMM model to search the optimal solution by simply defining a new goal as a product of probabilities 535 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 535–544, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Lingu"
D13-1051,P03-1021,0,0.0466198,"does not change the classic pipeline, thus the model and features are nearly identical to the 540 2. logarithm of language model score, L(h). 3. number of null edge, N umnull . 4. number of words, N umw . P P log(h) = span log( sys λsys p(w|sys, span)) + w0 L(h) + w1 N umnull + w2 N umw (8) Decoding a confusion network is straightforward, traversing each node from left to right, and the beam search algorithm will retain for each node an Nbest list. The final N-best can be acquired following (Huang and Chiang, 2005). The training process follows minimum error rate training (MERT) described in (Och, 2003; Koehn et al., 2003). In each iteration, the Powell algorithm would attempt to predict the optimal parameters on the cumulative N-best list. 5 Experiments We evaluate our method in two datasets in the Chinese-to-English task. In the first one, NIST MT 2002 and 2005 are used for tuning and testing respectively, and in the second, the newswire part of MT 2006 and 2008 are for tuning and testing. A 5gram language model is trained on the Xinhua portion of the Gigaword corpus. We report the casesensitive NIST-BLEU score. Four single machine translation systems participating in the system combinati"
D13-1051,D09-1147,0,0.0207414,"+ , Zongcheng Ji∗ , Shaodan Zhai+ , Yidong Chen++ , Qun Liu∗ , Shaojun Wang+ ++ Xiamen University, Xiamen 361005, P.R. China + Wright State University, 3640 Colonel Glenn Hwy, Dayton, OH 45435, USA ∗ Institute of Computing Technology, Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {jizongcheng, liuqun}@ict.ac.cn and ydchen@xmu.edu.cn {xia.7, zhai.6, shaojun.wang}@wright.edu Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC systems have been using IHMM or variants in their alignment algorithms more or less (Li et al., 2009; Feng et al., 2009). Ab"
D13-1051,N07-1029,0,0.44188,"et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC systems have been using IHMM or variants in their alignment algorithms more or less (Li et al., 2009; Feng et al., 2009). Abstract This paper proposes a multi-objective optimization framework which supports heterogeneous information sources to improve alignment in machine translation system combination techniques. In this area, most of techniques usually utilize confusion networks (CN) as their central data structure to compact an exponential number of an potential hypotheses, and because better hypothesis alignment may"
D13-1051,P07-1040,0,0.194111,"et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC systems have been using IHMM or variants in their alignment algorithms more or less (Li et al., 2009; Feng et al., 2009). Abstract This paper proposes a multi-objective optimization framework which supports heterogeneous information sources to improve alignment in machine translation system combination techniques. In this area, most of techniques usually utilize confusion networks (CN) as their central data structure to compact an exponential number of an potential hypotheses, and because better hypothesis alignment may"
D13-1051,W08-0329,0,0.468059,"g Alignment of System Combination by Using Multi-objective Optimization Tian Xia+ , Zongcheng Ji∗ , Shaodan Zhai+ , Yidong Chen++ , Qun Liu∗ , Shaojun Wang+ ++ Xiamen University, Xiamen 361005, P.R. China + Wright State University, 3640 Colonel Glenn Hwy, Dayton, OH 45435, USA ∗ Institute of Computing Technology, Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {jizongcheng, liuqun}@ict.ac.cn and ydchen@xmu.edu.cn {xia.7, zhai.6, shaojun.wang}@wright.edu Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC systems have been using IHMM or variants in"
D13-1051,N09-2052,0,0.0354845,"Missing"
D13-1108,P08-1009,0,0.0222484,"els (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) 1074 proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and the constituency-to-string model. Most works make effort to introduce linguistic knowledge into the phrase-based model and hierarchical phrase"
D13-1108,P05-1033,0,0.864996,"P2 , VP3 indicate the phrases which can not be captured by dependency syntactic phrases. (c) is the corresponding bilingual sentences. The subscripts of phrasal nodes are used for distinguishing the nodes with same phrasal categories. approach yields encouraging results by exploiting two types of trees. Large-scale experiments (Section 5) on Chinese-English translation show that our model significantly outperforms the state-ofthe-art single constituency-to-string model by averaged +2.45 BLEU points, dependency-to-string model by averaged +0.91 BLEU points, and hierarchical phrase-based model (Chiang, 2005) by averaged +1.12 BLEU points, on three Chinese-English NIST test sets. 2 Grammar We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. A headdependents relation consists of a head and all its dependents in dependency trees, and it can represent long distance dependencies. Incorporating phrasal nodes of constituency trees into head-dependents relations further enhances the compatibility with phrases of our rules. Figure 1 shows an example of phrases whic"
D13-1108,J07-2003,0,0.179871,"rt the input phrasal nodes labeled dependency tree into a target string among all possible derivations. Given the source constituency tree and dependency tree, we first generate phrasal nodes labeled dependency tree T as described in Section 3.1, then the decoder transverses each node in T by postorder. For each node n, it enumerates all instances of CHDR rooted at n, and checks the rule set for matched translation rules. A larger translation is generated by substituting the variables in the target side of a translation rule with the translations of the corresponding dependents. Cube pruning (Chiang, 2007; Huang and Chiang, 2007) is used to find the k-best items with integrated language model for each node. To balance the performance and speed of the decoder, we limit the search space by reducing the 1072 number of translation rules used for each node. There are two ways to limit the rule table size: by a fixed limit (rule-limit) of how many rules are retrieved for each input node, and by a threshold (rulethreshold) to specify that the rule with a score lower than β times of the best score should be discarded. On the other hand, instead of keeping the full list of candidates for a given node,"
D13-1108,P05-1066,0,0.290508,"Missing"
D13-1108,P05-1067,0,0.529981,"ndencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that represent the source side as head-dependents relations which ar"
D13-1108,P12-1100,1,0.661115,"1). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) 1074 proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and the constituency-to-string model. Most works make effort to introduce linguistic knowledge into the phrase-based model and hierarchical phrasebased model with constituency trees. Only the work proposed by Mi and Liu, (2010) utilized constituency and dependency trees, while their work applied two types of trees on two sides. Instead, our model simultaneously utilizes constituency and dependency trees on the source side to direct the translation, which is concerned w"
D13-1108,W02-1039,0,0.349783,"l phrasal nodes from the constituency tree C to the dependency tree T , which can be easily accomplished by phrases mapping according to the common covered source sequences. As dependency trees can capture some phrasal information by dependency syntactic 1069 phrases, in order to complement the information that dependency trees can not capture, we only label the phrasal nodes that cover dependency non-syntactic phrases. Then, we annotate alignment information to the phrasal nodes labeled dependency tree T , as shown in Figure 4. For description convenience, we make use of the notion of spans (Fox, 2002; Lin, 2004). Given a node n in the source phrasal nodes labeled T with word alignment information, the spans of n induced by the word alignment are consecutive sequences of words in the target sentence. As shown in Figure 4, we annotate each node n of phrasal nodes labeled T with two attributes: node span and subtree span; besides, we annotate phrasal span to the parts covered by phrasal nodes in each subtree rooted at n. The three types of spans are defined as follows: Definition 1 Given a node n, its node span nsp(n) is the consecutive target word sequence aligned with the node n. æ Take t"
D13-1108,N04-1035,0,0.61655,"4: An annotated dependency tree. Each node is annotated with two spans, the former is node span and the latter subtree span. The fragments covered by phrasal nodes are annotated with phrasal spans. The nodes denoted by the solid line box are not nsp consistent. 3 Rule Extraction In this section, we describe how to extract rules from a set of 4-tuples hC, T, S, Ai, where C is a source constituency tree, T is a source dependency tree, S is a target side sentence, and A is an word alignment relation between T /C and S. We extract CHDR rules from each 4-tuple hC, T, S, Ai based on GHKM algorithm (Galley et al., 2004) with three steps: 1. Label the dependency tree with phrasal nodes from the constituency tree, and annotate alignment information to the phrasal nodes labeled dependency tree (Section 3.1). 2. Identify acceptable CHDR fragments from the annotated dependency tree for rule induction (Section 3.2). 3. Induce a set of lexicalized and generalized CHDR rules from the acceptable fragments (Section 3.3). 3.1 Annotation Given a 4-tuple hC, T, S, Ai, we first label phrasal nodes from the constituency tree C to the dependency tree T , which can be easily accomplished by phrases mapping according to the c"
D13-1108,N04-1014,0,0.441996,"del achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents"
D13-1108,P07-1019,0,0.118738,"hrasal nodes labeled dependency tree into a target string among all possible derivations. Given the source constituency tree and dependency tree, we first generate phrasal nodes labeled dependency tree T as described in Section 3.1, then the decoder transverses each node in T by postorder. For each node n, it enumerates all instances of CHDR rooted at n, and checks the rule set for matched translation rules. A larger translation is generated by substituting the variables in the target side of a translation rule with the translations of the corresponding dependents. Cube pruning (Chiang, 2007; Huang and Chiang, 2007) is used to find the k-best items with integrated language model for each node. To balance the performance and speed of the decoder, we limit the search space by reducing the 1072 number of translation rules used for each node. There are two ways to limit the rule table size: by a fixed limit (rule-limit) of how many rules are retrieved for each input node, and by a threshold (rulethreshold) to specify that the rule with a score lower than β times of the best score should be discarded. On the other hand, instead of keeping the full list of candidates for a given node, we keep a topscoring subs"
D13-1108,2006.amta-papers.8,0,0.492272,"r the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that"
D13-1108,N03-1017,0,0.262119,"ording to the notion in the phrase-based model (Koehn et /NR), tsp( al., 2003). For example, nsp( /NN) and psp(NP1 ) are consistent while nsp( /JJ) and nsp( /NN) are not consistent. The annotation can be achieved by a single postorder transversal of the phrasal nodes labeled dependency tree. For simplicity, we call the annotated phrasal nodes labeled dependency tree annotated dependency tree. The extraction of bilingual phrases (including the translation of head node, dependency syntactic phrases and the fragment covered by a phrasal node) can be readily achieved by the algorithm described in Koehn et al., (2003). In the following, we focus on CHDR rules extraction.  ? 3.2 )P æ Intel 1 Before present the method of acceptable fragments identification, we give a brief description of CHDR fragments. A CHDR fragment is an annotated fragment that consists of a source head-dependents relation with/without constituency phrasal nodes, a target string and the word alignment information between the source and target side. We identify the acceptable CHDR fragments that are suitable for rule induction from the annotated dependency tree. We divide the acceptable CHDR fragments into two categories depending on w"
D13-1108,C04-1090,0,0.866617,"e) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that represent the source side as head-depend"
D13-1108,P06-1077,1,0.835734,"y improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our mode"
D13-1108,P09-1063,1,0.87343,"Missing"
D13-1108,P11-1128,1,0.890153,"Missing"
D13-1108,P08-1114,0,0.0207519,"2g Ñy 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) 1074 proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and"
D13-1108,P10-1145,1,0.777695,"al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) 1074 proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and the constituency-to-string model. Most works make effort to introduce linguistic knowledge into the phrase-based model and hierarchical phrasebased model with constituency trees. Only the work proposed by Mi and Liu, (2010) utilized constit"
D13-1108,P08-1023,1,0.872168,"which can be captured by our CHDR-phrasal rules and translated into the correct result “reemergence” with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 2g Ñy 2g Ñy 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu"
D13-1108,P02-1038,0,0.524086,"le the unaligned words of the target side by extending the node spans of the lexicalized head and leaf nodes, and the subtree spans of the lexicalized dependents, on both left and right directions. This procedure is similar with the method of Och and Ney, (2004). During this process, we might obtain m(m ≥ 1) CHDR rules from an acceptable fragment. Each of these rules is assigned with a fractional count 1/m. We take the extracted rule set as observed data and make use of relative frequency estimator to obtain the translation probabilities P (t|s) and P (s|t). 4 Decoding and the Model Following Och and Ney, (2002), we adopt a general loglinear model. Let d be a derivation that convert a source phrasal nodes labeled dependency tree into a target string e. The probability of d is defined as: Y P (d) ∝ φi (d)λi (1) i where φi are features defined on derivations and λi are feature weights. In our experiments of this paper, the features are used as follows: • CHDR rules translation probabilities P (t|s) and P (s|t), and CHDR rules lexical translation probabilities Plex (t|s) and Plex (s|t); • bilingual phrases translation probabilities Pbp (t|s) and Pbp (s|t), and bilingual phrases lexical translation proba"
D13-1108,J03-1002,0,0.00904566,"). Finally, we give detail analysis (Section 5.4). 5.1 Data Preparation Our training data consists of 1.25M sentence pairs extracted from LDC 1 data. We choose NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test sets 2003 (MT03), 2004 (MT04) and 2005 (MT05) as our test sets. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric 2 . We parse the source sentences to constituency trees (without binarization) and projective dependency trees with Stanford Parser (Klein and Manning, 2002). The word alignments are obtained by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and using the “grow-diag-finaland” balance strategy (Koehn et al., 2003). We get bilingual phrases from word-aligned data with algorithm described in Koehn et al. (2003) by running Moses Toolkit 3 . We apply SRI Language Modeling Toolkit (Stolcke and others, 2002) to train a 4-gram 1 Including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 2 ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 3 http://www.statmt.org/moses/ System Moses-chart cons2str dep2str consdep2str Rule # 116.4M 25.4M+32.5M 19.6M+32.5M 23"
D13-1108,J04-4002,0,0.206297,"rules in Xie et al., (2011). CHDR-normal rules are equivalent with the head-dependents relation rules and the CHDRphrasal rules are the extension of these rules. For convenience of description, we use the subscript to distinguish the phrasal nodes with the same category, such as VP2 and VP3 . In actual operation, we use VP instead of VP2 and VP3 . We handle the unaligned words of the target side by extending the node spans of the lexicalized head and leaf nodes, and the subtree spans of the lexicalized dependents, on both left and right directions. This procedure is similar with the method of Och and Ney, (2004). During this process, we might obtain m(m ≥ 1) CHDR rules from an acceptable fragment. Each of these rules is assigned with a fractional count 1/m. We take the extracted rule set as observed data and make use of relative frequency estimator to obtain the translation probabilities P (t|s) and P (s|t). 4 Decoding and the Model Following Och and Ney, (2002), we adopt a general loglinear model. Let d be a derivation that convert a source phrasal nodes labeled dependency tree into a target string e. The probability of d is defined as: Y P (d) ∝ φi (d)λi (1) i where φi are features defined on deriv"
D13-1108,P03-1021,0,0.347804,"Missing"
D13-1108,P05-1034,0,0.638161,"BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that represent the source side as head-dependents relations which are incorporated with"
D13-1108,P08-1066,0,0.730547,"oy single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that represent the source side as head-dependents relations which are incorporated with constituency phrasal nodes, and the tar"
D13-1108,D11-1020,1,0.723184,"Technology, Chinese Academy of Sciences §University of Chinese Academy of Sciences {mengfandong,xiejun,songlinfeng,lvyajuan}@ict.ac.cn ‡Centre for Next Generation Localisation Faculty of Engineering and Computing, Dublin City University qliu@computing.dcu.ie Abstract quences of words combine to form constituents, and constituency-based models show better compatibility with phrases. However, dependency trees describe the grammatical relation between words of the sentence, and represent long distance dependencies in a concise manner. Dependency-based models, such as dependency-to-string model (Xie et al., 2011), exhibit better capability of long distance reorderings. We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. Our rules hold the property of long distance reorderings and the compatibility with phrases. Large-scale experimental results show that our model achieves significantly imp"
D13-1108,W07-0706,1,0.921503,"els, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that represent the source side as head-dependents relations which are incorporated with constituency phrasal"
D13-1108,P01-1067,0,0.200551,"results show that our model achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and"
D13-1108,C12-1186,0,0.0116478,"translated into the correct result “reemergence” with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 2g Ñy 2g Ñy 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) 1074 proposed a constituency-to-dependency tran"
D13-1108,2007.mtsummit-papers.71,0,0.0319027,"(means verb phrase), which can be captured by our CHDR-phrasal rules and translated into the correct result “reemergence” with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 2g Ñy 2g Ñy 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based"
D13-1108,J08-3004,0,\N,Missing
D14-1021,W09-0437,0,0.0218632,"2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibility of function word usage, rich morphology and paraphrasing all add to the difficulty of rule extraction. In addition, restricting target word orders by hard translation rules can also hurt output fluency. ∗ * Work done while visiting Singapore University of Technology and Design (SUTD) 177 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 177–182, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics below λ · Pmax are filtered out, where Pmax is the probability of t"
D14-1021,P07-1020,0,0.0211782,"algorithm, and one potential of the SMT architecture is that it can directly benefit from advances in statistical NLG technology. As discussed in the introduction, our work is closely related to previous studies on syntactic MT, with the salient difference that we do not rely on hard translation rules, but allow free target synthesis. The contrast can be summarized as “translation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis comp"
D14-1021,P08-1024,0,0.0633776,"Missing"
D14-1021,J93-2003,0,0.0506297,"minimizing engineering efforts. Shown in Figure 1, the end-to-end system consists of two main components: lexical transfer and synthesis. The former provides candidate translations for (overlapping) source words and phrases. Although lexicons and rules can be used for this step, we take a simple statistical alignment-based approach. The latter searches for a target translation by constructing dependency trees bottom-up. The process can be viewed as a syntax-based generation process from a bag of overlapping translation options. 2.1 Lexical transfer We perform word alignment using IBM model 4 (Brown et al., 1993), and then extract phrase pairs according to the alignment and automaticallyannotated target syntax. In particular, consistent (Och et al., 1999) and cohesive (Fox, 2002) phrase pairs are extracted from intersected alignments in both directions: the target side must form a projective span, with a single root, and the source side must be contiguous. A resulting phrase pair consists of the source phrase, its target translation, as well as the head position and head part-of-speech (POS) of the target span, which are useful for target synthesis. We further restrict that neither the source nor the"
D14-1021,P05-1033,0,0.0949535,"et equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli"
D14-1021,E14-1028,0,0.178381,"Missing"
D14-1021,W11-2107,0,0.0896313,"Missing"
D14-1021,C12-1121,0,0.0359306,"Missing"
D14-1021,W99-0604,0,0.134086,"r provides candidate translations for (overlapping) source words and phrases. Although lexicons and rules can be used for this step, we take a simple statistical alignment-based approach. The latter searches for a target translation by constructing dependency trees bottom-up. The process can be viewed as a syntax-based generation process from a bag of overlapping translation options. 2.1 Lexical transfer We perform word alignment using IBM model 4 (Brown et al., 1993), and then extract phrase pairs according to the alignment and automaticallyannotated target syntax. In particular, consistent (Och et al., 1999) and cohesive (Fox, 2002) phrase pairs are extracted from intersected alignments in both directions: the target side must form a projective span, with a single root, and the source side must be contiguous. A resulting phrase pair consists of the source phrase, its target translation, as well as the head position and head part-of-speech (POS) of the target span, which are useful for target synthesis. We further restrict that neither the source nor the target side of a valid phrase pair contains over s words. Given an input source sentence, the lexical transfer unit finds all valid target transl"
D14-1021,P05-1034,0,0.0509849,"thesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translat"
D14-1021,N13-1025,0,0.0341822,"Missing"
D14-1021,D08-1052,0,0.0180782,"ependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibility of function word"
D14-1021,W02-1039,0,0.0634627,"s for (overlapping) source words and phrases. Although lexicons and rules can be used for this step, we take a simple statistical alignment-based approach. The latter searches for a target translation by constructing dependency trees bottom-up. The process can be viewed as a syntax-based generation process from a bag of overlapping translation options. 2.1 Lexical transfer We perform word alignment using IBM model 4 (Brown et al., 1993), and then extract phrase pairs according to the alignment and automaticallyannotated target syntax. In particular, consistent (Och et al., 1999) and cohesive (Fox, 2002) phrase pairs are extracted from intersected alignments in both directions: the target side must form a projective span, with a single root, and the source side must be contiguous. A resulting phrase pair consists of the source phrase, its target translation, as well as the head position and head part-of-speech (POS) of the target span, which are useful for target synthesis. We further restrict that neither the source nor the target side of a valid phrase pair contains over s words. Given an input source sentence, the lexical transfer unit finds all valid target translation options for overlap"
D14-1021,N04-1035,0,0.0948608,"phrases to their target equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage p"
D14-1021,P06-1139,0,0.0331291,"urrently rather separated research fields. The system is not strongly dependent on the specific generation algorithm, and one potential of the SMT architecture is that it can directly benefit from advances in statistical NLG technology. As discussed in the introduction, our work is closely related to previous studies on syntactic MT, with the salient difference that we do not rely on hard translation rules, but allow free target synthesis. The contrast can be summarized as “translation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reorderin"
D14-1021,P12-2061,0,0.0155088,"actic MT, with the salient difference that we do not rely on hard translation rules, but allow free target synthesis. The contrast can be summarized as “translation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming th"
D14-1021,P09-1091,0,0.221722,"ng on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et a"
D14-1021,E09-1097,0,0.299711,"et word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SM"
D14-1021,D09-1043,0,0.0746518,"U comparable to the state-of-the-art syntactic SMT systems. Figure 1: Overall system architecture. A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features. Both adequacy and fluency can potentially be improved by giving full flexibility to target synthesis, and leaving all options to the statistical model. The main challenge to this method is a significant increase in the search space (Knight, 1999). To this end, recent advances in tackling complex search tasks for text generation offer some solutions (White and Rajkumar, 2009; Zhang and Clark, 2011). In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG). The overall architecture is shown in Figure 1. Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis s"
D14-1021,J99-4005,0,0.137947,"ined consistently in a discriminative model. Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems. Figure 1: Overall system architecture. A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features. Both adequacy and fluency can potentially be improved by giving full flexibility to target synthesis, and leaving all options to the statistical model. The main challenge to this method is a significant increase in the search space (Knight, 1999). To this end, recent advances in tackling complex search tasks for text generation offer some solutions (White and Rajkumar, 2009; Zhang and Clark, 2011). In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG). The overall architecture is shown in Figure 1. Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing t"
D14-1021,J10-4005,0,0.0465794,"Missing"
D14-1021,P12-3004,1,0.847822,"Chinese-English dataset, which consists of training sentence pairs from the dialog task (dialog) and Basic Travel and Expression Corpus (BTEC). The union of dialog and BTEC are taken as our training set, which contains 30,033 sentence pairs. For system tuning, we use the IWSLT 2004 test set (also released as the second development test set of IWSLT 2010), which contains 500 sentences. For final test, we use the IWSLT 2003 test set (also released as the first development test set of IWSLT 2010), which contains 506 sentences. The Chinese sentences in the datasets are segmented using NiuTrans3 (Xiao et al., 2012), while POS-tagging of both English and Chinese is performed using ZPar4 version 0.5 (Zhang and Clark, 2011). We train the English POS-tagger using the WSJ sections of the Penn Treebank (Marcus et al., 1993), turned into lower-case. For syntactic parsing of both English and Chinese, we use the default models of ZPar 0.5. We choose three baseline systems: a string-totree (S2T) system, a tree-to-string (T2S) system and a tree-to-tree (T2T) system (Koehn, 2010). The Moses release 1.0 implementations of all three systems are used, with default parameter settings. IRSTLM5 release 5.80.03 (Federico"
D14-1021,D11-1020,1,0.858858,"both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibility of function word usage, rich morphol"
D14-1021,P06-1096,0,0.0607434,"Missing"
D14-1021,P06-1077,1,0.766363,"s, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), whi"
D14-1021,W06-1606,0,0.034268,"output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibi"
D14-1021,J93-2004,0,0.0459525,"Missing"
D14-1021,D13-1112,0,0.0442095,"Missing"
D14-1021,D11-1106,1,0.885569,"of-the-art syntactic SMT systems. Figure 1: Overall system architecture. A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features. Both adequacy and fluency can potentially be improved by giving full flexibility to target synthesis, and leaving all options to the statistical model. The main challenge to this method is a significant increase in the search space (Knight, 1999). To this end, recent advances in tackling complex search tasks for text generation offer some solutions (White and Rajkumar, 2009; Zhang and Clark, 2011). In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG). The overall architecture is shown in Figure 1. Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), w"
D14-1021,E12-1075,1,0.701608,") translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SMT. Acknowledgement The work has been supported by the Singapore Ministration of Education Tier 2 project T2"
D14-1021,P07-1002,0,\N,Missing
D14-1021,C14-1104,1,\N,Missing
D14-1060,E14-1035,0,0.0217039,"acketing model to reward translation hypothe546 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of mach"
D14-1060,E12-1073,0,0.0152036,"tes separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. This section presents the three models of term translation. They are the term translation disambiguation model, term translation consistency model and term bracketing model respectively. 4 4.1 Models Term Translation Disambiguation Model The most straightforward way to disambiguate term translations in different domain"
D14-1060,W14-3358,0,0.0581314,"acketing model to reward translation hypothe546 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of mach"
D14-1060,J07-2003,0,0.423966,"should be translated consistently. Term translation consistency has not been investigated in these studies. Our term bracketing model is also related to Xiong et al. (2009)’s syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is bracketable or not using rich syntactic constraints. The difference is that we construct the model with automatically created bilingual term bank and do not depend on any syntactic knowledge. ses where bracketable multi-word terms are translated as a whole unit. We integrate the three models into hierarchical phrase-based SMT (Chiang, 2007). Large-scale experiment results show that they are all able to achieve significant improvements of up to 0.89 BLEU points over the baseline. When simultaneously integrating the three models into SMT, we can gain a further improvement, which outperforms the baseline by up to 1.16 BLEU points. In the remainder of this paper, we begin with a brief overview of related work in Section 2, and bilingual term extraction in Section 3. We then elaborate the proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Secti"
D14-1060,W07-2415,0,0.0148819,"erefore, we use these two methods to extract monolingual multiword terms and then combine the extracted terms. Currently, there are mainly two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. This section presents the three models of term translation"
D14-1060,P11-2031,0,0.150467,"pirically, we set the maximum length of a term to 6 words5 . For both the C-value/NC-value and LLRbased extraction methods, we set the context window size to 5 words, which is a widely-used setting in previous work. And we set C-value/NCvalue score threshold to 0 and LLR score threshold to 10 according to the training corpora. We used the case-insensitive 4-gram BLEU6 as our evaluation metric. In order to alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for all our experiments and presented the average BLEU scores on the three runs following the suggestion by Clark et al. (2011). We used an in-house hierarchical phrase-based decoder to verify our proposed models. Although the decoder translates a document in a sentenceby-sentence fashion, it incorporates documentinformed information for sentence translation via the proposed term translation models trained on documents. Experiments In this section, we conducted experiments to answer the following three questions. 1. Are our term translation disambiguation, consistency and bracketing models able to improve translation quality in BLEU? 2. Does the combination of the three models provide further improvements? 3. To what"
D14-1060,itagaki-aikawa-2008-post,0,0.0268507,"ranslated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of machine translation. These studies do not focus on the three issues of term translation as discussed in Section 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit dom"
D14-1060,2007.mtsummit-papers.36,0,0.137099,"es is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of machine translation. These studies do not focus on the three issues of term translation as discussed in Section 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. (2007). Ture et al. (2012) use IR-inspired tf-idf scores to encourage consistent translation choice. Guillou (2013) investigates what kind of words should be translated consistently. Term translation consistency has not been"
D14-1060,N03-1017,0,0.0461755,"t of 4.28M sentence pairs extracted from LDC1 data with document boundaries explicitly provided. The bilingual training data contain 67,752 documents, 124.8M Chinese words and 140.3M English words. We chose NIST MT05 as the MERT (Och, 2003) tuning set, NIST MT06 as the development test set, and NIST MT08 as the final test set. The numbers of documents/sentences in NIST MT05, MT06 and MT08 are 100/1082, 79/1664 and 109/1357 respectively. The word alignments were obtained by running GIZA++ (Och and Ney, 2003) on the corpora in both directions and using the “grow-diag-finaland” balance strategy (Koehn et al., 2003). We adopted SRI Language Modeling Toolkit (Stolcke and others, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the English Gigaword corpus. For the topic model, we used the open source 2 http://sourceforge.net/projects/gibbslda/ http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://nlp.stanford.edu/software/tagger.shtml 5 We determine the maximum length of a term by testing {5, 6, 7, 8} in our preliminary experiments. We find that length 6 produces a slightly better performance than other values. 6 ftp://jaguar.ncsl.nist.gov/mt/"
D14-1060,P12-2023,0,0.0198425,"dels for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these studies, the topic model is not used to ad"
D14-1060,E09-1057,0,0.0196172,"o extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. This section presents the three models of term translation. They are the term translation disambiguation model, term translation consistency model and term bracketing model respectively. 4 4.1 Models Term Translation Disambiguation Model The most straightforward way to disambiguate term translati"
D14-1060,W07-2456,0,0.0231945,"pora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. This section presents the three models of term translation. They are the term translation disambiguation model, term translation consistency model and term bracketing model respectively. 4 4.1 Models Term Translation Disambiguation Model The most straightforward way to disam"
D14-1060,D11-1084,0,0.0602182,"istribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these studies, the topic model is not used to address the issues of term translation mentioned in Section 1. Our work is also related to document-level SMT in that we use document-informed information for term translation. Tiedemann (2010) propose cache-based language and translation models, which are built on recently translated sentences. Gong et al. (2011) extend this by further introducing two additional caches. They employ a static cache to store bilingual phrases extracted 3 Bilingual Term Extraction Bilingual term extraction is to extract terms from two languages with the purpose of creating or ex547 tending a bilingual term bank, which in turn can be used to improve other tasks such as information retrieval and machine translation. In this paper, we want to automatically build a bilingual term bank so that we can model term translation to improve translation quality of SMT. Our interest is to extract multi-word terms. tracts more flexible"
D14-1060,J03-1002,0,0.00837229,"hat extent do the proposed models affect the translations of test sets? 5.1 Setup Our training data consist of 4.28M sentence pairs extracted from LDC1 data with document boundaries explicitly provided. The bilingual training data contain 67,752 documents, 124.8M Chinese words and 140.3M English words. We chose NIST MT05 as the MERT (Och, 2003) tuning set, NIST MT06 as the development test set, and NIST MT08 as the final test set. The numbers of documents/sentences in NIST MT05, MT06 and MT08 are 100/1082, 79/1664 and 109/1357 respectively. The word alignments were obtained by running GIZA++ (Och and Ney, 2003) on the corpora in both directions and using the “grow-diag-finaland” balance strategy (Koehn et al., 2003). We adopted SRI Language Modeling Toolkit (Stolcke and others, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the English Gigaword corpus. For the topic model, we used the open source 2 http://sourceforge.net/projects/gibbslda/ http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://nlp.stanford.edu/software/tagger.shtml 5 We determine the maximum length of a term by testing {5, 6, 7, 8} in our preliminary experiments. We fi"
D14-1060,P03-1021,0,0.0697129,"n probability Pd (tei |tfi , D ) given the docu0 ment D is formulated as follows: 0 Pd (tei |tfi , D ) = K X 0 p(tei |tfi , z = k) ∗ p(z = k|D ) (3) k=1 Whenever a source term tfi is translated into tei , we check whether the pair of tfi and its translation tei can be found in our bilingual term bank. If it can be found, we calculate the conditional translation probability from tfi to tei given the document 0 D according to Eq. (3). The term translation disambiguation model is integrated into the log-linear model of SMT as a feature. Its weight is tuned via minimum error rate training (MERT) (Och, 2003). Through the feature, we can enable the decoder to favor translation hypotheses that contain target term translations appropriate for the domain represented by the topic distribution of the corresponding document. 4.2 Qk = m=1 n=1 Nm M X X qmn ∗ p(k|m) (5) m=1 n=1 where M is the number of documents in which the source term tf occurs, Nm is the number of unique corresponding term translations of tf in the mth document, qmn is the frequency of the nth translation of tf in the mth document, p(k|m) is the conditional probability of the mth document over topic k, and Qk is the normalization factor"
D14-1060,W13-3302,0,0.016021,"tion 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. (2007). Ture et al. (2012) use IR-inspired tf-idf scores to encourage consistent translation choice. Guillou (2013) investigates what kind of words should be translated consistently. Term translation consistency has not been investigated in these studies. Our term bracketing model is also related to Xiong et al. (2009)’s syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is bracketable or not using rich syntactic constraints. The difference is that we construct the model with automatically created bilingual term bank and do not depend on any syntactic knowledge. ses where bracketable multi-word terms are translated as a whole unit. We integrate the three models int"
D14-1060,D12-1108,0,0.0215582,"(Vasconcellos et al., 2001). In this paper, we study domain-specific and context-sensitive term translation for SMT. • Term Bracketing Model: We use the bracketing model to reward translation hypothe546 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicat"
D14-1060,W06-2403,0,0.012483,"cts more flexible terms, these two methods are complementary to each other. Therefore, we use these two methods to extract monolingual multiword terms and then combine the extracted terms. Currently, there are mainly two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as"
D14-1060,P09-1036,1,0.931411,"with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. (2007). Ture et al. (2012) use IR-inspired tf-idf scores to encourage consistent translation choice. Guillou (2013) investigates what kind of words should be translated consistently. Term translation consistency has not been investigated in these studies. Our term bracketing model is also related to Xiong et al. (2009)’s syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is bracketable or not using rich syntactic constraints. The difference is that we construct the model with automatically created bilingual term bank and do not depend on any syntactic knowledge. ses where bracketable multi-word terms are translated as a whole unit. We integrate the three models into hierarchical phrase-based SMT (Chiang, 2007). Large-scale experiment results show that they are all able to achieve significant improvements of up to 0.89 BLEU points over the baseline. When simultaneous"
D14-1060,W09-2907,1,0.898735,"Missing"
D14-1060,P12-1048,1,0.841057,"then elaborate the proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these"
D14-1060,D13-1163,1,0.848608,"we study domain-specific and context-sensitive term translation for SMT. • Term Bracketing Model: We use the bracketing model to reward translation hypothe546 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a"
D14-1060,W10-2602,0,0.0853383,"elationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these studies, the topic model is not used to address the issues of term translation mentioned in Section 1. Our work is also related to document-level SMT in that we use document-informed information for term translation. Tiedemann (2010) propose cache-based language and translation models, which are built on recently translated sentences. Gong et al. (2011) extend this by further introducing two additional caches. They employ a static cache to store bilingual phrases extracted 3 Bilingual Term Extraction Bilingual term extraction is to extract terms from two languages with the purpose of creating or ex547 tending a bilingual term bank, which in turn can be used to improve other tasks such as information retrieval and machine translation. In this paper, we want to automatically build a bilingual term bank so that we can model"
D14-1060,P06-2124,0,0.02353,"ion in Section 3. We then elaborate the proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during tran"
D14-1060,N12-1046,0,0.0216783,"ation. These studies do not focus on the three issues of term translation as discussed in Section 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. (2007). Ture et al. (2012) use IR-inspired tf-idf scores to encourage consistent translation choice. Guillou (2013) investigates what kind of words should be translated consistently. Term translation consistency has not been investigated in these studies. Our term bracketing model is also related to Xiong et al. (2009)’s syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is bracketable or not using rich syntactic constraints. The difference is that we construct the model with automatically created bilingual term bank and do not depend on any syntactic knowledge. ses where brack"
D14-1060,I08-2084,0,0.0314953,"ild a bilingual term bank so that we can model term translation to improve translation quality of SMT. Our interest is to extract multi-word terms. tracts more flexible terms, these two methods are complementary to each other. Therefore, we use these two methods to extract monolingual multiword terms and then combine the extracted terms. Currently, there are mainly two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases"
D14-1060,D12-1097,0,0.0241244,"01). In this paper, we study domain-specific and context-sensitive term translation for SMT. • Term Bracketing Model: We use the bracketing model to reward translation hypothe546 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingua"
D14-1060,P12-1079,1,0.927846,"e proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these studies, the topic"
D15-1004,W08-0336,0,0.372538,"Missing"
D15-1004,W09-2307,0,0.173319,"n is produced. 5 Experiment We conduct experiments on Chinese–English and German–English translation tasks. 5.1 Settings Datasets 5.3 The Chinese–English training corpus is from LDC, including LDC2002E18, LDC2003E07, LDC2003E14, LDC2004T07, the Hansards portion of LDC2004T08 and LDC2005T06. NIST 2002 is taken as a development set to tune weights, and NIST 2004 (MT04) and NIST 2005 (MT05) are two test sets to evaluate systems. Table 1 provides a summary of this corpus. The Stanford Chinese word segmenter (Chang et al., 2008) is used to segment Chinese sentences. The Stanford dependency parser (Chang et al., 2009) parses a Chinese sentence into a projective dependency tree which is then converted to a dependency graph in our model. The German–English training corpus is from WMT 2014, including Europarl V7 and News Commentary. News-test 2011 is taken as a development set, while News-test 2012 (WMT12) and News-test 2013 (WMT13) are our test sets. Table 1 provides a summary of this corpus. We Results Table 2 shows the scores of all three metrics on all systems. Similar to Li et al. (2014), in our experiments Dep2Str has on average a comparable result with Moses HPB in terms of BLEU and METEOR scores. Howe"
D15-1004,C12-1083,0,0.234876,"Missing"
D15-1004,P96-1041,0,0.110194,"periments. In addition, translation results from a recently open-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights. To obtain more reliable results, in each experiment, we run MERT three times and report average scores. These scores are calculated by three widely used automatic metrics in case-insensitive mode: BLEU, METEOR and TER. #words(EN) 55M+ 74,753 72,988 64,810 Table 1: Chinese–English (ZH–EN) and German– English (DE–EN) corpora. For the English side of dev and test sets, words counts are averaged across all references. for each fragment, the decoder finds rules to translate it. The translation of a large span can be obtained"
D15-1004,P13-1091,0,0.0258494,"ize and produce graphs. Following HRG, the graph we use in this 33 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 33–43, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Y X 1 Shijiebei Zai Y Nanfei 1 1 FIFA 1 Juxing Juxing Juxing S X Y Chenggong 2010 Shijiebei Chenggong Zai 2010 Zai 2010 Nanfei Chenggong FIFA Nanfei Figure 1: An example of a derivation in an ERG. Dark circles are external nodes. • S ∈ N is the start symbol. paper is connected, nodes ordered, acyclic and has edge labels but no node labels (Chiang et al., 2013). We provide some formal definitions on ERG. • V is a finite set of nodes. Figure 1 shows an example of a derivation in an ERG to produce a graph. Starting from the start symbol S, when a rule (A → R) is applied to an edge e, the edge is replaced by the graph fragment R. Just like in HRG, the ordering of nodes Ve in e and external nodes XR in R implies the mapping from Ve to XR (Chiang et al., 2013). • E ⊆ V 2 is a finite set of edges. 3 • φ : E → C assigns a label (drawn from C) to each edge. In SMT, we need a synchronous grammar to simultaneously parse an input graph and produce translations"
D15-1004,P07-2045,0,0.00801157,"n of an input graph, the decoder checks if it is a dependency-graph fragment. Then d∈D 37 (2) corpus train dev MT04 MT05 corpus train dev WMT12 WMT13 #sent. 1.5M+ 878 1,597 1,082 #sent. 2M+ 3,003 3,003 3,000 ZH–EN #words(ZH) 38M+ 22,655 43,719 29,880 DE–EN #words(DE) 52M+ 72,661 72,603 63,412 use mate-tools2 to perform morphological analysis and parse German sentences (Bohnet, 2010). Then MaltParser3 converts a parse result into a projective dependency tree (Nivre and Nilsson, 2005). #words(EN) ∼45M 26,905 52,705 35,326 5.2 In this paper, we mainly compare our system (DGST) with HPB in Moses (Koehn et al., 2007). We implement our model in Moses and take the same settings as Moses HPB in all experiments. In addition, translation results from a recently open-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portio"
D15-1004,P05-1033,0,0.313085,"Centre, School of Computing Dublin City University {liangyouli,away,qliu}@computing.dcu.ie Abstract As most available syntactic resources and tools are tree-based, in this paper we propose to convert dependency trees, which are usually taken as a kind of shallow semantic representation, to dependency graphs by labelling edges. We then use a synchronous version of edge replacement grammar (ERG) (Section 2), a special case of HRG, to translate these graphs. The resulting translation model has the same order of magnitude in terms of time complexity with the hierarchical phrasebased model (HPB) (Chiang, 2005) under a certain restriction (Section 3). Compared to dependency tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3). Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We defin"
D15-1004,P12-2007,0,0.0404576,"Missing"
D15-1004,W14-4014,1,0.750254,"ds(ZH) 38M+ 22,655 43,719 29,880 DE–EN #words(DE) 52M+ 72,661 72,603 63,412 use mate-tools2 to perform morphological analysis and parse German sentences (Bohnet, 2010). Then MaltParser3 converts a parse result into a projective dependency tree (Nivre and Nilsson, 2005). #words(EN) ∼45M 26,905 52,705 35,326 5.2 In this paper, we mainly compare our system (DGST) with HPB in Moses (Koehn et al., 2007). We implement our model in Moses and take the same settings as Moses HPB in all experiments. In addition, translation results from a recently open-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights. To obtain more reliable results, in"
D15-1004,W11-2107,0,0.0788531,"014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more suitable for long-distance reordering and translating long sentences. Compared to tree grammars, graph grammars have stronger generative capacity over structures. Based on an edge replacement grammar, in this paper we propose to use a synchronous graph-to-string grammar for statistical machine translation. The graph we use is directly converted from a dependency tree by labelling edges. We build our translation model in the log-linear framework with standard features. Large-scale experiments on Chinese–English and German–English tasks show th"
D15-1004,2005.mtsummit-ebmt.13,0,0.738381,"t allowing hyperedges or only using at most two external nodes reduces the phrase coverage in our model as well. Related Work Dependency structures have been used in SMT for a few years. Because of its better inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be beneficial to translation. Researchers have tried to use dependency structures on both target and source sides. Shen et al. (2010) propose a string-to-dependency model by using dependency fragments of neighbouring words on the target side, which makes the model easier to include a dependency-based language model. Menezes and Quirk (2005) and Quirk et al. (2005) propose the treelet approach which uses dependency structures on the source side. Xiong et al. (2007) extend this approach by allowing gaps in rules. However, their methods need a separate reordering model to decide the position of translated words (insertion problem). To avoid this problem, Xie et al. (2011) propose to use full head-dependent structures of a dependency tree and build a new dependency-to-string model. However, this model has difficulties in handling non-syntactic phrasal rules and ignores treelets. Meng et al. (2013) and Xie et al. (2014) further augme"
D15-1004,W02-1039,0,0.0791488,"re more suitable for longdistance reordering and translating long sentences. Although experiments show significant improvements over baselines, our model has limitations that can be avenues for future work. The restriction used in this paper reduces the time complexity but at the same time reduces the generative capacity of graph grammars. Without allowing hyperedges or only using at most two external nodes reduces the phrase coverage in our model as well. Related Work Dependency structures have been used in SMT for a few years. Because of its better inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be beneficial to translation. Researchers have tried to use dependency structures on both target and source sides. Shen et al. (2010) propose a string-to-dependency model by using dependency fragments of neighbouring words on the target side, which makes the model easier to include a dependency-based language model. Menezes and Quirk (2005) and Quirk et al. (2005) propose the treelet approach which uses dependency structures on the source side. Xiong et al. (2007) extend this approach by allowing gaps in rules. However, their methods need a separate reordering model to deci"
D15-1004,D13-1108,1,0.890369,"Missing"
D15-1004,P10-1064,1,0.888166,"Missing"
D15-1004,P05-1013,0,0.411158,"Missing"
D15-1004,W07-0706,1,0.902806,"dency structures have been used in SMT for a few years. Because of its better inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be beneficial to translation. Researchers have tried to use dependency structures on both target and source sides. Shen et al. (2010) propose a string-to-dependency model by using dependency fragments of neighbouring words on the target side, which makes the model easier to include a dependency-based language model. Menezes and Quirk (2005) and Quirk et al. (2005) propose the treelet approach which uses dependency structures on the source side. Xiong et al. (2007) extend this approach by allowing gaps in rules. However, their methods need a separate reordering model to decide the position of translated words (insertion problem). To avoid this problem, Xie et al. (2011) propose to use full head-dependent structures of a dependency tree and build a new dependency-to-string model. However, this model has difficulties in handling non-syntactic phrasal rules and ignores treelets. Meng et al. (2013) and Xie et al. (2014) further augment this model by incorporating constituent phrases and integrating fix/float structures (Shen et al., 2010), respectively, to"
D15-1004,P02-1038,0,0.101118,"tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3). Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more suitable for long-distance reordering and translating long sentences. Compared to tree grammars, graph grammars have stronger generative capacity over structures. Based on an edge replacement grammar, i"
D15-1004,J03-1002,0,0.00721639,"and Nilsson, 2005). #words(EN) ∼45M 26,905 52,705 35,326 5.2 In this paper, we mainly compare our system (DGST) with HPB in Moses (Koehn et al., 2007). We implement our model in Moses and take the same settings as Moses HPB in all experiments. In addition, translation results from a recently open-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights. To obtain more reliable results, in each experiment, we run MERT three times and report average scores. These scores are calculated by three widely used automatic metrics in case-insensitive mode: BLEU, METEOR and TER. #words(EN) 55M+ 74,753 72,988 64,810 Table 1: Chinese–English (ZH–EN)"
D15-1004,J04-4002,0,0.356022,"initial pair of P , iff: i where φi are features defined on derivations and λi are feature weights. In our experiments, we use 9 features: • translation probabilities P (s|t) and P (t|s), where s is the source graph fragment and t is the target string. • lexical translation probabilities Plex (s|t) and Plex (t|s). 1. Gji is a dependency-graph fragment. That means it is a connected sub-graph and has at most two external nodes, nodes which connect with nodes outside or are the root. • language model lm(e) over translation e. • rule penalty exp(−1). 2. It is consistent with the word alignment ∼ (Och and Ney, 2004). • word penalty exp(|e|). • glue penalty exp(−1). The set of rules from P satisfies the following: 0 1. If hGji , eji0 i is an initial pair, then • unknown words penalty exp(u(g)), where u(g) is the number of unknown words in a source graph g. 0 hN (Gji ) → Gji , X → eji0 i is a rule, where N (G) defines the nonterminal symbol for G. Our decoder is based on the conventional chart parsing CYK algorithm (Kasami, 1965; Younger, 1967; Cocke and Schwartz, 1970). It searches for the best derivation d∗ among all possible derivations D, as in Equation (2): 2. If hN (R) → R, X → R0 i is a rule of P an"
D15-1004,P03-1021,0,0.0525651,"pen-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights. To obtain more reliable results, in each experiment, we run MERT three times and report average scores. These scores are calculated by three widely used automatic metrics in case-insensitive mode: BLEU, METEOR and TER. #words(EN) 55M+ 74,753 72,988 64,810 Table 1: Chinese–English (ZH–EN) and German– English (DE–EN) corpora. For the English side of dev and test sets, words counts are averaged across all references. for each fragment, the decoder finds rules to translate it. The translation of a large span can be obtained by combining translations from its sub-span usin"
D15-1004,P02-1040,0,0.0927075,"handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more suitable for long-distance reordering and translating long sentences. Compared to tree grammars, graph grammars have stronger generative capacity over structures. Based on an edge replacement grammar, in this paper we propose to use a synchronous graph-to-string grammar for statistical machine translation. The graph we use is directly converted from a dependency tree by labelling edges. We build our translation model in the log-linear framework with standard features. Large-sca"
D15-1004,P05-1034,0,0.729257,"translation model has the same order of magnitude in terms of time complexity with the hierarchical phrasebased model (HPB) (Chiang, 2005) under a certain restriction (Section 3). Compared to dependency tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3). Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more"
D15-1004,2006.amta-papers.25,0,0.0415351,"(Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more suitable for long-distance reordering and translating long sentences. Compared to tree grammars, graph grammars have stronger generative capacity over structures. Based on an edge replacement grammar, in this paper we propose to use a synchronous graph-to-string grammar for statistical machine translation. The graph we use is directly converted from a dependency tree by labelling edges. We build our translation model in the log-linear framework with standard features. Large-scale experiments on Chinese–E"
D15-1004,D11-1020,1,0.962756,"der of magnitude in terms of time complexity with the hierarchical phrasebased model (HPB) (Chiang, 2005) under a certain restriction (Section 3). Compared to dependency tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3). Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more suitable for long-distance reor"
D15-1004,C14-1209,1,0.905946,"ion 2), a special case of HRG, to translate these graphs. The resulting translation model has the same order of magnitude in terms of time complexity with the hierarchical phrasebased model (HPB) (Chiang, 2005) under a certain restriction (Section 3). Compared to dependency tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3). Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denk"
D15-1004,J10-4005,0,\N,Missing
D15-1004,W14-3362,0,\N,Missing
D16-1027,D16-1053,0,0.025072,"Missing"
D16-1027,P15-2088,1,0.891904,"Missing"
D16-1027,D15-1166,0,0.0416826,"Missing"
D16-1027,P02-1040,0,0.097279,"Missing"
D16-1027,P16-1008,1,0.249335,"Missing"
D16-1027,D11-1020,1,0.66588,"Missing"
D16-1037,D15-1262,0,0.0464737,"hich we describe in succession. Implicit Discourse Relation Recognition Due to the release of Penn Discourse Treebank (Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015"
D16-1037,P15-2015,0,0.0197483,"(Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al. (2016). However, our work differs from theirs significantly, which can be summarized in the following three as"
D16-1037,C14-1088,0,0.0210779,"3,4 , Rongrong Ji1 , Hong Duan1 , Min Zhang2 Xiamen University, Xiamen, China 3610051 Provincial Key Laboratory for Computer Information Processing Technology Soochow University, Suzhou, China 2150062 ADAPT Centre, School of Computing, Dublin City University3 Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences4 zb@stu.xmu.edu.cn, {jssu, rrji, hduan}@xmu.edu.cn qun.liu@dcu.ie, {dyxiong, minzhang}@suda.edu.cn Abstract other relevant natural language processing tasks, such as text summarization (Yoshida et al., 2014), conversation (Higashinaka et al., 2014), question answering (Verberne et al., 2007) and information extraction (Cimiano et al., 2005). Generally, discourse relations can be divided into two categories: explicit and implicit, which can be illustrated in the following example: Implicit discourse relation recognition is a crucial component for automatic discourselevel analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models and propose a variational neural discourse"
D16-1037,Q15-1024,0,0.411507,"and the approximator but also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3). y N Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model pθ (x|z)pθ (y|z), dashed lines denote the variational approximation qφ (z|x, y) to the posterior p(z|x, y) and qφ0 (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ. tures in SVM-based recognition (Pitler et al., 2009; Lin et al., 2009) or sentence embeddings in neural networks-based recognition (Ji and Eisenstein, 2015; Zhang et al., 2015)), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data. Instead, we assume that there is a latent continuous variable z from an underlying semantic space. It is this latent variable that generates both discourse arguments and the corresponding relation"
D16-1037,N16-1037,0,0.0279266,"implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al. (2016). However, our work differs from theirs significantly, which can be summarized in the following three aspects: 1) they employ the recurrent neural network to represent the discourse arguments, while we use the simple feedforward neural network; 2) they treat the discourse relations directly as latent variables, rather than the underlying semantic representation of discourses; 3) their model is optimized in terms of the data likelihood, since the discourse relations are observed during training. However, VarNDRR is optimized under the variational theory. Variational Neural Model In the presence"
D16-1037,P13-1047,0,0.0602548,"ies (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al. (2016). However, our work differs from theirs significantly,"
D16-1037,D09-1036,0,0.36099,"sample z from qφ (z|x, y) that not only bridges the gap between the recognizer and the approximator but also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3). y N Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model pθ (x|z)pθ (y|z), dashed lines denote the variational approximation qφ (z|x, y) to the posterior p(z|x, y) and qφ0 (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ. tures in SVM-based recognition (Pitler et al., 2009; Lin et al., 2009) or sentence embeddings in neural networks-based recognition (Ji and Eisenstein, 2015; Zhang et al., 2015)), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data. Instead, we assume that there is a latent continuous variable z from an underlying semantic space. It is this l"
D16-1037,W10-4310,0,0.0179631,"nition and variational neural model, which we describe in succession. Implicit Discourse Relation Recognition Due to the release of Penn Discourse Treebank (Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud"
D16-1037,W12-1614,0,0.0150111,"Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capab"
D16-1037,D13-1094,0,0.0931866,"Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestig"
D16-1037,P09-1077,0,0.340564,"rization technique to sample z from qφ (z|x, y) that not only bridges the gap between the recognizer and the approximator but also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3). y N Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model pθ (x|z)pθ (y|z), dashed lines denote the variational approximation qφ (z|x, y) to the posterior p(z|x, y) and qφ0 (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ. tures in SVM-based recognition (Pitler et al., 2009; Lin et al., 2009) or sentence embeddings in neural networks-based recognition (Ji and Eisenstein, 2015; Zhang et al., 2015)), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data. Instead, we assume that there is a latent continuous variable z from an underlying semantic"
D16-1037,prasad-etal-2008-penn,0,0.672371,"us making the setting z˜ = µ0 during testing reasonable. The second term is the approximate expectation of Eqφ (z|x,y) [log pθ (x, y|z)], which is also differentiable. As the objective function in Eq. (13) is differentiable, we can optimize both the model parameters θ and variational parameters φ jointly using standard gradient ascent techniques. The training procedure for VarNDRR is summarized in Algorithm 1. 4 Experiments We conducted experiments on English implicit DRR task to validate the effectiveness of VarNDRR.4 4.1 Dataset We used the largest hand-annotated discourse corpus PDTB 2.05 (Prasad et al., 2008) (PDTB hereafter). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work (Pitler et al., 2009; Zhou et al., 2010; Lan et 4 Source code is available https://github.com/DeepLearnXMU/VarNDRR. 5 http://www.seas.upenn.edu/ pdtb/ at Model R & X (2015) J & E (2015) SVM SCNN VarNDRR Acc 70.27 63.10 60.42 63.30 P 22.79 22.00 24.00 R 64.47 67.76 71.05 F1 41.00 35.93 33.68 33.22 35.88 Model (R & X (2015)) (J & E (2015)) SVM SCNN VarNDRR (a) C OM vs Other Model (R & X (2015)) (J & E (2015)) SVM SCNN VarNDRR Acc 6"
D16-1037,E14-1068,0,0.0315926,"Due to the release of Penn Discourse Treebank (Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have bee"
D16-1037,N15-1081,0,0.502665,"h02 = dm = dhy = 400, dy = 2 for all experiments.7 . All parameters of VarNDRR are initialized by a Gaussian distribution (µ = 0, σ = 0.01). For Adam, we set β1 = 0.9, β2 = 0.999 with a learning rate 0.001. Additionally, we tied the following parameters in practice: Wh1 and Wh2 , Wx01 and Wx02 . We compared VarNDRR against the following two different baseline methods: • SVM: a support vector machine (SVM) classifier8 trained with several manual features. • SCNN: a shallow convolutional neural network proposed by Zhang et al. (2015). We also provide results from two state-of-the-art systems: • Rutherford and Xue (2015) convert explicit discourse relations into implicit instances. • Ji and Eisenstein (2015) augment discourse representations via entity connections. 7 8 http://nlp.stanford.edu/software/corenlp.shtml 387 There is one dimension in dx1 and dx2 for unknown words. http://svmlight.joachims.org/ 45 40 35 30 25 20 15 10 5 0 80 70 60 50 40 30 20 10 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 117 218 19 13 420 521 622 723 824 25 9 26 10 27 11 28 1229 1330 1431 1532 1633 1734 1835 1936 2037 2138 1 39 22 2340 2441 2542 43 26 44 27 45 28 46 2947 3048 3149 3250 3351 3452 3553 3654 -1270.24 25.3012 -207.21 26.0"
D16-1037,C12-1168,0,0.0167503,"en exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al. (2016). However, our work differs from thei"
D16-1037,D14-1196,0,0.0259392,", Deyi Xiong2∗, Jinsong Su1 , Qun Liu3,4 , Rongrong Ji1 , Hong Duan1 , Min Zhang2 Xiamen University, Xiamen, China 3610051 Provincial Key Laboratory for Computer Information Processing Technology Soochow University, Suzhou, China 2150062 ADAPT Centre, School of Computing, Dublin City University3 Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences4 zb@stu.xmu.edu.cn, {jssu, rrji, hduan}@xmu.edu.cn qun.liu@dcu.ie, {dyxiong, minzhang}@suda.edu.cn Abstract other relevant natural language processing tasks, such as text summarization (Yoshida et al., 2014), conversation (Higashinaka et al., 2014), question answering (Verberne et al., 2007) and information extraction (Cimiano et al., 2005). Generally, discourse relations can be divided into two categories: explicit and implicit, which can be illustrated in the following example: Implicit discourse relation recognition is a crucial component for automatic discourselevel analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models a"
D16-1037,D15-1266,1,0.82891,"also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3). y N Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model pθ (x|z)pθ (y|z), dashed lines denote the variational approximation qφ (z|x, y) to the posterior p(z|x, y) and qφ0 (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ. tures in SVM-based recognition (Pitler et al., 2009; Lin et al., 2009) or sentence embeddings in neural networks-based recognition (Ji and Eisenstein, 2015; Zhang et al., 2015)), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data. Instead, we assume that there is a latent continuous variable z from an underlying semantic space. It is this latent variable that generates both discourse arguments and the corresponding relation, i.e. p(x, y|z). The"
D16-1037,C10-2172,0,0.205752,"differentiable, we can optimize both the model parameters θ and variational parameters φ jointly using standard gradient ascent techniques. The training procedure for VarNDRR is summarized in Algorithm 1. 4 Experiments We conducted experiments on English implicit DRR task to validate the effectiveness of VarNDRR.4 4.1 Dataset We used the largest hand-annotated discourse corpus PDTB 2.05 (Prasad et al., 2008) (PDTB hereafter). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work (Pitler et al., 2009; Zhou et al., 2010; Lan et 4 Source code is available https://github.com/DeepLearnXMU/VarNDRR. 5 http://www.seas.upenn.edu/ pdtb/ at Model R & X (2015) J & E (2015) SVM SCNN VarNDRR Acc 70.27 63.10 60.42 63.30 P 22.79 22.00 24.00 R 64.47 67.76 71.05 F1 41.00 35.93 33.68 33.22 35.88 Model (R & X (2015)) (J & E (2015)) SVM SCNN VarNDRR (a) C OM vs Other Model (R & X (2015)) (J & E (2015)) SVM SCNN VarNDRR Acc 69.80 60.71 63.00 57.36 P 65.89 56.29 56.46 Acc 76.95 62.62 63.00 53.82 P 39.14 39.80 35.39 R 72.40 75.29 88.53 F1 53.80 52.78 50.82 52.04 50.56 R 68.24 62.35 97.65 F1 33.30 27.63 24.73 30.54 29.54 (b) C ON"
D16-1070,D14-1082,0,0.0427993,"Missing"
D16-1070,D15-1141,0,0.0247885,"main adaptation method of Daum´e III (2007), keeping a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different treebanks into a single combined labelset. A summary of such multi-view methods is shown in Figure 1(b), which demonstrates their main differences compared to stacking (Figure 1(a)). Recently, neural network has gained increasing research attention, with highly competitive results being reported for numerous NLP tasks, including word segmentation (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015), POS-tagging (Ma et al., 2014; Plank et al., 2016), and parsing (Chen and 731 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 731–741, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Training integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training. In addition, the advantage of neural multi-view learning over its discrete counterpart are many-fold. First, it is free from the necessity of manual cross-labelset interactive feature en"
D16-1070,P05-1066,0,0.0194503,"Missing"
D16-1070,P07-1033,0,0.182874,"Missing"
D16-1070,de-marneffe-etal-2006-generating,0,0.0491802,"Missing"
D16-1070,P15-1033,0,0.0250021,"Missing"
D16-1070,I05-3017,0,0.0514718,"dency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emerson, 2003; Emerson, 2005; Xue, 2003). These give rise to the research problem ∗ Work done when the first author was visiting SUTD. The second approach is based on multi-view learning (Johansson, 2013; Li et al., 2015). The idea is to address both annotation styles simultaneously by sharing common feature representations. In particular, Johansson (2013) trained dependency parsers using the domain adaptation method of Daum´e III (2007), keeping a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different treebanks into a s"
D16-1070,P04-1059,0,0.0471347,"RF baseline model. This is due to the higher computation cost of a deep neural network on a CPU. Compared with the CRF baseline, the CRF multi-view model is significantly slower because of its large feature set and the multi-label search space. However, the NN multi-view model achieves almost the same time cost with the NN baseline, and is much more efficient than the CRF counterpart. This shows the efficiency advantage of the NN multi-view model by parameter sharing and output splitting. 7 Related Work Early research on heterogeneous annotations focuses on annotation conversion. For example, Gao et al. (2004) proposed a transformation-based method to convert the annotation style of a word segmentation corpus to that of another. Manually designed transformation templates are used, which makes it difficult to generalize the method to other 4 http://hlt.suda.edu.cn/zhli/resources/zhenghua-acl2015resources.zip 738 tasks and treebanks. Jiang et al. (2009) described a stacking-based model for heterogeneous annotations, using a pipeline to integrate the knowledge from one corpus to another. Sun and Wan (2012) proposed a structure-based stacking model, which makes use of structured features such as sub-wo"
D16-1070,P09-1059,1,0.902162,"only better accuracy improvements, but also an order of magnitude faster speed compared to its discrete baseline, adding little time cost compared to a neural model trained on a single treebank. 1 The task has been tackled using two typical approaches. The first is based on stacking (Wolpert, 1992; Breiman, 1996; Wu et al., 2003). As shown in Figure 1(a), the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features. This method has been used for leveraging two different treebanks for word segmentation (Jiang et al., 2009; Sun and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granulariti"
D16-1070,J15-1005,1,0.892533,"Missing"
D16-1070,W07-2416,0,0.0115331,", the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features. This method has been used for leveraging two different treebanks for word segmentation (Jiang et al., 2009; Sun and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emerson, 2003; Emerson, 2005; Xue, 2003). These give rise to the research problem ∗ Work done when the first author was visiting SUTD. The second approach is based on multi-view learning (Johansson, 2013; Li et al., 2015). The idea is to address both annotation styles simultaneously by sharing common feature representations. In particular, Johansson"
D16-1070,N13-1013,0,0.0763771,"to its discrete baseline, adding little time cost compared to a neural model trained on a single treebank. 1 The task has been tackled using two typical approaches. The first is based on stacking (Wolpert, 1992; Breiman, 1996; Wu et al., 2003). As shown in Figure 1(a), the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features. This method has been used for leveraging two different treebanks for word segmentation (Jiang et al., 2009; Sun and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emerson, 2003; Emerson, 2005; Xue, 2003). These give rise to the researc"
D16-1070,N16-1179,0,0.0168262,"d bt , bc are model parameters. Finally, w  ci is concatenated with word embedding to form final word represeni : i = tation rw rw eiw ⊕ w  ci The output layer employs a conditional random field (CRF) to infer the POS ti of each word wi based on the feature layer outputs. The conditional probability of a tag sequence given an input sentence is: ∏ ∏n x, y i ) ni=1 Φ(x, y i , y i−1 ) i=1 Ψ( , p(y |x) = Z(x) where Z(x) is the partition function: 2.2 Feature Layer Recently, bi-directional LSTM has been successfully applied in various NLP tasks (Liu et al., 2015; Zhou and Xu, 2015; Klerke et al., 2016; Plank et al., 2016). The feature layer uses a bi-directional LSTM to extract a feature vector hi for each word wi , rei ⊕ i−1 ⊕ i−2 ⊕ rw rw spectively. An input vector xi = (rw i+2 ) is used to represent each word w i . i+1 ⊕  rw rw We use a LSTM variation with peephole connections (Graves and Schmidhuber, 2005) to extract features based on x(1:n) . The model computes a hidden vector hi for each input xi , passing information from h1 , ..., hi−1 to hn via a sequence of cell states 733 Z(x) = n ∑∏  y i=1 Ψ(x, y i ) n ∏ Φ(x, y i , y i−1 ) i=1 In particular, the output cliqu"
D16-1070,N16-1030,0,0.0251489,"he aforementioned methods on heterogeneous annotations are investigated mainly for discrete models. It remains an interesting research question how effective multiple treebanks can be utilized by neural NLP models, and we aim to investigate this empirically. We follow Li et al. (2015), taking POS-tagging for case study, using the methods of Jiang et al. (2009) and Li et al. (2015) as the discrete stacking and multi-view training baselines, respectively, and building neural network counterparts to their models for empirical comparison. The base tagger is a neural CRF model (Huang et al., 2015; Lample et al., 2016), which gives competitive accuracies to discrete CRF taggers. Results show that neural stacking allows deeper 732 We adopt a neural CRF with a Long-Short-TermMemory (LSTM) (Hochreiter and Schmidhuber, 1997) feature layer for baseline POS tagger. As shown in Figure 2, the model consists of three main neural layers: the input layer calculates dense representation of input words using attention model on character embeddings; the feature layer employs a bi-directional LSTM model to extract non-local features from input vectors; the output layer uses a CRF structure to infer the most likely label f"
D16-1070,P15-1172,0,0.076347,"guistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emerson, 2003; Emerson, 2005; Xue, 2003). These give rise to the research problem ∗ Work done when the first author was visiting SUTD. The second approach is based on multi-view learning (Johansson, 2013; Li et al., 2015). The idea is to address both annotation styles simultaneously by sharing common feature representations. In particular, Johansson (2013) trained dependency parsers using the domain adaptation method of Daum´e III (2007), keeping a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different treebanks into a single combined labelset. A summary of such multi-view methods is shown in Figure 1(b), which demonstrates their main differences compared to stacking (Figure 1(a)). Recently, neural network has"
D16-1070,D15-1168,0,0.0153753,"of current word wi . Wt , Ut , Wc and bt , bc are model parameters. Finally, w  ci is concatenated with word embedding to form final word represeni : i = tation rw rw eiw ⊕ w  ci The output layer employs a conditional random field (CRF) to infer the POS ti of each word wi based on the feature layer outputs. The conditional probability of a tag sequence given an input sentence is: ∏ ∏n x, y i ) ni=1 Φ(x, y i , y i−1 ) i=1 Ψ( , p(y |x) = Z(x) where Z(x) is the partition function: 2.2 Feature Layer Recently, bi-directional LSTM has been successfully applied in various NLP tasks (Liu et al., 2015; Zhou and Xu, 2015; Klerke et al., 2016; Plank et al., 2016). The feature layer uses a bi-directional LSTM to extract a feature vector hi for each word wi , rei ⊕ i−1 ⊕ i−2 ⊕ rw rw spectively. An input vector xi = (rw i+2 ) is used to represent each word w i . i+1 ⊕  rw rw We use a LSTM variation with peephole connections (Graves and Schmidhuber, 2005) to extract features based on x(1:n) . The model computes a hidden vector hi for each input xi , passing information from h1 , ..., hi−1 to hn via a sequence of cell states 733 Z(x) = n ∑∏  y i=1 Ψ(x, y i ) n ∏ Φ(x, y i , y i"
D16-1070,P14-1014,1,0.44397,"III (2007), keeping a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different treebanks into a single combined labelset. A summary of such multi-view methods is shown in Figure 1(b), which demonstrates their main differences compared to stacking (Figure 1(a)). Recently, neural network has gained increasing research attention, with highly competitive results being reported for numerous NLP tasks, including word segmentation (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015), POS-tagging (Ma et al., 2014; Plank et al., 2016), and parsing (Chen and 731 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 731–741, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Training integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training. In addition, the advantage of neural multi-view learning over its discrete counterpart are many-fold. First, it is free from the necessity of manual cross-labelset interactive feature engineering, which is far from t"
D16-1070,P08-1108,0,0.0109935,"ude faster speed compared to its discrete baseline, adding little time cost compared to a neural model trained on a single treebank. 1 The task has been tackled using two typical approaches. The first is based on stacking (Wolpert, 1992; Breiman, 1996; Wu et al., 2003). As shown in Figure 1(a), the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features. This method has been used for leveraging two different treebanks for word segmentation (Jiang et al., 2009; Sun and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emerson, 2003; Emerson, 2005; Xue, 2003). These give r"
D16-1070,P14-1028,0,0.0577997,"rsers using the domain adaptation method of Daum´e III (2007), keeping a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different treebanks into a single combined labelset. A summary of such multi-view methods is shown in Figure 1(b), which demonstrates their main differences compared to stacking (Figure 1(a)). Recently, neural network has gained increasing research attention, with highly competitive results being reported for numerous NLP tasks, including word segmentation (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015), POS-tagging (Ma et al., 2014; Plank et al., 2016), and parsing (Chen and 731 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 731–741, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Training integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training. In addition, the advantage of neural multi-view learning over its discrete counterpart are many-fold. First, it is free from the necessity of manual cross-labelset in"
D16-1070,P16-2067,0,0.11174,"ng a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different treebanks into a single combined labelset. A summary of such multi-view methods is shown in Figure 1(b), which demonstrates their main differences compared to stacking (Figure 1(a)). Recently, neural network has gained increasing research attention, with highly competitive results being reported for numerous NLP tasks, including word segmentation (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015), POS-tagging (Ma et al., 2014; Plank et al., 2016), and parsing (Chen and 731 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 731–741, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Training integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training. In addition, the advantage of neural multi-view learning over its discrete counterpart are many-fold. First, it is free from the necessity of manual cross-labelset interactive feature engineering, which is far from trivial for representi"
D16-1070,D13-1062,0,0.0224561,"the above discrete methods, our neural stacking method offers further feature integration by directly connecting the feature layer of the source tagger with the input layer of the target tagger. It also allows the finetuning of the source tagger. As one of the reviewers mentioned, two extensions of CRFs, dynamic CRFs (Sutton et al., 2004) and hidden-state CRFs (Quattoni et al., 2004), can also perform similar deep integration and fine-tuning. For multi-view training, Johansson (2013) used a shared feature representation along with separate individual feature representation for each treebank. Qiu et al. (2013) proposed a multi-task learning model to jointly predict two labelsets given an input sentences. The joint model uses the union of baseline features for each labelset, without considering additional features to capture the interaction between the two labelsets. Li et al. (2015) improves upon this method by using a tighter integration between the two labelsets, treating the Cartesian product of the base labels as a single combined labelset, and exploiting joint features from two labelsets. Though capturing label interaction, their method suffers speed penalty from the sharply increased search s"
D16-1070,W03-1719,0,0.0179464,"n and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emerson, 2003; Emerson, 2005; Xue, 2003). These give rise to the research problem ∗ Work done when the first author was visiting SUTD. The second approach is based on multi-view learning (Johansson, 2013; Li et al., 2015). The idea is to address both annotation styles simultaneously by sharing common feature representations. In particular, Johansson (2013) trained dependency parsers using the domain adaptation method of Daum´e III (2007), keeping a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different tre"
D16-1070,P12-1025,0,0.178224,"y improvements, but also an order of magnitude faster speed compared to its discrete baseline, adding little time cost compared to a neural model trained on a single treebank. 1 The task has been tackled using two typical approaches. The first is based on stacking (Wolpert, 1992; Breiman, 1996; Wu et al., 2003). As shown in Figure 1(a), the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features. This method has been used for leveraging two different treebanks for word segmentation (Jiang et al., 2009; Sun and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emers"
D16-1070,P15-1032,0,0.0373545,"Missing"
D16-1070,W03-0433,0,0.0528727,"rogeneous annotations using neural network models, building a neural network counterpart to discrete stacking and multiview learning, respectively, finding that neural models have their unique advantages thanks to the freedom from manual feature engineering. Neural model achieves not only better accuracy improvements, but also an order of magnitude faster speed compared to its discrete baseline, adding little time cost compared to a neural model trained on a single treebank. 1 The task has been tackled using two typical approaches. The first is based on stacking (Wolpert, 1992; Breiman, 1996; Wu et al., 2003). As shown in Figure 1(a), the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features. This method has been used for leveraging two different treebanks for word segmentation (Jiang et al., 2009; Sun and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada an"
D16-1070,O03-4002,0,0.105777,"Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emerson, 2003; Emerson, 2005; Xue, 2003). These give rise to the research problem ∗ Work done when the first author was visiting SUTD. The second approach is based on multi-view learning (Johansson, 2013; Li et al., 2015). The idea is to address both annotation styles simultaneously by sharing common feature representations. In particular, Johansson (2013) trained dependency parsers using the domain adaptation method of Daum´e III (2007), keeping a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different treebanks into a single combin"
D16-1070,W03-3023,0,0.027284,"l., 2003). As shown in Figure 1(a), the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features. This method has been used for leveraging two different treebanks for word segmentation (Jiang et al., 2009; Sun and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emerson, 2003; Emerson, 2005; Xue, 2003). These give rise to the research problem ∗ Work done when the first author was visiting SUTD. The second approach is based on multi-view learning (Johansson, 2013; Li et al., 2015). The idea is to address both annotation styles simultaneously by sharing common feature represe"
D16-1070,D08-1059,1,0.401755,"xd , Θ), 735 For neural multi-view model, we follow Li et al. (2015) and take a the corpus-weighting strategy to sample a number of training instances from both corpora for each training iteration, as shown in Algorithm 1. At each epoch, we randomly sample from the two datasets according to a corpus weights ratio, namely the ratio between the number of sentences in each dataset used for training, to form a training set for the epoch. Experiments 96 We adopt the Penn Chinese Treebank version 5.0 (CTB5) (Xue et al., 2005) as our main corpus, with the standard data split following previous work (Zhang and Clark, 2008; Li et al., 2015). People’s Daily (PD) is used as second corpus with a different scheme. We filter out PD sentences longer than 200 words. Details of the datasets are listed in Table 1. The standard token-wise POS tagging accuracy is used as the evaluation metric. The systems are implemented with LibN3L (Zhang et al., 2016). For all the neural models, we set the hidden layer size to 100, the initial learning rate for Adagrad to 0.01 and the regularization parameter λ to 10−8 . word2vec1 is used to pretrain word embeddings. The Chinese Giga-word corpus version 5 (Graff and Chen, 2005), segment"
D16-1070,J11-1005,1,0.489625,"2015). People’s Daily (PD) is used as second corpus with a different scheme. We filter out PD sentences longer than 200 words. Details of the datasets are listed in Table 1. The standard token-wise POS tagging accuracy is used as the evaluation metric. The systems are implemented with LibN3L (Zhang et al., 2016). For all the neural models, we set the hidden layer size to 100, the initial learning rate for Adagrad to 0.01 and the regularization parameter λ to 10−8 . word2vec1 is used to pretrain word embeddings. The Chinese Giga-word corpus version 5 (Graff and Chen, 2005), segmented by zpar2 (Zhang and Clark, 2011), is used for the training corpus for word embeddings. The size of word embedding is 50. 6.2 Development Experiments We use the development dataset for two main purposes. First, under each setting, we tune the model parameters, such as the number of training epochs. Second, we study the influence of several important hyper-parameters using the development dataset. For example, for the NN multi-view learning model, the corpus weights ratio (section 5) plays an important role for the performance. We determine the parameters of the model by studying the accuracy along with the increasing epochs."
D16-1070,P16-1147,0,0.102299,"his method by using a tighter integration between the two labelsets, treating the Cartesian product of the base labels as a single combined labelset, and exploiting joint features from two labelsets. Though capturing label interaction, their method suffers speed penalty from the sharply increased search space. In contrast to their methods, our neural approach enables parameter sharing in the hidden layers, thereby modeling label interaction without directly combining the two output labelsets. This leads to a lean model with almost the same time efficiency as a single-label baseline. Recently, Zhang and Weiss (2016) proposed a stack-propagation model for learning a stacked pipeline of POS tagging and dependency parsing. Their method is similar to our neural stacking in fine-tuning the stacked module which yields features for the target model. While their multi-task learning is on heterogenous tasks, our multi-task learning is defined on heterogenous treebanks. 8 Conclusion We investigated two methods for utilizing heterogeneous annotations for neural network models, showing that they have respective advantages compared to their discrete counterparts. In particular, neural stacking allows tighter feature"
D16-1070,L16-1034,1,0.832797,"atio between the number of sentences in each dataset used for training, to form a training set for the epoch. Experiments 96 We adopt the Penn Chinese Treebank version 5.0 (CTB5) (Xue et al., 2005) as our main corpus, with the standard data split following previous work (Zhang and Clark, 2008; Li et al., 2015). People’s Daily (PD) is used as second corpus with a different scheme. We filter out PD sentences longer than 200 words. Details of the datasets are listed in Table 1. The standard token-wise POS tagging accuracy is used as the evaluation metric. The systems are implemented with LibN3L (Zhang et al., 2016). For all the neural models, we set the hidden layer size to 100, the initial learning rate for Adagrad to 0.01 and the regularization parameter λ to 10−8 . word2vec1 is used to pretrain word embeddings. The Chinese Giga-word corpus version 5 (Graff and Chen, 2005), segmented by zpar2 (Zhang and Clark, 2011), is used for the training corpus for word embeddings. The size of word embedding is 50. 6.2 Development Experiments We use the development dataset for two main purposes. First, under each setting, we tune the model parameters, such as the number of training epochs. Second, we study the inf"
D16-1070,D13-1061,0,0.0166608,"rained dependency parsers using the domain adaptation method of Daum´e III (2007), keeping a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different treebanks into a single combined labelset. A summary of such multi-view methods is shown in Figure 1(b), which demonstrates their main differences compared to stacking (Figure 1(a)). Recently, neural network has gained increasing research attention, with highly competitive results being reported for numerous NLP tasks, including word segmentation (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015), POS-tagging (Ma et al., 2014; Plank et al., 2016), and parsing (Chen and 731 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 731–741, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Training integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training. In addition, the advantage of neural multi-view learning over its discrete counterpart are many-fold. First, it is free from the necessity of manual"
D16-1070,P15-1109,0,0.0129467,"i . Wt , Ut , Wc and bt , bc are model parameters. Finally, w  ci is concatenated with word embedding to form final word represeni : i = tation rw rw eiw ⊕ w  ci The output layer employs a conditional random field (CRF) to infer the POS ti of each word wi based on the feature layer outputs. The conditional probability of a tag sequence given an input sentence is: ∏ ∏n x, y i ) ni=1 Φ(x, y i , y i−1 ) i=1 Ψ( , p(y |x) = Z(x) where Z(x) is the partition function: 2.2 Feature Layer Recently, bi-directional LSTM has been successfully applied in various NLP tasks (Liu et al., 2015; Zhou and Xu, 2015; Klerke et al., 2016; Plank et al., 2016). The feature layer uses a bi-directional LSTM to extract a feature vector hi for each word wi , rei ⊕ i−1 ⊕ i−2 ⊕ rw rw spectively. An input vector xi = (rw i+2 ) is used to represent each word w i . i+1 ⊕  rw rw We use a LSTM variation with peephole connections (Graves and Schmidhuber, 2005) to extract features based on x(1:n) . The model computes a hidden vector hi for each input xi , passing information from h1 , ..., hi−1 to hn via a sequence of cell states 733 Z(x) = n ∑∏  y i=1 Ψ(x, y i ) n ∏ Φ(x, y i , y i−1 ) i=1 In particu"
D16-1070,P15-1117,1,0.472129,"plementations of our neural network stacking and multi-view learning models are available under GPL, at B model Train B model Corpus B & A labels A model Corpus B Train A model Corpus A Testing Output B B model Raw sentence Output A A model (a) stacking Multi-view model Training Train multi-view model Corpus B Corpus A Testing Output A https://github.com/chenhongshen/NNHetSeq. Output B Multi-view model 2 Baseline Neural Network Tagger Raw sentence (b) multi-view learning Figure 1: Two main approaches to utilizing heterogeneous annotations. Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015). On the other hand, the aforementioned methods on heterogeneous annotations are investigated mainly for discrete models. It remains an interesting research question how effective multiple treebanks can be utilized by neural NLP models, and we aim to investigate this empirically. We follow Li et al. (2015), taking POS-tagging for case study, using the methods of Jiang et al. (2009) and Li et al. (2015) as the discrete stacking and multi-view training baselines, respectively, and building neural network counterparts to their models for empirical comparison. The base tagger is a neural CRF model"
D17-1105,W16-2358,0,0.166954,"Missing"
D17-1105,W16-2359,1,0.456517,"-based models to translate between many languages (Dong et al., 2015; Firat et al., 2016). However, multi-modal MT has only recently been addressed by the MT community in the form of a shared task (Specia et al., 2016). We note that in the official results of this first shared task no submissions based on a purely neural architecture could improve on the Phrase-Based SMT (PBSMT) baseline. Nevertheless, researchers have proposed to include global visual features in reranking n-best lists generated by a PBSMT system or directly in a purely NMT framework with some success (Caglayan et al., 2016; Calixto et al., 2016; Libovick´y et al., 2016; Shah et al., 2016). The best results achieved by a purely NMT model in this shared task are those of Huang et al. (2016), who proposed to use global and regional image features extracted with the VGG19 (Simonyan and Zisserman, 2014) and the RCNN (Girshick et al., 2014) convolutional neural networks (CNNs). Similarly to one of the three models we proData Huang et al. (2016) use object detections obtained with the RCNN of Girshick et al. (2014) as additional data, whereas we study the impact that additional back-translated data brings. Performance All our models outper"
D17-1105,P17-1175,1,0.324376,".1) 39.5 (↓ 3.3) 68.1 (↑ 0.4) 68.3 (↑ 0.6) 68.5 (↑ 0.8) Table 3: Results for different combinations of multi-modal models, all trained on the original M30kT training data only, evaluated on the M30kT test set. PBSMT+ uses image features as well as additional data from WordNet and, to the best of our knowledge, is the best published model in this language pair and data set to date. and uses local image features. Finally, we have recently participated in the WMT 2017 multimodal MT shared task, and our system submissions ranked among the best performing systems under the constrained data regime (Calixto et al., 2017a). We note that our models performed particularly well on the ambiguous MSCOCO test set (Elliott et al., 2017), which indicate its ability to use the image information in disambiguating difficult source sentences into their correct translations. Ensemble decoding We now report on how can ensemble decoding be used to improve translations obtained with our multi-modal NMT models. In order to do that, we use different combinations of models trained on the original M30kT training set to translate from English into German. We built ensembles of different models by starting with our best performing"
D17-1105,W14-4012,0,0.182362,"Missing"
D17-1105,D14-1179,0,0.013097,"Missing"
D17-1105,P11-2031,0,0.0373624,"., 2017), where each training instance consists of one English sentence, one German sentence and one image. We apply early stopping for model selection based on BLEU scores, so that if a model does not improve on the validation set for more than 20 epochs, training is halted. We evaluate our models’ translation quality quantitatively in terms of BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al., 2006), and chrF3 scores5 (Popovi´c, 2015) and we report statistical significance for the three first metrics using approximate randomisation computed with MultEval (Clark et al., 2011). As our main baseline we train an attentionbased NMT model (§2) in which only the textual part of M30kT is used for training. We also train a PBSMT model built with Moses on the same English→German (German→English) data, where the LM is a 5–gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995) trained on the German (English) of the M30kT dataset. We use minimum error rate training (Och, 2003) for tuning the model parameters for BLEU scores. Our third baseline (English→German), is the best comparable multi-modal model by Huang et al. (2016) and also their best model with additional"
D17-1105,W14-3348,0,0.564574,"of 2.2.2 Images for encoder initialisation: IMGE In the original attention-based NMT model described in §2, the hidden state of the encoder is #» initialised with the zero vector 0 . Instead, we propose to use two new single-layer feed-forward neural networks to compute the initial states of the → − ← − forward RNN Φ enc and the backward RNN Φ enc , 4 Outputs would typically consist of sets of 2-5 words repeated many times, usually without any syntax. For comparison, translations for the translated Multi30k test set (described in §3) achieve just 3.8 BLEU (Papineni et al., 2002), 15.5 METEOR (Denkowski and Lavie, 2014) and 93.0 TER (Snover et al., 2006). 995 For each of the 30K images in the Flickr30k, the M30kT has one of its English descriptions manually translated into German by a professional translator. Training, validation and test sets contain 29K, 1014 and 1K images, respectively, each accompanied by one sentence pair (the original English sentence and its German translation). For each of the 30K images in the Flickr30k, the M30kC has five descriptions in German collected independently of the English descriptions. Training, validation and test sets contain 29K, 1014 and 1K images, respectively, each"
D17-1105,P15-1166,0,0.0284649,"features separately either as a word in the source sentence (§2.2.1) or directly for encoder (§2.2.2) or decoder initialisation (§2.2.3), whereas Huang et al. (2016) only use it as a word. We also show it is better to include an image exclusively for the encoder or the decoder initialisation (Tables 1 and 2). Related work Attention-based encoder-decoder models for MT have been actively investigated in recent years. Some researchers have studied how to improve attention mechanisms (Luong et al., 2015; Tu et al., 2016) and how to train attention-based models to translate between many languages (Dong et al., 2015; Firat et al., 2016). However, multi-modal MT has only recently been addressed by the MT community in the form of a shared task (Specia et al., 2016). We note that in the official results of this first shared task no submissions based on a purely neural architecture could improve on the Phrase-Based SMT (PBSMT) baseline. Nevertheless, researchers have proposed to include global visual features in reranking n-best lists generated by a PBSMT system or directly in a purely NMT framework with some success (Caglayan et al., 2016; Calixto et al., 2016; Libovick´y et al., 2016; Shah et al., 2016). T"
D17-1105,W16-3210,0,0.0444801,"Missing"
D17-1105,N16-1101,0,0.059879,"either as a word in the source sentence (§2.2.1) or directly for encoder (§2.2.2) or decoder initialisation (§2.2.3), whereas Huang et al. (2016) only use it as a word. We also show it is better to include an image exclusively for the encoder or the decoder initialisation (Tables 1 and 2). Related work Attention-based encoder-decoder models for MT have been actively investigated in recent years. Some researchers have studied how to improve attention mechanisms (Luong et al., 2015; Tu et al., 2016) and how to train attention-based models to translate between many languages (Dong et al., 2015; Firat et al., 2016). However, multi-modal MT has only recently been addressed by the MT community in the form of a shared task (Specia et al., 2016). We note that in the official results of this first shared task no submissions based on a purely neural architecture could improve on the Phrase-Based SMT (PBSMT) baseline. Nevertheless, researchers have proposed to include global visual features in reranking n-best lists generated by a PBSMT system or directly in a purely NMT framework with some success (Caglayan et al., 2016; Calixto et al., 2016; Libovick´y et al., 2016; Shah et al., 2016). The best results achie"
D17-1105,D15-1166,0,0.0523717,"the decoder (at each time step) prevents learning. The remainder of this paper is structured as follows. In §1.1 we briefly discuss relevant previous related work. We then revise the attention-based NMT framework and further expand it into different multi-modal NMT models (§2). In §3 we introduce the data sets we use in our experiments. In §4 we detail the hyperparameters, parameter initialisation and other relevant details of our models. Finally, in §6 we draw conclusions and provide some avenues for future work. 1.1 Architecture Their implementation is based on the attention-based model of Luong et al. (2015), which has some differences to that of Bahdanau et al. (2015), used in our work (§2.1). Their encoder is a single-layer unidirectional LSTM and they use the last hidden state of the encoder to initialise the decoder’s hidden state, therefore indirectly using the image features to do so. We use a bi-directional recurrent neural network (RNN) with GRU (Cho et al., 2014a) as our encoder, better encoding the semantics of the source sentence. Image features We include image features separately either as a word in the source sentence (§2.2.1) or directly for encoder (§2.2.2) or decoder initialisati"
D17-1105,W16-2360,0,0.331347,", 2013; Cho et al., 2014b; Sutskever et al., 2014). In this problem, each training example consists of one source and one target variable-length sequence, with no prior information regarding the alignments between the two. • We propose novel attention-based multimodal NMT models which incorporate visual features into the encoder and the decoder. • We discuss the impact that adding synthetic 992 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 992–1003 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics pose,1 Huang et al. (2016) extract global features for an image, project these features into the vector space of the source words and then add it as a word in the input sequence. Their best model improves over a strong NMT baseline and is comparable to results obtained with a PBSMT model trained on the same data, although not significantly better. For that reason, their models are used as baselines in our experiments. Next, we point out some key differences between the work of Huang et al. (2016) and ours. multi-modal and multilingual data brings to multi-modal NMT. • We show that images bring useful information to an"
D17-1105,D13-1176,0,0.0419834,"d find that the additional data have a positive impact on multi-modal models. We report new state-of-the-art results and our best models also significantly improve on a comparable Phrase-Based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all metrics evaluated on this data set. 1 Introduction Neural Machine Translation (NMT) has recently been proposed as an instantiation of the sequence to sequence (seq2seq) learning problem (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014). In this problem, each training example consists of one source and one target variable-length sequence, with no prior information regarding the alignments between the two. • We propose novel attention-based multimodal NMT models which incorporate visual features into the encoder and the decoder. • We discuss the impact that adding synthetic 992 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 992–1003 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics pose,1 Huang"
D17-1105,P07-2045,0,0.0265475,"kr30k, the M30kT has one of its English descriptions manually translated into German by a professional translator. Training, validation and test sets contain 29K, 1014 and 1K images, respectively, each accompanied by one sentence pair (the original English sentence and its German translation). For each of the 30K images in the Flickr30k, the M30kC has five descriptions in German collected independently of the English descriptions. Training, validation and test sets contain 29K, 1014 and 1K images, respectively, each accompanied by 5 English and 5 German sentences. We use the scripts in Moses (Koehn et al., 2007) to normalise, truecase and tokenize English and German descriptions and we also convert spaceseparated tokens into subwords (Sennrich et al., 2016b). All models use a common vocabulary of ∼83K English and ∼91K German subword tokens. If sentences in English or German are longer than 80 tokens, they are discarded. We use the entire M30kT training set for training, its validation set for model selection with BLEU, and its test set to evaluate our models. In order to study the impact that additional training data brings to the models, we use the baseline model described in §2 trained on the textu"
D17-1105,W16-2361,0,0.0970193,"Missing"
D17-1105,H92-1116,0,0.106219,"Missing"
D17-1105,P03-1021,0,0.0295234,"R (Snover et al., 2006), and chrF3 scores5 (Popovi´c, 2015) and we report statistical significance for the three first metrics using approximate randomisation computed with MultEval (Clark et al., 2011). As our main baseline we train an attentionbased NMT model (§2) in which only the textual part of M30kT is used for training. We also train a PBSMT model built with Moses on the same English→German (German→English) data, where the LM is a 5–gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995) trained on the German (English) of the M30kT dataset. We use minimum error rate training (Och, 2003) for tuning the model parameters for BLEU scores. Our third baseline (English→German), is the best comparable multi-modal model by Huang et al. (2016) and also their best model with additional object detections: respectively models m1 (image at head) and m3 in the authors’ paper. Finally, our fourth baseline (German→English) is 5 BLEU4↑ METEOR↑ TER↓ chrF3↑ English→German PBSMT NMT Huang + RCNN 32.9 33.7 35.1 36.5 54.1 52.3 52.2 54.1 45.1 46.7 — — 67.4 64.5 — — IMG1W IMG2W IMGE IMGD IMG2W+D IMGE+D 37.1†‡ (↑ 3.4) 36.9†‡ (↑ 3.2) 37.1†‡ (↑ 3.4) 37.3†‡ (↑ 3.6) 35.7†‡ (↑ 2.0) 37.0†‡ (↑ 3.3) 54.5†‡ ("
D17-1105,P02-1040,0,0.112983,"W ), and as the first and last words of 2.2.2 Images for encoder initialisation: IMGE In the original attention-based NMT model described in §2, the hidden state of the encoder is #» initialised with the zero vector 0 . Instead, we propose to use two new single-layer feed-forward neural networks to compute the initial states of the → − ← − forward RNN Φ enc and the backward RNN Φ enc , 4 Outputs would typically consist of sets of 2-5 words repeated many times, usually without any syntax. For comparison, translations for the translated Multi30k test set (described in §3) achieve just 3.8 BLEU (Papineni et al., 2002), 15.5 METEOR (Denkowski and Lavie, 2014) and 93.0 TER (Snover et al., 2006). 995 For each of the 30K images in the Flickr30k, the M30kT has one of its English descriptions manually translated into German by a professional translator. Training, validation and test sets contain 29K, 1014 and 1K images, respectively, each accompanied by one sentence pair (the original English sentence and its German translation). For each of the 30K images in the Flickr30k, the M30kC has five descriptions in German collected independently of the English descriptions. Training, validation and test sets contain 29"
D17-1105,W15-3049,0,0.0312543,"Missing"
D17-1105,P16-1009,0,0.0165304,"sets contain 29K, 1014 and 1K images, respectively, each accompanied by one sentence pair (the original English sentence and its German translation). For each of the 30K images in the Flickr30k, the M30kC has five descriptions in German collected independently of the English descriptions. Training, validation and test sets contain 29K, 1014 and 1K images, respectively, each accompanied by 5 English and 5 German sentences. We use the scripts in Moses (Koehn et al., 2007) to normalise, truecase and tokenize English and German descriptions and we also convert spaceseparated tokens into subwords (Sennrich et al., 2016b). All models use a common vocabulary of ∼83K English and ∼91K German subword tokens. If sentences in English or German are longer than 80 tokens, they are discarded. We use the entire M30kT training set for training, its validation set for model selection with BLEU, and its test set to evaluate our models. In order to study the impact that additional training data brings to the models, we use the baseline model described in §2 trained on the textual part of the M30kT data set (German→English and English→German) without the images to build back-translation models (Sennrich et al., 2016a). We"
D17-1105,W16-2363,0,0.196111,"es (Dong et al., 2015; Firat et al., 2016). However, multi-modal MT has only recently been addressed by the MT community in the form of a shared task (Specia et al., 2016). We note that in the official results of this first shared task no submissions based on a purely neural architecture could improve on the Phrase-Based SMT (PBSMT) baseline. Nevertheless, researchers have proposed to include global visual features in reranking n-best lists generated by a PBSMT system or directly in a purely NMT framework with some success (Caglayan et al., 2016; Calixto et al., 2016; Libovick´y et al., 2016; Shah et al., 2016). The best results achieved by a purely NMT model in this shared task are those of Huang et al. (2016), who proposed to use global and regional image features extracted with the VGG19 (Simonyan and Zisserman, 2014) and the RCNN (Girshick et al., 2014) convolutional neural networks (CNNs). Similarly to one of the three models we proData Huang et al. (2016) use object detections obtained with the RCNN of Girshick et al. (2014) as additional data, whereas we study the impact that additional back-translated data brings. Performance All our models outperform Huang et al. (2016)’s according to all m"
D17-1105,2006.amta-papers.25,0,0.359029,"n: IMGE In the original attention-based NMT model described in §2, the hidden state of the encoder is #» initialised with the zero vector 0 . Instead, we propose to use two new single-layer feed-forward neural networks to compute the initial states of the → − ← − forward RNN Φ enc and the backward RNN Φ enc , 4 Outputs would typically consist of sets of 2-5 words repeated many times, usually without any syntax. For comparison, translations for the translated Multi30k test set (described in §3) achieve just 3.8 BLEU (Papineni et al., 2002), 15.5 METEOR (Denkowski and Lavie, 2014) and 93.0 TER (Snover et al., 2006). 995 For each of the 30K images in the Flickr30k, the M30kT has one of its English descriptions manually translated into German by a professional translator. Training, validation and test sets contain 29K, 1014 and 1K images, respectively, each accompanied by one sentence pair (the original English sentence and its German translation). For each of the 30K images in the Flickr30k, the M30kC has five descriptions in German collected independently of the English descriptions. Training, validation and test sets contain 29K, 1014 and 1K images, respectively, each accompanied by 5 English and 5 Ger"
D17-1105,Q14-1006,0,0.107379,"s the image features d into the decoder hidden state dimensionality and Wdi and bdi are the same as in Equation (5). Once again we compute d by applying Equation (6) onto a global image feature vector q ∈ R4096 , only this time the parameters WI2 and b2I project the image features into the same dimensionality as the decoder hidden states. We illustrate this idea in Figure 1c. 3 Data set 4 Our multi-modal NMT models need bilingual sentences accompanied by one or more images as training data. The original Flickr30k data set contains 30K images and 5 English sentence descriptions for each image (Young et al., 2014). We use the translated and the comparable Multi30k datasets (Elliott et al., 2016), henceforth referred to as M30kT and M30kC , respectively, which are multilingual expansions of the original Flickr30k. Experimental setup Our encoder is a bidirectional RNN with GRU (one 1024D single-layer forward RNN and one 1024D single-layer backward RNN). Source and target word embeddings are 620D each and both are trained jointly with our model. All nonrecurrent matrices are initialised by sampling from a Gaussian (µ = 0, σ = 0.01), recurrent matrices are orthogonal and bias vectors are all initialised 99"
D17-1105,W16-2346,0,0.333609,"Missing"
D17-1105,P16-1008,0,0.0174746,"ncoder, better encoding the semantics of the source sentence. Image features We include image features separately either as a word in the source sentence (§2.2.1) or directly for encoder (§2.2.2) or decoder initialisation (§2.2.3), whereas Huang et al. (2016) only use it as a word. We also show it is better to include an image exclusively for the encoder or the decoder initialisation (Tables 1 and 2). Related work Attention-based encoder-decoder models for MT have been actively investigated in recent years. Some researchers have studied how to improve attention mechanisms (Luong et al., 2015; Tu et al., 2016) and how to train attention-based models to translate between many languages (Dong et al., 2015; Firat et al., 2016). However, multi-modal MT has only recently been addressed by the MT community in the form of a shared task (Specia et al., 2016). We note that in the official results of this first shared task no submissions based on a purely neural architecture could improve on the Phrase-Based SMT (PBSMT) baseline. Nevertheless, researchers have proposed to include global visual features in reranking n-best lists generated by a PBSMT system or directly in a purely NMT framework with some succe"
D17-1262,W07-0718,0,0.0556468,"ors of translation quality tend to be highly inconsistent. In recent Conference on Ma‡ Qun Liu† Computing and Info Systems University of Melbourne tb@ldwin.net chine Translation (WMT) shared tasks, for example, manual evaluators complete a relative ranking (RR) of the output of five alternate MT systems, where they must rank the quality of competing translations from best to worst. Within this set-up, when presented with the same pair of MT output translations, human assessors often disagree with one another’s preference, and even their own previous judgment about which translation is better (Callison-Burch et al., 2007; Bojar et al., 2016). Low levels of inter-annotator agreement in human evaluation of MT not only cause problems with respect to the reliability of MT system evaluations, but unfortunately have an additional knock-on effect with respect to the meta-evaluation of metrics, in providing an unstable gold standard. As such, provision of a fair and reliable human evaluation of MT remains a high priority for empirical evaluation. Direct assessment (DA) (Graham et al., 2013, 2014, 2016) is a relatively new human evaluation approach that overcomes previous challenges with respect to lack of reliability"
D17-1262,E06-1032,0,0.0800317,"s of potential analytical errors that force some important questions to be raised about the reliability of past conclusions, however. We subsequently carry out further investigation into reference bias via direct human assessment of MT adequacy via quality controlled crowd-sourcing. Contrary to both intuition and past conclusions, results show no significant evidence of reference bias in monolingual evaluation of MT. 1 Introduction Despite it being known for some time now that automatic metrics, such as BLEU (Papineni et al., 2002), provide a less than perfect substitute for human assessment (Callison-Burch et al., 2006), evaluation in MT more often than not still comprises BLEU scores. Besides increased time and resources required by the alternative, human evaluation of systems, human assessment of MT faces additional challenges, in particular the fact that human assessors of translation quality tend to be highly inconsistent. In recent Conference on Ma‡ Qun Liu† Computing and Info Systems University of Melbourne tb@ldwin.net chine Translation (WMT) shared tasks, for example, manual evaluators complete a relative ranking (RR) of the output of five alternate MT systems, where they must rank the quality of com"
D17-1262,1993.eamt-1.1,0,0.575525,"Missing"
D17-1262,P16-2013,0,0.447493,"y legitimate variations in scoring strategies. Despite efforts to avoid bias in Graham et al. (2013), since DA is a monolingual evaluation of MT that operates via comparison of MT output with a reference translation, it is therefore still possible, while avoiding other sources of bias, that DA incurs reference bias where the level of superficial similarity of translations with reference translations results in an unfair gain, or indeed an unfair disadvantage for systems that yield translations that legitimately deviate from the surface form of reference translations. Following this intuition, Fomicheva and Specia (2016) carry out an investigation into bias in monolingual evaluation of MT and conclude that in a monolingual setting, human assessors of MT are strongly biased by the reference translation. In this paper, we provide further analysis of experiments originally provided in Fomicheva and Specia (2016), in addition to further investigation into the degree to which the intuition about reference bias can be supported. 2 Background Fomicheva and Specia (2016) provide an investigation into reference bias in monolingual evaluation of MT. 100 Chinese to English MT output translations are assessed by 25 human"
D17-1262,W13-2305,1,0.950422,"ors often disagree with one another’s preference, and even their own previous judgment about which translation is better (Callison-Burch et al., 2007; Bojar et al., 2016). Low levels of inter-annotator agreement in human evaluation of MT not only cause problems with respect to the reliability of MT system evaluations, but unfortunately have an additional knock-on effect with respect to the meta-evaluation of metrics, in providing an unstable gold standard. As such, provision of a fair and reliable human evaluation of MT remains a high priority for empirical evaluation. Direct assessment (DA) (Graham et al., 2013, 2014, 2016) is a relatively new human evaluation approach that overcomes previous challenges with respect to lack of reliability of human judges. DA collects assessments of translations separately in the form of both fluency and adequacy on a 0–100 rating scale, and, by combination of repeat judgments for translations, produces scores that have been shown to be highly reliable in self-replication experiments (Graham et al., 2015). The main component of DA used to provide a primary ranking of systems is adequacy, where the MT output is assessed via a monolingual similarity of meaning assessme"
D17-1262,E14-1047,1,0.88998,"Missing"
D17-1262,N15-1124,1,0.852927,"an unstable gold standard. As such, provision of a fair and reliable human evaluation of MT remains a high priority for empirical evaluation. Direct assessment (DA) (Graham et al., 2013, 2014, 2016) is a relatively new human evaluation approach that overcomes previous challenges with respect to lack of reliability of human judges. DA collects assessments of translations separately in the form of both fluency and adequacy on a 0–100 rating scale, and, by combination of repeat judgments for translations, produces scores that have been shown to be highly reliable in self-replication experiments (Graham et al., 2015). The main component of DA used to provide a primary ranking of systems is adequacy, where the MT output is assessed via a monolingual similarity of meaning assessment. A reference translation is displayed to the human assessor (rendered in gray) and below it the MT output (in black), with the human judge asked to state the degree to which they agree that The black text adequately expresses the meaning of the gray text in English.1 The motivation behind 2476 1 Instructions are translated into a given target language. Proceedings of the 2017 Conference on Empirical Methods in Natural Language P"
D17-1262,P02-1040,0,0.111277,"ongly biased in this respect. On re-examination of past analyses, we identify a series of potential analytical errors that force some important questions to be raised about the reliability of past conclusions, however. We subsequently carry out further investigation into reference bias via direct human assessment of MT adequacy via quality controlled crowd-sourcing. Contrary to both intuition and past conclusions, results show no significant evidence of reference bias in monolingual evaluation of MT. 1 Introduction Despite it being known for some time now that automatic metrics, such as BLEU (Papineni et al., 2002), provide a less than perfect substitute for human assessment (Callison-Burch et al., 2006), evaluation in MT more often than not still comprises BLEU scores. Besides increased time and resources required by the alternative, human evaluation of systems, human assessment of MT faces additional challenges, in particular the fact that human assessors of translation quality tend to be highly inconsistent. In recent Conference on Ma‡ Qun Liu† Computing and Info Systems University of Melbourne tb@ldwin.net chine Translation (WMT) shared tasks, for example, manual evaluators complete a relative ranki"
D17-1301,D17-1105,1,0.644977,"Missing"
D17-1301,W12-3156,0,0.0426269,"ation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of ∗ Corresponding Author: Zhaopeng Tu a word. Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al., 2016). Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012). Nevertheless, current NMT models still process a documents by translating each sentence alone, suffering from inconsistency and ambiguity arising from a single source sentence. These problems are difficult to alleviate using only limited intra-sentence context. The cross-sentence context, or global context, has proven helpful to better capture the meaning or intention in sequential tasks such as query suggestion (Sordoni et al., 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al., 2016). The leverage of global context for NMT, however, has received relatively little attention fr"
D17-1301,P05-1066,0,0.303284,"Missing"
D17-1301,P15-1166,0,0.0160558,"de and select part of the previous source sentence for generating each target word. Calixto et al. (2017) utilize global image features extracted using a pre-trained convolutional neural network and incorporate them in NMT. As additional attention leads to more computational cost, they can only incorporate limited information such as single preceding sentence in Jean et al. (2017). However, our architecture is free to this limitation, thus we use multiple preceding sentences (e.g. K = 3) in our experiments. Our work is also related to multi-source (Zoph and Knight, 2016) and multi-target NMT (Dong et al., 2015), which incorporate additional source or target languages. They investigate one-tomany or many-to-one languages translation tasks by integrating additional encoders or decoders into encoder-decoder framework, and their experiments show promising results. 5 Conclusion and Future Work We proposed two complementary approaches to integrating cross-sentence context: 1) a warmstart of encoder and decoder with global context representation, and 2) cross-sentence context serves as an auxiliary information source for updating decoder states, in which an introduced context gate plays an important role."
D17-1301,D13-1176,0,0.00976873,"rical contextual information on the performance of neural machine translation (NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points. 1 Introduction Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016). The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation. Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol en"
D17-1301,P07-2045,0,0.00724727,"Missing"
D17-1301,D15-1166,0,0.0309999,"s been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016). The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation. Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of ∗ Corresponding Author: Zhaopeng Tu a word. Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al., 2016). Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012). Nevertheless, current NMT model"
D17-1301,P02-1040,0,0.101177,"Missing"
D17-1301,E17-3017,0,0.0145815,"Missing"
D17-1301,P16-1008,1,0.843029,"NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points. 1 Introduction Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016). The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation. Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding m"
D17-1301,2011.mtsummit-papers.13,0,0.0990302,"ical machine translation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of ∗ Corresponding Author: Zhaopeng Tu a word. Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al., 2016). Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012). Nevertheless, current NMT models still process a documents by translating each sentence alone, suffering from inconsistency and ambiguity arising from a single source sentence. These problems are difficult to alleviate using only limited intra-sentence context. The cross-sentence context, or global context, has proven helpful to better capture the meaning or intention in sequential tasks such as query suggestion (Sordoni et al., 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al., 2016). The leverage of global context for NMT, however, has received rel"
D17-1301,N16-1004,0,0.0544309,"or example, Jean et al. (2017) use it to encode and select part of the previous source sentence for generating each target word. Calixto et al. (2017) utilize global image features extracted using a pre-trained convolutional neural network and incorporate them in NMT. As additional attention leads to more computational cost, they can only incorporate limited information such as single preceding sentence in Jean et al. (2017). However, our architecture is free to this limitation, thus we use multiple preceding sentences (e.g. K = 3) in our experiments. Our work is also related to multi-source (Zoph and Knight, 2016) and multi-target NMT (Dong et al., 2015), which incorporate additional source or target languages. They investigate one-tomany or many-to-one languages translation tasks by integrating additional encoders or decoders into encoder-decoder framework, and their experiments show promising results. 5 Conclusion and Future Work We proposed two complementary approaches to integrating cross-sentence context: 1) a warmstart of encoder and decoder with global context representation, and 2) cross-sentence context serves as an auxiliary information source for updating decoder states, in which an introduc"
D18-1333,N18-1008,0,0.0193306,"rn to generate DPs at the predicted positions using a jointly trained DP predictor, which is fed with informative representations in the reconstructor. 1 Unless otherwise indicated, in the paper, the terms “DP” and “DP word” are identical. Approach Shared Reconstructor Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, Firat et al. (2016) share an attention model across languages while Dong et al. (2015) share an encoder. Our work is most similar to the work of Zoph and Knight (2016) and Anastasopoulos and Chiang (2018), which share a decoder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames. The architectures of our proposed shared reconstruction model are shown in Figure 2(a). Formally, the reconstructor reads from both the encoder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a reconstruction score. It uses two separate attention models to reconstruct the annotated source senˆ = {ˆ tence x x1 , x ˆ2 , . . . , x ˆT } word by word, and the reconstruction score is computed by R(ˆ x"
D18-1333,P05-1066,0,0.120726,"tperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating DPs. It suggests the necessity of jointly learning to translate and predict DPs. Experiment 4.1 Setup To compare our work with the results reported by previous work (Wang et al., 2018), we conducted experiments on their released Chinese⇒English TV Subtitle corpus.2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively. We used case-insensitive 4-gram NIST BLEU metrics (Papineni et al., 2002) for evaluation, and sign-test (Collins et al., 2005) to test for statistical significance. We implemented our models on the code repository released by Wang et al. (2018).3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results. It should be emphasized that we did not use the pre-train strategy as done in Wang et al. (2018), since we found training from scratch achieved a better performance in the shared reconstructor setting. 2 https://github.com/longyuewangdcu/ tvsub 3 https://github.com/tuzhaopeng/nmt Results Table 2 shows the translation results. It is clear that the proposed m"
D18-1333,P15-1166,0,0.0298915,"ch higher, which provides the chance to alleviate the error propagation problem. Intuitively, we can learn to generate DPs at the predicted positions using a jointly trained DP predictor, which is fed with informative representations in the reconstructor. 1 Unless otherwise indicated, in the paper, the terms “DP” and “DP word” are identical. Approach Shared Reconstructor Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, Firat et al. (2016) share an attention model across languages while Dong et al. (2015) share an encoder. Our work is most similar to the work of Zoph and Knight (2016) and Anastasopoulos and Chiang (2018), which share a decoder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames. The architectures of our proposed shared reconstruction model are shown in Figure 2(a). Formally, the reconstructor reads from both the encoder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a reconstruction score. It uses two separate attention models to reconstruct the annotat"
D18-1333,N16-1101,0,0.0216158,"r. Fortunately, the accuracy of predicting DP positions (DPPs) is much higher, which provides the chance to alleviate the error propagation problem. Intuitively, we can learn to generate DPs at the predicted positions using a jointly trained DP predictor, which is fed with informative representations in the reconstructor. 1 Unless otherwise indicated, in the paper, the terms “DP” and “DP word” are identical. Approach Shared Reconstructor Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, Firat et al. (2016) share an attention model across languages while Dong et al. (2015) share an encoder. Our work is most similar to the work of Zoph and Knight (2016) and Anastasopoulos and Chiang (2018), which share a decoder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames. The architectures of our proposed shared reconstruction model are shown in Figure 2(a). Formally, the reconstructor reads from both the encoder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a reconstruction scor"
D18-1333,W10-1737,0,0.232395,"Missing"
D18-1333,P02-1040,0,0.100689,"the DPP-annotated data (“Baseline (+DPPs)”, Row 4) outperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating DPs. It suggests the necessity of jointly learning to translate and predict DPs. Experiment 4.1 Setup To compare our work with the results reported by previous work (Wang et al., 2018), we conducted experiments on their released Chinese⇒English TV Subtitle corpus.2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively. We used case-insensitive 4-gram NIST BLEU metrics (Papineni et al., 2002) for evaluation, and sign-test (Collins et al., 2005) to test for statistical significance. We implemented our models on the code repository released by Wang et al. (2018).3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results. It should be emphasized that we did not use the pre-train strategy as done in Wang et al. (2018), since we found training from scratch achieved a better performance in the shared reconstructor setting. 2 https://github.com/longyuewangdcu/ tvsub 3 https://github.com/tuzhaopeng/nmt Results Table 2 shows the"
D18-1333,D17-1301,1,0.910434,"Missing"
D18-1333,N16-1113,1,0.686079,"Missing"
D18-1333,P13-1081,0,0.46132,"Missing"
D18-1333,N16-1004,0,0.0282224,"em. Intuitively, we can learn to generate DPs at the predicted positions using a jointly trained DP predictor, which is fed with informative representations in the reconstructor. 1 Unless otherwise indicated, in the paper, the terms “DP” and “DP word” are identical. Approach Shared Reconstructor Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, Firat et al. (2016) share an attention model across languages while Dong et al. (2015) share an encoder. Our work is most similar to the work of Zoph and Knight (2016) and Anastasopoulos and Chiang (2018), which share a decoder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames. The architectures of our proposed shared reconstruction model are shown in Figure 2(a). Formally, the reconstructor reads from both the encoder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a reconstruction score. It uses two separate attention models to reconstruct the annotated source senˆ = {ˆ tence x x1 , x ˆ2 , . . . , x ˆT } word by word, and the reco"
D18-1460,P05-1033,0,0.126753,"target hidden state sj is given by   ∗ sj = f eyj−1 , sj−1 , cj (6) The probability distribution Dj over all the words in the target vocabulary is predicted conditioned on the previous ground truth words, the context vector cj and the unrolled target information sj .   ∗ tj = g eyj−1 , cj , sj (7) oj = Wo tj (8) Dj = softmax (oj ) (9) where g stands for a linear transformation, Wo is used to map tj to oj so that each target word has one corresponding dimension in oj . 2.2 Cube Pruning The cube pruning algorithm, proposed by Chiang (2007) based on the k-best parsing algorithm of Huang and Chiang (2005), is actually an accelerated extension based on the naive beam search algorithm. Beam search, a heuristic dynamic programming searching algorithm, explores a graph by expanding the most promising nodes in a limited set and searches approximate optimal results from candidates. For the sequence-to-sequence learning task, given a pre-trained model, the beam search algorithm finds a sequence that approximately maximizes the conditional probability (Graves, 2012; Boulanger-Lewandowski et al., 2013). Both Sutskever et al. (2014) and Bahdanau et al. (2015) employed the beam search algorithm into the"
D18-1460,W14-4012,0,0.15052,"Missing"
D18-1460,D14-1179,0,0.0310691,"Missing"
D18-1460,P14-1129,0,0.0999653,"Missing"
D18-1460,P06-1121,0,0.0451297,"owski et al., 2013). Both Sutskever et al. (2014) and Bahdanau et al. (2015) employed the beam search algorithm into the NMT decoding to produce translations with relatively larger conditional probability with respect to the optimized model parameters. Remarkably, Huang and Chiang (2007) successfully applied the cube pruning algorithm to the decoding of SMT. They found that the beam search algorithm in SMT can be extended, and they utilized the cube pruning and some variants to optimize the search process in the decoding phase of phrase-based (Och and Ney, 2004) and syntaxbased (Chiang, 2005; Galley et al., 2006) systems, 4285 n) 0.1 0.2 1.1 0.1 0.2 1.1 0.1 0.2 1.1 3.2 2.1 2.2 2.3 3.2 2.1 2.2 2.3 3.2 2.1 2.2 2.3 3.2 (NP1,2 : The airplane) 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 3.5 3.6 4.5 3.4 3.5 3.6 4.5 3.4 3.5 3.6 4.5 3.4 3.5 3.6 4.5 (a) (b) (c) do w 4 3, P (V 1.1 2.3 3.4 ro ps off ) :t 3, 0.2 2.2 (NP1,2 : The apple) :d off ) ak es oo k :t 4 P 3, P (V (V 4 3, P (V 4 ) ps ro :d :t 4 3, P P (V (V off off ) ak es oo k :t 4 3, 4 3, P (V do w n) n) ) :d :t 4 3, ro ps off ) off ak es k oo :t 4 P 3, P (V (V 4 3, P (V do w n) do w ) ro :d :t 4 3, ps off off ) ak es oo k :t 4 3, P (V"
D18-1460,P17-1012,0,0.0219113,"translation to speed up the translation. To construct the equivalence class, similar target hidden states are combined, leading to less RNN expansion operations on the target side and less softmax operations over the large target vocabulary. The experiments show that, at the same or even better translation quality, our method can translate faster compared with naive beam search by 3.3× on GPUs and 3.5× on CPUs. 1 Introduction Neural machine translation (NMT) has shown promising results and drawn more attention recently (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Bahdanau et al., 2015; Gehring et al., 2017a,b; Vaswani et al., 2017). A widely used architecture is the attention-based encoder-decoder framework (Cho et al., 2014b; Bahdanau et al., 2015) which assumes there is a common semantic space between the source and target language pairs. The encoder encodes the source sentence to a representation in the common space with the recurrent neural network (RNN) (Hochreiter and Schmidhuber, 1997) and the decoder decodes this representation to generate the target sentence word by word. To generate a target word, a probability distribution over the target vocabulary is drawn based on the attention ov"
D18-1460,P82-1020,0,0.808202,"Missing"
D18-1460,2015.mtsummit-papers.23,0,0.737459,"Missing"
D18-1460,W05-1506,1,0.519405,"step, the target hidden state sj is given by   ∗ sj = f eyj−1 , sj−1 , cj (6) The probability distribution Dj over all the words in the target vocabulary is predicted conditioned on the previous ground truth words, the context vector cj and the unrolled target information sj .   ∗ tj = g eyj−1 , cj , sj (7) oj = Wo tj (8) Dj = softmax (oj ) (9) where g stands for a linear transformation, Wo is used to map tj to oj so that each target word has one corresponding dimension in oj . 2.2 Cube Pruning The cube pruning algorithm, proposed by Chiang (2007) based on the k-best parsing algorithm of Huang and Chiang (2005), is actually an accelerated extension based on the naive beam search algorithm. Beam search, a heuristic dynamic programming searching algorithm, explores a graph by expanding the most promising nodes in a limited set and searches approximate optimal results from candidates. For the sequence-to-sequence learning task, given a pre-trained model, the beam search algorithm finds a sequence that approximately maximizes the conditional probability (Graves, 2012; Boulanger-Lewandowski et al., 2013). Both Sutskever et al. (2014) and Bahdanau et al. (2015) employed the beam search algorithm into the"
D18-1460,P07-1019,1,0.863396,"employed to avoid unknown words (UNKs), which leads to a large number of normalization factors for the softmax operation when drawing the probability distribution. To accelerate the translation, the widely used method is to trade off between the translation quality and the decoding speed by reducing the size of vocabulary (Mi et al., 2016a) or/and the number of parameters, which can not realize the full potential of NMT. In this paper, we borrow ideas from phrasebased and syntax-based machine translation where cube pruning has been successfully applied to speed up the decoding (Chiang, 2007; Huang and Chiang, 2007). Informally, cube pruning “coarsens” the search space by clustering similar states according to some equivalence relations. To apply this idea to NMT, however, is much more involved. Specifically, in the process of beam search, we cluster similar target hidden states to construct equivalence classes, the three dimensions of which are target words in the target vocabulary, part translations retained in the beam search and different combinations of similar target hidden states, respectively. The clustering operation 4284 Proceedings of the 2018 Conference on Empirical Methods in Natural Languag"
D18-1460,P15-1001,0,0.0577112,"Missing"
D18-1460,W13-3214,0,0.0226871,"a popular technique to speed up dynamic programming, into neural machine translation to speed up the translation. To construct the equivalence class, similar target hidden states are combined, leading to less RNN expansion operations on the target side and less softmax operations over the large target vocabulary. The experiments show that, at the same or even better translation quality, our method can translate faster compared with naive beam search by 3.3× on GPUs and 3.5× on CPUs. 1 Introduction Neural machine translation (NMT) has shown promising results and drawn more attention recently (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Bahdanau et al., 2015; Gehring et al., 2017a,b; Vaswani et al., 2017). A widely used architecture is the attention-based encoder-decoder framework (Cho et al., 2014b; Bahdanau et al., 2015) which assumes there is a common semantic space between the source and target language pairs. The encoder encodes the source sentence to a representation in the common space with the recurrent neural network (RNN) (Hochreiter and Schmidhuber, 1997) and the decoder decodes this representation to generate the target sentence word by word. To generate a target word, a probability distributi"
D18-1460,D16-1096,0,0.0152537,"rd requires extensive computation to go through all the source words to calculate the attention. Worse still, due to the recurrence of RNNs, target words can only be generated sequentially rather than in parallel. The second reason is that large vocabulary on target side is employed to avoid unknown words (UNKs), which leads to a large number of normalization factors for the softmax operation when drawing the probability distribution. To accelerate the translation, the widely used method is to trade off between the translation quality and the decoding speed by reducing the size of vocabulary (Mi et al., 2016a) or/and the number of parameters, which can not realize the full potential of NMT. In this paper, we borrow ideas from phrasebased and syntax-based machine translation where cube pruning has been successfully applied to speed up the decoding (Chiang, 2007; Huang and Chiang, 2007). Informally, cube pruning “coarsens” the search space by clustering similar states according to some equivalence relations. To apply this idea to NMT, however, is much more involved. Specifically, in the process of beam search, we cluster similar target hidden states to construct equivalence classes, the three dimen"
D18-1460,P16-2021,0,0.0190433,"rd requires extensive computation to go through all the source words to calculate the attention. Worse still, due to the recurrence of RNNs, target words can only be generated sequentially rather than in parallel. The second reason is that large vocabulary on target side is employed to avoid unknown words (UNKs), which leads to a large number of normalization factors for the softmax operation when drawing the probability distribution. To accelerate the translation, the widely used method is to trade off between the translation quality and the decoding speed by reducing the size of vocabulary (Mi et al., 2016a) or/and the number of parameters, which can not realize the full potential of NMT. In this paper, we borrow ideas from phrasebased and syntax-based machine translation where cube pruning has been successfully applied to speed up the decoding (Chiang, 2007; Huang and Chiang, 2007). Informally, cube pruning “coarsens” the search space by clustering similar states according to some equivalence relations. To apply this idea to NMT, however, is much more involved. Specifically, in the process of beam search, we cluster similar target hidden states to construct equivalence classes, the three dimen"
D18-1460,J04-4002,0,0.0733736,"tional probability (Graves, 2012; Boulanger-Lewandowski et al., 2013). Both Sutskever et al. (2014) and Bahdanau et al. (2015) employed the beam search algorithm into the NMT decoding to produce translations with relatively larger conditional probability with respect to the optimized model parameters. Remarkably, Huang and Chiang (2007) successfully applied the cube pruning algorithm to the decoding of SMT. They found that the beam search algorithm in SMT can be extended, and they utilized the cube pruning and some variants to optimize the search process in the decoding phase of phrase-based (Och and Ney, 2004) and syntaxbased (Chiang, 2005; Galley et al., 2006) systems, 4285 n) 0.1 0.2 1.1 0.1 0.2 1.1 0.1 0.2 1.1 3.2 2.1 2.2 2.3 3.2 2.1 2.2 2.3 3.2 2.1 2.2 2.3 3.2 (NP1,2 : The airplane) 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 3.5 3.6 4.5 3.4 3.5 3.6 4.5 3.4 3.5 3.6 4.5 3.4 3.5 3.6 4.5 (a) (b) (c) do w 4 3, P (V 1.1 2.3 3.4 ro ps off ) :t 3, 0.2 2.2 (NP1,2 : The apple) :d off ) ak es oo k :t 4 P 3, P (V (V 4 3, P (V 4 ) ps ro :d :t 4 3, P P (V (V off off ) ak es oo k :t 4 3, 4 3, P (V do w n) n) ) :d :t 4 3, ro ps off ) off ak es k oo :t 4 P 3, P (V (V 4 3, P (V do w n) do w"
D18-1460,P02-1040,0,0.104328,"lish (ZhEn) translation task. 4.1 Data Preparation The Chinese-English training dataset consists of 1.25M sentence pairs3 . We used the NIST 2002 (MT02) dataset as the validation set with 878 sentences, and the NIST 2003 (MT03) dataset as the test dataset, which contains 919 sentences. The lengths of the sentences on both sides were limited up to 50 tokens, then actually 1.11M sentence pairs were left with 25.0M Chinese words and 27.0M English words. We extracted 30k most frequent words as the source and target vocabularies for both sides. In all the experiments, case-insensitive 4-gram BLEU (Papineni et al., 2002) was employed for the automatic evaluation, we used the script mteval-v11b.pl4 to calculate the BLEU score. 4.2 System The system is an improved version of attentionbased NMT system named RNNsearch (Bahdanau et al., 2015) where the decoder employs a conditional GRU layer with attention, consisting of two GRUs and an attention module for each step5 . Specifically, Equation (6) is replaced with the following two equations: ∗ s˜j = GRU1 (eyj−1 , sj−1 ) (13) sj = GRU2 (cj , s˜j ) (14) Besides, for the calculation of relevance in Equation (4), sj−1 is replaced with s˜j−1 . The other components of t"
D18-1460,1983.tc-1.13,0,0.635263,"Missing"
E14-4036,N10-1062,0,0.159017,"Missing"
E14-4036,W13-2235,0,0.0830041,"entation, post-editing (PE) of MT output by human translators (compared to human translation from scratch) results in notable productivity gains, as a number of industry studies have shown convincingly, e.g. (Plitt and Masselot, 2010). Furthermore, incremental retraining and update techniques (Bertoldi et al., 2013; Levenberg et al., • We propose novel selection criteria for ALbased PE: we adapt cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011), originally used for domain adaptation, and propose an extension to cross entropy difference with a vocabulary saturation filter (Lewis and Eetemadi, 2013). • While much of previous work concentrates on research datasets (e.g. Europarl, News Commentary), we use industry data (techni185 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185–189, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics cal manuals). (Bertoldi et al., 2013) shows that the repetition rate of news is considerably lower than that of technical documentation, which impacts on the results obtained with incremental retraining. method, where s and t stand for source and target, re"
E14-4036,D11-1033,0,0.261389,"al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation industry, as for many relevant domains such as technical documentation, post-editing (PE) of MT output by human translators (compared to human translation from scratch) results in notable productivity gains, as a number of industry studies have shown convincingly, e.g. (Plitt and Masselot, 2010). Furthermore, incremental retraining and update techniques (Bertoldi et al., 2013; Levenberg et al., • We propose novel selection criteria for ALbased PE: we adapt cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011), originally used for domain adaptation, and propose an extension to cross entropy difference with a vocabulary saturation filter (Lewis and Eetemadi, 2013). • While much of previous work concentrates on research datasets (e.g. Europarl, News Commentary), we use industry data (techni185 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185–189, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics cal manuals). (Bertoldi et al., 2013) shows that the repetition rate of news is considerably lower th"
E14-4036,W13-2237,0,0.194557,"Missing"
E14-4036,2013.mtsummit-papers.5,0,0.0193829,"Research Machine Translation (MT) has evolved dramatically over the last two decades, especially since the appearance of statistical approaches (Brown et al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation industry, as for many relevant domains such as technical documentation, post-editing (PE) of MT output by human translators (compared to human translation from scratch) results in notable productivity gains, as a number of industry studies have shown convincingly, e.g. (Plitt and Masselot, 2010). Furthermore, incremental retraining and update techniques (Bertoldi et al., 2013; Levenberg et al., • We propose novel selection criteria for ALbased PE: we adapt cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011), originally used for domain adaptation, and propose an extension to cross entropy difference with a vocabulary saturation filter (Lewis and Eetemadi, 2013). • While much of previous work concentrates on research datasets (e.g. Europarl, News Commentary), we use industry data (techni185 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185–189, c Gothenburg, Sweden, April 26-30 2"
E14-4036,N04-1043,0,0.053098,"AL-based selection works for PE-based incrementally retrained MT with overall performance gains around 0.67 to 2.65 TER absolute on average. We use two baselines, i.e. random and sequential. In the random baseline, the batch of sentences at each iteration are selected randomly. In the sequential baseline, the batches of sentences follow the same order as the data. Aside from the Random and Sequential baselines we use the following selection criteria: AL has been successfully applied to many tasks in natural language processing, including parsing (Tang et al., 2002), named entity recognition (Miller et al., 2004), to mention just a few. See (Olsson, 2009) for a comprehensie overview of the application of AL to natural language processing. (Haffari et al., 2009; Bloodgood and CallisonBurch, 2010) apply AL to MT where the aim is to build an optimal MT model from a given, static dataset. To the best of our knowledge, the most relevant previous research is (Gonz´alez-Rubio et al., 2012), which applies AL to interactive MT. In addition to differences in the AL selection criteria and data sets, our goals are fundamentally different: while the previous work aimed at reducing human effort in interactive MT, w"
E14-4036,P10-1088,0,0.0275825,"esented. In this paper, we add to the existing literature addressing the question whether and if so, to what extent, this process can be improved upon by Active Learning, where input is not presented chronologically but dynamically selected according to criteria that maximise performance with respect to (whatever is) the remaining data. We explore novel (source side-only) selection criteria and show performance increases of 0.67-2.65 points TER absolute on average on typical industry data sets compared to sequential PEbased incrementally retrained SMT. 1 • Previous work (Haffari et al., 2009; Bloodgood and Callison-Burch, 2010) shows that, given a (static) training set, AL can improve the quality of MT. By contrast, here we show that AL-based data selection for human PE improves incrementally and dynamically retrained MT, reducing overall PE time of translation jobs in the localisation industry application scenarios. Introduction and Related Research Machine Translation (MT) has evolved dramatically over the last two decades, especially since the appearance of statistical approaches (Brown et al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation industry, as for many relevant domai"
E14-4036,P10-2041,0,0.24501,"al approaches (Brown et al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation industry, as for many relevant domains such as technical documentation, post-editing (PE) of MT output by human translators (compared to human translation from scratch) results in notable productivity gains, as a number of industry studies have shown convincingly, e.g. (Plitt and Masselot, 2010). Furthermore, incremental retraining and update techniques (Bertoldi et al., 2013; Levenberg et al., • We propose novel selection criteria for ALbased PE: we adapt cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011), originally used for domain adaptation, and propose an extension to cross entropy difference with a vocabulary saturation filter (Lewis and Eetemadi, 2013). • While much of previous work concentrates on research datasets (e.g. Europarl, News Commentary), we use industry data (techni185 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185–189, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics cal manuals). (Bertoldi et al., 2013) shows that the repetition rate of news i"
E14-4036,J93-2003,0,0.0267032,"mpared to sequential PEbased incrementally retrained SMT. 1 • Previous work (Haffari et al., 2009; Bloodgood and Callison-Burch, 2010) shows that, given a (static) training set, AL can improve the quality of MT. By contrast, here we show that AL-based data selection for human PE improves incrementally and dynamically retrained MT, reducing overall PE time of translation jobs in the localisation industry application scenarios. Introduction and Related Research Machine Translation (MT) has evolved dramatically over the last two decades, especially since the appearance of statistical approaches (Brown et al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation industry, as for many relevant domains such as technical documentation, post-editing (PE) of MT output by human translators (compared to human translation from scratch) results in notable productivity gains, as a number of industry studies have shown convincingly, e.g. (Plitt and Masselot, 2010). Furthermore, incremental retraining and update techniques (Bertoldi et al., 2013; Levenberg et al., • We propose novel selection criteria for ALbased PE: we adapt cross-entropy difference (Moore and Lewis, 2010; Axelrod et"
E14-4036,E12-1025,0,0.178427,"Missing"
E14-4036,2013.mtsummit-papers.24,0,0.240964,"Missing"
E14-4036,N09-1047,0,0.217755,"translation job) is presented. In this paper, we add to the existing literature addressing the question whether and if so, to what extent, this process can be improved upon by Active Learning, where input is not presented chronologically but dynamically selected according to criteria that maximise performance with respect to (whatever is) the remaining data. We explore novel (source side-only) selection criteria and show performance increases of 0.67-2.65 points TER absolute on average on typical industry data sets compared to sequential PEbased incrementally retrained SMT. 1 • Previous work (Haffari et al., 2009; Bloodgood and Callison-Burch, 2010) shows that, given a (static) training set, AL can improve the quality of MT. By contrast, here we show that AL-based data selection for human PE improves incrementally and dynamically retrained MT, reducing overall PE time of translation jobs in the localisation industry application scenarios. Introduction and Related Research Machine Translation (MT) has evolved dramatically over the last two decades, especially since the appearance of statistical approaches (Brown et al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation"
E14-4036,2006.amta-papers.25,0,0.175582,"erences in the AL selection criteria and data sets, our goals are fundamentally different: while the previous work aimed at reducing human effort in interactive MT, we aim at reducing the overall PE time in PE-based incremental MT update applications in the localisation industry. In our experiments reported in Section 3 below we want to explore a space consisting of a considerable number of selection strategies and incremental retraining batch sizes. In order to be able to do this, we use the target side of our industry translation memory data to approximate human PE output and automatic TER (Snover et al., 2006) scores as a proxy for human PE times (O’Brien, 2011). 2 • N-gram Overlap. An SMT system will encounter problems translating sentences containing n-grams not seen in the training data. Thus, PEs of sentences with high number of unseen n-grams are considered to be more informative for updating the current MT system. However, for the MT system to translate unseen n-grams accurately, they need to be seen a minimum number V times.2 We use an n-gram overlap function similar to the one described in (Gonz´alez-Rubio et al., 2012) given in Equation 1 where N (T (i) ) and N (S (i) ) return i-grams in t"
E14-4036,P07-2045,0,0.00362596,"t results. 3 Dir EN→FR FR→EN EN→DE DE→EN Random 29.64 27.08 24.00 19.36 Seq. 29.81 27.04 24.08 19.34 Ngram 28.97 26.15 22.34 17.70 CED 29.25 26.63 22.60 17.97 CEDN 29.05 26.39 22.32 17.48 Table 2: TER average scores for Setting 1 Experiments and Results Dir EN→FR FR→EN EN→DE DE→EN We use technical documentation data taken from Symantec translation memories for the English– French (EN–FR) and English–German (EN–DE) language pairs (both directions) for our experiments. The statistics of the data (training and incremental splits) are shown in Table 1. All the systems are trained using the Moses (Koehn et al., 2007) phrase-based statistical MT system, with IRSTLM (Federico et al., 2008) for language modelling (n-grams up to order five) and with the alignment heuristic grow-diag-final-and. Random 36.23 33.26 32.23 27.24 Seq. 36.26 33.34 32.19 27.29 Ngram 35.20 32.26 30.58 26.10 CED 35.48 32.69 31.96 26.73 CEDN 35.17 32.17 29.98 24.94 Table 3: TER average scores for Setting 2 For Setting 1 (Table 2), the best result is obtained by the CEDN criterion for two out of the four directions. For EN→FR, n-gram overlap 4 As this study simulates the post-editing, we use the references of the translated segments inst"
E14-4036,P02-1016,0,0.0611916,"E MT applications. • Our experiments show that AL-based selection works for PE-based incrementally retrained MT with overall performance gains around 0.67 to 2.65 TER absolute on average. We use two baselines, i.e. random and sequential. In the random baseline, the batch of sentences at each iteration are selected randomly. In the sequential baseline, the batches of sentences follow the same order as the data. Aside from the Random and Sequential baselines we use the following selection criteria: AL has been successfully applied to many tasks in natural language processing, including parsing (Tang et al., 2002), named entity recognition (Miller et al., 2004), to mention just a few. See (Olsson, 2009) for a comprehensie overview of the application of AL to natural language processing. (Haffari et al., 2009; Bloodgood and CallisonBurch, 2010) apply AL to MT where the aim is to build an optimal MT model from a given, static dataset. To the best of our knowledge, the most relevant previous research is (Gonz´alez-Rubio et al., 2012), which applies AL to interactive MT. In addition to differences in the AL selection criteria and data sets, our goals are fundamentally different: while the previous work aim"
E17-1012,P02-1054,0,0.120133,"nswer based on the length-normalized BM25 formula (Robertson et al., 1994); (2) translation features: probability of the question being a translation of the answer computed using IBM’s Model 1 (Brown et al., 1993); (3) features measuring frequency and density of the question terms in the answer, such as the number of non-stop question words in the answer, the number of non-stop nouns, verbs and adjectives in the answer that do not appear in the question and tree kernel values for question and answer syntactic structures; (4) web correlation features based on Corrected Conditional Probability (Magnini et al., 2002) between the question and the answer. They explore these features both separately and in combination and find that the combination of all four feature types is most beneficial for answer reranking models. Jansen et al. (2014) describe answer reranking experiments on YA using a diverse range of lexical, syntactic and discourse features. In particular, they show how discourse information can complement distributed lexical semantic information obtained with a skip-gram model (Mikolov et al., 2013). In this paper we use their features (discussed in detail in Section 4) in combination with http://a"
E17-1012,N16-1154,1,0.690349,"et al., 2011), and translation probabilities (Fried et al., 2015; Surdeanu et al., 2011). The resulting feature vectors are then passed to a supervised ranking algorithm, such as SVMrank (Joachims, 2006), which ranks the candidates. There has been a recent shift in Natural Language Processing towards neural approaches involving minimal feature engineering. Several recent studies present purely neural approaches to answer reranking, with most of them focusing on the task of passage-level answer selection (dos Santos et al., 2016; Tan et al., 2015), rather than answer reranking in CQA websites (Bogdanova and Foster, 2016). These neural approaches aim to obviate the need for any feature engineering and instead focus on developing a neural architecture We show that a neural approach to the task of non-factoid answer reranking can benefit from the inclusion of tried-and-tested handcrafted features. We present a novel neural network architecture based on a combination of recurrent neural networks that are used to encode questions and answers, and a multilayer perceptron. We show how this approach can be combined with additional features, in particular, the discourse features presented by Jansen et al. (2014). Our"
E17-1012,J93-2003,0,0.041083,"are provided in Section 6. 2 Related Work Previous work on supervised non-factoid answer reranking on CQA datasets focused mainly on feature-rich approaches. Surdeanu et al. (2011) show that CQAs such as Yahoo! Answers are a good source of knowledge for non-factoid QA. They employ four types of features in their answer reranking model: (1) similarity features: the similarity between a question and an answer based on the length-normalized BM25 formula (Robertson et al., 1994); (2) translation features: probability of the question being a translation of the answer computed using IBM’s Model 1 (Brown et al., 1993); (3) features measuring frequency and density of the question terms in the answer, such as the number of non-stop question words in the answer, the number of non-stop nouns, verbs and adjectives in the answer that do not appear in the question and tree kernel values for question and answer syntactic structures; (4) web correlation features based on Corrected Conditional Probability (Magnini et al., 2002) between the question and the answer. They explore these features both separately and in combination and find that the combination of all four feature types is most beneficial for answer reran"
E17-1012,N15-1025,0,0.0565412,"data for the task. This is changing, however, with the growing popularity of Community Question Answering (CQA) websites, such as Quora,1 Yahoo! Answers2 and the Stack Exchange3 family of forums. One of the main components of a non-factoid question answering system is the answer reranking module. Given a question, it aims to rearrange the answers in order to boost the community-selected best answer to the top position. Most previous attempts to perform non-factoid answer reranking on CQA data are supervised, feature-based, learning-to-rank approaches (Jansen et al., 2014; Fried et al., 2015; Sharp et al., 2015). These methods represent the candidate answers as meaningful handcrafted features based on syntactic, semantic and discourse parses (Surdeanu et al., 2011; Jansen et al., 2014), web correlation (Surdeanu et al., 2011), and translation probabilities (Fried et al., 2015; Surdeanu et al., 2011). The resulting feature vectors are then passed to a supervised ranking algorithm, such as SVMrank (Joachims, 2006), which ranks the candidates. There has been a recent shift in Natural Language Processing towards neural approaches involving minimal feature engineering. Several recent studies present purel"
E17-1012,H05-1116,0,0.0994332,"Missing"
E17-1012,Q15-1015,0,0.137889,"of available labeled data for the task. This is changing, however, with the growing popularity of Community Question Answering (CQA) websites, such as Quora,1 Yahoo! Answers2 and the Stack Exchange3 family of forums. One of the main components of a non-factoid question answering system is the answer reranking module. Given a question, it aims to rearrange the answers in order to boost the community-selected best answer to the top position. Most previous attempts to perform non-factoid answer reranking on CQA data are supervised, feature-based, learning-to-rank approaches (Jansen et al., 2014; Fried et al., 2015; Sharp et al., 2015). These methods represent the candidate answers as meaningful handcrafted features based on syntactic, semantic and discourse parses (Surdeanu et al., 2011; Jansen et al., 2014), web correlation (Surdeanu et al., 2011), and translation probabilities (Fried et al., 2015; Surdeanu et al., 2011). The resulting feature vectors are then passed to a supervised ranking algorithm, such as SVMrank (Joachims, 2006), which ranks the candidates. There has been a recent shift in Natural Language Processing towards neural approaches involving minimal feature engineering. Several recent"
E17-1012,P14-1092,0,0.147716,"y due to the absence of available labeled data for the task. This is changing, however, with the growing popularity of Community Question Answering (CQA) websites, such as Quora,1 Yahoo! Answers2 and the Stack Exchange3 family of forums. One of the main components of a non-factoid question answering system is the answer reranking module. Given a question, it aims to rearrange the answers in order to boost the community-selected best answer to the top position. Most previous attempts to perform non-factoid answer reranking on CQA data are supervised, feature-based, learning-to-rank approaches (Jansen et al., 2014; Fried et al., 2015; Sharp et al., 2015). These methods represent the candidate answers as meaningful handcrafted features based on syntactic, semantic and discourse parses (Surdeanu et al., 2011; Jansen et al., 2014), web correlation (Surdeanu et al., 2011), and translation probabilities (Fried et al., 2015; Surdeanu et al., 2011). The resulting feature vectors are then passed to a supervised ranking algorithm, such as SVMrank (Joachims, 2006), which ranks the candidates. There has been a recent shift in Natural Language Processing towards neural approaches involving minimal feature engineer"
E17-1012,J11-2003,0,0.198638,"2 and the Stack Exchange3 family of forums. One of the main components of a non-factoid question answering system is the answer reranking module. Given a question, it aims to rearrange the answers in order to boost the community-selected best answer to the top position. Most previous attempts to perform non-factoid answer reranking on CQA data are supervised, feature-based, learning-to-rank approaches (Jansen et al., 2014; Fried et al., 2015; Sharp et al., 2015). These methods represent the candidate answers as meaningful handcrafted features based on syntactic, semantic and discourse parses (Surdeanu et al., 2011; Jansen et al., 2014), web correlation (Surdeanu et al., 2011), and translation probabilities (Fried et al., 2015; Surdeanu et al., 2011). The resulting feature vectors are then passed to a supervised ranking algorithm, such as SVMrank (Joachims, 2006), which ranks the candidates. There has been a recent shift in Natural Language Processing towards neural approaches involving minimal feature engineering. Several recent studies present purely neural approaches to answer reranking, with most of them focusing on the task of passage-level answer selection (dos Santos et al., 2016; Tan et al., 201"
E17-1012,N16-1152,0,0.0433374,"pproach. Fried et al. (2015) improve on the lexical semantic models of Jansen et al. (2014) by exploiting indirect associations between words using higher-order models. 3 Methods based purely on neural models have gained popularity in various areas of NLP in recent years. The main advantage of these models is that they are often able to achieve state-ofthe-art results while obviating the need for manual feature engineering. These approaches have been successful in the area of question answering. Several studies proposed models based on convolution neural networks (Severyn and Moschitti, 2015; Tymoshenko et al., 2016; Feng et al., 2015) for answer sentence selection for factoid question answering and models based on combinations of convolutional and recurrent neural networks for the task of passage-level non-factoid answer reranking (Tan et al., 2015; dos Santos et al., 2016). Recurrent neural networks and memory networks were successfully applied to the task of reading comprehension (Xiong et al., 2016; Sukhbaatar et al., 2015; Weston et al., 2015). A simple purely neural approach to non-factoid answer reranking in CQAs was proposed by Bogdanova and Foster (2016). The question-answer pairs are represente"
E17-2056,P15-2026,0,0.249424,"human post-editing (Knight and Chander, 1994). APE assumes the availability of source texts (src), corresponding MT output (mt) and the human postedited (pe) version of mt. However, APE systems can also be built without the availability of src, by using only sufficient amounts of target side “mono-lingual” parallel mt–pe data. Usually APE tasks focus on systematic errors made by first stage MT systems, acting as an effective remedy to some of the inaccuracies in raw MT output. APE approaches cover a wide methodological range such as SMT techniques (Simard et al., 2007a; Simard et al., 2007b; Chatterjee et al., 2015; Pal et al., 2015; Pal et al., 2016d) real time integration of post-editing in MT (Denkowski, 2015), rule-based approaches to APE (Mareˇcek et al., 2011; Rosa et al., 2012), neural APE (JunczysDowmunt and Grundkiewicz, 2016; Pal et al., 2016b), multi-engine and multi-alignment APE (Pal et al., 2016a), etc. We present a second-stage machine translation (MT) system based on a neural machine translation (NMT) approach to automatic post-editing (APE) that improves the translation quality provided by a firststage MT system. Our APE system (AP ESym ) is an extended version of an attention based NMT"
E17-2056,N16-1102,0,0.204776,"rate a corresponding aid based on the context swid appears in. The APE words are generated from aid by looking up the hybrid prior alignment look-up table (LUT). Neural MT jointly learns alignment and translation. Replacing the source and target words by swid and aid , respectively, implicitly integrates the prior alignment and lessens the burden of the attention model. Secondly, our approach bears a resemblance to the sense embedding approach (Li and Jurafsky, 2015) since an embedding is generated for each (swid , aid ) pair. quality. Our neural model of APE is based on the work described in Cohn et al. (2016) which implements structural alignment biases into an attention based bidirectional recurrent neural network (RNN) MT model (Bahdanau et al., 2015). Cohn et al. (2016) extends the attentional soft alignment model to traditional word alignment models (IBM models) and agreement over both translation directions (in our case mt → pe and pe → mt) to ensure better alignment consistency. We follow Cohn et al. (2016) in encouraging our alignment models to be symmetric (Och and Ney, 2003) in both translation directions with embedded prior alignments. Different from Cohn et al. (2016), we employed prior"
E17-2056,D15-1200,0,0.0129567,"unique identification number (aid ) and a vector representation is generated for each such aid . Given a swid , the neural APE model is trained to generate a corresponding aid based on the context swid appears in. The APE words are generated from aid by looking up the hybrid prior alignment look-up table (LUT). Neural MT jointly learns alignment and translation. Replacing the source and target words by swid and aid , respectively, implicitly integrates the prior alignment and lessens the burden of the attention model. Secondly, our approach bears a resemblance to the sense embedding approach (Li and Jurafsky, 2015) since an embedding is generated for each (swid , aid ) pair. quality. Our neural model of APE is based on the work described in Cohn et al. (2016) which implements structural alignment biases into an attention based bidirectional recurrent neural network (RNN) MT model (Bahdanau et al., 2015). Cohn et al. (2016) extends the attentional soft alignment model to traditional word alignment models (IBM models) and agreement over both translation directions (in our case mt → pe and pe → mt) to ensure better alignment consistency. We follow Cohn et al. (2016) in encouraging our alignment models to b"
E17-2056,W06-1607,0,0.0220804,"rd pairs from hybrid prior alignment (Section 2.1) between mt–pe (12K data) were used for the additional training data to build AP EB2 . The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both the source and target language. Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (1) in the PB-SMT framework. To compensate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003). kx and ky correspond to the vocabulary sizes of source and target languages, respectively. The hidden state of the decoder at time t is computed as ηt = f (ηt−1 , yt−1 , ct ), where ct is the context vecP x tor computed as ct = Ti=1 αti hi . Here, αti is the weight of each hi and can be computed as in Equation 1 exp(eti ) αti = Pm (1) j=1 exp(etj ) where eti = a(ηt−1 , hi ) is a word alignment model. Based on the input (mt) and output (pe) sequence lengths, Tx and Ty , the alignment model is computed Tx × Ty"
E17-2056,N06-1014,0,0.513033,"corrected by human translators. This task is referred to as post-editing (PE). PE is often understood as the process of improving a translation provided by an MT system with the minimum In this paper we present a neural network based APE system to improve raw first-stage MT output 349 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 349–355, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics grow-diag-final-and (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on Translation Edit Rate (TER) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow the alignment strategy described in (Pal et al., 2013; Pal et al., 2016a). The aligned word pairs are added as additional training examples to train our symmetric neural APE model. Each word in the first stage MT output is assigned a unique id (swid ). Each mt–pe word alignment also gets a unique identification number (aid ) and a vector representation is gener"
E17-2056,D08-1089,0,0.0468216,"ata. For building our AP EB2 system, we set a maximum phrase length of 7 for the translation model, and a 5-gram language model was trained using KenLM (Heafield, 2011). Word alignments between the mt and pe (4.5M synthetic mt-pe data + 12K WMT APE data) were established using the Berkeley Aligner (Liang et al., 2006), while word pairs from hybrid prior alignment (Section 2.1) between mt–pe (12K data) were used for the additional training data to build AP EB2 . The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both the source and target language. Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (1) in the PB-SMT framework. To compensate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003). kx and ky correspond to the vocabulary sizes of source and target languages, respectively. The hidden state of the decoder at time t is computed as ηt = f (ηt−1 , yt−1 , ct ), where ct is th"
E17-2056,W11-2123,0,0.0229802,"mt–pe symmetric model (AP ESym ) against the best performing system (W M TBest ) in the WMT 2016 APE task and the standard log-linear mt–pe PB-SMT model with hybrid prior alignment as described in Section 2.1 (AP EB2 ). AP EB2 and AP ESym models are trained on 4.55M (4.5M + 12K + pre-aligned word pairs) parallel mt–pe data. The pre-aligned word pairs are obtained from the hybrid prior word alignments (Section 2.1) of the 12K WMT APE training data. For building our AP EB2 system, we set a maximum phrase length of 7 for the translation model, and a 5-gram language model was trained using KenLM (Heafield, 2011). Word alignments between the mt and pe (4.5M synthetic mt-pe data + 12K WMT APE data) were established using the Berkeley Aligner (Liang et al., 2006), while word pairs from hybrid prior alignment (Section 2.1) between mt–pe (12K data) were used for the additional training data to build AP EB2 . The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both the source and target language. Phrase pairs that occur only once in the training data are assigned an unduly high pro"
E17-2056,J03-1002,0,0.0268,"edding is generated for each (swid , aid ) pair. quality. Our neural model of APE is based on the work described in Cohn et al. (2016) which implements structural alignment biases into an attention based bidirectional recurrent neural network (RNN) MT model (Bahdanau et al., 2015). Cohn et al. (2016) extends the attentional soft alignment model to traditional word alignment models (IBM models) and agreement over both translation directions (in our case mt → pe and pe → mt) to ensure better alignment consistency. We follow Cohn et al. (2016) in encouraging our alignment models to be symmetric (Och and Ney, 2003) in both translation directions with embedded prior alignments. Different from Cohn et al. (2016), we employed prior alignment computed by a hybrid multi-alignment approach. Evaluation results show consistent improvements over the raw firststage MT system output and over the previous best performing neural APE (Junczys-Dowmunt and Grundkiewicz, 2016) on the WMT 2016 APE test set. In addition we show that re-ranking n-best output from baseline and enhanced PB-SMT APE systems (Section 3) together with our neural APE output provides further statistically significant improvements over all the othe"
E17-2056,W16-2378,0,0.447981,"alignment model to traditional word alignment models (IBM models) and agreement over both translation directions (in our case mt → pe and pe → mt) to ensure better alignment consistency. We follow Cohn et al. (2016) in encouraging our alignment models to be symmetric (Och and Ney, 2003) in both translation directions with embedded prior alignments. Different from Cohn et al. (2016), we employed prior alignment computed by a hybrid multi-alignment approach. Evaluation results show consistent improvements over the raw firststage MT system output and over the previous best performing neural APE (Junczys-Dowmunt and Grundkiewicz, 2016) on the WMT 2016 APE test set. In addition we show that re-ranking n-best output from baseline and enhanced PB-SMT APE systems (Section 3) together with our neural APE output provides further statistically significant improvements over all the other systems. The main contributions of our research are (i) an application of bilingual symmetry of the bidirectional RNN for APE, (ii) using a hybrid multialignment based approach for the prior alignments, (iii) a smart way of embedding word alignment information in neural APE, and (iv) applying reranking for the APE task. The remainder of the paper i"
E17-2056,P03-1021,0,0.127663,"= σ(W r Ex is the word embedding matrix of the MT output, W r ∈ Rm×n and U r ∈ Rn×n are weight matrices, m is the word embedding dimensionality and n represents the number of hidden units. Symmetric Neural Automatic Post Editing Using Prior Alignment Below we describe bilingual symmetry of bidirectional RNN with embedded prior word alignment for APE. 2.1 Symmetric Neural APE Hybrid Prior Alignment The monolingual mt–pe parallel corpus is first word aligned using a hybrid word alignment method based on the alignment combination of three different statistical word alignment methods: (i) GIZA++ (Och, 2003) word alignment with 350 data described in Bojar et al. (2016) and for some experiments we also use the 4.5M artificially developed APE data described in Junczys-Dowmunt and Grundkiewicz (2016). The training data consists of English–German triplets containing source English text (src) from the IT domain, corresponding German translations (mt) from a firststage MT system and the corresponding human post-edited version (pe). Development and test data contain 1,000 and 2,000 triplets respectively. We considered two baselines: (i) the raw MT output provided by the first-stage MT system serves as B"
E17-2056,W13-2814,1,0.838103,"dings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 349–355, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics grow-diag-final-and (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on Translation Edit Rate (TER) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow the alignment strategy described in (Pal et al., 2013; Pal et al., 2016a). The aligned word pairs are added as additional training examples to train our symmetric neural APE model. Each word in the first stage MT output is assigned a unique id (swid ). Each mt–pe word alignment also gets a unique identification number (aid ) and a vector representation is generated for each such aid . Given a swid , the neural APE model is trained to generate a corresponding aid based on the context swid appears in. The APE words are generated from aid by looking up the hybrid prior alignment look-up table (LUT). Neural MT jointly learns alignment and translatio"
E17-2056,W15-3026,1,0.847079,"Missing"
E17-2056,W04-3250,0,0.0339845,"Missing"
E17-2056,J10-4005,0,0.0193819,"ions produced by MT systems often need to be corrected by human translators. This task is referred to as post-editing (PE). PE is often understood as the process of improving a translation provided by an MT system with the minimum In this paper we present a neural network based APE system to improve raw first-stage MT output 349 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 349–355, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics grow-diag-final-and (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on Translation Edit Rate (TER) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow the alignment strategy described in (Pal et al., 2013; Pal et al., 2016a). The aligned word pairs are added as additional training examples to train our symmetric neural APE model. Each word in the first stage MT output is assigned a unique id (swid ). Each mt–pe word alignment also gets a unique identification"
E17-2056,C16-1241,1,0.878149,"Missing"
E17-2056,W07-0734,0,0.0440759,"network based APE system to improve raw first-stage MT output 349 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 349–355, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics grow-diag-final-and (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on Translation Edit Rate (TER) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow the alignment strategy described in (Pal et al., 2013; Pal et al., 2016a). The aligned word pairs are added as additional training examples to train our symmetric neural APE model. Each word in the first stage MT output is assigned a unique id (swid ). Each mt–pe word alignment also gets a unique identification number (aid ) and a vector representation is generated for each such aid . Given a swid , the neural APE model is trained to generate a corresponding aid based on the context swid appears in. The APE words are generated from aid by looking up the hybrid prior alignment look-"
E17-2056,P16-2046,1,0.906567,"Missing"
E17-2056,C16-2021,1,0.88815,"Missing"
E17-2056,W16-2379,1,0.894217,"Missing"
E17-2056,P02-1040,0,0.0983167,"Missing"
E17-2056,W12-3146,0,0.137315,"Missing"
E17-2056,N07-1064,0,0.44034,"ving raw MT output, before performing actual human post-editing (Knight and Chander, 1994). APE assumes the availability of source texts (src), corresponding MT output (mt) and the human postedited (pe) version of mt. However, APE systems can also be built without the availability of src, by using only sufficient amounts of target side “mono-lingual” parallel mt–pe data. Usually APE tasks focus on systematic errors made by first stage MT systems, acting as an effective remedy to some of the inaccuracies in raw MT output. APE approaches cover a wide methodological range such as SMT techniques (Simard et al., 2007a; Simard et al., 2007b; Chatterjee et al., 2015; Pal et al., 2015; Pal et al., 2016d) real time integration of post-editing in MT (Denkowski, 2015), rule-based approaches to APE (Mareˇcek et al., 2011; Rosa et al., 2012), neural APE (JunczysDowmunt and Grundkiewicz, 2016; Pal et al., 2016b), multi-engine and multi-alignment APE (Pal et al., 2016a), etc. We present a second-stage machine translation (MT) system based on a neural machine translation (NMT) approach to automatic post-editing (APE) that improves the translation quality provided by a firststage MT system. Our APE system (AP ESym )"
E17-2056,W07-0728,0,0.0707865,"ving raw MT output, before performing actual human post-editing (Knight and Chander, 1994). APE assumes the availability of source texts (src), corresponding MT output (mt) and the human postedited (pe) version of mt. However, APE systems can also be built without the availability of src, by using only sufficient amounts of target side “mono-lingual” parallel mt–pe data. Usually APE tasks focus on systematic errors made by first stage MT systems, acting as an effective remedy to some of the inaccuracies in raw MT output. APE approaches cover a wide methodological range such as SMT techniques (Simard et al., 2007a; Simard et al., 2007b; Chatterjee et al., 2015; Pal et al., 2015; Pal et al., 2016d) real time integration of post-editing in MT (Denkowski, 2015), rule-based approaches to APE (Mareˇcek et al., 2011; Rosa et al., 2012), neural APE (JunczysDowmunt and Grundkiewicz, 2016; Pal et al., 2016b), multi-engine and multi-alignment APE (Pal et al., 2016a), etc. We present a second-stage machine translation (MT) system based on a neural machine translation (NMT) approach to automatic post-editing (APE) that improves the translation quality provided by a firststage MT system. Our APE system (AP ESym )"
E17-2056,2006.amta-papers.25,0,0.0951164,"In this paper we present a neural network based APE system to improve raw first-stage MT output 349 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 349–355, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics grow-diag-final-and (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on Translation Edit Rate (TER) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow the alignment strategy described in (Pal et al., 2013; Pal et al., 2016a). The aligned word pairs are added as additional training examples to train our symmetric neural APE model. Each word in the first stage MT output is assigned a unique id (swid ). Each mt–pe word alignment also gets a unique identification number (aid ) and a vector representation is generated for each such aid . Given a swid , the neural APE model is trained to generate a corresponding aid based on the context swid appears in. The APE words are generated from aid by lookin"
E17-2056,C96-2141,0,0.645894,"α are alignment sumj i αi,j (attention) matrices of Tx × Ty dimensions. The advantage of symmetrical alignment cells is that they are normalized using softmax (values in between 0 and 1), therefore, the trace term is bounded above by min(Tx , Ty ), representing perfect one-to-one alignments in both directions. To train each directional attention model (mt → pe and pe → mt), we follow the work described in Cohn et al. (2016), where absolute positional bias between the MT and PE translation (as in IBM Model 2), fertility relative position bias (as in IBM Models 3, 4, 5) and HMM-based Alignment (Vogel et al., 1996) are incorporated with an attention based soft alignment model. 3 Experiments and Results We carried out our experiments on the 12K English–German WMT 2016 APE task training 1 351 http://www.statmt.org/moses/ For setting up our neural network, previous to training the AP ESym model, we performed a number of preprocessing steps on the mt–pe parallel training data. First, we prepare a LUT containing mt–pe hybrid prior word alignment above (Section 2.1) a certain lexical translation probability threshold (0.3). To ensure efficient use of the hybrid prior alignment we replaced each mt word by a un"
E17-2057,N15-1124,1,0.941061,"slation quality and the actual quality of translated documents. Including such weights in the construction of a gold standard potentially invalidates the human evaluation, and is unfortunately very likely to exaggerate the apparent performance of some systems while under-rewarding others. 357 0.2 0.1 3 0.0 −0.2 0 10 20 30 40 Alternate Human Gold Standard A recent development in human evaluation of MT is direct assessment (“DA”), a human assessment shown to yield highly replicable segment-level scores, by combination of a minimum of 15 repeat human assessments per translation into mean scores (Graham et al., 2015). Human adequacy assessments are collected via a 0–100 rating scale that facilitates reliable quality control of crowd-sourcing. Document-level DA scores are computed by repeat assessment of the individual segments within a given document, computation of the mean score for each segment (micro-average), and finally, combination of the mean segment scores into an overall mean document score (macro-average).2 DA assessments are carried out by comparison of a given MT output segment (rendered in black) with a human-generated reference translation (in gray), and human annotators rate the degree to"
E17-2057,C16-1294,1,0.865252,"hree participating sys5 Post-editing cost estimates are based on 0.06 and 0.12 Euro per source document word converted to USD$. Further details provided by the post-editor in relation to estimates can be found at https://github.com/ygraham/ eacl2017 4 Variance in numbers of repeat assessments per document is due to sentences of all documents being sampled without preference for documents made up of larger numbers of sentences. 359 RTM-FS+PLS-TREE GRAPH-DISC BASE-EMB-GP BASELINE RTM-FS-SVR DA WMT-16 0.38 0.32 0.31 0.26 0.23 0.36 0.26 0.39 0.29 0.29 it in document-level QE evaluation therefore. Graham et al. (2016a) provide an investigation into reference bias in monolingual evaluation of MT and despite the risk of reference bias that DA adequacy could potentially encounter, experiment results show no evidence of reference bias. Human assessors of MT appear to genuinely read and compare the meaning of the reference translation and the MT output, as requested with DA, applying their human intelligence to the task in a reliable way, and are not overly influenced by the generic reference. Although DA fluency could still have its own applications, for the purpose of evaluating MT or MT QE, this additional"
E17-2057,P15-1174,1,0.837171,"ments (specifically Meteor scores (Denkowski and Lavie, 2011)), and system predictions were compared to gold labels via MAE. A conclusion that emerged from the initial shared task was that automatic metric scores were not adequate, based on the following observation: if the average of the training set scores is used as a prediction value for all data points in the test set, this results in a system as good as the baseline system when evaluated with MAE. The fact that average scores are good predictors is more likely a consequence of the applied evaluation measure, MAE, however, as outlined in Graham (2015). When evaluated with the Pearson correlation, such a set of predictions would not be a reasonable entry to the shared task since the prediction distribution would effectively be a constant and its correlation with anything is therefore undefined. Regardless of the predictability of automatic metric scores when evaluated with MAE, they unfortunately do not provide a suitable gold standard, simply because they are known to provide an insufficient substitute for human assessment, often unfairly penalizing translations that happen to be superficially dissimilar to reference translations (Callison"
E17-2057,E06-1032,0,0.076271,"m (2015). When evaluated with the Pearson correlation, such a set of predictions would not be a reasonable entry to the shared task since the prediction distribution would effectively be a constant and its correlation with anything is therefore undefined. Regardless of the predictability of automatic metric scores when evaluated with MAE, they unfortunately do not provide a suitable gold standard, simply because they are known to provide an insufficient substitute for human assessment, often unfairly penalizing translations that happen to be superficially dissimilar to reference translations (Callison-Burch et al., 2006). Consequently, for WMT-16, the gold standard was modified to take the form of a linear combination of two human-targeted translation edit rate (HTER) (Snover et al., 2006) scores assigned to a given document. Scores were produced via two human post-editing steps: firstly, sentences within a given MT-output document were post-edited independent of other sentences in that document, producing post-edition 1 (P E1 ). Secondly, P E1 sentences were concatenated to form a documentlevel translation, and post-edited a second time by the same annotator, with the aim of isolating errors only identifiabl"
E17-2057,W07-0718,0,0.0579243,"proving Evaluation of Document-level Machine Translation Quality Estimation Yvette Graham Dublin City University Qingsong Ma Chinese Academy of Sciences Timothy Baldwin University of Melbourne yvette.graham@dcu.ie maqingsong@ict.ac.cn tb@ldwin.net Qun Liu Dublin City University Carla Parra Dublin City University Carolina Scarton University of Sheffield qun.liu@dcu.ie carla.parra@adaptcentre.ie c.scarton@sheffield.ac.uk Abstract subjective, making high IAA difficult to achieve. For example, in past large-scale human evaluations of MT, low IAA levels have been highlighted as a cause of concern (Callison-Burch et al., 2007; Bojar et al., 2016). Such problems cause challenges not only for evaluation of MT systems, but also for MT quality estimation (QE), where the ideal gold standard comprises human assessment. Meaningful conclusions about the relative performance of NLP systems are only possible if the gold standard employed in a given evaluation is both valid and reliable. In this paper, we explore the validity of human annotations currently employed in the evaluation of document-level quality estimation for machine translation (MT). We demonstrate the degree to which MT system rankings are dependent on weight"
E17-2057,2006.amta-papers.25,0,0.11857,"ely be a constant and its correlation with anything is therefore undefined. Regardless of the predictability of automatic metric scores when evaluated with MAE, they unfortunately do not provide a suitable gold standard, simply because they are known to provide an insufficient substitute for human assessment, often unfairly penalizing translations that happen to be superficially dissimilar to reference translations (Callison-Burch et al., 2006). Consequently, for WMT-16, the gold standard was modified to take the form of a linear combination of two human-targeted translation edit rate (HTER) (Snover et al., 2006) scores assigned to a given document. Scores were produced via two human post-editing steps: firstly, sentences within a given MT-output document were post-edited independent of other sentences in that document, producing post-edition 1 (P E1 ). Secondly, P E1 sentences were concatenated to form a documentlevel translation, and post-edited a second time by the same annotator, with the aim of isolating errors only identifiable when more context is available, to produce post-edition 2 (P E2 ). Next, two translation edit rate (TER) scores were computed by: (1) comparing the document-level MT outp"
E17-2057,W11-2107,0,0.0349009,"machine translation (MT), human assessment is more 356 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 356–361, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Background label, G, as follows: Document-level QE (Soricut and Echihabi, 2010) is a relatively new area, with only two shared tasks taking place to date (Bojar et al., 2015; Bojar et al., 2016). In WMT-15, gold standard labels took the form of automatic metric scores for documents (specifically Meteor scores (Denkowski and Lavie, 2011)), and system predictions were compared to gold labels via MAE. A conclusion that emerged from the initial shared task was that automatic metric scores were not adequate, based on the following observation: if the average of the training set scores is used as a prediction value for all data points in the test set, this results in a system as good as the baseline system when evaluated with MAE. The fact that average scores are good predictors is more likely a consequence of the applied evaluation measure, MAE, however, as outlined in Graham (2015). When evaluated with the Pearson correlation, s"
E17-2057,P10-1063,0,0.0362197,"(IAA) enable the likelihood of replicability to be taken into account, were an evaluation to be repeated with a distinct set of human annotators. One approach to achieving high IAA is through the development of a strict set of annotation guidelines, while for machine translation (MT), human assessment is more 356 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 356–361, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Background label, G, as follows: Document-level QE (Soricut and Echihabi, 2010) is a relatively new area, with only two shared tasks taking place to date (Bojar et al., 2015; Bojar et al., 2016). In WMT-15, gold standard labels took the form of automatic metric scores for documents (specifically Meteor scores (Denkowski and Lavie, 2011)), and system predictions were compared to gold labels via MAE. A conclusion that emerged from the initial shared task was that automatic metric scores were not adequate, based on the following observation: if the average of the training set scores is used as a prediction value for all data points in the test set, this results in a system"
E17-2057,D14-1020,1,0.877386,"ct assessment (DA) and original gold standard (WMT-16 QE English to Spanish) tems, while under-rewarding two other systems. Notably, system GRAPH-DISC, which includes discourse features learned from document-level features, achieves a higher correlation when evaluated with DA compared to the original gold standard. Differences in correlations are small, however, and can’t be interpreted as differences in performance without significance testing. Differences in dependent correlations showed no significant difference for all pairs of competing systems according to Williams test (Williams, 1959; Graham and Baldwin, 2014). 3.3 4 Conclusion Methodological concerns were raised with respect to optimization of weights employed in construction of document-level QE gold standards in WMT-16. We demonstrated the degree to which MT system rankings are dependent on weights employed in the construction of the gold standard. Experiments showed with respect to the alternate gold standard we propose, direct assessment (DA), scores for documents are highly reliable, achieving a correlation of above 0.9 in a self-replication experiment. Finally, DA resulted in a substantial estimated cost reduction, with the original post-edi"
E17-2057,W13-2305,1,0.877135,"by qualitycontrolled crowd-sourcing in two separate data collection runs (Runs A and B) on Mechanical Turk, and compare scores for individual documents collected in each run. Quality control is carried out by inclusion of pairs of genuine MT outputs and automatically degraded versions of them (bad references) within 100-translation HITs, before a difference of means significance test is applied to the ratings belonging to a given worker. The resulting p-value is employed as an estimate of the reliability of a given human assessor to accurately distinguish between the quality of translations (Graham et al., 2013; Graham et al., 2014). Table 1 shows numbers of judgments collected in total for each data collection run on Mechanical Turk, including numbers of assessments before and after quality control filtering, where only data belonging to workers with a p-value below 0.05 were retained. Figure 2 shows the correlation between document-level DA scores collected in Run A with scores produced in Run B, where, for Run B, repeat assessments are down-sampled to show the increasing correspondence between scores as ever-increasing numbers of repeat assessments are collected for a given document. Correlation"
E17-2057,E14-1047,1,\N,Missing
E17-2057,W16-2301,1,\N,Missing
E17-2095,P96-1041,0,0.0522628,"ificantly better than GBMT at p ≤ 0.01. ∗ means a system is significantly better than PBMT at p ≤ 0.01. + means a system is significantly better than TBMT at p ≤ 0.01. Following Li et al. (2016), Chinese and German sentences are parsed into projective dependency trees which are then converted to graphs by adding bigram edges. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune feature weights. We report BLEU (Papineni et al., 2002) scores averaged on three runs of MIRA (Clark et al., 2011). We compare our system GBMTctx with several other systems. A system PBMT is built using the phrase-based model in Moses (Koehn et al., 2007). GBMT is the graph-based translation system described in Li et al. (2016). To examine the influence of bigram links, GBMT is also used to translate dependency trees where treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) are the basic translation units. Accordingl"
E17-2095,N12-1047,0,0.0158415,"1. ∗ means a system is significantly better than PBMT at p ≤ 0.01. + means a system is significantly better than TBMT at p ≤ 0.01. Following Li et al. (2016), Chinese and German sentences are parsed into projective dependency trees which are then converted to graphs by adding bigram edges. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune feature weights. We report BLEU (Papineni et al., 2002) scores averaged on three runs of MIRA (Clark et al., 2011). We compare our system GBMTctx with several other systems. A system PBMT is built using the phrase-based model in Moses (Koehn et al., 2007). GBMT is the graph-based translation system described in Li et al. (2016). To examine the influence of bigram links, GBMT is also used to translate dependency trees where treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) are the basic translation units. Accordingly, we name the system TBMT. All system"
E17-2095,P11-2031,0,0.022121,"wing Li et al. (2016), Chinese and German sentences are parsed into projective dependency trees which are then converted to graphs by adding bigram edges. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune feature weights. We report BLEU (Papineni et al., 2002) scores averaged on three runs of MIRA (Clark et al., 2011). We compare our system GBMTctx with several other systems. A system PBMT is built using the phrase-based model in Moses (Koehn et al., 2007). GBMT is the graph-based translation system described in Li et al. (2016). To examine the influence of bigram links, GBMT is also used to translate dependency trees where treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) are the basic translation units. Accordingly, we name the system TBMT. All systems are implemented in Moses. 3.2 Results and Discussion Table 1 shows BLEU scores of all systems. We found that GBMTctx is better th"
E17-2095,N10-1140,0,0.291447,"raphs by taking source context into consideration. Translations are generated by combining subgraph translations leftto-right using beam search. Experiments on Chinese–English and German–English demonstrate that the context-aware segmentation significantly improves the baseline graph-based model. 1 Introduction The well-known phrase-based statistical translation model (Koehn et al., 2003) extends the basic translation units from single words to continuous phrases to capture local phenomena. However, one of its significant weaknesses is that it cannot learn generalizations (Quirk et al., 2005; Galley and Manning, 2010). To allow discontinuous phrases (any subset of words of an input sentence), dependency treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) can be used, which are connected subgraphs on trees. However, continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance (Koehn et al., 2003; Hanneman and Lavie, 2009). To make use of the merits of both phrase-based models and treelet-based models, Li et al. (2016) proposed a graph-based translation model as in Equation (1): source subgraph g ai .1 d is a distance-"
E17-2095,W09-2301,0,0.0813015,"the basic translation units from single words to continuous phrases to capture local phenomena. However, one of its significant weaknesses is that it cannot learn generalizations (Quirk et al., 2005; Galley and Manning, 2010). To allow discontinuous phrases (any subset of words of an input sentence), dependency treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) can be used, which are connected subgraphs on trees. However, continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance (Koehn et al., 2003; Hanneman and Lavie, 2009). To make use of the merits of both phrase-based models and treelet-based models, Li et al. (2016) proposed a graph-based translation model as in Equation (1): source subgraph g ai .1 d is a distance-based reordering function which penalizes discontinuous phrases that have relatively long gaps (Galley and Manning, 2010). The model translates an input graph by segmenting it into subgraphs and generates a complete translation by combining subgraph translations left-to-right. However, the model treats different graph segmentations equally. Therefore, in this paper we propose a contextaware graph"
E17-2095,N03-1017,0,0.0108859,"School of Computing Dublin City University, Ireland {liangyou.li,andy.way,qun.liu}@adaptcentre.ie Abstract In this paper, we present an improved graph-based translation model which segments an input graph into node-induced subgraphs by taking source context into consideration. Translations are generated by combining subgraph translations leftto-right using beam search. Experiments on Chinese–English and German–English demonstrate that the context-aware segmentation significantly improves the baseline graph-based model. 1 Introduction The well-known phrase-based statistical translation model (Koehn et al., 2003) extends the basic translation units from single words to continuous phrases to capture local phenomena. However, one of its significant weaknesses is that it cannot learn generalizations (Quirk et al., 2005; Galley and Manning, 2010). To allow discontinuous phrases (any subset of words of an input sentence), dependency treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) can be used, which are connected subgraphs on trees. However, continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance (Koehn et al"
E17-2095,P07-2045,0,0.0137392,"bigram edges. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune feature weights. We report BLEU (Papineni et al., 2002) scores averaged on three runs of MIRA (Clark et al., 2011). We compare our system GBMTctx with several other systems. A system PBMT is built using the phrase-based model in Moses (Koehn et al., 2007). GBMT is the graph-based translation system described in Li et al. (2016). To examine the influence of bigram links, GBMT is also used to translate dependency trees where treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) are the basic translation units. Accordingly, we name the system TBMT. All systems are implemented in Moses. 3.2 Results and Discussion Table 1 shows BLEU scores of all systems. We found that GBMTctx is better than PBMT across all test sets. Specifically, the improvements are +2.0/+0.7 BLEU on average on ZH–EN and DE– EN, respectively. This improvemen"
E17-2095,P16-1010,1,0.525151,"e of its significant weaknesses is that it cannot learn generalizations (Quirk et al., 2005; Galley and Manning, 2010). To allow discontinuous phrases (any subset of words of an input sentence), dependency treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) can be used, which are connected subgraphs on trees. However, continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance (Koehn et al., 2003; Hanneman and Lavie, 2009). To make use of the merits of both phrase-based models and treelet-based models, Li et al. (2016) proposed a graph-based translation model as in Equation (1): source subgraph g ai .1 d is a distance-based reordering function which penalizes discontinuous phrases that have relatively long gaps (Galley and Manning, 2010). The model translates an input graph by segmenting it into subgraphs and generates a complete translation by combining subgraph translations left-to-right. However, the model treats different graph segmentations equally. Therefore, in this paper we propose a contextaware graph segmentation (Section 2): (i) we add contextual information to each translation rule during traini"
E17-2095,2005.mtsummit-ebmt.13,0,0.0951289,"ht using beam search. Experiments on Chinese–English and German–English demonstrate that the context-aware segmentation significantly improves the baseline graph-based model. 1 Introduction The well-known phrase-based statistical translation model (Koehn et al., 2003) extends the basic translation units from single words to continuous phrases to capture local phenomena. However, one of its significant weaknesses is that it cannot learn generalizations (Quirk et al., 2005; Galley and Manning, 2010). To allow discontinuous phrases (any subset of words of an input sentence), dependency treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) can be used, which are connected subgraphs on trees. However, continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance (Koehn et al., 2003; Hanneman and Lavie, 2009). To make use of the merits of both phrase-based models and treelet-based models, Li et al. (2016) proposed a graph-based translation model as in Equation (1): source subgraph g ai .1 d is a distance-based reordering function which penalizes discontinuous phrases that have relatively long gaps (Galley and Manning, 2010)"
E17-2095,P02-1038,0,0.0278917,"ti i to R ; 4 2010 FIFA 2010 FIFA // segmenting and selecting rules 5 6 7 8 9 10 for q = hsai+1 , ti+1 i in P do c is the set of edges between g ai and sai+1 ; add hg ai , c, ti i to R ; end end end Shijiebei Zai Nanfei Chenggong Juxing r2 : Shijiebei Juxing x h2 : World Cup was held 2010 FIFA World Cup was held Zai Nanfei Chenggong rules are then used to generate segmenting and selecting rules by extending them with contextual connections (Lines 5–8). 2.3 r3 : Zai Nanfei Chenggong h3 : Model and Decoding Following Li et al. (2016), we define our model in the well-known log-linear framework (Och and Ney, 2002). In our experiments, we use the following standard features: two translation probabilities p(g, c|t) and p(t|g, c), two lexical translation probabilities plex (g, c|t) and plex (t|g, c), a language model p(t), a rule penalty, a word penalty, and a distortion function as defined in Galley and Manning (2010). In addition, we add one more feature into our system: a basic-rule penalty to distinguish basic rules from segmenting and selecting rules. Our decoder is very similar to the one in the conventional graph-based model, which generates hypotheses left-to-right using beam search. A hypothesis"
E17-2095,J03-1002,0,0.013059,"BMT GBMT GBMTctx 33.2 33.8∗ 34.7∗+ 35.4∗+ 19.5 19.6 19.8∗+ 20.1∗+ Basic Rule Segmenting Rule Selecting Rule Total 31.8 31.7 32.4∗+ 33.7∗+ 21.9 22.1∗ 22.4∗+ 22.8∗+ Table 1: BLEU scores of all systems. Bold figures mean GBMTctx is significantly better than GBMT at p ≤ 0.01. ∗ means a system is significantly better than PBMT at p ≤ 0.01. + means a system is significantly better than TBMT at p ≤ 0.01. Following Li et al. (2016), Chinese and German sentences are parsed into projective dependency trees which are then converted to graphs by adding bigram edges. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune feature weights. We report BLEU (Papineni et al., 2002) scores averaged on three runs of MIRA (Clark et al., 2011). We compare our system GBMTctx with several other systems. A system PBMT is built using the phrase-based model in Moses (Koehn et al., 2007). GBMT is the graph-based translation system describ"
E17-2095,J04-4002,0,0.0964544,"with each other. A selecting rule is used to select one of them. For example, rule (6) can be applied to (7) to translate 2010Nian FIFA to 2010 FIFA. In this example, the x in rule (6) matches with Chenggong Juxing (in the dashed rectangle) in (7). During training, given a word-aligned graph– string pair hg, t, ai, we extract translation rules hg ai , cai , ti i, each of which consists of a continuous target phrase ti , a source subgraph gai aligned to ti , and a source context cai . We first find initial pairs. h˜ sai , ti i is an initial pair, iff it is consistent with the word alignment a (Och and Ney, 2004). s˜aj is a set of source words which are aligned to ti . Then, the set of rules satisfies the following: 2010Nian FIFA Shijiebei (4) where dashed links are contextual connections. During decoding, when the context matches, rule (4) translates a subgraph over 2010Nian FIFA into a target phrase 2010 FIFA. For example, it can be applied to graph (5) where Shijiebei Zai Nanfei (in the dashed rectangle) is treated as x: Training 1. If h˜ sai , ti i is an initial pair and s˜ai is covered by a subgraph g ai which is connected, then hg ai , ∗, ti i is a basic rule. cai = ∗ means that a basic rule is"
E17-2095,P02-1040,0,0.098098,"a system is significantly better than TBMT at p ≤ 0.01. Following Li et al. (2016), Chinese and German sentences are parsed into projective dependency trees which are then converted to graphs by adding bigram edges. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune feature weights. We report BLEU (Papineni et al., 2002) scores averaged on three runs of MIRA (Clark et al., 2011). We compare our system GBMTctx with several other systems. A system PBMT is built using the phrase-based model in Moses (Koehn et al., 2007). GBMT is the graph-based translation system described in Li et al. (2016). To examine the influence of bigram links, GBMT is also used to translate dependency trees where treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) are the basic translation units. Accordingly, we name the system TBMT. All systems are implemented in Moses. 3.2 Results and Discussion Table 1 shows BLE"
E17-2095,P05-1034,0,0.384189,"to node-induced subgraphs by taking source context into consideration. Translations are generated by combining subgraph translations leftto-right using beam search. Experiments on Chinese–English and German–English demonstrate that the context-aware segmentation significantly improves the baseline graph-based model. 1 Introduction The well-known phrase-based statistical translation model (Koehn et al., 2003) extends the basic translation units from single words to continuous phrases to capture local phenomena. However, one of its significant weaknesses is that it cannot learn generalizations (Quirk et al., 2005; Galley and Manning, 2010). To allow discontinuous phrases (any subset of words of an input sentence), dependency treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) can be used, which are connected subgraphs on trees. However, continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance (Koehn et al., 2003; Hanneman and Lavie, 2009). To make use of the merits of both phrase-based models and treelet-based models, Li et al. (2016) proposed a graph-based translation model as in Equation (1): source subgra"
E17-2095,W07-0706,1,0.838957,"English and German–English demonstrate that the context-aware segmentation significantly improves the baseline graph-based model. 1 Introduction The well-known phrase-based statistical translation model (Koehn et al., 2003) extends the basic translation units from single words to continuous phrases to capture local phenomena. However, one of its significant weaknesses is that it cannot learn generalizations (Quirk et al., 2005; Galley and Manning, 2010). To allow discontinuous phrases (any subset of words of an input sentence), dependency treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) can be used, which are connected subgraphs on trees. However, continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance (Koehn et al., 2003; Hanneman and Lavie, 2009). To make use of the merits of both phrase-based models and treelet-based models, Li et al. (2016) proposed a graph-based translation model as in Equation (1): source subgraph g ai .1 d is a distance-based reordering function which penalizes discontinuous phrases that have relatively long gaps (Galley and Manning, 2010). The model translates an input graph by"
I05-1007,P96-1051,0,0.00907816,"f information available in treebank. 1 Introduction In the recent development of full parsing technology, semantic knowledge is seldom used, though it is known to be useful for resolving syntactic ambiguities. The reasons for this may be twofold. The ﬁrst one is that it can be very diﬃcult to add additional features which are not available in treebanks to generative models like Collins (see [1]), which are very popular for full parsing. For smaller tasks, like prepositional phrase attachment disambiguation, semantic knowledge can be incorporated ﬂexibly using diﬀerent learning algorithms (see [2,3,4,5]). For full parsing with generative models, however, incorporating semantic knowledge may involve great changes of model structures. The second reason is that semantic knowledge from external dictionaries seems to be noisy, ambiguous and not available in explicit forms, compared with the information from treebanks. Given these two reasons, it seems to be diﬃcult to combine the two diﬀerent information sources–treebank and semantic knowledge–into one integrated statistical parsing model. One feasible way to solve this problem is to keep the original parsing model unchanged and build an addition"
I05-1007,W98-0717,0,0.024978,"f information available in treebank. 1 Introduction In the recent development of full parsing technology, semantic knowledge is seldom used, though it is known to be useful for resolving syntactic ambiguities. The reasons for this may be twofold. The ﬁrst one is that it can be very diﬃcult to add additional features which are not available in treebanks to generative models like Collins (see [1]), which are very popular for full parsing. For smaller tasks, like prepositional phrase attachment disambiguation, semantic knowledge can be incorporated ﬂexibly using diﬀerent learning algorithms (see [2,3,4,5]). For full parsing with generative models, however, incorporating semantic knowledge may involve great changes of model structures. The second reason is that semantic knowledge from external dictionaries seems to be noisy, ambiguous and not available in explicit forms, compared with the information from treebanks. Given these two reasons, it seems to be diﬃcult to combine the two diﬀerent information sources–treebank and semantic knowledge–into one integrated statistical parsing model. One feasible way to solve this problem is to keep the original parsing model unchanged and build an addition"
I05-1007,W04-2410,0,0.0115748,"f information available in treebank. 1 Introduction In the recent development of full parsing technology, semantic knowledge is seldom used, though it is known to be useful for resolving syntactic ambiguities. The reasons for this may be twofold. The ﬁrst one is that it can be very diﬃcult to add additional features which are not available in treebanks to generative models like Collins (see [1]), which are very popular for full parsing. For smaller tasks, like prepositional phrase attachment disambiguation, semantic knowledge can be incorporated ﬂexibly using diﬀerent learning algorithms (see [2,3,4,5]). For full parsing with generative models, however, incorporating semantic knowledge may involve great changes of model structures. The second reason is that semantic knowledge from external dictionaries seems to be noisy, ambiguous and not available in explicit forms, compared with the information from treebanks. Given these two reasons, it seems to be diﬃcult to combine the two diﬀerent information sources–treebank and semantic knowledge–into one integrated statistical parsing model. One feasible way to solve this problem is to keep the original parsing model unchanged and build an addition"
I05-1007,P03-1054,0,0.0932181,"dependencies of internal structures of NCs due to the sparseness of bilexical dependencies. In our new parser, however, the selection preference model is able to build semantically preferable structures through word-class dependency statistics. For NCs like (n1 , n2 , n3 ), where ni is a noun, dependency structures 7 8 9 “Every way ambiguous” constructions are those for which the number of analyses is the number of binary trees over the terminal elements. Prepositional phrase attachment, coordination, and nominal compounds are all ”every way ambiguous” constructions. Just as Klein et al. (see [8]) said, one million words of training data just isn’t enough. Henceforth, [s1 , s2 ] denotes a dependency structure, where s1 is a modiﬁer word or its semantic class (C), and s2 is the head word. 78 a. D. Xiong et al. NPB  H H NR NN NN Ӕ ᆟ کห b. NP HH NP  H H NPB NPB NPB NN NR NN ห Ӕ ᆟک Fig. 1. Nominal Compounds: The North Korean government’s special envoy. a. is the incorrect ﬂat parse, b. is the right one in corpus {[Cn1 , n2 ], [Cn1 , n3 ], [Cn2 , n3 ]} will be checked in terms of semantic acceptability and semantically preferable structures will be built ﬁnally. For more comp"
I05-1007,W01-0521,0,0.011109,"Missing"
I05-1007,P03-1056,0,0.456973,"wo Chinese electronic dictionaries and their combination as our semantic information sources. Several experiments are carried out on the Penn Chinese Treebank to test our hypotheses. The results indicate that a signiﬁcant improvement in performance is achieved when semantic knowledge is incorporated into parsing model. Further improvement analysis is made. We conﬁrm that semantic knowledge is indeed useful for nominal compounds and coordination ambiguity resolution. And surprisingly, semantic knowledge is also helpful to correct Chinese NV mistagging errors mentioned by Levy and Manning (see [12]). Yet another great beneﬁt to incorporating semantic knowledge is to alleviate the sparseness of information available in treebank. 2 The Baseline Parser Our baseline parsing model is similar to the history-based, generative and lexicalized Model 1 of Collins (see [1]). In this model, the right hand side of lexicalized rules is decomposed into smaller linguistic objects as follows: P (h) → #Ln (ln )...L1 (l1 )H(h)R1 (r1 )...Rm (rm )# . The uppercase letters are delexicalized nonterminals, while the lowercase letters are lexical items, e.g. head word and head tag (part-of-speech tag of the hea"
I05-1007,W00-1201,0,0.0572772,"semantic knowledge indeed helps alleviate the fundamental sparseness of the lexical dependency information available in the CTB. For many word pairs [mod,head], whose count information is not available in the training data, the dependency statistics of head and modiﬁer can still work through the semantic category of mod. During our manual analysis of performance improvement, many other structural ambiguities are addressed due to the smoothing function of semantic knowledge. 4 Related Work on CTB Parsing Previous work on CTB parsing and their results are shown in table 5. Bikel and Chiang (see [14]) used two diﬀerent models on CTB, one based on the modiﬁed BBN model which is very similar to our baseline model, the other on Tree Insertion Grammar (TIG). While our baseline model used the same unknown word threshold with Bikel and Chiang but smaller beam width, our result outperforms theirs due to other features like distance, basic NP re-annotation used by our baseline model. Levy and Manning (see [12]) used a factored model with rich re-annotations guided by error analysis. In the baseline model, we also used several re-annotations but ﬁnd most re-annotations they suggested do not ﬁt 80"
I05-1007,C02-1126,0,0.118335,"Missing"
I05-1007,J04-4004,0,\N,Missing
I05-1007,J03-4003,0,\N,Missing
I08-1066,P06-1067,0,0.122811,"Missing"
I08-1066,H05-1098,0,0.184236,"h space using swapping window and punctuation restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a st"
I08-1066,J07-2003,0,0.0945551,"exity of BTG decoding with m-gram language model is O(n3+4(m−1) ). If a 4-gram language model is used (common in many current SMT systems), the time complexity is as high as O(n15 ). Therefore with this time complexity translating long sentences is time-consuming even with highly stringent pruning strategy. To speed up BTG decoding, Huang et al. (2005) adapted the hook trick which changes the time complexity from O(n3+4(m−1) ) to O(n3+3(m−1) ). However, the implementation of the hook trick with pruning is quite complicated. Another method to increase decoding speed is cube pruning proposed by Chiang (2007) which reduces search space significantly. In this paper, we propose two refinements to address the two issues, including (1) reordering heuristics to prevent incorrect swapping and reduce search space using swapping window and punctuation restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT a"
I08-1066,W00-2010,0,0.0338613,"swapping window and punctuation restriction. Swapping Window (SW): It constrains block swapping in the following way ACTIVATE A → hA1 , A2 i IF |A1s |+ |A2s |&lt; sws where |Ais |denotes the number of words on the source side Ais of block Ai , sws is a pre-defined swapping window size. Any inverted reordering beyond the pre-defined swapping window size is prohibited. Punctuation Restriction (PR): If two neighboring blocks include any of the punctuation marks p ∈ {， 、 ： ； 「 」 《 》 （ ） “ ”}, the two blocks will be merged with straight order. Punctuation marks were already used in parsing (Christine Doran, 2000) and statistical machine translation (Och et al., 2003). In (Och et al., 2003), three kinds of features are defined, all related to punctuation marks like quotes, parentheses and commas. Unfortunately, no statistically significant improvement on the BLEU score was reported in (Och et al., 2003). In this paper, we consider this problem from a different perspective. We emphasize that words around punctuation marks are reordered ungrammatically and therefore we positively use punctuation marks as a hard decision to restrict such reordering around punctuations. This is straightforward but yet resu"
I08-1066,W05-1507,0,0.0156052,"of bilingual phrases as features to predict their orders. Xiong et al. (2006) reported significant performance improvement on Chinese-English translation tasks in two different domains when compared with both Pharaoh (Koehn, 2004) and the original BTG using flat reordering. However, error analysis of the translation output of Xiong et al. (2006) reveals that boundary words predict wrong swapping, especially for long phrases although the MaxEnt-based reordering model shows better performance than baseline reordering models. Another big problem with BTG-based SMT is the high computational cost. Huang et al. (2005) reported that the time complexity of BTG decoding with m-gram language model is O(n3+4(m−1) ). If a 4-gram language model is used (common in many current SMT systems), the time complexity is as high as O(n15 ). Therefore with this time complexity translating long sentences is time-consuming even with highly stringent pruning strategy. To speed up BTG decoding, Huang et al. (2005) adapted the hook trick which changes the time complexity from O(n3+4(m−1) ) to O(n3+3(m−1) ). However, the implementation of the hook trick with pruning is quite complicated. Another method to increase decoding speed"
I08-1066,koen-2004-pharaoh,0,0.044683,"ss, BTG restriction is widely used for reordering in SMT (Zens et al., 2004). However, BTG restriction does not provide a mechanism to predict final orders between two neighboring blocks. 505 {htmi, liuqun, sxlin}@ict.ac.cn To solve this problem, Xiong et al. (2006) proposed an enhanced BTG with a maximum entropy (MaxEnt) based reordering model (MEBTG). MEBTG uses boundary words of bilingual phrases as features to predict their orders. Xiong et al. (2006) reported significant performance improvement on Chinese-English translation tasks in two different domains when compared with both Pharaoh (Koehn, 2004) and the original BTG using flat reordering. However, error analysis of the translation output of Xiong et al. (2006) reveals that boundary words predict wrong swapping, especially for long phrases although the MaxEnt-based reordering model shows better performance than baseline reordering models. Another big problem with BTG-based SMT is the high computational cost. Huang et al. (2005) reported that the time complexity of BTG decoding with m-gram language model is O(n3+4(m−1) ). If a 4-gram language model is used (common in many current SMT systems), the time complexity is as high as O(n15 )."
I08-1066,P07-2045,0,0.00543539,"prevent incorrect swapping and reduce search space using swapping window and punctuation restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has"
I08-1066,P06-1077,1,0.851772,"tion restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a stochastic BTG, whose rules are weighted"
I08-1066,W02-2018,0,0.00938526,"hrase penalty and word penalty, respectively and λs are weights of features. These features are commonly used in the state-of-the-art systems (Koehn et al., 2005; Chiang et al., 2005). 2.2 MaxEnt-based Reordering Model The MaxEnt-based reordering model is defined on two consecutive blocks A1 and A2 together with their order o ∈ {straight, inverted} according to the maximum entropy framework. P exp( i θi hi (o, A1 , A2 )) P Ω = pθ (o|A , A ) = P 1 2 o exp( i θi hi (o, A , A )) (6) where the functions hi ∈ {0, 1} are model features and θi are weights of the model features trained automatically (Malouf, 2002). There are three steps to train a MaxEnt-based reordering model. First, we need to extract reordering examples from unannotated bilingual data, then generate features from these examples and finally estimate feature weights. 1 2 For extracting reordering examples, there are two points worth mentioning: 1. In the extraction of useful reordering examples, there is no length limitation over blocks compared with extracting bilingual phrases. 2. When enumerating all combinations of neighboring blocks, a good way to keep the number of reordering examples acceptable is to extract smallest blocks wit"
I08-1066,W06-1606,0,0.050471,"g window and punctuation restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a stochastic BTG, whose"
I08-1066,P02-1038,0,0.0523918,"ining data, (3) a CKY-style decoder using beam search similar to that of Wu (1996). We describe the first two components briefly below. 2.1 Model The translation process is modeled using BTG rules which are listed as follows A → [A1 , A2 ] (1) A → hA1 , A2 i (2) A → x/y (3) The lexical rule (3) is used to translate source phrase x into target phrase y and generate a block A. The 506 two rules (1) and (2) are used to merge two consecutive blocks into a single larger block in a straight or inverted order. To construct a stochastic BTG, we calculate rule probabilities using the log-linear model (Och and Ney, 2002). For the two merging rules (1) and (2), the assigned probability P rm (A) is defined as follows LM P rm (A) = ΩλΩ · 4λpLM (A1 ,A2 ) (4) where Ω, the reordering score of block A1 and A2 , is calculated using the MaxEnt-based reordering model (Xiong et al., 2006) described in the next section, λΩ is the weight of Ω, and 4pLM (A1 ,A2 ) is the increment of language model score of the two blocks according to their final order, λLM is its weight. For the lexical rule (3), it is applied with a probability P rl (A) P rl (A) = p(x|y)λ1 · p(y|x)λ2 · plex (x|y)λ3 ·plex (y|x)λ4 · exp(1)λ5 · exp(|y|)λ6 LM"
I08-1066,P96-1021,0,0.151694,"valuation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a stochastic BTG, whose rules are weighted using different features in log-linear form, (2) a MaxEnt-based reordering model with features automatically learned from bilingual training data, (3) a CKY-style decoder using beam search similar to that of Wu (1996). We describe the first two components briefly below. 2.1 Model The translation process is modeled using BTG rules which are listed as follows A → [A1 , A2 ] (1) A → hA1 , A2 i (2) A → x/y (3) The lexical rule (3) is used to translate source phrase x into target phrase y and generate a block A. The 506 two rules (1) and (2) are used to merge two consecutive blocks into a single larger block in a straight or inverted order. To construct a stochastic BTG, we calculate rule probabilities using the log-linear model (Och and Ney, 2002). For the two merging rules (1) and (2), the assigned probabilit"
I08-1066,P06-1066,1,0.959397,"tegies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a stochastic BTG, whose rules are weighted using different features in log-linear form, (2) a MaxEnt-based reordering model with features automatically learned from bilingual training data, (3) a CKY-style decoder using beam search similar to that of Wu (1996). We describe the first two components briefly below. 2.1 Model The translation process is modeled using BTG rules which are listed as follows A → [A1 , A2 ] (1) A → hA1 , A2 i (2) A → x/y (3) The lexical rule (3) is used to translate source ph"
I08-1066,C04-1030,0,0.374843,"Missing"
I08-1066,zhang-etal-2004-interpreting,0,0.0650612,"Missing"
I08-1066,2005.iwslt-1.8,0,\N,Missing
I11-1145,P05-1033,0,0.668587,"i Zhaopeng Tu, Yang Liu, Qun Liu and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji"
I11-1145,P05-1066,0,0.271267,"Missing"
I11-1145,N03-1017,0,0.106149,"han economy ’s China fazhan economy ’s China Abstract zhongguo de jingji Zhaopeng Tu, Yang Liu, Qun Liu and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not e"
I11-1145,P06-1077,1,0.914482,"Yang Liu, Qun Liu and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji, China X1 economy"
I11-1145,D10-1053,0,0.130861,"Missing"
I11-1145,D09-1106,1,0.815845,"Missing"
I11-1145,P08-1010,0,0.0172715,"the two approaches. Criteria Total Rules Best Rule More Rules 9.2M 21.4M NIST03 Rules BLEU 590K 28.92 1.54M 29.07 NIST04 Rules BLEU 836K 31.77 1.97M 31.66 NIST05 Rules BLEU 677K 28.87 1.72M 29.02 Table 6: Comparison of rule tables learned from weighted matrices using different criterions. “Best Rule” denotes the rule table using the criteria described in Section 3.2, “More Rules” denotes the rule table using the criteria that retains all candidate target phrases that reach pruning threshold. and Huang (2008) and Tu et al. (2010) use forests instead of 1-best trees; Venugopal et al. (2003) and Deng et al. (2008) soft the alignment consistency constraint to extract more rules; Dyer et al. (2008) use word lattices instead of 1-best segmentations to generate more alignments for a sentence pair; Venugopal et al. (2008) use n-best alignments directly for rule extraction. To generate larger rule sets, de Gispert et al. (2010) extract hierarchical rules from alignment posterior probabilities. They concern how to extract larger rule sets using simple yet powerful hierarchical grammar, while we focus on whether weighted alignment matrix could overcome the alignment errors for different translation models (e.g"
I11-1145,D08-1022,0,0.184832,"Missing"
I11-1145,P08-1115,0,0.0604813,"s BLEU 590K 28.92 1.54M 29.07 NIST04 Rules BLEU 836K 31.77 1.97M 31.66 NIST05 Rules BLEU 677K 28.87 1.72M 29.02 Table 6: Comparison of rule tables learned from weighted matrices using different criterions. “Best Rule” denotes the rule table using the criteria described in Section 3.2, “More Rules” denotes the rule table using the criteria that retains all candidate target phrases that reach pruning threshold. and Huang (2008) and Tu et al. (2010) use forests instead of 1-best trees; Venugopal et al. (2003) and Deng et al. (2008) soft the alignment consistency constraint to extract more rules; Dyer et al. (2008) use word lattices instead of 1-best segmentations to generate more alignments for a sentence pair; Venugopal et al. (2008) use n-best alignments directly for rule extraction. To generate larger rule sets, de Gispert et al. (2010) extract hierarchical rules from alignment posterior probabilities. They concern how to extract larger rule sets using simple yet powerful hierarchical grammar, while we focus on whether weighted alignment matrix could overcome the alignment errors for different translation models (e.g. phrase-based, hierarchical phrase-based and tree-based models). They use phrase po"
I11-1145,P02-1038,0,0.0206047,"p(′ s|N U LL) × 0.36 ×   p(economy|jingji) × 1.0 ′ ′ Here the probability that economy translates a source NULL token is 0.0. 4 Experiments 4.1 Data Preparation Our experiments are on Chinese-English translation based on replications of hierarchical phrasebased system (Chiang, 2007) and tree-to-string system (Liu et al., 2006). We train a 4-gram language model on the Xinhua portion of GIGAWORD corpus using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995). We optimize feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) on the NIST 2003/2004/2005 test sets. To obtain weighted alignment matrices, we follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 20-best lists in two translation directions, then used “grow-diag-finaland” (Koehn et al., 2003) to all 20 × 20 bidirectional alignment pairs. We follow Liu et al. (2009) to use ps2t × pt2s as the probabilities of an alignment pair. Analogously, we abandon duplicate alignments that are produced from different alignmen"
I11-1145,P06-1121,0,0.373917,"and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji, China X1 economy).To alleviate this p"
I11-1145,P09-1104,0,0.0325122,"Missing"
I11-1145,2006.amta-papers.8,0,0.437981,"Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji, China X1 economy).To alleviate this problem, a natural so"
I11-1145,J04-4002,0,0.0845271,"o-string model. fazhan economy ’s China fazhan economy ’s China Abstract zhongguo de jingji Zhaopeng Tu, Yang Liu, Qun Liu and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de,"
I11-1145,P02-1040,0,0.0830624,"oken is 0.0. 4 Experiments 4.1 Data Preparation Our experiments are on Chinese-English translation based on replications of hierarchical phrasebased system (Chiang, 2007) and tree-to-string system (Liu et al., 2006). We train a 4-gram language model on the Xinhua portion of GIGAWORD corpus using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995). We optimize feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) on the NIST 2003/2004/2005 test sets. To obtain weighted alignment matrices, we follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 20-best lists in two translation directions, then used “grow-diag-finaland” (Koehn et al., 2003) to all 20 × 20 bidirectional alignment pairs. We follow Liu et al. (2009) to use ps2t × pt2s as the probabilities of an alignment pair. Analogously, we abandon duplicate alignments that are produced from different alignment pairs. After these steps, there are 110 candidate alignments on average for each sentence pair. We obtained n-best lists"
I11-1145,P08-1066,0,0.158557,"Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji, China X1 economy).To alleviate this problem, a natural solution is to extract"
I11-1145,J10-4005,0,0.0532568,"s the complement. Note that the probabilities of “Shared” rules are different for the two approaches. Criteria Total Rules Best Rule More Rules 9.2M 21.4M NIST03 Rules BLEU 590K 28.92 1.54M 29.07 NIST04 Rules BLEU 836K 31.77 1.97M 31.66 NIST05 Rules BLEU 677K 28.87 1.72M 29.02 Table 6: Comparison of rule tables learned from weighted matrices using different criterions. “Best Rule” denotes the rule table using the criteria described in Section 3.2, “More Rules” denotes the rule table using the criteria that retains all candidate target phrases that reach pruning threshold. and Huang (2008) and Tu et al. (2010) use forests instead of 1-best trees; Venugopal et al. (2003) and Deng et al. (2008) soft the alignment consistency constraint to extract more rules; Dyer et al. (2008) use word lattices instead of 1-best segmentations to generate more alignments for a sentence pair; Venugopal et al. (2008) use n-best alignments directly for rule extraction. To generate larger rule sets, de Gispert et al. (2010) extract hierarchical rules from alignment posterior probabilities. They concern how to extract larger rule sets using simple yet powerful hierarchical grammar, while we focus on whether weighted alignm"
I11-1145,P03-1041,0,0.0272029,"ed” rules are different for the two approaches. Criteria Total Rules Best Rule More Rules 9.2M 21.4M NIST03 Rules BLEU 590K 28.92 1.54M 29.07 NIST04 Rules BLEU 836K 31.77 1.97M 31.66 NIST05 Rules BLEU 677K 28.87 1.72M 29.02 Table 6: Comparison of rule tables learned from weighted matrices using different criterions. “Best Rule” denotes the rule table using the criteria described in Section 3.2, “More Rules” denotes the rule table using the criteria that retains all candidate target phrases that reach pruning threshold. and Huang (2008) and Tu et al. (2010) use forests instead of 1-best trees; Venugopal et al. (2003) and Deng et al. (2008) soft the alignment consistency constraint to extract more rules; Dyer et al. (2008) use word lattices instead of 1-best segmentations to generate more alignments for a sentence pair; Venugopal et al. (2008) use n-best alignments directly for rule extraction. To generate larger rule sets, de Gispert et al. (2010) extract hierarchical rules from alignment posterior probabilities. They concern how to extract larger rule sets using simple yet powerful hierarchical grammar, while we focus on whether weighted alignment matrix could overcome the alignment errors for different"
I11-1145,2008.amta-papers.18,0,0.410017,"ord aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji, China X1 economy).To alleviate this problem, a natural solution is to extract rules from nbest alignments (Venugopal et al., 2008). However, using n-best alignments still face two major challenges. First, n-best alignments have to be processed individually although they share many links, see (zhongguo, China) and (jingji, economy) in Figure 1. Second, regardless of probabilities of links in each alignment, numerous wrong rule would be extracted from n-best alignments. For example, a wrong rule (X1 de jingji, of X1 ’s economy) would be extracted from the alignment in Figure 1(a). Since Liu et al. (2009) show that weighted alignment matrix provides an elegant solution to these two drawbacks, we apply it to the hierarchical"
I11-1145,P06-1066,1,0.872693,"fazhan economy ’s China Abstract zhongguo de jingji Zhaopeng Tu, Yang Liu, Qun Liu and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hi"
I11-1145,P05-1059,0,0.0789146,"Missing"
I11-1145,J07-2003,0,\N,Missing
I13-1051,D07-1090,0,0.0249087,"to train a sub model, then these sub models are used together with different weights during decoding. Those works typically use word similarities and sentence level information, while our work extents the context into the document level. the baseline model. 2 Related Work Previous works devoted to improving language models in SMT mostly focus on utilizing more contextual information, such as syntax-based LMs (Charniak et al., 2003; Schwartz et al., 2011; Shen et al., 2008; Hassan et al., 2009), Forward & MI trigger LM (Xiong et al., 2011), and large-scale language models (Zhang et al., 2006; Brants et al., 2007; Emami et al., 2007; Talbot and Osborne, 2007). Since our philosophy is fundamentally different from them in that we incorporate information at document level to build language models. So we discuss previous works that explore topic information for SMT in this section. Researchers have been trying to incorporate topic information into language models in several ways. Gildea and Hofmann (1999) use EM algorithm to perform a topic factor decomposition based on a segmented training corpus. They estimate unigram topic-based probability and combine it with standard n-gram model. Tam et al. (2007) a"
I13-1051,J93-2003,0,0.0426971,"of the translated-document. As there is no explicit correspondence between topics on both sides, we project the source-side topic distribution to the target side as a trigger to our topic specific language models. As compared with previous works, our model takes advantage of the topical information on both sides, thus breaking down the context barrier for language model. Experimental results on various Chinese-English test sets show that our method gains an average improvement of +0.76 Bleu points and a perplexity reduction over Language model (LM) measures the fluency of translation outputs (Brown et al., 1993), and plays an important role in statistical machine translation (SMT). Traditional language model predicts the next word conditioning only on the preceding n−1 words, thus ignores syntactic structures in the sentence and global information over the document. One direct approach to handle this problem is to explore sentence-level context, such as syntaxbased language model for reranking (Charniak et al., 2003), and dependency language model for String-to-Dependency model (Shen et al., 2008). But these methods are still not robust enough to handle the polysemy and domain changes, as they lack t"
I13-1051,2003.mtsummit-papers.6,0,0.40867,"ese-English test sets show that our method gains an average improvement of +0.76 Bleu points and a perplexity reduction over Language model (LM) measures the fluency of translation outputs (Brown et al., 1993), and plays an important role in statistical machine translation (SMT). Traditional language model predicts the next word conditioning only on the preceding n−1 words, thus ignores syntactic structures in the sentence and global information over the document. One direct approach to handle this problem is to explore sentence-level context, such as syntaxbased language model for reranking (Charniak et al., 2003), and dependency language model for String-to-Dependency model (Shen et al., 2008). But these methods are still not robust enough to handle the polysemy and domain changes, as they lack the global-context information. Another interesting line is to utilize information at document-level. Intuitively, different domains or topics have different n-gram probability distributions. Thus, we should take into account the topic information when we translate a document. Topic model has been learned in several 447 International Joint Conference on Natural Language Processing, pages 447–454, Nagoya, Japan,"
I13-1051,J07-2003,0,0.329253,"n 1.5M sentence pair with 38M Chinese words and 32M English words. The monolingual data for training English language model includes the Xinhua portion of the GIGAWORD corpus, which contains 10M sentences. We used the NIST evaluation set of 2006(MT06) as our development set, and sets of MT04/05/08 as test sets. Corpus statistics are shown in Table 1. We obtain symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2004) in both directions and then applying refinement rule ”grow-diag-final-and” (Koehn et al., 2003). We re-implement the Hierarchical phrasebased system (Chiang, 2007) and extract SCFG (t f ,te ,a) (i, j)∈a where δ(x; y) is the Kronecker function, which is 1 if x = y and 0 otherwise. We then compute the 450 ppl 04 05 08 documents 980K Base LM 158.42 134.59 208.11 99.4K Topic LM 148.11 119.17 200.41 52 Table 3: 4-gram word perplexity results of our 200 method in terms of ppl, We compare our model 100 with baseline n-gram model (”Base LM”) on three 109 test-sets. Table 1: Training, tuning and test data used for evaluating Bleu score. follow the method by Tam et al. (2007), denote as ”Tam”, and generate topic-based marginals to rules from this word-aligned tra"
I13-1051,W07-0722,0,0.0240884,"o account the topic information when we translate a document. Topic model has been learned in several 447 International Joint Conference on Natural Language Processing, pages 447–454, Nagoya, Japan, 14-18 October 2013. so it is in the same spirit of mixture modeling. Heidel et al. (2007) use topic distribution to cluster the training corpora and train LMs accordingly. Our method is different from theirs in that we assign topic probabilities to training sentences rather than segment them into different topics, so our model is more robust to data sparse problem. Besides, Foster and Kuhn (2007), Civera and Juan (2007), L¨u et al. (2007) also adapt mixture modeling framework to exploit the full potential of existing corpus. Adopting this framework, the training corpus is first divided into different parts, each of which is used to train a sub model, then these sub models are used together with different weights during decoding. Those works typically use word similarities and sentence level information, while our work extents the context into the document level. the baseline model. 2 Related Work Previous works devoted to improving language models in SMT mostly focus on utilizing more contextual information,"
I13-1051,W07-0717,0,0.0196706,"Thus, we should take into account the topic information when we translate a document. Topic model has been learned in several 447 International Joint Conference on Natural Language Processing, pages 447–454, Nagoya, Japan, 14-18 October 2013. so it is in the same spirit of mixture modeling. Heidel et al. (2007) use topic distribution to cluster the training corpora and train LMs accordingly. Our method is different from theirs in that we assign topic probabilities to training sentences rather than segment them into different topics, so our model is more robust to data sparse problem. Besides, Foster and Kuhn (2007), Civera and Juan (2007), L¨u et al. (2007) also adapt mixture modeling framework to exploit the full potential of existing corpus. Adopting this framework, the training corpus is first divided into different parts, each of which is used to train a sub model, then these sub models are used together with different weights during decoding. Those works typically use word similarities and sentence level information, while our work extents the context into the document level. the baseline model. 2 Related Work Previous works devoted to improving language models in SMT mostly focus on utilizing more"
I13-1051,2011.mtsummit-papers.57,0,0.0369799,"across languages. Based on the BLSA, they are able to transfer source-side topic weights into target-side and use them to generate topic-based marginals to adapt ngram language model. Our model is different from theirs in that rather than using topic-based probabilities to adapt n-gram model, we directly calculate LM probability conditioned on topic distributions. There are also some valuable applications of topic model for machine translation. Zhao and Xing (2006) propose the Bilingual Topic Admixture Model (BiTAM) for word alignment and extract topic-dependent translation model accordingly. Gong et al. (2011) introduce topic model for filtering topic-mismatched phrase pairs. Su et al. (2012) use the topic distribution of in-domain monolingual corpus to adapt the translation model. Xiao et al. (2012) introduce a topic similarity model to select the synchronous rules for hierarchical phrase-based translation. Our work is in the same spirit with those works, but we are interested in LM problem rather than other parts in SMT. Our work models topic probabilities into training corpus and trains several topic-specific LMs, 3 Topic triggered Language Model Polysemy is a difficult problem for statistical m"
I13-1051,D09-1123,0,0.0304243,"Missing"
I13-1051,P12-1048,1,0.869348,"Missing"
I13-1051,P07-1066,0,0.358669,"But these methods are still not robust enough to handle the polysemy and domain changes, as they lack the global-context information. Another interesting line is to utilize information at document-level. Intuitively, different domains or topics have different n-gram probability distributions. Thus, we should take into account the topic information when we translate a document. Topic model has been learned in several 447 International Joint Conference on Natural Language Processing, pages 447–454, Nagoya, Japan, 14-18 October 2013. so it is in the same spirit of mixture modeling. Heidel et al. (2007) use topic distribution to cluster the training corpora and train LMs accordingly. Our method is different from theirs in that we assign topic probabilities to training sentences rather than segment them into different topics, so our model is more robust to data sparse problem. Besides, Foster and Kuhn (2007), Civera and Juan (2007), L¨u et al. (2007) also adapt mixture modeling framework to exploit the full potential of existing corpus. Adopting this framework, the training corpus is first divided into different parts, each of which is used to train a sub model, then these sub models are used"
I13-1051,P11-1021,0,0.0139907,"Chinese Academy of Sciences ∗ Software School of Xiamen University ‡ Centre for Next Generation Localisation Faculty of Engineering and Computing, Dublin City University {yuheng,lvyajuan,liuqun}@ict.ac.cn jssu@xmu.edu.cn qliu@computing.dcu.ie Abstract parts of SMT, such as word-alignment (Zhao and Xing, 2006; Zhao and Xing, 2007; Gong et al., 2011), translation model (Xiao et al., 2012). All these works show that a particular translation often appears in some specific topical context, so it is reasonable to enhance the prediction ability of language model by incorporating topical information. Tan et al. (2011) introduces a large scale distributed composite language model incorporating document-level information. But they only focus on the target side and explore in n-best reranking task which has a limited search space, while another promising application is taking account of topical information on both sides and integrate the LM into decoding to online select translation hypotheses. However, the integration is not easy. Since the test-document can be from any topic, it is hard to dynamically estimate language model probability according to various topic distributions. Language model is an essentia"
I13-1051,N03-1017,0,0.00644178,"ranslation tasks. The bilingual training data for translation model contain 1.5M sentence pair with 38M Chinese words and 32M English words. The monolingual data for training English language model includes the Xinhua portion of the GIGAWORD corpus, which contains 10M sentences. We used the NIST evaluation set of 2006(MT06) as our development set, and sets of MT04/05/08 as test sets. Corpus statistics are shown in Table 1. We obtain symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2004) in both directions and then applying refinement rule ”grow-diag-final-and” (Koehn et al., 2003). We re-implement the Hierarchical phrasebased system (Chiang, 2007) and extract SCFG (t f ,te ,a) (i, j)∈a where δ(x; y) is the Kronecker function, which is 1 if x = y and 0 otherwise. We then compute the 450 ppl 04 05 08 documents 980K Base LM 158.42 134.59 208.11 99.4K Topic LM 148.11 119.17 200.41 52 Table 3: 4-gram word perplexity results of our 200 method in terms of ppl, We compare our model 100 with baseline n-gram model (”Base LM”) on three 109 test-sets. Table 1: Training, tuning and test data used for evaluating Bleu score. follow the method by Tam et al. (2007), denote as ”Tam”, an"
I13-1051,W04-3250,0,0.341936,"M toolkit (Stolcke, 2002). Caseall test-sets. The baseline system achieves Bleu insensitive NIST BLEU (Papineni et al., 2002) is score of 37.43 on NIST04, 33.67 on NIST05 and used to measure translation performance. We use 28.54 on NIST08 set. Our method(STLM) gains minimum error rate training (Och, 2003) for optian average improvement of +0.76 Bleu and an avmizing the feature weights. erage reduction of −0.88 TER over the baseline. To obtain topic distribution, We use the open Results on NIST MT 04, 05, 08 are statistically source LDA tool Open HTMM developed by Grusignificant with p < 0.05 (Koehn, 2004). This verber et al. (2007) for estimation and inference. Durifies that our topic-triggered language model is a ing this process, we empirically set the paramegood complement for n-gram model to further imter values for HTMM training as: α = 1.5, β = prove translation quality. We can also see that our 1.01, iters = 100. See Gruber et al. (2007) for the method generally out-performs the Tam’s method, meanings of these parameters. and set the topic because our model can capture n-gram level topic number to 30 1 for both source and target side. The information, rather than only focus on estimatin"
I13-1051,D07-1036,1,0.870824,"Missing"
I13-1051,J04-4002,0,0.0168302,"than N-gram model. 4.1 Experiment setup We present our experiments on the NIST ChineseEnglish translation tasks. The bilingual training data for translation model contain 1.5M sentence pair with 38M Chinese words and 32M English words. The monolingual data for training English language model includes the Xinhua portion of the GIGAWORD corpus, which contains 10M sentences. We used the NIST evaluation set of 2006(MT06) as our development set, and sets of MT04/05/08 as test sets. Corpus statistics are shown in Table 1. We obtain symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2004) in both directions and then applying refinement rule ”grow-diag-final-and” (Koehn et al., 2003). We re-implement the Hierarchical phrasebased system (Chiang, 2007) and extract SCFG (t f ,te ,a) (i, j)∈a where δ(x; y) is the Kronecker function, which is 1 if x = y and 0 otherwise. We then compute the 450 ppl 04 05 08 documents 980K Base LM 158.42 134.59 208.11 99.4K Topic LM 148.11 119.17 200.41 52 Table 3: 4-gram word perplexity results of our 200 method in terms of ppl, We compare our model 100 with baseline n-gram model (”Base LM”) on three 109 test-sets. Table 1: Training, tuning and test"
I13-1051,P12-1079,1,0.887159,"is different from theirs in that rather than using topic-based probabilities to adapt n-gram model, we directly calculate LM probability conditioned on topic distributions. There are also some valuable applications of topic model for machine translation. Zhao and Xing (2006) propose the Bilingual Topic Admixture Model (BiTAM) for word alignment and extract topic-dependent translation model accordingly. Gong et al. (2011) introduce topic model for filtering topic-mismatched phrase pairs. Su et al. (2012) use the topic distribution of in-domain monolingual corpus to adapt the translation model. Xiao et al. (2012) introduce a topic similarity model to select the synchronous rules for hierarchical phrase-based translation. Our work is in the same spirit with those works, but we are interested in LM problem rather than other parts in SMT. Our work models topic probabilities into training corpus and trains several topic-specific LMs, 3 Topic triggered Language Model Polysemy is a difficult problem for statistical machine translation. As shown in Figure 1, English sentence ”give me a shot” has different meanings in different domains. Using traditional LM, which only considers the local context information"
I13-1051,P03-1021,0,0.00872365,"for evaluating Bleu score. follow the method by Tam et al. (2007), denote as ”Tam”, and generate topic-based marginals to rules from this word-aligned training data. A 4adapt n-gram language model. gram language model is trained on the monolinTable 2 reports the Bleu and TER scores on gual data by SRILM toolkit (Stolcke, 2002). Caseall test-sets. The baseline system achieves Bleu insensitive NIST BLEU (Papineni et al., 2002) is score of 37.43 on NIST04, 33.67 on NIST05 and used to measure translation performance. We use 28.54 on NIST08 set. Our method(STLM) gains minimum error rate training (Och, 2003) for optian average improvement of +0.76 Bleu and an avmizing the feature weights. erage reduction of −0.88 TER over the baseline. To obtain topic distribution, We use the open Results on NIST MT 04, 05, 08 are statistically source LDA tool Open HTMM developed by Grusignificant with p < 0.05 (Koehn, 2004). This verber et al. (2007) for estimation and inference. Durifies that our topic-triggered language model is a ing this process, we empirically set the paramegood complement for n-gram model to further imter values for HTMM training as: α = 1.5, β = prove translation quality. We can also see"
I13-1051,P02-1040,0,0.0873504,"m word perplexity results of our 200 method in terms of ppl, We compare our model 100 with baseline n-gram model (”Base LM”) on three 109 test-sets. Table 1: Training, tuning and test data used for evaluating Bleu score. follow the method by Tam et al. (2007), denote as ”Tam”, and generate topic-based marginals to rules from this word-aligned training data. A 4adapt n-gram language model. gram language model is trained on the monolinTable 2 reports the Bleu and TER scores on gual data by SRILM toolkit (Stolcke, 2002). Caseall test-sets. The baseline system achieves Bleu insensitive NIST BLEU (Papineni et al., 2002) is score of 37.43 on NIST04, 33.67 on NIST05 and used to measure translation performance. We use 28.54 on NIST08 set. Our method(STLM) gains minimum error rate training (Och, 2003) for optian average improvement of +0.76 Bleu and an avmizing the feature weights. erage reduction of −0.88 TER over the baseline. To obtain topic distribution, We use the open Results on NIST MT 04, 05, 08 are statistically source LDA tool Open HTMM developed by Grusignificant with p < 0.05 (Koehn, 2004). This verber et al. (2007) for estimation and inference. Durifies that our topic-triggered language model is a i"
I13-1051,P11-1129,0,0.0363899,"Missing"
I13-1051,W11-2133,0,0.0152618,"mami et al., 2007; Talbot and Osborne, 2007). Since our philosophy is fundamentally different from them in that we incorporate information at document level to build language models. So we discuss previous works that explore topic information for SMT in this section. Researchers have been trying to incorporate topic information into language models in several ways. Gildea and Hofmann (1999) use EM algorithm to perform a topic factor decomposition based on a segmented training corpus. They estimate unigram topic-based probability and combine it with standard n-gram model. Tam et al. (2007) and Ruiz and Federico (2011) introduce topic model for cross-lingual language model adaptation task. They use bilingual topic model to project latent topic distribution across languages. Based on the BLSA, they are able to transfer source-side topic weights into target-side and use them to generate topic-based marginals to adapt ngram language model. Our model is different from theirs in that rather than using topic-based probabilities to adapt n-gram model, we directly calculate LM probability conditioned on topic distributions. There are also some valuable applications of topic model for machine translation. Zhao and X"
I13-1051,W06-1626,0,0.0203118,"ach of which is used to train a sub model, then these sub models are used together with different weights during decoding. Those works typically use word similarities and sentence level information, while our work extents the context into the document level. the baseline model. 2 Related Work Previous works devoted to improving language models in SMT mostly focus on utilizing more contextual information, such as syntax-based LMs (Charniak et al., 2003; Schwartz et al., 2011; Shen et al., 2008; Hassan et al., 2009), Forward & MI trigger LM (Xiong et al., 2011), and large-scale language models (Zhang et al., 2006; Brants et al., 2007; Emami et al., 2007; Talbot and Osborne, 2007). Since our philosophy is fundamentally different from them in that we incorporate information at document level to build language models. So we discuss previous works that explore topic information for SMT in this section. Researchers have been trying to incorporate topic information into language models in several ways. Gildea and Hofmann (1999) use EM algorithm to perform a topic factor decomposition based on a segmented training corpus. They estimate unigram topic-based probability and combine it with standard n-gram model"
I13-1051,P11-1063,0,0.0213179,"amework to exploit the full potential of existing corpus. Adopting this framework, the training corpus is first divided into different parts, each of which is used to train a sub model, then these sub models are used together with different weights during decoding. Those works typically use word similarities and sentence level information, while our work extents the context into the document level. the baseline model. 2 Related Work Previous works devoted to improving language models in SMT mostly focus on utilizing more contextual information, such as syntax-based LMs (Charniak et al., 2003; Schwartz et al., 2011; Shen et al., 2008; Hassan et al., 2009), Forward & MI trigger LM (Xiong et al., 2011), and large-scale language models (Zhang et al., 2006; Brants et al., 2007; Emami et al., 2007; Talbot and Osborne, 2007). Since our philosophy is fundamentally different from them in that we incorporate information at document level to build language models. So we discuss previous works that explore topic information for SMT in this section. Researchers have been trying to incorporate topic information into language models in several ways. Gildea and Hofmann (1999) use EM algorithm to perform a topic factor"
I13-1051,P06-2124,0,0.0223876,"ico (2011) introduce topic model for cross-lingual language model adaptation task. They use bilingual topic model to project latent topic distribution across languages. Based on the BLSA, they are able to transfer source-side topic weights into target-side and use them to generate topic-based marginals to adapt ngram language model. Our model is different from theirs in that rather than using topic-based probabilities to adapt n-gram model, we directly calculate LM probability conditioned on topic distributions. There are also some valuable applications of topic model for machine translation. Zhao and Xing (2006) propose the Bilingual Topic Admixture Model (BiTAM) for word alignment and extract topic-dependent translation model accordingly. Gong et al. (2011) introduce topic model for filtering topic-mismatched phrase pairs. Su et al. (2012) use the topic distribution of in-domain monolingual corpus to adapt the translation model. Xiao et al. (2012) introduce a topic similarity model to select the synchronous rules for hierarchical phrase-based translation. Our work is in the same spirit with those works, but we are interested in LM problem rather than other parts in SMT. Our work models topic probabi"
I13-1051,P08-1066,0,0.124574,"points and a perplexity reduction over Language model (LM) measures the fluency of translation outputs (Brown et al., 1993), and plays an important role in statistical machine translation (SMT). Traditional language model predicts the next word conditioning only on the preceding n−1 words, thus ignores syntactic structures in the sentence and global information over the document. One direct approach to handle this problem is to explore sentence-level context, such as syntaxbased language model for reranking (Charniak et al., 2003), and dependency language model for String-to-Dependency model (Shen et al., 2008). But these methods are still not robust enough to handle the polysemy and domain changes, as they lack the global-context information. Another interesting line is to utilize information at document-level. Intuitively, different domains or topics have different n-gram probability distributions. Thus, we should take into account the topic information when we translate a document. Topic model has been learned in several 447 International Joint Conference on Natural Language Processing, pages 447–454, Nagoya, Japan, 14-18 October 2013. so it is in the same spirit of mixture modeling. Heidel et al"
I13-1051,P07-1065,0,\N,Missing
I17-3009,P07-2045,0,0.0193009,"Missing"
I17-3009,P03-1021,0,0.0209504,"SLT DIALOG corpus,we select 1,023 and 1053 hotel booking sentences (34/36 dialogues) as development set and test set, respectively. We combine our home-made travel domain corpora as in-domain training data (180K). We also use domain adaptation techniques to select in-domain data from movie subtitles (Wang et al., 2016b). We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on Chinese (ZH)–English (EN). Furthermore, we train a 5-gram language model using the SRI Language Toolkit (Stolcke, 2002). We run GIZA++ (Och and Ney, 2003) for alignment and use MERT (Och, 2003) to optimize the feature weights. We develop TODAY on the basis of an open-source live support application Mibew4 by integrating our semantics-enhanced SMT system and the semantic form filling. 3.2 4 +6.5 +6.9 Table 2: Performance with task-oriented NE recognition. Experiments and Analysis 3.1 EN-ZH 24.20 30.70 31.10 Evaluation of Task-Oriented Named Entity and Translation We manually annotated Chinese and English sentences in the test set to evaluate the proposed taskoriented NE recognition and translation in terms of accuracy, recall and F1. In Table 4, Recog indicates NE recognition on the"
I17-3009,J03-1002,0,0.0059894,"20.20 Table 3: Overall performance. From the IWSLT DIALOG corpus,we select 1,023 and 1053 hotel booking sentences (34/36 dialogues) as development set and test set, respectively. We combine our home-made travel domain corpora as in-domain training data (180K). We also use domain adaptation techniques to select in-domain data from movie subtitles (Wang et al., 2016b). We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on Chinese (ZH)–English (EN). Furthermore, we train a 5-gram language model using the SRI Language Toolkit (Stolcke, 2002). We run GIZA++ (Och and Ney, 2003) for alignment and use MERT (Och, 2003) to optimize the feature weights. We develop TODAY on the basis of an open-source live support application Mibew4 by integrating our semantics-enhanced SMT system and the semantic form filling. 3.2 4 +6.5 +6.9 Table 2: Performance with task-oriented NE recognition. Experiments and Analysis 3.1 EN-ZH 24.20 30.70 31.10 Evaluation of Task-Oriented Named Entity and Translation We manually annotated Chinese and English sentences in the test set to evaluate the proposed taskoriented NE recognition and translation in terms of accuracy, recall and F1. In Table 4,"
I17-3009,D17-1301,1,0.67952,"Missing"
I17-3009,N16-1113,1,0.838057,"he selected pseudo in-domain data can improve the performance by at most +1.09 and +1.24 on EN-ZH (top-50K) and ZH-EN (top50K), respectively. However, bring more pseudo in-domain data (&gt; top − 250K), the performance drops sharply. 4 ZH-EN 17.90 20.30 20.20 Table 3: Overall performance. From the IWSLT DIALOG corpus,we select 1,023 and 1053 hotel booking sentences (34/36 dialogues) as development set and test set, respectively. We combine our home-made travel domain corpora as in-domain training data (180K). We also use domain adaptation techniques to select in-domain data from movie subtitles (Wang et al., 2016b). We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on Chinese (ZH)–English (EN). Furthermore, we train a 5-gram language model using the SRI Language Toolkit (Stolcke, 2002). We run GIZA++ (Och and Ney, 2003) for alignment and use MERT (Och, 2003) to optimize the feature weights. We develop TODAY on the basis of an open-source live support application Mibew4 by integrating our semantics-enhanced SMT system and the semantic form filling. 3.2 4 +6.5 +6.9 Table 2: Performance with task-oriented NE recognition. Experiments and Analysis 3.1 EN-ZH 24.20 30"
I17-3009,L16-1436,1,0.749687,"he selected pseudo in-domain data can improve the performance by at most +1.09 and +1.24 on EN-ZH (top-50K) and ZH-EN (top50K), respectively. However, bring more pseudo in-domain data (&gt; top − 250K), the performance drops sharply. 4 ZH-EN 17.90 20.30 20.20 Table 3: Overall performance. From the IWSLT DIALOG corpus,we select 1,023 and 1053 hotel booking sentences (34/36 dialogues) as development set and test set, respectively. We combine our home-made travel domain corpora as in-domain training data (180K). We also use domain adaptation techniques to select in-domain data from movie subtitles (Wang et al., 2016b). We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on Chinese (ZH)–English (EN). Furthermore, we train a 5-gram language model using the SRI Language Toolkit (Stolcke, 2002). We run GIZA++ (Och and Ney, 2003) for alignment and use MERT (Och, 2003) to optimize the feature weights. We develop TODAY on the basis of an open-source live support application Mibew4 by integrating our semantics-enhanced SMT system and the semantic form filling. 3.2 4 +6.5 +6.9 Table 2: Performance with task-oriented NE recognition. Experiments and Analysis 3.1 EN-ZH 24.20 30"
J10-3002,P06-1002,0,0.0788809,"Missing"
J10-3002,N06-1013,0,0.0974408,"Missing"
J10-3002,H05-1024,0,0.0373228,"Missing"
J10-3002,H05-1009,0,0.356028,"Missing"
J10-3002,P06-1009,0,0.0816815,"Missing"
J10-3002,J93-2003,0,0.141447,"Missing"
J10-3002,P03-1012,0,0.104375,"Missing"
J10-3002,P06-2014,0,0.0374391,"Missing"
J10-3002,P05-1033,0,0.227161,"Missing"
J10-3002,J07-2003,0,0.219245,"Missing"
J10-3002,E09-1020,0,0.0767397,"Missing"
J10-3002,P06-1097,0,0.0306278,"Missing"
J10-3002,D07-1006,0,0.0780369,"Missing"
J10-3002,J07-3002,0,0.288262,"Missing"
J10-3002,P06-1121,0,0.146113,"Missing"
J10-3002,D08-1011,0,0.0449614,"Missing"
J10-3002,D07-1091,0,0.0407599,"Missing"
J10-3002,N03-1017,0,0.0433454,"Missing"
J10-3002,N06-1015,0,0.0264963,"Missing"
J10-3002,N06-1014,0,0.0937511,"Missing"
J10-3002,P05-1057,1,0.623793,"Missing"
J10-3002,P06-1077,1,0.887335,"Missing"
J10-3002,W06-1606,0,0.0510981,"Missing"
J10-3002,W05-0809,0,0.0666533,"Missing"
J10-3002,J00-2004,0,0.116772,"Missing"
J10-3002,W03-0301,0,0.0158309,"Missing"
J10-3002,H05-1011,0,0.0700406,"Missing"
J10-3002,P06-1065,0,0.333202,"Missing"
J10-3002,W08-0303,0,0.0323884,"Missing"
J10-3002,P03-1021,0,0.0209775,"Missing"
J10-3002,P02-1038,0,0.204882,"Missing"
J10-3002,J03-1002,0,0.0439931,"Missing"
J10-3002,J04-4002,0,0.376267,"Missing"
J10-3002,P05-1034,0,0.0691884,"Missing"
J10-3002,H05-1010,0,0.110155,"Missing"
J10-3002,C96-2141,0,0.848696,"Missing"
J10-3002,P07-1040,0,\N,Missing
J10-3002,P09-1104,0,\N,Missing
J15-1005,W06-1615,0,0.233269,"Missing"
J15-1005,J95-4004,0,0.372531,"point of view, the practical value of the current work on bilingual projection and unsupervised induction may be underestimated, and annotation adaptation could make better use of the projected or induced knowledge.3 7. Related Work There has already been some preliminary work tackling the divergence between different annotation guidelines. Gao et al. (2004) described a transformation-based converter to transfer a certain word segmentation result to another annotation guideline. They designed class-type transformation templates and used the transformation-based error-driven learning method of Brill (1995) to learn what word delimiters should be modified. Many efforts have been devoted to manual treebank transformation, where PTB is adapted to other grammar formalisms, such as CCG and LFG (Cahill et al. 2002; Hockenmaier and Steedman 2007). However, all these are heuristic-based—that is, they need manually designed transformation templates and involve heavy human engineering. Such strategies are hard to be generalized to POS tagging, not to mention other complicated structural prediction tasks. We investigated the automatic integration of word segmentation knowledge in differently annotated cor"
J15-1005,W06-2920,0,0.037011,"expensive to build, especially on a large scale. The creation of treebanks is a prime example (Marcus, Santorini, and Marcinkiewicz 1993). However, the linguistic theories motivating these annotation efforts are often heavily debated, and as a result there often exist multiple corpora for the same task with vastly different and incompatible annotation philosophies. For example, there are several treebanks for English, including the Chomskian-style Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), the HPSG LinGo Redwoods Treebank (Oepen et al. 2002), and a smaller dependency treebank (Buchholz and Marsi 2006). From the perspective of resource accumulation, it seems a waste in human efforts.1 A second, related problem is that the raw texts are also drawn from different domains, which for the above example range from financial news (Penn Treebank/Wall Street Journal) to transcribed dialog (LinGo). It would be nice if a system could be automatically ported from one set of guidelines and/or domain to another, in order to exploit a much larger data set. The second problem, domain adaptation, is very well studied (e.g., Blitzer, McDonald, & Pereira 2006; Daum´e III 2007). This work focuses on the widely"
J15-1005,S12-1050,0,0.314988,"sident into two words and combines the phrase visited-China as a compound, compared with the segmentation following the CTB annotation guideline. It is preferable to transfer knowledge from PD to CTB because the latter also annotates tree structures, which are useful for downstream applications like parsing, summarization, and machine translation, yet it is much smaller in size. For dependency parsing, we use the dependency treebank (DCTB) extracted from CTB according to the rules of Yamada and Matsumoto (2003), and the Semantic Dependency Treebank (SDT) built on a small part of the CTB text (Che et al. 2012). Compared with the automatically extracted dependencies in DCTB, semantic dependencies in SDT reveal semantic relationships between words, rather than the syntactic relationships in syntactic dependencies. Figure 2 shows an example. Experiments on both word segmentation and dependency parsing show that annotation adaptation results in significant improvement over the baselines, and achieves the state-of-the-art with only local features. The rest of the article is organized as follows. Section 2 gives a description of the problem of annotation adaptation. Section 3 briefly introduces the tasks"
J15-1005,W02-1001,0,0.184222,"ion: y˜ = argmax f(x, y) · w y = argmax y X f(xi , yi ) · w (1) xi ∈x,yi ∈y Where function f maps (x, y) into a feature vector, w is the parameter vector generated by the training algorithm, and f(x, y) · w is the inner product of f(x, y) and w. The score of the sentence is further factorized into each character, where yi is the character classification label of character xi . The training procedure of perceptron learns a discriminative model mapping from the inputs x to the outputs y. Algorithm 1 shows the perceptron algorithm for tuning the parameter w. The “averaged parameters” technology (Collins 2002) is used for better performance. The feature templates of the classifier is shown in Table 1. The function Pu(· ) returns true for a punctuation character and false for others; the function T(· ) classifies a character into four types: number, date, English letter, and others, corresponding to function values 1, 2, 3, and 4, respectively. 3.2 Dependency Parsing and Spanning Tree Method Dependency parsing aims to link each word to its arguments so as to form a directed graph spanning the whole sentence. Normally, the directed graph is restricted to a 123 Computational Linguistics Volume 41, Num"
J15-1005,P11-1061,0,0.0168485,"h FBIS as the bilingual corpus. By annotation adaptation, the projected corpus for word segmentation brings a significant F-measure increment of nearly 0.6 points over the baseline trained on CTB only. 143 Computational Linguistics Volume 41, Number 1 The training procedure for an annotation adaptation model requires a parallel annotated corpus (which may be automatically generated); this fact puts the method into the neighborhood of the family of approaches known as annotation projection (Hwa et al. 2002, 2005; Ganchev, Gillenwater, and Taskar 2009; Smith and Eisner 2009; Jiang and Liu 2010; Das and Petrov 2011). Essentially, annotation adaptation and annotation projection tackle different problems; the former aims to transform the annotations from one guideline to another (of course in the same language), whereas the latter aims to project the annotation (as well as the annotation guideline) from one language to another. Therefore, the machine learning methods for annotation adaptation pay attention to automatic transformation of annotations, while for annotation projection, the machine learning methods focus on the bilingual projection across languages. Co-training (Sarkar 2001) and classifier comb"
J15-1005,P07-1033,0,0.456207,"Missing"
J15-1005,C96-1058,0,0.160427,"root r ∈ x in the tree y has no head word, and each of the other words, j(j ∈ x and j 6= r), depends on a head word i(i ∈ x and i 6= j). For many languages, the dependency structures are supposed to be projective. If xj is dependent on xi , then all the words between i and j must be directly or indirectly dependent on xi . Therefore, if we put the words in their linear order, preceded by the root, all edges can be drawn above the words without crossing. We follow this constraint because the dependency treebanks in our experiments are projective. Following the edge-based factorization method (Eisner 1996), the score of a dependency tree can be factorized into the dependency edges in the tree. The spanning Table 1 Feature templates and instances for character classification-based word segmentation model. C0 denotes the current character, and C−i /Ci denote the ith character to the left/right of C0 . Suppose we are considering the third character “总” in “美-副-总-统-访-华”. Type Templates Instances n-gram C−2 C−1 C0 C1 C2 C−2 C−1 C−1 C0 C0 C1 C1 C2 C−1 C1 C−2 =美 C−1 =副 C0 =总 C1 =统 C2 =访 C−2 C−1 =美副 C−1 C0 =副总 C0 C1 =总统 C1 C2 =统访 C−1 C1 =副统 function Pu(C0 ) T(C−2:2 ) Pu(C0 )=true T(C−2:2 )= 44444 124 J"
J15-1005,P09-1042,0,0.0605038,"Missing"
J15-1005,P11-2095,0,0.0204332,"ation to improve transformation accuracy. In recent years much effort has been devoted to the improvement of word segmentation and dependency parsing. For example, the introduction of global training or complicated features (Zhang and Clark 2007, 2010); the investigation of word structures (Li 2011); the strategies of hybrid, joint, or stacked modeling (Nakagawa and Uchimoto 2007; Kruengkrai et al. 2009; Wang, Zong, and Su 2010; Sun 2011); and the semi-supervised and unsupervised technologies utilizing raw text (Zhao and Kit 2008; Johnson and Goldwater 2009; Mochihashi, Yamada, and Ueda 2009; Hewlett and Cohen 2011). We believe that the annotation adaptation technologies can be adopted jointly with complicated features, system combination, and semi-supervised/unsupervised technologies to further improve the performance of word segmentation and dependency parsing. 8. Conclusion and Future Work We have described the problem of annotation adaptation and the intrinsic principles of its solutions, and proposed a series of successively enhanced models that can automatically adapt the divergence between different annotation formats. These models learn the statistical regularities of adaptation between different"
J15-1005,J07-3004,0,0.0188995,"Related Work There has already been some preliminary work tackling the divergence between different annotation guidelines. Gao et al. (2004) described a transformation-based converter to transfer a certain word segmentation result to another annotation guideline. They designed class-type transformation templates and used the transformation-based error-driven learning method of Brill (1995) to learn what word delimiters should be modified. Many efforts have been devoted to manual treebank transformation, where PTB is adapted to other grammar formalisms, such as CCG and LFG (Cahill et al. 2002; Hockenmaier and Steedman 2007). However, all these are heuristic-based—that is, they need manually designed transformation templates and involve heavy human engineering. Such strategies are hard to be generalized to POS tagging, not to mention other complicated structural prediction tasks. We investigated the automatic integration of word segmentation knowledge in differently annotated corpora (Jiang, Huang, and Liu 2009; Jiang et al. 2012), which can be seen as the preliminary work of automatic annotation adaptation. Motivated by our initial investigation, researchers applied similar methodologies to constituency parsing"
J15-1005,P02-1050,0,0.0651491,"on word segmentation. Bilingual projection was conducted from English to Chinese with the Chinese–English FBIS as the bilingual corpus. By annotation adaptation, the projected corpus for word segmentation brings a significant F-measure increment of nearly 0.6 points over the baseline trained on CTB only. 143 Computational Linguistics Volume 41, Number 1 The training procedure for an annotation adaptation model requires a parallel annotated corpus (which may be automatically generated); this fact puts the method into the neighborhood of the family of approaches known as annotation projection (Hwa et al. 2002, 2005; Ganchev, Gillenwater, and Taskar 2009; Smith and Eisner 2009; Jiang and Liu 2010; Das and Petrov 2011). Essentially, annotation adaptation and annotation projection tackle different problems; the former aims to transform the annotations from one guideline to another (of course in the same language), whereas the latter aims to project the annotation (as well as the annotation guideline) from one language to another. Therefore, the machine learning methods for annotation adaptation pay attention to automatic transformation of annotations, while for annotation projection, the machine lear"
J15-1005,P09-1059,1,0.817336,"Missing"
J15-1005,P08-1102,1,0.875465,"Missing"
J15-1005,P10-1002,1,0.821867,"h the Chinese–English FBIS as the bilingual corpus. By annotation adaptation, the projected corpus for word segmentation brings a significant F-measure increment of nearly 0.6 points over the baseline trained on CTB only. 143 Computational Linguistics Volume 41, Number 1 The training procedure for an annotation adaptation model requires a parallel annotated corpus (which may be automatically generated); this fact puts the method into the neighborhood of the family of approaches known as annotation projection (Hwa et al. 2002, 2005; Ganchev, Gillenwater, and Taskar 2009; Smith and Eisner 2009; Jiang and Liu 2010; Das and Petrov 2011). Essentially, annotation adaptation and annotation projection tackle different problems; the former aims to transform the annotations from one guideline to another (of course in the same language), whereas the latter aims to project the annotation (as well as the annotation guideline) from one language to another. Therefore, the machine learning methods for annotation adaptation pay attention to automatic transformation of annotations, while for annotation projection, the machine learning methods focus on the bilingual projection across languages. Co-training (Sarkar 200"
J15-1005,D12-1038,1,0.621231,"Missing"
J15-1005,N09-1036,0,0.0347124,"I 2009). We adapt this idea to the scenario of annotation adaptation to improve transformation accuracy. In recent years much effort has been devoted to the improvement of word segmentation and dependency parsing. For example, the introduction of global training or complicated features (Zhang and Clark 2007, 2010); the investigation of word structures (Li 2011); the strategies of hybrid, joint, or stacked modeling (Nakagawa and Uchimoto 2007; Kruengkrai et al. 2009; Wang, Zong, and Su 2010; Sun 2011); and the semi-supervised and unsupervised technologies utilizing raw text (Zhao and Kit 2008; Johnson and Goldwater 2009; Mochihashi, Yamada, and Ueda 2009; Hewlett and Cohen 2011). We believe that the annotation adaptation technologies can be adopted jointly with complicated features, system combination, and semi-supervised/unsupervised technologies to further improve the performance of word segmentation and dependency parsing. 8. Conclusion and Future Work We have described the problem of annotation adaptation and the intrinsic principles of its solutions, and proposed a series of successively enhanced models that can automatically adapt the divergence between different annotation formats. These models learn"
J15-1005,P09-1058,0,0.0211221,"redict-self methodology is implicit in many unsupervised learning approaches; it has been successfully used in unsupervised dependency parsing (Daum´e III 2009). We adapt this idea to the scenario of annotation adaptation to improve transformation accuracy. In recent years much effort has been devoted to the improvement of word segmentation and dependency parsing. For example, the introduction of global training or complicated features (Zhang and Clark 2007, 2010); the investigation of word structures (Li 2011); the strategies of hybrid, joint, or stacked modeling (Nakagawa and Uchimoto 2007; Kruengkrai et al. 2009; Wang, Zong, and Su 2010; Sun 2011); and the semi-supervised and unsupervised technologies utilizing raw text (Zhao and Kit 2008; Johnson and Goldwater 2009; Mochihashi, Yamada, and Ueda 2009; Hewlett and Cohen 2011). We believe that the annotation adaptation technologies can be adopted jointly with complicated features, system combination, and semi-supervised/unsupervised technologies to further improve the performance of word segmentation and dependency parsing. 8. Conclusion and Future Work We have described the problem of annotation adaptation and the intrinsic principles of its solutions"
J15-1005,P11-1141,0,0.0132605,"optimization of the parallel annotated corpora used to train the transfer classifiers. The predict-self methodology is implicit in many unsupervised learning approaches; it has been successfully used in unsupervised dependency parsing (Daum´e III 2009). We adapt this idea to the scenario of annotation adaptation to improve transformation accuracy. In recent years much effort has been devoted to the improvement of word segmentation and dependency parsing. For example, the introduction of global training or complicated features (Zhang and Clark 2007, 2010); the investigation of word structures (Li 2011); the strategies of hybrid, joint, or stacked modeling (Nakagawa and Uchimoto 2007; Kruengkrai et al. 2009; Wang, Zong, and Su 2010; Sun 2011); and the semi-supervised and unsupervised technologies utilizing raw text (Zhao and Kit 2008; Johnson and Goldwater 2009; Mochihashi, Yamada, and Ueda 2009; Hewlett and Cohen 2011). We believe that the annotation adaptation technologies can be adopted jointly with complicated features, system combination, and semi-supervised/unsupervised technologies to further improve the performance of word segmentation and dependency parsing. 8. Conclusion and Future"
J15-1005,J93-2004,0,0.0517662,"Missing"
J15-1005,D08-1017,0,0.0206003,"est training iterations for the source classifier and the transfer classifier are determined on the development sets of the source corpus and target corpus. In the decoding procedure, a sequence of characters (for word segmentation) or words (for dependency parsing) is input into the source classifier to obtain a classification result under the source guideline; then it is input into the transfer classifier with this classification result as the guiding information to get the final result following the target guideline. This coincides with the stacking method for combining dependency parsers (Martins et al. 2008; Nivre and McDonald 2008), and is also similar to the Pred baseline for domain adaptation in Daum´e et al. (Daum´e III and Marcu 2006; Daum´e III 2007). Figure 5 shows the pipeline for decoding. raw sentence source classifier result with source guideline transfer classifier result with target guideline Figure 5 The pipeline for decoding of Model 1. 129 Computational Linguistics Volume 41, Number 1 4.3 Model 2 The previous model has a drawback: It has to cascade two classifiers in decoding to integrate the knowledge in two corpora, which seriously degrades the processing speed. Here we describ"
J15-1005,P05-1012,0,0.160861,"Missing"
J15-1005,E06-1011,0,0.0205287,"dditional guiding features. For word segmentation, the most intuitive guiding feature is the source annotation label itself. For dependency parsing, an effective guiding feature is the dependency path between the hypothetic head and modifier, as shown in Figure 3. However, our effort is not limited to this, and more special features are introduced: A classification label or dependency path is attached to each feature of the baseline classifier to generate combined guiding features. This is similar to the feature design in discriminative dependency parsing (McDonald, Crammer, and Pereira 2005; McDonald and Pereira 2006), where the basic features, composed of words and POSs in the context, are also conjoined with link direction and distance in order to generate more special features. Table 3 shows an example of guide features (as well as baseline features) for word segmentation, where “α = B” indicates that the source classification label of the current character is B, demarcating the beginning of a word. The combination strategy derives a series of specific features, helping the transfer classifier to produce more precise classifications. The parameter-tuning procedure of the transfer classifier will automat"
J15-1005,P09-1012,0,0.073578,"Missing"
J15-1005,P07-2055,0,0.0227552,"transfer classifiers. The predict-self methodology is implicit in many unsupervised learning approaches; it has been successfully used in unsupervised dependency parsing (Daum´e III 2009). We adapt this idea to the scenario of annotation adaptation to improve transformation accuracy. In recent years much effort has been devoted to the improvement of word segmentation and dependency parsing. For example, the introduction of global training or complicated features (Zhang and Clark 2007, 2010); the investigation of word structures (Li 2011); the strategies of hybrid, joint, or stacked modeling (Nakagawa and Uchimoto 2007; Kruengkrai et al. 2009; Wang, Zong, and Su 2010; Sun 2011); and the semi-supervised and unsupervised technologies utilizing raw text (Zhao and Kit 2008; Johnson and Goldwater 2009; Mochihashi, Yamada, and Ueda 2009; Hewlett and Cohen 2011). We believe that the annotation adaptation technologies can be adopted jointly with complicated features, system combination, and semi-supervised/unsupervised technologies to further improve the performance of word segmentation and dependency parsing. 8. Conclusion and Future Work We have described the problem of annotation adaptation and the intrinsic pri"
J15-1005,W04-3236,0,0.0280485,"no explicit word boundaries, thus word segmentation is a fundamental task for the processing and understanding of these languages. Given a sentence as a sequence of n characters: x = x1 x2 .. xn where xi is a character, word segmentation aims to split the sequence into m(≤ n) words: x1:e1 xe1 +1:e2 .. xem−1+1:em where each subsequence xi:j indicates a Chinese word spanning from characters xi to xj . Word segmentation can be formalized as a sequence labeling problem (Xue and Shen 2003), where each character in the sentence is given a boundary tag representing its position in a word. Following Ng and Low (2004), joint word segmentation and partof-speech (POS) tagging can also be solved using a character classification approach by extending boundary tags to include POS information. For word segmentation we adopt the four boundary tags of Ng and Low (2004), B, M, E, and S, where B, M, and E mean the beginning, the middle, and the end of a word, respectively, and S indicates a singlecharacter word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern S or BM∗ E, indicating single-character words or multi-character words, respectively. Gi"
J15-1005,P08-1108,0,0.60101,"ns for the source classifier and the transfer classifier are determined on the development sets of the source corpus and target corpus. In the decoding procedure, a sequence of characters (for word segmentation) or words (for dependency parsing) is input into the source classifier to obtain a classification result under the source guideline; then it is input into the transfer classifier with this classification result as the guiding information to get the final result following the target guideline. This coincides with the stacking method for combining dependency parsers (Martins et al. 2008; Nivre and McDonald 2008), and is also similar to the Pred baseline for domain adaptation in Daum´e et al. (Daum´e III and Marcu 2006; Daum´e III 2007). Figure 5 shows the pipeline for decoding. raw sentence source classifier result with source guideline transfer classifier result with target guideline Figure 5 The pipeline for decoding of Model 1. 129 Computational Linguistics Volume 41, Number 1 4.3 Model 2 The previous model has a drawback: It has to cascade two classifiers in decoding to integrate the knowledge in two corpora, which seriously degrades the processing speed. Here we describe a variant of the previou"
J15-1005,C02-2025,0,0.0192385,"to train models, but annotated resources are extremely expensive to build, especially on a large scale. The creation of treebanks is a prime example (Marcus, Santorini, and Marcinkiewicz 1993). However, the linguistic theories motivating these annotation efforts are often heavily debated, and as a result there often exist multiple corpora for the same task with vastly different and incompatible annotation philosophies. For example, there are several treebanks for English, including the Chomskian-style Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), the HPSG LinGo Redwoods Treebank (Oepen et al. 2002), and a smaller dependency treebank (Buchholz and Marsi 2006). From the perspective of resource accumulation, it seems a waste in human efforts.1 A second, related problem is that the raw texts are also drawn from different domains, which for the above example range from financial news (Penn Treebank/Wall Street Journal) to transcribed dialog (LinGo). It would be nice if a system could be automatically ported from one set of guidelines and/or domain to another, in order to exploit a much larger data set. The second problem, domain adaptation, is very well studied (e.g., Blitzer, McDonald, & Pe"
J15-1005,N01-1023,0,0.0589285,"d Liu 2010; Das and Petrov 2011). Essentially, annotation adaptation and annotation projection tackle different problems; the former aims to transform the annotations from one guideline to another (of course in the same language), whereas the latter aims to project the annotation (as well as the annotation guideline) from one language to another. Therefore, the machine learning methods for annotation adaptation pay attention to automatic transformation of annotations, while for annotation projection, the machine learning methods focus on the bilingual projection across languages. Co-training (Sarkar 2001) and classifier combination (Nivre and McDonald 2008) are two techniques for training improved dependency parsers. The co-training technique lets two different parsing models learn from each other during the parsing of unlabeled text: One model selects some unlabeled sentences it can confidently parse, and provides them to the other model as additional training data in order to train more powerful parsers. The classifier combination lets graph-based and transition-based dependency parsers utilize the features extracted from each other’s parsing results, to obtain combined, enhanced parsers. Th"
J15-1005,D09-1086,0,0.0250397,"English to Chinese with the Chinese–English FBIS as the bilingual corpus. By annotation adaptation, the projected corpus for word segmentation brings a significant F-measure increment of nearly 0.6 points over the baseline trained on CTB only. 143 Computational Linguistics Volume 41, Number 1 The training procedure for an annotation adaptation model requires a parallel annotated corpus (which may be automatically generated); this fact puts the method into the neighborhood of the family of approaches known as annotation projection (Hwa et al. 2002, 2005; Ganchev, Gillenwater, and Taskar 2009; Smith and Eisner 2009; Jiang and Liu 2010; Das and Petrov 2011). Essentially, annotation adaptation and annotation projection tackle different problems; the former aims to transform the annotations from one guideline to another (of course in the same language), whereas the latter aims to project the annotation (as well as the annotation guideline) from one language to another. Therefore, the machine learning methods for annotation adaptation pay attention to automatic transformation of annotations, while for annotation projection, the machine learning methods focus on the bilingual projection across languages. Co-"
J15-1005,P11-1139,0,0.0142567,"ervised learning approaches; it has been successfully used in unsupervised dependency parsing (Daum´e III 2009). We adapt this idea to the scenario of annotation adaptation to improve transformation accuracy. In recent years much effort has been devoted to the improvement of word segmentation and dependency parsing. For example, the introduction of global training or complicated features (Zhang and Clark 2007, 2010); the investigation of word structures (Li 2011); the strategies of hybrid, joint, or stacked modeling (Nakagawa and Uchimoto 2007; Kruengkrai et al. 2009; Wang, Zong, and Su 2010; Sun 2011); and the semi-supervised and unsupervised technologies utilizing raw text (Zhao and Kit 2008; Johnson and Goldwater 2009; Mochihashi, Yamada, and Ueda 2009; Hewlett and Cohen 2011). We believe that the annotation adaptation technologies can be adopted jointly with complicated features, system combination, and semi-supervised/unsupervised technologies to further improve the performance of word segmentation and dependency parsing. 8. Conclusion and Future Work We have described the problem of annotation adaptation and the intrinsic principles of its solutions, and proposed a series of successiv"
J15-1005,P12-1025,0,0.2503,"ned transformation templates and involve heavy human engineering. Such strategies are hard to be generalized to POS tagging, not to mention other complicated structural prediction tasks. We investigated the automatic integration of word segmentation knowledge in differently annotated corpora (Jiang, Huang, and Liu 2009; Jiang et al. 2012), which can be seen as the preliminary work of automatic annotation adaptation. Motivated by our initial investigation, researchers applied similar methodologies to constituency parsing (Sun, Wang, and Zhang 2010; Zhu, Zhu, and Hu 2011) and word segmentation (Sun and Wan 2012). This previous work verified the effectiveness of automatic annotation adaptation, but did not reveal the essential definition of the problem nor the intrinsic principles of the solutions. Instead, this work clearly defines the problem of annotation adaptation, reveals the intrinsic principles of the solutions, and systematically describes a series of gradually improved models. The most advanced model learns transformation regularities much better and achieves significant higher accuracy for both word segmentation and dependency parsing, without slowing down the final language processors. The"
J15-1005,W10-4144,0,0.0623024,"Missing"
J15-1005,C10-1132,0,0.0500624,"Missing"
J15-1005,W03-1728,0,0.0305531,"Segmentation and Dependency Parsing 3.1 Word Segmentation and Character Classification Method In many Asian languages there are no explicit word boundaries, thus word segmentation is a fundamental task for the processing and understanding of these languages. Given a sentence as a sequence of n characters: x = x1 x2 .. xn where xi is a character, word segmentation aims to split the sequence into m(≤ n) words: x1:e1 xe1 +1:e2 .. xem−1+1:em where each subsequence xi:j indicates a Chinese word spanning from characters xi to xj . Word segmentation can be formalized as a sequence labeling problem (Xue and Shen 2003), where each character in the sentence is given a boundary tag representing its position in a word. Following Ng and Low (2004), joint word segmentation and partof-speech (POS) tagging can also be solved using a character classification approach by extending boundary tags to include POS information. For word segmentation we adopt the four boundary tags of Ng and Low (2004), B, M, E, and S, where B, M, and E mean the beginning, the middle, and the end of a word, respectively, and S indicates a singlecharacter word. The word segmentation result can be generated by splitting the labeled character"
J15-1005,W03-3023,0,0.587227,"005). They utilize very different segmentation guidelines; for example, as shown in Figure 1, PD breaks VicePresident into two words and combines the phrase visited-China as a compound, compared with the segmentation following the CTB annotation guideline. It is preferable to transfer knowledge from PD to CTB because the latter also annotates tree structures, which are useful for downstream applications like parsing, summarization, and machine translation, yet it is much smaller in size. For dependency parsing, we use the dependency treebank (DCTB) extracted from CTB according to the rules of Yamada and Matsumoto (2003), and the Semantic Dependency Treebank (SDT) built on a small part of the CTB text (Che et al. 2012). Compared with the automatically extracted dependencies in DCTB, semantic dependencies in SDT reveal semantic relationships between words, rather than the syntactic relationships in syntactic dependencies. Figure 2 shows an example. Experiments on both word segmentation and dependency parsing show that annotation adaptation results in significant improvement over the baselines, and achieves the state-of-the-art with only local features. The rest of the article is organized as follows. Section 2"
J15-1005,P07-1106,0,0.026795,"rative training for annotation adaptation emphasizes the iterative optimization of the parallel annotated corpora used to train the transfer classifiers. The predict-self methodology is implicit in many unsupervised learning approaches; it has been successfully used in unsupervised dependency parsing (Daum´e III 2009). We adapt this idea to the scenario of annotation adaptation to improve transformation accuracy. In recent years much effort has been devoted to the improvement of word segmentation and dependency parsing. For example, the introduction of global training or complicated features (Zhang and Clark 2007, 2010); the investigation of word structures (Li 2011); the strategies of hybrid, joint, or stacked modeling (Nakagawa and Uchimoto 2007; Kruengkrai et al. 2009; Wang, Zong, and Su 2010; Sun 2011); and the semi-supervised and unsupervised technologies utilizing raw text (Zhao and Kit 2008; Johnson and Goldwater 2009; Mochihashi, Yamada, and Ueda 2009; Hewlett and Cohen 2011). We believe that the annotation adaptation technologies can be adopted jointly with complicated features, system combination, and semi-supervised/unsupervised technologies to further improve the performance of word segmen"
J15-1005,D10-1082,0,0.0316826,"Missing"
J15-1005,I08-4017,0,0.0347548,"parsing (Daum´e III 2009). We adapt this idea to the scenario of annotation adaptation to improve transformation accuracy. In recent years much effort has been devoted to the improvement of word segmentation and dependency parsing. For example, the introduction of global training or complicated features (Zhang and Clark 2007, 2010); the investigation of word structures (Li 2011); the strategies of hybrid, joint, or stacked modeling (Nakagawa and Uchimoto 2007; Kruengkrai et al. 2009; Wang, Zong, and Su 2010; Sun 2011); and the semi-supervised and unsupervised technologies utilizing raw text (Zhao and Kit 2008; Johnson and Goldwater 2009; Mochihashi, Yamada, and Ueda 2009; Hewlett and Cohen 2011). We believe that the annotation adaptation technologies can be adopted jointly with complicated features, system combination, and semi-supervised/unsupervised technologies to further improve the performance of word segmentation and dependency parsing. 8. Conclusion and Future Work We have described the problem of annotation adaptation and the intrinsic principles of its solutions, and proposed a series of successively enhanced models that can automatically adapt the divergence between different annotation"
J15-1005,P11-2126,0,0.0386,"Missing"
J15-1005,C08-1049,1,\N,Missing
J15-1005,P04-1059,0,\N,Missing
K15-2011,S15-1002,0,0.106015,"ation, are described in (Lin et al., 2014). We consider the embedding models which lead to two different types of intermediate representations. The relational phrase embedding model considers the dependency within words uniformly without considering the second-order effect. The word-pair embedding model considers the secondorder effect of specific combinations within the word-pairs in Arg1 and Arg2. If we plug in a paragraph vector model for the relational phrase embedding model, the model considers the effect of uni-gram within a sentence as a sequence. If we plug in a RNN-LSTM model (Le and Zuidema, 2015), the model considers the effect of uni-gram within a sentence as a tree. Introduction This paper describes the discourse parsing system developed at Dublin City University for participation in the CoNLL 2015 shared task (Xue et al., 2015). We participated in two tasks: a connective and argument identification task and a sense classification task. This paper focuses on the latter task. We divide the whole process into two stages: the first stage concerns an identification of triples (Arg1, Conn, Arg2) and pairs (Arg1, Arg2) while the second stage concerns a sense classification of the identifi"
K15-2011,P14-2131,0,0.0292953,"the hard research of seeking out individual companies it doesn’t take much to get burned L = word vectors word vectors paragraph ID word vectors the word vectors word vectors cat sat [γ + d(Arg1 , Arg2) − d(Arg1, Arg2 )]+ But where [x]+ denotes the positive part of x, γ &gt; 0 is a margin hyperparameter. S ′ denotes a set of corrupted pair where Arg1 or Arg2 is replaced by a random entity (but not both at the same time). Readers should see the detailed explanation in (Bordes et al., 2013). It is noted that we tried indicator function (alternatively called discrete-valued vector, bucket function (Bansal et al., 2014), binarization of embeddings (Guo et al., 2014)) for embeddings which are converted from real-valued vector. Although we have not tested sufficiently due to the timing constraint, we did not include this method in our experiments since we could not have any gain. Figure 1: Figure shows relational paragraph embeddings. We use a paragraph vector model to obtain the feature for Arg1 and Arg2 (Le and Mikolov, 2014). The paragraph vector model is an idea to obtain a real-valued vector in the similar construction with the word vector model (or word2vec) (Mikolov et al., 2013b) where the detailed exp"
K15-2011,D09-1036,0,0.223578,"Missing"
K15-2011,P09-1077,0,0.156889,"Missing"
K15-2011,N15-1081,0,0.123319,"developed at Dublin City University for participation in the CoNLL 2015 shared task. We participated in two tasks: a connective and argument identification task and a sense classification task. This paper focuses on the latter task and especially the sense classification for implicit connectives. 1 Implicit Explicit Prod (2.2) yes/no1 no Word pair (2.3) yes/no2 no Heuristic feat (2.4) no yes Table 1: Overview of features used for implicit/explicit classification. discourse parsing. Production features are proposed in (Lin et al., 2014) and word-pair features are reported in (Lin et al., 2014; Rutherford and Xue, 2015). Heuristic features, which is specific for explicit sense classification, are described in (Lin et al., 2014). We consider the embedding models which lead to two different types of intermediate representations. The relational phrase embedding model considers the dependency within words uniformly without considering the second-order effect. The word-pair embedding model considers the secondorder effect of specific combinations within the word-pairs in Arg1 and Arg2. If we plug in a paragraph vector model for the relational phrase embedding model, the model considers the effect of uni-gram with"
K15-2011,K15-2014,1,0.365842,"urse parsing system developed at Dublin City University for participation in the CoNLL 2015 shared task (Xue et al., 2015). We participated in two tasks: a connective and argument identification task and a sense classification task. This paper focuses on the latter task. We divide the whole process into two stages: the first stage concerns an identification of triples (Arg1, Conn, Arg2) and pairs (Arg1, Arg2) while the second stage concerns a sense classification of the identified individual triples and pairs. The first phase of the identification of connective and arguments are described in (Wang et al., 2015), which bases on the framework of (Lin et al., 2009) and is also presented in this shared task as a different paper. Hence, we omit the detailed description of the first stage (See (Wang et al., 2015) for identification of connectives and arguments). This paper focuses on the second stage which concerns sense classification. 2 Rel phrase (2.1) yes yes 2.1 Relational Phrase Embedding Features Phrase embeddings (or sentence embeddings) are distributed representation in a higher level than a word level. We used a paragraph vector model to obtain these phrase embeddings (Le and Mikolov, 2014). Upo"
K15-2011,K15-2001,0,0.293003,"t considering the second-order effect. The word-pair embedding model considers the secondorder effect of specific combinations within the word-pairs in Arg1 and Arg2. If we plug in a paragraph vector model for the relational phrase embedding model, the model considers the effect of uni-gram within a sentence as a sequence. If we plug in a RNN-LSTM model (Le and Zuidema, 2015), the model considers the effect of uni-gram within a sentence as a tree. Introduction This paper describes the discourse parsing system developed at Dublin City University for participation in the CoNLL 2015 shared task (Xue et al., 2015). We participated in two tasks: a connective and argument identification task and a sense classification task. This paper focuses on the latter task. We divide the whole process into two stages: the first stage concerns an identification of triples (Arg1, Conn, Arg2) and pairs (Arg1, Arg2) while the second stage concerns a sense classification of the identified individual triples and pairs. The first phase of the identification of connective and arguments are described in (Wang et al., 2015), which bases on the framework of (Lin et al., 2009) and is also presented in this shared task as a diff"
K15-2011,D14-1012,0,0.0185658,"ies it doesn’t take much to get burned L = word vectors word vectors paragraph ID word vectors the word vectors word vectors cat sat [γ + d(Arg1 , Arg2) − d(Arg1, Arg2 )]+ But where [x]+ denotes the positive part of x, γ &gt; 0 is a margin hyperparameter. S ′ denotes a set of corrupted pair where Arg1 or Arg2 is replaced by a random entity (but not both at the same time). Readers should see the detailed explanation in (Bordes et al., 2013). It is noted that we tried indicator function (alternatively called discrete-valued vector, bucket function (Bansal et al., 2014), binarization of embeddings (Guo et al., 2014)) for embeddings which are converted from real-valued vector. Although we have not tested sufficiently due to the timing constraint, we did not include this method in our experiments since we could not have any gain. Figure 1: Figure shows relational paragraph embeddings. We use a paragraph vector model to obtain the feature for Arg1 and Arg2 (Le and Mikolov, 2014). The paragraph vector model is an idea to obtain a real-valued vector in the similar construction with the word vector model (or word2vec) (Mikolov et al., 2013b) where the detailed explanation can be obtained. In implicit/explicit"
K15-2011,P14-1062,0,0.0351575,"nc as well. The relation of the latter is in an opposite direction. We provided the wordpair model which works for these categories but in a different perspective. Further work includes the mechanism how to make it work for Comp.Cont and Comp.Conc. Although a paragraph vector did not work efficiently, our model has a tentative model which does not have interaction between relational, paragraph, and word embeddings such as in (Denil et al., 2015), which is one immediate challenge. Then, other challenge includes replacement of a paragraph vector model with a convolutional sentence vector model (Kalchbrenner et al., 2014) and RNN-LSTM model (Le and Zuidema, 2015). The former approach is related to the supervised learning instead of unsupervised learning. The latter approach is to employ the structure of tree instead of a sequence. Table 5: Official results for explicit/implicit sense classification for test set. the averaged embedding in a sentence will perform meaning establishment in the intermediate representation which capture the characteristics of Arg1, Arg2, and Conn. First, Comp.Cont or Comp.Conc may include sentence polarity with some additional condition that these polarities may be reversed. Against"
K15-2011,P10-1052,0,0.0313451,"Missing"
K15-2011,J92-4003,0,\N,Missing
K15-2014,prasad-etal-2008-penn,0,0.270113,"g the explicit relations, the nonexplicit part extracts all the adjacent sentence pairs which are not explicit relations and then infers implicit relations. As we mainly focus on explicit relations, in this part, we only apply a simple majority function to give all candidate pairs the same results. 2.1 2.2 Argument Position Classification arg2 is the argument with which the connective is syntactically associated, and its position is fixed once we have located the connective from the previous component (Section 2.1). Thus, the challenging step for this task is to identify the location of arg1. Prasad et al. (2008) show that arg1 may be located in various positions to the connective, such as within the same sentence (SS), before (PS), or after (FS) the sentence containing the connective. Furthermore, arg1 may be adjacent or nonadjacent with connective sentence. arg1 may also contain one or more sentences. Table 2 shows the statistics of each of above scenarios. Relative Position SS FS PS Other Scenarios Connective Classifier As words which can be discourse connectives do not always function as discourse connectives, we need to identify if an instance of a connective candidate is a functional connective"
K15-2014,I11-1120,0,0.0683619,". Understanding such discourse information is clearly an important component of natural language understanding that impacts a wide range of downstream natural language applications. Since Penn Discourse Treebank was released, a number of data driven approaches have been proposed to deal with different challenging subtasks of discourse parsing. As explicit arguments may be intra-sentential or inter-sentential, Lin et al. (2012), Xu et al. (2012), Stepanov and Riccardi (2012) propose to employ argument position classification as heuristic and then apply separated models for argument extraction. Ghosh et al. (2011) regarded argument extraction as a tokenlevel sequence labeling task, applying conditional random fields (CRFs) to label each token in a sentence. Following on this work, Ghosh et al. (2012) 89 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 89–94, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics As shown in Table 1, we use three feature classes: lexical, syntactic and others. Especially, we employ the position of connection as a new feature (i.e., beginning or not), because we observe that the candidates"
K15-2014,E14-1068,0,\N,Missing
K15-2014,C12-2130,0,\N,Missing
K15-2014,W13-5704,0,\N,Missing
L16-1352,W14-3302,0,0.0356516,"Missing"
L16-1352,N06-1003,0,0.0246612,"tors easily visualise the relations between the source and target rules. 2. Related Work All existing computer-aided authoring tools within a translation context employ a kind of interactive paradigm with a CL. Mitamura (1999) allows users to compose from scratch, and discusses the issues in designing a CL for rule based machine translation. Power et al. (2003) describe a CL authoring tool for multilingual generation. Marti et al. (2010) present a rule-based rewriting tool which performs syntactic analysis. Mirkin et al. (2013) introduce a confidence-driven rewriting tool which is inspired by Callison-Burch et al. (2006) and Du et al. (2010) 2214 that paraphrases the out-of-vocabulary words (OOV) or the “hard-to-translate-part” of the source side in order to improve SMT performance. Figure 1: SMT-driven Authoring Tool by Venkatapathy and Mirkin (2012) To the best of our knowledge, Venkatapathy and Mirkin (2012) is the first interface that could be called an SMTdriven CL authoring tool, shown in Figure 1. Their tool provides users with the word, phrase, even sentence-level auto-suggestions which are obtained from an existing translation model. Nevertheless, it lacks syntactically-informed suggestions and const"
L16-1352,P05-1033,0,0.495203,"rt” of the source side in order to improve SMT performance. Figure 1: SMT-driven Authoring Tool by Venkatapathy and Mirkin (2012) To the best of our knowledge, Venkatapathy and Mirkin (2012) is the first interface that could be called an SMTdriven CL authoring tool, shown in Figure 1. Their tool provides users with the word, phrase, even sentence-level auto-suggestions which are obtained from an existing translation model. Nevertheless, it lacks syntactically-informed suggestions and constraints. Sentences in all languages contain recursive structure. Synchronous context-free grammars (SCFG) (Chiang, 2005) and stochastic inversion transduction grammars (ITG) (Wu, 1997) have been widely used in SMT and achieve impressive performance. However, MT systems which make use of SCFG tend to generate an enormous phrase table containing many erroneous rules. This huge search space not only leads to unreliable output, but also restricts the input sentence length that the system can handle. Other treebased SMT models such as Liu et al. (2006) and Shen et al. (2008) depend heavily on the accuracy of the parsing algorithm which introduces noise upstream to the MT system. Our method, ProphetMT, allows monolin"
L16-1352,D10-1041,1,0.800129,"ns between the source and target rules. 2. Related Work All existing computer-aided authoring tools within a translation context employ a kind of interactive paradigm with a CL. Mitamura (1999) allows users to compose from scratch, and discusses the issues in designing a CL for rule based machine translation. Power et al. (2003) describe a CL authoring tool for multilingual generation. Marti et al. (2010) present a rule-based rewriting tool which performs syntactic analysis. Mirkin et al. (2013) introduce a confidence-driven rewriting tool which is inspired by Callison-Burch et al. (2006) and Du et al. (2010) 2214 that paraphrases the out-of-vocabulary words (OOV) or the “hard-to-translate-part” of the source side in order to improve SMT performance. Figure 1: SMT-driven Authoring Tool by Venkatapathy and Mirkin (2012) To the best of our knowledge, Venkatapathy and Mirkin (2012) is the first interface that could be called an SMTdriven CL authoring tool, shown in Figure 1. Their tool provides users with the word, phrase, even sentence-level auto-suggestions which are obtained from an existing translation model. Nevertheless, it lacks syntactically-informed suggestions and constraints. Sentences in"
L16-1352,2003.mtsummit-papers.18,1,0.692067,"e to further improve the productivity of post-editors by explicitly featuring the relations between the source and target rules. Keywords: Controlled Language, Authoring Tool, Post-Editing, Statistical Machine Translation 1. Introduction A Controlled Language (CL) can be defined as “an explicitly defined restriction of a natural language that specifies constraints on lexicon, grammar, and style” (Huijsen, 1998). CL is widely used in professional writing where the aim is to write for a certain standard and style demanded by a particular profession, such as law, medicine, patent, technique etc (Gough and Way, 2003; Gough and Way, 2004). For multilingual documents, CL has been shown to improve the quality of the translation output, whether the translation is done by humans or machines (Nyberg et al., 2003). The advantages of applying CL are self-evident: clear and consistent composition guidelines as well as less ambiguity in translation. However, the problems are also obvious: designing the rules usually requires human linguists, and rules may be difficult for end-users to grasp. In addition, the sentences that can be generated are often limited in length and complexity (O’Brien, 2003). To the best of"
L16-1352,2004.eamt-1.9,1,0.708592,"the productivity of post-editors by explicitly featuring the relations between the source and target rules. Keywords: Controlled Language, Authoring Tool, Post-Editing, Statistical Machine Translation 1. Introduction A Controlled Language (CL) can be defined as “an explicitly defined restriction of a natural language that specifies constraints on lexicon, grammar, and style” (Huijsen, 1998). CL is widely used in professional writing where the aim is to write for a certain standard and style demanded by a particular profession, such as law, medicine, patent, technique etc (Gough and Way, 2003; Gough and Way, 2004). For multilingual documents, CL has been shown to improve the quality of the translation output, whether the translation is done by humans or machines (Nyberg et al., 2003). The advantages of applying CL are self-evident: clear and consistent composition guidelines as well as less ambiguity in translation. However, the problems are also obvious: designing the rules usually requires human linguists, and rules may be difficult for end-users to grasp. In addition, the sentences that can be generated are often limited in length and complexity (O’Brien, 2003). To the best of our knowledge, most of"
L16-1352,J07-1006,0,0.027759,"les usually requires human linguists, and rules may be difficult for end-users to grasp. In addition, the sentences that can be generated are often limited in length and complexity (O’Brien, 2003). To the best of our knowledge, most of the existing computer-aided authoring methods (Acrolinx, for example) employ a kind of interactive paradigm with a CL together with a grammar checker which provides user feedback. Users have to follow the ‘compose, check, revise’ loop until the sentence is consistent according to a given parser. Exceptions usually follow an approach called conceptual authoring (Hallett et al., 2007; Hart et al., 2008) where texts are created by short cycles of language generation and user-triggered modification actions. Obviously, these methods severely restrict a user’s expressiveness in the authoring process. Even with the help of CL, current state-of-the-art machine translation (MT) methods still fail to produce reliable outputs. For example, for English-to-Chinese translation, given the sentence “allows the client computers that connect through a token ring adapter to access the network”, it is very hard for computers to figure out: (i) the skeleton of this sentence is ‘allow someth"
L16-1352,D09-1127,1,0.81682,"s use of auto-suggestion both for syntactic templates and for terms. A shift-reduce-like (Aho, 2003) authoring interface, which allows users to easily parse the “already composed part” of the sentence, is also applied to maintain the structural correctness and unambiguous parsing while the source sentence is being composed. 3. 3.1. The main components are: A: the source-side auto-suggestion server, which stores the source-side rules obtained from the HPB server (component C) sorted according to their occurrence. B: the main UI for users to compose text. Note that we employ the ‘shift-reduce’ (Huang et al., 2009) manner to ensure the ‘left-to-right’ (or ‘right-to-left’, for Arabic) writing style which is more natural. C: modified HPB Moses server, with two main modifications: Firstly, the decoder is constrained to use the structural metadata provided by users, i.e. the parse tree is automatically constructed when the user composes the source sentence. Note that the gluegrammar (Chiang, 2005) is applied when the decoder is incapable of deriving a sound parse tree, which in our case is not an option. Therefore, the use of the glue-grammar is also prevented (i.e. the glue-grammar can be applied only at t"
L16-1352,P07-2045,0,0.00860239,"ce pairs and the test set has 943 sentence pairs. From the test set, we randomly select 150 sentences for our evaluation. Our baseline is a standard HPB-SMT model with all the default settings: maximum 2 non-terminals; maximum 5 tokens for each rule; max-chart span is 10; etc. As for ProphetMT, we modify the decoder as described in Figure 2 (C). We use the GIZA++ (Och and Ney, 2003) implementation of IBM word alignment 4, 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke and Laboratory, 2002) on the Chinese side of the training data, and Moses (Koehn et al., 2007) to decode. 1 https://code.google.com/p/berkeleyparser/ http://www.statmt.org/moses/ 3 http://www.cis.upenn.edu/ ccb/ppdb/ 2 Writing instructions The ultimate goal of ProphetMT is to allow users to easily compose sentences in a subset of language that is ‘understood’ by the computer. Ideally this can be conducted by either letting different users describe the same picture or to paraphrase the same sentence. For simplicity, our experiment only requires users to rewrite the sentences in the test set using ProphetMT. According to Wang et al. (2007), which systematically investigates English-Chine"
L16-1352,2014.amta-researchers.19,1,0.750394,"decoder, the normal restrictions of tree-based models, such as the maximum span (which is <= 20) and the NT numbers (which is usually <= 2), can be removed. 4.4. Paraphrase Auto-suggestions If the user inputs an OOV, a paraphrase engine will be queried to try to suggest terms within the current SMT model. Paraphrases are obtained from PPDB3 . If the OOV is not found in PPDB, then the user will be forced to choose another word. 5. Experiments This section describes the preliminary experiments conducted. 5.1. Experimental Settings Our raw data set is the English-to-Chinese translation memory in Li et al. (2014), consisting of 86k sentence pairs. The average sentence length of the training set are 13.2 and 13.5 for English and Chinese, respectively. The development set has 762 sentence pairs and the test set has 943 sentence pairs. From the test set, we randomly select 150 sentences for our evaluation. Our baseline is a standard HPB-SMT model with all the default settings: maximum 2 non-terminals; maximum 5 tokens for each rule; max-chart span is 10; etc. As for ProphetMT, we modify the decoder as described in Figure 2 (C). We use the GIZA++ (Och and Ney, 2003) implementation of IBM word alignment 4,"
L16-1352,P06-1077,1,0.756691,"vertheless, it lacks syntactically-informed suggestions and constraints. Sentences in all languages contain recursive structure. Synchronous context-free grammars (SCFG) (Chiang, 2005) and stochastic inversion transduction grammars (ITG) (Wu, 1997) have been widely used in SMT and achieve impressive performance. However, MT systems which make use of SCFG tend to generate an enormous phrase table containing many erroneous rules. This huge search space not only leads to unreliable output, but also restricts the input sentence length that the system can handle. Other treebased SMT models such as Liu et al. (2006) and Shen et al. (2008) depend heavily on the accuracy of the parsing algorithm which introduces noise upstream to the MT system. Our method, ProphetMT, allows monolingual users to easily and naturally write correct in-domain sentences while also providing the structural metadata needed to make the parsing of the sentence unambiguous. The set of structural templates is provided by the tree-based MT system itself, meaning that highly reliable MT results can be generated directly from the user’s composition. Syntactic annotation is a tedious task which has traditionally required specialised trai"
L16-1352,P13-4015,0,0.01856,"r translations. Finally, ProphetMT also employs a useful colour scheme to help post-editors easily visualise the relations between the source and target rules. 2. Related Work All existing computer-aided authoring tools within a translation context employ a kind of interactive paradigm with a CL. Mitamura (1999) allows users to compose from scratch, and discusses the issues in designing a CL for rule based machine translation. Power et al. (2003) describe a CL authoring tool for multilingual generation. Marti et al. (2010) present a rule-based rewriting tool which performs syntactic analysis. Mirkin et al. (2013) introduce a confidence-driven rewriting tool which is inspired by Callison-Burch et al. (2006) and Du et al. (2010) 2214 that paraphrases the out-of-vocabulary words (OOV) or the “hard-to-translate-part” of the source side in order to improve SMT performance. Figure 1: SMT-driven Authoring Tool by Venkatapathy and Mirkin (2012) To the best of our knowledge, Venkatapathy and Mirkin (2012) is the first interface that could be called an SMTdriven CL authoring tool, shown in Figure 1. Their tool provides users with the word, phrase, even sentence-level auto-suggestions which are obtained from an"
L16-1352,1999.mtsummit-1.8,0,0.152233,"A user-friendly interface that facilitates the composition of text which is suitable (with respect to both terminal and non-terminal phrases) for an existing treebased MT model; 3. Allows users to easily attach structural metadata while authoring; 4. The metadata can help the MT decoder to find better translations. Finally, ProphetMT also employs a useful colour scheme to help post-editors easily visualise the relations between the source and target rules. 2. Related Work All existing computer-aided authoring tools within a translation context employ a kind of interactive paradigm with a CL. Mitamura (1999) allows users to compose from scratch, and discusses the issues in designing a CL for rule based machine translation. Power et al. (2003) describe a CL authoring tool for multilingual generation. Marti et al. (2010) present a rule-based rewriting tool which performs syntactic analysis. Mirkin et al. (2013) introduce a confidence-driven rewriting tool which is inspired by Callison-Burch et al. (2006) and Du et al. (2010) 2214 that paraphrases the out-of-vocabulary words (OOV) or the “hard-to-translate-part” of the source side in order to improve SMT performance. Figure 1: SMT-driven Authoring T"
L16-1352,J03-1002,0,0.00501789,"English-to-Chinese translation memory in Li et al. (2014), consisting of 86k sentence pairs. The average sentence length of the training set are 13.2 and 13.5 for English and Chinese, respectively. The development set has 762 sentence pairs and the test set has 943 sentence pairs. From the test set, we randomly select 150 sentences for our evaluation. Our baseline is a standard HPB-SMT model with all the default settings: maximum 2 non-terminals; maximum 5 tokens for each rule; max-chart span is 10; etc. As for ProphetMT, we modify the decoder as described in Figure 2 (C). We use the GIZA++ (Och and Ney, 2003) implementation of IBM word alignment 4, 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke and Laboratory, 2002) on the Chinese side of the training data, and Moses (Koehn et al., 2007) to decode. 1 https://code.google.com/p/berkeleyparser/ http://www.statmt.org/moses/ 3 http://www.cis.upenn.edu/ ccb/ppdb/ 2 Writing instructions The ultimate goal of ProphetMT is to allow users to easily compose sentences in a subset of language that is ‘understood’ by the computer. Ideally this can be conducted by either letting different users describe the same"
L16-1352,P03-1021,0,0.0460033,"d Translatability. 4.2. Non-Terminal Rule (NTR) Auto-suggestions In order to extract NTRs which are meaningful to humans, we parse the source side of the training corpus with the Berkley Parser.1 Then we wrap the parsed result with the xml for Moses2 hierarchical phrase-based model to extract rules. The ranking of NTR suggestions follows the same methodology that was employed for phrase suggestions. The automatic evaluation metric we employ is character based 5-gram BLEU (Papineni et al., 2002), which is one of the standards in Chinese-as-target evaluation.4 After minimum-error-rate training (Och, 2003), the baseline obtains 49.0 BLEU score on the test set. Note that while the BLEU score might be viewed as rather high for this language pair, it is actually quite typical of the scores seen when using TMs from industry, which show much more repetition than training data used in most ‘academic’ MT papers. 5.2. 4.3. Filtering To further reduce the amount of rules, the NTRs containing content words like nouns, pronouns and numbers can be removed. This filtering is based on the observation that the structure of a sentence is primarily dictated by function words as well as verbs. The phrase-level a"
L16-1352,P02-1040,0,0.0953931,"inal rank of the proposed phrases is based on the minimization of the Semantic Distance and maiximization of the Fluency and Translatability. 4.2. Non-Terminal Rule (NTR) Auto-suggestions In order to extract NTRs which are meaningful to humans, we parse the source side of the training corpus with the Berkley Parser.1 Then we wrap the parsed result with the xml for Moses2 hierarchical phrase-based model to extract rules. The ranking of NTR suggestions follows the same methodology that was employed for phrase suggestions. The automatic evaluation metric we employ is character based 5-gram BLEU (Papineni et al., 2002), which is one of the standards in Chinese-as-target evaluation.4 After minimum-error-rate training (Och, 2003), the baseline obtains 49.0 BLEU score on the test set. Note that while the BLEU score might be viewed as rather high for this language pair, it is actually quite typical of the scores seen when using TMs from industry, which show much more repetition than training data used in most ‘academic’ MT papers. 5.2. 4.3. Filtering To further reduce the amount of rules, the NTRs containing content words like nouns, pronouns and numbers can be removed. This filtering is based on the observatio"
L16-1352,2003.eamt-1.13,0,0.0642068,"phrases) for an existing treebased MT model; 3. Allows users to easily attach structural metadata while authoring; 4. The metadata can help the MT decoder to find better translations. Finally, ProphetMT also employs a useful colour scheme to help post-editors easily visualise the relations between the source and target rules. 2. Related Work All existing computer-aided authoring tools within a translation context employ a kind of interactive paradigm with a CL. Mitamura (1999) allows users to compose from scratch, and discusses the issues in designing a CL for rule based machine translation. Power et al. (2003) describe a CL authoring tool for multilingual generation. Marti et al. (2010) present a rule-based rewriting tool which performs syntactic analysis. Mirkin et al. (2013) introduce a confidence-driven rewriting tool which is inspired by Callison-Burch et al. (2006) and Du et al. (2010) 2214 that paraphrases the out-of-vocabulary words (OOV) or the “hard-to-translate-part” of the source side in order to improve SMT performance. Figure 1: SMT-driven Authoring Tool by Venkatapathy and Mirkin (2012) To the best of our knowledge, Venkatapathy and Mirkin (2012) is the first interface that could be c"
L16-1352,P08-1066,0,0.0248572,"yntactically-informed suggestions and constraints. Sentences in all languages contain recursive structure. Synchronous context-free grammars (SCFG) (Chiang, 2005) and stochastic inversion transduction grammars (ITG) (Wu, 1997) have been widely used in SMT and achieve impressive performance. However, MT systems which make use of SCFG tend to generate an enormous phrase table containing many erroneous rules. This huge search space not only leads to unreliable output, but also restricts the input sentence length that the system can handle. Other treebased SMT models such as Liu et al. (2006) and Shen et al. (2008) depend heavily on the accuracy of the parsing algorithm which introduces noise upstream to the MT system. Our method, ProphetMT, allows monolingual users to easily and naturally write correct in-domain sentences while also providing the structural metadata needed to make the parsing of the sentence unambiguous. The set of structural templates is provided by the tree-based MT system itself, meaning that highly reliable MT results can be generated directly from the user’s composition. Syntactic annotation is a tedious task which has traditionally required specialised training. In order to maint"
L16-1352,C12-3058,0,0.0882714,"users to compose from scratch, and discusses the issues in designing a CL for rule based machine translation. Power et al. (2003) describe a CL authoring tool for multilingual generation. Marti et al. (2010) present a rule-based rewriting tool which performs syntactic analysis. Mirkin et al. (2013) introduce a confidence-driven rewriting tool which is inspired by Callison-Burch et al. (2006) and Du et al. (2010) 2214 that paraphrases the out-of-vocabulary words (OOV) or the “hard-to-translate-part” of the source side in order to improve SMT performance. Figure 1: SMT-driven Authoring Tool by Venkatapathy and Mirkin (2012) To the best of our knowledge, Venkatapathy and Mirkin (2012) is the first interface that could be called an SMTdriven CL authoring tool, shown in Figure 1. Their tool provides users with the word, phrase, even sentence-level auto-suggestions which are obtained from an existing translation model. Nevertheless, it lacks syntactically-informed suggestions and constraints. Sentences in all languages contain recursive structure. Synchronous context-free grammars (SCFG) (Chiang, 2005) and stochastic inversion transduction grammars (ITG) (Wu, 1997) have been widely used in SMT and achieve impressive"
L16-1352,D07-1077,0,0.0157802,"e Chinese side of the training data, and Moses (Koehn et al., 2007) to decode. 1 https://code.google.com/p/berkeleyparser/ http://www.statmt.org/moses/ 3 http://www.cis.upenn.edu/ ccb/ppdb/ 2 Writing instructions The ultimate goal of ProphetMT is to allow users to easily compose sentences in a subset of language that is ‘understood’ by the computer. Ideally this can be conducted by either letting different users describe the same picture or to paraphrase the same sentence. For simplicity, our experiment only requires users to rewrite the sentences in the test set using ProphetMT. According to Wang et al. (2007), which systematically investigates English-Chinese reordering, we define the writing instructions as follows (note: words in parentheses are the expanded non-terminals): 1. verbs must be used in non-terminal rules. 2. prefer the longest phrase that composes a constituent, e.g. in Figure 3, “the firewall”, “the client computer” are two phrases that act as a noun; “(the firewall) is not running” is preferable to “(the firewall) is not (running)” or “(the firewall) is (not (running))”. 3. prefer a noun phrase attacheing its preposition at the right adjacent place, e.g. in Figure 3, “outbound tra"
L16-1352,J97-3002,0,0.169314,": SMT-driven Authoring Tool by Venkatapathy and Mirkin (2012) To the best of our knowledge, Venkatapathy and Mirkin (2012) is the first interface that could be called an SMTdriven CL authoring tool, shown in Figure 1. Their tool provides users with the word, phrase, even sentence-level auto-suggestions which are obtained from an existing translation model. Nevertheless, it lacks syntactically-informed suggestions and constraints. Sentences in all languages contain recursive structure. Synchronous context-free grammars (SCFG) (Chiang, 2005) and stochastic inversion transduction grammars (ITG) (Wu, 1997) have been widely used in SMT and achieve impressive performance. However, MT systems which make use of SCFG tend to generate an enormous phrase table containing many erroneous rules. This huge search space not only leads to unreliable output, but also restricts the input sentence length that the system can handle. Other treebased SMT models such as Liu et al. (2006) and Shen et al. (2008) depend heavily on the accuracy of the parsing algorithm which introduces noise upstream to the MT system. Our method, ProphetMT, allows monolingual users to easily and naturally write correct in-domain sente"
L16-1436,P12-2040,0,0.0758794,"consecutive/simultaneous interpreting in the booth. The German VERBMOBIL speech-to-speech translation programme (Wahlster, 2013) also collected and transcribed task-oriented dialogue data. This related work focused on speech-to-speech translation including three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Danescu-Niculescu-Mizil and Lee (2011) created a conversation corpus containing large metadata-rich collections of fictional conversations extracted from raw movie scripts. Both Banchs (2012) and CMU released dialogue corpora extracted from the Internet Movie Script Database (IMSDb).3 Based on IMSDb, Walker et al. (2012) annotated 862 film scripts to learn and characterize the character style for an interactive story system, and Schmitt et al. (2012) annotated 347 dialogues to explore a spoken dialogue system. The resource of movie scripts, such as IMSDb, is good enough to generate conversational discourse for dialogue processing. However, monolingual movie scripts are not enough for MT which needs a large-scale bilingual dialogue corpus to train and tune translation models. 3. Bu"
L16-1436,W11-0609,0,0.0490398,"2002; Ryu et al., 2003; Takezawa, 2003). They collected speech dialogue corpora for machine interpretation research via recording and transcribing Japanese/English interpreters’ consecutive/simultaneous interpreting in the booth. The German VERBMOBIL speech-to-speech translation programme (Wahlster, 2013) also collected and transcribed task-oriented dialogue data. This related work focused on speech-to-speech translation including three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Danescu-Niculescu-Mizil and Lee (2011) created a conversation corpus containing large metadata-rich collections of fictional conversations extracted from raw movie scripts. Both Banchs (2012) and CMU released dialogue corpora extracted from the Internet Movie Script Database (IMSDb).3 Based on IMSDb, Walker et al. (2012) annotated 862 film scripts to learn and characterize the character style for an interactive story system, and Schmitt et al. (2012) annotated 347 dialogues to explore a spoken dialogue system. The resource of movie scripts, such as IMSDb, is good enough to generate conversational discourse for dialogue processing."
L16-1436,itamar-itai-2008-using,0,0.207421,"MT) of conversational material by exploiting their internal structure. This lack of research on the dialogue MT is a surprising fact, since dialogue exhibits more cohesiveness than single sentence and at least as much than textual discourse. Although there are a number of papers on corpus construction for various natural language processing (NLP) tasks, dialogue corpora are still scarce for MT. Some work regarding bilingual subtitles as parallel corpora exists, but it lacks rich information between utterances (sentence-level corpus) (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Itamar and Itai, 2008; Tiedemann, 2008; Xiao and Wang, 2009; Tiedemann, 2012; Zhang et al., 2014). Other work focuses on mining the internal structure in dialogue data from movie scripts. However, these are monolingual data which cannot used for MT (DanescuNiculescu-Mizil and Lee, 2011; Banchs, 2012; Walker et al., 2012; Schmitt et al., 2012). In general, the fact is that bilingual subtitles are ideal resources to extract parallel sentence-level utterances, and movie scripts contain rich information such as dialogue boundaries and speaker tags. Inspired by these facts, our initial idea was to build dialogue discou"
L16-1436,P07-2045,0,0.0127474,"Missing"
L16-1436,matsubara-etal-2002-bilingual,0,0.0781911,"Wang, 2009; Tiedemann, 2012). Thanks to the effects of crowdsourcing and fan translation in audiovisual translation (O’Hagan, 2012), we can regard subtitles as parallel corpora. Zhang et al. (2014) leveraged the existence of bilingual subtitles as a source of parallel data for the Chinese-English language pair to improve the MT systems in the movie domain. However, their work only considers sentence-level data instead of extracting more useful information for dialogues. Besides, Japanese researchers constructed a speech dialogue corpus for a machine interpretation system (Aizawa et al., 2000; Matsubara et al., 2002; Ryu et al., 2003; Takezawa, 2003). They collected speech dialogue corpora for machine interpretation research via recording and transcribing Japanese/English interpreters’ consecutive/simultaneous interpreting in the booth. The German VERBMOBIL speech-to-speech translation programme (Wahlster, 2013) also collected and transcribed task-oriented dialogue data. This related work focused on speech-to-speech translation including three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Dane"
L16-1436,W12-0117,0,0.0160222,"ed Work In the specific case of dialogue MT system, data acquisition can impose challenges including data scarcity, translation quality and scalability. The release of the Penn Discourse Treebank (PDTB)2 (Prasad et al., 2008) helped bring about 1 We release our DCU English-Chinese Dialogue Corpus in http://computing.dcu.ie/˜lwang/resource. html. 2 Available at https://www.seas.upenn.edu/˜pdtb. 2748 a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on PDTB, some have applied the insights to MT (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in dialogue or conversation as well. There are two directions of work related to dialogue corpus construction. One is parallel corpora construction for dialogue or conversation MT (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Tiedemann, 2008; Itamar and Itai, 2008; Xiao and Wang, 2009; Tiedemann, 2012). Thanks to the effects of crowdsourcing and fan translation in audiovisual translation (O’Hagan, 2012), we can regard subtitles as parallel corpora. Zhang et al. (2014) leveraged"
L16-1436,J03-1002,0,0.00968861,"Missing"
L16-1436,P03-1021,0,0.0215298,"Missing"
L16-1436,prasad-etal-2008-penn,0,0.0125033,"ue MT system. The rest of the paper is organized as follows. In Section 2, we describe related work. Section 3 describes in detail our approaches to build a dialogue corpus as well as the structure of the generated database. The experimental results for both corpus annotation and translation are reported in Section 4. Finally, Section 5 presents our conclusions and future work plans. 2. Related Work In the specific case of dialogue MT system, data acquisition can impose challenges including data scarcity, translation quality and scalability. The release of the Penn Discourse Treebank (PDTB)2 (Prasad et al., 2008) helped bring about 1 We release our DCU English-Chinese Dialogue Corpus in http://computing.dcu.ie/˜lwang/resource. html. 2 Available at https://www.seas.upenn.edu/˜pdtb. 2748 a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on PDTB, some have applied the insights to MT (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in dialogue or conversation as well. There are two directions of work related to dialogue co"
L16-1436,schmitt-etal-2012-parameterized,0,0.0221098,"g three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Danescu-Niculescu-Mizil and Lee (2011) created a conversation corpus containing large metadata-rich collections of fictional conversations extracted from raw movie scripts. Both Banchs (2012) and CMU released dialogue corpora extracted from the Internet Movie Script Database (IMSDb).3 Based on IMSDb, Walker et al. (2012) annotated 862 film scripts to learn and characterize the character style for an interactive story system, and Schmitt et al. (2012) annotated 347 dialogues to explore a spoken dialogue system. The resource of movie scripts, such as IMSDb, is good enough to generate conversational discourse for dialogue processing. However, monolingual movie scripts are not enough for MT which needs a large-scale bilingual dialogue corpus to train and tune translation models. 3. Building A Parallel Dialogue Corpus 4 (1) given a monolingual movie/episode script, we identify dialogue boundaries and speaker tags using clues such as format and story structure tags in the script; (2) for a bilingual subtitle, we align each sentence with its tra"
L16-1436,tiedemann-2008-synchronizing,0,0.197336,"aterial by exploiting their internal structure. This lack of research on the dialogue MT is a surprising fact, since dialogue exhibits more cohesiveness than single sentence and at least as much than textual discourse. Although there are a number of papers on corpus construction for various natural language processing (NLP) tasks, dialogue corpora are still scarce for MT. Some work regarding bilingual subtitles as parallel corpora exists, but it lacks rich information between utterances (sentence-level corpus) (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Itamar and Itai, 2008; Tiedemann, 2008; Xiao and Wang, 2009; Tiedemann, 2012; Zhang et al., 2014). Other work focuses on mining the internal structure in dialogue data from movie scripts. However, these are monolingual data which cannot used for MT (DanescuNiculescu-Mizil and Lee, 2011; Banchs, 2012; Walker et al., 2012; Schmitt et al., 2012). In general, the fact is that bilingual subtitles are ideal resources to extract parallel sentence-level utterances, and movie scripts contain rich information such as dialogue boundaries and speaker tags. Inspired by these facts, our initial idea was to build dialogue discourse corpus by bri"
L16-1436,tiedemann-2012-parallel,0,0.140544,"tructure. This lack of research on the dialogue MT is a surprising fact, since dialogue exhibits more cohesiveness than single sentence and at least as much than textual discourse. Although there are a number of papers on corpus construction for various natural language processing (NLP) tasks, dialogue corpora are still scarce for MT. Some work regarding bilingual subtitles as parallel corpora exists, but it lacks rich information between utterances (sentence-level corpus) (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Itamar and Itai, 2008; Tiedemann, 2008; Xiao and Wang, 2009; Tiedemann, 2012; Zhang et al., 2014). Other work focuses on mining the internal structure in dialogue data from movie scripts. However, these are monolingual data which cannot used for MT (DanescuNiculescu-Mizil and Lee, 2011; Banchs, 2012; Walker et al., 2012; Schmitt et al., 2012). In general, the fact is that bilingual subtitles are ideal resources to extract parallel sentence-level utterances, and movie scripts contain rich information such as dialogue boundaries and speaker tags. Inspired by these facts, our initial idea was to build dialogue discourse corpus by bridging the information in these two kin"
L16-1436,walker-etal-2012-annotated,0,0.171098,"013) also collected and transcribed task-oriented dialogue data. This related work focused on speech-to-speech translation including three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Danescu-Niculescu-Mizil and Lee (2011) created a conversation corpus containing large metadata-rich collections of fictional conversations extracted from raw movie scripts. Both Banchs (2012) and CMU released dialogue corpora extracted from the Internet Movie Script Database (IMSDb).3 Based on IMSDb, Walker et al. (2012) annotated 862 film scripts to learn and characterize the character style for an interactive story system, and Schmitt et al. (2012) annotated 347 dialogues to explore a spoken dialogue system. The resource of movie scripts, such as IMSDb, is good enough to generate conversational discourse for dialogue processing. However, monolingual movie scripts are not enough for MT which needs a large-scale bilingual dialogue corpus to train and tune translation models. 3. Building A Parallel Dialogue Corpus 4 (1) given a monolingual movie/episode script, we identify dialogue boundaries and speaker tags"
L16-1436,O12-1015,1,0.894007,"change is made to accommodate the size of the TV screen. It is a big challenge to deal with these changed, missing or duplicated terms during matching. All the above problems make the task a complex N to-N matching where N ≥ 0. ith utterance Di in the script is represented as a vector Di = [w1,i , w2,i , ...wk,i ], in which k is the size of the term vocabulary. Many similarity functions can be employed to calculate the similarity between two utterance vectors (Cha, 2007). Here we apply the cosine distance: sim(di , dj ) = N X k=1 Therefore, we regard the matching and projection as an IR task (Wang et al., 2012a). The Vector Space Model (VSM) (Salton et al., 1975) is a state-of-the-art IR model in which each document is represented as a vector of identifiers (here we describe each identifier as a term). The wi,k · wj,k v uN uX t w i,k k=1 v uN uX ·t w j,k (1) k=1 where N is the number of terms in an utterance vector, and wi,k and wj,k represent the weight of the ith/jth term in the utterance Di /Dj respectively. Technically, the distance between documents in VSM is calculated by comparing the 2750 Item Total number of scripts processed Total number of dialogues Total number of speakers Total number"
L16-1436,W12-6310,1,0.929649,"change is made to accommodate the size of the TV screen. It is a big challenge to deal with these changed, missing or duplicated terms during matching. All the above problems make the task a complex N to-N matching where N ≥ 0. ith utterance Di in the script is represented as a vector Di = [w1,i , w2,i , ...wk,i ], in which k is the size of the term vocabulary. Many similarity functions can be employed to calculate the similarity between two utterance vectors (Cha, 2007). Here we apply the cosine distance: sim(di , dj ) = N X k=1 Therefore, we regard the matching and projection as an IR task (Wang et al., 2012a). The Vector Space Model (VSM) (Salton et al., 1975) is a state-of-the-art IR model in which each document is represented as a vector of identifiers (here we describe each identifier as a term). The wi,k · wj,k v uN uX t w i,k k=1 v uN uX ·t w j,k (1) k=1 where N is the number of terms in an utterance vector, and wi,k and wj,k represent the weight of the ith/jth term in the utterance Di /Dj respectively. Technically, the distance between documents in VSM is calculated by comparing the 2750 Item Total number of scripts processed Total number of dialogues Total number of speakers Total number"
L16-1436,W14-3331,1,0.893281,"Missing"
L16-1436,zhang-etal-2014-dual,0,0.0684567,"eyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in dialogue or conversation as well. There are two directions of work related to dialogue corpus construction. One is parallel corpora construction for dialogue or conversation MT (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Tiedemann, 2008; Itamar and Itai, 2008; Xiao and Wang, 2009; Tiedemann, 2012). Thanks to the effects of crowdsourcing and fan translation in audiovisual translation (O’Hagan, 2012), we can regard subtitles as parallel corpora. Zhang et al. (2014) leveraged the existence of bilingual subtitles as a source of parallel data for the Chinese-English language pair to improve the MT systems in the movie domain. However, their work only considers sentence-level data instead of extracting more useful information for dialogues. Besides, Japanese researchers constructed a speech dialogue corpus for a machine interpretation system (Aizawa et al., 2000; Matsubara et al., 2002; Ryu et al., 2003; Takezawa, 2003). They collected speech dialogue corpora for machine interpretation research via recording and transcribing Japanese/English interpreters’ c"
N16-1001,D14-1020,1,0.72989,"Missing"
N16-1001,W13-2305,1,0.802481,"rank order in the original sample and hybrid super-sample evaluations are marked with • and confidence intervals that do 0.50 TerrorCat 0.45 25 20 BLEU 30 0.55 not include zero are in bold. −0.4 −0.2 0.0 0.2 −0.4 Standardized Human −0.2 0.0 0.2 Standardized Human Figure 4: Human, T ERROR C AT and B LEU scores for 10k super-sampled hybrid MT systems for WMT-12 Spanish-to-English. 7 more challenging, but the method of human evaluation we employ facilitates the straight-forward computation of human scores for vast numbers of systems directly from the original human evaluation of only n systems. Graham et al. (2013) provide a human evaluation of MT that elicits adequacy assessments of translations, independent of other translations on a fine-grained 100-point rating scale. After score standardization to iron-out differences in individual human assessor scoring strategies, the overall human score for a MT system is simply computed as the mean of the ratings attributed to its translations, and this facilitates the straight-forward computation of a human score for any hybrid system from the original human evaluation of n systems. To demonstrate, we replicate a previous year’s WMT metrics shared task, constr"
N16-1001,W13-2202,0,0.266957,"Missing"
N16-1001,W14-3336,0,0.178616,"University qliu@computing.dcu.ie Abstract scores for a sample of MT systems correlate with human assessment of that same set of systems. A main venue for evaluation of MT metrics is the annual Workshop for Statistical Machine Translation (WMT) (Bojar et al., 2015) where large-scale human evaluation takes place, primarily for the purpose of ranking systems competing in the translation shared task, but additionally to use the resulting system rankings for evaluation of automatic metrics. Since 2014, WMT has used the Pearson correlation as the official measure for evaluation of metrics (Mach´acˇ ek and Bojar, 2014; Stanojevi´c et al., 2015). Comparison of the performance of any two metrics involves the comparison of two Pearson correlation point estimates computed over a sample of MT systems, therefore. Table 1 shows correlations with human assessment of each of the metrics participating in the Czech-to-English component of WMT14 metrics shared task, and, for example, if we wish to compare the performance of the top-performing metric, R ED S YS S ENT (Wu et al., 2014), with the popular metric B LEU (Papineni et al., 2001), this involves comparison of the correlation point estimate of R ED S YS S ENT, r"
N16-1001,2001.mtsummit-papers.68,0,0.342751,"d the Pearson correlation as the official measure for evaluation of metrics (Mach´acˇ ek and Bojar, 2014; Stanojevi´c et al., 2015). Comparison of the performance of any two metrics involves the comparison of two Pearson correlation point estimates computed over a sample of MT systems, therefore. Table 1 shows correlations with human assessment of each of the metrics participating in the Czech-to-English component of WMT14 metrics shared task, and, for example, if we wish to compare the performance of the top-performing metric, R ED S YS S ENT (Wu et al., 2014), with the popular metric B LEU (Papineni et al., 2001), this involves comparison of the correlation point estimate of R ED S YS S ENT, r = 0.993, with the weaker correlation point estimate of B LEU, r = 0.909, with both computed with reference to human assessment of a sample of 5 MT systems. Automatic Machine Translation metrics, such as B LEU, are widely used in empirical evaluation as a substitute for human assessment. Subsequently, the performance of a given metric is measured by its strength of correlation with human judgment. When a newly proposed metric achieves a stronger correlation over that of a baseline, it is important to take into ac"
N16-1001,W15-3031,0,0.0698975,"Missing"
N16-1001,W14-3355,1,0.825922,"ion of automatic metrics. Since 2014, WMT has used the Pearson correlation as the official measure for evaluation of metrics (Mach´acˇ ek and Bojar, 2014; Stanojevi´c et al., 2015). Comparison of the performance of any two metrics involves the comparison of two Pearson correlation point estimates computed over a sample of MT systems, therefore. Table 1 shows correlations with human assessment of each of the metrics participating in the Czech-to-English component of WMT14 metrics shared task, and, for example, if we wish to compare the performance of the top-performing metric, R ED S YS S ENT (Wu et al., 2014), with the popular metric B LEU (Papineni et al., 2001), this involves comparison of the correlation point estimate of R ED S YS S ENT, r = 0.993, with the weaker correlation point estimate of B LEU, r = 0.909, with both computed with reference to human assessment of a sample of 5 MT systems. Automatic Machine Translation metrics, such as B LEU, are widely used in empirical evaluation as a substitute for human assessment. Subsequently, the performance of a given metric is measured by its strength of correlation with human judgment. When a newly proposed metric achieves a stronger correlation o"
N16-1001,P02-1040,0,\N,Missing
N16-1113,P11-2037,0,0.0300195,"Missing"
N16-1113,D13-1135,0,0.169061,"Missing"
N16-1113,D10-1062,0,0.769955,"hat there are 6.5M Chinese pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing. In response to this problem, we propose to find a general and replicable way to improve translation quality. The main challenge of this research is that training data for DP generation are scarce. Most 983 Proceedings of NAACL-HLT 2016, pages 983–993, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics works either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013). In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora. The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language). To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring. After building the training data for DP generation, we apply a supervised approach to build our"
N16-1113,P07-2045,0,0.0104744,"ing the approach described in Section 2.1. There are two different language models for the DP annotation (Section 2.1) and translation tasks, respectively: one is trained on the 2.13TB Chinese Web Page Collection Corpus5 while the other one is trained on all extracted 7M English subtitle data (Wang et al., 2016). Corpus Lang. Sentents Train Dev Test ZH EN ZH EN ZH EN 1,037,292 1,037,292 1,086 1,086 1,154 1,154 Pronouns 604,896 816,610 756 1,025 762 958 Ave. Len. 5.91 7.87 6.13 8.46 5.81 8.17 Table 3: Statistics of corpora. We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on a Chinese–English dialogue translation task. Furthermore, we train 5-gram language models using the SRI Language Toolkit (Stolcke, 2002). To obtain a good word alignment, we run GIZA++ (Och and Ney, 2003) on the training data together with another larger parallel subtitle corpus that contains 6M sentence pairs.6 We use minimum error rate training (Och, 2003) to optimize the feature weights. The RNN models are implemented using the common Theano neural network toolkit (Bergstra et al., 2010). We use a pre-trained word embedding via a lookup table. We use the following settings: windows = 5,"
N16-1113,D10-1086,0,0.132,"Missing"
N16-1113,W10-1737,0,0.380169,"Missing"
N16-1113,D09-1106,1,0.87907,"Missing"
N16-1113,P13-2064,1,0.842545,"missing pronouns in the translation might be recalled. This makes the input sentences and DP-inserted TM more consistent in terms of recalling DPs. 2.3.3 N-best inputs However, the above method suffers from a major drawback: it only uses the 1-best prediction result for decoding, which potentially introduces translation mistakes due to the propagation of prediction errors. To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013). In the same direction, we propose to feed the decoder N -best prediction results, which allows the system to arbitrate between multiple ambiguous hypotheses from upstream processing so that the best translation can be produced. The general method is to make the input with N -best DPs into a confusion network. In our experiment, each prediction result in the N-best list is assigned a weight of 1/N . 3 Related Work There is some work related to DP generation. One is zero pronoun resolution (ZP), which is a subdirection of co-reference resolution (CR). The difference to our task is that ZP cont"
N16-1113,C14-1003,0,0.0608436,"Missing"
N16-1113,J03-1002,0,0.0251063,"on. Related work is described in Section 3. The experimental results for both the DP generator and translation are reported in Section 4. Section 5 analyses some real examples which is followed by our conclusion in Section 6. 2 Methodology The architecture of our proposed method is shown in Figure 2, which can be divided into three phases: DP corpus annotation, DP generation, and SMT integration. 2.1 DP Training Corpus Annotation We propose an approach to automatically annotate DPs by utilizing alignment information. Given a parallel corpus, we first use an unsupervised word alignment method (Och and Ney, 2003; Tu et al., 2012) to produce a word alignment. From observing of the alignment matrix, we found it is possible to detect DPs by projecting misaligned pronouns from the non-pro-drop target side (English) to the pro-drop source side (Chinese). In this work, we focus on nominative and accusative pronouns including personal, possessive and reflexive instances, as listed in Table 1. Figure 2: Architecture of proposed method. Category Subjective Personal Objective Personal Possessive Objective Possessive Reflexive Pronouns 我 (I), 我们 (we), 你/你们 (you), 他 (he), 她 (she), 它 (it), 他们/她们/它 们 (they). 我 (me"
N16-1113,P03-1021,0,0.094528,"37,292 1,037,292 1,086 1,086 1,154 1,154 Pronouns 604,896 816,610 756 1,025 762 958 Ave. Len. 5.91 7.87 6.13 8.46 5.81 8.17 Table 3: Statistics of corpora. We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on a Chinese–English dialogue translation task. Furthermore, we train 5-gram language models using the SRI Language Toolkit (Stolcke, 2002). To obtain a good word alignment, we run GIZA++ (Och and Ney, 2003) on the training data together with another larger parallel subtitle corpus that contains 6M sentence pairs.6 We use minimum error rate training (Och, 2003) to optimize the feature weights. The RNN models are implemented using the common Theano neural network toolkit (Bergstra et al., 2010). We use a pre-trained word embedding via a lookup table. We use the following settings: windows = 5, the size of the single hidden layer = 200, iterations = 10, embeddings = 200. The MLP classifier use random initialized embeddings, with the following settings: the size of the single hidden layer = 200, embeddings = 100, iterations = 200. For end-to-end evaluation, case-insensitive BLEU (Papineni et al., 2002) is used to measure 5 Available at http://www.sogou"
N16-1113,P02-1040,0,0.0971952,"on, we pre-process the input sentences by inserting possible DPs via the DP generation model. This makes the input sentences more consistent with the additional pronoun-complete rule table. To alleviate the propagation of DP prediction errors, we feed the translation system N -best prediction results via confusion network decoding (Rosti et al., 2007). To validate the effect of the proposed approach, we carried out experiments on a Chinese–English translation task. Experimental results on a largescale subtitle corpus show that our approach improves translation performance by 0.61 BLEU points (Papineni et al., 2002) using the additional translation model trained on the DP-inserted corpus. Working together with DP-generated input sentences achieves a further improvement of nearly 1.0 984 BLEU point. Furthermore, translation performance with N -best integration is much better than its 1-best counterpart (i.e. +0.84 BLEU points). Generally, the contributions of this paper include the following: • We propose an automatic method to build a large-scale DP training corpus. Given that the DPs are annotated in the parallel corpus, models trained on this data are more appropriate to the translation task; • Benefit"
N16-1113,W12-4501,0,0.0351916,"Missing"
N16-1113,N07-1029,0,0.0127834,"anslation of missing pronouns by explicitly recalling DPs for both parallel data and monolingual input sentences. More specifically, we extract an additional rule table from the DP-inserted parallel corpus to produce a “pronoun-complete” translation model. In addition, we pre-process the input sentences by inserting possible DPs via the DP generation model. This makes the input sentences more consistent with the additional pronoun-complete rule table. To alleviate the propagation of DP prediction errors, we feed the translation system N -best prediction results via confusion network decoding (Rosti et al., 2007). To validate the effect of the proposed approach, we carried out experiments on a Chinese–English translation task. Experimental results on a largescale subtitle corpus show that our approach improves translation performance by 0.61 BLEU points (Papineni et al., 2002) using the additional translation model trained on the DP-inserted corpus. Working together with DP-generated input sentences achieves a further improvement of nearly 1.0 984 BLEU point. Furthermore, translation performance with N -best integration is much better than its 1-best counterpart (i.e. +0.84 BLEU points). Generally, th"
N16-1113,W12-4213,0,0.348159,"Missing"
N16-1113,C10-1123,1,0.824671,"et language, so that the possibly missing pronouns in the translation might be recalled. This makes the input sentences and DP-inserted TM more consistent in terms of recalling DPs. 2.3.3 N-best inputs However, the above method suffers from a major drawback: it only uses the 1-best prediction result for decoding, which potentially introduces translation mistakes due to the propagation of prediction errors. To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013). In the same direction, we propose to feed the decoder N -best prediction results, which allows the system to arbitrate between multiple ambiguous hypotheses from upstream processing so that the best translation can be produced. The general method is to make the input with N -best DPs into a confusion network. In our experiment, each prediction result in the N-best list is assigned a weight of 1/N . 3 Related Work There is some work related to DP generation. One is zero pronoun resolution (ZP), which is a subdirection of co-reference resolution (CR). The di"
N16-1113,I11-1145,1,0.817536,"hat the possibly missing pronouns in the translation might be recalled. This makes the input sentences and DP-inserted TM more consistent in terms of recalling DPs. 2.3.3 N-best inputs However, the above method suffers from a major drawback: it only uses the 1-best prediction result for decoding, which potentially introduces translation mistakes due to the propagation of prediction errors. To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013). In the same direction, we propose to feed the decoder N -best prediction results, which allows the system to arbitrate between multiple ambiguous hypotheses from upstream processing so that the best translation can be produced. The general method is to make the input with N -best DPs into a confusion network. In our experiment, each prediction result in the N-best list is assigned a weight of 1/N . 3 Related Work There is some work related to DP generation. One is zero pronoun resolution (ZP), which is a subdirection of co-reference resolution (CR). The difference to our t"
N16-1113,C12-2122,1,0.905413,"Missing"
N16-1113,L16-1436,1,0.843791,"Missing"
N16-1113,P13-1081,0,0.692401,"se pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing. In response to this problem, we propose to find a general and replicable way to improve translation quality. The main challenge of this research is that training data for DP generation are scarce. Most 983 Proceedings of NAACL-HLT 2016, pages 983–993, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics works either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013). In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora. The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language). To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring. After building the training data for DP generation, we apply a supervised approach to build our DP generator. We div"
N16-1113,N13-1125,0,0.0308356,"Missing"
N16-1113,C10-2158,0,0.0510881,"Missing"
N16-1113,P15-2051,0,0.709643,"consists of 1M sentence pairs extracted from movie and TV episode subtitles. We found that there are 6.5M Chinese pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing. In response to this problem, we propose to find a general and replicable way to improve translation quality. The main challenge of this research is that training data for DP generation are scarce. Most 983 Proceedings of NAACL-HLT 2016, pages 983–993, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics works either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013). In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora. The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language). To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring. Aft"
N16-1113,zhang-etal-2014-dual,0,0.0371089,"Missing"
N16-1113,D07-1057,0,0.339977,"Missing"
N18-1006,P17-1175,1,0.886944,"Missing"
N18-1006,W04-3250,0,0.227175,"Missing"
N18-1006,L16-1147,0,0.0402812,"work comparable we try to follow the same experimental setting used in CDNMT, where the GRU size is 1024, the affix and word embedding size is 512, and the beam width is 20. Our models are trained using stochastic gradient descent with Adam (Kingma and Ba, 2015). Chung et al. (2016) and Sennrich et al. (2016) demonstrated that bpe boosts NMT, so similar to CDNMT we also preprocess the source side of our corpora using bpe. We use WMT-15 corpora1 to train the models, newstest-2013 for tuning and newstest-2015 as the test sets. For English–Turkish (En–Tr) we use the OpenSubtitle2016 collection (Lison and Tiedemann, 2016). The training side of the English–German (En–De), English–Russian (En– Ru), and En–Tr corpora include 4.5, 2.1, and 4 million parallel sentences, respectively. We randomly select 3K sentences for each of the development and test sets for En–Tr. For all language pairs we keep the 400 most frequent characters as the target-side character set and replace the remainder (infrequent characters) with a specific character. One of the key modules in our architecture is the morphology table. In order to implement it we use a look-up table whose columns include embeddings for the target language’s affix"
N18-1006,D14-1179,0,0.0539509,"Missing"
N18-1006,P16-1100,0,0.0311023,"dels. Jean et al. (2015) proposed a model to handle very large vocabularies. Luong et al. (2015) addressed the problem of rare words and OOVs with the help of a post-translation phase to exchange unknown tokens with their potential translations. Sennrich et al. (2016) used subword units for NMT. The model relies on frequent subword units instead of words. Costa-jussà and Fonollosa (2016) designed a model for translating from MRLs. The model encodes source words with a convolutional module proposed by Kim et al. (2016). Each word is represented by a convolutional combination of its characters. Luong and Manning (2016) used a hybrid model for representing words. In their model, unseen and complex words are encoded with a character-based representation, with other words encoded via the usual surface-form embeddings. Vylomova et al. (2016) compared differCharacter-level decoding limits the search space by dramatically reducing the size of the target vocabulary, but at the same time widens the search space by working with characters whose sampling seems to be harder than words. The freedom in selection and sampling of characters can mislead the decoder, which prevents us from taking the maximum advantages of c"
N18-1006,P16-1160,0,0.189931,"V problem. For these reasons we propose an NMT engine which works at the character level. Recently, neural machine translation (NMT) has emerged as a powerful alternative to conventional statistical approaches. However, its performance drops considerably in the presence of morphologically rich languages (MRLs). Neural engines usually fail to tackle the large vocabulary and high out-of-vocabulary (OOV) word rate of MRLs. Therefore, it is not suitable to exploit existing word-based models to translate this set of languages. In this paper, we propose an extension to the state-of-the-art model of Chung et al. (2016), which works at the character level and boosts the decoder with target-side morphological information. In our architecture, an additional morphology table is plugged into the model. Each time the decoder samples from a target vocabulary, the table sends auxiliary signals from the most relevant affixes in order to enrich the decoder’s current state and constrain it to provide better predictions. We evaluated our model to translate English into German, Russian, and Turkish as three MRLs and observed significant improvements. 1 Word Translation terbiye terbiye.siz terbiye.siz.lik terbiye.siz.lik"
N18-1006,P15-1002,0,0.0387071,"tly better than their wordbased versions. This underlines the fact that the character-based decoder is perhaps not fed with sufficient information to provide improved performance compared to word-based models. 2 NMT for MRLs There are several models for NMT of MRLs which are designed to deal with morphological complexities. García-Martínez et al. (2016) and Sennrich and Haddow (2016) adapted the factored machine translation approach to neural models. Morphological annotations can be treated as extra factors in such models. Jean et al. (2015) proposed a model to handle very large vocabularies. Luong et al. (2015) addressed the problem of rare words and OOVs with the help of a post-translation phase to exchange unknown tokens with their potential translations. Sennrich et al. (2016) used subword units for NMT. The model relies on frequent subword units instead of words. Costa-jussà and Fonollosa (2016) designed a model for translating from MRLs. The model encodes source words with a convolutional module proposed by Kim et al. (2016). Each word is represented by a convolutional combination of its characters. Luong and Manning (2016) used a hybrid model for representing words. In their model, unseen and"
N18-1006,P16-2058,0,0.0359841,"ned to deal with morphological complexities. García-Martínez et al. (2016) and Sennrich and Haddow (2016) adapted the factored machine translation approach to neural models. Morphological annotations can be treated as extra factors in such models. Jean et al. (2015) proposed a model to handle very large vocabularies. Luong et al. (2015) addressed the problem of rare words and OOVs with the help of a post-translation phase to exchange unknown tokens with their potential translations. Sennrich et al. (2016) used subword units for NMT. The model relies on frequent subword units instead of words. Costa-jussà and Fonollosa (2016) designed a model for translating from MRLs. The model encodes source words with a convolutional module proposed by Kim et al. (2016). Each word is represented by a convolutional combination of its characters. Luong and Manning (2016) used a hybrid model for representing words. In their model, unseen and complex words are encoded with a character-based representation, with other words encoded via the usual surface-form embeddings. Vylomova et al. (2016) compared differCharacter-level decoding limits the search space by dramatically reducing the size of the target vocabulary, but at the same ti"
N18-1006,P02-1040,0,0.106636,"ults. We report results for the bpe→char setting, which means the source token is a bpe unit and the decoder samples a character at each time step. CDNMT is the baseline model. Table 3 includes scores reported from the original CDNMT model (Chung et al., 2016) as well as the scores from our reimplementation. To make our work comparable and show the impact of the new architecture, we tried to replicate CDNMT’s results in our experimental setting, we kept everything (parameters, iterations, epochs etc.) unchanged and evaluated the extended model in the same setting. Table 3 reports BLEU scores (Papineni et al., 2002) of our NMT models. Model CDNMT CDNMT∗ CDNMT∗m CDNMT∗o CDNMT∗mo En→De En→Ru En→Tr 21.33 21.01 21.27 21.39 21.48 26.00 26.23 26.78 26.39 26.84 18.01 18.44 18.59 18.70 • The combination of the morphology table and the extra output channel provides the best result for all languages. Figure 3 depicts the impact of the morphology table and the extra output channel for each language. 0.8 0.7 0.61 0.6 0.55 0.47 0.4 Table 3: CDNMT∗ is our implementation of CDNMT. m and o indicates that the base model is extended with the morphology table and the additional output channel, respectively. mo is the combi"
N18-1006,W17-4727,0,0.019935,"entation of the input sequence. ent representation models (word-, morpheme, and character-level models) which try to capture complexities on the source side, for the task of translating from MRLs. Chung et al. (2016) proposed an architecture which benefits from different segmentation schemes. On the encoder side, words are segmented into subunits with the byte-pair segmentation model (bpe) (Sennrich et al., 2016), and on the decoder side, one target character is produced at each time step. Accordingly, the target sequence is treated as a long chain of characters without explicit segmentation. Grönroos et al. (2017) focused on translating from English into Finnish and implicitly incorporated morphological information into NMT through multi-task learning. Passban (2018) comprehensively studied the problem of translating MRLs and addressed potential challenges in the field. Among all the models reviewed in this section, the network proposed by Chung et al. (2016) could be seen as the best alternative for translating into MRLs as it works at the character level on the decoder side and it was evaluated in different settings on different languages. Consequently, we consider it as a baseline model in our exper"
N18-1006,W16-2360,0,0.0233384,"formulated as in (2): cm i = |A| X βiu fu u=1 exp (em m iu ) ; em βiu = P |A| iu = a (fu , hi−1 ) v=1 exp (eiv ) (2) where fu represents the embedding of the u-th affix (u-th column) in the morphology/affix table A, βiu is the weight assigned to fu when predicting the i-th target token, and am is a feed-forward connection between the morphology table and the decoder. The attention module in general can be considered as a search mechanism, e.g. in the original encoder-decoder architecture the basic attention module finds the most relevant input words to make the prediction. In multi-modal NMT (Huang et al., 2016; Calixto et al., 2017) an extra attention module is added to the basic one in order to search the image input to find the most relevant image segments. In our case we have a similar additional attention module which searches the morphology table. In this scenario, the morphology table including the target language’s affixes can be considered as an external knowledge repository that sends auxiliary signals which accompany the main input sequence at all time steps. Such a table certainly includes useful information for the decoder. As we are not sure which affix preserves those pieces of useful"
N18-1006,W16-2209,0,0.033435,"we consider the decoder as a classifier, it should in principle be able to perform much better over hundreds of classes (characters) rather than thousands (words), but the performance of character-based models is almost the same as or slightly better than their wordbased versions. This underlines the fact that the character-based decoder is perhaps not fed with sufficient information to provide improved performance compared to word-based models. 2 NMT for MRLs There are several models for NMT of MRLs which are designed to deal with morphological complexities. García-Martínez et al. (2016) and Sennrich and Haddow (2016) adapted the factored machine translation approach to neural models. Morphological annotations can be treated as extra factors in such models. Jean et al. (2015) proposed a model to handle very large vocabularies. Luong et al. (2015) addressed the problem of rare words and OOVs with the help of a post-translation phase to exchange unknown tokens with their potential translations. Sennrich et al. (2016) used subword units for NMT. The model relies on frequent subword units instead of words. Costa-jussà and Fonollosa (2016) designed a model for translating from MRLs. The model encodes source wor"
N18-1006,P15-1001,0,0.0626427,"he performance of character-based models is almost the same as or slightly better than their wordbased versions. This underlines the fact that the character-based decoder is perhaps not fed with sufficient information to provide improved performance compared to word-based models. 2 NMT for MRLs There are several models for NMT of MRLs which are designed to deal with morphological complexities. García-Martínez et al. (2016) and Sennrich and Haddow (2016) adapted the factored machine translation approach to neural models. Morphological annotations can be treated as extra factors in such models. Jean et al. (2015) proposed a model to handle very large vocabularies. Luong et al. (2015) addressed the problem of rare words and OOVs with the help of a post-translation phase to exchange unknown tokens with their potential translations. Sennrich et al. (2016) used subword units for NMT. The model relies on frequent subword units instead of words. Costa-jussà and Fonollosa (2016) designed a model for translating from MRLs. The model encodes source words with a convolutional module proposed by Kim et al. (2016). Each word is represented by a convolutional combination of its characters. Luong and Manning (2016)"
N18-1006,E14-2006,0,0.0656194,"constituents on the decoder side is that of Chung et al. (2016), which should be an appropriate baseline for our comparisons. Moreover, it outperforms other existing NMT models, so we prefer to compare our network to the best existing model. This model is referred to as CDNMT in our experiments. In the next sections first we explain our experimental setting, corpora, and how we build the morphology table (Section 4.1), and then report experimental results (Section 4.2). 62 4.1 Experimental Setting to manipulate the training corpus to decompose words into morphemes for which we use Morfessor (Smit et al., 2014), an unsupervised morphological analyzer. Using Morfessor each word is segmented into different subunits where we consider the longest part as the stem of each word; what appears before the stem is taken as a member of the set of prefixes (there might be one or more prefixes) and what follows the stem is considered as a member of the set of suffixes. In order to make our work comparable we try to follow the same experimental setting used in CDNMT, where the GRU size is 1024, the affix and word embedding size is 512, and the beam width is 20. Our models are trained using stochastic gradient des"
N18-1006,P16-1162,0,\N,Missing
N19-1312,W15-3001,0,0.0170069,"in, containing about 1.25M sentence pairs, and in-domain data is mainly related to the News Commentary domain which is more informal compared to the news corpus, containing about 59.1K sentences. We also used the development set of the domain adaptation shared task. Finally, we tested our method on the NC test set of WMT 3085 1 https://www.ldc.upenn.edu/ http://www.statmt.org/moses/ 3 https://nlp.stanford.edu/ 2 2006 and WMT 2007. We tokenized and lowercased the corpora. German→English For this task, out-of-domain corpus is from the WMT 2015 en-de translation task which are mainly News texts (Bojar et al., 2015) containing about 4.2M sentence pairs. For the in-domain corpus, we used the parallel training data from the IWSLT 2015 which is mainly from the the TED talks containing about 190K sentences. In addition, dev2012 and test2013/2014/2015 of IWSLT 2015 were selected as the development and test data, respectively. We tokenized and truecased the corpora. Besides, 16K, 16K and 32K merging operations were performed to learn byte-pair encoding(BPE) (Sennrich et al., 2015b) on both sides of the parallel training data and sentences longer than 50, 50 and 80 tokens were removed from the training data, re"
N19-1312,W17-4712,0,0.505001,"l applications, NMT models usually need to perform translation for some specific domain with only a small quantity of in*Corresponding Author domain training data but a large amount of out-ofdomain data. Simply combining in-domain training data with out-of-domain data will lead to overfitting to the out-of-domain data. Therefore, some domain adaptation technique should be adopted to improve in-domain translation. Fortunately, out-of-domain data still embodies common knowledge shared between domains. And incorporating the common knowledge from out-of-domain data can help in-domain translation. Britz et al. (2017) have done this kind of attempts and managed to improve in-domain translation. The common architecture of this method is to share a single encoder and decoder among all the domains and add a discriminator to the encoder to distinguish the domains of the input sentences. The training is based on adversarial learning between the discriminator and the translation , ensuring the encoder can learn common knowledge across domains that can help to generate target translation. Zeng et al. (2018) extend this line of work by introducing a private encoder to learn some domain specific knowledge. They hav"
N19-1312,W07-0718,0,0.0279898,"contains 1.25M sentence pairs. The LDC data is mainly related to the News domain. We chose the parallel sentences with the domain label Laws from the UMCorpus (Tian et al., 2014) as our in-domain data. We chose 109K, 1K and 1K sentences from the UM-Corpus randomly as our training, development and test data. We tokenized and lowercased the English sentences with Moses2 scripts. For the Chinese data, we performed word segmentation using Stanford Segmenter3 . English→German For this task, the training data is from the Europarl corpus distributed for the shared domain adaptation task of WMT 2007 (Callison-Burch et al., 2007) where the outof-domain data is mainly related to the News domain, containing about 1.25M sentence pairs, and in-domain data is mainly related to the News Commentary domain which is more informal compared to the news corpus, containing about 59.1K sentences. We also used the development set of the domain adaptation shared task. Finally, we tested our method on the NC test set of WMT 3085 1 https://www.ldc.upenn.edu/ http://www.statmt.org/moses/ 3 https://nlp.stanford.edu/ 2 2006 and WMT 2007. We tokenized and lowercased the corpora. German→English For this task, out-of-domain corpus is from th"
N19-1312,2016.amta-researchers.10,0,0.0212992,"information or out-of-domain parallel data. To exploit in-domain monolingual data, Gülçehre et al. (2015) train a RNNLM on the target side monolingual data first and then use it in decoding. Domhan and Hieber (2017) further extend this work by training the RNNLM part and translation part jointly. Sennrich et al. (2015a) propose to conduct back translation for the monolingual target data so as to generate the corresponding parallel data. Zhang and Zong (2016) employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. To introduce meta information, Chen et al. (2016) use the topic or category information of the input text to assistant the decoder and Kobus et al. (2017) extend the generic NMT models, which are trained on a diverse set of data to, specific domains with the specialized terminology and style. To make use of out-of-domain parallel data, Luong and Manning (2015) first train an NMT model with a large amount of out-of-domain data, then fine tune the model with in-domain data. Wang et al. (2017a) select sentence pairs from the outof-domain data set according to their similarity to the in-domain data and then add them to the indomain training data"
N19-1312,P17-1110,0,0.0255623,"common encoder, Zeng et al. (2018) further introduce a domain-specific encoder to each domain together with a domain-specific classifier to ensure the features extracted by the domain-specific encoder is proper. Compared to our method, they focus on the encoder and do not distinguish the information in the decoder. Adversarial Networks have achieved great success in some areas (Ganin et al., 2016; Goodfellow et al., 2014). Inspired by these work, we also employ a domain discriminator to extract some domain invariant features which has already shown its effectiveness in some related NLP tasks. Chen et al. (2017) use a classifier to exploit the shared information between different Chinese word segment criteria. Gui et al. (2017) tries to learn common features of the out-domain data and indomain data through adversarial discriminator for the part-of-speech tagging problem. Kim et al. (2017) train a cross-lingual model with languageadversarial training to generate the general information across different languages for the POS tagging problem. All these work try to utilize a discriminator to distinguish invariant features across the divergence. 3082 Decoder MLE ∗ yj−1 ... sj−1 sj RNN Unit Attention aj ="
N19-1312,P17-2061,0,0.249934,"Missing"
N19-1312,P05-1066,0,0.181363,"Missing"
N19-1312,D17-1158,0,0.0275401,"et al., 2017) and get consistently significant improvements over several strong baselines. 2 Related Work The task of domain adaptation for NMT is to translate a text in-domain for which only a small number of parallel sentences is available. The main idea of the work for domain adaptation is to introduce external information to help in-domain translation which may include in-domain monolingual data, meta information or out-of-domain parallel data. To exploit in-domain monolingual data, Gülçehre et al. (2015) train a RNNLM on the target side monolingual data first and then use it in decoding. Domhan and Hieber (2017) further extend this work by training the RNNLM part and translation part jointly. Sennrich et al. (2015a) propose to conduct back translation for the monolingual target data so as to generate the corresponding parallel data. Zhang and Zong (2016) employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. To introduce meta information, Chen et al. (2016) use the topic or category information of the input text to assistant the decoder and Kobus et al. (2017) extend the generic NMT models, which are trained on a diverse set of data to, specific doma"
N19-1312,D17-1256,0,0.02753,"ecific classifier to ensure the features extracted by the domain-specific encoder is proper. Compared to our method, they focus on the encoder and do not distinguish the information in the decoder. Adversarial Networks have achieved great success in some areas (Ganin et al., 2016; Goodfellow et al., 2014). Inspired by these work, we also employ a domain discriminator to extract some domain invariant features which has already shown its effectiveness in some related NLP tasks. Chen et al. (2017) use a classifier to exploit the shared information between different Chinese word segment criteria. Gui et al. (2017) tries to learn common features of the out-domain data and indomain data through adversarial discriminator for the part-of-speech tagging problem. Kim et al. (2017) train a cross-lingual model with languageadversarial training to generate the general information across different languages for the POS tagging problem. All these work try to utilize a discriminator to distinguish invariant features across the divergence. 3082 Decoder MLE ∗ yj−1 ... sj−1 sj RNN Unit Attention aj = αji h1 Encoder − → h1 ← − h1 x1 yj∗ yj Logistic Layer ! ℓs i=1 Out-of Domain Data ... Out-of Decoder + αij hi Shared E"
N19-1312,D13-1176,0,0.0862069,"rivate decoder for each domain which are used to model domain-specific information. In the meantime, we introduce a common encoder and a common decoder shared by all the domains which can only have domainindependent information flow through. Besides, we add a discriminator to the shared encoder and employ adversarial training for the whole model to reinforce the performance of information separation and machine translation simultaneously. Experiment results show that our method can outperform competitive baselines greatly on multiple data sets. 1 Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017) has made great progress and drawn much attention recently. Most NMT models are based on the encoder-decoder architecture, where all the sentence pairs share the same set of parameters for the encoder and decoder which makes NMT models have a tendency towards overfitting to frequent observations (e.g., words, word cooccurrences, translation patterns), but overlooking special cases that are not frequently observed. However, in practical applications, NMT models usually need to perform translation for some sp"
N19-1312,D17-1302,0,0.0188713,"h the information in the decoder. Adversarial Networks have achieved great success in some areas (Ganin et al., 2016; Goodfellow et al., 2014). Inspired by these work, we also employ a domain discriminator to extract some domain invariant features which has already shown its effectiveness in some related NLP tasks. Chen et al. (2017) use a classifier to exploit the shared information between different Chinese word segment criteria. Gui et al. (2017) tries to learn common features of the out-domain data and indomain data through adversarial discriminator for the part-of-speech tagging problem. Kim et al. (2017) train a cross-lingual model with languageadversarial training to generate the general information across different languages for the POS tagging problem. All these work try to utilize a discriminator to distinguish invariant features across the divergence. 3082 Decoder MLE ∗ yj−1 ... sj−1 sj RNN Unit Attention aj = αji h1 Encoder − → h1 ← − h1 x1 yj∗ yj Logistic Layer ! ℓs i=1 Out-of Domain Data ... Out-of Decoder + αij hi Shared Enocder Shared Decoder h 2 . . . h i . . . h ℓs ← − h2 − → h2 ←− h ℓs ... x2 GRL −→ h ℓs ... In Domain Data Domain Discriminator x ℓs ... Prediction Layer + Figure 1"
N19-1312,kobus-etal-2017-domain,0,0.516405,") train a RNNLM on the target side monolingual data first and then use it in decoding. Domhan and Hieber (2017) further extend this work by training the RNNLM part and translation part jointly. Sennrich et al. (2015a) propose to conduct back translation for the monolingual target data so as to generate the corresponding parallel data. Zhang and Zong (2016) employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. To introduce meta information, Chen et al. (2016) use the topic or category information of the input text to assistant the decoder and Kobus et al. (2017) extend the generic NMT models, which are trained on a diverse set of data to, specific domains with the specialized terminology and style. To make use of out-of-domain parallel data, Luong and Manning (2015) first train an NMT model with a large amount of out-of-domain data, then fine tune the model with in-domain data. Wang et al. (2017a) select sentence pairs from the outof-domain data set according to their similarity to the in-domain data and then add them to the indomain training data. Chu et al. (2017) construct the training data set for the NMT model by combining out-of-domain data wit"
N19-1312,2015.iwslt-evaluation.11,0,0.865654,"et al. (2015a) propose to conduct back translation for the monolingual target data so as to generate the corresponding parallel data. Zhang and Zong (2016) employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. To introduce meta information, Chen et al. (2016) use the topic or category information of the input text to assistant the decoder and Kobus et al. (2017) extend the generic NMT models, which are trained on a diverse set of data to, specific domains with the specialized terminology and style. To make use of out-of-domain parallel data, Luong and Manning (2015) first train an NMT model with a large amount of out-of-domain data, then fine tune the model with in-domain data. Wang et al. (2017a) select sentence pairs from the outof-domain data set according to their similarity to the in-domain data and then add them to the indomain training data. Chu et al. (2017) construct the training data set for the NMT model by combining out-of-domain data with the over-sampled in-domain data. Wang et al. (2017b) combine the in-domain and out-of-domain data together as the training data but apply instance weighting to get a weight for each sentence pair in the out"
N19-1312,P02-1040,0,0.105149,"mains. The biggest difference is that we add private parts to preserve the domain specific features. Besides we also applied a different training strategy as the section 5 describes so that our method can handle more generic situations. Noting that our model has a private encoderdecoder which brings extra parameters, we just slightly extend the hidden size of the contrast model to make sure that the total parameter number of the contrast model is equal to the number of our model’s translation part. 6.3 Main Results The En-Zh Experiments Results are measured using char based 5-gram BLEU score (Papineni et al., 2002) by the multi-bleu.pl script. The main results are shown in Table 1. On both of the development set and test set, our model significantly outperforms the baseline models and other contrast models. Furthermore, we got the following conclusions: First, the baseline model ’In’ surpass the ’Out + In’ model which shows that the NMT model tends to fit out-of-domain features if we directly include 3086 (a) without discriminator (b) full model Figure 3: The shared encoder’s hidden state of the two models. Data from the out-of-domain are presented as blue dots while data from the in-domain are presente"
N19-1312,P16-1009,0,0.127958,"Missing"
N19-1312,tian-etal-2014-um,0,0.278045,"Missing"
N19-1312,P17-2089,0,0.070391,"Missing"
N19-1312,D17-1155,0,0.348359,"Missing"
N19-1312,D18-1041,0,0.334609,"Missing"
N19-1312,D16-1160,0,0.0242738,"n idea of the work for domain adaptation is to introduce external information to help in-domain translation which may include in-domain monolingual data, meta information or out-of-domain parallel data. To exploit in-domain monolingual data, Gülçehre et al. (2015) train a RNNLM on the target side monolingual data first and then use it in decoding. Domhan and Hieber (2017) further extend this work by training the RNNLM part and translation part jointly. Sennrich et al. (2015a) propose to conduct back translation for the monolingual target data so as to generate the corresponding parallel data. Zhang and Zong (2016) employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. To introduce meta information, Chen et al. (2016) use the topic or category information of the input text to assistant the decoder and Kobus et al. (2017) extend the generic NMT models, which are trained on a diverse set of data to, specific domains with the specialized terminology and style. To make use of out-of-domain parallel data, Luong and Manning (2015) first train an NMT model with a large amount of out-of-domain data, then fine tune the model with in-domain data. Wang et al. (201"
N19-1312,C18-1110,1,0.874985,"Missing"
O02-2003,P93-1022,0,0.0432139,"Missing"
O03-5002,M98-1028,0,0.0191505,"nition (NER) is an essential process widely required in natural language understanding and many other text-based applications, such as question answering, information retrieval, and information extraction. NER is also an important subtask of the Multilingual Entity Task (MET), which was established in the spring of 1996 and run in conjunction with the Message Understanding Conference (MUC). The entities defined in MET are divided into three categories: entities [organizations (ORG), persons (PER), locations (LOC)], times (dates and times), and quantities (monetary values and percentages) [N.A.Chinchor, 1998]. As for NE in Chinese, we further divide PER into two sub-classes: Chinese PER and transliterated PER on the basis of their distinct features. Similarly, LOC is split into Chinese LOC and transliterated LOC. In this work, we only focus on those more difficult but commonly used categories: PER, LOC and ORG. Other NE such as times (TIME) and quantities (QUAN), in a border sense, can be recognized simply via finite state automata. Chinese NER has not been researched intensively till now, while English NER has received much attention. Because of the inherent difference between the two languages,"
O03-5002,P02-1060,0,0.0189394,"华|PER) p (张|BEG) p (平等|着) 平等 p (PER|BEG) 张华平 等着 P(等着|你) END p (等着|PER) p’(张华平|PER) Figure 6:Demonstration of segmentation on “张华平等着你” using the class-based approach. Note: “张华平”(Zhang Hua-Ping) and “张华” are NE candidates from role models. 5. Comparison with Previous Works Since MET came into existence, NER has received increasing attention, especially in research on written and spoken English. Some systems have been put into practice. The approaches tend to involve statistics mixed with rules, such as the hidden Markov model (HMM), the expectation maximum, transformation-based learning, etc. [Zhou and Su, 2002; Bikel et al. 1997; Borthwick et al. 1999 ]. Besides making use of a corpus with labels, Andrei et al. [1999] proposed another statistical method without Gazetteers. Historically, much work has been done on Chinese NER, but the research is still in its early stages. Previous solutions can be broadly categorized into rule-based approaches [Luo, 2001; Ji, 2001; Song, 1993; Tan, 1999], statistics-based ones [Zhang et al. 2002; Sun et al. 2002; Sun, 1993] and approaches that are a combination of both [Ye, 2002, Lv et al. 2001]. Compared with our approach using the role model, previous works have"
O03-5002,W02-1817,1,\N,Missing
O03-5002,C02-1080,0,\N,Missing
O03-5002,C02-1012,0,\N,Missing
P05-1057,J00-2004,0,0.367369,"Missing"
P05-1057,P02-1038,0,0.441599,"of IBM translation models and HMM model. Although Model 6 yields better results than naive IBM models, it fails to include dependencies other than IBM models and HMM model. Cherry and Lin (2003) developed a 459 Proceedings of the 43rd Annual Meeting of the ACL, pages 459–466, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics statistical model to find word alignments, which allow easy integration of context-specific features. Log-linear models, which are very suitable to incorporate additional dependencies, have been successfully applied to statistical machine translation (Och and Ney, 2002). In this paper, we present a framework for word alignment based on log-linear models, allowing statistical models to be easily extended by incorporating additional syntactic dependencies. We use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features. Our experiments show that log-linear models significantly outperform IBM translation models. We begin by describing log-linear models for word alignment. The design of feature functions is discussed then. Next, we present the training method and the search algorithm for log-linear models. We will fo"
P05-1057,P03-1021,0,0.0507453,"Missing"
P05-1057,J03-1002,0,0.488153,"iu, liuqun, sxlin}@ict.ac.cn Abstract data, try to describe the relationship between a bilingual sentence pair (Brown et al., 1993; Vogel and Ney, 1996). Heuristic approaches obtain word alignments by using various similarity functions between the types of the two languages (Smadja et al., 1996; Ker and Chang, 1997; Melamed, 2000). The central distinction between statistical and heuristic approaches is that statistical approaches are based on well-founded probabilistic models while heuristic ones are not. Studies reveal that statistical alignment models outperform the simple Dice coefficient (Och and Ney, 2003). We present a framework for word alignment based on log-linear models. All knowledge sources are treated as feature functions, which depend on the source langauge sentence, the target language sentence and possible additional variables. Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information. In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features. Our experiments show that log-linear models significantly outperform IBM translation models. 1 Introduction Word alignmen"
P05-1057,J96-1002,0,0.0242527,"Missing"
P05-1057,J96-1001,0,0.125699,"Missing"
P05-1057,J95-4004,0,0.0974074,"ptimized on the development corpus. Therefore, we have a new search algorithm: Input: e, f , eT, fT, D and t Output: a 1. Start with a = φ. 2. Do for each l = (i, j) and l ∈ / a: Compute gain(a, l) 3 We still call the new heuristic function gain to reduce notational overhead, although the gain in Eq. 13 is not equivalent to the one in Eq. 12. 463 The Chinese sentences in both the development and test corpus are segmented and POS tagged by ICTCLAS (Zhang et al., 2003). The English sentences are tokenized by a simple tokenizer of ours and POS tagged by a rule-based tagger written by Eric Brill (Brill, 1995). We manually aligned 935 sentences, in which we selected 500 sentences as test corpus. The remaining 435 sentences are used as development corpus to train POS tags transition probabilities and to optimize the model parameters and gain threshold. Provided with human-annotated word-level alignment, we use precision, recall and AER (Och and Model 3 E → C Model 3 C → E Intersection Union Refined Method Model 3 E → C + Model 3 C → E + POS E → C + POS C → E + Dict 1K 0.4497 0.4688 0.4588 0.4596 0.4154 0.4490 0.3970 0.3828 0.3795 0.3650 Size of Training Corpus 5K 9K 39K 0.4081 0.4009 0.3791 0.4261 0"
P05-1057,E03-1026,0,0.0457116,"aches and heuristic approaches. Statistical approaches, which depend on a set of unknown parameters that are learned from training Finding word alignments between parallel texts, however, is still far from a trivial work due to the diversity of natural languages. For example, the alignment of words within idiomatic expressions, free translations, and missing content or function words is problematic. When two languages widely differ in word order, finding word alignments is especially hard. Therefore, it is necessary to incorporate all useful linguistic information to alleviate these problems. Tiedemann (2003) introduced a word alignment approach based on combination of association clues. Clues combination is done by disjunction of single clues, which are defined as probabilities of associations. The crucial assumption of clue combination that clues are independent of each other, however, is not always true. Och and Ney (2003) proposed Model 6, a log-linear combination of IBM translation models and HMM model. Although Model 6 yields better results than naive IBM models, it fails to include dependencies other than IBM models and HMM model. Cherry and Lin (2003) developed a 459 Proceedings of the 43r"
P05-1057,J93-2003,0,0.0881567,"Missing"
P05-1057,P03-1012,0,0.464916,"ic information to alleviate these problems. Tiedemann (2003) introduced a word alignment approach based on combination of association clues. Clues combination is done by disjunction of single clues, which are defined as probabilities of associations. The crucial assumption of clue combination that clues are independent of each other, however, is not always true. Och and Ney (2003) proposed Model 6, a log-linear combination of IBM translation models and HMM model. Although Model 6 yields better results than naive IBM models, it fails to include dependencies other than IBM models and HMM model. Cherry and Lin (2003) developed a 459 Proceedings of the 43rd Annual Meeting of the ACL, pages 459–466, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics statistical model to find word alignments, which allow easy integration of context-specific features. Log-linear models, which are very suitable to incorporate additional dependencies, have been successfully applied to statistical machine translation (Och and Ney, 2002). In this paper, we present a framework for word alignment based on log-linear models, allowing statistical models to be easily extended by incorporating additional syntactic d"
P05-1057,C96-2141,0,0.953108,"Missing"
P05-1057,P03-1011,0,0.0495679,"Missing"
P05-1057,J97-2004,0,0.032521,"Missing"
P05-1057,P01-1067,0,0.229123,"Missing"
P05-1057,W03-1730,1,0.260281,"l, e, f ) − hm (a, e, f )] Note that we restrict h(a, e, f ) ≥ 0 for all feature functions. Gain threshold t is a real-valued number, which can be optimized on the development corpus. Therefore, we have a new search algorithm: Input: e, f , eT, fT, D and t Output: a 1. Start with a = φ. 2. Do for each l = (i, j) and l ∈ / a: Compute gain(a, l) 3 We still call the new heuristic function gain to reduce notational overhead, although the gain in Eq. 13 is not equivalent to the one in Eq. 12. 463 The Chinese sentences in both the development and test corpus are segmented and POS tagged by ICTCLAS (Zhang et al., 2003). The English sentences are tokenized by a simple tokenizer of ours and POS tagged by a rule-based tagger written by Eric Brill (Brill, 1995). We manually aligned 935 sentences, in which we selected 500 sentences as test corpus. The remaining 435 sentences are used as development corpus to train POS tags transition probabilities and to optimize the model parameters and gain threshold. Provided with human-annotated word-level alignment, we use precision, recall and AER (Och and Model 3 E → C Model 3 C → E Intersection Union Refined Method Model 3 E → C + Model 3 C → E + POS E → C + POS C → E +"
P05-1057,W02-1012,0,\N,Missing
P06-1066,P00-1056,0,0.682597,"Missing"
P06-1066,P03-1021,0,0.0549363,"inese-English tasks, the probability for the straight order is set at pm = 0.95. This is because word order in Chinese and English is usually similar. The last one is the maximum entropy based reordering model proposed by us, which will be described in the next section. We define a derivation D as a sequence of applications of rules (1) − (3), and let c(D) and e(D) be the Chinese and English yields of D. The probability of a derivation D is P r(D) = Y P r(i) (7) Another feature of our decoder is the k-best list generation. The k-best list is very important for the minimum error rate training (Och, 2003a) which is used for tuning the weights λ for our model. We use a very lazy algorithm for the k-best list generation, which runs two phases similarly to the one by Huang et al. (2005). In the first phase, the decoder runs as usual except that it keeps some information of weaker derivations which are to be discarded during recombination. This will generate not only the first-best of final derivation but also a shared forest. In the second phase, the lazy algorithm runs recursively on the shared forest. It finds the second-best of the final derivation, which makes its children to find their seco"
P06-1066,J04-4002,0,0.59146,"ion, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks. 1 Introduction Phrase reordering is of great importance for phrase-based SMT systems and becoming an active area of research recently. Compared with word-based SMT systems, phrase-based systems can easily address reorderings of words within phrases. However, at the phrase level, reordering is still a computationally expensive problem just like reordering at the word level (Knight, 1999). Many systems use very simple models to reorder phrases 1 . One is distortion model (Och and Ney, 2004; Koehn et al., 2003) which penalizes translations according to their jump distance instead of their content. For example, if N words are skipped, a penalty of N will be paid regardless of which words are reordered. This model takes the risk of penalizing long distance jumps 1 In this paper, we focus our discussions on phrases that are not necessarily aligned to syntactic constituent boundary. 521 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 521–528, c Sydney, July 2006. 2006 Association for Computational Linguistics cl"
P06-1066,W02-1039,0,0.0516889,"1 .t1 = E1 , o = O 0, otherwise 1, b1 .t1 = E1 , b2 .t1 = E2 , o = O 0, otherwise Figure 3: MaxEnt-based reordering feature templates. The first one is a lexical feature, and the second one is a target collocation feature, where Ei are English words, O ∈ {straight, inverted}. is block collocation, b1 .s1 &b1 .t1 and b2 .s1 &b2 .t1 . The templates for the lexical feature and the collocation feature are shown in Figure 3. Why do we use the first words as features? These words are nicely at the boundary of blocks. One of assumptions of phrase-based SMT is that phrase cohere across two languages (Fox, 2002), which means phrases in one language tend to be moved together during translation. This indicates that boundary words of blocks may keep information for their movements/reorderings. To test this hypothesis, we calculate the information gain ratio (IGR) for boundary words as well as the whole blocks against the order on the reordering examples extracted by the algorithm described above. The IGR is the measure used in the decision tree learning to select features (Quinlan, 1993). It represents how precisely the feature predicate the class. For feature f and class c, the IGR(f, c) Figure 2: Reor"
P06-1066,J99-4005,0,0.0622403,"ing events of neighbor blocks from bilingual data. In our experiments on Chineseto-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks. 1 Introduction Phrase reordering is of great importance for phrase-based SMT systems and becoming an active area of research recently. Compared with word-based SMT systems, phrase-based systems can easily address reorderings of words within phrases. However, at the phrase level, reordering is still a computationally expensive problem just like reordering at the word level (Knight, 1999). Many systems use very simple models to reorder phrases 1 . One is distortion model (Och and Ney, 2004; Koehn et al., 2003) which penalizes translations according to their jump distance instead of their content. For example, if N words are skipped, a penalty of N will be paid regardless of which words are reordered. This model takes the risk of penalizing long distance jumps 1 In this paper, we focus our discussions on phrases that are not necessarily aligned to syntactic constituent boundary. 521 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Me"
P06-1066,W05-1506,0,0.030856,"Missing"
P06-1066,N03-1017,0,0.0910021,"sed reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks. 1 Introduction Phrase reordering is of great importance for phrase-based SMT systems and becoming an active area of research recently. Compared with word-based SMT systems, phrase-based systems can easily address reorderings of words within phrases. However, at the phrase level, reordering is still a computationally expensive problem just like reordering at the word level (Knight, 1999). Many systems use very simple models to reorder phrases 1 . One is distortion model (Och and Ney, 2004; Koehn et al., 2003) which penalizes translations according to their jump distance instead of their content. For example, if N words are skipped, a penalty of N will be paid regardless of which words are reordered. This model takes the risk of penalizing long distance jumps 1 In this paper, we focus our discussions on phrases that are not necessarily aligned to syntactic constituent boundary. 521 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 521–528, c Sydney, July 2006. 2006 Association for Computational Linguistics classification with onl"
P06-1066,koen-2004-pharaoh,0,0.29269,"rings will be extracted, not limited to reorderings of bilingual phrases of length less than a predefined number of words. Secondly, features will be extracted from reordering examples according to feature templates. Finally, a maximum entropy classifier will be trained on the features. In this paper we describe our system and the MaxEnt-based reordering model with the associated algorithm. We also present experiments that indicate that the MaxEnt-based reordering model improves translation significantly compared with other reordering approaches and a state-of-the-art distortion-based system (Koehn, 2004). used to translate source phrase y into target phrase x and generate a block A. Later, the straight rule (1) merges two consecutive blocks into a single larger block in the straight order; while the inverted rule (2) merges them in the inverted order. These two merging rules will be used continuously until the whole source sentence is covered. When the translation is finished, a tree indicating the hierarchical segmentation of the source sentence is also produced. In the following, we will define the model in a straight way, not in the dynamic programming recursion way used by (Wu, 1996; Zens"
P06-1066,2005.eamt-1.36,0,0.0562487,"Missing"
P06-1066,C04-1030,0,0.2676,"However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause other problems, e.g. the data sparseness problem. Another smart reordering model was proposed by Chiang (2005). In his approach, phrases are reorganized into hierarchical ones by reducing subphrases to variables. This template-based scheme not only captures the reorderings of phrases, but also integrates some phrasal generalizations into the global model. In this paper, we propose a novel solution for phrasal reordering. Here, under the ITG constraint (Wu, 1997; Zens et al., 2004), we need to consider just two kinds of reorderings, straight and inverted between two consecutive blocks. Therefore reordering can be modelled as a problem of We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs). The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext. We present an algorithm to extract all reordering events of neighbor blocks from bilingual"
P06-1066,N04-4026,0,0.493594,"Chinese Academy of Sciences {liuqun, sxlin}@ict.ac.cn dyxiong@ict.ac.cn Abstract which are common between two languages with very different orders. Another simple model is flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005) which is not content dependent either. Flat model assigns constant probabilities for monotone order and non-monotone order. The two probabilities can be set to prefer monotone or non-monotone orientations depending on the language pairs. In view of content-independency of the distortion and flat reordering models, several researchers (Och et al., 2004; Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005) proposed a more powerful model called lexicalized reordering model that is phrase dependent. Lexicalized reordering model learns local orientations (monotone or non-monotone) with probabilities for each bilingual phrase from training data. During decoding, the model attempts to finding a Viterbi local orientation sequence. Performance gains have been reported for systems with lexicalized reordering model. However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause other problems,"
P06-1066,W02-2018,0,0.0177583,"ures of blocks as reordering evidences. Good features can not only capture reorderings, avoid sparseness, but also integrate generalizations. It is very straight to use maximum entropy model to integrate features to predicate reorderings of blocks. Under the MaxEnt model, we have b must be consistent with the word alignment M ∀(i, j) ∈ M, i1 ≤ i ≤ i2 ↔ j1 ≤ j ≤ j2 P exp( i θi hi (o, A1 , A2 )) P Ω = pθ (o|A , A ) = P 1 2 o exp( i θi hi (o, A , A )) (10) where the functions hi ∈ {0, 1} are model features and the θi are weights of the model features which can be trained by different algorithms (Malouf, 2002). 1 2 3.1 Reordering Example Extraction Algorithm The input for the algorithm is a bilingual corpus with high-precision word alignments. We obtain the word alignments using the way of Koehn et al. (2005). After running GIZA++ (Och and Ney, 524 This definition is similar to that of bilingual phrase except that there is no length limitation over block. A reordering example is a triple of (o, b1 , b2 ) where b1 and b2 are two neighbor blocks and o is the order between them. We define each vertex of block as corner. Each corner has four links in four directions: topright, topleft, bottomright, bot"
P06-1066,P05-1069,0,0.0165247,"Missing"
P06-1066,H05-1021,0,0.477653,"Missing"
P06-1066,P05-1033,0,0.930274,"exicalized reordering model that is phrase dependent. Lexicalized reordering model learns local orientations (monotone or non-monotone) with probabilities for each bilingual phrase from training data. During decoding, the model attempts to finding a Viterbi local orientation sequence. Performance gains have been reported for systems with lexicalized reordering model. However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause other problems, e.g. the data sparseness problem. Another smart reordering model was proposed by Chiang (2005). In his approach, phrases are reorganized into hierarchical ones by reducing subphrases to variables. This template-based scheme not only captures the reorderings of phrases, but also integrates some phrasal generalizations into the global model. In this paper, we propose a novel solution for phrasal reordering. Here, under the ITG constraint (Wu, 1997; Zens et al., 2004), we need to consider just two kinds of reorderings, straight and inverted between two consecutive blocks. Therefore reordering can be modelled as a problem of We propose a novel reordering model for phrase-based statistical"
P06-1066,zhang-etal-2004-interpreting,0,0.0749989,"Missing"
P06-1066,P96-1021,0,0.83763,"006. 2006 Association for Computational Linguistics classification with only two labels, straight and inverted. In this paper, we build a maximum entropy based classification model as the reordering model. Different from lexicalized reordering, we do not use the whole block as reordering evidence, but only features extracted from blocks. This is more flexible. It makes our model reorder any blocks, observed in training or not. The whole maximum entropy based reordering model is embedded inside a log-linear phrase-based model of translation. Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically. To create a maximum entropy based reordering model, the first step is learning reordering examples from training data, similar to the lexicalized reordering model. But in our way, any evidences of reorderings will be extracted, not limited to reorderings of bilingual phrases of length less than a predefined number of words. Secondly, features will be extracted from reordering examples according to feature templates. Finally, a maximum entropy classifier will be trained on the features. In"
P06-1066,J97-3002,0,0.946004,"ng model. However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause other problems, e.g. the data sparseness problem. Another smart reordering model was proposed by Chiang (2005). In his approach, phrases are reorganized into hierarchical ones by reducing subphrases to variables. This template-based scheme not only captures the reorderings of phrases, but also integrates some phrasal generalizations into the global model. In this paper, we propose a novel solution for phrasal reordering. Here, under the ITG constraint (Wu, 1997; Zens et al., 2004), we need to consider just two kinds of reorderings, straight and inverted between two consecutive blocks. Therefore reordering can be modelled as a problem of We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs). The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext. We present an algorithm to extract all reordering events of neighbor b"
P06-1066,2005.iwslt-1.8,0,\N,Missing
P06-1077,P02-1038,0,0.220937,") (6) D = X D = X D (1) P r(D|T (f1J ), f1J ) K Y P r(S˜k |T˜k ) (7) k=1 3 The notational convention will be as follows. We use the symbol P r(·) to denote general probability distribution with no specific assumptions. In contrast, for model-based probability distributions, we use generic symbol p(·). 2 We use T (·) to denote a parse tree. To reduce notational overhead, we use T (z) to represent the parse tree in z. Similarly, S(z) denotes the string in z. 610 中国 的 经济 发展 ⇒ X3 X4 of China parsing ⇒ economic X4 of China NP DNP ⇒ economic development of China NP NP DEG NN NN NR 的 经济 发展 Following Och and Ney (2002), we base our model on log-linear framework. Hence, all knowledge sources are described as feature functions that include the given source string f1J , the target string eI1 , and hidden variables. The hidden variable T (f1J ) is omitted because we usually make use of only single best output of a parser. As we assume that all detachment have the same probability, the hidden variable D is also omitted. As a result, the model we actually adopt for experiments is limited because the parse, detachment, and TAT application sub-models are simplified. 中国 detachment production NP DNP NP NP NP NP NR NN"
P06-1077,J00-1004,0,0.0168889,"to be observed on training data. However, a key limitation of phrase-based models is that they fail to model reordering at the phrase level robustly. Typically, phrase reordering is modeled in terms of offset positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information. Recent research on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert"
P06-1077,J04-4002,0,0.859996,"g both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models. 1 Introduction Phrase-based translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004), which go beyond the original IBM translation models (Brown et al., 1993) 1 by modeling translations of phrases rather than individual words, have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations. In phrase-based models, phrases are usually strings of adjacent words instead of syntactic constituents, excelling at capturing local reordering and performing translations that are localized to 1 The mathematical notation we use in this paper is taken from that paper: a source string f1J = f1 , . . . , fj , . . . , fJ is to be translated into a ta"
P06-1077,J93-2003,0,0.0130061,"Missing"
P06-1077,P03-1021,0,0.0395834,"Missing"
P06-1077,P02-1040,0,0.105269,"Missing"
P06-1077,P05-1034,0,0.674858,"ematical notation we use in this paper is taken from that paper: a source string f1J = f1 , . . . , fj , . . . , fJ is to be translated into a target string eI1 = e1 , . . . , ei , . . . , eI . Here, I is the length of the target string, and J is the length of the source string. 609 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 609–616, c Sydney, July 2006. 2006 Association for Computational Linguistics operations that transform a target parse tree into a source string. Paying more attention to source language analysis, Quirk et al. (2005) employ a source language dependency parser, a target language word segmentation component, and an unsupervised word alignment component to learn treelet translations from parallel corpus. In this paper, we propose a statistical translation model based on tree-to-string alignment template which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side"
P06-1077,P05-1033,0,0.68391,"earch on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both source and target languages. Another class of approaches make use of syntactic information in the target language alone, treat"
P06-1077,P05-1067,0,0.44874,"on as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both source and target languages. Another class of approaches make use of syntactic information in the target language alone, treating the translation problem as a parsing problem. Yamada and Knight (2001) use a parser in the target language to train probabilities on a set of We present a novel transla"
P06-1077,N04-1035,0,0.79996,"h low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. One advantage of our model is that TATs can be automatically acquired to capture linguistically motivated reordering at both low (word) and high (phrase, clause) levels. In addition, the training of TAT-based model is less computationally expensive than tree-to-tree models. Similarly to (Galley et al., 2004), the tree-to-string alignment templates discussed in this paper are actually transformation rules. The major difference is that we model the syntax of the source language instead of the target side. As a result, the task of our decoder is to find the best target string while Galley’s is to seek the most likely target tree. 2 Figure 1 shows three TATs automatically learned from training data. Note that when demonstrating a TAT graphically, we represent non-terminals in the target strings by blanks. NP NR NN 布什 总统 President Bush NP NR CC 美国 和 NP LC NR DNP 间 NP NP DEG between United States and F"
P06-1077,2005.eamt-1.36,0,0.0313335,"Missing"
P06-1077,N04-1014,0,0.0569363,"set positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information. Recent research on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both sour"
P06-1077,J97-3002,0,0.322061,"of Sciences No.6 Kexueyuan South Road, Haidian District P. O. Box 2704, Beijing, 100080, China {yliu,liuqun,sxlin}@ict.ac.cn Abstract substrings that are common enough to be observed on training data. However, a key limitation of phrase-based models is that they fail to model reordering at the phrase level robustly. Typically, phrase reordering is modeled in terms of offset positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information. Recent research on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are"
P06-1077,N03-1017,0,0.154587,"Missing"
P06-1077,I05-1007,1,0.458254,"Missing"
P06-1077,koen-2004-pharaoh,0,0.0144452,"ion economic development of China P r(eI1 , z1K |f1J ) Figure 2: Graphic illustration for translation process P I J K exp[ M m=1 λm hm (e1 , f1 , z1 )] =P PM J 0K 0I e0 I ,z 0 K exp[ m=1 λm hm (e 1 , f1 , z 1 )] 1 ˜ T˜), the tree-toTo further decompose P r(S| string alignment template, denoted by the variable z, is introduced as a hidden variable. = X X ˜ z|T˜) P r(S, (8) ˜ T˜) P r(z|T˜)P r(S|z, (9) ˜ T˜) = P r(S| eˆI1 1 = argmax X M eI1 ,z1K  λm hm (eI1 , f1J , z1K ) m=1 For our experiments we use the following seven feature functions 4 that are analogous to default feature set of Pharaoh (Koehn, 2004). To simplify the notation, we omit the dependence on the hidden variables of the model. z z Therefore, the TAT-based translation model can be decomposed into four sub-models: h1 (eI1 , f1J ) = log 1. parse model: P r(T (f1J )|f1J ) 2. detachment model: P r(D|T (f1J ), f1J ) h2 (eI1 , f1J ) = log 3. TAT selection model: P r(z|T˜) h3 (eI1 , f1J ) = log ˜ T˜) 4. TAT application model: P r(S|z, Figure 2 shows how TATs work to perform translation. First, the input source sentence is parsed. Next, the parse tree is detached into five subtrees with a preorder transversal. For each subtree, a TAT is"
P06-1077,P01-1067,0,0.778013,"es hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both source and target languages. Another class of approaches make use of syntactic information in the target language alone, treating the translation problem as a parsing problem. Yamada and Knight (2001) use a parser in the target language to train probabilities on a set of We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs t"
P06-1077,zhang-etal-2004-interpreting,0,0.0519163,"Missing"
P06-1077,P05-1057,1,0.629394,"urce side, bilingual phrases can be utilized for the TAT-based model to get further improvement. It should be emphasized that the restrictions we impose on TAT extraction limit the expressive power of TAT. Preliminary experiments reveal that removing these restrictions does improve translation quality, but leads to large memory requirements. We feel that both parsing and word alignment qualities have important effects on the TATbased model. We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al., 2005). 5.4 Using bilingual phrases It is interesting to use bilingual phrases to strengthen the TAT-based model. As we mentioned before, some useful non-syntactic phrase pairs can never be obtained in form of TAT because we restrict that there must be a corresponding parse tree for the source phrase. Moreover, it takes more time to obtain TATs than bilingual phrases on the same training data because parsing is usually very time-consuming. Given an input subtree T (Fjj12 ), if Fjj12 is a string of terminals, we find all bilingual phrases that the source phrase is equal to Fjj12 . Then we build a 0 0"
P06-1077,W02-1018,0,0.0149416,"rget string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models. 1 Introduction Phrase-based translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004), which go beyond the original IBM translation models (Brown et al., 1993) 1 by modeling translations of phrases rather than individual words, have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations. In phrase-based models, phrases are usually strings of adjacent words instead of syntactic constituents, excelling at capturing local reordering and performing translations that are localized to 1 The mathematical notation we use in this paper is taken from that paper: a source string f1J = f1 , . . . , fj ,"
P06-1077,P04-1083,0,0.00484775,"to model reordering at the phrase level robustly. Typically, phrase reordering is modeled in terms of offset positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information. Recent research on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches,"
P06-1077,P00-1056,0,0.750306,"Missing"
P06-1077,J08-3004,0,\N,Missing
P07-1089,P00-1056,0,0.239329,"Missing"
P07-1089,P02-1038,0,0.153368,"of rules used for each cell. There are two ways to limit the rule table size: by a fixed limit a of how many rules are retrieved for each cell, and by a probability threshold α that specify that the rule probability has to be above some value. Also, instead of keeping the full list of derivations for a cell, we store a top-scoring subset of the derivations. This can also be done by a fixed limit b or a threshold β. The subcell division array D, in which divisions containing forest cells have priority over those composed of only tree cells, is pruned by keeping only a-best divisions. Following Och and Ney (2002), we base our model on log-linear framework and adopt the seven feature functions described in (Liu et al., 2006). It is very important to balance the preference between conventional tree-to-string rules and the newlyintroduced forest-to-string and auxiliary rules. As the probabilities of auxiliary rules are not learnt from training data, we add a feature that sums up the 5 There are no default rules for forests because only tree-tostring rules are essential to tree-to-string translation models. 709 node count of auxiliary rules of a derivation to penalize the use of forest-to-string and auxil"
P07-1089,J04-4002,0,0.294023,"Missing"
P07-1089,P03-1021,0,0.253825,"Missing"
P07-1089,P02-1040,0,0.10786,"Missing"
P07-1089,P05-1033,0,0.439763,"Missing"
P07-1089,N04-1035,0,0.482432,"Missing"
P07-1089,P06-1121,0,0.480961,"Missing"
P07-1089,N03-1017,0,0.0750438,"hows the statistics of rules used in our experiments. We find that even though forest-to-string rules are introduced the total number (i.e. 73, 592) of lexicalized tree-to-string and forest-to-string rules is still far less than that (i.e. 251, 173) of bilingual phrases. This difference results from the restriction we impose in training that both the first and last symbols in the target string must be aligned to some source symbols. For the forest-to-string rules, partial lexicalized ones are in the majority. We compared our system Lynx against a freely available phrase-based decoder Pharaoh (Koehn et al., 2003). For Pharaoh, we set a = 20, α = 0, b = 100, β = 10−5 , and distortion limit dl = 4. For Lynx, we set a = 20, α = 0, b = 100, and β = 0. Two postprocessing procedures ran to improve the outputs of both systems: OOVs removal and recapitalization. Table 5 shows results on test set using Pharaoh and Lynx with different rule sets. Note that Lynx is capable of using only bilingual phrases plus de710 Forest-to-String Rule Set None L P U L+P+U BLEU4 0.2225 ± 0.0085 0.2297 ± 0.0081 0.2279 ± 0.0083 0.2270 ± 0.0087 0.2312 ± 0.0082 Table 6: Effect of lexicalized, partial lexicalized, and unlexicalized f"
P07-1089,P06-1077,1,0.81906,"Marcu et al. (2006) create a xRS rule headed by a pseudo, nonsyntactic nonterminal symbol that subsumes the phrase and corresponding multi-headed syntactic structure; and one sibling xRS rule that explains how the non-syntactic nonterminal symbol can be combined with other genuine nonterminals so as to obtain genuine parse trees. The name of the pseudo nonterminal is designed to reflect how the corresponding rule can be fully realized. However, they neglect alignment consistency when creating sibling rules. In addition, it is hard for the naming mechanism to deal with more complex phenomena. Liu et al. (2006) treat bilingual phrases as lexicalized TATs (Tree-to-string Alignment Template). A bilingual phrase can be used in decoding if the source phrase is subsumed by the input parse tree. Although this solution does help, only syntactic bilingual phrases are available to the TAT-based model. Moreover, it is problematic to combine the translation probabilities of bilingual phrases and TATs, which are estimated independently. In this paper, we propose forest-to-string rules which describe the correspondence between multiple parse trees and a string. They can not only capture non-syntactic phrase pair"
P07-1089,W06-1606,0,0.390951,"ax into statistical translation. On the other hand, the performance of linguistically syntax-based models can be hindered by making use of only syntactic phrase pairs. Studies reveal that linguistically syntax-based models are sensitive to syntactic analysis (Quirk and Corston-Oliver, 2006), which is still not reliable enough to handle real-world texts due to limited size and domain of training data. Various solutions are proposed to tackle the problem. Galley et al. (2004) handle non-constituent phrasal translation by traversing the tree upwards until reaches a node that subsumes the phrase. Marcu et al. (2006) argue that this choice is inap704 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 704–711, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics propriate because large applicability contexts are required. For a non-syntactic phrase pair, Marcu et al. (2006) create a xRS rule headed by a pseudo, nonsyntactic nonterminal symbol that subsumes the phrase and corresponding multi-headed syntactic structure; and one sibling xRS rule that explains how the non-syntactic nonterminal symbol can be combined with other genuine no"
P07-1089,W06-1608,0,0.0370969,"Missing"
P07-1089,P05-1034,0,0.369713,"Missing"
P07-1089,2005.eamt-1.36,0,0.0380564,"Missing"
P07-1089,I05-1007,1,0.276878,"Missing"
P07-1089,zhang-etal-2004-interpreting,0,0.0432704,"Missing"
P08-1023,P89-1018,0,0.664511,"re head (e) ∈ V is the consequent node in the deductive step, and tails(e) ∈ V ∗ is the list of antecedent nodes. For example, the hyperedge for deduction (*) is notated: h(NPB0,1 , CC1,2 , NPB2,3 ), NP0,3 i. There is also a distinguished root node TOP in each forest, denoting the goal item in parsing, which is simply S0,l where S is the start symbol and l is the sentence length. 3.2 Translation Forest 3.1 Parse Forest Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Billot and Lang, 1989). For 194 Given a parse forest and a translation rule set R, we can generate a translation forest which has a similar hypergraph structure. Basically, just as the depthfirst traversal procedure in tree-based decoding (Figure 2), we visit in top-down order each node v in the IP0,6 VP1,6 NP0,3 (a) PP1,3 P1,2 CC1,2 NPB0,1 VPB3,6 NPB2,3 VV3,4 AS4,5 NR2,3 NR0,1 Sh¯al´ong yˇu B`ush´ı NPB5,6 NN5,6 jˇux´ıng le hu`ıt´an ⇓ translation rule set R IP0,6 e1 e2 NP0,3 e5 (b) VP1,6 e4 e3 PP1,3 NPB0,1 (c) translation hyperedge e1 e2 e3 e4 e5 e6 CC1,2 r1 r6 r3 r7 r8 r9 P1,2 VPB3,6 NPB2,3 VV3,4 AS4,5 e6 NPB5,6 t"
P08-1023,P05-1033,0,0.909811,"tially many parses, which encodes many more alternatives than standard n-best lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary"
P08-1023,J07-2003,0,0.655297,"he subtranslations of the matched variable nodes will be substituted for the variables in s(r) to get a complete translation for node v. So a translation hyperedge e is a triple htails(e), head (e), si where s is the target string from the rule, for example, up the computation. An +LM item of node v has the form (v a⋆b ), where a and b are the target-language ⋆ Sharon boundary words. For example, (VP held ) is an 1,6 +LM item with its translation starting with “held” and ending with “Sharon”. This scheme can be easily extended to work with a general n-gram by storing n − 1 words at both ends (Chiang, 2007). For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM forest, resulting from the intersection of the translation forest and the LM, with its nodes being the +LM items during cube pruning. Although this new forest is prohibitively large, Algorithm 3 is very efficient with minimal overhead on top of 1-best. 3.4 Forest Pruning Algorithm We use the prun"
P08-1023,P05-1066,0,0.13289,"Missing"
P08-1023,P05-1067,0,0.390349,"1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and"
P08-1023,N04-1035,0,0.515361,"ide tree, whose internal nodes are labeled by nonterminal symbols in N , and whose frontier nodes are labeled by source-side terminals in Σ or variables from a set X = {x1 , x2 , . . .}; s ∈ (X ∪ ∆)∗ is the target-side string where ∆ is the target language terminal set; and φ is a mapping from X to nonterminals in N . Each variable xi ∈ X occurs exactly once in t and exactly once in s. We denote R to be the translation rule set. A similar formalism appears in another form in (Liu et al., 2006). These rules are in the reverse direction of the original string-to-tree transducer rules defined by Galley et al. (2004). Finally, from step (d) we apply rules r4 and r5 example, consider the Chinese sentence in Example (2) above, which has (at least) two readings depending on the part-of-speech of the word yˇu, which can be either a preposition (P “with”) or a conjunction (CC “and”). The parse tree for the preposition case is shown in Figure 2(b) as the 1-best parse, while for the conjunction case, the two proper nouns (B`ush´ı and Sh¯al´ong) are combined to form a coordinated NP NPB0,1 CC1,2 NP0,3 NPB2,3 (*) which functions as the subject of the sentence. In this case the Chinese sentence is translated into ("
P08-1023,P06-1121,0,0.50156,"rses, which encodes many more alternatives than standard n-best lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as"
P08-1023,W05-1506,1,0.816055,"ize. The pruned parse forest will then be used to direct the translation. In the decoding step, we first convert the parse forest into a translation forest using the translation rule set, by similar techniques of pattern-matching from tree-based decoding (Section 3.2). Then the decoder searches for the best derivation on the translation forest and outputs the target string (Section 3.3). Shown in Figure 3(a), these two parse trees can be represented as a single forest by sharing common subtrees such as NPB0,1 and VPB3,6 . Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like NP0,3 are called nodes, and deductive steps like (*) correspond to hyperedges. More formally, a forest is a pair hV, Ei, where V is the set of nodes, and E the set of hyperedges. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form of X i,j , which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, wi+1 . . . wj ). Each hyperedge e ∈ E is a pair htails(e), head (e)i, where head (e) ∈ V is the consequent node in the deductive step, and tails(e) ∈ V ∗ is the list of antecedent nodes. For example, the hypered"
P08-1023,P07-1019,1,0.473411,"or each node v, and then compute the merit αβ(e) for each hyperedge: X β(ui ) (4) αβ(e) = α(head (e)) + ui ∈tails(e) e3 = h(NPB2,3 , NPB5,6 ), VP1,6 , “held x2 with x1 ”i. This procedure is summarized in Pseudocode 1. 3.3 Decoding Algorithms The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and k-best search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing (Huang and Chiang, 2005). For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Huang and Chiang (2005) to speed 196 Intuitively, this merit is the cost of the best derivation that traverses e, and the difference δ(e) = αβ(e) − β(TOP) can be seen as the distance away from the globally best derivation. We prune away a hyperedge e if δ(e) &gt; p for a threshold p. Nodes with all incoming hyperedges pruned are also pruned. 4 Experiments We can extend the s"
P08-1023,2006.amta-papers.8,1,0.895534,"decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2"
P08-1023,P08-1067,1,0.711475,"languages without enough Treebank data to train a high-accuracy parser. One obvious solution to this problem is to take as input k-best parses, instead of a single tree. This kbest list postpones some disambiguation to the decoder, which may recover from parsing errors by getting a better translation from a non 1-best parse. However, a k-best list, with its limited scope, often has too few variations and too many redundancies; for example, a 50-best list typically encodes a combination of 5 or 6 binary ambiguities (since 25 < 50 < 26 ), and many subtrees are repeated across different parses (Huang, 2008). It is thus inefficient either to decode separately with each of these very similar trees. Longer sentences will also aggravate this situation as the number of parses grows exponentially with the sentence length. We instead propose a new approach, forest-based translation (Section 3), where the decoder translates a packed forest of exponentially many parses,1 1 There has been some confusion in the MT literature regarding the term forest: the word “forest” in “forest-to-string rules” 192 Proceedings of ACL-08: HLT, pages 192–199, c Columbus, Ohio, USA, June 2008. 2008 Association for Computati"
P08-1023,W01-1812,0,0.351606,"duce it to a reasonable size. The pruned parse forest will then be used to direct the translation. In the decoding step, we first convert the parse forest into a translation forest using the translation rule set, by similar techniques of pattern-matching from tree-based decoding (Section 3.2). Then the decoder searches for the best derivation on the translation forest and outputs the target string (Section 3.3). Shown in Figure 3(a), these two parse trees can be represented as a single forest by sharing common subtrees such as NPB0,1 and VPB3,6 . Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like NP0,3 are called nodes, and deductive steps like (*) correspond to hyperedges. More formally, a forest is a pair hV, Ei, where V is the set of nodes, and E the set of hyperedges. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form of X i,j , which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, wi+1 . . . wj ). Each hyperedge e ∈ E is a pair htails(e), head (e)i, where head (e) ∈ V is the consequent node in the deductive step, and tails(e) ∈ V ∗ is the list of antecedent nodes."
P08-1023,N03-1017,0,0.0444445,"babilities of each parse hyperedge ep covered in the pattern-match of t against Hp (which can be recorded at conversion time): Y P(t |Hp ) = P(ep ). (8) ep ∈Hp , ep covered by t 4.1 Data preparation Our experiments are on Chinese-to-English translation, and we use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Following Huang (2008), we modify the parser to output a packed forest for each sentence. Our training corpus consists of 31,011 sentence pairs with 0.8M Chinese words and 0.9M English words. We first word-align them by GIZA++ refined by “diagand” from Koehn et al. (2003), and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006), which resulted in 346K translation rules. Note that our rule extraction is still done on 1-best parses, while decoding is on k-best parses or packed forests. We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram language model with Kneser-Ney smoothing on the English side of the bitext. We use the 2002 NIST MT Evaluation test set as our development set (878 sentences) and the 2005 197 BLEU score on target translation. The derivation probability conditioned on 1-best tree"
P08-1023,koen-2004-pharaoh,0,0.0609331,"Missing"
P08-1023,C04-1090,0,0.192459,"ts over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free gram"
P08-1023,P06-1077,1,0.882144,"oints higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the la"
P08-1023,P07-1089,1,0.746035,"nguage string among all possible derivations D: ∗ d = arg max P(d|T ). d∈D À Æ (d) Bush (1) We will now proceed with a running example translating from Chinese to English: (2) r2 ⇓  &gt;L   B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an Bush with/and Sharon1 hold pass. talk2 “Bush held a talk2 with Sharon1 ” Figure 2 shows how this process works. The Chinese sentence (a) is first parsed into tree (b), which will be converted into an English string in 5 steps. First, at the root node, we apply rule r1 preserving top-level word-order between English and Chinese, (r1 ) IP(x1 :NPB x2 :VP) → x1 x2 (Liu et al., 2007) was a misnomer which actually refers to a set of several unrelated subtrees over disjoint spans, and should not be confused with the standard concept of packed forest. 193 hu`ıt´an r3 ⇓ held NPB with NN NR hu`ıt´an Sh¯al´ong r4 ⇓ (e) Bush NPB [held a talk]2 r5 ⇓ [with Sharon]1 Figure 2: An example derivation of tree-to-string translation. Shaded regions denote parts of the tree that is pattern-matched with the rule being applied. which results in two unfinished subtrees in (c). Then rule r2 grabs the B`ush´ı subtree and transliterate it (r2 ) NPB(NR(B`ush´ı)) → Bush. Similarly, rule r3 shown"
P08-1023,P03-1021,0,0.0947585,"bilities of translation rules r ∈ d: Y P(r) (6) P(d |Hp ) = 0.250 0.248 0.246 0.244 0.242 0.240 0.238 0.236 0.234 0.232 0.230 p=12 p=5 k=30 k=100 k=10 1-best k-best trees forests decoding 0 5 10 15 20 25 30 35 average decoding time (secs/sentence) Figure 4: Comparison of decoding on forests with decoding on k-best trees. NIST MT Evaluation test set as our test set (1082 sentences), with on average 28.28 and 26.31 words per sentence, respectively. We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002). We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on the dev set. On dev and test sets, we prune the Chinese parse forests by the forest pruning algorithm in Section 3.4 with a threshold of p = 12, and then convert them into translation forests using the algorithm in Section 3.2. To increase the coverage of the rule set, we also introduce a default translation hyperedge for each parse hyperedge by monotonically translating each tail node, so that we can always at least get a complete translation in the end. 4.2 Results The BLEU score of the baseline 1-best decoding is 0.2325, wh"
P08-1023,P02-1040,0,0.104735,"where Hp is the parse forest, which decomposes into the product of probabilities of translation rules r ∈ d: Y P(r) (6) P(d |Hp ) = 0.250 0.248 0.246 0.244 0.242 0.240 0.238 0.236 0.234 0.232 0.230 p=12 p=5 k=30 k=100 k=10 1-best k-best trees forests decoding 0 5 10 15 20 25 30 35 average decoding time (secs/sentence) Figure 4: Comparison of decoding on forests with decoding on k-best trees. NIST MT Evaluation test set as our test set (1082 sentences), with on average 28.28 and 26.31 words per sentence, respectively. We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002). We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on the dev set. On dev and test sets, we prune the Chinese parse forests by the forest pruning algorithm in Section 3.4 with a threshold of p = 12, and then convert them into translation forests using the algorithm in Section 3.2. To increase the coverage of the rule set, we also introduce a default translation hyperedge for each parse hyperedge by monotonically translating each tail node, so that we can always at least get a complete translation in the end. 4.2 Results"
P08-1023,W06-1608,0,0.0649947,"ne Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2006). However, despite these advantages, current tree-based systems suffer from a major drawback: they only use the 1best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). This situation becomes worse with resource-poor source languages without enough Treebank data to train a high-accuracy parser. One obvious solution to this problem is to take as input k-best parses, instead of a single tree. This kbest list postpones some disambiguation to the decoder, which may recover from parsing errors by getting a better translation from a non 1-best parse. However, a k-best list, with its limited scope, often has too few variations and too many redundancies; for example, a 50-best list typically encodes a combination of 5 or 6 binary ambiguities (since 25 < 50 < 26 ),"
P08-1023,P05-1034,0,0.485947,"result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution"
P08-1023,J97-3002,0,0.0539771,"of exponentially many parses, which encodes many more alternatives than standard n-best lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not re"
P08-1023,I05-1007,1,0.648768,"of rule r, respectively, P(t |s) and P(s |t) are the two translation probabilities, and Plex (·) are the lexical probabilities. The only extra term in forest-based decoding is P(t |Hp ) denoting the source side parsing probability of the current translation rule r in the parse forest, which is the product of probabilities of each parse hyperedge ep covered in the pattern-match of t against Hp (which can be recorded at conversion time): Y P(t |Hp ) = P(ep ). (8) ep ∈Hp , ep covered by t 4.1 Data preparation Our experiments are on Chinese-to-English translation, and we use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Following Huang (2008), we modify the parser to output a packed forest for each sentence. Our training corpus consists of 31,011 sentence pairs with 0.8M Chinese words and 0.9M English words. We first word-align them by GIZA++ refined by “diagand” from Koehn et al. (2003), and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006), which resulted in 346K translation rules. Note that our rule extraction is still done on 1-best parses, while decoding is on k-best parses or packed forests. We also use the SRI Language Mo"
P08-1023,N06-1033,1,0.506875,"tems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2006). However, despite these advantages, current tree-based systems suffer from a major drawback: they only use the 1best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). This situation becomes worse with resource-poor source languages without enough Treebank data to train a high-accuracy parser. One obvious solution to this problem is"
P08-1102,W02-1001,0,0.671743,"Introduction Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages. Several models were introduced for these problems, for example, the Hidden Markov Model (HMM) (Rabiner, 1989), Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), and Conditional Random Fields (CRFs) (Lafferty et al., 2001). CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM, and usually behaves the best in the two tasks. Another widely used discriminative method is the perceptron algorithm (Collins, 2002), which achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron. Qun Liu † ‡ Yajuan Lu¨ † Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T). Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a bounda"
P08-1102,P06-1043,0,0.0212833,"Missing"
P08-1102,W04-3236,0,0.625941,"Lu¨ † Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T). Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004). Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004). Besides the usual character-based features, additional features dependent on POS’s or words can also be employed to improve the performance. However, as such features are generated dynamically during the decoding procedure, two limitation arise: on the one hand, the amount of parameters increases rapidly, which is apt to overfit on training corpus; on the other hand, exact inference by dynamic programming is intractable"
P08-1102,J04-4002,0,0.00572952,"ever, as such features are generated dynamically during the decoding procedure, two limitation arise: on the one hand, the amount of parameters increases rapidly, which is apt to overfit on training corpus; on the other hand, exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications. As a result, many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently. To cope with this problem, we propose a cascaded linear model inspired by the log-linear model (Och and Ney, 2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources. Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models. We 897 Proceedings of ACL-08: HLT, pages 897–904, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics f1 f2 f|R| α ~ Core Linear Model (Perceptron) P g1 = i αi × fi g1 Word LM: g2 = Pwlm (W ) g2 POS LM: g3 = Ptlm (T ) g3 Labelling: g4 = P (T |W ) g4 Generating: g5 = P (W |T ) g5 Length: g6"
P08-1102,P03-1021,0,0.00342095,"Missing"
P08-1102,W03-1728,0,0.0484926,"achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron. Qun Liu † ‡ Yajuan Lu¨ † Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T). Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004). Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004). Besides the usual character-based features, additional features dependent on POS’s or words can also be employed to improve the performance. However, as such features are generated dynamically during the decoding procedure, two limitation arise: on the one hand, the amount of parameters increases ra"
P08-1102,P07-1106,0,0.142969,") C−1 C1 Lexical-target C0 Cn (n = −2..2) C0 Cn Cn+1 (n = −2..1) C0 C−1 C1 Instances C−2 =e, C−1 = , C0 =U, C1 =/, C2 =¡ C−2 C−1 =e , C−1 C0 = U, C0 C1 =U/, C1 C2 =/¡ C−1 C1 = / Instances C0 C−2 =Ue, C0 C−1 =U , C0 C0 =UU, C0 C1 =U/, C0 C2 =U¡ C0 C−2 C−1 =Ue , C0 C−1 C0 =U U, C0 C0 C1 =UU/, C0 C1 C2 =U/¡ C0 C−1 C1 = U / Table 1: Feature templates and instances. Suppose we are considering the third character ”U” in ”e U /¡”. to CRFs, while with much faster training. The perceptron has been used in many NLP tasks, such as POS tagging (Collins, 2002), Chinese word segmentation (Ng and Low, 2004; Zhang and Clark, 2007) and so on. We trained a character-based perceptron for Chinese Joint S&T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T. In following subsections, we describe the feature templates and the perceptron training algorithm. 3.1 Feature Templates The feature templates we adopted are selected from those of Ng and Low (2004). To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, P u(C0 ), indicating whether character C0 is a punctuation. All feature templates and their ins"
P08-2041,N06-1003,0,0.0235949,"ual phrases cannot be fully matched. In this paper, we address the problem of translation of unseen phrases, the source phrases that are not observed in the training corpus. The alignment template model (Och and Ney, 2004) enhanced phrasal generalizations by using words classes rather than the words themselves. But the phrases are overly generalized. The hierarchical phrase-based model (Chiang, 2005) used hierarchical phrase pairs to strengthen the generalization ability of phrases and allow long distance reorderings. However, the huge grammar table greatly increases computational complexity. Callison-Burch et al. (2006) used paraphrases of the trainig corpus for translating unseen phrases. But they only found and used the semantically similar phrases. Another method is to use multi-parallel corpora (Cohn and Lapata, 2007; Utiyama and Isahara, 2007) to improve phrase coverage and translation quality. This paper presents a partial matching strategy for translating unseen phrases. When encountering unseen phrases in a source sentence, we search partially matched phrase pairs from the phrase table. Then we keep the translations of the matched part and translate the unmatched part by word substitution. The advant"
P08-2041,P05-1033,0,0.063652,"ne hand, the data sparseness problem is serious, especially when the amount of the bilingual data is limited. On the other hand, for a certain source text, the phrase table is redundant since most of the bilingual phrases cannot be fully matched. In this paper, we address the problem of translation of unseen phrases, the source phrases that are not observed in the training corpus. The alignment template model (Och and Ney, 2004) enhanced phrasal generalizations by using words classes rather than the words themselves. But the phrases are overly generalized. The hierarchical phrase-based model (Chiang, 2005) used hierarchical phrase pairs to strengthen the generalization ability of phrases and allow long distance reorderings. However, the huge grammar table greatly increases computational complexity. Callison-Burch et al. (2006) used paraphrases of the trainig corpus for translating unseen phrases. But they only found and used the semantically similar phrases. Another method is to use multi-parallel corpora (Cohn and Lapata, 2007; Utiyama and Isahara, 2007) to improve phrase coverage and translation quality. This paper presents a partial matching strategy for translating unseen phrases. When enco"
P08-2041,P07-1092,0,0.0122814,"Ney, 2004) enhanced phrasal generalizations by using words classes rather than the words themselves. But the phrases are overly generalized. The hierarchical phrase-based model (Chiang, 2005) used hierarchical phrase pairs to strengthen the generalization ability of phrases and allow long distance reorderings. However, the huge grammar table greatly increases computational complexity. Callison-Burch et al. (2006) used paraphrases of the trainig corpus for translating unseen phrases. But they only found and used the semantically similar phrases. Another method is to use multi-parallel corpora (Cohn and Lapata, 2007; Utiyama and Isahara, 2007) to improve phrase coverage and translation quality. This paper presents a partial matching strategy for translating unseen phrases. When encountering unseen phrases in a source sentence, we search partially matched phrase pairs from the phrase table. Then we keep the translations of the matched part and translate the unmatched part by word substitution. The advantage of our approach is that we alleviate the data sparseness problem without increasing the amount of bilingual corpus. Moreover, the partially matched phrases are not necessarily synonymous. We incorporat"
P08-2041,N03-1017,0,0.00463535,"or phrase-based statistical machine translation (PBSMT). Source phrases which do not appear in the training corpus can be translated by word substitution according to partially matched phrases. The advantage of this method is that it can alleviate the data sparseness problem if the amount of bilingual corpus is limited. We incorporate our approach into the state-of-the-art PBSMT system Moses and achieve statistically significant improvements on both small and large corpora. 1 Introduction Currently, most of the phrase-based statistical machine translation (PBSMT) models (Marcu and Wong, 2002; Koehn et al., 2003) adopt full matching strategy for phrase translation, which means that a phrase pair (fe, ee) can be used for translating a source phrase f¯, only if fe = f¯. Due to lack of generalization ability, the full matching strategy has some limitations. On one hand, the data sparseness problem is serious, especially when the amount of the bilingual data is limited. On the other hand, for a certain source text, the phrase table is redundant since most of the bilingual phrases cannot be fully matched. In this paper, we address the problem of translation of unseen phrases, the source phrases that are no"
P08-2041,2002.tmi-tutorials.2,0,0.0138982,"al matching strategy for phrase-based statistical machine translation (PBSMT). Source phrases which do not appear in the training corpus can be translated by word substitution according to partially matched phrases. The advantage of this method is that it can alleviate the data sparseness problem if the amount of bilingual corpus is limited. We incorporate our approach into the state-of-the-art PBSMT system Moses and achieve statistically significant improvements on both small and large corpora. 1 Introduction Currently, most of the phrase-based statistical machine translation (PBSMT) models (Marcu and Wong, 2002; Koehn et al., 2003) adopt full matching strategy for phrase translation, which means that a phrase pair (fe, ee) can be used for translating a source phrase f¯, only if fe = f¯. Due to lack of generalization ability, the full matching strategy has some limitations. On one hand, the data sparseness problem is serious, especially when the amount of the bilingual data is limited. On the other hand, for a certain source text, the phrase table is redundant since most of the bilingual phrases cannot be fully matched. In this paper, we address the problem of translation of unseen phrases, the sourc"
P08-2041,J04-4002,0,0.0272274,"that a phrase pair (fe, ee) can be used for translating a source phrase f¯, only if fe = f¯. Due to lack of generalization ability, the full matching strategy has some limitations. On one hand, the data sparseness problem is serious, especially when the amount of the bilingual data is limited. On the other hand, for a certain source text, the phrase table is redundant since most of the bilingual phrases cannot be fully matched. In this paper, we address the problem of translation of unseen phrases, the source phrases that are not observed in the training corpus. The alignment template model (Och and Ney, 2004) enhanced phrasal generalizations by using words classes rather than the words themselves. But the phrases are overly generalized. The hierarchical phrase-based model (Chiang, 2005) used hierarchical phrase pairs to strengthen the generalization ability of phrases and allow long distance reorderings. However, the huge grammar table greatly increases computational complexity. Callison-Burch et al. (2006) used paraphrases of the trainig corpus for translating unseen phrases. But they only found and used the semantically similar phrases. Another method is to use multi-parallel corpora (Cohn and L"
P08-2041,P03-1021,0,0.016373,"Moses. We use the ICTCLAS toolkit2 to perform Chinese word segmentation and POS tagging. The training script of Moses is used to train the bilingual corpus. We set the maximum length of the source phrase to 7, and record word alignment information in the phrase table. For the language model, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of the Gigaword corpus. To run the decoder, we set ttable-limit=20, distortion-limit=6, stack=100. The translation quality is evaluated by BLEU-4 (case-sensitive). We perform minimum-error-rate training (Och, 2003) to tune the feature weights of the translation model to maximize the BLEU score on development set. 2 http://www.nlp.org.cn/project/project.php?proj id=6 163 α BLEU 1.0 24.44 0.7 24.43 0.5 24.86 0.3 25.31 0.1 25.13 Table 1: Effect of matching threshold on BLEU score. 3.1 Small-scale Task Table 1 shows the effect of matching threshold on translation quality. The baseline uses full matching (α=1.0) for phrase translation and achieves a BLEU score of 24.44. With the decrease of the matching threshold, the BLEU scores increase. when α=0.3, the system obtains the highest BLEU score of 25.31, which"
P08-2041,N07-1061,0,0.0205331,"asal generalizations by using words classes rather than the words themselves. But the phrases are overly generalized. The hierarchical phrase-based model (Chiang, 2005) used hierarchical phrase pairs to strengthen the generalization ability of phrases and allow long distance reorderings. However, the huge grammar table greatly increases computational complexity. Callison-Burch et al. (2006) used paraphrases of the trainig corpus for translating unseen phrases. But they only found and used the semantically similar phrases. Another method is to use multi-parallel corpora (Cohn and Lapata, 2007; Utiyama and Isahara, 2007) to improve phrase coverage and translation quality. This paper presents a partial matching strategy for translating unseen phrases. When encountering unseen phrases in a source sentence, we search partially matched phrase pairs from the phrase table. Then we keep the translations of the matched part and translate the unmatched part by word substitution. The advantage of our approach is that we alleviate the data sparseness problem without increasing the amount of bilingual corpus. Moreover, the partially matched phrases are not necessarily synonymous. We incorporate the partial matching metho"
P08-2041,zhang-etal-2004-interpreting,0,\N,Missing
P09-1059,W06-2920,0,0.0564951,"Missing"
P09-1059,J07-2003,0,0.0113011,"Missing"
P09-1059,P04-1015,0,0.0199976,"ining Algorithm and Features Input: Training examples (xi , yi ) α ~ ←0 for t ← 1 .. T do for i ← 1 .. N do zi ← argmaxz∈GEN(xi ) Φ(xi , z) · α ~ if zi 6= yi then α ~ ←α ~ + Φ(xi , yi ) − Φ(xi , zi ) Output: Parameters α ~ Now we will show the training algorithm of the classifier and the features used. Several classification models can be adopted here, however, we choose the averaged perceptron algorithm (Collins, 2002) because of its simplicity and high accuracy. It is an online training algorithm and has been successfully used in many NLP tasks, such as POS tagging (Collins, 2002), parsing (Collins and Roark, 2004), Chinese word segmentation (Zhang and Clark, 2007; Jiang et al., 2008), and so on. Similar to the situation in other sequence labeling problems, the training procedure is to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results. Following Collins, we use a function GEN(x) enumerating the candidate results of an input x , a representation Φ mapping each training example (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd , and a parameter vector α ~ ∈ Rd corresponding to th"
P09-1059,W02-1001,0,0.343902,"e2 .. Cem−1 +1:em where each subsequence Ci:j indicates a Chinese word spanning from characters Ci to Cj (both in523 Algorithm 1 Perceptron training algorithm. 1: 2: 3: 4: 5: 6: 7: 8: 2.2 Training Algorithm and Features Input: Training examples (xi , yi ) α ~ ←0 for t ← 1 .. T do for i ← 1 .. N do zi ← argmaxz∈GEN(xi ) Φ(xi , z) · α ~ if zi 6= yi then α ~ ←α ~ + Φ(xi , yi ) − Φ(xi , zi ) Output: Parameters α ~ Now we will show the training algorithm of the classifier and the features used. Several classification models can be adopted here, however, we choose the averaged perceptron algorithm (Collins, 2002) because of its simplicity and high accuracy. It is an online training algorithm and has been successfully used in many NLP tasks, such as POS tagging (Collins, 2002), parsing (Collins and Roark, 2004), Chinese word segmentation (Zhang and Clark, 2007; Jiang et al., 2008), and so on. Similar to the situation in other sequence labeling problems, the training procedure is to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results. Following Collins, we use a function GEN(x)"
P09-1059,P07-1033,0,0.0655148,"Missing"
P09-1059,J93-2004,0,0.0333518,"Missing"
P09-1059,P08-1102,1,0.732523,"Missing"
P09-1059,W00-1201,0,0.0159542,"Missing"
P09-1059,D08-1017,0,0.0491519,"source standard to target standard. This regularity is incorporated together with the knowledge learnt from the target corpus itself, so as to obtain enhanced predication accuracy. For a given un-classified character sequence, the decoding is analogous to the training. First, the character sequence is input into the source classifier to obtain an source standard annotated classification result, then it is input into the target classifier with this classification result as additional information to get the final result. This coincides with the stacking method for combining dependency parsers (Martins et al., 2008; Nivre and McDonsource classifier source annotation classification result target classifier target annotation classification result Figure 3: The pipeline for decoding. ald, 2008), and is also similar to the Pred baseline for domain adaptation in (Daum´e III and Marcu, 2006; Daum´e III, 2007). Figures 2 and 3 show the flow charts for training and decoding. The utilization of the source classifier’s classification result as additional guide information resorts to the introduction of new features. For the current considering character waiting for classification, the most intuitive guide feature"
P09-1059,W06-1615,0,0.051839,"3) the HPSG LinGo Redwoods Treebank (Oepen et al., 2002), and a smaller dependency treebank (Buchholz and Marsi, 2006). A second, related problem is that the raw texts are also drawn from different domains, which for the above example range from financial news (PTB/WSJ) to transcribed dialog (LinGo). These two problems seem be a great waste in human efforts, and it would be nice if one could automatically adapt from one annotation standard and/or domain to another in order to exploit much larger datasets for better training. The second problem, domain adaptation, is very well-studied, e.g. by Blitzer et al. (2006) and Daum´e III (2007) (and see below for discussions), so in this paper we focus on the less studied, but equally important problem of annotationstyle adaptation. Introduction Much of statistical NLP research relies on some sort of manually annotated corpora to train their models, but these resources are extremely expensive to build, especially at a large scale, for example in treebanking (Marcus et al., 1993). However the linguistic theories underlying these annotation efforts are often heavily debated, and as a result there often exist multiple corpora for the same task with vastly differen"
P09-1059,E06-1011,0,0.0211674,"Missing"
P09-1059,J95-4004,0,0.212485,"e source classifier’s output, guide features can also be the classification results of several successive characters. We leave them as future research. 4 Table 2: An example of basic features and guide features of standard-adaptation for word segmentation. Suppose we are considering the third character “o” in “{B o Úu”. annotation-styles. Gao et al. (2004) described a transformationbased converter to transfer a certain annotationstyle word segmentation result to another style. They design some class-type transformation templates and use the transformation-based errordriven learning method of Brill (1995) to learn what word delimiters should be modified. However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging, not to mention other structure labeling tasks. Moreover, the processing procedure is divided into two isolated steps, conversion after segmentation, which suffers from error propagation and wastes the knowledge in the corpora. On the contrary, our strategy is automatic, generalizable and effective. Related Works Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training impr"
P09-1059,W04-3236,0,0.320016,"tor Φ(x, y) ∈ Rd , and a parameter vector α ~ ∈ Rd corresponding to the feature vector. For an input character sequence x, we aim to find an output F (x) that satisfies: clusive). While in Joint S&T, each word is further annotated with a POS tag: C1:e1 /t1 Ce1 +1:e2 /t2 .. Cem−1 +1:em /tm where tk (k = 1..m) denotes the POS tag for the word Cek−1 +1:ek . 2.1 Character Classification Method Xue and Shen (2003) describe for the first time the character classification approach for Chinese word segmentation, where each character is given a boundary tag denoting its relative position in a word. In Ng and Low (2004), Joint S&T can also be treated as a character classification problem, where a boundary tag is combined with a POS tag in order to give the POS information of the word containing these characters. In addition, Ng and Low (2004) find that, compared with POS tagging after word segmentation, Joint S&T can achieve higher accuracy on both segmentation and POS tagging. This paper adopts the tag representation of Ng and Low (2004). For word segmentation only, there are four boundary tags: F (x) = argmax Φ(x, y) · α ~ (1) y∈GEN(x) where Φ(x, y)· α ~ denotes the inner product of feature vector Φ(x, y)"
P09-1059,P08-1108,0,0.0403739,"transformation-based errordriven learning method of Brill (1995) to learn what word delimiters should be modified. However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging, not to mention other structure labeling tasks. Moreover, the processing procedure is divided into two isolated steps, conversion after segmentation, which suffers from error propagation and wastes the knowledge in the corpora. On the contrary, our strategy is automatic, generalizable and effective. Related Works Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers. The co-training technology lets two different parsing models learn from each other during parsing an unlabelled corpus: one model selects some unlabelled sentences it can confidently parse, and provide them to the other model as additional training corpus in order to train more powerful parsers. The classifier combination lets graph-based and transition-based dependency parsers to utilize the features extracted from each other’s parsing results, to obtain combined, enhanced parsers. The two technologies aim to let two models learn"
P09-1059,C02-2025,0,0.0196693,"Missing"
P09-1059,N01-1023,0,0.0304051,"type transformation templates and use the transformation-based errordriven learning method of Brill (1995) to learn what word delimiters should be modified. However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging, not to mention other structure labeling tasks. Moreover, the processing procedure is divided into two isolated steps, conversion after segmentation, which suffers from error propagation and wastes the knowledge in the corpora. On the contrary, our strategy is automatic, generalizable and effective. Related Works Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers. The co-training technology lets two different parsing models learn from each other during parsing an unlabelled corpus: one model selects some unlabelled sentences it can confidently parse, and provide them to the other model as additional training corpus in order to train more powerful parsers. The classifier combination lets graph-based and transition-based dependency parsers to utilize the features extracted from each other’s parsing results, to obtain combined, enhanced par"
P09-1059,I05-1007,1,0.604558,"Missing"
P09-1059,W03-1728,0,0.0997578,"corresponding labelled results. Following Collins, we use a function GEN(x) enumerating the candidate results of an input x , a representation Φ mapping each training example (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd , and a parameter vector α ~ ∈ Rd corresponding to the feature vector. For an input character sequence x, we aim to find an output F (x) that satisfies: clusive). While in Joint S&T, each word is further annotated with a POS tag: C1:e1 /t1 Ce1 +1:e2 /t2 .. Cem−1 +1:em /tm where tk (k = 1..m) denotes the POS tag for the word Cek−1 +1:ek . 2.1 Character Classification Method Xue and Shen (2003) describe for the first time the character classification approach for Chinese word segmentation, where each character is given a boundary tag denoting its relative position in a word. In Ng and Low (2004), Joint S&T can also be treated as a character classification problem, where a boundary tag is combined with a POS tag in order to give the POS information of the word containing these characters. In addition, Ng and Low (2004) find that, compared with POS tagging after word segmentation, Joint S&T can achieve higher accuracy on both segmentation and POS tagging. This paper adopts the tag rep"
P09-1059,P07-1106,0,0.224874,"(xi , yi ) α ~ ←0 for t ← 1 .. T do for i ← 1 .. N do zi ← argmaxz∈GEN(xi ) Φ(xi , z) · α ~ if zi 6= yi then α ~ ←α ~ + Φ(xi , yi ) − Φ(xi , zi ) Output: Parameters α ~ Now we will show the training algorithm of the classifier and the features used. Several classification models can be adopted here, however, we choose the averaged perceptron algorithm (Collins, 2002) because of its simplicity and high accuracy. It is an online training algorithm and has been successfully used in many NLP tasks, such as POS tagging (Collins, 2002), parsing (Collins and Roark, 2004), Chinese word segmentation (Zhang and Clark, 2007; Jiang et al., 2008), and so on. Similar to the situation in other sequence labeling problems, the training procedure is to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results. Following Collins, we use a function GEN(x) enumerating the candidate results of an input x , a representation Φ mapping each training example (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd , and a parameter vector α ~ ∈ Rd corresponding to the feature vector. For an input character sequence"
P09-1059,P05-1012,0,\N,Missing
P09-1059,P04-1059,0,\N,Missing
P09-1059,J07-3004,0,\N,Missing
P09-1063,N03-1017,0,0.0413288,"Missing"
P09-1063,P07-2045,0,0.00662405,"Missing"
P09-1063,P06-1077,1,0.717464,"ding target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. 558 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558–566, c Suntec, Singapore, 2-7 August 2009."
P09-1063,P05-1022,0,0.0179469,"h translation. The training corpus contains 840K Chinese words and 950K English words. A trigram language model was trained on the English sentences of the training corpus. We used the 2002 NIST MT Evaluation test set as our development set, and used the 2005 NIST MT Evaluation test set as our test set. We evaluated the translation quality using the BLEU metric, as calculated by mteval-v11b.pl with its default setting except that we used case-insensitive matching of n-grams. To obtain packed forests, we used the Chinese parser (Xiong et al., 2005) modified by Haitao Mi and the English parser (Charniak and Johnson, 2005) modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned away if the difference is greater than a threshold p. Nodes with all incoming hyperedges pruned are also pruned. The greater the threshold p is, p(ts ) × α(root(ts )) × v∈leaves(ts ) β(v) c(r) = β(¯ vs ) Q p(tt ) × α(root(tt"
P09-1063,J07-2003,0,0.588754,"ncies in two directions, lexical weights in two directions, number of rules used, language model score, number of target words produced, and the probability of matched source tree (Mi et al., 2008). Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al. (2008) to produce a translation forest. The translation forest has a similar hypergraph structure. While the nodes are the same as those of the parse forest, each hyperedge is associated with an STSG rule. Then, the decoder runs on the translation forest. We use the cube pruning method (Chiang, 2007) to approximately intersect the translation forest with the language model. Traversing the translation forest in a bottom-up order, the decoder tries to build target parses at each node. After the first pass, we use lazy Algorithm 3 (Huang and Chiang, 2005) to generate k-best translations for minimum error rate training. Inferring Composed Rules After minimal rules are learned, composed rules can be obtained by composing two or more minimal rules. For example, the composition of the second rule and the third rule in Table 1 produces a new rule: NP-B(NR(shalong)) → NP(NNP(Sharon)) While minimal"
P09-1063,P07-1089,1,0.944778,"odels that rely on 1-best parses are prone to learn noisy translation rules in training phase and produce degenerate translations in decoding phase (Quirk and CorstonOliver, 2006). This situation aggravates for treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based"
P09-1063,W06-1606,0,0.546348,"tion, syntax-based models that rely on 1-best parses are prone to learn noisy translation rules in training phase and produce degenerate translations in decoding phase (Quirk and CorstonOliver, 2006). This situation aggravates for treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses,"
P09-1063,D08-1022,0,0.604845,"or treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based tree-to-tree model achieves an absolute improvement of 3.6 BLEU points over conventional tree-based model. Current tree-to-tree models suffer from parsing errors as they usually use only 1best parses for rule ex"
P09-1063,W06-1628,0,0.393124,"Missing"
P09-1063,P08-1023,1,0.832306,"tion aggravates for treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based tree-to-tree model achieves an absolute improvement of 3.6 BLEU points over conventional tree-based model. Current tree-to-tree models suffer from parsing errors as they usually use only 1be"
P09-1063,D07-1079,0,0.0594257,"1-best parses are prone to learn noisy translation rules in training phase and produce degenerate translations in decoding phase (Quirk and CorstonOliver, 2006). This situation aggravates for treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based tree-to-tree model ach"
P09-1063,P05-1067,0,0.266114,"m Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. 558 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558–566, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP IP 1 VP 3 NP 2 PP 4 NP-B6 NR9 VP-B5 NP-B7 CC10P 11 NR12 NP-B8 VV13 AS"
P09-1063,P03-2041,0,0.736034,"se-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. 558 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558–566, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP IP 1 VP 3 NP 2 PP 4 NP-B6 NR9 VP-B5 NP-B7 CC10P"
P09-1063,P02-1038,0,0.0541119,"Missing"
P09-1063,J03-1002,0,0.0077588,"Missing"
P09-1063,W06-1608,0,0.11842,"Missing"
P09-1063,N04-1035,0,0.578009,"rs. 3 1 For example, the span of the source node “VP-B5 ” is {4, 5, 6} as it covers three source words: “juxing”, “le”, and “huitan”. For convenience, we use {4-6} to denotes a contiguous span {4, 5, 6}. Table 2: Node attributes of the example forest pair. 3 Rule Extraction Definition 2 Given a node v, its corresponding span γ(v) is the index set of aligned words on another side. Given an aligned forest pair as shown in Figure 1, how to extract all valid tree-to-tree rules that explain its synchronous generation process? By constructing a theory that gives formal semantics to word alignments, Galley et al. (2004) give principled answers to these questions for extracting tree-to-string rules. Their GHKM procedure draws connections among word alignments, derivations, and rules. They first identify the tree nodes that subsume tree-string pairs consistent with word alignments and then extract rules from these nodes. By this means, GHKM proves to be able to extract all valid tree-to-string rules from training instances. Although originally developed for the tree-to-string case, it is possible to extend GHKM to extract all valid tree-to-tree rules from aligned packed forests. In this section, we introduce o"
P09-1063,P06-1121,0,0.565149,"es in the source forest into elementary trees using the source projection of STSG while building target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. 558 Proceedings of the 47th Annual Meeting of the"
P09-1063,W05-1506,0,0.530286,"ree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. 558 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558–566, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP IP 1 VP 3 NP 2 PP 4 NP-B6 NR9 VP-B5 NP-B7 CC10P 11 NR12 NP-B8 VV13 AS 14 NN15 bushi yu shalong juxing le huitan Bush held a talk with Sharon NNP16 VBD17 NP22 Formally, a packed parse forest is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar. Huang and Chiang (2005) define a forest as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, and R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form of Xi,j , which denotes the recognition of non-terminal X spanning the substring from positions i through j (that is, wi+1 . . . wj ). Each hyperedge e ∈ E is a triple e = hT (e), h(e), f (e)i, where h(e) ∈ V is its head, T (e) ∈ V ∗ is a vector of tail nodes, and f (e) is a weight function from R|T (e) |to R. Our fo"
P09-1063,P07-1019,0,0.16388,"extraction time grew faster than decoding time with the increase of p. One possible reason is that the number of frontier tree pairs (see Figure 3) rose dramatically when more parses were included in packed forests. 5.5 Effect of Maximal Node Count Figure 5 shows the effect of maximal node count on BLEU scores. With the increase of maximal node count, the BLEU score increased dramatically. This implies that allowing tree-to-tree rules to capture larger contexts will strengthen the expressive power of tree-to-tree model. 5.6 In machine translation, the concept of packed forest is first used by Huang and Chiang (2007) to characterize the search space of decoding with language models. The first direct use of packed forest is proposed by Mi et al. (2008). They replace 1-best trees with packed forests both in training and decoding and show superior translation quality over the state-of-the-art hierarchical phrasebased system. We follow the same direction and apply packed forests to tree-to-tree translation. Zhang et al. (2008) present a tree-to-tree model that uses STSG. To capture non-syntactic phrases, they apply tree-sequence rules (Liu et al., 2007) to tree-to-tree models. Their extraction algorithm first"
P09-1063,2006.amta-papers.8,0,0.646284,"P-B PP P 11 IN 20 7 PP (a) 7 NP-B 12 NR 4 IN P 4 (b) (c) 21 NNP 24 NP 26 20 PP 4 NP-B NP 7 24 26 (d) Figure 2: (a) A frontier tree; (b) a minimal frontier tree; (c) a frontier tree pair; (d) a minimal frontier tree pair. All trees are taken from the example forest pair in Figure 1. Shaded nodes are frontier nodes. Each node is assigned an identity for reference. Definition 5 A node v is said to be a frontier node if and only if: 3.2 Identifying Minimum Rules Given the frontier nodes, the next step is to identify aligned tree pairs, from which tree-to-tree rules derive. Following Galley et al. (2006), we distinguish between minimal and composed rules. As a composed rule can be decomposed as a sequence of minimal rules, we are particularly interested in how to extract minimal rules. Also, we introduce a number of notions to help identify minimal rules. 1. v is consistent; 2. There exists at least one consistent node v ′ on another side satisfying: • closure(γ(v ′ )) ⊆ σ(v); • closure(γ(v)) ⊆ σ(v ′ ). v ′ is said to be a counterpart of v. We use τ (v) to denote the set of counterparts of v. Definition 6 A frontier tree is a subtree in a forest satisfying: A frontier node often has multiple"
P09-1063,P08-1066,0,0.11728,"rees using the source projection of STSG while building target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. 558 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pag"
P09-1063,I05-1007,1,0.700477,"s 5.1 Data Preparation We evaluated our model on Chinese-to-English translation. The training corpus contains 840K Chinese words and 950K English words. A trigram language model was trained on the English sentences of the training corpus. We used the 2002 NIST MT Evaluation test set as our development set, and used the 2005 NIST MT Evaluation test set as our test set. We evaluated the translation quality using the BLEU metric, as calculated by mteval-v11b.pl with its default setting except that we used case-insensitive matching of n-grams. To obtain packed forests, we used the Chinese parser (Xiong et al., 2005) modified by Haitao Mi and the English parser (Charniak and Johnson, 2005) modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned away if the difference is greater than a threshold p. Nodes with all incoming hyperedges pruned are also pruned. The greater the threshold p is, p(ts )"
P09-1063,zhang-etal-2004-interpreting,0,0.0787519,"Missing"
P09-1063,P08-1064,0,0.77893,"ne to learn noisy translation rules in training phase and produce degenerate translations in decoding phase (Quirk and CorstonOliver, 2006). This situation aggravates for treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based tree-to-tree model achieves an absolute imp"
P09-1063,P08-1067,0,\N,Missing
P09-1065,P06-1077,1,0.898463,"e word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly in deWe evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1 It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase rather than postprocessing system outputs. 576 Proceedings"
P09-1065,P07-1089,1,0.914806,"Missing"
P09-1065,D08-1076,0,0.100386,"Missing"
P09-1065,D08-1022,1,0.459626,"case. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Comparison with System Combination Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the system combination method achieved a BLEU score of 31.50, slightly lower than that of jo"
P09-1065,D08-1023,0,0.00900115,"j − i = l do for all m ∈ M do A DD(G, i, j, m) end for P RUNE(G, i, j) end for end for end procedure IP(x1 :VV, x2 :NN) → x1 x2 X → hfabiao, givei X → hyanjiang, a talki Figure 4: A derivation composed of both SCFG and tree-to-string rules. pairs and tree-to-string rules. Hierarchical phrase pairs are used for translating smaller units and tree-to-string rules for bigger ones. It is appealing to combine them in such a way because the hierarchical phrase-based model provides excellent rule coverage while the tree-to-string model offers linguistically motivated non-local reordering. Similarly, Blunsom and Osborne (2008) use both hierarchical phrase pairs and tree-to-string rules in decoding, where source parse trees serve as conditioning context rather than hard constraints. Depending on the target side output, we distinguish between string-targeted and tree-targeted models. String-targeted models include phrasebased, hierarchical phrase-based, and tree-tostring models. Tree-targeted models include string-to-tree and tree-to-tree models. All models can be combined at the translation level. Models that share with same target output structure can be further combined at the derivation level. The joint decoder u"
P09-1065,P08-1024,0,0.0833649,"ces of decisions that translate a source sentence into a target sentence step by step. For example, Figure 1 shows a sequence of SCFG rules (Chiang, 2005; Chiang, 2007) that translates a Chinese sentence “fabiao yanjiang” into an English sentence “give a talk”. Such sequence of decisions is called a derivation. In phrase-based models, a decision can be translating a source phrase into a target phrase or reordering the target phrases. In syntax-based models, decisions usually correspond to transduction rules. Often, there are many derivations that are distinct yet produce the same translation. Blunsom et al. (2008) present a latent variable model that describes the relationship between translation and derivation clearly. Given a source sentence f , the probability of a target sentence e being its translation is the sum over all possible derivations: ˆ = argmax e e P r(d, e|f ) = m λm hm (d, e, f ) Z(f ) ( X d∈∆(e,f ) ˆ ≈ argmax e e,d exp X ) λm hm (d, e, f ) m X m  λm hm (d, e, f ) (6) We refer to Eq. (5) as max-translation decoding and Eq. (6) as max-derivation decoding, which are first termed by Blunsom et al. (2008). By now, most current SMT systems, adopting either max-derivation decoding or max-t"
P09-1065,P05-1033,0,0.867394,"of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly in deWe evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1 It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phas"
P09-1065,J07-2003,0,0.876253,"of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly in deWe evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1 It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase rather than p"
P09-1065,P08-1023,1,0.682905,"decoding in this case. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Comparison with System Combination Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the system combination method achieved a BLEU score of 31.50, slightly"
P09-1065,P02-1038,0,0.474353,"and corresponding translation e conditioned on a source sentence f : P (4) where Z(f ) is not needed in decoding because it is independent of e. Most SMT systems approximate the summation over all possible derivations by using 1-best derivation for efficiency. They search for the 1best derivation and take its target yield as the best translation: d∈∆(e,f ) exp p(d) where d is a decision in the derivation d. Although originally proposed for supporting large sets of non-independent and overlapping features, the latent variable model is actually a more general form of conventional linear model (Och and Ney, 2002). Accordingly, decoding for the latent variable model can be formalized as 2 Background P r(d, e|f ) Y d∈d ing with multiple models achieves an absolute improvement of 1.5 BLEU points over individual decoding with single models (Section 5). X (3) A feature value is usually decomposed as the product of decision probabilities: 2 h(d, e, f ) = P r(e|f ) = λm hm (d, e, f ) m e d∈∆(e,f ) Figure 1: A derivation composed of SCFG rules that translates a Chinese sentence “fabiao yanjiang” into an English sentence “give a talk”. X 3 Joint Decoding There are two major challenges for combining multiple mo"
P09-1065,J03-1002,0,0.00639286,"Missing"
P09-1065,J04-4002,0,0.208011,"ning further improvements. In this paper, we propose a framework for combining multiple translation models directly in deWe evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1 It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase rather than postprocessing system outputs. 576 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 576–584, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP where hm is a feature function, λm is the associated feature weight, and Z(f ) is a constant for normalization: S → hX1 , X1 i X → hfabiao X1 , give a X1 i X → hyanjiang, talki Z(f ) = X X exp Stati"
P09-1065,P03-1021,0,0.815341,"rtial translations (Section 3.2). Although such translation-level combination will not produce new translations, it does change the way of selecting promising candidates. • Two models could even share derivations with each other if they produce the same structures on the target side (Section 3.3), which we refer to as derivation-level combination. This method enlarges the search space by allowing for mixing different types of translation rules within one derivation. • As multiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a seq"
P09-1065,A94-1016,0,0.256094,"derivation-level combination. This method enlarges the search space by allowing for mixing different types of translation rules within one derivation. • As multiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in"
P09-1065,P06-1121,0,0.228974,"ion hypergraph produced by another model; (c) the packed translation hypergraph based on (a) and (b). Solid and dashed lines denote the translation rules of the two models, respectively. Shaded nodes occur in both (a) and (b), indicating that the two models produce the same translations. and thus need to collect different information during decoding. For example, taking a source parse as input, a tree-to-string decoder (e.g., (Liu et al., 2006)) pattern-matches the source parse with treeto-string rules and produces a string on the target side. On the contrary, a string-to-tree decoder (e.g., (Galley et al., 2006; Shen et al., 2008)) is a parser that applies string-to-tree rules to obtain a target parse for the source string. As a result, the hypothesis structures of the two models are fundamentally different. tion 3.3). 3.1 Translation Hypergraph Despite the diversity of translation models, they all have to produce partial translations for substrings of input sentences. Therefore, we represent the search space of a translation model as a structure called translation hypergraph. Figure 2(a) demonstrates a translation hypergraph for one model, for example, a hierarchical phrase-based model. A node in a"
P09-1065,D08-1011,0,0.0936928,"are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly"
P09-1065,W99-0623,0,0.227171,"Missing"
P09-1065,W05-1506,0,0.6237,"g translation rule derivation model first parses f to obtain a source tree T (f ) and then transforms T (f ) to the target sentence e. Conversely, a string-to-tree model first parses f into a target tree T (e) and then takes the surface string e as the translation. Despite different inside, their derivations must begin with f and end with e. This situation remains the same for derivations between a source substring fij and its partial translation t during joint decoding: Table 1: Correspondence between translation hypergraph and decoding. More formally, a hypergraph (Klein and Manning., 2001; Huang and Chiang, 2005) is a tuple hV, E, Ri, where V is a set of nodes, E is a set of hyperedges, and R is a set of weights. For a given source sentence f = f1n = f1 . . . fn , each node v ∈ V is in the form of ht, [i, j]i, which denotes the recognition of t as one translation of the source substring spanning from i through j (that is, fi+1 . . . fj ). Each hyperedge e ∈ E is a tuple e = htails(e), head(e), w(e)i, where head(e) ∈ V is the consequent node in the deductive step, tails(e) ∈ V ∗ is the list of antecedent nodes, and w(e) is a weight function from R|tails(e) |to R. As a general representation, a translat"
P09-1065,P07-1019,0,0.0420047,"o surpass max-derivation decoding in this case. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Comparison with System Combination Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the system combination method achieved a BLEU score of"
P09-1065,P08-1067,0,0.0234869,"ax-translation decoding still failed to surpass max-derivation decoding in this case. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Comparison with System Combination Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the sy"
P09-1065,W01-1812,0,0.192643,"Missing"
P09-1065,P02-1040,0,0.106668,"ections while two lines have at most one intersection. Fortunately, as the curve is monotonically increasing, 5 Experiments 5.1 Data Preparation Our experiments were on Chinese-to-English translation. We used the FBIS corpus (6.9M + 8.9M words) as the training corpus. For language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD corpus. We used the NIST 2002 MT Evaluation test set as our development set, and used the NIST 2005 test set as test set. We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002). Our joint decoder included two models. The 581 Model Combination hierarchical tree-to-string N/A N/A translation derivation both Max-derivation Time BLEU 40.53 30.11 6.13 27.23 N/A N/A 48.45 31.63 Max-translation Time BLEU 44.87 29.82 6.69 27.11 55.89 30.79 54.91 31.49 Table 2: Comparison of individual decoding and joint decoding on average decoding time (seconds/sentence) and BLEU score (case-insensitive). 5.2 percentage first model was the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007). We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) a"
P09-1065,P07-1040,0,0.331325,"ultiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translati"
P09-1065,P08-1066,0,0.0477497,"ed by another model; (c) the packed translation hypergraph based on (a) and (b). Solid and dashed lines denote the translation rules of the two models, respectively. Shaded nodes occur in both (a) and (b), indicating that the two models produce the same translations. and thus need to collect different information during decoding. For example, taking a source parse as input, a tree-to-string decoder (e.g., (Liu et al., 2006)) pattern-matches the source parse with treeto-string rules and produces a string on the target side. On the contrary, a string-to-tree decoder (e.g., (Galley et al., 2006; Shen et al., 2008)) is a parser that applies string-to-tree rules to obtain a target parse for the source string. As a result, the hypothesis structures of the two models are fundamentally different. tion 3.3). 3.1 Translation Hypergraph Despite the diversity of translation models, they all have to produce partial translations for substrings of input sentences. Therefore, we represent the search space of a translation model as a structure called translation hypergraph. Figure 2(a) demonstrates a translation hypergraph for one model, for example, a hierarchical phrase-based model. A node in a hypergraph denotes"
P09-1065,P05-1044,0,0.021838,"Missing"
P09-1065,P06-1098,0,0.0135744,"ensus translations among different models. Now the question is how to decode with multiple models jointly in just one decoder. We believe that both left-to-right and bottom-up strategies can be used for joint decoding. Although phrase-based decoders usually produce translations from left to right, they can adopt bottom-up decoding in principle. Xiong et al. (2006) develop a bottom-up decoder for BTG (Wu, 1997) that uses only phrase pairs. They treat reordering of phrases as a binary classification problem. On the other hand, it is possible for syntax-based models to decode from left to right. Watanabe et al. (2006) propose leftto-right target generation for hierarchical phrasebased translation. Although left-to-right decoding might enable a more efficient use of language models and hopefully produce better translations, we adopt bottom-up decoding in this paper just for convenience. Figure 3 demonstrates the search algorithm of our joint decoder. The input is a source language sentence f1n , and a set of translation models M Translation-Level Combination The conventional interpretation of Eq. (1) is that the probability of a translation is the sum over all possible derivations coming from the same model"
P09-1065,J97-3002,0,0.0490287,"model. The shaded nodes are shared by both models. Therefore, the two models are combined at the translation level. Intuitively, shared nodes should be favored in decoding because they offer consensus translations among different models. Now the question is how to decode with multiple models jointly in just one decoder. We believe that both left-to-right and bottom-up strategies can be used for joint decoding. Although phrase-based decoders usually produce translations from left to right, they can adopt bottom-up decoding in principle. Xiong et al. (2006) develop a bottom-up decoder for BTG (Wu, 1997) that uses only phrase pairs. They treat reordering of phrases as a binary classification problem. On the other hand, it is possible for syntax-based models to decode from left to right. Watanabe et al. (2006) propose leftto-right target generation for hierarchical phrasebased translation. Although left-to-right decoding might enable a more efficient use of language models and hopefully produce better translations, we adopt bottom-up decoding in this paper just for convenience. Figure 3 demonstrates the search algorithm of our joint decoder. The input is a source language sentence f1n , and a"
P09-1065,P06-1066,1,0.502138,"rst model and the dashed lines denote those of the second model. The shaded nodes are shared by both models. Therefore, the two models are combined at the translation level. Intuitively, shared nodes should be favored in decoding because they offer consensus translations among different models. Now the question is how to decode with multiple models jointly in just one decoder. We believe that both left-to-right and bottom-up strategies can be used for joint decoding. Although phrase-based decoders usually produce translations from left to right, they can adopt bottom-up decoding in principle. Xiong et al. (2006) develop a bottom-up decoder for BTG (Wu, 1997) that uses only phrase pairs. They treat reordering of phrases as a binary classification problem. On the other hand, it is possible for syntax-based models to decode from left to right. Watanabe et al. (2006) propose leftto-right target generation for hierarchical phrasebased translation. Although left-to-right decoding might enable a more efficient use of language models and hopefully produce better translations, we adopt bottom-up decoding in this paper just for convenience. Figure 3 demonstrates the search algorithm of our joint decoder. The i"
P09-1065,N03-1017,0,\N,Missing
P09-2031,P05-1033,0,0.493,"n various phrases, such as , accept France ’s invitation”. “ While f2 almost always appears in f1 , indicating that the variable X may not be replaced with other words expect “President”. It indicates that R2 is not likely to be useful, although f2 may appears frequently in a corpus. 4. N (p), the number of phrases that contain p as a substring. É {I   Given a monolingual corpus, key phrases can be extracted efficiently according to Algorithm 1. Firstly (line 1), all possible phrases are extracted as candidates of key phrases. This step is analogous to the rule extraction as described in (Chiang, 2005). The basic difference is that there are no word alignment constraints for monolingual phrase extraction, which therefore results in a substantial number of candidate phrases. We use the following restrictions to limit the phrase number: 2.2 C-value C-value, a measurement of automatic term recognition, is proposed by Frantzi and Ananiadou (1996) to extract nested collocations, collocations that substrings of other longer ones. We use C-value for two reasons: on one hand, it uses rich factors besides phrase frequency, e.g. the phrase length, the frequency that a sub-phrase appears in longer phr"
P09-2031,C96-1009,0,0.0494903,"as a substring. É {I   Given a monolingual corpus, key phrases can be extracted efficiently according to Algorithm 1. Firstly (line 1), all possible phrases are extracted as candidates of key phrases. This step is analogous to the rule extraction as described in (Chiang, 2005). The basic difference is that there are no word alignment constraints for monolingual phrase extraction, which therefore results in a substantial number of candidate phrases. We use the following restrictions to limit the phrase number: 2.2 C-value C-value, a measurement of automatic term recognition, is proposed by Frantzi and Ananiadou (1996) to extract nested collocations, collocations that substrings of other longer ones. We use C-value for two reasons: on one hand, it uses rich factors besides phrase frequency, e.g. the phrase length, the frequency that a sub-phrase appears in longer phrases. Thus it is appropriate for extracting hierarchical phrases. On the other hand, the computation of C-value is efficient. Analogous to (Frantzi and Ananiadou, 1996), we use 4 factors (L, F, S, N ) to determine if a phrase p is a key phrase: 1. The length of a candidate phrase is limited to pl; 2. The length of the initial phrase used to crea"
P09-2031,D07-1103,0,0.085662,"Missing"
P09-2031,N03-1017,0,0.0142339,"Missing"
P09-2031,W04-3250,0,0.268048,"Missing"
P09-2031,P08-1066,0,0.0397018,"Missing"
P09-2035,N03-1017,0,0.0252889,"Missing"
P09-2035,P06-1077,1,0.876446,"Missing"
P09-2035,P09-1063,1,0.78314,"Missing"
P09-2035,D08-1022,1,0.882756,"(Liu et al., 2006; Huang et al., 2006), tree-to-tree models (Quirk et al.,2005;Zhang et al., 2008). Especially, when incorporated with forest, the correspondent forest-based tree-to-string models (Mi et al., 2008; Zhang et al., 2009), tree-to-tree models (Liu et al., 2009) have achieved a promising improvements over correspondent treebased systems. However, when we translate long sentences, we argue that two major issues will be raised. On one hand, parsing accuracy will be lower as the length of sentence grows. It will inevitably hurt the translation quality (Quirk and Corston-Oliver, 2006; Mi and Huang, 2008). On the other hand, decoding on long sentences will be time consuming, especially for forest approaches. So splitting long sentences into subsentences becomes a natural way in MT literature. A simple way is to split long sentences by punctuations. However, without concerning about the original whole tree structures, this approach will result in ill-formed sub-trees which don’t respect to original structures. In this paper, we present a new approach, which pays more attention to parse trees on the long sentences. We firstly parse the long sentences into trees, and then divide them accordingly"
P09-2035,P03-1021,0,0.0109418,"on forest-based tree-tostring translation model (Mi et al. 2008). Our training corpus consists of 2.56 million sentence pairs. Forest-based rule extractor (Mi and Huang 2008) is used with a pruning threshold p=3. And we use SRI Language Modeling Toolkit (Stolcke, 2002) to train two 5-gram language models with Kneser-Ney smoothing on the English side of the training corpus and the Xinhua portion of Gigaword corpora respectively. We use 2006 NIST MT Evaluation test set as development set, and 2002, 2005 and 2008 NIST MT Evaluation test sets as test sets. We also use minimum error-rate training (Och, 2003) to tune our feature weights. We evaluate our results with case-sensitive BLEU-4 metric (Papineni et al., 2002). The pruning threshold p for parse forest in decoding time is 12. 5.2 Results The final BLEU results are shown in Table 2, our approach has achieved a BLEU score that is 1.1 higher than direct decoding and 0.3 higher than always splitting on commas. The decoding time results are presented in Table 3. The search space of our experiment is extremely large due to the large pruning threshold (p=12), thus resulting in a long decoding time. However, our approach has reduced the decoding ti"
P09-2035,P02-1040,0,0.0782487,"Missing"
P09-2035,P05-1034,0,0.145683,"Missing"
P09-2035,W05-1506,0,0.0668665,"Missing"
P09-2035,2006.amta-papers.8,0,\N,Missing
P09-2035,I05-1007,1,\N,Missing
P09-2035,P09-1020,0,\N,Missing
P09-2035,P08-1023,1,\N,Missing
P09-2035,P08-1064,0,\N,Missing
P09-2035,W06-1608,0,\N,Missing
P09-2035,W06-1606,0,\N,Missing
P09-2035,P07-1019,0,\N,Missing
P10-1002,J96-1002,0,0.0140398,"ation instances (section 3). Then we describe an application of the projected parser: boosting a state-of-the-art 2nd-ordered MST parser (section 4). After the comparisons with previous works on dependency parsing and projection, we finally five the experimental results. can be a boolean value: C(i, j) = p (1) as produced by a support vector machine (SVM) classifier (Vapnik, 1998). p = 1 indicates that the classifier supports the candidate edge (i, j), and p = 0 the contrary. C(i, j) can also be a realvalued probability: C(i, j) = p 0≤p≤1 (2) as produced by an maximum entropy (ME) classifier (Berger et al., 1996). p is a probability which indicates the degree the classifier support the candidate edge (i, j). Ideally, given the classification results for all candidate word pairs, the dependency parse tree can be composed of the candidate edges with higher score (1 for the boolean-valued classifier, and large p for the real-valued classifier). However, more robust strategies should be investigated since the ambiguity of the language syntax and the classification errors usually lead to illegal or incomplete parsing result, as shown in Figure 1. Follow the edge based factorization method (Eisner, 1996), w"
P10-1002,W06-2925,0,0.0124445,"completely to the target language sentence. On the contrary, our dependency projection strategy prefer to extract a set of dependency instances, which coincides our model’s demand for training corpus. An obvious advantage of this strategy is that, we can select an appropriate filtering threshold to obtain dependency instances of good quality. In addition, our word-pair classification model can be integrated deeply into a state-of-the-art MST dependency model. Since both of them are 5 Related Works 5.1 Dependency Parsing Both the graph-based (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras et al., 2006) and the transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms are related to our word-pair classification model. Similar to the graph-based method, our model is factored on dependency edges, and its decoding procedure also aims to find a maximum spanning tree in a fully connected directed graph. From this point, our model can be classified into the graph-based category. On the training method, however, our model obviously differs from other graph-based models, that we only need a set of word-pair dependency instances rather than a regular dependency treebank. Th"
P10-1002,W08-2102,0,0.0127024,"arser ˜ according to both the baseselects the best parse y line model B and the projected classifier C. (7) Then the score that word i and word j in the target sentence y forms a projected dependency edge, ˜ = argmax[sB (x, y) + λsC (x, y)] y y 15 (11) Here, sB and sC denote the evaluation functions of the baseline model and the projected classifier, respectively. The parameter λ is the relative weight of the projected classifier against the baseline model. There are several strategies to integrate the two evaluation functions. For example, they can be integrated deeply at each decoding step (Carreras et al., 2008; Zhang and Clark, 2008; Huang, 2008), or can be integrated shallowly in a reranking manner (Collins, 2000; Charniak and Johnson, 2005). As described previously, the score of a dependency tree given by a word-pair classifier can be factored into each candidate dependency edge in this tree. Therefore, the projected classifier can be integrated with a baseline model deeply at each dependency edge, if the evaluation score given by the baseline model can also be factored into dependency edges. We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. Especially, the effect"
P10-1002,W02-1001,0,0.835514,"th English and Chinese, the word-pair classification model falls behind of the state-of-the-art. We think that it is probably Word-Pair Classification Model We experiment on two popular treebanks, the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993), and the Penn Chinese Treebank (CTB) 5.0 (Xue et al., 2005). The constituent trees in the two treebanks are transformed to dependency trees according to the headfinding rules of Yamada and Matsumoto (2003). For English, we use the automatically-assigned POS tags produced by an implementation of the POS tagger of Collins (2002). While for Chinese, we just use the gold-standard POS tags following the tradition. Each treebank is splitted into three partitions, for training, development and testing, respectively, as shown in Table 2. For a dependency tree with n words, only n − 1 positive dependency instances can be extracted. They account for only a small proportion of all the dependency instances. As we know, it is important to balance the proportions of the positive and the negative instances for a batched-trained classifier. We define a new parameter r to denote the ratio of the negative instances relative to the p"
P10-1002,C96-1058,0,0.0178516,"er et al., 1996). p is a probability which indicates the degree the classifier support the candidate edge (i, j). Ideally, given the classification results for all candidate word pairs, the dependency parse tree can be composed of the candidate edges with higher score (1 for the boolean-valued classifier, and large p for the real-valued classifier). However, more robust strategies should be investigated since the ambiguity of the language syntax and the classification errors usually lead to illegal or incomplete parsing result, as shown in Figure 1. Follow the edge based factorization method (Eisner, 1996), we factorize the score of a dependency tree s(x, y) into its dependency edges, and design a dynamic programming algorithm to search for the candidate parse with maximum score. This strategy alleviate the classification errors to some degree and ensure a valid, complete dependency parsing tree. If a boolean-valued classifier is used, the search algorithm can be formalized as: 2 Word-Pair Classification Model 2.1 p ∈ {0, 1} Model Definition Following (McDonald et al., 2005a), x is used to denote the sentence to be parsed, and xi to denote the i-th word in the sentence. y denotes the dependency"
P10-1002,P09-1042,0,0.474786,"utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (Hwa et al., 2002; Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2009). We propose an effective method for dependency projection, which does not have to produce complete projected trees. Given a wordaligned bilingual corpus with source language sentences parsed, the dependency relationships of the word pairs in the source language are projected to the word pairs of the target language. A dependency relationship is a boolean value that represents whether this word pair forms a dependency edge. Thus a set of classification instances are obtained. Meanwhile, we propose an intuitionistic model for dependency parsing, whic"
P10-1002,W05-1506,0,0.0127933,"ction return d Figure 2: The word alignment matrix between a Chinese sentence and its English translation. Note that probabilities need not to be normalized across rows or columns. used in McDonald et al. (2005b), is also applicable here. In this work, however, we still adopt the more general, bottom-up dynamic programming algorithm Algorithm 1 in order to facilitate the possible expansions. Here, V[i, j] contains the candidate parsing segments of the span [i, j], and the function E VAL(d) accumulates the scores of all the edges in dependency segment d. In practice, the cube-pruning strategy (Huang and Chiang, 2005) is used to speed up the enumeration of derivations (loops started by line 4 and 5). s+ (i, j), can be defined as: X s+ (i, j) = Ai,i′ × Aj,j ′ × δ(ye , i′ , j ′ , +) (8) i′ ,j ′ The score that they do not form a projected dependency edge can be defined similarly: X Ai,i′ × Aj,j ′ × δ(ye , i′ , j ′ , −) (9) s− (i, j) = i′ ,j ′ Note that for simplicity, the condition factors ye and A are omitted from these two formulas. We finally define the probability of the supposed projected dependency edge as: 3 Projected Classification Instance After the introduction of the word-pair classification model,"
P10-1002,P08-1067,0,0.0149197,"best parse y line model B and the projected classifier C. (7) Then the score that word i and word j in the target sentence y forms a projected dependency edge, ˜ = argmax[sB (x, y) + λsC (x, y)] y y 15 (11) Here, sB and sC denote the evaluation functions of the baseline model and the projected classifier, respectively. The parameter λ is the relative weight of the projected classifier against the baseline model. There are several strategies to integrate the two evaluation functions. For example, they can be integrated deeply at each decoding step (Carreras et al., 2008; Zhang and Clark, 2008; Huang, 2008), or can be integrated shallowly in a reranking manner (Collins, 2000; Charniak and Johnson, 2005). As described previously, the score of a dependency tree given by a word-pair classifier can be factored into each candidate dependency edge in this tree. Therefore, the projected classifier can be integrated with a baseline model deeply at each dependency edge, if the evaluation score given by the baseline model can also be factored into dependency edges. We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. Especially, the effect of the Collins distance in the baseli"
P10-1002,P02-1050,0,0.139104,"works have also been devoted to the utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (Hwa et al., 2002; Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2009). We propose an effective method for dependency projection, which does not have to produce complete projected trees. Given a wordaligned bilingual corpus with source language sentences parsed, the dependency relationships of the word pairs in the source language are projected to the word pairs of the target language. A dependency relationship is a boolean value that represents whether this word pair forms a dependency edge. Thus a set of classification instances are obtained. Meanwhile, we propose an intuition"
P10-1002,P03-1021,0,0.0134137,"classifier can be factored into each candidate dependency edge in this tree. Therefore, the projected classifier can be integrated with a baseline model deeply at each dependency edge, if the evaluation score given by the baseline model can also be factored into dependency edges. We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. Especially, the effect of the Collins distance in the baseline model is also investigated. The relative weight λ is adjusted to maximize the performance on the development set, using an algorithm similar to minimum error-rate training (Och, 2003). they differ from each other in the classification results. The classifier in our model predicates a dependency probability for each pair of words, while the classifier in a transition-based model gives a possible next transition operation such as shift or reduce. Another difference lies in the factorization strategy. For our method, the evaluation score of a candidate parse is factorized into each dependency edge, while for the transition-based models, the score is factorized into each transition operation. Thanks to the reminding of the third reviewer of our paper, we find that the pairwise"
P10-1002,W09-3803,1,0.92773,"across the word alignment to words in the unparsed sentences, according to the DCA assumption (Hwa et al., 2005). Such a projection procedure suffers much from the word alignment errors and syntactic isomerism between languages, which usually lead to relationship projection conflict and incomplete projected dependency structures. To tackle this problem, Hwa et al. (2005) use some filtering rules to reduce noise, and some hand-designed rules to handle language heterogeneity. Smith and Eisner (2009) perform dependency projection and annotation adaptation with quasi-synchronous grammar features. Jiang and Liu (2009) resort to a dynamic programming procedure to search for a completed projected tree. However, these strategies are all confined to the same category that dependency projection must produce completed projected trees. Because of the free translation, the syntactic isomerism between languages and word alignment errors, it would be strained to completely project the dependency structure from one language to another. In this paper we describe an intuitionistic method for dependency parsing, where a classifier is used to determine whether a pair of words forms a dependency edge. And we also propose"
P10-1002,D09-1086,0,0.49921,"ac.cn Abstract For dependency projection, the relationship between words in the parsed sentences can be simply projected across the word alignment to words in the unparsed sentences, according to the DCA assumption (Hwa et al., 2005). Such a projection procedure suffers much from the word alignment errors and syntactic isomerism between languages, which usually lead to relationship projection conflict and incomplete projected dependency structures. To tackle this problem, Hwa et al. (2005) use some filtering rules to reduce noise, and some hand-designed rules to handle language heterogeneity. Smith and Eisner (2009) perform dependency projection and annotation adaptation with quasi-synchronous grammar features. Jiang and Liu (2009) resort to a dynamic programming procedure to search for a completed projected tree. However, these strategies are all confined to the same category that dependency projection must produce completed projected trees. Because of the free translation, the syntactic isomerism between languages and word alignment errors, it would be strained to completely project the dependency structure from one language to another. In this paper we describe an intuitionistic method for dependency"
P10-1002,P09-1059,1,0.838617,"the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (Hwa et al., 2002; Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2009). We propose an effective method for dependency projection, which does not have to produce complete projected trees. Given a wordaligned bilingual corpus with source language sentences parsed, the dependency relationships of the word pairs in the source language are projected to the word pairs of the target language. A dependency relationship is a boolean value that represents whether this word pair forms a dependency edge. Thus a set of classification instances are obtained. Meanwhile, we propose an intuitionistic model for dependency parsing, which uses a classifier to determine whether a pa"
P10-1002,E99-1026,0,0.0161237,"r in our model predicates a dependency probability for each pair of words, while the classifier in a transition-based model gives a possible next transition operation such as shift or reduce. Another difference lies in the factorization strategy. For our method, the evaluation score of a candidate parse is factorized into each dependency edge, while for the transition-based models, the score is factorized into each transition operation. Thanks to the reminding of the third reviewer of our paper, we find that the pairwise classification schema has also been used in Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2000). However, our work shows more advantage in feature engineering, model training and decoding algorithm. 5.2 Dependency Projection Many works try to learn parsing knowledge from bilingual corpora. L¨u et al. (2002) aims to obtain Chinese bracketing knowledge via ITG (Wu, 1997) alignment. Hwa et al. (2005) and Ganchev et al. (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. Smith and Eisner (2009) perform dependency projection and annotation ad"
P10-1002,P04-1061,0,0.0452962,"on instances significantly outperforms previous projected dependency parsers. More importantly, when this classifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline. 1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (McDonald et al., 2005a; McDonald and Pereira, 2006; Nivre et al., 2006). Since it is costly and difficult to build humanannotated treebanks, a lot of works have also been devoted to the utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (Hwa et al., 2002; Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2009). We propose an effective method for depen"
P10-1002,P08-1068,0,0.0311723,"fier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline. 1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (McDonald et al., 2005a; McDonald and Pereira, 2006; Nivre et al., 2006). Since it is costly and difficult to build humanannotated treebanks, a lot of works have also been devoted to the utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (Hwa et al., 2002; Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2009). We propose an effective method for dependency projection, which does not have to produce complete projected trees. Given a wordaligned bilingual"
P10-1002,J97-3002,0,0.0294688,"zed into each dependency edge, while for the transition-based models, the score is factorized into each transition operation. Thanks to the reminding of the third reviewer of our paper, we find that the pairwise classification schema has also been used in Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2000). However, our work shows more advantage in feature engineering, model training and decoding algorithm. 5.2 Dependency Projection Many works try to learn parsing knowledge from bilingual corpora. L¨u et al. (2002) aims to obtain Chinese bracketing knowledge via ITG (Wu, 1997) alignment. Hwa et al. (2005) and Ganchev et al. (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. Smith and Eisner (2009) perform dependency projection and annotation adaptation with Quasi-Synchronous Grammar features. Jiang and Liu (2009) refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees. All previous works for dependency projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang an"
P10-1002,W00-1303,0,0.032383,"es a dependency probability for each pair of words, while the classifier in a transition-based model gives a possible next transition operation such as shift or reduce. Another difference lies in the factorization strategy. For our method, the evaluation score of a candidate parse is factorized into each dependency edge, while for the transition-based models, the score is factorized into each transition operation. Thanks to the reminding of the third reviewer of our paper, we find that the pairwise classification schema has also been used in Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2000). However, our work shows more advantage in feature engineering, model training and decoding algorithm. 5.2 Dependency Projection Many works try to learn parsing knowledge from bilingual corpora. L¨u et al. (2002) aims to obtain Chinese bracketing knowledge via ITG (Wu, 1997) alignment. Hwa et al. (2005) and Ganchev et al. (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. Smith and Eisner (2009) perform dependency projection and annotation adaptation with Quasi-Synchro"
P10-1002,D09-1106,1,0.82246,"rojection from English to Chinese. We use the FBIS Chinese-English bitext as the bilingual corpus for dependency projection. It contains 239K sentence pairs with about 6.9M/8.9M words in Chinese/English. Both English and Chinese sentences are tagged by the implementations of the POS tagger of Collins (2002), which trained on WSJ and CTB 5.0 respectively. The English sentences are then parsed by an implementation of 2nd-ordered MST model of McDonald and Pereira (2006), which is trained on dependency trees extracted from WSJ. The alignment matrixes for sentence pairs are generated according to (Liu et al., 2009). Similar to the ratio r, the threshold b need also be assigned an appropriate value to achieve a better performance. Larger thresholds result in better but less classification instances, the lower coverage of the instances would hurt the performance of the classifier. On the other hand, smaller thresholds lead to worse but more instances, and too much noisy instances will bring down the classifier’s discriminating power. We extract a series of classification instance sets 6.3 Integrated Dependency Parser We integrate the word-pair classification model into the state-of-the-art 2nd-ordered MST"
P10-1002,W03-3023,0,0.528582,"e contrary, our dependency projection strategy prefer to extract a set of dependency instances, which coincides our model’s demand for training corpus. An obvious advantage of this strategy is that, we can select an appropriate filtering threshold to obtain dependency instances of good quality. In addition, our word-pair classification model can be integrated deeply into a state-of-the-art MST dependency model. Since both of them are 5 Related Works 5.1 Dependency Parsing Both the graph-based (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras et al., 2006) and the transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms are related to our word-pair classification model. Similar to the graph-based method, our model is factored on dependency edges, and its decoding procedure also aims to find a maximum spanning tree in a fully connected directed graph. From this point, our model can be classified into the graph-based category. On the training method, however, our model obviously differs from other graph-based models, that we only need a set of word-pair dependency instances rather than a regular dependency treebank. Therefore, our model is more suitable for the partially"
P10-1002,C02-1003,0,0.0331532,"Missing"
P10-1002,P08-1101,0,0.0179065,"th the baseselects the best parse y line model B and the projected classifier C. (7) Then the score that word i and word j in the target sentence y forms a projected dependency edge, ˜ = argmax[sB (x, y) + λsC (x, y)] y y 15 (11) Here, sB and sC denote the evaluation functions of the baseline model and the projected classifier, respectively. The parameter λ is the relative weight of the projected classifier against the baseline model. There are several strategies to integrate the two evaluation functions. For example, they can be integrated deeply at each decoding step (Carreras et al., 2008; Zhang and Clark, 2008; Huang, 2008), or can be integrated shallowly in a reranking manner (Collins, 2000; Charniak and Johnson, 2005). As described previously, the score of a dependency tree given by a word-pair classifier can be factored into each candidate dependency edge in this tree. Therefore, the projected classifier can be integrated with a baseline model deeply at each dependency edge, if the evaluation score given by the baseline model can also be factored into dependency edges. We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. Especially, the effect of the Collins distance"
P10-1002,J93-2004,0,0.0343916,"r both English and Chinese, maximum performance is achieved at about r = 2.5. 3 The English and Chinese classifiers trained on the instance sets with r = 2.5 are used in the final evaluation phase. Table 3 shows the performances on the test sets of WSJ and CTB 5.0. We also compare them with previous works on the same test sets. On both English and Chinese, the word-pair classification model falls behind of the state-of-the-art. We think that it is probably Word-Pair Classification Model We experiment on two popular treebanks, the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993), and the Penn Chinese Treebank (CTB) 5.0 (Xue et al., 2005). The constituent trees in the two treebanks are transformed to dependency trees according to the headfinding rules of Yamada and Matsumoto (2003). For English, we use the automatically-assigned POS tags produced by an implementation of the POS tagger of Collins (2002). While for Chinese, we just use the gold-standard POS tags following the tradition. Each treebank is splitted into three partitions, for training, development and testing, respectively, as shown in Table 2. For a dependency tree with n words, only n − 1 positive depende"
P10-1002,E06-1011,0,0.506149,"f the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this classifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline. 1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (McDonald et al., 2005a; McDonald and Pereira, 2006; Nivre et al., 2006). Since it is costly and difficult to build humanannotated treebanks, a lot of works have also been devoted to the utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the depe"
P10-1002,P05-1012,0,0.494068,"pendency relationships of the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this classifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline. 1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (McDonald et al., 2005a; McDonald and Pereira, 2006; Nivre et al., 2006). Since it is costly and difficult to build humanannotated treebanks, a lot of works have also been devoted to the utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising"
P10-1002,H05-1066,0,0.313843,"pendency relationships of the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this classifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline. 1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (McDonald et al., 2005a; McDonald and Pereira, 2006; Nivre et al., 2006). Since it is costly and difficult to build humanannotated treebanks, a lot of works have also been devoted to the utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising"
P10-1002,C04-1010,0,0.0306486,"Missing"
P10-1002,W06-2933,0,0.23551,"ce language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this classifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline. 1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (McDonald et al., 2005a; McDonald and Pereira, 2006; Nivre et al., 2006). Since it is costly and difficult to build humanannotated treebanks, a lot of works have also been devoted to the utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures fro"
P10-1002,P00-1056,0,0.0212341,"ted dependency edge can be defined similarly: X Ai,i′ × Aj,j ′ × δ(ye , i′ , j ′ , −) (9) s− (i, j) = i′ ,j ′ Note that for simplicity, the condition factors ye and A are omitted from these two formulas. We finally define the probability of the supposed projected dependency edge as: 3 Projected Classification Instance After the introduction of the word-pair classification model, we now describe the extraction of projected dependency instances. In order to alleviate the effect of word alignment errors, we base the projection on the alignment matrix, a compact representation of multiple GIZA++ (Och and Ney, 2000) results, rather than a single word alignment in previous dependency projection works. Figure 2 shows an example. Suppose a bilingual sentence pair, composed of a source sentence e and its target translation f . ye is the parse tree of the source sentence. A is the alignment matrix between them, and each element Ai,j denotes the degree of the alignment between word ei and word fj . We define a boolean-valued function δ(y, i, j, r) to investigate the dependency relationship of word i and word j in parse tree y:  (i, j) ∈ y and r = +     1 or  δ(y, i, j, r) = (i, j) ∈ / y and r = −"
P10-1002,P96-1025,0,\N,Missing
P10-1002,P05-1022,0,\N,Missing
P10-1145,P89-1018,0,0.329379,"translates source constituency forests into target dependency trees with a set of features (Section 4). Medium data experiments (Section 5) show a statistically significant improvement of +0.7 BLEU points over a stateof-the-art forest-based tree-to-string system even with less translation rules, this is also the first time that a tree-to-tree model can surpass tree-to-string counterparts. 2 2.1 Constituency Forests on the Source Side A constituency forest (in Figure 1 left) is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a contextfree grammar (Billot and Lang, 1989). More formally, following Huang (2008), such a constituency forest is a pair Fc = Gf = hV f , H f i, where V f is the set of nodes, and H f the set of hyperedges. For a given source sentence c1:m = c1 . . . cm , each node v f ∈ V f is in the form of X i,j , which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, ci+1 . . . cj ). Each hyperedge hf ∈ H f is a pair htails(hf ), head (hf )i, where head (hf ) ∈ V f is the consequent node in the deductive step, and tails(hf ) ∈ (V f )∗ is the list of antecedent nodes. For example, the hyperedge hf0"
P10-1145,A00-2018,0,0.163944,". In order to alleviate the problem of data sparse, we also compute a dependency language model for POS tages over a dependency tree. We store Our training corpus consists of 239K sentence pairs with about 6.9M/8.9M words in Chinese/English, respectively. We first word-align them by GIZA++ (Och and Ney, 2000) with refinement option “grow-diag-and” (Koehn et al., 2003), and then parse the Chinese sentences using the parser of Xiong et al. (2005) into parse forests, which are pruned into relatively small forests with a pruning threshold 3. We also parse the English sentences using the parser of Charniak (2000) into 1-best constituency trees, which will be converted into dependency trees using Magerman (1995)’s head rules. We also store the POS tag information for each word in dependency trees, and compute two different dependency language models for words and POS tags in dependency tree separately. Finally, we apply translation rule extraction algorithm described in Section 3. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Gigaword corpus. At the decoding step, we again parse the input sentenc"
P10-1145,P05-1033,0,0.657891,"he head word of the CFG rule: NP → NPB CC NPB. After the translation forest is constructed, we traverse each node in translation forest also in bottom-up fashion. For each node, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) to produce partial hypotheses and compute all the feature scores including the dependency language model score (Section 4.1). If all the nodes are visited, we trace back along the 1-best derivation at goal item S0,m and build a target side dependency tree. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. · P(el2 |el1 , eh §) the POS tag information on the target side for each constituency-to-dependency rule. So we will also generate a POS taged dependency tree simultaneously at the decoding time. We calculate this dependency language model by simply replacing each ei in equation 9 with its tag t(ei ). (10) · P(eln |eln−1 , eln−2 ). We use the suffix “§” to distinguish the head word and child words in the dependency language model. In order to alleviate the problem of data spa"
P10-1145,J07-2003,0,0.790092,"or parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over the state-of-the-art formally syntax-based system Hiero (Chiang, 2007). However, those systems also have some limitations that they run slowly (in cubic time) (Huang et al., 2006), and do not utilize the useful syntactic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics a novel constituency-to-dependency model, which uses constituency forests on the source side to direct translation, and depende"
P10-1145,P05-1066,0,0.0597057,"EU points if it is used), their forest-based constituency-to-constituency model achieves a BLEU score of 30.6, which is similar to Moses (Koehn et al., 2007). So our baseline system is much better than the BLEU score (30.6+1) of the constituency-to-constituency system and Moses. System forest c2s forest c2d Rule Set Type # c2s 31.9M s2s 77.9M c2d 13.8M s2d 9.0M c2d 13.8M s2s 77.9M c2d 13.8M s2d 9.0M c2d 13.8M s2s-dep 77.9M BLEU 34.17 32.48(↓1.7) 34.03(↓0.1) 33.25(↓0.9) 34.88(↑0.7) Table 2: Statistics of different types of rules extracted on training corpus and the BLEU scores on the test set. Collins et al. (2005)). For the first time, a tree-totree model can surpass tree-to-string counterparts significantly even with fewer rules. 6 Related Work The concept of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models. Mi et al. (2008) and Mi and Huang (2008) use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following t"
P10-1145,P05-1067,0,0.218116,"lize the useful syntactic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics a novel constituency-to-dependency model, which uses constituency forests on the source side to direct translation, and dependency trees on the target side to guarantee grammaticality of the output. In contrast to conventional tree-to-tree approaches (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009), which only make use of a single type of trees, our model is able to combine two types of trees, outperforming both phrasebased and tree-to-string systems. Current tree-totree models (Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009) still have not outperformed the phrase-based system Moses (Koehn et al., 2007) significantly even with the help of forests.1 Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (S"
P10-1145,W02-1039,0,0.230077,"atched at high level, which is more reasonable than using glue rules in Shen et al. (2008)’s scenario; finally, the most important one is that our model runs very faster. Liu et al. (2009) propose a forest-based constituency-to-constituency model, they put more emphasize on how to utilize parse forest to increase the tree-to-tree rule coverage. By contrast, we only use 1-best dependency trees on the target side to explore long distance relations and extract translation rules. Theoretically, we can extract more rules since dependency tree has the best inter-lingual phrasal cohesion properties (Fox, 2002). 7 Conclusion and Future Work In this paper, we presented a novel forest-based constituency-to-dependency translation model, which combines the advantages of both tree-tostring and string-to-tree systems, runs fast and guarantees grammaticality of the output. To learn the constituency-to-dependency translation rules, we first identify the frontier set for all the nodes in the constituency forest on the source side. Then we fragment them and extract minimal rules. Finally, we glue them together to be composed rules. At the decoding step, we first parse the input sentence into a constituency fo"
P10-1145,N04-1035,0,0.452953,"and vkd ∈ / M, ∀hd if tails(hd ) = vkd ⇒ head (hd ) ∈ ei:j . Take the “ (Bush) held ((a) talk))(with (Sharon)) ” for example: partial fixed examples are “ (Bush) held ” and “ held ((a) talk)”; while the partial floating examples are “ (talk) (with (Sharon)) ” and “ ((a) talk) (with (Sharon)) ”. Please note that the floating structure “ (talk) (with (Sharon)) ” can not be allowed in Shen et al. (2008)’s model. The dependency structure “ held ((a))” is not a well formed structure, since the head of word “a” is out of scope of this structure. (2) fragmentation, (3) composition. The frontier set (Galley et al., 2004) is the potential points to “cut” the forest and dependency tree pair into fragments, each of which will form a minimal rule (Galley et al., 2006). However, not every fragment can be used for rule extraction, since it may or may not respect to the restrictions, such as word alignments and well formed dependency structures. So we say a fragment is extractable if it respects to all restrictions. The root node of every extractable tree fragment corresponds to a faithful structure on the target side, in which case there is a “translational equivalence” between the subtree rooted at the node and th"
P10-1145,P06-1121,0,0.875619,"near time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over the state-of-the-art formally syntax-based system Hiero (Chiang, 2007). However, those systems also have some limitations that they run slowly (in cubic time) (Huang et al., 2006), and do not utilize the useful syntactic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, c Uppsala, Sweden, 11-"
P10-1145,W05-1506,0,0.117175,":NPB is the head word of the CFG rule: NP → NPB CC NPB. After the translation forest is constructed, we traverse each node in translation forest also in bottom-up fashion. For each node, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) to produce partial hypotheses and compute all the feature scores including the dependency language model score (Section 4.1). If all the nodes are visited, we trace back along the 1-best derivation at goal item S0,m and build a target side dependency tree. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. · P(el2 |el1 , eh §) the POS tag information on the target side for each constituency-to-dependency rule. So we will also generate a POS taged dependency tree simultaneously at the decoding time. We calculate this dependency language model by simply replacing each ei in equation 9 with its tag t(ei ). (10) · P(eln |eln−1 , eln−2 ). We use the suffix “§” to distinguish the head word and child words in the dependency language model. In order to alleviate the problem of data spa"
P10-1145,P07-1019,0,0.234129,"y Shen et al. (2008). For each nonterminal node vhd = eh in De and its children sequences Ll = el1 , el2 ...eli and Lr = er1 , er2 ...erj , the probability of a trigram is computed as follows: P(Ll , Lr |eh §) = P(Ll |eh §) · P(Lr |eh §), (9) where the P(Ll |eh §) is decomposed to be: P(Ll |eh §) =P(ell |eh §) ... 5 Experiments 5.1 Data Preparation since the x3 :NPB is the head word of the CFG rule: NP → NPB CC NPB. After the translation forest is constructed, we traverse each node in translation forest also in bottom-up fashion. For each node, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) to produce partial hypotheses and compute all the feature scores including the dependency language model score (Section 4.1). If all the nodes are visited, we trace back along the 1-best derivation at goal item S0,m and build a target side dependency tree. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. · P(el2 |el1 , eh §) the POS tag information on the target side for each constituency-to-dependency rule. So we will"
P10-1145,2006.amta-papers.8,0,0.130613,"ed better than phrasebased counterparts in reorderings. Depending on the type of input, these models can be broadly divided into two categories (see Table 1): the stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar, and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string. When we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee gram"
P10-1145,J09-4009,0,0.0296507,"account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over the state-of-the-art fo"
P10-1145,P08-1067,0,0.118665,"o two categories (see Table 1): the stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar, and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string. When we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-"
P10-1145,N03-1017,0,0.155896,"urce forest Fc , the decoder searches for the best derivation o∗ among the set of all possible derivations O, each of which forms a source side constituent tree Tc (o), a target side string e(o), and where each P(r) is the product of five probabilities: P(r) =P(r |lhs(r))λ9 · P(r |rhs(r))λ10 · P(r |root(lhs(r)))λ11 · Plex (lhs(r) |rhs(r))λ12 (8) · Plex (rhs(r) |lhs(r))λ13 , where the first three are conditional probabilities based on fractional counts of rules defined in Section 3.4, and the last two are lexical probabilities. When computing the lexical translation probabilities described in (Koehn et al., 2003), we only take into accout the terminals in a rule. If there is no terminal, we set the lexical probability to 1. The decoding algorithm works in a bottom-up search fashion by traversing each node in forest Fc . We first use pattern-matching algorithm of Mi et al. (2008) to convert Fc into a translation forest, each hyperedge of which is associated with a constituency to dependency translation rule. However, pattern-matching failure2 at a node v f will Pattern-matching failure at a node v f means there is no translation rule can be matched at v f or no translation hyperedge can be constructed"
P10-1145,P07-2045,0,0.0461637,"onstituency forests on the source side to direct translation, and dependency trees on the target side to guarantee grammaticality of the output. In contrast to conventional tree-to-tree approaches (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009), which only make use of a single type of trees, our model is able to combine two types of trees, outperforming both phrasebased and tree-to-string systems. Current tree-totree models (Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009) still have not outperformed the phrase-based system Moses (Koehn et al., 2007) significantly even with the help of forests.1 Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (Section 3), and translates source constituency forests into target dependency trees with a set of features (Section 4). Medium data experiments (Section 5) show a statistically significant improvement of +0.7 BLEU points over a stateof-the-art forest-based tree-to-string system even with less translation rules, this is also the first time that a tree-to-tree model can surpass tree-to-string counter"
P10-1145,P06-1077,1,0.915915,"t, they are believed better than phrasebased counterparts in reorderings. Depending on the type of input, these models can be broadly divided into two categories (see Table 1): the stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar, and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string. When we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechan"
P10-1145,P07-1089,1,0.806526,"pt of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models. Mi et al. (2008) and Mi and Huang (2008) use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following this direction, Liu et al. (2009) and Zhang et al. (2009) apply forest into tree-to-tree (Zhang et al., 2007) and tree-sequence-to-string models(Liu et al., 2007) respectively. Different from Liu et al. (2009), we apply forest into a new constituency tree to dependency tree translation model rather than constituency tree-to-tree model. Shen et al. (2008) present a string-todependency model. They define the well-formed dependency structures to reduce the size of translation rule set, and integrate a dependency language model in decoding step to exploit long distance word relations. This model shows a significant improvement over the state-of-the-art hierarchical phrase-based system (Chiang, 2005). Compared with this work, we put fewer restrictions on th"
P10-1145,P09-1063,1,0.787813,"Missing"
P10-1145,P08-1066,0,0.646018,"ry-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over the state-of-the-art formally syntax-based system Hiero (Chiang, 2007). However, those systems also have some limitations that they run slowly (in cubic time) (Huang et al., 2006), and do not utilize the useful syntactic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics a novel cons"
P10-1145,I05-1007,1,0.747477,"ch ei in equation 9 with its tag t(ei ). (10) · P(eln |eln−1 , eln−2 ). We use the suffix “§” to distinguish the head word and child words in the dependency language model. In order to alleviate the problem of data sparse, we also compute a dependency language model for POS tages over a dependency tree. We store Our training corpus consists of 239K sentence pairs with about 6.9M/8.9M words in Chinese/English, respectively. We first word-align them by GIZA++ (Och and Ney, 2000) with refinement option “grow-diag-and” (Koehn et al., 2003), and then parse the Chinese sentences using the parser of Xiong et al. (2005) into parse forests, which are pruned into relatively small forests with a pruning threshold 3. We also parse the English sentences using the parser of Charniak (2000) into 1-best constituency trees, which will be converted into dependency trees using Magerman (1995)’s head rules. We also store the POS tag information for each word in dependency trees, and compute two different dependency language models for words and POS tags in dependency tree separately. Finally, we apply translation rule extraction algorithm described in Section 3. We use SRI Language Modeling Toolkit (Stolcke, 2002) to tr"
P10-1145,W07-0706,1,0.884932,"e source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics a novel constituency-to-dependency model, which uses constituency forests on the source side to direct translation, and dependency trees on the target side to guarantee grammaticality of the output. In contrast to conventional tree-to-tree approaches (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009), which only make use of a single type of trees, our model is able to combine two types of trees, outperforming both phrasebased and tree-to-string systems. Current tree-totree models (Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009) still have not outperformed the phrase-based system Moses (Koehn et al., 2007) significantly even with the help of forests.1 Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (Section 3), and translates source constit"
P10-1145,N06-1033,0,0.0319615,"n we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over th"
P10-1145,2007.mtsummit-papers.71,0,0.459482,"us combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics a novel constituency-to-dependency model, which uses constituency forests on the source side to direct translation, and dependency trees on the target side to guarantee grammaticality of the output. In contrast to conventional tree-to-tree approaches (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009), which only make use of a single type of trees, our model is able to combine two types of trees, outperforming both phrasebased and tree-to-string systems. Current tree-totree models (Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009) still have not outperformed the phrase-based system Moses (Koehn et al., 2007) significantly even with the help of forests.1 Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (Section 3), and translates source constituency forests into t"
P10-1145,P09-1020,0,0.505547,"the stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar, and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string. When we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the gramm"
P10-1145,P95-1037,0,0.506184,"Mi et al. (2008) to convert Fc into a translation forest, each hyperedge of which is associated with a constituency to dependency translation rule. However, pattern-matching failure2 at a node v f will Pattern-matching failure at a node v f means there is no translation rule can be matched at v f or no translation hyperedge can be constructed at v f . 1438 2 cut the derivation path and lead to translation failure. To tackle this problem, we construct a pseudo translation rule for each parse hyperedge hf ∈ IN (v f ) by mapping the CFG rule into a target dependency tree using the head rules of Magerman (1995). Take the hyperedge hf0 in Figure1 for example, the corresponding pseudo translation rule is: NP(x1 :NPB x2 :CC x3 :NPB) → (x1 ) (x2 ) x3 , 4.1 Dependency Language Model Computing We compute the score of a dependency language model for a dependency tree De in the same way proposed by Shen et al. (2008). For each nonterminal node vhd = eh in De and its children sequences Ll = el1 , el2 ...eli and Lr = er1 , er2 ...erj , the probability of a trigram is computed as follows: P(Ll , Lr |eh §) = P(Ll |eh §) · P(Lr |eh §), (9) where the P(Ll |eh §) is decomposed to be: P(Ll |eh §) =P(ell |eh §) ..."
P10-1145,W06-1606,0,0.0341931,"ime, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over the state-of-the-art formally syntax-based system Hiero (Chiang, 2007). However, those systems also have some limitations that they run slowly (in cubic time) (Huang et al., 2006), and do not utilize the useful syntactic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, c Uppsala, Sweden, 11-16 July 2010. 2010 As"
P10-1145,D08-1022,1,0.634082,"ded into two categories (see Table 1): the stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar, and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string. When we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-"
P10-1145,P08-1023,1,0.779093,"n be broadly divided into two categories (see Table 1): the stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar, and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string. When we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the ot"
P10-1145,P00-1056,0,0.144015,"erate a POS taged dependency tree simultaneously at the decoding time. We calculate this dependency language model by simply replacing each ei in equation 9 with its tag t(ei ). (10) · P(eln |eln−1 , eln−2 ). We use the suffix “§” to distinguish the head word and child words in the dependency language model. In order to alleviate the problem of data sparse, we also compute a dependency language model for POS tages over a dependency tree. We store Our training corpus consists of 239K sentence pairs with about 6.9M/8.9M words in Chinese/English, respectively. We first word-align them by GIZA++ (Och and Ney, 2000) with refinement option “grow-diag-and” (Koehn et al., 2003), and then parse the Chinese sentences using the parser of Xiong et al. (2005) into parse forests, which are pruned into relatively small forests with a pruning threshold 3. We also parse the English sentences using the parser of Charniak (2000) into 1-best constituency trees, which will be converted into dependency trees using Magerman (1995)’s head rules. We also store the POS tag information for each word in dependency trees, and compute two different dependency language models for words and POS tags in dependency tree separately."
P10-1145,P03-1021,0,0.0525495,"oothing on the first 1/3 of the Xinhua portion of Gigaword corpus. At the decoding step, we again parse the input sentences into forests and prune them with a threshold 10, which will direct the translation (Section 4). We use the 2002 NIST MT Evaluation test set as our development set and the 2005 NIST MT Evaluation test set as our test set. We evaluate the translation quality using the BLEU-4 metric (Papineni et al., 2002), which is calculated by the script mteval-v11b.pl with its default setting which is case-insensitive matching of n-grams. We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set. 5.2 Results Table 2 shows the results on the test set. Our baseline system is a state-of-the-art forest-based constituency-to-string model (Mi et al., 2008), or forest c2s for short, which translates a source forest into a target string by pattern-matching the 1439 constituency-to-string (c2s) rules and the bilingual phrases (s2s). The baseline system extracts 31.9M c2s rules, 77.9M s2s rules respectively and achieves a BLEU score of 34.17 on the test set3 . At first, we investigate the influence of different"
P10-1145,P02-1040,0,0.0858967,"parately. Finally, we apply translation rule extraction algorithm described in Section 3. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Gigaword corpus. At the decoding step, we again parse the input sentences into forests and prune them with a threshold 10, which will direct the translation (Section 4). We use the 2002 NIST MT Evaluation test set as our development set and the 2005 NIST MT Evaluation test set as our test set. We evaluate the translation quality using the BLEU-4 metric (Papineni et al., 2002), which is calculated by the script mteval-v11b.pl with its default setting which is case-insensitive matching of n-grams. We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set. 5.2 Results Table 2 shows the results on the test set. Our baseline system is a state-of-the-art forest-based constituency-to-string model (Mi et al., 2008), or forest c2s for short, which translates a source forest into a target string by pattern-matching the 1439 constituency-to-string (c2s) rules and the bilingual phrases (s2s)."
P10-1145,P05-1034,0,0.168882,"ic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics a novel constituency-to-dependency model, which uses constituency forests on the source side to direct translation, and dependency trees on the target side to guarantee grammaticality of the output. In contrast to conventional tree-to-tree approaches (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009), which only make use of a single type of trees, our model is able to combine two types of trees, outperforming both phrasebased and tree-to-string systems. Current tree-totree models (Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009) still have not outperformed the phrase-based system Moses (Koehn et al., 2007) significantly even with the help of forests.1 Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (Section 3), and trans"
P10-1145,P08-1064,0,\N,Missing
P10-2003,P05-1022,0,0.10367,"Missing"
P10-2003,P08-1067,0,0.0732652,"Missing"
P10-2003,N03-1017,0,0.0461596,"ations of a sentence pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method. 1 Figure 1: Occurrence of a swap with different numbers of adjacent bilingual phrases: only one phrase in (a) and three phrases in (b). Black squares denote word alignments and gray rectangles denote bilingual phrases. [s,t] indicates the target-side span of bilingual phrase bp and [u,v] represents the source-side span of bilingual phrase bp. Introduction Phrase-based translation systems (Koehn et al., 2003; Och and Ney, 2004) prove to be the stateof-the-art as they have delivered translation performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering mod"
P10-2003,P07-2045,0,0.162702,"ion evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s − 1, u − 1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model (Tillmann, 2004) determines the presence of the adjacent bi"
P10-2003,W04-3250,0,0.0231813,"e used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU scores differences. 3.2 Experimental Results Table 2 shows the results of experiments with the small training corpus. For the msd-fe model, the BLEU scores by our method are 30.51 32.78 and 29.50, achieving absolute improvements of 0.89, 0.66 and 0.62 on the three test sets, respectively. For the msd-bidirectional-fe model, our method obtains BLEU scores of 30.49 32.73 and 29.24, with absolute improvements of 1.11, 0.73 and 0.60 over the baseline method. 1 The phrase-based lexical reordering model (Tillmann, 2004) is also closely related to our model. However,"
P10-2003,J03-1002,0,0.00561231,"inuous. If a msd-bidirectional-fe model is used, then the number of features doubles: one for each direction. 3.1 Experiment Setup Two different sizes of training corpora are used in our experiments: one is a small-scale corpus that comes from FBIS corpus consisting of 239K bilingual sentence pairs, the other is a large-scale corpus that includes 1.55M bilingual sentence pairs from LDC. The 2002 NIST MT evaluation test data is used as the development set and the 2003, 2004, 2005 NIST MT test data are the test sets. We choose the MOSES1 (Koehn et al., 2007) as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Fin"
P10-2003,J04-4002,0,0.0483088,"pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method. 1 Figure 1: Occurrence of a swap with different numbers of adjacent bilingual phrases: only one phrase in (a) and three phrases in (b). Black squares denote word alignments and gray rectangles denote bilingual phrases. [s,t] indicates the target-side span of bilingual phrase bp and [u,v] represents the source-side span of bilingual phrase bp. Introduction Phrase-based translation systems (Koehn et al., 2003; Och and Ney, 2004) prove to be the stateof-the-art as they have delivered translation performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advant"
P10-2003,P03-1021,0,0.210537,"cally, the model collects bilingual phrases and distinguishes their orientations with respect to the previous bilingual phrase into three categories:   M o= S   D ai − ai−1 = 1 ai − ai−1 = −1 |ai − ai−1 |6= 1 (1) Using the relative-frequency approach, the reordering probability regarding bp is Count(o, bp) 0 o0 Count(o , bp) p(o|bp) = P (2) 2.2 Reordering Graph For a parallel sentence pair, its reordering graph indicates all possible translation derivations consisting of the extracted bilingual phrases. To construct a reordering graph, we first extract bilingual phrases using the way of (Och, 2003). Then, the adjacent bilingual phrases are linked according to the targetside order. Some bilingual phrases, which have no adjacent bilingual phrases because of maximum length limitation, are linked to the nearest bilingual phrases in the target-side order. Shown in Figure 2(b), the reordering graph for the parallel sentence pair (Figure 2(a)) can be represented as an undirected graph, where each rectangle corresponds to a phrase pair, each link is the orientation relationship between adjacent bilingual phrases, and two distinguished rectangles bs and be indicate the beginning and ending of th"
P10-2003,P02-1040,0,0.082129,"GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU scores differences. 3.2 Experimental Results Table 2 shows the results of experiments with the small training corpus. For the msd-fe model, the BLEU scores by our method are 30.51 32.78 and 29.50, achieving absolute improvements of 0.89, 0.66 and 0.62 on the three test sets, respectively. For the msd-bidirectional-fe model, our method obtains BLEU scores of 30.49 32.73 and 29.24, with absolute improvements of 1.11, 0.73 and 0.60 over the baseline method. 1 The phrase-based lexical reordering model (T"
P10-2003,N04-4026,0,0.371205,"vered translation performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s − 1, u − 1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model"
P10-2003,P06-1066,1,0.837358,"n performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s − 1, u − 1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model (Tillmann, 2004) de"
P10-2003,W06-3108,0,0.02101,"ent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s − 1, u − 1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model (Tillmann, 2004) determines the presenc"
P10-2003,D08-1089,0,\N,Missing
P10-2026,J93-2003,0,0.0150146,"rules while with translation performance improvement, and the new feature brings another improvement to the baseline system, especially on larger corpus. Source Target e Figure 1: Solid wire reveals the dependency relation pointing from the child to the parent. Target word e is triggered by the source word f and it’s head word f ′ , p(e|f → f ′ ). Based on the relaxed-well-formed dependency structure, we also introduce a new linguistic feature to enhance translation performance. In the traditional phrase-based SMT model, there are always lexical translation probabilities based on IBM model 1 (Brown et al., 1993), i.e. p(e|f ), namely, the target word e is triggered by the source word f . Intuitively, however, the generation of e is not only involved with f , sometimes may also be triggered by other context words in the source side. Here we assume that the dependency edge (f → f ′ ) of word f generates target word e (we call it head word trigger in Section 4). Therefore, two words in one language trigger one word in another, which provides a more sophisticated and better choice for the target word, i.e. Figure 1. Similarly, the dependency feature works well in Chinese-to-English translation task, espe"
P10-2026,J07-2003,0,0.0968596,"Missing"
P10-2026,P05-1067,0,0.0252036,"phrase translate to each other. Source word f aligns with target word e, according to the IBM model 1, the lexical translation probability is p(e|f ). However, in the sense of dependency relationship, we believe that the generation of the target word e, is not only triggered by the aligned source word f , but also associated with f ’s head word f ′ . Therefore, the lexical translation probability becomes p(e|f → f ′ ), which of course allows for a more fine-grained lexical choice of 3 Relaxed-well-formed Dependency Structure Dependency models have recently gained considerable interest in SMT (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Dependency tree can represent richer structural information. It reveals long-distance relation between words and directly models the semantic structure of a sentence without any constituent labels. Fig143 the target word. More specifically, the probability could be estimated by the maximum likelihood (MLE) approach, For language model, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4gram model on the first 1/3 of the Xinhua portion of GIGAWORD corpus. And we use the NIST 2002 MT evaluation test set as our development count(e, f → f"
P10-2026,N09-2005,0,0.0124096,"es of the target-side are well-formed (WF) dependency structure, but this filtering led to degradation in translation performance. They obtained improvements by adding an additional dependency language model. The basic difference of our method from (Shen et al., 2008) is that we keep rules that both sides should be relaxed-wellformed dependency structure, not just the target side. Besides, our system complexity is not increased because no additional language model is introduced. The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach (Hasan and Ney, 2009). Hasan and Ney (2009) introduced a second word to trigger the target word without considering any linguistic information. Furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved. Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding. However, as the size of the corpus increases, the maximum entropy model will become larger. Similarly, In (Shen et al., 2009), context language model is proposed for better rule selection. Ta"
P10-2026,P03-1021,0,0.0148163,"in a 4gram model on the first 1/3 of the Xinhua portion of GIGAWORD corpus. And we use the NIST 2002 MT evaluation test set as our development count(e, f → f ′ ) ′ set, and NIST 2004, 2005 test sets as our blind p(e|f → f ) = P (1) ′ ′ e′ count(e , f → f ) test sets. We evaluate the translation quality using case-insensitive BLEU metric (Papineni et Given a phrase pair f , e and word alignment al., 2002) without dropping OOV words, and the a, and the dependent relation of the source senfeature weights are tuned by minimum error rate tence dJ1 (J is the length of the source sentence, training (Och, 2003). I is the length of the target sentence). Therefore, In order to get the dependency relation of the given the lexical translation probability distributraining corpus, we re-implement a beam-search tion p(e|f → f ′ ), we compute the feature score of style monolingual dependency parser according a phrase pair (f , e) as to (Nivre and Scholz, 2004). Then we use the same method suggested in (Chiang, 2005) to extract SCFG grammar rules within dependency J p(e|f , d1 , a) constraint on both sides except that unaligned X 1 |e| (2) words are allowed at the edge of phrases. Pa= Πi=1 p(ei |fj → fdj ) |"
P10-2026,P02-1040,0,0.079149,"Missing"
P10-2026,P05-1034,0,0.0329543,"h other. Source word f aligns with target word e, according to the IBM model 1, the lexical translation probability is p(e|f ). However, in the sense of dependency relationship, we believe that the generation of the target word e, is not only triggered by the aligned source word f , but also associated with f ’s head word f ′ . Therefore, the lexical translation probability becomes p(e|f → f ′ ), which of course allows for a more fine-grained lexical choice of 3 Relaxed-well-formed Dependency Structure Dependency models have recently gained considerable interest in SMT (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Dependency tree can represent richer structural information. It reveals long-distance relation between words and directly models the semantic structure of a sentence without any constituent labels. Fig143 the target word. More specifically, the probability could be estimated by the maximum likelihood (MLE) approach, For language model, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4gram model on the first 1/3 of the Xinhua portion of GIGAWORD corpus. And we use the NIST 2002 MT evaluation test set as our development count(e, f → f ′ ) ′ set, and NIST"
P10-2026,P08-1066,0,0.0961538,"wj are directly or indirectly depended on wh or -1 (here we define h = −1). If and only if it satisfies the following conditions found girl The lovely house a beautiful Figure 2: An example of dependency tree. The corresponding plain sentence is The lovely girl found a beautiful house. most entries of the rule table by using the constraint that rules of the target-side are well-formed (WF) dependency structure, but this filtering led to degradation in translation performance. They obtained improvements by adding an additional dependency language model. The basic difference of our method from (Shen et al., 2008) is that we keep rules that both sides should be relaxed-wellformed dependency structure, not just the target side. Besides, our system complexity is not increased because no additional language model is introduced. The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach (Hasan and Ney, 2009). Hasan and Ney (2009) introduced a second word to trigger the target word without considering any linguistic information. Furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of param"
P10-2026,D08-1039,0,0.0239488,"Missing"
P10-2026,D09-1008,0,0.0115696,"he log-linear model is motivated by the trigger-based approach (Hasan and Ney, 2009). Hasan and Ney (2009) introduced a second word to trigger the target word without considering any linguistic information. Furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved. Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding. However, as the size of the corpus increases, the maximum entropy model will become larger. Similarly, In (Shen et al., 2009), context language model is proposed for better rule selection. Taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information. / [i, j] • dh ∈ • ∀k ∈ [i, j], dk ∈ [i, j] or dk = h From the definition above, we can see that the relaxed-well-formed structure obviously covers the well-formed one. In this structure, we don’t constrain that all the children of the sub-root should be complete. Let’s review the dependency tree in Figure 2 as an example. Except for the wellformed structure, we could also extract girl found a beautiful"
P10-2026,C08-1041,1,0.849291,"at both sides should be relaxed-wellformed dependency structure, not just the target side. Besides, our system complexity is not increased because no additional language model is introduced. The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach (Hasan and Ney, 2009). Hasan and Ney (2009) introduced a second word to trigger the target word without considering any linguistic information. Furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved. Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding. However, as the size of the corpus increases, the maximum entropy model will become larger. Similarly, In (Shen et al., 2009), context language model is proposed for better rule selection. Taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information. / [i, j] • dh ∈ • ∀k ∈ [i, j], dk ∈ [i, j] or dk = h From the definition above, we can see that the relaxed-well-formed structure obviously covers the well-form"
P10-2026,P09-2031,1,0.89014,"Missing"
P10-2026,D09-1127,1,0.892316,"Missing"
P10-2026,E09-1044,0,0.0381346,"Missing"
P10-2026,N03-1017,0,0.00692778,"pproach is very different from previous approaches of exploring context information. / [i, j] • dh ∈ • ∀k ∈ [i, j], dk ∈ [i, j] or dk = h From the definition above, we can see that the relaxed-well-formed structure obviously covers the well-formed one. In this structure, we don’t constrain that all the children of the sub-root should be complete. Let’s review the dependency tree in Figure 2 as an example. Except for the wellformed structure, we could also extract girl found a beautiful house. Therefore, if the modifier The lovely changes to The cute, this rule also works. 4 Head Word Trigger (Koehn et al., 2003) introduced the concept of lexical weighting to check how well words of the phrase translate to each other. Source word f aligns with target word e, according to the IBM model 1, the lexical translation probability is p(e|f ). However, in the sense of dependency relationship, we believe that the generation of the target word e, is not only triggered by the aligned source word f , but also associated with f ’s head word f ′ . Therefore, the lexical translation probability becomes p(e|f → f ′ ), which of course allows for a more fine-grained lexical choice of 3 Relaxed-well-formed Dependency Str"
P10-2026,C04-1010,0,0.0115405,"ric (Papineni et Given a phrase pair f , e and word alignment al., 2002) without dropping OOV words, and the a, and the dependent relation of the source senfeature weights are tuned by minimum error rate tence dJ1 (J is the length of the source sentence, training (Och, 2003). I is the length of the target sentence). Therefore, In order to get the dependency relation of the given the lexical translation probability distributraining corpus, we re-implement a beam-search tion p(e|f → f ′ ), we compute the feature score of style monolingual dependency parser according a phrase pair (f , e) as to (Nivre and Scholz, 2004). Then we use the same method suggested in (Chiang, 2005) to extract SCFG grammar rules within dependency J p(e|f , d1 , a) constraint on both sides except that unaligned X 1 |e| (2) words are allowed at the edge of phrases. Pa= Πi=1 p(ei |fj → fdj ) |{j|(j, i) ∈ a}| rameters of head word trigger are estimated as de∀(j,i)∈a scribed in Section 4. As a default, the maximum initial phrase length is set to 10 and the maximum Now we get p(e|f , dJ1 , a), we could obtain rule length of the source side is set to 5. Besides, p(f |e, dI1 , a) (dI1 represents dependent relation of we also re-implement t"
P10-2026,P05-1033,0,\N,Missing
P11-1128,C90-3001,0,0.408538,"od candidate. As a formal tree rewriting system, TAG (Joshi et al., 1975; Joshi, 1985) provides a larger domain of locality than CFG to state linguistic dependencies that are far apart since the formalism treats trees as basic building blocks. As a mildly context-sensitive grammar, TAG is conjectured to be powerful enough to model natural languages. Synchronous TAG generalizes TAG by allowing the construction of a pair of trees using the TAG operations of substitution and adjoining on tree pairs. The idea of using synchronous TAG in machine translation has been pursued by several researchers (Abeille et al., 1990; Prigent, 1994; Dras, 1999), but only recently in its probabilistic form (Nesson et al., 2006; DeNeefe and Knight, 2009). Shieber (2007) argues that probabilistic synchronous TAG possesses appealing properties such as expressivity and trainability for building a machine translation system. However, one major challenge for applying synchronous TAG to machine translation is computational complexity. While TAG requires O(n6 ) time for monolingual parsing, synchronous TAG requires O(n12 ) for bilingual parsing. One solution is to use tree insertion grammars (TIG) introduced by Schabes and Waters"
P11-1128,2000.iwpt-1.9,0,0.081984,"h α; Ps (α|η) is the probability of substituting α at η; Pa (β|η) is the probability of adjoining β at η; finally, Pa (NONE|η) is the probability of nothing adjoining at η. For tree-to-string translation, these parameters can be treated as feature functions of a discriminative framework (Och, 2003) combined with other conventional features such as relative frequency, lexical weight, rule count, language model, and word count (Liu et al., 2006). 3 Rule Extraction Inducing a synchronous TAG from training data often begins with converting Treebank-style parse trees to TAG derivations (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2003). DeNeefe and 1281 Knight (2009) propose an algorithm to extract synchronous TIG rules from an aligned and parsed bilingual corpus. They first classify tree nodes into heads, arguments, and adjuncts using heuristics (Collins, 2003), then transform a Treebank-style tree into a TIG derivation, and finally extract minimallysized rules from the derivation tree and the string on the other side, constrained by the alignments. Probabilistic models can be estimated by collecting counts over the derivation trees. However, one challenge is that there are many TAG derivations that can yiel"
P11-1128,J07-2003,0,0.834293,"ing translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets. 1 Introduction Syntax-based translation models, which exploit hierarchical structures of natural languages to guide machine translation, have become increasingly popular in recent years. So far, most of them have been based on synchronous context-free grammars (CFG) (Chiang, 2007), tree substitution grammars (TSG) (Eisner, 2003; Galley et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and inversion transduction grammars (ITG) (Wu, 1997; Xiong et al., 2006). Although these formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax. For example, Chiang (2006) points out that the translation of languages that can stack an unbounded number of clauses in an “inside-out” way (Wu, 1997) 1278 provably goes beyond the e"
P11-1128,P10-1146,0,0.0616372,"Missing"
P11-1128,J03-4003,0,0.0141421,"a discriminative framework (Och, 2003) combined with other conventional features such as relative frequency, lexical weight, rule count, language model, and word count (Liu et al., 2006). 3 Rule Extraction Inducing a synchronous TAG from training data often begins with converting Treebank-style parse trees to TAG derivations (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2003). DeNeefe and 1281 Knight (2009) propose an algorithm to extract synchronous TIG rules from an aligned and parsed bilingual corpus. They first classify tree nodes into heads, arguments, and adjuncts using heuristics (Collins, 2003), then transform a Treebank-style tree into a TIG derivation, and finally extract minimallysized rules from the derivation tree and the string on the other side, constrained by the alignments. Probabilistic models can be estimated by collecting counts over the derivation trees. However, one challenge is that there are many TAG derivations that can yield the same derived tree, even with respect to a single grammar. It is difficult to choose appropriate single derivations that enable the resulting grammar to translate unseen data well. DeNeefe and Knight (2009) indicate that the way to reconstru"
P11-1128,D09-1076,0,0.500129,"ocality than CFG to state linguistic dependencies that are far apart since the formalism treats trees as basic building blocks. As a mildly context-sensitive grammar, TAG is conjectured to be powerful enough to model natural languages. Synchronous TAG generalizes TAG by allowing the construction of a pair of trees using the TAG operations of substitution and adjoining on tree pairs. The idea of using synchronous TAG in machine translation has been pursued by several researchers (Abeille et al., 1990; Prigent, 1994; Dras, 1999), but only recently in its probabilistic form (Nesson et al., 2006; DeNeefe and Knight, 2009). Shieber (2007) argues that probabilistic synchronous TAG possesses appealing properties such as expressivity and trainability for building a machine translation system. However, one major challenge for applying synchronous TAG to machine translation is computational complexity. While TAG requires O(n6 ) time for monolingual parsing, synchronous TAG requires O(n12 ) for bilingual parsing. One solution is to use tree insertion grammars (TIG) introduced by Schabes and Waters (1995). As a restricted form of TAG, TIG still allows for adjoining of unbounded trees but only requires O(n3 ) time for"
P11-1128,P99-1011,0,0.0396303,"ing system, TAG (Joshi et al., 1975; Joshi, 1985) provides a larger domain of locality than CFG to state linguistic dependencies that are far apart since the formalism treats trees as basic building blocks. As a mildly context-sensitive grammar, TAG is conjectured to be powerful enough to model natural languages. Synchronous TAG generalizes TAG by allowing the construction of a pair of trees using the TAG operations of substitution and adjoining on tree pairs. The idea of using synchronous TAG in machine translation has been pursued by several researchers (Abeille et al., 1990; Prigent, 1994; Dras, 1999), but only recently in its probabilistic form (Nesson et al., 2006; DeNeefe and Knight, 2009). Shieber (2007) argues that probabilistic synchronous TAG possesses appealing properties such as expressivity and trainability for building a machine translation system. However, one major challenge for applying synchronous TAG to machine translation is computational complexity. While TAG requires O(n6 ) time for monolingual parsing, synchronous TAG requires O(n12 ) for bilingual parsing. One solution is to use tree insertion grammars (TIG) introduced by Schabes and Waters (1995). As a restricted form"
P11-1128,P03-2041,0,0.501101,"problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets. 1 Introduction Syntax-based translation models, which exploit hierarchical structures of natural languages to guide machine translation, have become increasingly popular in recent years. So far, most of them have been based on synchronous context-free grammars (CFG) (Chiang, 2007), tree substitution grammars (TSG) (Eisner, 2003; Galley et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and inversion transduction grammars (ITG) (Wu, 1997; Xiong et al., 2006). Although these formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax. For example, Chiang (2006) points out that the translation of languages that can stack an unbounded number of clauses in an “inside-out” way (Wu, 1997) 1278 provably goes beyond the expressive power of synchronous CFG and TSG. Ther"
P11-1128,N04-1035,0,0.830186,"adjoined at NN-X in α2 to generate α3 . the use of synchronous TIG for machine translation and report promising results. DeNeefe and Knight (2009) prove that adjoining can improve translation quality significantly over a state-of-the-art stringto-tree system (Galley et al., 2006) that uses synchronous TSG with tractable computational complexity. In this paper, we introduce synchronous TAG into tree-to-string translation (Liu et al., 2006; Huang et al., 2006), which is the simplest and fastest among syntax-based approaches (Section 2). We propose a new rule extraction algorithm based on GHKM (Galley et al., 2004) that directly induces a synchronous TAG from an aligned and parsed bilingual corpus without converting Treebank-style trees to TAG derivations explicitly (Section 3). As tree-tostring translation takes a source parse tree as input, the decoding can be cast as a tree parsing problem (Eisner, 2003): reconstructing TAG derivations from a derived tree using tree-to-string rules that allow for both substitution and adjoining. We describe how to convert TAG derivations to translation forest (Section 4). We evaluated the new tree-to-string system on NIST Chinese-English tests and obtained consistent"
P11-1128,P06-1121,0,0.528016,"than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets. 1 Introduction Syntax-based translation models, which exploit hierarchical structures of natural languages to guide machine translation, have become increasingly popular in recent years. So far, most of them have been based on synchronous context-free grammars (CFG) (Chiang, 2007), tree substitution grammars (TSG) (Eisner, 2003; Galley et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and inversion transduction grammars (ITG) (Wu, 1997; Xiong et al., 2006). Although these formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax. For example, Chiang (2006) points out that the translation of languages that can stack an unbounded number of clauses in an “inside-out” way (Wu, 1997) 1278 provably goes beyond the expressive power of synchronous CFG and TSG. Therefore, it is necessar"
P11-1128,P07-1019,0,0.349006,"ring is no longer than 7. To learn the probability models Pi (α), Ps (α|η), Pa (β|η), and Pa (NONE|η), we collect and normalize counts over these extracted rules following DeNeefe and Knight (2009). 4 Decoding Given a synchronous TAG and a derived source tree π, a tree-to-string decoder finds the English yield of the best derivation of which the Chinese yield matches π:   eˆ = e arg max P (D) (4) D s.t. f (D)=π This is called tree parsing (Eisner, 2003) as the decoder finds ways of decomposing π into elementary trees. Tree-to-string decoding with STSG is usually treated as forest rescoring (Huang and Chiang, 2007) that involves two steps. The decoder first converts the input tree into a translation forest using a translation rule set by pattern matching. Huang et al. (2006) show that this step is a depth-first search with memorization in O(n) time. Then, the decoder searches for the best derivation in the translation forest intersected with n-gram language models and outputs the target string. 2 Decoding with STAG, however, poses one major challenge to forest rescoring. As translation forest only supports substitution, it is difficult to construct a translation forest for STAG derivations because of 2"
P11-1128,D10-1027,0,0.0471897,"slation forest using a translation rule set by pattern matching. Huang et al. (2006) show that this step is a depth-first search with memorization in O(n) time. Then, the decoder searches for the best derivation in the translation forest intersected with n-gram language models and outputs the target string. 2 Decoding with STAG, however, poses one major challenge to forest rescoring. As translation forest only supports substitution, it is difficult to construct a translation forest for STAG derivations because of 2 Mi et al. (2008) give a detailed description of the two-step decoding process. Huang and Mi (2010) systematically analyze the decoding complexity of tree-to-string translation. α1 α2 IP0,8 NP2,3 β1 NR2,3 nê VP3,8 ↓ ` aob¯ amˇ a NR2,3 ↓ β2 NP0,2 ↓ NP0,1 NR0,1 ↓ r1 r2 r3 r4 r5 r6 NP2,3 ∗ NN1,2 ↓ NP0,2 NP2,3 ∗ elementary tree α1 α2 β1 β2 β3 α3 NP1,2 α3 β3 NP0,3 NP0,3 NN2,3 oÚ NP1,2 ∗ zˇ ongtˇ ong translation rule ( IP ( NP0:1 ( x1 :NR↓ ) ) ( x2 :VP↓ ) ) → x1 x2 ( NR a` ob¯amˇa ) → Obama ( NP ( NP0:1 ( x1 :NN↓ ) ) ( x2 :NP∗ ) ) → x1 x2 ( NP ( x1 :NP↓ ) ( x2 :NP∗ ) ) → x1 x2 ( NP ( NP ( x1 :NR↓ ) ) ( x2 :NP∗ ) ) → x1 x2 ( NN zˇongtˇong ) → President Figure 3: Matched trees and corresponding rul"
P11-1128,2006.amta-papers.8,0,0.27879,"chine translation, have become increasingly popular in recent years. So far, most of them have been based on synchronous context-free grammars (CFG) (Chiang, 2007), tree substitution grammars (TSG) (Eisner, 2003; Galley et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and inversion transduction grammars (ITG) (Wu, 1997; Xiong et al., 2006). Although these formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax. For example, Chiang (2006) points out that the translation of languages that can stack an unbounded number of clauses in an “inside-out” way (Wu, 1997) 1278 provably goes beyond the expressive power of synchronous CFG and TSG. Therefore, it is necessary to find ways to take advantage of more powerful synchronous grammars to improve machine translation. Synchronous tree adjoining grammars (TAG) (Shieber and Schabes, 1990) are a good candidate. As a formal tree rewriting system, TAG (Joshi et al., 1975; Joshi, 1985) provides a larger domain of locality than CFG to state linguistic dependencies that are far apart since th"
P11-1128,P07-2045,0,0.00420358,"e root node. For example, in Figure 2, the distance between NP0,1 and NP0,3 is 2 and the distance between VP6,8 and VP3,8 is 1. As most foot nodes are usually very close to the root nodes, we restrict that a foot node must be the direct descendant of the root node in our experiments. Table 3 shows the BLEU scores on the NIST Chinese-English test sets. Our baseline system is the tree-to-string system using STSG (Liu et al., 2006; Huang et al., 2006). The STAG system outperforms the STSG system significantly on the MT04 and MT05 test sets at pl.01 level. Table 3 also gives the results of Moses (Koehn et al., 2007) and an in-house hierarchical phrase-based system (Chiang, 2007). Our STAG system achieves comparable performance with the hierarchical system. The absolute improvement of +0.7 BLEU over STSG is close to the finding of DeNeefe and Knight (2009) on string-to-tree translation. We feel that one major obstacle for achieving further improvement is that composed rules generated on the fly during decoding (e.g., r1 + r3 + r5 in Figure 4) usually have too many non-terminals, making cube pruning in the inmatching conversion intersection other total STSG 0.086 0.000 0.946 0.012 1.044 STAG 0.109 0.562 1."
P11-1128,P06-1077,1,0.400643,"coder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets. 1 Introduction Syntax-based translation models, which exploit hierarchical structures of natural languages to guide machine translation, have become increasingly popular in recent years. So far, most of them have been based on synchronous context-free grammars (CFG) (Chiang, 2007), tree substitution grammars (TSG) (Eisner, 2003; Galley et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and inversion transduction grammars (ITG) (Wu, 1997; Xiong et al., 2006). Although these formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax. For example, Chiang (2006) points out that the translation of languages that can stack an unbounded number of clauses in an “inside-out” way (Wu, 1997) 1278 provably goes beyond the expressive power of synchronous CFG and TSG. Therefore, it is necessary to find ways to"
P11-1128,P09-1063,1,0.901939,"Missing"
P11-1128,D08-1022,0,0.0473835,"Missing"
P11-1128,P08-1023,1,0.946539,"that involves two steps. The decoder first converts the input tree into a translation forest using a translation rule set by pattern matching. Huang et al. (2006) show that this step is a depth-first search with memorization in O(n) time. Then, the decoder searches for the best derivation in the translation forest intersected with n-gram language models and outputs the target string. 2 Decoding with STAG, however, poses one major challenge to forest rescoring. As translation forest only supports substitution, it is difficult to construct a translation forest for STAG derivations because of 2 Mi et al. (2008) give a detailed description of the two-step decoding process. Huang and Mi (2010) systematically analyze the decoding complexity of tree-to-string translation. α1 α2 IP0,8 NP2,3 β1 NR2,3 nê VP3,8 ↓ ` aob¯ amˇ a NR2,3 ↓ β2 NP0,2 ↓ NP0,1 NR0,1 ↓ r1 r2 r3 r4 r5 r6 NP2,3 ∗ NN1,2 ↓ NP0,2 NP2,3 ∗ elementary tree α1 α2 β1 β2 β3 α3 NP1,2 α3 β3 NP0,3 NP0,3 NN2,3 oÚ NP1,2 ∗ zˇ ongtˇ ong translation rule ( IP ( NP0:1 ( x1 :NR↓ ) ) ( x2 :VP↓ ) ) → x1 x2 ( NR a` ob¯amˇa ) → Obama ( NP ( NP0:1 ( x1 :NN↓ ) ) ( x2 :NP∗ ) ) → x1 x2 ( NP ( x1 :NP↓ ) ( x2 :NP∗ ) ) → x1 x2 ( NP ( NP ( x1 :NR↓ ) ) ( x2 :NP∗ ) ) →"
P11-1128,2006.amta-papers.15,0,0.0196565,"a larger domain of locality than CFG to state linguistic dependencies that are far apart since the formalism treats trees as basic building blocks. As a mildly context-sensitive grammar, TAG is conjectured to be powerful enough to model natural languages. Synchronous TAG generalizes TAG by allowing the construction of a pair of trees using the TAG operations of substitution and adjoining on tree pairs. The idea of using synchronous TAG in machine translation has been pursued by several researchers (Abeille et al., 1990; Prigent, 1994; Dras, 1999), but only recently in its probabilistic form (Nesson et al., 2006; DeNeefe and Knight, 2009). Shieber (2007) argues that probabilistic synchronous TAG possesses appealing properties such as expressivity and trainability for building a machine translation system. However, one major challenge for applying synchronous TAG to machine translation is computational complexity. While TAG requires O(n6 ) time for monolingual parsing, synchronous TAG requires O(n12 ) for bilingual parsing. One solution is to use tree insertion grammars (TIG) introduced by Schabes and Waters (1995). As a restricted form of TAG, TIG still allows for adjoining of unbounded trees but onl"
P11-1128,J03-1002,0,0.00360945,"n training corpus. Figure 5: Average occurrences of foot node labels VP, NP, and IP over various distances. 5 Evaluation We evaluated our adjoining tree-to-string translation system on Chinese-English translation. The bilingual corpus consists of 1.5M sentences with 42.1M Chinese words and 48.3M English words. The Chinese sentences in the bilingual corpus were parsed by an in-house parser. To maintain a reasonable grammar size, we follow Liu et al. (2006) to restrict that the height of a rule tree is no greater than 3 and the surface string’s length is no greater than 7. After running GIZA++ (Och and Ney, 2003) to obtain word alignment, our rule extraction algorithm extracted 23.0M initial rules without adjoining sites, 6.6M initial rules with adjoining sites, and 5.3M auxiliary rules. We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus, which contains 238M English words. We used the 2002 NIST MT Chinese-English test set as the development set and the 2003-2005 NIST test sets as the test sets. We evaluated translation quality using the BLEU metric, as calculated by mteval-v11b.pl with case-insensitive matching of n-grams. Table 2 sh"
P11-1128,P03-1021,0,0.0220395,"n source and target non-terminals. The parameters of a probabilistic synchronous TAG are X Pi (α) = 1 (1) Ps (α|η) = 1 (2) Pa (β|η) + Pa (NONE|η) = 1 (3) α X α X β where α ranges over initial tree pairs, β over auxiliary tree pairs, and η over node pairs. Pi (α) is the probability of beginning a derivation with α; Ps (α|η) is the probability of substituting α at η; Pa (β|η) is the probability of adjoining β at η; finally, Pa (NONE|η) is the probability of nothing adjoining at η. For tree-to-string translation, these parameters can be treated as feature functions of a discriminative framework (Och, 2003) combined with other conventional features such as relative frequency, lexical weight, rule count, language model, and word count (Liu et al., 2006). 3 Rule Extraction Inducing a synchronous TAG from training data often begins with converting Treebank-style parse trees to TAG derivations (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2003). DeNeefe and 1281 Knight (2009) propose an algorithm to extract synchronous TIG rules from an aligned and parsed bilingual corpus. They first classify tree nodes into heads, arguments, and adjuncts using heuristics (Collins, 2003), then transform a Treeba"
P11-1128,J95-4002,0,0.132014,"beille et al., 1990; Prigent, 1994; Dras, 1999), but only recently in its probabilistic form (Nesson et al., 2006; DeNeefe and Knight, 2009). Shieber (2007) argues that probabilistic synchronous TAG possesses appealing properties such as expressivity and trainability for building a machine translation system. However, one major challenge for applying synchronous TAG to machine translation is computational complexity. While TAG requires O(n6 ) time for monolingual parsing, synchronous TAG requires O(n12 ) for bilingual parsing. One solution is to use tree insertion grammars (TIG) introduced by Schabes and Waters (1995). As a restricted form of TAG, TIG still allows for adjoining of unbounded trees but only requires O(n3 ) time for monolingual parsing. Nesson et al. (2006) firstly demonstrate Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1278–1287, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics NP α1 NP NP α2 X NN , oÚ President NR {I zˇ ongtˇ ong X , X , β1 NP∗ US NP↓ X∗ X↓ mˇeigu´ o NP NP∗ X X∗ NP β2 NP X , NN President oÚ zˇ ongtˇ ong α3 NP X X NP X , NR NN US President {I oÚ ongtˇ ong mˇeigu´ o zˇ Figure 1: Initial and"
P11-1128,C90-3045,0,0.442401,"ese formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax. For example, Chiang (2006) points out that the translation of languages that can stack an unbounded number of clauses in an “inside-out” way (Wu, 1997) 1278 provably goes beyond the expressive power of synchronous CFG and TSG. Therefore, it is necessary to find ways to take advantage of more powerful synchronous grammars to improve machine translation. Synchronous tree adjoining grammars (TAG) (Shieber and Schabes, 1990) are a good candidate. As a formal tree rewriting system, TAG (Joshi et al., 1975; Joshi, 1985) provides a larger domain of locality than CFG to state linguistic dependencies that are far apart since the formalism treats trees as basic building blocks. As a mildly context-sensitive grammar, TAG is conjectured to be powerful enough to model natural languages. Synchronous TAG generalizes TAG by allowing the construction of a pair of trees using the TAG operations of substitution and adjoining on tree pairs. The idea of using synchronous TAG in machine translation has been pursued by several rese"
P11-1128,W07-0412,0,0.157041,"inguistic dependencies that are far apart since the formalism treats trees as basic building blocks. As a mildly context-sensitive grammar, TAG is conjectured to be powerful enough to model natural languages. Synchronous TAG generalizes TAG by allowing the construction of a pair of trees using the TAG operations of substitution and adjoining on tree pairs. The idea of using synchronous TAG in machine translation has been pursued by several researchers (Abeille et al., 1990; Prigent, 1994; Dras, 1999), but only recently in its probabilistic form (Nesson et al., 2006; DeNeefe and Knight, 2009). Shieber (2007) argues that probabilistic synchronous TAG possesses appealing properties such as expressivity and trainability for building a machine translation system. However, one major challenge for applying synchronous TAG to machine translation is computational complexity. While TAG requires O(n6 ) time for monolingual parsing, synchronous TAG requires O(n12 ) for bilingual parsing. One solution is to use tree insertion grammars (TIG) introduced by Schabes and Waters (1995). As a restricted form of TAG, TIG still allows for adjoining of unbounded trees but only requires O(n3 ) time for monolingual pars"
P11-1128,J97-3002,0,0.124134,"g system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets. 1 Introduction Syntax-based translation models, which exploit hierarchical structures of natural languages to guide machine translation, have become increasingly popular in recent years. So far, most of them have been based on synchronous context-free grammars (CFG) (Chiang, 2007), tree substitution grammars (TSG) (Eisner, 2003; Galley et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and inversion transduction grammars (ITG) (Wu, 1997; Xiong et al., 2006). Although these formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax. For example, Chiang (2006) points out that the translation of languages that can stack an unbounded number of clauses in an “inside-out” way (Wu, 1997) 1278 provably goes beyond the expressive power of synchronous CFG and TSG. Therefore, it is necessary to find ways to take advantage of more powerful synchronous grammars to improve machine translation. Synchrono"
P11-1128,P06-1066,1,0.784987,"mproves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets. 1 Introduction Syntax-based translation models, which exploit hierarchical structures of natural languages to guide machine translation, have become increasingly popular in recent years. So far, most of them have been based on synchronous context-free grammars (CFG) (Chiang, 2007), tree substitution grammars (TSG) (Eisner, 2003; Galley et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and inversion transduction grammars (ITG) (Wu, 1997; Xiong et al., 2006). Although these formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax. For example, Chiang (2006) points out that the translation of languages that can stack an unbounded number of clauses in an “inside-out” way (Wu, 1997) 1278 provably goes beyond the expressive power of synchronous CFG and TSG. Therefore, it is necessary to find ways to take advantage of more powerful synchronous grammars to improve machine translation. Synchronous tree adjoining gra"
P11-1128,P00-1058,0,\N,Missing
P11-1128,P08-1064,0,\N,Missing
P11-1128,W90-0102,0,\N,Missing
P12-1048,J05-4003,0,0.0943735,"Missing"
P12-1048,W09-0432,0,0.0300305,"t Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of"
P12-1048,D07-1007,0,0.0296716,") also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of addit"
P12-1048,P07-1005,0,0.406273,"uccessfully in NLP community. Based on the “bag-of-words” assumption that the order of words can be ignored, these methods model the text corpus by using a co-occurrence matrix of words and documents, and build generative models to infer the latent aspects or topics. Using these models, the words can be clustered into the derived topics with a probability distribution, and the correlation between words can be automatically captured via topics. However, the “bag-of-words” assumption is an unrealistic oversimplification because it ignores the order of words. To remedy this problem, Gruber et al.(2007) propose HTMM, which models the topics of words in the document as a Markov chain. Based on the assumption that all words in the same sentence have the same topic and the successive sentences are more likely to have the same topic, HTMM incorporates the local dependency between words by Hidden Markov Model for better topic estimation. HTMM can also be viewed as a soft clustering tool for words in training corpus. That is, HTMM can estimate the probability distribution of a topic over words, i.e. the topic-word distribution P (word|topic) during training. Besides, HTMM derives inherent topics i"
P12-1048,P10-1086,0,0.0111327,"in SMT. Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity. Tam et al.(2007) proposed a bilingual LSA, which enforces one-to-one topic correspondence and enables latent topic distributions to be efficiently transferred across languages, to crosslingual language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; C"
P12-1048,P10-1146,0,0.0129201,"ship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely ∗ Part of this work was done during the first author’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific tra"
P12-1048,W07-0722,0,0.159911,"dapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpo"
P12-1048,eck-etal-2004-language,0,0.0665867,"Missing"
P12-1048,2005.mtsummit-papers.30,0,0.060786,"Missing"
P12-1048,W07-0717,0,0.227606,"re we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Lingui"
P12-1048,D10-1044,0,0.0292696,"in SMT. Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity. Tam et al.(2007) proposed a bilingual LSA, which enforces one-to-one topic correspondence and enables latent topic distributions to be efficiently transferred across languages, to crosslingual language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; C"
P12-1048,P06-1121,0,0.0055969,"ility estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely ∗ Part of this work was done during the first author’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-d"
P12-1048,D08-1039,0,0.0258649,"Missing"
P12-1048,C08-1041,1,0.583545,"odeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of additional in-domain m"
P12-1048,2005.eamt-1.19,0,0.0557771,"Missing"
P12-1048,J03-1002,0,0.00333165,"conduct topic model training. During this process, we empirically set the same parameter values for the HTMM training of different corpora: topics = 50, α = 1.5, β = 1.01, iters = 100. See (Gruber et al., 2007) for the meanings of these parameters. Besides, we set the interpolation weight θ in formula (10) to 0.5 by observing the results on development set in the additional experiments. We choose MOSES, a famous open-source 1 2 http://blog.sohu.com/ http://u.cs.biu.ac.il/ koppel/BlogCorpus.html phrase-based machine translation system (Koehn et al., 2007), as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al.,"
P12-1048,J04-4002,0,0.647358,"translation probability estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely ∗ Part of this work was done during the first author’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the"
P12-1048,N03-1017,0,0.0215688,"Missing"
P12-1048,W04-3250,0,0.0780006,"generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Result and Analysis 4.2.1 Effect of Different Smoothing Methods Our first experiments investigate the effect of different smoothing methods for the in-domain phrasetopic distribution: “Noisy-OR” and “Averaging”. We build adapted phrase tables with these two methods, and then respectively use them in place of the out-of-domain phrase table to test the system performance. For the purpose of studying the generality of our approach, we carry out comparative experiments on two sizes of in-domain monolingual corpora: 5K and 40K. Adaptation Met"
P12-1048,P07-2045,0,0.00336531,"vely, we use HTMM tool developed by Gruber et al.(2007) to conduct topic model training. During this process, we empirically set the same parameter values for the HTMM training of different corpora: topics = 50, α = 1.5, β = 1.01, iters = 100. See (Gruber et al., 2007) for the meanings of these parameters. Besides, we set the interpolation weight θ in formula (10) to 0.5 by observing the results on development set in the additional experiments. We choose MOSES, a famous open-source 1 2 http://blog.sohu.com/ http://u.cs.biu.ac.il/ koppel/BlogCorpus.html phrase-based machine translation system (Koehn et al., 2007), as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evalu"
P12-1048,P06-1077,1,0.129484,"method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely ∗ Part of this work was done during the first author’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual co"
P12-1048,D07-1036,1,0.789966,"l, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpora. Our approach i"
P12-1048,D09-1022,0,0.0366086,"l language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to"
P12-1048,D09-1074,0,0.027418,"Missing"
P12-1048,W11-2133,0,0.0654102,"gnoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ ang”, and occurs in the sentences related to the geography topic when translated to “h´ ea `n”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases. In a monolingual corpus, if “bank” occurs more often in the sentences rela"
P12-1048,P02-1040,0,0.105802,"ch and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Result and Analysis 4.2.1 Effect of Different Smoothing Methods Our first experiments investigate the effect of different smoothing methods for the in-domain phrasetopic distribution: “Noisy-OR” and “Averaging”. We build adapted phrase tables with these two methods, and then respectively use them in place of the out-of-domain phrase table to test the system performance. For the purpose of studying the generality of our approach, we carry out comparative experiments on two sizes"
P12-1048,2009.mtsummit-posters.17,0,0.0128304,"ion emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computati"
P12-1048,2007.mtsummit-tutorials.1,0,0.0760626,"rpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ ang”, and occurs in the sentences related to the geography topic when translated to “h´ ea `n”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases. In a monolingual corpus, if"
P12-1048,C08-1125,1,0.73343,"or’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings"
P12-1048,N04-1033,0,0.157585,"which is decomposed into the topic posterior distribution at the word level, and θ is the interpolation weight that can be optimized over the development data. Given the number of the phrase f˜ in sentence f denoted as countf (f˜), we compute the in-domain phrase-topic distribution in the following way: Pmle (tf P = f ∈Cf P tf in ˜ in |f ) countf (f˜) · P (tf in |f ) in countf (f˜) · P (tf P f ∈Cf in in |f ) (11) Under the assumption that the topics of all words in the same phrase are independent, we consider two methods to calculate Pword (tf in |f˜). One is a “Noisy-OR” combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages. Using this method, Pword (tf in |f˜) is defined as: Pword (tf in |f˜) = 1 − Pword (t¯f in |f˜) Y ≈ 1− P (t¯f in |fj ) fj ∈f˜ = 1− Y (1 − P (tf in |fj )) (12) fj ∈f˜ where Pword (t¯f in |f˜) represents the probability that tf in is not the topic of the phrase f˜. Similarly, P (t¯f in |fj ) indicates the probability that tf in is not the topic of the word fj . The other method is an “Averaging” combination one. With the assumption that tf in is the topic of f˜ if at least one of the words"
P12-1048,W06-1626,0,0.0730106,"Missing"
P12-1048,C04-1059,0,0.0592364,"Missing"
P12-1048,P06-2124,0,0.82898,"hes focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ ang”, and occurs in the sentences related to the geography topic when translated to “h´ ea `n”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates"
P12-1048,D08-1010,1,0.848211,"in adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of additional in-domain monolingual corpora,"
P12-1048,J07-2003,0,\N,Missing
P12-1048,2006.iwslt-evaluation.15,0,\N,Missing
P12-1079,W09-0432,0,0.0127262,"y rule (c). To address such issue of the topic similarity model, we further introduce a topic sensitivity model to describe the topic sensitivity of a rule using entropy as a metric: Sensitivity(P (z|r)) (1) =− k=1 Hellinger function is used to calculate distribution distance and is popular in topic model (Blei and Lafferty, 2007).1 By topic similarity, we aim to encourage or penalize the application of a rule for a given document according to their topic distributions, which then helps the SMT system make better translation decisions. 3.2 Topic Sensitivity Domain adaptation (Wu et al., 2008; Bertoldi and Federico, 2009) often distinguishes general-domain data from in-domain data. Similarly, we divide the rules into topic-insensitive rules and topic-sensitive 1 We also try other distance functions, including Euclidean distance, Kullback-Leibler divergence and cosine function. They produce similar results in our preliminary experiments. 752 K ∑ P (z = k|r) × log (P (z = k|r)) (2) k=1 According to the Eq. (2), a topic-insensitive rule has a large entropy, while a topic-sensitive rule has a smaller entropy. By incorporating the topic sensitivity model with the topic similarity model, we enable our SMT system to"
P12-1079,2007.mtsummit-papers.11,0,0.0504094,"cific topic. A phrase pair will be discarded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level. 8 Conclusion and Future Work We have presented a topic similarity model which incorporates the rule-topic distributions on both the source and target side into traditional hierarchical phrase-based system. Our experimental results show that our model achieves a better performance with faster decoding speed than previous work on topicspecific lexicon translation. This verifies the advantage of exploiting topic model at t"
P12-1079,D08-1024,0,0.0143161,"one-to-one mapping between source-side and target-side topics. 6.5 Effect on Various Types of Rules To get a more detailed analysis of the result, we further compare the effect of our method on different types of rules. We divide the rules into three types: phrase rules, which only contain terminals and are the same as the phrase pairs in phrasebased system; monotone rules, which contain nonterminals and produce monotone translations; reordering rules, which also contain non-terminals but change the order of translations. We define the monotone and reordering rules according to Chiang et al., (2008). Table 5 show the results. We can see that our method achieves improvements on all the three types of rules. Our topic similarity method on monotone rule achieves the most improvement which is 0.6 B LEU points, while the improvement on reordering rules is the smallest among the three types. This shows that topic information also helps the selections of rules with non-terminals. 7 Related Work In addition to the topic-specific lexicon translation method mentioned in the previous sections, researchers also explore topic model for machine translation in other ways. Foster and Kunh (2007) describ"
P12-1079,J07-2003,0,0.909756,"nstitute of Computing Technology Institute for Infocomm Research Chinese Academy of Sciences {xiaoxinyan, liuqun, sxlin}@ict.ac.cn Abstract by these probabilities. However, the state-of-theart SMT systems translate sentences by using sequences of synchronous rules or phrases, instead of translating word by word. Since a synchronous rule is rarely factorized into individual words, we believe that it is more reasonable to incorporate the topic model directly at the rule level rather than the word level. Consequently, we propose a topic similarity model for hierarchical phrase-based translation (Chiang, 2007), where each synchronous rule is associated with a topic distribution. In particular, Previous work using topic model for statistical machine translation (SMT) explore topic information at the word level. However, SMT has been advanced from word-based paradigm to phrase/rule-based paradigm. We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given doc"
P12-1079,W07-0717,0,0.166869,"Missing"
P12-1079,C08-1041,1,0.127263,"pair will be discarded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level. 8 Conclusion and Future Work We have presented a topic similarity model which incorporates the rule-topic distributions on both the source and target side into traditional hierarchical phrase-based system. Our experimental results show that our model achieves a better performance with faster decoding speed than previous work on topicspecific lexicon translation. This verifies the advantage of exploiting topic model at the rule level ove"
P12-1079,N03-1017,0,0.025585,"which comes from the FBIS portion of LDC data. There are 10,947 documents in the FBIS corpus. The monolingual data for training English language model includes the Xinhua portion of the GIGAWORD corpus, which contains 238M English words. We used the NIST evaluation set of 2005 (MT05) as our development set, and sets of MT06/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST B LEU (Papineni et al., 2002) was used to mea755 sure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampling for parameter estimation and inference. The source-side and target-side topic models are est"
P12-1079,W04-3250,0,0.229373,"2.29 22.69 22.39 22.69 22.92 Avg 26.07 26.47 26.55 26.45 26.71 26.94 Speed 12.6 3.3 11.5 11.7 11.2 10.2 Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the traditional hierarchical system (“Baseline”) and the topic-specific lexicon translation method (“TopicLex”). “SimSrc” and “SimTgt” denote similarity by source-side and target-side rule-distribution respectively, while “Sim+Sen” activates the two similarity and two sensitivity features. “Avg” is the average B LEU score on the two test sets. Scores marked in bold mean significantly (Koehn, 2004) better than Baseline (p &lt; 0.01). 2. Is it helpful to introduce the topic sensitivity model to distinguish topic-insensitive and topic-sensitive rules? 3. Is it necessary to project topics by one-to-many correspondence instead of one-to-one correspondence? 4. What is the effect of our method on various types of rules, such as phrase rules and rules with non-terminals? 6.1 Data We present our experiments on the NIST ChineseEnglish translation tasks. The bilingual training data contains 239K sentence pairs with 6.9M Chinese words and 9.14M English words, which comes from the FBIS portion of LDC"
P12-1079,D09-1092,0,0.0451605,"Missing"
P12-1079,P02-1038,0,0.0955267,"Missing"
P12-1079,J03-1002,0,0.00274223,"al training data contains 239K sentence pairs with 6.9M Chinese words and 9.14M English words, which comes from the FBIS portion of LDC data. There are 10,947 documents in the FBIS corpus. The monolingual data for training English language model includes the Xinhua portion of the GIGAWORD corpus, which contains 238M English words. We used the NIST evaluation set of 2005 (MT05) as our development set, and sets of MT06/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST B LEU (Papineni et al., 2002) was used to mea755 sure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampl"
P12-1079,P03-1021,0,0.0245589,"6/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST B LEU (Papineni et al., 2002) was used to mea755 sure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampling for parameter estimation and inference. The source-side and target-side topic models are estimated from the Chinese part and English part of FBIS corpus respectively. We set the number of topic K = 30 for both source-side and target-side, and use the default setting of the tool for training and inference.4 During decoding, we first infer the topic distribution of given documents before translation accord"
P12-1079,P02-1040,0,0.0984212,"ns 238M English words. We used the NIST evaluation set of 2005 (MT05) as our development set, and sets of MT06/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST B LEU (Papineni et al., 2002) was used to mea755 sure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampling for parameter estimation and inference. The source-side and target-side topic models are estimated from the Chinese part and English part of FBIS corpus respectively. We set the number of topic K = 30 for both source-side and target-side, and use the default setting of the tool for training and inference.4 Du"
P12-1079,W11-2133,0,0.168652,". Finally, they 757 combine a specific domain translation model with a general domain translation model depending on various text distances. One way to calculate the distance is using topic model. Gong et al. (2010) introduce topic model for filtering topic-mismatched phrase pairs. They first assign a specific topic for the document to be translated. Similarly, each phrase pair is also assigned with one specific topic. A phrase pair will be discarded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level. 8 Conclusion and"
P12-1079,D09-1008,0,0.0603759,"arded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level. 8 Conclusion and Future Work We have presented a topic similarity model which incorporates the rule-topic distributions on both the source and target side into traditional hierarchical phrase-based system. Our experimental results show that our model achieves a better performance with faster decoding speed than previous work on topicspecific lexicon translation. This verifies the advantage of exploiting topic model at the rule level over the word level. Fu"
P12-1079,C08-1125,0,0.0128983,"one-to-one mapping between source-side and target-side topics. 6.5 Effect on Various Types of Rules To get a more detailed analysis of the result, we further compare the effect of our method on different types of rules. We divide the rules into three types: phrase rules, which only contain terminals and are the same as the phrase pairs in phrasebased system; monotone rules, which contain nonterminals and produce monotone translations; reordering rules, which also contain non-terminals but change the order of translations. We define the monotone and reordering rules according to Chiang et al., (2008). Table 5 show the results. We can see that our method achieves improvements on all the three types of rules. Our topic similarity method on monotone rule achieves the most improvement which is 0.6 B LEU points, while the improvement on reordering rules is the smallest among the three types. This shows that topic information also helps the selections of rules with non-terminals. 7 Related Work In addition to the topic-specific lexicon translation method mentioned in the previous sections, researchers also explore topic model for machine translation in other ways. Foster and Kunh (2007) describ"
P12-1079,P06-2124,0,0.674686,"aches that work at the word level. 1 • Given a document to be translated, we calculate the topic similarity between a rule and the document based on their topic distributions. We augment the hierarchical phrase-based system by integrating the proposed topic similarity model as a new feature (Section 3.1). Introduction Topic model (Hofmann, 1999; Blei et al., 2003) is a popular technique for discovering the underlying topic structure of documents. To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality. Topic-specific lexicon translation models focus on word-level translations. Such models first estimate word translation probabilities conditioned on topics, and then adapt lexical weights of phrases ∗ {dyxiong, mzhang∗ }@i2r.a-star.edu.sg Corresponding author • As we will discuss in Section 3.2, the similarity between a generic rule and a given source document computed by our topic similarity model is often very low. We don’t want to penalize these generic rules. Therefore we further propose a topic sensitivity model whic"
P12-1100,P08-1009,0,0.283488,"longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring translation model to combine the me"
P12-1100,P05-1033,0,0.38994,"A derivation yields a pair of strings on the right-hand side which are translation of each other. In a weighted SCFG, each rule has a weight and the total weight of a derivation is the production of the weights of the rules used by the derivation. A translation may be produced by many different derivations and we only use the best derivation to evaluate its probability. With d denoting a derivation and r denoting a rule, we have p(e|f ) = max p(d, e|f , cˆ) d Y = max p(r, e|f , cˆ) d X (6) k We employ the same set of features for the loglinear model as the hierarchical phrase-based model does(Chiang, 2005). We further refine our hierarchical chunk-to-string model into two models: a loose model which is more similar to the hierarchical phrase-based model and a tight model which is more similar to the tree-tostring model. The two models differ in the form of rules and the way of estimating rule probabilities. While for decoding, we employ the same decoding algorithm for the two models: given a test sentence, the decoders first perform shallow parsing to get the best chunk sequence, then apply a CYK parsing algorithm with beam search. 2.1 A Loose Model In our model, we employ rules containing nont"
P12-1100,J07-2003,0,0.433856,"grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks. X → hX 1 for X 2 , X 2 de X 1 i can be applied to both of the following strings in Figure 1 “A request for a purchase of shares” “filed for bankruptcy”, and get the following translation, respectively “goumai gufen de shenqing” “pochan de shenqing”. 1 Introduction The hierarchical phrase-based model (Chiang, 2007) makes an advance of statistical machine translation by employing hierarchical phrases, which not only uses phrases to learn local translations but also uses hierarchical phrases to capture reorderings of words and subphrases which can cover a large scope. Besides, this model is formal syntax-based and does not need to specify the syntactic constituents of subphrases, so it can directly learn synchronous context-free grammars (SCFG) from a parallel text without relying on any linguistic annotations or assumptions, which makes it used conveniently and widely. ∗ This work was done when the first"
P12-1100,P05-1066,0,0.0777595,"ed decoder, tree represents the tree-to-string decoder, tight represents our tight decoder and loose represents our loose decoder. The speed is reported by seconds per sentence. The speed for the tree-tostring decoder includes the parsing time (0.23s) and the speed for the tight and loose models includes the shallow parsing time, too. extraction as: the height up to 3 and the number of leaf nodes up to 5. We give the results in Table 2. From the results, we can see that both the loose and tight decoders outperform the baseline decoders and the improvement is significant using the sign-test of Collins et al. (2005) (p < 0.01). Specifically, the loose model has a better performance while the tight model has a faster speed. Compared with the hierarchical phrase-based model, the loose model only imposes syntactic cohesion cohesion to nonterminals while the tight model imposes syntax cohesion to both rules and nonterminals which reduces search space, so it decoders faster. We can conclude that linguistic syntax can indeed improve the translation performance; syntactic cohesion for nonterminals can explain linguistic phenomena well; noncohesive rules are useful, too. The extra time consumption against hierar"
P12-1100,P03-2041,0,0.0452022,"ntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hier"
P12-1100,C10-2033,1,0.84756,"reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring translation model to combine the merits of the two mode"
P12-1100,W02-1039,0,0.1364,"a NP and this rule acts correctly while in the latter “filed” is a VP and this rule gives a wrong reordering. If we specify the first X on the right-hand side to NP, this kind of errors can be avoided. The tree-to-string model (Liu et al., 2006; Huang et al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-t"
P12-1100,N04-1035,0,0.0742159,"tion We used the NIST training set for Chinese-English translation tasks excluding the Hong Kong Law and Hong Kong Hansard2 as the training data, which contains 470K sentence pairs. For the training data set, we first performed word alignment in both directions using GIZA++ toolkit (Och and Ney, 2000) then refined the alignments using “final-and”. We trained a 5-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of LDC Chinese Gigaword corpus. For the tree-to-string model, we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm (Galley et al., 2004). We used our in-house English-Chinese data set as the development set and used the 2008 NIST English-Chinese MT test set (1859 sentences) as the test set. Our evaluation metric was BLEU-4 (Papineni et al., 2002) based on characters (as the target language is Chinese), which performed caseinsensitive matching of n-grams up to n = 4 and used the shortest reference for the brevity penalty. We used the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. 6.2 Shallow Parsing The standard evaluation metrics for shallow parsi"
P12-1100,N04-1014,0,0.119779,"e parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring t"
P12-1100,2006.amta-papers.8,0,0.0819158,"he syntactic constituents of subphrases, so it can directly learn synchronous context-free grammars (SCFG) from a parallel text without relying on any linguistic annotations or assumptions, which makes it used conveniently and widely. ∗ This work was done when the first author visited Microsoft Research Asia as an intern. In the former, “A request” is a NP and this rule acts correctly while in the latter “filed” is a VP and this rule gives a wrong reordering. If we specify the first X on the right-hand side to NP, this kind of errors can be avoided. The tree-to-string model (Liu et al., 2006; Huang et al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997;"
P12-1100,N03-1017,0,0.0974779,"oduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side. 1 We use the open source toolkit CRF++ got in http://code.google.com/p/crfpp/ . 954 Given a sentence pair hf , e, ∼i, we extract rules for the loose model as follows 1. If hfjj12 , eii21 i is a basic phrase, then we can have a rule X → hfjj12 , eii21 i 2. Assume X → hα, βi is a rule with α = α1 fjj12 α2 and β = β1 eii21 β2 , and hfjj12 , eii21 i is a chunk-based phrase with a chunk sequence Yu · · · Yv , then we have the following rule X → hα1 Yu -Yv k α2 , β1 Yu -"
P12-1100,C10-1080,1,0.843744,"al phrase-based model. Cherry (2008) used dependency tree to add syntactic cohesion. These methods work with the original SCFG defined by hierarchical phrase-based model and use linguistic knowledge to assist translation. Instead, our model works under the new defined SCFG with chunks. Besides, some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1-best parse. Mi et al. (2008) presented forest-based translation where the decoder translates a packed forest of exponentially many parses instead of i-best parse. Liu and Liu (2010) proposed to parse and to translate jointly by taking tree-based translation as parsing. Given a source sentence, this decoder produces a parse tree on the source side and a translation on the target side simultaneously. Both the models perform in the unit of tree nodes rather than chunks. 955 6 Experiments 6.1 Data Preparation Data for shallow parsing We got training data and test data for shallow parsing from the standard Penn Tree Bank (PTB) English parsing task by splitting the sections 02-21 on the Wall Street Journal Portion (Marcus et al., 1993) into two sets: the last 1000 sentences as"
P12-1100,P06-1077,1,0.904774,"need to specify the syntactic constituents of subphrases, so it can directly learn synchronous context-free grammars (SCFG) from a parallel text without relying on any linguistic annotations or assumptions, which makes it used conveniently and widely. ∗ This work was done when the first author visited Microsoft Research Asia as an intern. In the former, “A request” is a NP and this rule acts correctly while in the latter “filed” is a VP and this rule gives a wrong reordering. If we specify the first X on the right-hand side to NP, this kind of errors can be avoided. The tree-to-string model (Liu et al., 2006; Huang et al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some langu"
P12-1100,J93-2004,0,0.0396526,"tially many parses instead of i-best parse. Liu and Liu (2010) proposed to parse and to translate jointly by taking tree-based translation as parsing. Given a source sentence, this decoder produces a parse tree on the source side and a translation on the target side simultaneously. Both the models perform in the unit of tree nodes rather than chunks. 955 6 Experiments 6.1 Data Preparation Data for shallow parsing We got training data and test data for shallow parsing from the standard Penn Tree Bank (PTB) English parsing task by splitting the sections 02-21 on the Wall Street Journal Portion (Marcus et al., 1993) into two sets: the last 1000 sentences as the test set and the rest as the training set. We filtered the features whose frequency was lower than 3 and substituted ‘‘ and ’’ with ˝ to keep consistent with translation data. We used L2 algorithm to train CRF. Data for Translation We used the NIST training set for Chinese-English translation tasks excluding the Hong Kong Law and Hong Kong Hansard2 as the training data, which contains 470K sentence pairs. For the training data set, we first performed word alignment in both directions using GIZA++ toolkit (Och and Ney, 2000) then refined the alignm"
P12-1100,P08-1114,0,0.0508566,"model. Their model reorders chunks resorting to a distortion model while our model reorders chunks according to SCFG rules which retain the relative positions of chunks. Nguyen et al. (2008) presented a tree-to-string phrase-based method which is based on SCFGs. This method generates SCFGs through syntactic transformation including a word-to-phrase tree transformation model and a phrase reordering model while our model learns SCFG-based rules from word-aligned bilingual corpus directly There are also some works aiming to introduce linguistic knowledge into the hierarchical phrasebased model. Marton and Resnik (2008) took the source parse tree into account and added soft constraints to hierarchical phrase-based model. Cherry (2008) used dependency tree to add syntactic cohesion. These methods work with the original SCFG defined by hierarchical phrase-based model and use linguistic knowledge to assist translation. Instead, our model works under the new defined SCFG with chunks. Besides, some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1-best parse. Mi et al. (2008) presented forest-based translation where the decoder"
P12-1100,P08-1023,1,0.860438,"edge into the hierarchical phrasebased model. Marton and Resnik (2008) took the source parse tree into account and added soft constraints to hierarchical phrase-based model. Cherry (2008) used dependency tree to add syntactic cohesion. These methods work with the original SCFG defined by hierarchical phrase-based model and use linguistic knowledge to assist translation. Instead, our model works under the new defined SCFG with chunks. Besides, some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1-best parse. Mi et al. (2008) presented forest-based translation where the decoder translates a packed forest of exponentially many parses instead of i-best parse. Liu and Liu (2010) proposed to parse and to translate jointly by taking tree-based translation as parsing. Given a source sentence, this decoder produces a parse tree on the source side and a translation on the target side simultaneously. Both the models perform in the unit of tree nodes rather than chunks. 955 6 Experiments 6.1 Data Preparation Data for shallow parsing We got training data and test data for shallow parsing from the standard Penn Tree Bank (PTB"
P12-1100,P00-1056,0,0.604931,"sk and employ Conditional Random Field (CRF)1 to train a chunker. CRF is a good choice for label tasks as it can avoid label bias and use more statistical correlated features. We employ the features described in Sha and Pereira (2003) for CRF. We do not introduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side. 1 We use the open source toolkit CRF++ got in http://code.google.com/p/crfpp/ . 954 Given a sentence pair hf , e, ∼i, we extract rules for the loose model as follows 1. If hfjj12 , eii21 i is a basic phrase, the"
P12-1100,P02-1038,0,0.20294,"ploy rules containing nonterminals to handle long-distance reordering where boundary words play an important role. So for the subphrases which cover more than one chunk, we just maintain boundary chunks: we bundle adjacent chunks into one nonterminal and denote it as the first chunk tag immediately followed by “-” and next followed by the last chunk tag. Then, for the string pair <filed for bankruptcy, shenqing pochan>, we can get the rule r1 : X → hVBN 1 for NP 2 , VBN 1 NP 2 i while for the string pair <A request for a purchase of shares, goumai gufen de shenqing>, we can get r2 : Following Och and Ney (2002), we frame our model as a log-linear model: P exp k λk Hk (d, e, cˆ, f ) P (5) p(e|f ) = exp d′ ,e′ ,k λk Hk (d′ , e′ , cˆ, f ) Hk (d, e, ˆ c, f ) = λk Hk (d, e, cˆ, f ) X → hNP 1 for NP-NP 2 , NP-NP 2 de NP 1 i. (4) r∈d where X hk (f , cˆ, r) r 952 The rule matching “A request for a purchase of shares was” will be r3 : X → hNP-NP 1 VBD 2 , NP-NP 1 VBD 2 i. We can see that in contrast to the method of representing each chunk separately, this representation form can alleviate data sparseness and the influence of parsing errors. hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i ⇒ hX 4 X 3 , X 4 X 3 i ⇒ hNP-NP"
P12-1100,J04-4002,0,0.217737,"kier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side. 1 We use the open source toolkit CRF++ got in http://code.google.com/p/crfpp/ . 954 Given a sentence pair hf , e, ∼i, we extract rules for the loose model as follows 1. If hfjj12 , eii21 i is a basic phrase, then we can have a rule X → hfjj12 , eii21 i 2. Assume X → hα, βi is a rule with α = α1 fjj12 α2 and β = β1 eii21 β2 , and hfjj12 , eii21 i is a chunk-based phrase with a chunk sequence Yu · · · Yv , then we have the following rule X → hα1 Yu -Yv k α2 , β1 Yu -Yv k β2 i. We evalu"
P12-1100,P03-1021,0,0.171403,"Missing"
P12-1100,P02-1040,0,0.0852269,"e first performed word alignment in both directions using GIZA++ toolkit (Och and Ney, 2000) then refined the alignments using “final-and”. We trained a 5-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of LDC Chinese Gigaword corpus. For the tree-to-string model, we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm (Galley et al., 2004). We used our in-house English-Chinese data set as the development set and used the 2008 NIST English-Chinese MT test set (1859 sentences) as the test set. Our evaluation metric was BLEU-4 (Papineni et al., 2002) based on characters (as the target language is Chinese), which performed caseinsensitive matching of n-grams up to n = 4 and used the shortest reference for the brevity penalty. We used the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. 6.2 Shallow Parsing The standard evaluation metrics for shallow parsing are precision P, recall R, and their harmonic mean F1 score, given by: number of exactly recognized chunks number of output chunks number of exactly recognized chunks R= number of reference chunks P = 2 The so"
P12-1100,P05-1034,0,0.106643,"ordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring translation model to"
P12-1100,N03-1028,0,0.0397363,"gnized as a chunk, we skip its children. In this way, we can get a sole chunk sequence given a parse tree. Then we label each word with a label indicating whether the word starts a chunk (B-) or continues a chunk (I-). Figure 2(a) gives an example. In this method, we get the training data for shallow parsing from Penn Tree Bank. We take shallow Parsing (chunking) as a sequence label task and employ Conditional Random Field (CRF)1 to train a chunker. CRF is a good choice for label tasks as it can avoid label bias and use more statistical correlated features. We employ the features described in Sha and Pereira (2003) for CRF. We do not introduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heurist"
P12-1100,P03-1039,0,0.0253069,"e way as Liu et al. (2006). For the loose model, the nonterminals must be cohesive, while the whole rule can be noncohesive: if both ends of a rule are nonterminals, the whole rule is cohesive, otherwise, it may be noncohesive. In contrast, for the tight model, both the whole rule and the nonterminal are cohesive. Even with the cohesion constraints, our model still generates a large number of rules, but not all of the rules are useful for translation. So we follow the method described in Chiang (2007) to filter the rule set except that we allow two nonterminals to be adjacent. 5 Related Works Watanabe et al. (2003) presented a chunk-to-string translation model where the decoder generates a translation by first translating the words in each chunk, then reordering the translation of chunks. Our model distinguishes from their model mainly in reordering model. Their model reorders chunks resorting to a distortion model while our model reorders chunks according to SCFG rules which retain the relative positions of chunks. Nguyen et al. (2008) presented a tree-to-string phrase-based method which is based on SCFGs. This method generates SCFGs through syntactic transformation including a word-to-phrase tree tran"
P12-1100,J97-3002,0,0.132427,"al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse e"
P12-1100,P01-1067,0,0.0718737,"introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we"
P12-1100,W08-2119,0,\N,Missing
P12-1100,J08-3004,0,\N,Missing
P12-2066,W11-0705,0,0.149937,"which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively. However, as these are considerably shorter than documents, their feature space is less complex, and pruning is not as pertinent. 3 Kernels for Sentiment Classification 3.1 Linguistic Representations We explore both sequence and convolution kernels to exploit information on surface and syn"
P12-2066,H05-1091,0,0.0191743,") represent a document as a bag-of-words; Matsumoto et al., (2005) extract frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context contai"
P12-2066,W08-1301,0,0.0692146,"Missing"
P12-2066,W10-2910,0,0.0159615,"gly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively. However, as these are considerably shorter than documents, their feature space is less complex, and pruning is not as pertinent. 3 Kernels for Sentiment Classification 3.1 Linguistic Representations We explore both sequence and co"
P12-2066,P09-2079,0,0.0921626,"-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification Zhaopeng Tu† Yifan He‡§ Jennifer Foster§ Josef van Genabith§ Qun Liu† Shouxun Lin† † Key Lab. of Intelligent Info. Processing ‡ Computer Science Department § School of Computing Institute of Computing Technology, CAS New York University Dublin City University † {tuzhaopeng,liuqun,sxlin}@ict.ac.cn, ‡ yhe@cs.nyu.edu, § {jfoster,josef}@computing.dcu.ie Abstract uments. More recently, there have been several approaches which employ features based on deep linguistic analysis with encouraging results including Joshi and Penstein-Rose (2009) and Liu and Seneff (2009). However, as they select features manually, these methods would require additional labor when ported to other languages and domains. Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types"
P12-2066,P03-1054,0,0.00724357,"directly connected to the subjective word. For instance, given the node tragic in Figure 1(d), we will extract its direct parent waste integrated with dependency relations and (possibly) POS, as in Figure 2(b). We further add two background scopes, one being subjective sentences (the sentences that contain subjective words), and the entire document. 4 Experiments 4.1 Setup We carried out experiments on the movie review dataset (Pang and Lee, 2004), which consists of 1000 positive reviews and 1000 negative reviews. To obtain constituency trees, we parsed the document using the Stanford Parser (Klein and Manning, 2003). To obtain dependency trees, we passed the Stanford constituency trees through the Stanford constituency-to-dependency converter (de Marneffe and Manning, 2008). We exploited Subset Tree (SST) (Collins and Duffy, 2001) and Partial Tree (PT) kernels (Moschitti, 2006) for constituent and dependency parse trees1 , respectively. A sequential kernel is applied for lexical sequences. Kernels were combined using plain (unweighted) summation. Corpus statistics are provided in Table 1. We use a manually constructed polarity lexicon (Wilson et al., 2005), in which each entry is annotated with its degre"
P12-2066,D09-1017,0,0.350748,"ion Kernels in Document-level Sentiment Classification Zhaopeng Tu† Yifan He‡§ Jennifer Foster§ Josef van Genabith§ Qun Liu† Shouxun Lin† † Key Lab. of Intelligent Info. Processing ‡ Computer Science Department § School of Computing Institute of Computing Technology, CAS New York University Dublin City University † {tuzhaopeng,liuqun,sxlin}@ict.ac.cn, ‡ yhe@cs.nyu.edu, § {jfoster,josef}@computing.dcu.ie Abstract uments. More recently, there have been several approaches which employ features based on deep linguistic analysis with encouraging results including Joshi and Penstein-Rose (2009) and Liu and Seneff (2009). However, as they select features manually, these methods would require additional labor when ported to other languages and domains. Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types of substructures in docum"
P12-2066,P08-2029,0,0.0249684,"rom dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively."
P12-2066,J08-2003,0,0.102698,"e docIn this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment classification: previous work has already shown that categorically using the entire syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective paragraphs to simply describe"
P12-2066,P06-2079,0,0.451789,"Missing"
P12-2066,D09-1143,0,0.0168412,"frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et"
P12-2066,P04-1035,0,0.860926,"xicon show 1.45 point absolute improvement in accuracy over a bag-of-words classifier on a widely used sentiment corpus. 1 Introduction An important subtask in sentiment analysis is sentiment classification. Sentiment classification involves the identification of positive and negative opinions from a text segment at various levels of granularity including document-level, paragraphlevel, sentence-level and phrase-level. This paper focuses on document-level sentiment classification. There has been a substantial amount of work on document-level sentiment classification. In early pioneering work, Pang and Lee (2004) use a flat feature vector (e.g., a bag-of-words) to represent the documents. A bag-of-words approach, however, cannot capture important information obtained from structural linguistic analysis of the docIn this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment clas"
P12-2066,H05-1044,0,0.861528,"tics, pages 338–343, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics cation task. Indeed, separating objective sentences from subjective sentences in a document produces encouraging results (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009). Our research is inspired by these observations. Unlike in the previous work, however, we focus on syntactic substructures (rather than entire paragraphs or sentences) that contain subjective words. More specifically, we use the terms in the lexicon constructed from (Wilson et al., 2005) as the indicators to identify the substructures for the convolution kernels, and extract different sub-structures according to these indicators for various types of parse trees (Section 3). An empirical evaluation on a widely used sentiment corpus shows an improvement of 1.45 point in accuracy over the baseline resulting from a combination of bag-of-words and high-impact parse features (Section 4). 2 Related Work Our research builds on previous work in the field of sentiment classification and convolution kernels. For sentiment classification, the design of lexical and syntactic features is a"
P12-2066,W03-1017,0,0.174347,"categorically using the entire syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective paragraphs to simply describe the plot. Such objective portions do not contain the author’s opinion and are irrelevant with respect to the sentiment classifi338 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 338–343, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics cation task. Indeed, separating objective sentences from subjective sentences in a document produces encouraging results (Yu and Ha"
P12-2066,P06-1104,0,0.0670835,"istic analysis of the docIn this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment classification: previous work has already shown that categorically using the entire syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective parag"
P12-2066,N10-1121,0,\N,Missing
P13-1075,P11-2095,0,0.0207432,"spects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, thus to result in a model with better coverage and higher accuracy. Researchers have also investigated unsupervised methods in word segmentation (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). An unsupervised method mines the latent distribution regularity in the raw text, and automatically induces word segmentation knowledge from it. Our method also needs large amounts of external data, but it aims to leverage the knowledge in the fuzzy and sparse annotations. It is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways."
P13-1075,P08-1102,1,0.899147,"Missing"
P13-1075,P09-1059,1,0.952897,"uable knowledge for problems such as word segmentation and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences. Figure 1 shows an example. The hyperlink indicates 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in 761 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 761–769, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Type n-gram a Chinese phrase (meaning NLP), and it pro"
P13-1075,N09-1036,0,0.00837892,"r method is essentially different from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, thus to result in a model with better coverage and higher accuracy. Researchers have also investigated unsupervised methods in word segmentation (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). An unsupervised method mines the latent distribution regularity in the raw text, and automatically induces word segmentation knowledge from it. Our method also needs large amounts of external data, but it aims to leverage the knowledge in the fuzzy and sparse annotations. It is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the impr"
P13-1075,J09-4006,0,0.0164926,"ns to improve the unsupervised dependency grammar induction. Compared with their method, the strategy we proposed is formal and universal, the discriminative learning strategy and the quantitative measurement of fuzzy knowledge enable more effective utilization of the natural annotation on the Internet when adapted to parsing. resorting to complicated features, system combination and other semi-supervised technologies. What is more, since the text on Internet is widecoveraged and real-time updated, our strategy also helps a word segmenter be more domain adaptive and up to date. 6 Related Work Li and Sun (2009) extracted character classification instances from raw text for Chinese word segmentation, resorting to the indication of punctuation marks between characters. Sun and Xu (Sun and Xu, 2011) utilized the features derived from large-scaled unlabeled text to improve Chinese word segmentation. Although the two work also made use of large-scaled raw text, our method is essentially different from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzu"
P13-1075,P11-1141,0,0.0466246,"and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences. Figure 1 shows an example. The hyperlink indicates 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in 761 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 761–769, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Type n-gram a Chinese phrase (meaning NLP), and it probably corresponds to a connected sub-graph for"
P13-1075,P11-1070,0,0.122249,"i-1 NLP has already ... ᆑ ௶ ဴ ཝ  स ྸ i j ࠼ n j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶ ဴ ཝ  स ྸ i-1 i j ࠼ n ࠼ n j+1 (b) Knowledge for word segmentation n ఊ น ᆑ ௶ ဴ ཝ  स ྸ i-1 i j j+1 (c) Knowledge for dependency parsing Figure 1: Natural annotations for word segmentation and dependency parsing. size (e.g. tens of thousands), the performance of word segmentation tends to degrade sharply when applied to new domains. Internet provides large amounts of raw text, and statistics collected from it have been used to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Bansal and Klein, 2011; Zhou et al., 2011). The Internet also gives massive (although slight and sparse) natural annotations in the forms of structural information including hyperlinks, fonts, colors and layouts (Sun, 2011a). These annotations usually imply valuable knowledge for problems such as word segmentation and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences. Figure 1 shows an example. The hyperlink indicates 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate te"
P13-1075,W02-1001,0,0.233781,"e to its simplicity and popularity. The feature templates for the classifier is shown in Table 1. C0 denotes the current character, while C−k /Ck denote the kth character to the left/right of C0 . The function P u(·) returns true for a punctuation character and false for others, the function T (·) classifies a character into four types, 1, 2, 3 and 4, representing number, date, English letter and others, respectively. The classifier can be trained with online learning algorithms such as perceptron, or offline learning models such as support vector machines. We choose the perceptron algorithm (Collins, 2002) to train the classifier for the character classification-based word segmentation model. It learns a discriminative model mapping from the inputs x ∈ X to the outputs y˜ ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. Algorithm 1 shows the perceptron algorithm for tuning the parameter α ~ . The “averaged parameters” technology (Collins, 2002) is used for better performance. 3 Knowledge in Natural Annotations Web text gives massive natural annotations in the form of structural informations, including hyperlinks, fonts, colors and l"
P13-1075,C12-2073,0,0.156928,"Missing"
P13-1075,E09-1063,0,0.0243828,"Missing"
P13-1075,N06-1020,0,0.0253024,"e degree of difference between the two predications represents the amount of new knowledge introduced by the natural annotations over the baseline. The baseline model α ~ is trained on an existing human-annotated corpus. A set of sentences F with natural annotations are extracted from the Chinese wikipedia, and we reserve the ones for which constraint decoding and normal decoding give different predications. The predictions of reserved sentences by constraint decoding are used as additional training data for the enhanced classifier. The overall training pipeline is analogous to self-training (McClosky et al., 2006), Algorithm 2 shows the pseudo-codes. Considering the online characteristic of the perceptron algorithm, if we are able to leverage much more (than the Chinese wikipedia) data with natural annotations, an online version of learning procedure shown in Algorithm 3 would be a better choice. The technology of “averaged parameters” (Collins, 2002) is easily to be adapted here for better performance. When constraint decoding and normal decoding give different predications, we only know that the former is probably better than the latter. Although there is no explicit evidence for us to measure how mu"
P13-1075,P09-1012,0,0.0156602,"rent from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, thus to result in a model with better coverage and higher accuracy. Researchers have also investigated unsupervised methods in word segmentation (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). An unsupervised method mines the latent distribution regularity in the raw text, and automatically induces word segmentation knowledge from it. Our method also needs large amounts of external data, but it aims to leverage the knowledge in the fuzzy and sparse annotations. It is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the improvement of word segmentat"
P13-1075,J05-4005,0,0.0342105,"olors and layouts (Sun, 2011a). These annotations usually imply valuable knowledge for problems such as word segmentation and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences. Figure 1 shows an example. The hyperlink indicates 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in 761 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 761–769, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computation"
P13-1075,P07-2055,0,0.0266497,"(Sun, 2011a). These annotations usually imply valuable knowledge for problems such as word segmentation and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences. Figure 1 shows an example. The hyperlink indicates 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in 761 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 761–769, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Type n-gram a"
P13-1075,W10-4132,0,0.0407601,"Missing"
P13-1075,H05-1105,0,0.219753,"with a single classifier and local features. i-1 NLP has already ... ᆑ ௶ ဴ ཝ  स ྸ i j ࠼ n j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶ ဴ ཝ  स ྸ i-1 i j ࠼ n ࠼ n j+1 (b) Knowledge for word segmentation n ఊ น ᆑ ௶ ဴ ཝ  स ྸ i-1 i j j+1 (c) Knowledge for dependency parsing Figure 1: Natural annotations for word segmentation and dependency parsing. size (e.g. tens of thousands), the performance of word segmentation tends to degrade sharply when applied to new domains. Internet provides large amounts of raw text, and statistics collected from it have been used to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Bansal and Klein, 2011; Zhou et al., 2011). The Internet also gives massive (although slight and sparse) natural annotations in the forms of structural information including hyperlinks, fonts, colors and layouts (Sun, 2011a). These annotations usually imply valuable knowledge for problems such as word segmentation and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences. Figure 1 shows an example. The hyperlink indicates 1 Introduction Problems related to information retrieval, machine translation a"
P13-1075,W04-3236,0,0.045779,"perlinks, fonts, colors and layouts (Sun, 2011a). These annotations usually imply valuable knowledge for problems such as word segmentation and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences. Figure 1 shows an example. The hyperlink indicates 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in 761 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 761–769, c Sofia, Bulgaria, August 4-9 2013. 2013 Associati"
P13-1075,P92-1017,0,0.10306,", it focuses on the utilization of fuzzy and sparse knowledge on the Internet rather than making full use of a specific humanannotated corpus. On the other hand, since only a single classifier and local features are used in our method, better performance could be achieved 766 solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of word co-occurrence statistics collected in raw text or Internet n-grams to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011; Bansal and Klein, 2011). When enriching the related work"
P13-1075,P07-1106,0,0.0578219,"utomatically induces word segmentation knowledge from it. Our method also needs large amounts of external data, but it aims to leverage the knowledge in the fuzzy and sparse annotations. It is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways. Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated 7 Conclusion and Future Work This work presents a novel discriminative learning algorithm to utilize the knowledge in the massive natural annotations on the Internet. Natural annotations implied by structural information are used to decrease the searching space of the classifier, then the constraint decoding in the pruned searching space gives predictions not worse than the no"
P13-1075,C10-1100,0,0.0445762,"r and local features. i-1 NLP has already ... ᆑ ௶ ဴ ཝ  स ྸ i j ࠼ n j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶ ဴ ཝ  स ྸ i-1 i j ࠼ n ࠼ n j+1 (b) Knowledge for word segmentation n ఊ น ᆑ ௶ ဴ ཝ  स ྸ i-1 i j j+1 (c) Knowledge for dependency parsing Figure 1: Natural annotations for word segmentation and dependency parsing. size (e.g. tens of thousands), the performance of word segmentation tends to degrade sharply when applied to new domains. Internet provides large amounts of raw text, and statistics collected from it have been used to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Bansal and Klein, 2011; Zhou et al., 2011). The Internet also gives massive (although slight and sparse) natural annotations in the forms of structural information including hyperlinks, fonts, colors and layouts (Sun, 2011a). These annotations usually imply valuable knowledge for problems such as word segmentation and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences. Figure 1 shows an example. The hyperlink indicates 1 Introduction Problems related to information retrieval, machine translation and social computing n"
P13-1075,D10-1082,0,0.0526805,"problems such as word segmentation and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences. Figure 1 shows an example. The hyperlink indicates 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in 761 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 761–769, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Type n-gram a Chinese phrase (meaning NLP), and it probably corresponds to a"
P13-1075,P10-1130,0,0.0247878,"rs the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of word co-occurrence statistics collected in raw text or Internet n-grams to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011; Bansal and Klein, 2011). When enriching the related work during writing, we found a work on dependency parsing (Spitkovsky et al., 2010) who utilized parsing constraints derived from hypertext annotations to improve the unsupervised dependency grammar induction. Compared with their method, the strategy we proposed is formal and universal, the discriminative learning strategy and the quantitative measurement of fuzzy knowledge enable more effective utilization of the natural annotation on the Internet when adapted to parsing. resorting to complicated features, system combination and other semi-supervised technologies. What is more, since the text on Internet is widecoveraged and real-time updated, our strategy also helps a word"
P13-1075,W03-1730,1,0.386667,"ource utilization, the comparison between our system and previous work without using additional training data is unfair. However, we believe this work shows another interesting way to improve Chinese word segmentation, it focuses on the utilization of fuzzy and sparse knowledge on the Internet rather than making full use of a specific humanannotated corpus. On the other hand, since only a single classifier and local features are used in our method, better performance could be achieved 766 solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of"
P13-1075,D11-1090,0,0.169866,"itative measurement of fuzzy knowledge enable more effective utilization of the natural annotation on the Internet when adapted to parsing. resorting to complicated features, system combination and other semi-supervised technologies. What is more, since the text on Internet is widecoveraged and real-time updated, our strategy also helps a word segmenter be more domain adaptive and up to date. 6 Related Work Li and Sun (2009) extracted character classification instances from raw text for Chinese word segmentation, resorting to the indication of punctuation marks between characters. Sun and Xu (Sun and Xu, 2011) utilized the features derived from large-scaled unlabeled text to improve Chinese word segmentation. Although the two work also made use of large-scaled raw text, our method is essentially different from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data,"
P13-1075,I08-4017,0,0.114848,"ns usually imply valuable knowledge for problems such as word segmentation and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences. Figure 1 shows an example. The hyperlink indicates 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in 761 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 761–769, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Type n-gram a Chinese phrase (mean"
P13-1075,P11-1139,0,0.131131,"dependency parsing Figure 1: Natural annotations for word segmentation and dependency parsing. size (e.g. tens of thousands), the performance of word segmentation tends to degrade sharply when applied to new domains. Internet provides large amounts of raw text, and statistics collected from it have been used to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Bansal and Klein, 2011; Zhou et al., 2011). The Internet also gives massive (although slight and sparse) natural annotations in the forms of structural information including hyperlinks, fonts, colors and layouts (Sun, 2011a). These annotations usually imply valuable knowledge for problems such as word segmentation and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences. Figure 1 shows an example. The hyperlink indicates 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa a"
P13-1075,P08-1076,0,0.0750259,"009) extracted character classification instances from raw text for Chinese word segmentation, resorting to the indication of punctuation marks between characters. Sun and Xu (Sun and Xu, 2011) utilized the features derived from large-scaled unlabeled text to improve Chinese word segmentation. Although the two work also made use of large-scaled raw text, our method is essentially different from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, thus to result in a model with better coverage and higher accuracy. Researchers have also investigated unsupervised methods in word segmentation (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). An unsupervised method mines the latent distribution regularity in the raw text, and automatically induces word segmentation knowledge from it. Our method also needs large amounts of ex"
P13-1075,P09-1117,0,0.0211159,"om raw text for Chinese word segmentation, resorting to the indication of punctuation marks between characters. Sun and Xu (Sun and Xu, 2011) utilized the features derived from large-scaled unlabeled text to improve Chinese word segmentation. Although the two work also made use of large-scaled raw text, our method is essentially different from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, thus to result in a model with better coverage and higher accuracy. Researchers have also investigated unsupervised methods in word segmentation (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). An unsupervised method mines the latent distribution regularity in the raw text, and automatically induces word segmentation knowledge from it. Our method also needs large amounts of external data, but it aims to leverage the knowledge"
P13-1075,C10-1132,0,0.0120202,"ing way to improve Chinese word segmentation, it focuses on the utilization of fuzzy and sparse knowledge on the Internet rather than making full use of a specific humanannotated corpus. On the other hand, since only a single classifier and local features are used in our method, better performance could be achieved 766 solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of word co-occurrence statistics collected in raw text or Internet n-grams to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011; Ban"
P13-1075,I11-1035,0,0.157525,") Newswire 97.35 Out-of-Domain Chemistry 93.61 Physics 95.10 Machinery 96.08 92.42 Literature Finance 92.50 Computer 89.46 Medicine 91.88 Average 93.01 97.4 97.3 using selected sentences 97.2 using all sentences 97.1 10000 20000 40000 80000 160000 320000 640000 Count of selected sentences Table 3: Performance of the baseline classifier and the classifier enhanced with natural annotations in Chinese wikipedia. 5.2 97.5 Figure 4: Performance curve of the classifier enhanced with selected sentences of different scales. Model (Jiang et al., 2008) (Kruengkrai et al., 2009) (Zhang and Clark, 2010) (Wang et al., 2011) (Sun, 2011b) Our Work Classifier Enhanced with Natural Annotations The Chinese wikipedia contains about 0.5 million items. From their description text, about 3.9 millions of sentences with natural annotations are extracted. With the CTB training set as the existing corpus C, about 0.8 million sentences are reserved according to Algorithm 2, the segmentations given by constraint decoding are used as additional training data for the enhanced classifier. According to the previous description, the difference of the scores of constraint decoding and normal decoding, δ = S(y) − S(˜ y ), indicates t"
P13-1075,O03-4001,0,0.0212423,"tations. It is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways. Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated 7 Conclusion and Future Work This work presents a novel discriminative learning algorithm to utilize the knowledge in the massive natural annotations on the Internet. Natural annotations implied by structural information are used to decrease the searching space of the classifier, then the constraint decoding in the pruned searching space gives predictions not worse than the normal decoding does. Annotation differences between the outputs of constraint decoding and normal decoding are used to train the enhanced classifier, linguistic knowl"
P13-1075,C08-1128,0,0.00915802,"ork Li and Sun (2009) extracted character classification instances from raw text for Chinese word segmentation, resorting to the indication of punctuation marks between characters. Sun and Xu (Sun and Xu, 2011) utilized the features derived from large-scaled unlabeled text to improve Chinese word segmentation. Although the two work also made use of large-scaled raw text, our method is essentially different from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, thus to result in a model with better coverage and higher accuracy. Researchers have also investigated unsupervised methods in word segmentation (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). An unsupervised method mines the latent distribution regularity in the raw text, and automatically induces word segmentation knowledge from it. Our method also"
P13-1075,W03-1728,0,0.212908,"rmation including hyperlinks, fonts, colors and layouts (Sun, 2011a). These annotations usually imply valuable knowledge for problems such as word segmentation and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences. Figure 1 shows an example. The hyperlink indicates 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in 761 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 761–769, c Sofia, Bulgaria, August 4-9 20"
P13-1075,E09-1100,0,0.0208935,"of external data, but it aims to leverage the knowledge in the fuzzy and sparse annotations. It is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways. Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated 7 Conclusion and Future Work This work presents a novel discriminative learning algorithm to utilize the knowledge in the massive natural annotations on the Internet. Natural annotations implied by structural information are used to decrease the searching space of the classifier, then the constraint decoding in the pruned searching space gives predictions not worse than the normal decoding does. Annotation differences between the outputs of constraint decodi"
P13-1075,W03-1711,0,0.0407695,"he viewpoint of resource utilization, the comparison between our system and previous work without using additional training data is unfair. However, we believe this work shows another interesting way to improve Chinese word segmentation, it focuses on the utilization of fuzzy and sparse knowledge on the Internet rather than making full use of a specific humanannotated corpus. On the other hand, since only a single classifier and local features are used in our method, better performance could be achieved 766 solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are als"
P13-1075,P11-1156,0,0.0603158,"ᆑ ௶ ဴ ཝ  स ྸ i j ࠼ n j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶ ဴ ཝ  स ྸ i-1 i j ࠼ n ࠼ n j+1 (b) Knowledge for word segmentation n ఊ น ᆑ ௶ ဴ ཝ  स ྸ i-1 i j j+1 (c) Knowledge for dependency parsing Figure 1: Natural annotations for word segmentation and dependency parsing. size (e.g. tens of thousands), the performance of word segmentation tends to degrade sharply when applied to new domains. Internet provides large amounts of raw text, and statistics collected from it have been used to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Bansal and Klein, 2011; Zhou et al., 2011). The Internet also gives massive (although slight and sparse) natural annotations in the forms of structural information including hyperlinks, fonts, colors and layouts (Sun, 2011a). These annotations usually imply valuable knowledge for problems such as word segmentation and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences. Figure 1 shows an example. The hyperlink indicates 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for e"
P13-1075,P09-1058,0,\N,Missing
P13-1075,P04-1059,0,\N,Missing
P13-1075,P08-1043,0,\N,Missing
P13-1075,W10-4126,0,\N,Missing
P13-1075,C08-1039,0,\N,Missing
P13-1105,J03-4003,0,0.0135797,"Missing"
P13-1105,P11-1061,0,0.0458434,"gaard, 2011) employ non-lexicalized parser trained on other languages to process a target language. McDonald et al. (2011) adapts their multi-source parser according to DCA, while Naseem et al. (2012) selects a selective sharing model to make better use of grammar information in multi-sources. Due to similar reasons, many works are devoted to POS projection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems. Some seek for unsupervised methods, e.g. Naseem et al. (2009), and some further improve the projection by a graphbased projection (Das and Petrov, 2011). Our model differs from the approaches above in its emphasis on utilizing information from both sides of bilingual corpus in an unsupervised training framework, while most of the work above only utilize the information from a single side. 6 Experiments In this section, we evaluate the performance of the MST dependency parser (McDonald et al., 2005b) which is trained by our bilingually-guided model on 5 languages. And the features used in our experiments are summarized in Table 1. 6.1 Experiment Setup Datasets and Evaluation Our experiments are run on five different languages: Chinese(ch), Dan"
P13-1105,P09-1042,0,0.224158,"and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., 2010), which resorts to bilingual text with one language parsed, and projects the syntactic information from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et al., 2010). Relying on a predefined distributional assumption and iteratively maximizing an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising substitute for language"
P13-1105,P02-1050,0,0.0436828,"ces (step 2). Lines 5-6 are equivalent to the E-step of the EM algorithm, and lines 7-8 are equivalent to the M-step. 5 Related work The DMV (Klein and Manning, 2004) is a singlestate head automata model (Alshawi, 1996) which is based on POS tags. And DMV learns the grammar via inside-outside re-estimation (Baker, 1979) without any smoothing, while Spitkovsky et al. (2010) utilizes smoothing and learning strategy during grammar learning and William et al. (2009) improves DMV with richer context. The dependency projection method DPA (Hwa et al., 2005) based on Direct Correspondence Assumption (Hwa et al., 2002) can be described as: if there is a pair of source words with a dependency relationship, the corresponding aligned words in target sentence can be considered as having the same dependency relationship equivalently (e.g. Figure 2). The Word Pair Classification (WPC) method (Jiang and Liu, 2010) modifies the DPA method and makes it more robust. Smith and Eisner (2009) propose an adaptation method founded on quasi-synchronous grammar features 1067 Type Feature Template wordi ◦ posi wordj ◦ posj Unigram wordi wordj posi posj Bigram wordi ◦ posj wordi ◦ wordj wordi ◦ posi ◦ posj wordi ◦ posi ◦ word"
P13-1105,P10-1002,1,0.937948,"e to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et al., 2010). Relying on a predefined distributional assumption and iteratively maximizing an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising substitute for languages with a large amount of bilingual sentences and an existing parser of the counterpart language. By projecting syntactic structures directly (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) across bilingual texts or indirectly across multilingual texts (Snyder et al., 2009; McDonald et al., 2011; Naseem et al., 2012), a better dependency grammar can be easily induced, if syntactic isomorphism is largely maintained between target and source languages. Unsupervised induction and bilingual projection run according to totally different princ"
P13-1105,P04-1061,0,0.871764,"aseline on average. 1 Introduction In past decades supervised methods achieved the state-of-the-art in constituency parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006) and dependency parsing (McDonald et al., 2005a; McDonald et al., 2006; Nivre et al., 2006; Nivre et al., 2007; Koo and Collins, 2010). For supervised models, the human-annotated corpora on which models are trained, however, are expensive and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., 2010), which resorts to bilingual text with one language parsed, and projects the syntactic information from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et"
P13-1105,N10-1083,0,0.0617381,"Missing"
P13-1105,P10-1001,0,0.0462684,"Missing"
P13-1105,P06-1109,0,0.0838493,"lein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., 2010), which resorts to bilingual text with one language parsed, and projects the syntactic information from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et al., 2010). Relying on a predefined distributional assumption and iteratively maximizing an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising substitute for languages with a large amount of bilingual sentences and an existing parser of the counterpart language. By projecting syntactic structures directly (Hwa et al., 2005; Smi"
P13-1105,P08-1068,0,0.0224104,"n constituency parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006) and dependency parsing (McDonald et al., 2005a; McDonald et al., 2006; Nivre et al., 2006; Nivre et al., 2007; Koo and Collins, 2010). For supervised models, the human-annotated corpora on which models are trained, however, are expensive and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., 2010), which resorts to bilingual text with one language parsed, and projects the syntactic information from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et al., 2010). Relying on a predefined distributional assumption and iter"
P13-1105,W06-2920,0,0.123476,"ervised training framework, while most of the work above only utilize the information from a single side. 6 Experiments In this section, we evaluate the performance of the MST dependency parser (McDonald et al., 2005b) which is trained by our bilingually-guided model on 5 languages. And the features used in our experiments are summarized in Table 1. 6.1 Experiment Setup Datasets and Evaluation Our experiments are run on five different languages: Chinese(ch), Danish(da), Dutch(nl), Portuguese(pt) and Swedish(sv) (da, nl, pt and sv are free data sets distributed for the 2006 CoNLL Shared Tasks (Buchholz and Marsi, 2006)). For all languages, we only use English-target parallel data: we take the FBIS English-Chinese bitext as bilingual corpus for English-Chinese dependency projection which contains 239K sentence pairs with about 8.9M/6.9M words in English/Chinese, and for other languages we use the readily available data in the Europarl corpus. Then we run tests on the Penn Chinese Treebank (CTB) and CoNLL-X test sets. English sentences are tagged by the implementations of the POS tagger of Collins (2002), which is trained on WSJ. The source sentences are then parsed by an implementation of 2nd-ordered MST mod"
P13-1105,E06-1011,0,0.0517647,"all languages, we only use English-target parallel data: we take the FBIS English-Chinese bitext as bilingual corpus for English-Chinese dependency projection which contains 239K sentence pairs with about 8.9M/6.9M words in English/Chinese, and for other languages we use the readily available data in the Europarl corpus. Then we run tests on the Penn Chinese Treebank (CTB) and CoNLL-X test sets. English sentences are tagged by the implementations of the POS tagger of Collins (2002), which is trained on WSJ. The source sentences are then parsed by an implementation of 2nd-ordered MST model of McDonald and Pereira (2006), which is trained on dependency trees extracted from Penn Treebank. As the evaluation metric, we use parsing accuracy which is the percentage of the words which have found their correct parents. We evaluate on sentences with all length for our method. Training Regime In experiments, we use the projection method proposed by Jiang and Liu (2010) to provide the projection instances. And we train the projection part α = 0 first for initialization, on which the whole model will be trained. Availing of the initialization method, the model can converge very fast (about 3 iterations is sufficient) an"
P13-1105,P05-1022,0,0.0833433,"Missing"
P13-1105,P05-1012,0,0.207625,"rojection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems. Some seek for unsupervised methods, e.g. Naseem et al. (2009), and some further improve the projection by a graphbased projection (Das and Petrov, 2011). Our model differs from the approaches above in its emphasis on utilizing information from both sides of bilingual corpus in an unsupervised training framework, while most of the work above only utilize the information from a single side. 6 Experiments In this section, we evaluate the performance of the MST dependency parser (McDonald et al., 2005b) which is trained by our bilingually-guided model on 5 languages. And the features used in our experiments are summarized in Table 1. 6.1 Experiment Setup Datasets and Evaluation Our experiments are run on five different languages: Chinese(ch), Danish(da), Dutch(nl), Portuguese(pt) and Swedish(sv) (da, nl, pt and sv are free data sets distributed for the 2006 CoNLL Shared Tasks (Buchholz and Marsi, 2006)). For all languages, we only use English-target parallel data: we take the FBIS English-Chinese bitext as bilingual corpus for English-Chinese dependency projection which contains 239K sente"
P13-1105,P10-1003,0,0.0805739,"et al., 2006; Nivre et al., 2006; Nivre et al., 2007; Koo and Collins, 2010). For supervised models, the human-annotated corpora on which models are trained, however, are expensive and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., 2010), which resorts to bilingual text with one language parsed, and projects the syntactic information from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et al., 2010). Relying on a predefined distributional assumption and iteratively maximizing an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower pe"
P13-1105,H05-1066,0,0.272755,"Missing"
P13-1105,D11-1005,0,0.0309407,"1 ◦ posj+1 posi−1 ◦ posi ◦ posj+1 posi+1 ◦ posj−1 ◦ posj Surrounding posi−1 ◦ posi ◦ posj posi ◦ posj ◦ posj+1 posi−1 ◦ posj−1 ◦ posj posi ◦ posi+1 ◦ posj−1 posi−1 ◦ posi ◦ posj−1 ◦ posj posi ◦ posi+1 ◦ posj−1 ◦ posj Table 1: Feature templates for dependency parsing. For edge deij : wordi is the parent word and wordj is the child word, similar to ”pos”. ”+1” denotes the preceding token of the sentence, similar to ”-1”. for dependency projection and annotation, which requires a small set of dependency annotated corpus of target language. Similarly, using indirect information from multilingual (Cohen et al., 2011; T¨ackstr¨om et al., 2012) is an effective way to improve unsupervised parsing. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) employ non-lexicalized parser trained on other languages to process a target language. McDonald et al. (2011) adapts their multi-source parser according to DCA, while Naseem et al. (2012) selects a selective sharing model to make better use of grammar information in multi-sources. Due to similar reasons, many works are devoted to POS projection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems."
P13-1105,W06-2932,0,0.0686556,"Missing"
P13-1105,D11-1006,0,0.417181,"an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising substitute for languages with a large amount of bilingual sentences and an existing parser of the counterpart language. By projecting syntactic structures directly (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) across bilingual texts or indirectly across multilingual texts (Snyder et al., 2009; McDonald et al., 2011; Naseem et al., 2012), a better dependency grammar can be easily induced, if syntactic isomorphism is largely maintained between target and source languages. Unsupervised induction and bilingual projection run according to totally different principles, the former mines the underlying structure of the monolingual language, while the latter leverages the syntactic knowledge of the parsed counter1063 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1063–1072, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Target se"
P13-1105,P12-1066,0,0.383231,"r (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising substitute for languages with a large amount of bilingual sentences and an existing parser of the counterpart language. By projecting syntactic structures directly (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) across bilingual texts or indirectly across multilingual texts (Snyder et al., 2009; McDonald et al., 2011; Naseem et al., 2012), a better dependency grammar can be easily induced, if syntactic isomorphism is largely maintained between target and source languages. Unsupervised induction and bilingual projection run according to totally different principles, the former mines the underlying structure of the monolingual language, while the latter leverages the syntactic knowledge of the parsed counter1063 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1063–1072, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Target sentences Random Treeban"
P13-1105,W06-2933,0,0.0492141,"Missing"
P13-1105,H01-1035,0,0.127549,"tated corpus of target language. Similarly, using indirect information from multilingual (Cohen et al., 2011; T¨ackstr¨om et al., 2012) is an effective way to improve unsupervised parsing. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) employ non-lexicalized parser trained on other languages to process a target language. McDonald et al. (2011) adapts their multi-source parser according to DCA, while Naseem et al. (2012) selects a selective sharing model to make better use of grammar information in multi-sources. Due to similar reasons, many works are devoted to POS projection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems. Some seek for unsupervised methods, e.g. Naseem et al. (2009), and some further improve the projection by a graphbased projection (Das and Petrov, 2011). Our model differs from the approaches above in its emphasis on utilizing information from both sides of bilingual corpus in an unsupervised training framework, while most of the work above only utilize the information from a single side. 6 Experiments In this section, we evaluate the performance of the MST dependency parser (McDonald et al., 2005b) which is"
P13-1105,I08-3008,0,0.0394136,"j ◦ posj+1 posi−1 ◦ posj−1 ◦ posj posi ◦ posi+1 ◦ posj−1 posi−1 ◦ posi ◦ posj−1 ◦ posj posi ◦ posi+1 ◦ posj−1 ◦ posj Table 1: Feature templates for dependency parsing. For edge deij : wordi is the parent word and wordj is the child word, similar to ”pos”. ”+1” denotes the preceding token of the sentence, similar to ”-1”. for dependency projection and annotation, which requires a small set of dependency annotated corpus of target language. Similarly, using indirect information from multilingual (Cohen et al., 2011; T¨ackstr¨om et al., 2012) is an effective way to improve unsupervised parsing. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) employ non-lexicalized parser trained on other languages to process a target language. McDonald et al. (2011) adapts their multi-source parser according to DCA, while Naseem et al. (2012) selects a selective sharing model to make better use of grammar information in multi-sources. Due to similar reasons, many works are devoted to POS projection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems. Some seek for unsupervised methods, e.g. Naseem et al. (2009), and some further improve the projection b"
P13-1105,P06-1055,0,0.0401425,"Missing"
P13-1105,N01-1023,0,0.0174482,"model to build new treebank on target language for next iteration. 4. Repeat steps 1, 2 and 3 until convergence. The unsupervised objective is optimized by the loop—”tree bank→optimized model→new tree bank”. The treebank is evolved for runs. The unsupervised model gets projection constraint implicitly from those parse trees which contain information from projection part. The projection objective is optimized by the circulation—”projected instances→optimized model”, these projected instances will not change once we get them. The iterative procedure proposed here is not a co-training algorithm (Sarkar, 2001; Hwa et al., 2003), because the input of the projection objective is static. 1066 4.1 Algorithm 3 Training joint model Joint Objective 1: 2: 3: 4: 5: 6: 7: 8: 9: For multi-objective optimization method, we employ the classical weighted-sum approach which just calculates the weighted linear sum of the objectives: OBJ = X weightm objm (8) DP , DN ← proj(F , DF , A, E) build random DE λ ← train(DP , DN ) repeat ⊲ E step for each E ∈ E do DE ← parse(E, λ) e , DP , DN , ℓ(λ)) ∇ℓ(λ) ← grad(DE , D E λ ←climb(ℓ(λ), ∇ℓ(λ), λ) ⊲ M step until convergence m We combine the unsupervised objective (Formula"
P13-1105,P07-1096,0,0.0294276,"language. Similarly, using indirect information from multilingual (Cohen et al., 2011; T¨ackstr¨om et al., 2012) is an effective way to improve unsupervised parsing. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) employ non-lexicalized parser trained on other languages to process a target language. McDonald et al. (2011) adapts their multi-source parser according to DCA, while Naseem et al. (2012) selects a selective sharing model to make better use of grammar information in multi-sources. Due to similar reasons, many works are devoted to POS projection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems. Some seek for unsupervised methods, e.g. Naseem et al. (2009), and some further improve the projection by a graphbased projection (Das and Petrov, 2011). Our model differs from the approaches above in its emphasis on utilizing information from both sides of bilingual corpus in an unsupervised training framework, while most of the work above only utilize the information from a single side. 6 Experiments In this section, we evaluate the performance of the MST dependency parser (McDonald et al., 2005b) which is trained by our bil"
P13-1105,P05-1044,0,0.234358,"roduction In past decades supervised methods achieved the state-of-the-art in constituency parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006) and dependency parsing (McDonald et al., 2005a; McDonald et al., 2006; Nivre et al., 2006; Nivre et al., 2007; Koo and Collins, 2010). For supervised models, the human-annotated corpora on which models are trained, however, are expensive and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., 2010), which resorts to bilingual text with one language parsed, and projects the syntactic information from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et"
P13-1105,D09-1086,0,0.454207,"from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et al., 2010). Relying on a predefined distributional assumption and iteratively maximizing an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising substitute for languages with a large amount of bilingual sentences and an existing parser of the counterpart language. By projecting syntactic structures directly (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) across bilingual texts or indirectly across multilingual texts (Snyder et al., 2009; McDonald et al., 2011; Naseem et al., 2012), a better dependency grammar can be easily induced, if syntactic isomorphism is largely maintained between target and source languages. Unsupervised induction and bilingual projection run according to t"
P13-1105,P09-1009,0,0.042788,"eratively maximizing an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising substitute for languages with a large amount of bilingual sentences and an existing parser of the counterpart language. By projecting syntactic structures directly (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) across bilingual texts or indirectly across multilingual texts (Snyder et al., 2009; McDonald et al., 2011; Naseem et al., 2012), a better dependency grammar can be easily induced, if syntactic isomorphism is largely maintained between target and source languages. Unsupervised induction and bilingual projection run according to totally different principles, the former mines the underlying structure of the monolingual language, while the latter leverages the syntactic knowledge of the parsed counter1063 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1063–1072, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computationa"
P13-1105,P11-2120,0,0.0271364,"◦ posj−1 posi−1 ◦ posi ◦ posj−1 ◦ posj posi ◦ posi+1 ◦ posj−1 ◦ posj Table 1: Feature templates for dependency parsing. For edge deij : wordi is the parent word and wordj is the child word, similar to ”pos”. ”+1” denotes the preceding token of the sentence, similar to ”-1”. for dependency projection and annotation, which requires a small set of dependency annotated corpus of target language. Similarly, using indirect information from multilingual (Cohen et al., 2011; T¨ackstr¨om et al., 2012) is an effective way to improve unsupervised parsing. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) employ non-lexicalized parser trained on other languages to process a target language. McDonald et al. (2011) adapts their multi-source parser according to DCA, while Naseem et al. (2012) selects a selective sharing model to make better use of grammar information in multi-sources. Due to similar reasons, many works are devoted to POS projection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems. Some seek for unsupervised methods, e.g. Naseem et al. (2009), and some further improve the projection by a graphbased projection (Das and Petr"
P13-1105,N10-1116,0,0.135655,"Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., 2010), which resorts to bilingual text with one language parsed, and projects the syntactic information from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et al., 2010). Relying on a predefined distributional assumption and iteratively maximizing an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising substitute for languages with a large amount of bilingual sentences and an existing parser of the counterpart language. By projecting syntactic structures directly (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) across"
P13-1105,N12-1052,0,0.223986,"Missing"
P13-1105,N09-1012,0,0.11569,"Missing"
P13-1105,W02-1001,0,\N,Missing
P13-2064,J93-2003,0,0.0364286,"proach outperforms both 1-best and n-best alignments. Figure 1: A bigraph constructed from an alignment (a), and its disjoint MCSs (b). of independent subhypergraphs, which is computationally feasible in practice (§ 3.2). Experimental results show that our approach significantly improves translation performance by up to 1.3 BLEU points over 1-best alignments (§ 4.3). 1 Introduction Word alignment is the task of identifying translational relations between words in parallel corpora, in which a word at one language is usually translated into several words at the other language (fertility model) (Brown et al., 1993). Given that many-to-many links are common in natural languages (Moore, 2005), it is necessary to pay attention to the relations among alignment links. In this paper, we have proposed a novel graphbased compact representation of word alignment, which takes into account the joint distribution of alignment links. We first transform each alignment to a bigraph that can be decomposed into a set of subgraphs, where all interrelated links are in the same subgraph (§ 2.1). Then we employ a weighted partite hypergraph to encode multiple bigraphs (§ 2.2). The main challenge of this research is to effic"
P13-2064,J07-2003,0,0.301186,"w well it is. Formally, a weighted bipartite hypergraph H is a triple hS, T, Ei where S and T are two sets of vertices on the source and target sides, and E are hyperedges associated with weights. Currently, we estimate the weights of hyperedges from an nbest list by calculating relative frequencies: w(ei ) = P BG∈N P 3 Graph-based Rule Extraction In this section we describe how to extract translation rules from a hypergraph (§ 3.1) and how to estimate their probabilities (§ 3.2). 3.1 Extraction Algorithm We extract translation rules from a hypergraph for the hierarchical phrase-based system (Chiang, 2007). Chiang (2007) describes a rule extraction algorithm that involves two steps: (1) extract phrases from 1-best alignments; (2) obtain variable rules by replacing sub-phrase pairs with nonterminals. Our extraction algorithm differs at the first step, in which we extract phrases from hypergraphs instead of 1-best alignments. Rather than restricting ourselves by the alignment consistency in the traditional algorithm, we extract all possible candidate target phrases for each source phrase. To maintain a reasonable rule table size, we filter out less promising candidates that have a fractional coun"
P13-2064,J04-4002,0,0.289326,"alignments, it is unrealistic to enumerate all consistent alignments explicitly for each phrase pair. Recall that a hypergraph can be decomposed to a list of independent subhypergraphs, and an alignment is a combination of the sub-alignments from the decompositions. We observe that a phrase pair is absolutely consistent with the subalignments from some subhypergraphs, while possibly consistent with the others. As an example, p(A|H, P ) c(P |H) = = p(A|H) Q p(A|hi , P ) hi ∈OS p(A|hi ) h ∈OS Qi After we get the fractional counts of translation rules, we can estimate their relative frequencies (Och and Ney, 2004). We follow (Liu et al., 2009; Tu et al., 2011) to learn lexical tables from n-best lists and then calculate the lexical weights. 360 Rules from. . . 1-best 10-best Hypergraph Rules 257M 427M 426M MT03 33.45 34.10 34.71 MT04 35.25 35.71 36.24 MT05 33.63 34.04 34.41 Avg. 34.11 34.62 35.12 Table 1: Evaluation of translation quality. 4 Experiments 4.1 1 vertices hyperedges Setup 0.8 4.2 percentage We carry out our experiments on Chinese-English translation tasks using a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 million sentence pairs f"
P13-2064,P03-1021,0,0.0636987,"6.24 MT05 33.63 34.04 34.41 Avg. 34.11 34.62 35.12 Table 1: Evaluation of translation quality. 4 Experiments 4.1 1 vertices hyperedges Setup 0.8 4.2 percentage We carry out our experiments on Chinese-English translation tasks using a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 million sentence pairs from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, caseinsensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We first follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 10-best lists in two translation directions, and use “growdiag-final-and” strategy (Koehn et al., 2003) to generate the final n-best lists by selecting the top n alignments. We re-estimated the probability of each alignment in the n-best list using renormalization (Venugopal et al., 2008). Finally we construct"
P13-2064,P02-1040,0,0.0864087,"4.2 percentage We carry out our experiments on Chinese-English translation tasks using a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 million sentence pairs from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, caseinsensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We first follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 10-best lists in two translation directions, and use “growdiag-final-and” strategy (Koehn et al., 2003) to generate the final n-best lists by selecting the top n alignments. We re-estimated the probability of each alignment in the n-best list using renormalization (Venugopal et al., 2008). Finally we construct weighted alignment hypergraphs from these n-best lists.2 When extracting rules from hypergraphs, we set the pruning threshold t = 0.5. 0.6 0.4 0.2 0 1 2 3 4"
P13-2064,P05-1066,0,0.134044,"Missing"
P13-2064,D10-1053,0,0.0313696,"Missing"
P13-2064,P08-1115,0,0.0231616,"ergraphs outperform n-best lists, indicating: (1) our approach has a better estimation of rule probabilities because we estimate the probabilities from a much larger alignment space that can not be represented by n-best lists, (2) our approach can extract good rules that cannot be extracted from any single alignments in the n-best lists. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g., using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010; Tu et al., 2012a), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices over 1-best word alignments (Liu et al., 2009; Tu et al., 2011; Tu et al., 2012b). Liu et al., (2009) estimate the link probabilities from n-best lists, while Gispert et al., (2010) learn the alignment posterior probabilities directly from IBM models. However, both of them ignore the relations among alignment links. By contrast, our approach takes into account the joint distribution of alignment links and explores the fertility model past the link level. 6 Conclusion We have presented a novel compact representation of word alignment, named weighted bipartite h"
P13-2064,N03-1017,0,0.00715537,"train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, caseinsensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We first follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 10-best lists in two translation directions, and use “growdiag-final-and” strategy (Koehn et al., 2003) to generate the final n-best lists by selecting the top n alignments. We re-estimated the probability of each alignment in the n-best list using renormalization (Venugopal et al., 2008). Finally we construct weighted alignment hypergraphs from these n-best lists.2 When extracting rules from hypergraphs, we set the pruning threshold t = 0.5. 0.6 0.4 0.2 0 1 2 3 4 5 6 7 8 9 10 number of vertices (hyperedges) Figure 4: The distribution of vertices (hyperedges) number of the subhypergraphs. peredges on average. This suggests that the divideand-conquer strategy makes the extraction computationally"
P13-2064,W02-1019,0,0.0329766,"opose a computationally tractable divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. Experimental results show that our approach outperforms both 1-best and n-best alignments. 5 Related Work Our research builds on previous work in the field of graph models and compact representations. Graph models have been used before in word alignment: the search space of word alignment can be structured as a graph and the search problem can be reformulated as finding the optimal path though this graph (e.g., (Och and Ney, 2004; Liu et al., 2010)). In addition, Kumar and Byrne (2002) define a graph distance as a loss function for minimum Bayes-risk word alignment, Riesa and Marcu (2010) open up the word alignment task to advances in hypergraph algorithms currently used in parsing. As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. Acknowledgement The authors are supported by 863 State Key Project No. 2011AA01A207, National Key Technology R&D Program No. 2012BAH39B03 and National Natural Science Foundation of China (Contracts 61202216). Qun Liu’s work is partially supported by Science"
P13-2064,D09-1106,1,0.910519,"Missing"
P13-2064,P10-1017,0,0.0188702,"ndependent subhypergraphs. Experimental results show that our approach outperforms both 1-best and n-best alignments. 5 Related Work Our research builds on previous work in the field of graph models and compact representations. Graph models have been used before in word alignment: the search space of word alignment can be structured as a graph and the search problem can be reformulated as finding the optimal path though this graph (e.g., (Och and Ney, 2004; Liu et al., 2010)). In addition, Kumar and Byrne (2002) define a graph distance as a loss function for minimum Bayes-risk word alignment, Riesa and Marcu (2010) open up the word alignment task to advances in hypergraph algorithms currently used in parsing. As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. Acknowledgement The authors are supported by 863 State Key Project No. 2011AA01A207, National Key Technology R&D Program No. 2012BAH39B03 and National Natural Science Foundation of China (Contracts 61202216). Qun Liu’s work is partially supported by Science Foundation Ireland (Grant No.07/CE/I1142) as part of the CNGL at Dublin City University. We thank Junhui"
P13-2064,C10-1123,1,0.883284,"es. We can see that both the “Shared” and “Non-shared” rules learned from hypergraphs outperform n-best lists, indicating: (1) our approach has a better estimation of rule probabilities because we estimate the probabilities from a much larger alignment space that can not be represented by n-best lists, (2) our approach can extract good rules that cannot be extracted from any single alignments in the n-best lists. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g., using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010; Tu et al., 2012a), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices over 1-best word alignments (Liu et al., 2009; Tu et al., 2011; Tu et al., 2012b). Liu et al., (2009) estimate the link probabilities from n-best lists, while Gispert et al., (2010) learn the alignment posterior probabilities directly from IBM models. However, both of them ignore the relations among alignment links. By contrast, our approach takes into account the joint distribution of alignment links and explores the fertility model past the link level. 6 Conclusion We have presen"
P13-2064,I11-1145,1,0.901381,"nsistent alignments explicitly for each phrase pair. Recall that a hypergraph can be decomposed to a list of independent subhypergraphs, and an alignment is a combination of the sub-alignments from the decompositions. We observe that a phrase pair is absolutely consistent with the subalignments from some subhypergraphs, while possibly consistent with the others. As an example, p(A|H, P ) c(P |H) = = p(A|H) Q p(A|hi , P ) hi ∈OS p(A|hi ) h ∈OS Qi After we get the fractional counts of translation rules, we can estimate their relative frequencies (Och and Ney, 2004). We follow (Liu et al., 2009; Tu et al., 2011) to learn lexical tables from n-best lists and then calculate the lexical weights. 360 Rules from. . . 1-best 10-best Hypergraph Rules 257M 427M 426M MT03 33.45 34.10 34.71 MT04 35.25 35.71 36.24 MT05 33.63 34.04 34.41 Avg. 34.11 34.62 35.12 Table 1: Evaluation of translation quality. 4 Experiments 4.1 1 vertices hyperedges Setup 0.8 4.2 percentage We carry out our experiments on Chinese-English translation tasks using a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 million sentence pairs from LDC dataset.1 We train a 4-gram language mo"
P13-2064,C12-2122,1,0.832873,"Missing"
P13-2064,2008.amta-papers.18,0,0.0162348,"sing a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 million sentence pairs from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, caseinsensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We first follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 10-best lists in two translation directions, and use “growdiag-final-and” strategy (Koehn et al., 2003) to generate the final n-best lists by selecting the top n alignments. We re-estimated the probability of each alignment in the n-best list using renormalization (Venugopal et al., 2008). Finally we construct weighted alignment hypergraphs from these n-best lists.2 When extracting rules from hypergraphs, we set the pruning threshold t = 0.5. 0.6 0.4 0.2 0 1 2 3 4 5 6 7 8 9 10 number of vertices (hyperedges) Figure 4: The distribution of vertices"
P13-2064,J10-3002,1,0.85755,"is an NP-complete problem, we propose a computationally tractable divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. Experimental results show that our approach outperforms both 1-best and n-best alignments. 5 Related Work Our research builds on previous work in the field of graph models and compact representations. Graph models have been used before in word alignment: the search space of word alignment can be structured as a graph and the search problem can be reformulated as finding the optimal path though this graph (e.g., (Och and Ney, 2004; Liu et al., 2010)). In addition, Kumar and Byrne (2002) define a graph distance as a loss function for minimum Bayes-risk word alignment, Riesa and Marcu (2010) open up the word alignment task to advances in hypergraph algorithms currently used in parsing. As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. Acknowledgement The authors are supported by 863 State Key Project No. 2011AA01A207, National Key Technology R&D Program No. 2012BAH39B03 and National Natural Science Foundation of China (Contracts 61202216). Qun Liu’s w"
P13-2064,D08-1022,0,0.02252,"for the two approaches. We can see that both the “Shared” and “Non-shared” rules learned from hypergraphs outperform n-best lists, indicating: (1) our approach has a better estimation of rule probabilities because we estimate the probabilities from a much larger alignment space that can not be represented by n-best lists, (2) our approach can extract good rules that cannot be extracted from any single alignments in the n-best lists. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g., using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010; Tu et al., 2012a), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices over 1-best word alignments (Liu et al., 2009; Tu et al., 2011; Tu et al., 2012b). Liu et al., (2009) estimate the link probabilities from n-best lists, while Gispert et al., (2010) learn the alignment posterior probabilities directly from IBM models. However, both of them ignore the relations among alignment links. By contrast, our approach takes into account the joint distribution of alignment links and explores the fertility model past the link level. 6 Conclusi"
P13-2064,H05-1011,0,0.0325573,"d from an alignment (a), and its disjoint MCSs (b). of independent subhypergraphs, which is computationally feasible in practice (§ 3.2). Experimental results show that our approach significantly improves translation performance by up to 1.3 BLEU points over 1-best alignments (§ 4.3). 1 Introduction Word alignment is the task of identifying translational relations between words in parallel corpora, in which a word at one language is usually translated into several words at the other language (fertility model) (Brown et al., 1993). Given that many-to-many links are common in natural languages (Moore, 2005), it is necessary to pay attention to the relations among alignment links. In this paper, we have proposed a novel graphbased compact representation of word alignment, which takes into account the joint distribution of alignment links. We first transform each alignment to a bigraph that can be decomposed into a set of subgraphs, where all interrelated links are in the same subgraph (§ 2.1). Then we employ a weighted partite hypergraph to encode multiple bigraphs (§ 2.2). The main challenge of this research is to efficiently calculate the fractional counts for rules extracted from hypergraphs."
P13-2065,P06-1121,0,0.0287603,", and ultimately improve the translation performance. Currently, most methods on statistical machine translation (SMT) are developed for translation of languages with limited morphology (e.g., English, Chinese). They assumed that word was the atomic translation unit (ATU), always ignoring the internal morphological structure of word. This assumption can be traced back to the original IBM word-based models (Brown et al., 1993) and several significantly improved models, including phrase-based (Och and Ney, 2004; Koehn et al., 2003), hierarchical (Chiang, 2005) and syntactic (Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006) models. These improved models worked well for translating languages like English with large scale parallel corpora available. 364 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 364–369, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (A) Instances of translation rule zunyi /STM yighin /STM zunyi /STM i /SUF (1) Original:zunyi yighin+i+gha Meaning:of zunyi conference gha /SUF yighin /STM (2) zunyi /STM i /SUF Original:zunyi yighin+i+da Meaning:on zunyi conference yighin /STM i /SUF da /SUF (3)"
P13-2065,H05-1085,0,0.0363908,"eading to tens of hundreds of possible inflected variants of lexicons for a single stem. Modeling each lexical form as a separate word will generate high out-of-vocabulary rate for SMT. Theoretically, ways like morphological analysis and increasing bilingual corpora could alleviate the problem of data sparsity, but most agglutinative languages are less-studied and suffer from the problem of resource-scarceness. Therefore, previous research mainly focused on the different inflected variants of the same stem and made various transformation of input by morphological analysis, such as (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006; Habash and Sadat, 2006; Bisazza and Federico, 2009; Wang et al., 2011). These work still assume that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. Current translation models are mainly designed for languages with limited morphology, which are not readily applicable to agglutinative languages as the difference in the way lexical forms are generated. In this paper, we propose a novel approach for translating agglutinative languages by treating stems and affixes differently. We employ stem as the at"
P13-2065,C08-1041,1,0.837614,"Missing"
P13-2065,D07-1091,0,0.0270072,"ets. For the language model, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 5-gram model with the target side of training corpus. And phrase-based Moses3 is used as our Related Work Most previous work on agglutinative language translation mainly focus on Turkish and Finnish. Bisazza and Federico (2009) and Mermer and Saraclar (2011) optimized morphological analysis as a pre-processing step to improve the translation between Turkish and English. Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). Yang 2 3 366 http://mt.xmu.edu.cn/cwmt2011/en/index.html. http://www.statmt.org/moses/ word stem morph affix UY-CH 31.74+0.0 33.74+2.0 32.69+0.95 34.34+2.6 KA-CH 28.64+0.0 30.14+1.5 29.21+0.57 30.19+2.27 UY #Type stem #Token #Type affix #Token KI-CH 35.05+0.0 35.52+0.47 34.97−0.08 35.96+0.91 Table 2: Translation results from Turkic languages to Chinese. word: ATU is surface form, stem: ATU is represented stem, morph: ATU denotes morpheme, aﬃx: stem translation with affix distribution similarity. BLEU scores in bold means significantly better than the baseline according to (Koehn, 2004) for p"
P13-2065,N03-1017,0,0.0231754,"n. In this way, we can achieve a balance between rule coverage and matching accuracy, and ultimately improve the translation performance. Currently, most methods on statistical machine translation (SMT) are developed for translation of languages with limited morphology (e.g., English, Chinese). They assumed that word was the atomic translation unit (ATU), always ignoring the internal morphological structure of word. This assumption can be traced back to the original IBM word-based models (Brown et al., 1993) and several significantly improved models, including phrase-based (Och and Ney, 2004; Koehn et al., 2003), hierarchical (Chiang, 2005) and syntactic (Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006) models. These improved models worked well for translating languages like English with large scale parallel corpora available. 364 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 364–369, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (A) Instances of translation rule zunyi /STM yighin /STM zunyi /STM i /SUF (1) Original:zunyi yighin+i+gha Meaning:of zunyi conference gha /SUF yighin /STM (2) zunyi /STM i /SUF O"
P13-2065,2009.iwslt-papers.1,0,0.103146,"le stem. Modeling each lexical form as a separate word will generate high out-of-vocabulary rate for SMT. Theoretically, ways like morphological analysis and increasing bilingual corpora could alleviate the problem of data sparsity, but most agglutinative languages are less-studied and suffer from the problem of resource-scarceness. Therefore, previous research mainly focused on the different inflected variants of the same stem and made various transformation of input by morphological analysis, such as (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006; Habash and Sadat, 2006; Bisazza and Federico, 2009; Wang et al., 2011). These work still assume that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. Current translation models are mainly designed for languages with limited morphology, which are not readily applicable to agglutinative languages as the difference in the way lexical forms are generated. In this paper, we propose a novel approach for translating agglutinative languages by treating stems and affixes differently. We employ stem as the atomic translation unit to alleviate data spareness. In addition, we associate e"
P13-2065,W04-3250,0,0.239852,"Missing"
P13-2065,J93-2003,0,0.0274257,"ased rule selection takes advantage of auxiliary syntactic roles of affixes to make a better rule selection. In this way, we can achieve a balance between rule coverage and matching accuracy, and ultimately improve the translation performance. Currently, most methods on statistical machine translation (SMT) are developed for translation of languages with limited morphology (e.g., English, Chinese). They assumed that word was the atomic translation unit (ATU), always ignoring the internal morphological structure of word. This assumption can be traced back to the original IBM word-based models (Brown et al., 1993) and several significantly improved models, including phrase-based (Och and Ney, 2004; Koehn et al., 2003), hierarchical (Chiang, 2005) and syntactic (Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006) models. These improved models worked well for translating languages like English with large scale parallel corpora available. 364 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 364–369, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (A) Instances of translation rule zunyi /STM yighin /STM zunyi /STM i /SU"
P13-2065,N04-4015,0,0.0334412,"xes, thus leading to tens of hundreds of possible inflected variants of lexicons for a single stem. Modeling each lexical form as a separate word will generate high out-of-vocabulary rate for SMT. Theoretically, ways like morphological analysis and increasing bilingual corpora could alleviate the problem of data sparsity, but most agglutinative languages are less-studied and suffer from the problem of resource-scarceness. Therefore, previous research mainly focused on the different inflected variants of the same stem and made various transformation of input by morphological analysis, such as (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006; Habash and Sadat, 2006; Bisazza and Federico, 2009; Wang et al., 2011). These work still assume that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. Current translation models are mainly designed for languages with limited morphology, which are not readily applicable to agglutinative languages as the difference in the way lexical forms are generated. In this paper, we propose a novel approach for translating agglutinative languages by treating stems and affixes differe"
P13-2065,P06-1077,1,0.774502,"ove the translation performance. Currently, most methods on statistical machine translation (SMT) are developed for translation of languages with limited morphology (e.g., English, Chinese). They assumed that word was the atomic translation unit (ATU), always ignoring the internal morphological structure of word. This assumption can be traced back to the original IBM word-based models (Brown et al., 1993) and several significantly improved models, including phrase-based (Och and Ney, 2004; Koehn et al., 2003), hierarchical (Chiang, 2005) and syntactic (Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006) models. These improved models worked well for translating languages like English with large scale parallel corpora available. 364 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 364–369, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (A) Instances of translation rule zunyi /STM yighin /STM zunyi /STM i /SUF (1) Original:zunyi yighin+i+gha Meaning:of zunyi conference gha /SUF yighin /STM (2) zunyi /STM i /SUF Original:zunyi yighin+i+da Meaning:on zunyi conference yighin /STM i /SUF da /SUF (3) Original:zunyi yig"
P13-2065,D07-1007,0,0.018872,"y is measured using the cosine distance similarity metric, given by sim(d1 , d2 ) = d1 · d2 ∥d1 ∥ × ∥d2 ∥ and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table. These works still assumed that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. There are also some work that employed the context information to make a better choice of translation rules (Carpuat and Wu, 2007; Chan et al., 2007; He et al., 2008; Cui et al., 2010). all the work employed rich context information, such as POS, syntactic, etc., and experiments were mostly done on less inflectional languages (i.e. Chinese, English) and resourceful languages (i.e. Arabic). (3) where di corresponds to a vector indicating affix distribution, and “·” denotes the inner product of the two vectors. Therefore, for a specific span to be translated, we first analyze it to get the corresponding stem sequence and related affix distribution represented as a vector. Then the stem sequence is used to search the trans"
P13-2065,C10-1084,0,0.015308,"1.3M 18K 10.8K 1.5M 15K 5.8K Table 1: Statistics of data sets. ∗N means the number of reference, morph is short to morpheme. UY, KA, KI, CH represent Uyghur, Kazakh, Kirghiz and Chinese respectively. dr is the final affix distribution. By comparing the similarity of affix distributions, we are able to decide whether a translation rule is suitable for a span to be translated. In this work, similarity is measured using the cosine distance similarity metric, given by sim(d1 , d2 ) = d1 · d2 ∥d1 ∥ × ∥d2 ∥ and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table. These works still assumed that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. There are also some work that employed the context information to make a better choice of translation rules (Carpuat and Wu, 2007; Chan et al., 2007; He et al., 2008; Cui et al., 2010). all the work employed rich context information, such as POS, syntactic, etc., and experiments were mostly done on less inflectional languages"
P13-2065,P07-1005,0,0.0194894,"e cosine distance similarity metric, given by sim(d1 , d2 ) = d1 · d2 ∥d1 ∥ × ∥d2 ∥ and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table. These works still assumed that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. There are also some work that employed the context information to make a better choice of translation rules (Carpuat and Wu, 2007; Chan et al., 2007; He et al., 2008; Cui et al., 2010). all the work employed rich context information, such as POS, syntactic, etc., and experiments were mostly done on less inflectional languages (i.e. Chinese, English) and resourceful languages (i.e. Arabic). (3) where di corresponds to a vector indicating affix distribution, and “·” denotes the inner product of the two vectors. Therefore, for a specific span to be translated, we first analyze it to get the corresponding stem sequence and related affix distribution represented as a vector. Then the stem sequence is used to search the translation rule table."
P13-2065,P05-1033,0,0.0635476,"lance between rule coverage and matching accuracy, and ultimately improve the translation performance. Currently, most methods on statistical machine translation (SMT) are developed for translation of languages with limited morphology (e.g., English, Chinese). They assumed that word was the atomic translation unit (ATU), always ignoring the internal morphological structure of word. This assumption can be traced back to the original IBM word-based models (Brown et al., 1993) and several significantly improved models, including phrase-based (Och and Ney, 2004; Koehn et al., 2003), hierarchical (Chiang, 2005) and syntactic (Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006) models. These improved models worked well for translating languages like English with large scale parallel corpora available. 364 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 364–369, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (A) Instances of translation rule zunyi /STM yighin /STM zunyi /STM i /SUF (1) Original:zunyi yighin+i+gha Meaning:of zunyi conference gha /SUF yighin /STM (2) zunyi /STM i /SUF Original:zunyi yighin+i+da Mea"
P13-2065,D10-1015,0,0.0145712,"5.8K Table 1: Statistics of data sets. ∗N means the number of reference, morph is short to morpheme. UY, KA, KI, CH represent Uyghur, Kazakh, Kirghiz and Chinese respectively. dr is the final affix distribution. By comparing the similarity of affix distributions, we are able to decide whether a translation rule is suitable for a span to be translated. In this work, similarity is measured using the cosine distance similarity metric, given by sim(d1 , d2 ) = d1 · d2 ∥d1 ∥ × ∥d2 ∥ and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table. These works still assumed that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. There are also some work that employed the context information to make a better choice of translation rules (Carpuat and Wu, 2007; Chan et al., 2007; He et al., 2008; Cui et al., 2010). all the work employed rich context information, such as POS, syntactic, etc., and experiments were mostly done on less inflectional languages (i.e. Chinese, English)"
P13-2065,P10-2002,0,0.0139781,"given by sim(d1 , d2 ) = d1 · d2 ∥d1 ∥ × ∥d2 ∥ and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table. These works still assumed that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. There are also some work that employed the context information to make a better choice of translation rules (Carpuat and Wu, 2007; Chan et al., 2007; He et al., 2008; Cui et al., 2010). all the work employed rich context information, such as POS, syntactic, etc., and experiments were mostly done on less inflectional languages (i.e. Chinese, English) and resourceful languages (i.e. Arabic). (3) where di corresponds to a vector indicating affix distribution, and “·” denotes the inner product of the two vectors. Therefore, for a specific span to be translated, we first analyze it to get the corresponding stem sequence and related affix distribution represented as a vector. Then the stem sequence is used to search the translation rule table. If the source part is matched, the s"
P13-2065,P02-1038,0,0.144949,"d “·” denotes the inner product of the two vectors. Therefore, for a specific span to be translated, we first analyze it to get the corresponding stem sequence and related affix distribution represented as a vector. Then the stem sequence is used to search the translation rule table. If the source part is matched, the similarity will be calculated for each candidate translation rule by cosine similarity (as in equation 3). Therefore, in addition to the traditional translation features on stem level, our model also adds the affix similarity score as a dynamic feature into the log-linear model (Och and Ney, 2002). 3 4 Experiments In this work, we conduct our experiments on three different agglutinative languages, including Uyghur, Kazakh and Kirghiz. All of them are derived from Altaic language family, belonging to Turkic languages, and mostly spoken by people in Central Asia. There are about 24 million people take these languages as mother tongue. All of the tasks are derived from the evaluation of China Workshop of Machine Translation (CWMT)2 . Table 1 shows the statistics of data sets. For the language model, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 5-gram model with the"
P13-2065,J04-4002,0,0.0940974,"etter rule selection. In this way, we can achieve a balance between rule coverage and matching accuracy, and ultimately improve the translation performance. Currently, most methods on statistical machine translation (SMT) are developed for translation of languages with limited morphology (e.g., English, Chinese). They assumed that word was the atomic translation unit (ATU), always ignoring the internal morphological structure of word. This assumption can be traced back to the original IBM word-based models (Brown et al., 1993) and several significantly improved models, including phrase-based (Och and Ney, 2004; Koehn et al., 2003), hierarchical (Chiang, 2005) and syntactic (Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006) models. These improved models worked well for translating languages like English with large scale parallel corpora available. 364 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 364–369, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (A) Instances of translation rule zunyi /STM yighin /STM zunyi /STM i /SUF (1) Original:zunyi yighin+i+gha Meaning:of zunyi conference gha /SUF yighin /STM (2"
P13-2065,P03-1021,0,0.0791843,"Missing"
P13-2065,P02-1040,0,0.0926047,"Missing"
P13-2065,P05-1034,0,0.0331406,"nd matching accuracy, and ultimately improve the translation performance. Currently, most methods on statistical machine translation (SMT) are developed for translation of languages with limited morphology (e.g., English, Chinese). They assumed that word was the atomic translation unit (ATU), always ignoring the internal morphological structure of word. This assumption can be traced back to the original IBM word-based models (Brown et al., 1993) and several significantly improved models, including phrase-based (Och and Ney, 2004; Koehn et al., 2003), hierarchical (Chiang, 2005) and syntactic (Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006) models. These improved models worked well for translating languages like English with large scale parallel corpora available. 364 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 364–369, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (A) Instances of translation rule zunyi /STM yighin /STM zunyi /STM i /SUF (1) Original:zunyi yighin+i+gha Meaning:of zunyi conference gha /SUF yighin /STM (2) zunyi /STM i /SUF Original:zunyi yighin+i+da Meaning:on zunyi conference yighin /S"
P13-2065,2007.mtsummit-papers.65,0,0.0435433,"Missing"
P13-2065,2011.mtsummit-papers.42,1,0.8554,"Missing"
P13-2065,E06-1006,0,0.025526,"possible inflected variants of lexicons for a single stem. Modeling each lexical form as a separate word will generate high out-of-vocabulary rate for SMT. Theoretically, ways like morphological analysis and increasing bilingual corpora could alleviate the problem of data sparsity, but most agglutinative languages are less-studied and suffer from the problem of resource-scarceness. Therefore, previous research mainly focused on the different inflected variants of the same stem and made various transformation of input by morphological analysis, such as (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006; Habash and Sadat, 2006; Bisazza and Federico, 2009; Wang et al., 2011). These work still assume that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. Current translation models are mainly designed for languages with limited morphology, which are not readily applicable to agglutinative languages as the difference in the way lexical forms are generated. In this paper, we propose a novel approach for translating agglutinative languages by treating stems and affixes differently. We employ stem as the atomic translation unit to a"
P13-2065,P10-1047,0,0.0156834,"he tasks are derived from the evaluation of China Workshop of Machine Translation (CWMT)2 . Table 1 shows the statistics of data sets. For the language model, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 5-gram model with the target side of training corpus. And phrase-based Moses3 is used as our Related Work Most previous work on agglutinative language translation mainly focus on Turkish and Finnish. Bisazza and Federico (2009) and Mermer and Saraclar (2011) optimized morphological analysis as a pre-processing step to improve the translation between Turkish and English. Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). Yang 2 3 366 http://mt.xmu.edu.cn/cwmt2011/en/index.html. http://www.statmt.org/moses/ word stem morph affix UY-CH 31.74+0.0 33.74+2.0 32.69+0.95 34.34+2.6 KA-CH 28.64+0.0 30.14+1.5 29.21+0.57 30.19+2.27 UY #Type stem #Token #Type affix #Token KI-CH 35.05+0.0 35.52+0.47 34.97−0.08 35.96+0.91 Table 2: Translation results from Turkic languages to Chinese. word: ATU is surface form, stem: ATU is represented stem, morph: ATU denotes morpheme, aﬃx: stem translation with affi"
P13-2065,N06-2013,0,\N,Missing
P13-2068,D11-1084,0,0.0638986,"utput, cohesion has served as a high level quality criterion in post-editing (Vasconcellos, 1989). As a part of COMTIS project, grammatical cohesion is integrated into machine translation models to capture inter-sentential links (Cartoni et al., 2011). Wong and Kit (2012) incorporate lexical cohesion to machine translation evaluation metrics to evaluate document-level machine translation quality. Xiong et al. (2013) integrate various target-side lexical cohesion devices into document-level machine translation. Lexical cohesion is also partially explored in the cachebased translation models of Gong et al. (2011) and translation consistency constraints of Xiao et al. Corresponding author 382 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 382–386, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics s(w). Near-synonym set s1 is defined as the union of all synsets that are defined by the function s(w) where w∈ s0 . It can be formulated as follows. [ s(w) (1) s1 = (2011). All previous methods on lexical cohesion for document-level machine translation as mentioned above have one thing in common, which is that they do not use any"
P13-2068,N03-1017,0,0.0466197,"Missing"
P13-2068,J03-1002,0,0.00769993,"Missing"
P13-2068,P03-1021,0,0.180049,"Missing"
P13-2068,P02-1040,0,0.0858031,"Missing"
P13-2068,D12-1097,0,0.471355,"ation. We integrate the model into hierarchical phrase-based machine translation and achieve an absolute improvement of 0.85 BLEU points on average over the baseline on NIST Chinese-English test sets. 1 Introduction Current statistical machine translation (SMT) systems are mostly sentence-based. The major drawback of such a sentence-based translation fashion is the neglect of inter-sentential dependencies. As a linguistic means to establish inter-sentential links, lexical cohesion ties sentences together into a meaningfully interwoven structure through words with the same or related meanings (Wong and Kit, 2012). This paper studies lexical cohesion devices and incorporate them into document-level machine translation. We propose a bilingual lexical cohesion trigger model to capture lexical cohesion for document-level SMT. We consider a lexical cohesion item in the source language and its corresponding counterpart in the target language as a trigger pair, in which we treat the source language lexical cohesion item as the trigger and its target language counterpart as the triggered item. Then we use mutual information to measure the strength of the dependency between the trigger and triggered item. We i"
P13-2068,2011.mtsummit-papers.13,0,0.354878,"Missing"
P13-2068,P11-1129,1,0.892762,"Missing"
P13-2068,J07-2003,0,\N,Missing
P13-2105,P05-1022,0,0.354533,"ne parser trained on CTB only. 2 Automatic Annotation Transformation In this section, we present an effective approach that transforms the source treebank to another compatible with the target annotation guideline, then describe an optimization strategy of iterative training that conducts several rounds of bidirectional annotation transformation and improves the transformation performance gradually from a global view. 2.2 Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram"
P13-2105,J05-1003,0,0.0308726,"y. 2 Automatic Annotation Transformation In this section, we present an effective approach that transforms the source treebank to another compatible with the target annotation guideline, then describe an optimization strategy of iterative training that conducts several rounds of bidirectional annotation transformation and improves the transformation performance gradually from a global view. 2.2 Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram constituent relation For"
P13-2105,P04-1015,0,0.0270835,"roves the transformation performance gradually from a global view. 2.2 Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram constituent relation For two consecutive fundamental constituents si and sj in the target parse tree, we find the minimum categories Ni and Nj of the spans of si and sj in the source parse tree respectively. Here 2.1 Principle for Annotation Transformation In training procedure, the source parser is used to parse the sentences in the target treebank s"
P13-2105,P99-1065,0,0.206222,"Missing"
P13-2105,W02-1001,0,0.224921,"Missing"
P13-2105,daum-etal-2004-automatic,0,0.0687766,"Missing"
P13-2105,D09-1087,0,0.0192773,"treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; McClosky et 3.3 Iterative Transformation We use the iterative training method for annotation transformation. The CTB developing set is used to determine the optimal training iteration. After each iteration, we test the performance of a parser trained on the combined treebank. Fig594 References al., 2006b; Huang and Harper, 2009). The selftraining methodology enlightens us on getting annotated treebank compatibal with another annotation guideline. Our approach places extra emphasis on improving the transformation performance with the help of source annotation knowledge. Apart from constituency-to-constituency treebank transformation, there also exists some research on dependency-to-constituency treebank transformation. Collins et al. (1999) used transformed constituency treebank from Prague Dependency Treebank for constituent parsing on Czech. Xia and Palmer (2001) explored different algorithms that transform dependen"
P13-2105,P09-1059,1,0.612564,"used for training. We also can find that our approach further extends the advantage over the two baseline systems as the amount of CTB training data decreases in Figure 2. The figure confirms our approach is effective for improving parser performance, specially for the scenario where the target treebank is scarce. 4 Related Work Treebank transformation is an effective strategy to reuse existing annotated data. Wang et al. (1994) proposed an approach to transform a treebank into another with a different grammar using their matching metric based on the bracket information of original treebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; Mc"
P13-2105,N06-1020,0,0.0380705,"ebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; McClosky et 3.3 Iterative Transformation We use the iterative training method for annotation transformation. The CTB developing set is used to determine the optimal training iteration. After each iteration, we test the performance of a parser trained on the combined treebank. Fig594 References al., 2006b; Huang and Harper, 2009). The selftraining methodology enlightens us on getting annotated treebank compatibal with another annotation guideline. Our approach places extra emphasis on improving the transformation performance with the help of source annotation knowledge. Apart from constituen"
P13-2105,P06-1043,0,0.024697,"ebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; McClosky et 3.3 Iterative Transformation We use the iterative training method for annotation transformation. The CTB developing set is used to determine the optimal training iteration. After each iteration, we test the performance of a parser trained on the combined treebank. Fig594 References al., 2006b; Huang and Harper, 2009). The selftraining methodology enlightens us on getting annotated treebank compatibal with another annotation guideline. Our approach places extra emphasis on improving the transformation performance with the help of source annotation knowledge. Apart from constituen"
P13-2105,P09-1006,0,0.599005,"Missing"
P13-2105,N07-1051,0,0.0610134,"Missing"
P13-2105,P06-1055,0,0.156433,"Missing"
P13-2105,D09-1086,0,0.0528911,"Missing"
P13-2105,P12-1025,0,0.0189739,"ases in Figure 2. The figure confirms our approach is effective for improving parser performance, specially for the scenario where the target treebank is scarce. 4 Related Work Treebank transformation is an effective strategy to reuse existing annotated data. Wang et al. (1994) proposed an approach to transform a treebank into another with a different grammar using their matching metric based on the bracket information of original treebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; McClosky et 3.3 Iterative Transformation We use the iterative training method for annotation transformation. The CTB developing set is used to determine the"
P13-2105,W10-4144,0,0.0716351,"om the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram constituent relation For two consecutive fundamental constituents si and sj in the target parse tree, we find the minimum categories Ni and Nj of the spans of si and sj in the source parse tree respectively. Here 2.1 Principle for Annotation Transformation In training procedure, the source parser is used to parse the sentences in the target treebank so that there are k-best parse trees with the source annotation guideline and one gold tree with the target annotation guideline for each sentenc"
P13-2105,P94-1034,0,0.383953,"Missing"
P13-2105,H01-1014,0,0.12377,"Missing"
P13-2105,P07-1106,0,0.0218642,"rom a global view. 2.2 Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram constituent relation For two consecutive fundamental constituents si and sj in the target parse tree, we find the minimum categories Ni and Nj of the spans of si and sj in the source parse tree respectively. Here 2.1 Principle for Annotation Transformation In training procedure, the source parser is used to parse the sentences in the target treebank so that there are k-best parse trees with the"
P13-2105,D10-1082,0,0.0157555,"Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram constituent relation For two consecutive fundamental constituents si and sj in the target parse tree, we find the minimum categories Ni and Nj of the spans of si and sj in the source parse tree respectively. Here 2.1 Principle for Annotation Transformation In training procedure, the source parser is used to parse the sentences in the target treebank so that there are k-best parse trees with the source annotation guidel"
P13-2105,P11-2126,0,0.272128,"raining data decreases in Figure 2. The figure confirms our approach is effective for improving parser performance, specially for the scenario where the target treebank is scarce. 4 Related Work Treebank transformation is an effective strategy to reuse existing annotated data. Wang et al. (1994) proposed an approach to transform a treebank into another with a different grammar using their matching metric based on the bracket information of original treebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; McClosky et 3.3 Iterative Transformation We use the iterative training method for annotation transformation. The CTB developing set is u"
P13-2105,levy-andrew-2006-tregex,0,\N,Missing
P13-2105,P12-1071,0,\N,Missing
P13-2105,D12-1038,1,\N,Missing
P13-2105,W07-2416,0,\N,Missing
P15-1003,P14-1062,0,0.207783,"Missing"
P15-1003,J07-2003,0,0.064903,"Missing"
P15-1003,D14-1179,0,0.0842528,"Missing"
P15-1003,P05-1066,0,0.106045,"Missing"
P15-1003,P14-1129,0,0.0614801,"Missing"
P15-1003,N04-1035,0,0.0736712,"Missing"
P15-1003,P07-1019,0,0.0144132,"Missing"
P15-1003,D13-1176,0,0.131209,"Missing"
P15-1003,N03-1017,0,0.00644014,"Missing"
P15-1003,P07-2045,0,0.0132544,"Missing"
P15-1003,D13-1108,1,0.848069,"Missing"
P15-1003,P02-1038,0,0.428481,"Missing"
P15-1003,J03-1002,0,0.00890853,"Missing"
P15-1003,P03-1021,0,0.072223,"Missing"
P15-1003,P08-1066,0,0.0292075,"Missing"
P15-1003,D11-1020,1,0.715388,"Missing"
P15-1003,D13-1106,0,\N,Missing
P15-1151,D13-1106,0,0.0780662,"Missing"
P15-1151,D11-1033,0,0.0341918,"Missing"
P15-1151,J92-1002,0,0.559939,"Missing"
P15-1151,P96-1041,0,0.351522,"Missing"
P15-1151,P14-1129,0,0.0225834,"Missing"
P15-1151,D13-1176,0,0.0950312,"Missing"
P15-1151,P14-1062,0,0.0830922,"Missing"
P15-1151,P02-1038,0,0.155749,"Missing"
P15-1151,D13-1140,0,0.0268192,"Missing"
P16-1010,J93-2003,0,0.0574552,"explicitly guide the selection of subgraphs. In experiments, this model further improves our system. In the future, we will extend this model to allow discontinuity on target sides and explore the possibility of directly encoding reordering information in translation rules. We are also interested in using graphs for neural machine translation to see how it can translate and benefit from graphs. Related Work Starting from sequence-based models, SMT has been benefiting increasingly from complex structures. Sequence-based MT: Since the breakthrough made by IBM on word-based models in the 1990s (Brown et al., 1993), SMT has developed rapidly. The PB model (Koehn et al., 2003) advanced the state-of-the-art by translating multi-word units, which makes it better able to capture local phenomena. However, a major drawback in PBMT is that only continuous phrases are considered. Galley and Manning (2010) extend PBMT by allowing discontinuity. However, without linguistic structure information such as syntax trees, sequence-based models can learn a large amount of phrases which may be unreliable. Tree-based MT: Compared to sequences, trees provide recursive structures over sentences and can handle long-distance"
P16-1010,W08-0336,0,0.0790836,"Missing"
P16-1010,N04-1035,0,0.419182,"duction Statistical machine translation (SMT) starts from sequence-based models. The well-known phrasebased (PB) translation model (Koehn et al., 2003) has significantly advanced the progress of SMT by extending translation units from single words to phrases. By using phrases, PB models can capture local phenomena, such as word order, word deletion, and word insertion. However, one of the significant weaknesses in conventional PB models is that only continuous phrases are used, so generalizations such as French ne . . . pas to English not cannot be learned. To solve this, syntax-based models (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Marcu et al., 2006) take tree structures into consideration to learn translation patterns by using non-terminals for generalization. • We propose to use a graph structure to combine a sequence and a tree (Section 3.1). The 97 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 97–107, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics graph contains both local relations between words from the sequence and long-distance relations from the tree. • • • • We present a translation model to"
P16-1010,W09-2307,0,0.0255999,"C in C out P out P in C in C out C in C out P in Figure 4: An illustration of extracting sparse features for each node in a subgraph during decoding. The decoder segments the graph in Figure 2 into three subgraphs (solid rectangles) and produces a complete translation by combining translations of each subgraph (dashed rectangles). In this figure, the class of a word is randomly assigned. 2004 (MT04) and NIST 2005 (MT05) are two test sets used to evaluate the systems. The Stanford Chinese word segmenter (Chang et al., 2008) is used to segment Chinese sentences. The Stanford dependency parser (Chang et al., 2009) parses a Chinese sentence into a projective dependency tree which is then converted to a graph by adding bigram relations. The DE–EN training corpus is from WMT 2014, including Europarl V7 and News Commentary. News-Test 2011 (WMT11) is taken as a development set while News-Test 2012 (WMT12) and News-Test 2013 (WMT13) are test sets. We use mate-tools2 to perform morphological analysis and parse German sentences (Bohnet, 2010). Then, MaltParser3 converts a parse result into a projective dependency tree (Nivre and Nilsson, 2005). 5.1 2007). Treelet extends PBMT by taking treelets as the basic tr"
P16-1010,W09-2301,0,0.815184,"add hG(˜ s), ti to R; 8 end 9 if |s˜ |< L then 10 for each unaligned word si adjacent to s˜ do 11 s˜0 = extend s˜ with si ; 12 add s˜0 to Q; 13 end 14 end 15 end 16 end successfully Chenggong South Africa Nanfei Figure 2: An example graph for a Chinese sentence. Each node includes a Chinese word and its English meaning. Dashed red lines are bigram relations. Solid lines are dependency relations. Dotted blue lines are shared by bigram and dependency relations. of continuous words. Phrases connected by bigram relations (i.e. continuous phrases) are known to be useful to improve phrase coverage (Hanneman and Lavie, 2009). By contrast, dependency relations come from dependency structures which model syntactic and semantic relations between words. Phrases whose words are connected by dependency relations (also known as treelets) are linguistic-motivated and thus more reliable (Quirk et al., 2005). By combining these two relations together in graphs, we can make use of both continuous and linguistic-informed discontinuous phrases as long as they are connected subgraphs. 3.2 (lines 1–2), and outputs hG(˜ s), ti if s˜ is covered by a connected subgraph G(˜ s) (lines 6–8). A source phrase can be extended with unali"
P16-1010,P96-1041,0,0.206429,"to right and uses beam search for decoding. DTU extends the PB model by allowing discontinuous phrases (Galley and Manning, 2010). We implement DTU with source discontinuity in Moses.4 GBMT is our basic graph-based translation system while GSM adds the graph segmentation model into GBMT. Both systems are implemented in Moses. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function growdiag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune weights. BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006) are used for evaluation. Settings In this paper, we mainly report results from five systems under the same configuration. PBMT is built by the PB model in Moses (Koehn et al., 2 3 4 The re-implementation of DTU in Moses makes it easier to meaningfully compare systems under the same settings. http://code.google.com/p/mate-tools/ http://www.maltparser.org/ 101 Metric BLEU ↑ METEOR ↑ TER ↓ System ZH–EN MT04 MT05 DE–EN WMT12 WMT13 PBMT Tree"
P16-1010,N12-1047,0,0.0597396,"coding. DTU extends the PB model by allowing discontinuous phrases (Galley and Manning, 2010). We implement DTU with source discontinuity in Moses.4 GBMT is our basic graph-based translation system while GSM adds the graph segmentation model into GBMT. Both systems are implemented in Moses. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function growdiag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune weights. BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006) are used for evaluation. Settings In this paper, we mainly report results from five systems under the same configuration. PBMT is built by the PB model in Moses (Koehn et al., 2 3 4 The re-implementation of DTU in Moses makes it easier to meaningfully compare systems under the same settings. http://code.google.com/p/mate-tools/ http://www.maltparser.org/ 101 Metric BLEU ↑ METEOR ↑ TER ↓ System ZH–EN MT04 MT05 DE–EN WMT12 WMT13 PBMT Treelet DTU GBMT GSM PBMT Treelet DTU GBMT"
P16-1010,C12-1083,0,0.0570256,"Missing"
P16-1010,N13-1003,0,0.0137791,"d by translating an uncovered subgraph instead of a phrase. Positions covered by the subgraph are then marked as translated. 4 ZH–EN Graph Segmentation Model where n.w and n.c are the word and class of the current node n, and n0 .w and n0 .c are the word and class of a node n0 connected to n. C, P , and H denote that the node n0 is in the current subgraph Gi or the adjacent previous subgraph Gi−1 or other previous subgraphs, respectively. Note that we treat the adjacent previous subgraph differently from others since information from the last previous unit is quite useful (Xiong et al., 2006; Cherry, 2013). in and out denote that the edge is an incoming edge or outgoing edge for the current node n. Figure 4 shows an example of extracting sparse features for a subgraph. Inspired by success in using sparse features in SMT (Cherry, 2013), in this paper we lexicalize only on the top-100 most frequent words. In addition, we group source words into 50 classes by using mkcls which should provide useful generalization (Cherry, 2013) for our model. Each derivation in our graph-based translation model implies a sequence of subgraphs (also called a segmentation). By default, similar to PB translation, our"
P16-1010,N03-1017,0,0.406976,"ich combine bigram and dependency relations and propose a graph-based translation model. The model segments an input graph into connected subgraphs, each of which may cover a discontinuous phrase. We use beam search to combine translations of each subgraph left-to-right to produce a complete translation. Experiments on Chinese–English and German– English tasks show that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores. By explicitly modeling the graph segmentation, our system obtains further improvement, especially on German–English. 1 Model C D S (Koehn et al., 2003) (Galley and Manning, 2010) (Quirk et al., 2005) and (Menezes and Quirk, 2005) This work • • • sequence sequence • tree • graph • Table 1: Comparison between our work and previous work in terms of three aspects: keeping continuous phrases (C), allowing discontinuous phrases (D), and input structures (S). However, the expressiveness of these models is confined by hierarchical constraints of the grammars used (Galley and Manning, 2010) since these patterns still cover continuous spans of an input sentence. By contrast, Quirk et al. (2005), Menezes and Quirk (2005) and Xiong et al. (2007) take tr"
P16-1010,P05-1033,0,0.593295,"achine translation (SMT) starts from sequence-based models. The well-known phrasebased (PB) translation model (Koehn et al., 2003) has significantly advanced the progress of SMT by extending translation units from single words to phrases. By using phrases, PB models can capture local phenomena, such as word order, word deletion, and word insertion. However, one of the significant weaknesses in conventional PB models is that only continuous phrases are used, so generalizations such as French ne . . . pas to English not cannot be learned. To solve this, syntax-based models (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Marcu et al., 2006) take tree structures into consideration to learn translation patterns by using non-terminals for generalization. • We propose to use a graph structure to combine a sequence and a tree (Section 3.1). The 97 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 97–107, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics graph contains both local relations between words from the sequence and long-distance relations from the tree. • • • • We present a translation model to translate a g"
P16-1010,2005.iwslt-1.8,0,0.0448647,"r to those models, our model is weak at phrase reordering as well. However, we are interesting in the potential power of our model by incorporating lexical reordering (LR) models and comparing it with syntax-based models. Table 5 shows BLEU scores of the hierarchical phrase-based (HPB) system (Chiang, 2005) in Moses6 and GBMT combined with a word-based System GBMT+LR HPB ZH–EN MT04 MT05 36.0 36.1 33.9 34.1 DE–EN WMT12 WMT13 20.6 20.3 23.6 22.8 Table 5: BLEU scores of a Moses hierarchical phrase-based system (HPB) and our system (GBMT) with a word-based lexical reordering model (LR). LR model (Koehn et al., 2005). We find that the LR model significantly improves our system. GBMT+LR is comparable with the Moses HPB model on Chinese–English and better than HPB on German–English. 5.3 Examples Figure 6 shows three examples from MT04 to better explain the differences of each system. Example 1 shows that systems which allow discontinuous phrases (namely Treelet, DTU, GBMT, and GSM) successfully translate a Chinese collocation “Yu . . . Wuguan” to “have nothing to do with” while PBMT fails to catch the generalization since it only allows continuous phrases. In Example 2, Treelet translates a discontinuous ph"
P16-1010,P11-2031,0,0.0430053,"–EN MT04 MT05 DE–EN WMT12 WMT13 PBMT Treelet DTU GBMT GSM PBMT Treelet DTU GBMT GSM PBMT Treelet DTU GBMT GSM 33.2 33.8∗ 34.5∗ 34.7∗ 34.9∗+ 32.1 31.9 32.3∗ 32.4∗+ 32.7∗+ 60.6 60.1∗ 60.0∗ 59.8∗+ 60.5 19.5 19.6 19.8∗ 19.8∗ 20.3∗+ 28.0 28.0 28.2∗ 28.2∗ 28.5∗+ 63.7 63.2∗ 63.5∗ 63.5∗ 63.1∗+ 31.8 31.7 32.3∗ 32.4∗ 32.7∗+ 32.3 31.8 32.4 32.5∗ 32.6∗+ 61.6 61.4 61.5 61.3∗ 62.1 21.9 22.1∗ 22.3∗ 22.4∗ 22.9∗+ 29.2 29.1 29.5∗ 29.4∗ 29.8∗+ 60.2 59.6∗ 59.8∗ 59.8∗ 59.3∗+ Table 3: Metric scores for all systems on Chinese–English (ZH–EN) and German–English (DE–EN). Each score is an average over three MIRA runs (Clark et al., 2011). ∗ means a system is significantly better than PBMT at p ≤ 0.01. Bold figures mean a system is significantly better than Treelet at p ≤ 0.01. + means a system is significantly better than DTU at p ≤ 0.01. In this table, we mark a system by comparing it with previous ones. 5.2 Results and Discussion Table 3 shows our evaluation results. We find that our GBMT system is significantly better than PBMT as measured by all three metrics across all test sets. Specifically, the improvements are up to +1.5/+0.5 BLEU, +0.3/+0.2 METEOR, and -0.8/0.4 TER on ZH–EN and DE–EN, respectively. This improvement"
P16-1010,P07-2045,0,0.0121936,"Missing"
P16-1010,J03-1002,0,0.0243166,"(Nivre and Nilsson, 2005). 5.1 2007). Treelet extends PBMT by taking treelets as the basic translation units (Quirk et al., 2005; Menezes and Quirk, 2005). We implement a Treelet model in Moses which produces translations from left to right and uses beam search for decoding. DTU extends the PB model by allowing discontinuous phrases (Galley and Manning, 2010). We implement DTU with source discontinuity in Moses.4 GBMT is our basic graph-based translation system while GSM adds the graph segmentation model into GBMT. Both systems are implemented in Moses. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function growdiag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune weights. BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006) are used for evaluation. Settings In this paper, we mainly report results from five systems under the same configuration. PBMT is built by the PB model in Moses (Koehn et al., 2 3 4 The re-implementation"
P16-1010,J10-4005,0,0.0182637,"parse features to explicitly model the graph segmentation (Section 4). These features are based on edges in the input graph, each of which is either inside a subgraph or connects the subgraph with a previous subgraph. 2 •• I p(t1 |G(˜ sI1 )) I Y = p(ti |G(˜ sai ))d(G(˜ sai ), G(˜ sai−1 )) (1) i=1 The target sentence T is broken into I phrases t1 · · · tI , each of which is a translation of a source phrase sai . d is a distance-based reordering model. Note that in the basic PB model, the phrase segmentation is not explicitly modeled which means that different segmentations are treated equally (Koehn, 2010). The performance of PB translation relies on the quality of phrase pairs in a translation table. Conventionally, a phrase pair hs, ti has two properties: (i) s and t are continuous phrases. (ii) hs, ti is consistent with a word alignment A (Och and Ney, 2004): ∀(i, j) ∈ A, si ∈ s ⇔ tj ∈ t and ∃si ∈ s, tj ∈ t, (i, j) ∈ A. PB decoders generate hypotheses (partial translations) from left to right. Each hypothesis maintains a coverage vector to indicate which source words have been translated so far. A hypothesis can be extended on the right by translating an ≈ i=1 I Y (2) sai ))d(˜ sai , s˜ai−1"
P16-1010,J04-4002,0,0.324209,"G(˜ sai ))d(G(˜ sai ), G(˜ sai−1 )) (1) i=1 The target sentence T is broken into I phrases t1 · · · tI , each of which is a translation of a source phrase sai . d is a distance-based reordering model. Note that in the basic PB model, the phrase segmentation is not explicitly modeled which means that different segmentations are treated equally (Koehn, 2010). The performance of PB translation relies on the quality of phrase pairs in a translation table. Conventionally, a phrase pair hs, ti has two properties: (i) s and t are continuous phrases. (ii) hs, ti is consistent with a word alignment A (Och and Ney, 2004): ∀(i, j) ∈ A, si ∈ s ⇔ tj ∈ t and ∃si ∈ s, tj ∈ t, (i, j) ∈ A. PB decoders generate hypotheses (partial translations) from left to right. Each hypothesis maintains a coverage vector to indicate which source words have been translated so far. A hypothesis can be extended on the right by translating an ≈ i=1 I Y (2) sai ))d(˜ sai , s˜ai−1 ) p(ti |G(˜ i=1 where G(˜ si ) denotes a connected source subgraph which covers a (discontinuous) phrase s˜i . 3.1 Building Graphs As a more powerful and natural structure for sentence modeling, a graph can model various kinds of word-relations together in a u"
P16-1010,W14-4014,1,0.764896,"hat only continuous phrases are considered. Galley and Manning (2010) extend PBMT by allowing discontinuity. However, without linguistic structure information such as syntax trees, sequence-based models can learn a large amount of phrases which may be unreliable. Tree-based MT: Compared to sequences, trees provide recursive structures over sentences and can handle long-distance relations. Typically, trees used in SMT are either phrasal structures (Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006) or dependency structures (Menezes and Quirk, 2005; Xiong et al., 2007; Xie et al., 2011; Li et al., 2014). However, conventional treebased models only use linguistically well-formed phrases. Although they are more reliable in theory, discarding all phrase pairs which are not linguistically motivated is an overly harsh decision. Therefore, exploring more translation rules usually can significantly improve translation performance (Marcu et al., 2006; DeNeefe et al., 2007; Wang et al., 2007; Mi et al., 2008). Graph-based MT: Compared to sequences and trees, graphs are more general and can represent more relations between words. In recent years, Acknowledgments This research has received funding from"
P16-1010,P02-1040,0,0.106811,"us phrases (Galley and Manning, 2010). We implement DTU with source discontinuity in Moses.4 GBMT is our basic graph-based translation system while GSM adds the graph segmentation model into GBMT. Both systems are implemented in Moses. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function growdiag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune weights. BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006) are used for evaluation. Settings In this paper, we mainly report results from five systems under the same configuration. PBMT is built by the PB model in Moses (Koehn et al., 2 3 4 The re-implementation of DTU in Moses makes it easier to meaningfully compare systems under the same settings. http://code.google.com/p/mate-tools/ http://www.maltparser.org/ 101 Metric BLEU ↑ METEOR ↑ TER ↓ System ZH–EN MT04 MT05 DE–EN WMT12 WMT13 PBMT Treelet DTU GBMT GSM PBMT Treelet DTU GBMT GSM PBMT Treelet DTU GBMT GSM 33.2 33.8∗ 34.5∗ 34.7∗"
P16-1010,D15-1004,1,0.652183,"tors of the community Guangyi . and draw on collective wisdom . Figure 6: Translation examples from MT04 produced by different systems. Each source sentence is annotated by dependency relations and additional bigram relations (dotted red edges). We also annotate phrase alignments produced by our system GSM. 104 graphs have been drawing quite a lot of attention from researchers. Jones et al. (2012) propose a hypergraph-based translation model where hypergraphs are taken as a meaning representation of sentences. However, large corpora with annotated hypergraphs are not readily available for MT. Li et al. (2015) use an edge replacement grammar to translate dependency graphs which are converted from dependency trees by labeling edges. However, their model only focuses on subgraphs which cover continuous phrases. linguistically motivated and could be unreliable. By disallowing phrases which are not connected in the input graph, GBMT and GSM produce better translations. Example 3 illustrates that our graph segmentation model helps to select better subgraphs. After obtaining a partial translation “the government must”, GSM chooses to translate a subgraph which covers a discontinuous phrase “Jixu . . . Zu"
P16-1010,P05-1034,0,0.874736,"propose a graph-based translation model. The model segments an input graph into connected subgraphs, each of which may cover a discontinuous phrase. We use beam search to combine translations of each subgraph left-to-right to produce a complete translation. Experiments on Chinese–English and German– English tasks show that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores. By explicitly modeling the graph segmentation, our system obtains further improvement, especially on German–English. 1 Model C D S (Koehn et al., 2003) (Galley and Manning, 2010) (Quirk et al., 2005) and (Menezes and Quirk, 2005) This work • • • sequence sequence • tree • graph • Table 1: Comparison between our work and previous work in terms of three aspects: keeping continuous phrases (C), allowing discontinuous phrases (D), and input structures (S). However, the expressiveness of these models is confined by hierarchical constraints of the grammars used (Galley and Manning, 2010) since these patterns still cover continuous spans of an input sentence. By contrast, Quirk et al. (2005), Menezes and Quirk (2005) and Xiong et al. (2007) take treelets from dependency trees as the basic transl"
P16-1010,D14-1209,0,0.013343,"search strategy to reduce the size of the decoding space. Hypotheses which cover the same number of source words are grouped in a stack. Hypotheses can be pruned according to their partial translation cost and an estimated future cost. • Experiments (Section 5) on Chinese–English and German–English tasks show that our model is significantly better than the PB model. After incorporating the segmentation model, our system achieves still further improvement. p(t1 |sI1 ) = •• ••• Figure 1: Beam search for phrase-based MT. • denotes a covered source position while indicates an uncovered position (Liu and Huang, 2014). • We present a set of sparse features to explicitly model the graph segmentation (Section 4). These features are based on edges in the input graph, each of which is either inside a subgraph or connects the subgraph with a previous subgraph. 2 •• I p(t1 |G(˜ sI1 )) I Y = p(ti |G(˜ sai ))d(G(˜ sai ), G(˜ sai−1 )) (1) i=1 The target sentence T is broken into I phrases t1 · · · tI , each of which is a translation of a source phrase sai . d is a distance-based reordering model. Note that in the basic PB model, the phrase segmentation is not explicitly modeled which means that different segmentati"
P16-1010,2006.amta-papers.25,0,0.0271408,"iscontinuity in Moses.4 GBMT is our basic graph-based translation system while GSM adds the graph segmentation model into GBMT. Both systems are implemented in Moses. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function growdiag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune weights. BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006) are used for evaluation. Settings In this paper, we mainly report results from five systems under the same configuration. PBMT is built by the PB model in Moses (Koehn et al., 2 3 4 The re-implementation of DTU in Moses makes it easier to meaningfully compare systems under the same settings. http://code.google.com/p/mate-tools/ http://www.maltparser.org/ 101 Metric BLEU ↑ METEOR ↑ TER ↓ System ZH–EN MT04 MT05 DE–EN WMT12 WMT13 PBMT Treelet DTU GBMT GSM PBMT Treelet DTU GBMT GSM PBMT Treelet DTU GBMT GSM 33.2 33.8∗ 34.5∗ 34.7∗ 34.9∗+ 32.1 31.9 32.3∗ 32.4∗+ 32.7∗+ 60.6 60.1∗ 60.0∗ 59.8∗+ 60.5 1"
P16-1010,P06-1077,1,0.875045,"tion (SMT) starts from sequence-based models. The well-known phrasebased (PB) translation model (Koehn et al., 2003) has significantly advanced the progress of SMT by extending translation units from single words to phrases. By using phrases, PB models can capture local phenomena, such as word order, word deletion, and word insertion. However, one of the significant weaknesses in conventional PB models is that only continuous phrases are used, so generalizations such as French ne . . . pas to English not cannot be learned. To solve this, syntax-based models (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Marcu et al., 2006) take tree structures into consideration to learn translation patterns by using non-terminals for generalization. • We propose to use a graph structure to combine a sequence and a tree (Section 3.1). The 97 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 97–107, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics graph contains both local relations between words from the sequence and long-distance relations from the tree. • • • • We present a translation model to translate a graph (Section 3)."
P16-1010,W06-1606,0,0.201369,"from sequence-based models. The well-known phrasebased (PB) translation model (Koehn et al., 2003) has significantly advanced the progress of SMT by extending translation units from single words to phrases. By using phrases, PB models can capture local phenomena, such as word order, word deletion, and word insertion. However, one of the significant weaknesses in conventional PB models is that only continuous phrases are used, so generalizations such as French ne . . . pas to English not cannot be learned. To solve this, syntax-based models (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Marcu et al., 2006) take tree structures into consideration to learn translation patterns by using non-terminals for generalization. • We propose to use a graph structure to combine a sequence and a tree (Section 3.1). The 97 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 97–107, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics graph contains both local relations between words from the sequence and long-distance relations from the tree. • • • • We present a translation model to translate a graph (Section 3). The model segments th"
P16-1010,D07-1078,0,0.0867248,"Missing"
P16-1010,2005.mtsummit-ebmt.13,0,0.681184,"nslation model. The model segments an input graph into connected subgraphs, each of which may cover a discontinuous phrase. We use beam search to combine translations of each subgraph left-to-right to produce a complete translation. Experiments on Chinese–English and German– English tasks show that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores. By explicitly modeling the graph segmentation, our system obtains further improvement, especially on German–English. 1 Model C D S (Koehn et al., 2003) (Galley and Manning, 2010) (Quirk et al., 2005) and (Menezes and Quirk, 2005) This work • • • sequence sequence • tree • graph • Table 1: Comparison between our work and previous work in terms of three aspects: keeping continuous phrases (C), allowing discontinuous phrases (D), and input structures (S). However, the expressiveness of these models is confined by hierarchical constraints of the grammars used (Galley and Manning, 2010) since these patterns still cover continuous spans of an input sentence. By contrast, Quirk et al. (2005), Menezes and Quirk (2005) and Xiong et al. (2007) take treelets from dependency trees as the basic translation units. These treelets ar"
P16-1010,D11-1020,1,0.811514,"wback in PBMT is that only continuous phrases are considered. Galley and Manning (2010) extend PBMT by allowing discontinuity. However, without linguistic structure information such as syntax trees, sequence-based models can learn a large amount of phrases which may be unreliable. Tree-based MT: Compared to sequences, trees provide recursive structures over sentences and can handle long-distance relations. Typically, trees used in SMT are either phrasal structures (Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006) or dependency structures (Menezes and Quirk, 2005; Xiong et al., 2007; Xie et al., 2011; Li et al., 2014). However, conventional treebased models only use linguistically well-formed phrases. Although they are more reliable in theory, discarding all phrase pairs which are not linguistically motivated is an overly harsh decision. Therefore, exploring more translation rules usually can significantly improve translation performance (Marcu et al., 2006; DeNeefe et al., 2007; Wang et al., 2007; Mi et al., 2008). Graph-based MT: Compared to sequences and trees, graphs are more general and can represent more relations between words. In recent years, Acknowledgments This research has rec"
P16-1010,P08-1023,1,0.780891,"ees used in SMT are either phrasal structures (Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006) or dependency structures (Menezes and Quirk, 2005; Xiong et al., 2007; Xie et al., 2011; Li et al., 2014). However, conventional treebased models only use linguistically well-formed phrases. Although they are more reliable in theory, discarding all phrase pairs which are not linguistically motivated is an overly harsh decision. Therefore, exploring more translation rules usually can significantly improve translation performance (Marcu et al., 2006; DeNeefe et al., 2007; Wang et al., 2007; Mi et al., 2008). Graph-based MT: Compared to sequences and trees, graphs are more general and can represent more relations between words. In recent years, Acknowledgments This research has received funding from the People Programme (Marie Curie Actions) of the European Union’s Framework Programme (FP7/20072013) under REA grant agreement no 317471 and the European Union’s Horizon 2020 research and innovation programme under grant agreement no 645452 (QT21). The ADAPT Centre for Digital Content Technology is funded under the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European"
P16-1010,P06-1066,1,0.759455,"ypothesis is extended by translating an uncovered subgraph instead of a phrase. Positions covered by the subgraph are then marked as translated. 4 ZH–EN Graph Segmentation Model where n.w and n.c are the word and class of the current node n, and n0 .w and n0 .c are the word and class of a node n0 connected to n. C, P , and H denote that the node n0 is in the current subgraph Gi or the adjacent previous subgraph Gi−1 or other previous subgraphs, respectively. Note that we treat the adjacent previous subgraph differently from others since information from the last previous unit is quite useful (Xiong et al., 2006; Cherry, 2013). in and out denote that the edge is an incoming edge or outgoing edge for the current node n. Figure 4 shows an example of extracting sparse features for a subgraph. Inspired by success in using sparse features in SMT (Cherry, 2013), in this paper we lexicalize only on the top-100 most frequent words. In addition, we group source words into 50 classes by using mkcls which should provide useful generalization (Cherry, 2013) for our model. Each derivation in our graph-based translation model implies a sequence of subgraphs (also called a segmentation). By default, similar to PB t"
P16-1010,P05-1013,0,0.0618967,"8) is used to segment Chinese sentences. The Stanford dependency parser (Chang et al., 2009) parses a Chinese sentence into a projective dependency tree which is then converted to a graph by adding bigram relations. The DE–EN training corpus is from WMT 2014, including Europarl V7 and News Commentary. News-Test 2011 (WMT11) is taken as a development set while News-Test 2012 (WMT12) and News-Test 2013 (WMT13) are test sets. We use mate-tools2 to perform morphological analysis and parse German sentences (Bohnet, 2010). Then, MaltParser3 converts a parse result into a projective dependency tree (Nivre and Nilsson, 2005). 5.1 2007). Treelet extends PBMT by taking treelets as the basic translation units (Quirk et al., 2005; Menezes and Quirk, 2005). We implement a Treelet model in Moses which produces translations from left to right and uses beam search for decoding. DTU extends the PB model by allowing discontinuous phrases (Galley and Manning, 2010). We implement DTU with source discontinuity in Moses.4 GBMT is our basic graph-based translation system while GSM adds the graph segmentation model into GBMT. Both systems are implemented in Moses. Word alignment is performed by GIZA++ (Och and Ney, 2003) with th"
P16-1010,P02-1038,0,0.0689957,"A source phrase can be extended with unaligned source words which are adjacent to the phrase (lines 9– 14). We use a queue Q to store all phrases which are consistently aligned to the same target phrase (line 3). Training Different from PB translation, the basic translation units in our model are subgraphs. Thus, during training, we extract subgraph–phrase pairs instead of phrase pairs on parallel graph–string sentences associated with word alignments.1 An example of a translation rule is as follows: FIFA Shijiebei Juxing 3.3 Model and Decoding We define our model in the log-linear framework (Och and Ney, 2002) over a derivation D = r1 r2 · · · rN , as in Equation (3): Y p(D) ∝ φi (D)λi (3) FIFA World Cup was held i Note that the source side of a rule in our model is a graph which can be used to cover either a continuous phrase or a discontinuous phrase according to its match in an input graph during decoding. The algorithm for extracting translation rules is shown in Algorithm 1. This algorithm traverses each phrase pair h˜ s, ti, which is within a length limit and consistent with a given word alignment where ri are translation rules, φi are features defined on derivations and λi are feature weight"
P16-1010,W07-0706,1,0.855471,"C D S (Koehn et al., 2003) (Galley and Manning, 2010) (Quirk et al., 2005) and (Menezes and Quirk, 2005) This work • • • sequence sequence • tree • graph • Table 1: Comparison between our work and previous work in terms of three aspects: keeping continuous phrases (C), allowing discontinuous phrases (D), and input structures (S). However, the expressiveness of these models is confined by hierarchical constraints of the grammars used (Galley and Manning, 2010) since these patterns still cover continuous spans of an input sentence. By contrast, Quirk et al. (2005), Menezes and Quirk (2005) and Xiong et al. (2007) take treelets from dependency trees as the basic translation units. These treelets are connected and may cover discontinuous phrases. However, their models lack the ability to handle continuous phrases which are not connected in trees but could in fact be extremely important to system performance (Koehn et al., 2003). Galley and Manning (2010) directly extract discontinuous phrases from input sequences. However, without imposing additional restrictions on discontinuity, the amount of extracted rules can be very large and unreliable. Different from previous work (as shown in Table 1), in this"
P16-1010,N10-1140,0,\N,Missing
P16-1010,W11-2107,0,\N,Missing
P16-2045,N12-1047,0,0.0457588,"Missing"
P16-2045,J03-1002,0,0.00586571,"pair applied to translate an input phrase, WANG finds its corresponding phrase pairs in a TM instance and then extracts features which are directly added to the loglinear framework (Och and Ney, 2002) as sparse features. We build three systems based on our approach: CWLadd only uses constraints from addition; CWLsub only uses constraints from subtraction; CWLboth uses constraints from both. Table 1 shows a summary of our datasets. The EN–ZH dataset is a translation memory from Symantec. Our EN–FR dataset is from the publicly available JRC-Acquis corpus.4 Word alignment is performed by GIZA++ (Och and Ney, 2003) with heuristic function grow-diag-final-and. After obtaining the segmentation, we create a constrained input for each f i using subtraction and add it to the lattice by creating a path covering f i . The path contains one or more edges, each of which is labeled either by an input word or a constraint in the constrained input. 2.3 Sentences Decoding The decoder for integrating word lattices into the phrase-based model (Koehn et al., 2003) works similarly to the phrase-based decoder, except that it tracks nodes instead of words (Dyer et al., 2008): given the topological order of nodes in a latt"
P16-2045,P08-1115,0,0.0279922,"corpus.4 Word alignment is performed by GIZA++ (Och and Ney, 2003) with heuristic function grow-diag-final-and. After obtaining the segmentation, we create a constrained input for each f i using subtraction and add it to the lattice by creating a path covering f i . The path contains one or more edges, each of which is labeled either by an input word or a constraint in the constrained input. 2.3 Sentences Decoding The decoder for integrating word lattices into the phrase-based model (Koehn et al., 2003) works similarly to the phrase-based decoder, except that it tracks nodes instead of words (Dyer et al., 2008): given the topological order of nodes in a lattice, the decoder builds a translation hypothesis from left to right by selecting a range of untranslated nodes. The decoder for a constrained lattice works similarly except that, for a constrained edge, the decoder can only build its translation directly from the constraint. For example, in Figure 2, the translation of the edge “1 → 5” is “, le texte du deuxi`eme alin´ea”. 4 http://ipsc.jrc.ec.europa.eu/index. php?id=198 277 EN–ZH BLEU↑ TER↓ Systems PB EN–FR BLEU↑ TER↓ 44.3 40.0 65.7 Sentence-Level Combination 25.9 45.6* 39.2* 64.2 49.4* 36.3* 64"
P16-2045,J04-4002,0,0.0292856,"ntences are taken from Koehn and Senellart (2010). 1. Building an initial lattice for an input sentence. This produces a chain. 2.2 Subtraction In addition, matched input words are directly replaced by their translations from a retrieved TM, which means that addition follows the word order of an input sentence. This property makes it easy to obtain constraints for an input phrase. For an input phrase f , we firstly find its matched phrase f 0 from f 0 via string edits3 between f and f 0 , so that f = f 0 . Then, we extract its translation e0 from e0 , which is consistent with the alignment A (Och and Ney, 2004). To build a lattice using addition, we directly add a new edge to the lattice which covers f and is labeled by e0 . For example, dash-dotted lines in Figure 2 are labeled by constraints from addition. In subtraction, mismatched input words in f are inserted into e0 and mismatched words in e0 are removed. The inserted position is determined by A. The advantage of subtraction is that it keeps the word order of e0 . This is important since the reordering of target words is one of the fundamental problems in SMT, especially for language pairs which have a high degree of syntactic reordering. Howe"
P16-2045,P10-1064,1,0.868718,"Missing"
P16-2045,P02-1040,0,0.0950648,"Missing"
P16-2045,2010.jec-1.4,0,0.405533,"g a constrained word lattice, which encodes input phrases and TM constraints together, to combine SMT and TM at phrase-level. Experiments on English– Chinese and English–French show that our approach is significantly better than previous combination methods, including sentence-level constrained translation and a recent phrase-level combination. 1 Introduction The combination of statistical machine translation (SMT) and translation memory (TM) has proven to be beneficial in improving translation quality and has drawn attention from many researchers (Bic¸ici and Dymetman, 2008; He et al., 2010; Koehn and Senellart, 2010; Ma et al., 2011; Wang et al., 2013; Li et al., 2014). Among various combination approaches, constrained translation (Koehn and Senellart, 2010; Ma et al., 2011) is a simple one and can be readily adopted. Given an input sentence, constrained translation retrieves similar TM instances and uses matched segments to constrain the translation space of the input by generating a constrained input. Then an SMT engine is used to search for a complete translation of the constrained input. Despite its effectiveness in improving SMT, previous constrained translation works at the sentence-level, which me"
P16-2045,2006.amta-papers.25,0,0.0794351,"Missing"
P16-2045,N03-1017,0,0.0356054,"–ZH dataset is a translation memory from Symantec. Our EN–FR dataset is from the publicly available JRC-Acquis corpus.4 Word alignment is performed by GIZA++ (Och and Ney, 2003) with heuristic function grow-diag-final-and. After obtaining the segmentation, we create a constrained input for each f i using subtraction and add it to the lattice by creating a path covering f i . The path contains one or more edges, each of which is labeled either by an input word or a constraint in the constrained input. 2.3 Sentences Decoding The decoder for integrating word lattices into the phrase-based model (Koehn et al., 2003) works similarly to the phrase-based decoder, except that it tracks nodes instead of words (Dyer et al., 2008): given the topological order of nodes in a lattice, the decoder builds a translation hypothesis from left to right by selecting a range of untranslated nodes. The decoder for a constrained lattice works similarly except that, for a constrained edge, the decoder can only build its translation directly from the constraint. For example, in Figure 2, the translation of the edge “1 → 5” is “, le texte du deuxi`eme alin´ea”. 4 http://ipsc.jrc.ec.europa.eu/index. php?id=198 277 EN–ZH BLEU↑ T"
P16-2045,P07-2045,0,0.0129872,"ted position of a mismatched unaligned word depends on the alignment of the word before it. Train Dev Test Train Dev Test EN–FR W/S (EN) W/S (ZH) 84,871 734 943 13.5 14.3 17.4 13.8 14.5 17.4 Sentences W/S (EN) W/S (FR) 751,548 2,665 2,655 26.9 26.8 27.1 29.3 29.2 29.4 Table 1: Summary of English–Chinese (EN–ZH) and English–French (EN–FR) datasets 3 4. No smaller tuples may be extracted without violating restrictions 1–3. This allows us to obtain a unique segmentation where each tuple is minimal. Experiment In our experiments, a baseline system PB is built with the phrase-based model in Moses (Koehn et al., 2007). We compare our approach with three other combination methods. ADD combines PB with addition (Ma et al., 2011), while SUB combines PB with subtraction (Koehn and Senellart, 2010). WANG combines SMT and TM at phraselevel during decoding (Wang et al., 2013; Li et al., 2014). For each phrase pair applied to translate an input phrase, WANG finds its corresponding phrase pairs in a TM instance and then extracts features which are directly added to the loglinear framework (Och and Ney, 2002) as sparse features. We build three systems based on our approach: CWLadd only uses constraints from addition"
P16-2045,P13-1002,0,0.0680504,"input phrases and TM constraints together, to combine SMT and TM at phrase-level. Experiments on English– Chinese and English–French show that our approach is significantly better than previous combination methods, including sentence-level constrained translation and a recent phrase-level combination. 1 Introduction The combination of statistical machine translation (SMT) and translation memory (TM) has proven to be beneficial in improving translation quality and has drawn attention from many researchers (Bic¸ici and Dymetman, 2008; He et al., 2010; Koehn and Senellart, 2010; Ma et al., 2011; Wang et al., 2013; Li et al., 2014). Among various combination approaches, constrained translation (Koehn and Senellart, 2010; Ma et al., 2011) is a simple one and can be readily adopted. Given an input sentence, constrained translation retrieves similar TM instances and uses matched segments to constrain the translation space of the input by generating a constrained input. Then an SMT engine is used to search for a complete translation of the constrained input. Despite its effectiveness in improving SMT, previous constrained translation works at the sentence-level, which means that matched segments in a TM in"
P16-2045,2014.amta-researchers.19,1,0.627439,"M constraints together, to combine SMT and TM at phrase-level. Experiments on English– Chinese and English–French show that our approach is significantly better than previous combination methods, including sentence-level constrained translation and a recent phrase-level combination. 1 Introduction The combination of statistical machine translation (SMT) and translation memory (TM) has proven to be beneficial in improving translation quality and has drawn attention from many researchers (Bic¸ici and Dymetman, 2008; He et al., 2010; Koehn and Senellart, 2010; Ma et al., 2011; Wang et al., 2013; Li et al., 2014). Among various combination approaches, constrained translation (Koehn and Senellart, 2010; Ma et al., 2011) is a simple one and can be readily adopted. Given an input sentence, constrained translation retrieves similar TM instances and uses matched segments to constrain the translation space of the input by generating a constrained input. Then an SMT engine is used to search for a complete translation of the constrained input. Despite its effectiveness in improving SMT, previous constrained translation works at the sentence-level, which means that matched segments in a TM instance are either"
P16-2045,P11-1124,1,0.820404,"Missing"
P16-2045,P02-1038,0,0.264534,"minimal. Experiment In our experiments, a baseline system PB is built with the phrase-based model in Moses (Koehn et al., 2007). We compare our approach with three other combination methods. ADD combines PB with addition (Ma et al., 2011), while SUB combines PB with subtraction (Koehn and Senellart, 2010). WANG combines SMT and TM at phraselevel during decoding (Wang et al., 2013; Li et al., 2014). For each phrase pair applied to translate an input phrase, WANG finds its corresponding phrase pairs in a TM instance and then extracts features which are directly added to the loglinear framework (Och and Ney, 2002) as sparse features. We build three systems based on our approach: CWLadd only uses constraints from addition; CWLsub only uses constraints from subtraction; CWLboth uses constraints from both. Table 1 shows a summary of our datasets. The EN–ZH dataset is a translation memory from Symantec. Our EN–FR dataset is from the publicly available JRC-Acquis corpus.4 Word alignment is performed by GIZA++ (Och and Ney, 2003) with heuristic function grow-diag-final-and. After obtaining the segmentation, we create a constrained input for each f i using subtraction and add it to the lattice by creating a p"
P17-1013,buck-etal-2014-n,0,0.111414,"Missing"
P17-1013,P05-1033,0,0.0333576,"d the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014). Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivastava et al., 201"
P17-1013,D14-1179,0,0.0307604,"Missing"
P17-1013,P15-1001,0,0.26178,"study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014"
P17-1013,N03-1017,0,0.0167194,"EU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014). Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivasta"
P17-1013,P06-1077,1,0.570823,"orted results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014). Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivastava et al., 2015), research in NM"
P17-1013,D15-1166,0,0.699847,"through both space and time direction. The model is quite simple, but it is surprisingly effective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed repres"
P17-1013,P02-1040,0,0.0977629,"arget word embedding at time step t, ct is dynamically obtained follows Equation (10). There are Ldec layers of RNNs armed with LAUs in the decoder. At inference stage, we only utilize the top-most hidden states sLdec to make the final prediction with a softmax layer: p(yi |y&lt;i , x) = softmax(Wo siLdec ) (12) . 4 Experiments 4.1 Setup We mainly evaluated our approaches on the widely used NIST Chinese-English translation task. In order to show the usefulness of our approaches, we also provide results on other two translation tasks: English-French, EnglishGerman. The evaluation metric is BLEU2 (Papineni et al., 2002). For Chinese-English, our training data consists of 1.25M sentence pairs extracted from 1 github.com/nyu-dl/dl4mt-tutorial/ tree/master/session2 2 For Chinese-English task, we apply case-insensitive NIST BLEU. For other tasks, we tokenized the reference and evaluated the performance with multi-bleu.pl. The metrics are exactly the same as in previous work. LDC corpora3 , with 27.9M Chinese words and 34.5M English words respectively. We choose NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) and 2006 (MT06) datasets as our test sets. For English"
P17-1013,D16-1050,0,0.0425113,"Missing"
P17-1013,D16-1160,0,0.0389859,"ective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sut"
P17-1013,Q16-1027,1,0.434023,"nts, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014). Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivastava et al., 2015), research in NMT has recently turned towards studying Deep Neural Networks (DNNs). Wu et al. (2016) and Zhou et al. (2016) found that deep architectures in both the encoder and decoder are essential for capturing subtle irregularities in the source and target languages. However, training a deep neural network is not as simple as stacking layers. Optimization often becomes increasingly difficult with more layers. One reasonable explanation is the notorious problem of vanishing/exploding gradients which was first studied in the context of vanilla RNNs (Pascanu et al., 2013b). Most prevalent approaches to solve this problem rely on short-cut connections between adjacent layers such as residual or fastforward connect"
P17-1013,P16-1159,0,0.0575075,"Missing"
P17-1013,P16-1008,1,0.825673,"surprisingly effective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the inp"
P17-1013,P06-1066,1,0.72775,"he same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014). Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivastava et al., 2015), research in NMT has recently turne"
P17-1013,P08-1023,1,\N,Missing
P17-1013,P16-5005,0,\N,Missing
P17-1140,P05-1033,0,0.0285008,"mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve signiﬁcant translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src re"
P17-1140,P06-1067,0,0.478247,"are usually inferior in comparison with the phrase-based SMT, but our model surpasses phrase-based Moses by average 4.43 BLEU points and outperforms the attention-based NMT baseline system by 5.09 BLEU points. Figure 1: The source word “yiju” does not obtain appropriate attention and its word sense is completely neglected. To enhance the attention mechanism, implicit word reordering knowledge needs to be incorporated into attention-based NMT. In this paper, we introduce three distortion models that originated from SMT (Brown et al., 1993; Koehn et al., 2003; Och et al., 2004; Tillmann, 2004; Al-Onaizan and Papineni, 2006), so as to model the word reordering knowledge as the probability distribution of the relative jump distances between the newly translated source word and the to-be-translated source word. Our focus is to extend the attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Our models have three merits: 1. Extended word reordering knowledge. Our models capture explicit word reordering knowledge to guide the attending process for attention mechanism. 2. Convenient to be incorporated into attention-based NMT. Our distortion models are d"
P17-1140,W14-4012,0,0.0500402,"Missing"
P17-1140,D14-1179,0,0.0527966,"Missing"
P17-1140,P05-1066,0,0.0969476,"task on the ChineseEnglish direction to evaluate the eﬀectiveness of our models. To investigate the word alignment quality, we take the word alignment quality evaluation on the manually aligned corpus. We also conduct the experiments to observe eﬀects of hyper-parameters and the training strategies. 4.1 2003-2006 are used as test sets. To assess the word alignment quality, we employ Tsinghua dataset (Liu and Sun, 2015) which contains 900 manually aligned sentence pairs. Metrics: The translation quality evaluation metric is the case-insensitive 4-gram BLEU3 (Papineni et al., 2002). Sign-test (Collins et al., 2005) is exploited for statistical signiﬁcance test. Alignment error rate (AER) (Och and Ney, 2003) is calculated to assess the word alignment quality. The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. Hyper parameters: The sentence length for training NMTs is up to 50, while SMT model exploits whole training data without any restrictions. Following Bahdanau et al. (2015), we use bi-directional Gated Recurrent Unit (GRU) as the encoder. The forward representation and the backward representation are concatenated at the corresponding p"
P17-1140,N04-1035,0,0.0412781,"the context Ψ. Function Γ(·) for shifting the alignment vector is deﬁned as Γ(αt−1 , k) =   {αt−1,−k , ..., αt−1,m , 0, ..., 0}, αt−1 ,   {0, ..., 0, αt−1,1 , ..., αt−1,m−k }, k<0 k= 0 k>0 (9) which can be implemented as matrix multiplication computations. S-Distortion model adopts previous source context ct−1 as the context Ψ with the intuition that certain source word indicate certain jump distance. The to-be-translated source word have intense positional relations with the newly translated one. The underlying linguistic intuition is that synchronous grammars (Yamada and Knight, 2001; Galley et al., 2004) can be extracted from language pairs. Word categories such as verb, adjective and preposition carry general word reordering knowledge and words carry speciﬁc word reordering knowledge. To further illustrate this idea, we present some common synchronous grammar rules that can be extracted from the example in Table 1 as follows, N P −→ JJ N N |JJ NN JJ −→ zuixin |latest. (10) From the above grammar, we can conjecture the speculation that after the word ”zuixin(latest)” is translated, the translation orientation is forward with shift distance 1. The probability function in S-Distortion model is"
P17-1140,P15-1001,0,0.0584537,"dentical corpora, our system achieves the state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count”"
P17-1140,D13-1176,0,0.052373,"translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the lat"
P17-1140,C16-1205,1,0.702274,"proving translation quality. Comparison with previous work: We present the performance comparison with pre1529 System Coverage MEMDEC NMTIA Our work Length 80 50 80 50 MT03 36.16 35.69 37.93 MT04 39.81 39.24 40.40 MT05 32.73 35.91 35.74 36.81 MT06 32.47 35.98 35.10 35.77 Average 36.95 36.44 37.73 Table 3: Comparison with previous work on identical training corpora. Coverage (Tu et al., 2016) is a basic RNNsearch model with a coverage model to alleviate the over-translation and under-translation problems. MEMDEC (Wang et al., 2016) is to improve translation quality with external memory. NMTIA (Meng et al., 2016) exploits a readable and writable attention mechanism to keep track of interactive history in decoding. Our work is NMT with H-Distortion model. The vocabulary sizes of all work are 30K and maximum lengths of sentence diﬀer. (a) (b) Figure 4: (a) is the output of the distortion model and is calculated on shift actions of previous alignment vector. (b) is the ultimate word alignment matrix of attention-based NMT with H-Distortion model. Compared with Figure 1, (b) is more centralized and accurate. Systems RNNsearch∗ (30K) + T-Distortion + S-Distortion + H-Distortion BLEU 20.90 24.33‡ 24.10‡ 24."
P17-1140,J03-1002,0,0.0524536,"Missing"
P17-1140,P02-1040,0,0.0984942,"3 Training We carry the translation task on the ChineseEnglish direction to evaluate the eﬀectiveness of our models. To investigate the word alignment quality, we take the word alignment quality evaluation on the manually aligned corpus. We also conduct the experiments to observe eﬀects of hyper-parameters and the training strategies. 4.1 2003-2006 are used as test sets. To assess the word alignment quality, we employ Tsinghua dataset (Liu and Sun, 2015) which contains 900 manually aligned sentence pairs. Metrics: The translation quality evaluation metric is the case-insensitive 4-gram BLEU3 (Papineni et al., 2002). Sign-test (Collins et al., 2005) is exploited for statistical signiﬁcance test. Alignment error rate (AER) (Och and Ney, 2003) is calculated to assess the word alignment quality. The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. Hyper parameters: The sentence length for training NMTs is up to 50, while SMT model exploits whole training data without any restrictions. Following Bahdanau et al. (2015), we use bi-directional Gated Recurrent Unit (GRU) as the encoder. The forward representation and the backward representation are c"
P17-1140,P16-1162,0,0.0948834,"performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count” represents the frequency of the word collocation in the trainin"
P17-1140,P16-1159,0,0.0390894,"e state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count” represents the frequency of the word col"
P17-1140,N03-1017,0,0.31876,"ls enable attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve signiﬁcant translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al"
P17-1140,D15-1166,0,0.274308,"ur system achieves the state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count” represents the frequ"
P17-1140,P16-1008,0,0.387371,"ion quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count” represents the frequency of the word collocation in the training corpus. The col"
P17-1140,D16-1027,1,0.702952,"s improvements on BLEU scores proves the eﬀectiveness of proposed approaches in improving translation quality. Comparison with previous work: We present the performance comparison with pre1529 System Coverage MEMDEC NMTIA Our work Length 80 50 80 50 MT03 36.16 35.69 37.93 MT04 39.81 39.24 40.40 MT05 32.73 35.91 35.74 36.81 MT06 32.47 35.98 35.10 35.77 Average 36.95 36.44 37.73 Table 3: Comparison with previous work on identical training corpora. Coverage (Tu et al., 2016) is a basic RNNsearch model with a coverage model to alleviate the over-translation and under-translation problems. MEMDEC (Wang et al., 2016) is to improve translation quality with external memory. NMTIA (Meng et al., 2016) exploits a readable and writable attention mechanism to keep track of interactive history in decoding. Our work is NMT with H-Distortion model. The vocabulary sizes of all work are 30K and maximum lengths of sentence diﬀer. (a) (b) Figure 4: (a) is the output of the distortion model and is calculated on shift actions of previous alignment vector. (b) is the ultimate word alignment matrix of attention-based NMT with H-Distortion model. Compared with Figure 1, (b) is more centralized and accurate. Systems RNNsearc"
P17-1140,P01-1067,0,0.216794,"nce k that conditioned on the context Ψ. Function Γ(·) for shifting the alignment vector is deﬁned as Γ(αt−1 , k) =   {αt−1,−k , ..., αt−1,m , 0, ..., 0}, αt−1 ,   {0, ..., 0, αt−1,1 , ..., αt−1,m−k }, k<0 k= 0 k>0 (9) which can be implemented as matrix multiplication computations. S-Distortion model adopts previous source context ct−1 as the context Ψ with the intuition that certain source word indicate certain jump distance. The to-be-translated source word have intense positional relations with the newly translated one. The underlying linguistic intuition is that synchronous grammars (Yamada and Knight, 2001; Galley et al., 2004) can be extracted from language pairs. Word categories such as verb, adjective and preposition carry general word reordering knowledge and words carry speciﬁc word reordering knowledge. To further illustrate this idea, we present some common synchronous grammar rules that can be extracted from the example in Table 1 as follows, N P −→ JJ N N |JJ NN JJ −→ zuixin |latest. (10) From the above grammar, we can conjecture the speculation that after the word ”zuixin(latest)” is translated, the translation orientation is forward with shift distance 1. The probability function in"
P17-1140,Q16-1027,1,0.71397,"troduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count” represents the frequency of the word collocation in the training corpus. The collocation “zuixin yi"
P17-1140,N04-4026,0,\N,Missing
P17-1141,J09-1002,0,0.159544,"Missing"
P17-1141,N16-1148,0,0.324309,"st computes a vector of output probabilities ot = sof tmax(g(yt−1 , si , ci ))3 using the state information available from hypt−1 . and returns the best k continuations, i.e. Eq. 4: gt = k-argmax oti . (4) i The start and continue functions simply index into the softmax output of the model, selecting specific tokens instead of doing a k-argmax over the entire target language vocabulary. For example, to start constraint ci , we find the score of token ci0 , i.e. otci0 . 4 4.1 Experiments Pick-Revise for Interactive Post Editing Pick-Revise is an interaction cycle for MT PostEditing proposed by Cheng et al. (2016). Starting 3 we use the notation for the g function from Bahdanau et al. (2014) 1539 ITERATION Strict Constraints EN-DE EN-FR EN-PT* Relaxed Constraints EN-DE EN-FR EN-PT* 0 1 2 3 18.44 28.07 15.41 27.64 (+9.20) 36.71 (+8.64) 23.54 (+8.25) 36.66 (+9.01) 44.84 (+8.13) 31.14 (+7.60) 43.92 (+7.26) 45.48 +(0.63) 35.89 (+4.75) 18.44 28.07 15.41 26.43 (+7.98) 33.8 (+5.72) 23.22 (+7.80) 34.48 (+8.04) 40.33 (+6.53) 33.82 (+10.6) 41.82 (+7.34) 47.0 (+6.67) 40.75 (+6.93) Table 1: Results for four simulated editing cycles using WMT test data. EN-DE uses newstest2013, EN-FR uses newstest2014, and EN-PT us"
P17-1141,D14-1179,0,0.0226272,"Missing"
P17-1141,J90-1003,0,0.453082,"ppings as constraints, allowing any existing system to adapt to the terminology used in a new test domain. For the target domain data, we use the Autodesk Post-Editing corpus (Zhechev, 2012), which is a dataset collected from actual MT post-editing sessions. The corpus is focused upon software localization, a domain which is likely to be very different from the WMT data used to train our general domain models. We divide the corpus into approximately 100,000 training sentences, and 1000 test segments, and automatically generate a terminology by computing the Pointwise Mutual Information (PMI) (Church and Hanks, 1990) between source and target n-grams in the training set. We extract all n-grams from length 2-5 as terminology candidates. p(x, y) pmi(x; y) = log (5) p(x)p(y) NMT models do not use explicit alignment between source and target, so we cannot use alignment information to map target phrases to source phrases pmi(x; y) h(x, y) (6) Equations 5 and 6 show how we compute the normalized PMI for a terminology candidate pair. The PMI score is normalized to the range [−1, +1] by dividing by the entropy h of the joint probability p(x, y). We then filter the candidates to only include pairs whose PMI is ≥ 0"
P17-1141,W16-3415,0,0.0621113,"s (1) drenagem ao selec(2) Modi- fique os (3) recursos Table 3: Manual analysis of examples from lexically constrained decoding experiments. “-” followed by whitespace indicates the internal segmentation of the translation model (see Section 3.2) only make use of constraints that match phrase boundaries, because constraints are implemented as “rules” enforcing that source phrases must be translated as the aligned target phrases that have been selected as constraints. In contrast, our approach decodes at the token level, and is not dependent upon any explicit structure in the underlying model. Domingo et al. (2016) also consider an interactive scenario where users first choose portions of an MT hypothesis to keep, then query for an updated translation which preserves these portions. The MT system decodes the source phrases which are not aligned to the user-selected phrases until the source sentence is fully covered. This approach is similar to the system of Cheng et al., and uses the “XML input” feature in Moses (Koehn et al., 2007). Some recent work considers the inclusion of soft lexical constraints directly into deep models for dialog generation, and special cases, such as recipe generation from a li"
P17-1141,W02-1020,0,0.577894,"tput with high confidence. When the domain of the input is known, a domain terminology may be employed to ensure specific phrases are present in a system’s predictions. Our goal in this work is to find a way to force the output of a model to contain such lexical constraints, while still taking advantage of the distribution learned from training data. For Machine Translation (MT) usecases in particular, final translations are often produced by combining automatically translated output with user inputs. Examples include Post-Editing (PE) (Koehn, 2009; Specia, 2011) and InteractivePredictive MT (Foster, 2002; Barrachina et al., 2009; Green, 2014). These interactive scenarios can be unified by considering user inputs to be lexical constraints which guide the search for the optimal output sequence. In this paper, we formalize the notion of lexical constraints, and propose a decoding algorithm which allows the specification of subsequences that are required to be present in a model’s output. Individual constraints may be single tokens or multi-word phrases, and any number of constraints may be specified simultaneously. Although we focus upon interactive applications for MT in our experiments, lexica"
P17-1141,D16-1032,0,0.461507,"vector of output probabilities ot = sof tmax(g(yt−1 , si , ci ))3 using the state information available from hypt−1 . and returns the best k continuations, i.e. Eq. 4: gt = k-argmax oti . (4) i The start and continue functions simply index into the softmax output of the model, selecting specific tokens instead of doing a k-argmax over the entire target language vocabulary. For example, to start constraint ci , we find the score of token ci0 , i.e. otci0 . 4 4.1 Experiments Pick-Revise for Interactive Post Editing Pick-Revise is an interaction cycle for MT PostEditing proposed by Cheng et al. (2016). Starting 3 we use the notation for the g function from Bahdanau et al. (2014) 1539 ITERATION Strict Constraints EN-DE EN-FR EN-PT* Relaxed Constraints EN-DE EN-FR EN-PT* 0 1 2 3 18.44 28.07 15.41 27.64 (+9.20) 36.71 (+8.64) 23.54 (+8.25) 36.66 (+9.01) 44.84 (+8.13) 31.14 (+7.60) 43.92 (+7.26) 45.48 +(0.63) 35.89 (+4.75) 18.44 28.07 15.41 26.43 (+7.98) 33.8 (+5.72) 23.22 (+7.80) 34.48 (+8.04) 40.33 (+6.53) 33.82 (+10.6) 41.82 (+7.34) 47.0 (+6.67) 40.75 (+6.93) Table 1: Results for four simulated editing cycles using WMT test data. EN-DE uses newstest2013, EN-FR uses newstest2014, and EN-PT us"
P17-1141,2016.amta-researchers.9,0,0.0580107,"ffix given a complete source sentence and a desired prefix for the target output (Foster, 2002; Barrachina et al., 2009; Green, 2014). IMT can be viewed as subcase of constrained decoding, where there is only one constraint which is guaranteed to be placed at the beginning of the output sequence. Wuebker et al. (2016) introduce prefix-decoding, which modifies the SMT beam search to first ensure that the target prefix is covered, and only then continues to build hypotheses for the suffix using beams organized by coverage of the remaining phrases in the source segment. Wuebker et al. (2016) and Knowles and Koehn (2016) also present a simple modification of NMT models for IMT, enabling models to predict suffixes for user-supplied prefixes. Recently, some attention has also been given to SMT decoding with multiple lexical constraints. The Pick-Revise (PRIMT) (Cheng et al., 2016) framework for Interactive Post Editing introduces the concept of edit cycles. Translators specify constraints by editing a part of the MT output that is incorrect, and then asking the system for a new hypothesis, which must contain the user-provided correction. This process is repeated, maintaining constraints from previous iterations"
P17-1141,J10-4005,0,0.106615,"lity and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios. 1 Introduction The output of many natural language processing models is a sequence of text. Examples include automatic summarization (Rush et al., 2015), machine translation (Koehn, 2010; Bahdanau et al., 2014), caption generation (Xu et al., 2015), and dialog generation (Serban et al., 2016), among others. In some real-world scenarios, additional information that could inform the search for the optimal output sequence may be available at inference Qun Liu ADAPT Centre Dublin City University qun.liu@dcu.ie time. Humans can provide corrections after viewing a system’s initial output, or separate classification models may be able to predict parts of the output with high confidence. When the domain of the input is known, a domain terminology may be employed to ensure specific ph"
P17-1141,P07-2045,0,0.0162578,"ses that have been selected as constraints. In contrast, our approach decodes at the token level, and is not dependent upon any explicit structure in the underlying model. Domingo et al. (2016) also consider an interactive scenario where users first choose portions of an MT hypothesis to keep, then query for an updated translation which preserves these portions. The MT system decodes the source phrases which are not aligned to the user-selected phrases until the source sentence is fully covered. This approach is similar to the system of Cheng et al., and uses the “XML input” feature in Moses (Koehn et al., 2007). Some recent work considers the inclusion of soft lexical constraints directly into deep models for dialog generation, and special cases, such as recipe generation from a list of ingredients (Wen et al., 2015; Kiddon et al., 2016). Such constraintaware models are complementary to our work, and could be used with GBS decoding without any change to the underlying models. To the best of our knowledge, ours is the first work which considers general lexically constrained decoding for any model which outputs sequences, without relying upon alignments between input and output, and without using a se"
P17-1141,J04-4002,0,0.0470865,"lly globally sub-optimal. On the other hand, an exhaustive exploration of the output space would require scoring |v|T sequences, which is intractable for most real-world models. Thus, a search or decoding algorithm is often used as a compromise between these two extremes. A common solution is to use a heuristic search to attempt to find the best output efficiently (Pearl, 1984; Koehn, 2010; Rush et al., 2013). The key idea is to discard bad options early, while trying to avoid discarding candidates that may be locally risky, but could eventually result in the best overall output. Beam search (Och and Ney, 2004) is probably the most popular search algorithm for decoding sequences. Beam search is simple to implement, and is flexible in the sense that the semantics of the 1536 Figure 2: Different structures for beam search. Boxes represent beams which hold k-best lists of hypotheses. (A) Chart Parsing using SCFG rules to cover spans in the input. (B) Source coverage as used in PB-SMT. (C) Sequence timesteps (as used in Neural Sequence Models), GBS is an extension of (C). In (A) and (B), hypotheses are finished once they reach the final beam. In (C), a hypothesis is only complete if it has generated an"
P17-1141,D13-1022,0,0.0045335,"{v} (2) The standard approach is thus to generate the output sequence from beginning to end, conditioning the output at each timestep upon the input x, risks making locally optimal decisions which are actually globally sub-optimal. On the other hand, an exhaustive exploration of the output space would require scoring |v|T sequences, which is intractable for most real-world models. Thus, a search or decoding algorithm is often used as a compromise between these two extremes. A common solution is to use a heuristic search to attempt to find the best output efficiently (Pearl, 1984; Koehn, 2010; Rush et al., 2013). The key idea is to discard bad options early, while trying to avoid discarding candidates that may be locally risky, but could eventually result in the best overall output. Beam search (Och and Ney, 2004) is probably the most popular search algorithm for decoding sequences. Beam search is simple to implement, and is flexible in the sense that the semantics of the 1536 Figure 2: Different structures for beam search. Boxes represent beams which hold k-best lists of hypotheses. (A) Chart Parsing using SCFG rules to cover spans in the input. (B) Source coverage as used in PB-SMT. (C) Sequence ti"
P17-1141,D15-1044,0,0.035328,"training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios. 1 Introduction The output of many natural language processing models is a sequence of text. Examples include automatic summarization (Rush et al., 2015), machine translation (Koehn, 2010; Bahdanau et al., 2014), caption generation (Xu et al., 2015), and dialog generation (Serban et al., 2016), among others. In some real-world scenarios, additional information that could inform the search for the optimal output sequence may be available at inference Qun Liu ADAPT Centre Dublin City University qun.liu@dcu.ie time. Humans can provide corrections after viewing a system’s initial output, or separate classification models may be able to predict parts of the output with high confidence. When the domain of the input is known, a domain terminology may"
P17-1141,P16-1162,0,0.136929,"or a hypothesis, and the granularity of the tokens (character, subword, word, etc...) are left to the underlying model. Because our decoder can handle arbitrary constraints, there is a risk that constraints will contain tokens that were never observed in the training data, and thus are unknown by the model. Especially in domain adaptation scenarios, some userspecified constraints are very likely to contain unseen tokens. Subword representations provide an elegant way to circumvent this problem, by breaking unknown or rare tokens into character n-grams which are part of the model’s vocabulary (Sennrich et al., 2016; Wu et al., 2016). In the experiments in Section 4, we use this technique to ensure that no input tokens are unknown, even if a constraint contains words which never appeared in the training data.2 3.3 Efficiency Because the number of beams is multiplied by the number of constraints, the runtime complexity of a naive implementation of GBS is O(ktc). Standard time-based beam search is O(kt); therefore, 2 If a character that was not observed in training data is observed at prediction time, it will be unknown. However, we did not observe this in any of our experiments. some consideration must be"
P17-1141,P13-1135,0,0.0216437,"Missing"
P17-1141,2011.eamt-1.12,0,0.0151939,"odels may be able to predict parts of the output with high confidence. When the domain of the input is known, a domain terminology may be employed to ensure specific phrases are present in a system’s predictions. Our goal in this work is to find a way to force the output of a model to contain such lexical constraints, while still taking advantage of the distribution learned from training data. For Machine Translation (MT) usecases in particular, final translations are often produced by combining automatically translated output with user inputs. Examples include Post-Editing (PE) (Koehn, 2009; Specia, 2011) and InteractivePredictive MT (Foster, 2002; Barrachina et al., 2009; Green, 2014). These interactive scenarios can be unified by considering user inputs to be lexical constraints which guide the search for the optimal output sequence. In this paper, we formalize the notion of lexical constraints, and propose a decoding algorithm which allows the specification of subsequences that are required to be present in a model’s output. Individual constraints may be single tokens or multi-word phrases, and any number of constraints may be specified simultaneously. Although we focus upon interactive app"
P17-1141,steinberger-etal-2006-jrc,0,0.0184759,"Missing"
P17-1141,D15-1199,0,0.0407336,"Missing"
P17-1141,P16-1007,0,0.0209774,"ce translations. 5 Related Work Most related work to date has presented modifications of SMT systems for specific usecases which constrain MT output via auxilliary inputs. The largest body of work considers Interactive Machine Translation (IMT): an MT system searches for the optimal target-language suffix given a complete source sentence and a desired prefix for the target output (Foster, 2002; Barrachina et al., 2009; Green, 2014). IMT can be viewed as subcase of constrained decoding, where there is only one constraint which is guaranteed to be placed at the beginning of the output sequence. Wuebker et al. (2016) introduce prefix-decoding, which modifies the SMT beam search to first ensure that the target prefix is covered, and only then continues to build hypotheses for the suffix using beams organized by coverage of the remaining phrases in the source segment. Wuebker et al. (2016) and Knowles and Koehn (2016) also present a simple modification of NMT models for IMT, enabling models to predict suffixes for user-supplied prefixes. Recently, some attention has also been given to SMT decoding with multiple lexical constraints. The Pick-Revise (PRIMT) (Cheng et al., 2016) framework for Interactive Post"
P17-1141,2012.amta-wptp.10,0,0.0341419,"holder tokens into NMT systems, which requires modifying the pre- and post- processing of the data, and training the system with 4 data that contains the same placeholders which occur in the test data (Crego et al., 2016). The MT system also loses any possibility to model the tokens in the terminology, since they are represented by abstract tokens such as “hTERM 1i”. An attractive alternative is to simply provide term mappings as constraints, allowing any existing system to adapt to the terminology used in a new test domain. For the target domain data, we use the Autodesk Post-Editing corpus (Zhechev, 2012), which is a dataset collected from actual MT post-editing sessions. The corpus is focused upon software localization, a domain which is likely to be very different from the WMT data used to train our general domain models. We divide the corpus into approximately 100,000 training sentences, and 1000 test segments, and automatically generate a terminology by computing the Pointwise Mutual Information (PMI) (Church and Hanks, 1990) between source and target n-grams in the training set. We extract all n-grams from length 2-5 as terminology candidates. p(x, y) pmi(x; y) = log (5) p(x)p(y) NMT mode"
P17-1175,W15-3001,0,0.00500996,"Multi30kT data set (German→English and English→German), without images. We use this model to back-translate the 145k German (English) descriptions in the Multi30kC into English (German) and include the triples (synthetic English description, German description, image) when translating into German, and the triples (synthetic German description, English description, image) when translating into English, as additional training data (Sennrich et al., 2016a). We also use the WMT 2015 text-only parallel corpora available for the English–German language pair, consisting of about 4.3M sentence pairs (Bojar et al., 2015). These include the Eu1916 p(yt = k |y&lt;t , C, A) ∝ exp(Lo tanh(Ls st + Lw Ey [ˆ yt−1 ] + Lcs ct + Lci it )). roparl v7 (Koehn, 2005), News Commentary and Common Crawl corpora, which are concatenated and used for pre-training. We use the scripts in the Moses SMT Toolkit (Koehn et al., 2007) to normalise and tokenize English and German descriptions, and we also convert space-separated tokens into subwords (Sennrich et al., 2016b). All models use a common vocabulary of 83, 093 English and 91, 141 German subword tokens. If sentences in English or German are longer than 80 tokens, they are discarde"
P17-1175,W16-2358,0,0.114919,"Missing"
P17-1175,J82-2005,0,0.769702,"Missing"
P17-1175,W16-2359,1,0.819841,"u et al. (2015) proposed an attention-based model for the task of image description generation (IDG) where a model learns to attend to specific parts of an image representation (the source) as it generates its description (the target) in natural language. We are inspired by recent successes in applying attention-based models to NMT and IDG. In this To the best of our knowledge, previous MNMT models in the literature that utilised spatial visual features did not significantly improve over a comparable model that used global visual features or even only textual features (Caglayan et al., 2016a; Calixto et al., 2016; Huang et al., 2016; Libovick´y et al., 2016; Specia et al., 2016). In this work, we wish to address this issue and propose an MNMT model that uses, in addition to an attention mechanism over the source-language words, an additional visual attention mechanism to incorporate spatial visual features, and still improves on simpler text-only and multi-modal attention-based NMT models. The remainder of this paper is structured as follows. We first briefly revisit the attentionbased NMT framework (§2) and expand it into an MNMT framework (§3). In §4, we introduce the 1913 Proceedings of the 55th An"
P17-1175,W17-2004,1,0.414445,"t al., 2015; Firat et al., 2016; Zoph and Knight, 2016). To the best of our knowledge, we are among the first6 to integrate multi-modal inputs in NMT via 6 As pointed out by an anonymous reviewer, Caglayan et al. (2016b) have also experimented with attention-based independent attention mechanisms. Applications Initial experiments with model NMTSRC+IMG have been reported in Calixto et al. (2016). Additionally, NMTSRC+IMG has been applied to the machine translation of user-generated product listings from an e-commerce website, while also making use of the product images to improve translations (Calixto et al., 2017b,a). 7 Conclusions and Future Work We have introduced a novel attention-based, multi-modal NMT model to incorporate spatial visual information into NMT. We have reported state-of-the-art results on the M30kT test set, improving on previous multi-modal attention-based models. We have also showed that our model can be efficiently pre-trained on both mediumsized back-translated in-domain multi-modal data as well as also large general-domain text-only MT corpora, finding that it is able to exploit the additional data regardless of the domain. Our model also compares favourably to both NMT and PBS"
P17-1175,E17-2101,1,0.577314,"t al., 2015; Firat et al., 2016; Zoph and Knight, 2016). To the best of our knowledge, we are among the first6 to integrate multi-modal inputs in NMT via 6 As pointed out by an anonymous reviewer, Caglayan et al. (2016b) have also experimented with attention-based independent attention mechanisms. Applications Initial experiments with model NMTSRC+IMG have been reported in Calixto et al. (2016). Additionally, NMTSRC+IMG has been applied to the machine translation of user-generated product listings from an e-commerce website, while also making use of the product images to improve translations (Calixto et al., 2017b,a). 7 Conclusions and Future Work We have introduced a novel attention-based, multi-modal NMT model to incorporate spatial visual information into NMT. We have reported state-of-the-art results on the M30kT test set, improving on previous multi-modal attention-based models. We have also showed that our model can be efficiently pre-trained on both mediumsized back-translated in-domain multi-modal data as well as also large general-domain text-only MT corpora, finding that it is able to exploit the additional data regardless of the domain. Our model also compares favourably to both NMT and PBS"
P17-1175,D14-1179,0,0.0375937,"Missing"
P17-1175,P11-2031,0,0.00936316,"(MNMT), where each training instance consists of one English sentence, one German sentence and one image (MNMT). We apply early stopping for model selection based on BLEU4, so that if a model does not improve on BLEU4 in the validation set for more than 20 epochs, training is halted. The translation quality of our models is evaluated quantitatively in terms of BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al., 2006), and (10) chrF3 (Popovi´c, 2015).3 We report statistical significance with approximate randomisation for the first three metrics with MultEval (Clark et al., 2011). 5.1 Baselines We train a text-only phrase-based SMT (PBSMT) system and a text-only NMT model for comparison (English→German and German→English). Our PBSMT baseline is built with Moses and uses a 5–gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995). It is trained on the English→German (German→English) descriptions of the M30kT , whereas its LM is trained on the German (English) descriptions only. We use minimum error rate training to tune the model with BLEU (Och, 2003). The text-only NMT baseline is the one described in §2.1 and is trained on the M30kT ’s English–German descri"
P17-1175,W14-3348,0,0.0356652,"al and the decoder RNN using one same mask in all time steps. All models are trained using stochastic gradient descent with ADADELTA (Zeiler, 2012) with minibatches of size 80 (text-only NMT) or 40 (MNMT), where each training instance consists of one English sentence, one German sentence and one image (MNMT). We apply early stopping for model selection based on BLEU4, so that if a model does not improve on BLEU4 in the validation set for more than 20 epochs, training is halted. The translation quality of our models is evaluated quantitatively in terms of BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al., 2006), and (10) chrF3 (Popovi´c, 2015).3 We report statistical significance with approximate randomisation for the first three metrics with MultEval (Clark et al., 2011). 5.1 Baselines We train a text-only phrase-based SMT (PBSMT) system and a text-only NMT model for comparison (English→German and German→English). Our PBSMT baseline is built with Moses and uses a 5–gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995). It is trained on the English→German (German→English) descriptions of the M30kT , whereas its LM is trained on the German (English) descriptions"
P17-1175,P15-1166,0,0.0693606,"nd-to-end to generate textual descriptions of open-domain videos from the video frames based on the sequence-to-sequence framework. Finally, Xu et al. (2015) introduced the first attention-based IDG model where an attentive decoder learns to attend to different parts of an image as it generates its description in natural language. In the context of NMT, Zoph and Knight (2016) introduced a multi-source attention-based NMT model trained to translate a pair of sentences in two different source languages into a target language, and reported considerable improvements over a single-source baseline. Dong et al. (2015) proposed a multi-task learning approach where a model is trained to translate from one source language into multiple target languages. Firat et al. (2016) put forward a multi-way model trained to translate between many different source and target languages. Instead of one attention mechanism per language pair as in Dong et al. (2015), which would lead to a quadratic number of attention mechanisms in relation to language pairs, they use a shared attention mechanism where each target language has one attention shared by all source languages. Luong et al. (2016) proposed a multitask approach whe"
P17-1175,W16-3210,0,0.411031,"Missing"
P17-1175,N16-1101,0,0.0690336,"15) introduced the first attention-based IDG model where an attentive decoder learns to attend to different parts of an image as it generates its description in natural language. In the context of NMT, Zoph and Knight (2016) introduced a multi-source attention-based NMT model trained to translate a pair of sentences in two different source languages into a target language, and reported considerable improvements over a single-source baseline. Dong et al. (2015) proposed a multi-task learning approach where a model is trained to translate from one source language into multiple target languages. Firat et al. (2016) put forward a multi-way model trained to translate between many different source and target languages. Instead of one attention mechanism per language pair as in Dong et al. (2015), which would lead to a quadratic number of attention mechanisms in relation to language pairs, they use a shared attention mechanism where each target language has one attention shared by all source languages. Luong et al. (2016) proposed a multitask approach where they train a model using two tasks and a shared decoder: the main task is to translate from German into English and the sec5 Although their model has no"
P17-1175,P16-1227,0,0.0354635,"ared by all source languages. Luong et al. (2016) proposed a multitask approach where they train a model using two tasks and a shared decoder: the main task is to translate from German into English and the sec5 Although their model has not been devised with translation as its primary goal, theirs is one of the baselines of the first shared task in multi-modal MT in WMT 2016 (Specia et al., 2016). 1920 ondary task is to generate English image descriptions. They show improvements in the main translation task when also training for the secondary image description task. Although not an NMT model, Hitschler et al. (2016) recently used image features to re-rank translations of image descriptions generated by an SMT model and reported significant improvements. Although no purely neural multi-modal model to date significantly improves on both text-only NMT and SMT models (Specia et al., 2016), different research groups have proposed to include global and spatial visual features in re-ranking n-best lists generated by an SMT system or directly in an NMT framework with some success (Caglayan et al., 2016a; Calixto et al., 2016; Huang et al., 2016; Libovick´y et al., 2016; Shah et al., 2016). To the best of our kno"
P17-1175,W16-2360,0,0.703773,"ed an attention-based model for the task of image description generation (IDG) where a model learns to attend to specific parts of an image representation (the source) as it generates its description (the target) in natural language. We are inspired by recent successes in applying attention-based models to NMT and IDG. In this To the best of our knowledge, previous MNMT models in the literature that utilised spatial visual features did not significantly improve over a comparable model that used global visual features or even only textual features (Caglayan et al., 2016a; Calixto et al., 2016; Huang et al., 2016; Libovick´y et al., 2016; Specia et al., 2016). In this work, we wish to address this issue and propose an MNMT model that uses, in addition to an attention mechanism over the source-language words, an additional visual attention mechanism to incorporate spatial visual features, and still improves on simpler text-only and multi-modal attention-based NMT models. The remainder of this paper is structured as follows. We first briefly revisit the attentionbased NMT framework (§2) and expand it into an MNMT framework (§3). In §4, we introduce the 1913 Proceedings of the 55th Annual Meeting of the"
P17-1175,P15-1001,0,0.0248236,"GG19 network (Simonyan and Zisserman, 2015) for an entire image, and also for regions of the image obtained using the RCNN of Girshick et al. (2014). Their best model improves over a strong text-only NMT baseline and is comparable to results obtained with an SMT model trained on the same data. For that reason, their models are used as baselines in our experiments whenever possible. Our work differs from previous work in that, first, we propose attention-based MNMT models. This is an important difference since the use of attention in NMT has become standard and is the current state-of-the-art (Jean et al., 2015; Luong et al., 2015; Firat et al., 2016; Sennrich et al., 2016b). Second, we propose a doublyattentive model where we effectively fuse two mono-modal attention mechanisms into one multimodal decoder, training the entire model jointly and end-to-end. Additionally, we are interested in how to merge textual and visual representations into multi-modal representations when generating words in the target language, which differs substantially from text-only translation tasks even when these translate from many source languages and/or into many target languages (Dong et al., 2015; Firat et al., 2016;"
P17-1175,D13-1176,0,0.0158224,"• We propose a novel attention-based MNMT model which incorporates spatial visual features in a separate visual attention mechanism; • We use a medium-sized, back-translated multi-modal in-domain data set and large general-domain text-only MT corpora to pretrain our models and show that our MNMT model can efficiently exploit both; • We show that images bring useful information into an NMT model, e.g. in situations in which sentences describe objects illustrated in the image. Introduction Neural Machine Translation (NMT) has been successfully tackled as a sequence to sequence learning problem (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014) where each training example consists of one source and one target variable-length sequences, with no prior information on the alignment between the two. In the context of NMT, Bahdanau et al. (2015) first proposed to use an attention mechanism in the decoder, which is trained to attend to the relevant source-language words as it generates each word of the target sentence. Similarly, Xu et al. (2015) proposed an attention-based model for the task of image description generation (IDG) where a model learns to attend to specific parts of an image repres"
P17-1175,2005.mtsummit-papers.11,0,0.0453406,"iptions in the Multi30kC into English (German) and include the triples (synthetic English description, German description, image) when translating into German, and the triples (synthetic German description, English description, image) when translating into English, as additional training data (Sennrich et al., 2016a). We also use the WMT 2015 text-only parallel corpora available for the English–German language pair, consisting of about 4.3M sentence pairs (Bojar et al., 2015). These include the Eu1916 p(yt = k |y&lt;t , C, A) ∝ exp(Lo tanh(Ls st + Lw Ey [ˆ yt−1 ] + Lcs ct + Lci it )). roparl v7 (Koehn, 2005), News Commentary and Common Crawl corpora, which are concatenated and used for pre-training. We use the scripts in the Moses SMT Toolkit (Koehn et al., 2007) to normalise and tokenize English and German descriptions, and we also convert space-separated tokens into subwords (Sennrich et al., 2016b). All models use a common vocabulary of 83, 093 English and 91, 141 German subword tokens. If sentences in English or German are longer than 80 tokens, they are discarded. We train models to translate from English into German, as well as for German into English, and report evaluation of cased, tokeni"
P17-1175,N03-1017,0,0.0162951,"t al. (2016)’s improvements over the best text-only baseline in parentheses. Results are significantly better than the NMT baseline († ) and the SMT baseline (‡ ) with p &lt; 0.01 (no pre-training) or p &lt; 0.05 (when pre-training either on the back-translated M30kC or WMT’15 corpora). metrics, i.e. METEOR and chrF3, whereas NMT is better at precision-oriented ones, i.e. BLEU4. This is somehow expected, since the attention mechanism in NMT (Bahdanau et al., 2015) does not explicitly take attention weights from previous time steps into account, an thus lacks the notion of source coverage as in SMT (Koehn et al., 2003; Tu et al., 2016). We note that these ideas are complementary and incorporating coverage into model NMTSRC+IMG could lead to more improvements, especially in recall-oriented metrics. Nonetheless, our doubly-attentive model shows consistent gains in both precision- and recall-oriented metrics in comparison to the text-only NMT baseline, i.e. it is significantly better according to BLEU4, METEOR and TER (p &lt; 0.01), and it also improves chrF3 by +2.1. In comparison to the PBSMT baseline, our proposed model still significantly improves according to both BLEU4 and TER (p &lt; 0.01), also increasing M"
P17-1175,W16-2361,0,0.0875627,"Missing"
P17-1175,D15-1166,0,0.119578,"yan and Zisserman, 2015) for an entire image, and also for regions of the image obtained using the RCNN of Girshick et al. (2014). Their best model improves over a strong text-only NMT baseline and is comparable to results obtained with an SMT model trained on the same data. For that reason, their models are used as baselines in our experiments whenever possible. Our work differs from previous work in that, first, we propose attention-based MNMT models. This is an important difference since the use of attention in NMT has become standard and is the current state-of-the-art (Jean et al., 2015; Luong et al., 2015; Firat et al., 2016; Sennrich et al., 2016b). Second, we propose a doublyattentive model where we effectively fuse two mono-modal attention mechanisms into one multimodal decoder, training the entire model jointly and end-to-end. Additionally, we are interested in how to merge textual and visual representations into multi-modal representations when generating words in the target language, which differs substantially from text-only translation tasks even when these translate from many source languages and/or into many target languages (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 20"
P17-1175,P03-1021,0,0.00902646,"t statistical significance with approximate randomisation for the first three metrics with MultEval (Clark et al., 2011). 5.1 Baselines We train a text-only phrase-based SMT (PBSMT) system and a text-only NMT model for comparison (English→German and German→English). Our PBSMT baseline is built with Moses and uses a 5–gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995). It is trained on the English→German (German→English) descriptions of the M30kT , whereas its LM is trained on the German (English) descriptions only. We use minimum error rate training to tune the model with BLEU (Och, 2003). The text-only NMT baseline is the one described in §2.1 and is trained on the M30kT ’s English–German descriptions, again in both language directions. When translating into German, we also compare our model against two publicly available results obtained with multi-modal attention-based NMT models. The first model is Huang et al. (2016)’s best model trained on the same data, and the second is their best model using additional object detections, respectively models m1 (image at head) and m3 in the authors’ paper. 5.2 Results In Table 1, we show results for the two textonly baselines NMT and P"
P17-1175,P02-1040,0,0.124293,"Training, validation and test sets contain 29k, 1,014 and 1k images respectively, each accompanied by a sentence pair (the original English sentence and its translation into German). For each of the 30k images in the Flickr30k, the M30kC has five descriptions in German collected independently from the English descriptions. Training, validation and test sets contain 29k, 1,014 and 1k images respectively, each accompanied by five sentences in English and five sentences in German. We use the entire M30kT training set for training our MNMT models, its validation set for model selection with BLEU (Papineni et al., 2002), and its test set for evaluation. In addition, since the amount of training data available is small, we build a back-translation model using the text-only NMT model described in §2.1 trained on the Multi30kT data set (German→English and English→German), without images. We use this model to back-translate the 145k German (English) descriptions in the Multi30kC into English (German) and include the triples (synthetic English description, German description, image) when translating into German, and the triples (synthetic German description, English description, image) when translating into Engli"
P17-1175,W15-3049,0,0.0123234,"Missing"
P17-1175,P16-1009,0,0.151995,". In addition, since the amount of training data available is small, we build a back-translation model using the text-only NMT model described in §2.1 trained on the Multi30kT data set (German→English and English→German), without images. We use this model to back-translate the 145k German (English) descriptions in the Multi30kC into English (German) and include the triples (synthetic English description, German description, image) when translating into German, and the triples (synthetic German description, English description, image) when translating into English, as additional training data (Sennrich et al., 2016a). We also use the WMT 2015 text-only parallel corpora available for the English–German language pair, consisting of about 4.3M sentence pairs (Bojar et al., 2015). These include the Eu1916 p(yt = k |y&lt;t , C, A) ∝ exp(Lo tanh(Ls st + Lw Ey [ˆ yt−1 ] + Lcs ct + Lci it )). roparl v7 (Koehn, 2005), News Commentary and Common Crawl corpora, which are concatenated and used for pre-training. We use the scripts in the Moses SMT Toolkit (Koehn et al., 2007) to normalise and tokenize English and German descriptions, and we also convert space-separated tokens into subwords (Sennrich et al., 2016b). All"
P17-1175,W16-2363,0,0.250575,"not an NMT model, Hitschler et al. (2016) recently used image features to re-rank translations of image descriptions generated by an SMT model and reported significant improvements. Although no purely neural multi-modal model to date significantly improves on both text-only NMT and SMT models (Specia et al., 2016), different research groups have proposed to include global and spatial visual features in re-ranking n-best lists generated by an SMT system or directly in an NMT framework with some success (Caglayan et al., 2016a; Calixto et al., 2016; Huang et al., 2016; Libovick´y et al., 2016; Shah et al., 2016). To the best of our knowledge, the best published results of a purely MNMT model are those of Huang et al. (2016), who proposed to use global visual features extracted with the VGG19 network (Simonyan and Zisserman, 2015) for an entire image, and also for regions of the image obtained using the RCNN of Girshick et al. (2014). Their best model improves over a strong text-only NMT baseline and is comparable to results obtained with an SMT model trained on the same data. For that reason, their models are used as baselines in our experiments whenever possible. Our work differs from previous work"
P17-1175,2006.amta-papers.25,0,0.0710451,"same mask in all time steps. All models are trained using stochastic gradient descent with ADADELTA (Zeiler, 2012) with minibatches of size 80 (text-only NMT) or 40 (MNMT), where each training instance consists of one English sentence, one German sentence and one image (MNMT). We apply early stopping for model selection based on BLEU4, so that if a model does not improve on BLEU4 in the validation set for more than 20 epochs, training is halted. The translation quality of our models is evaluated quantitatively in terms of BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al., 2006), and (10) chrF3 (Popovi´c, 2015).3 We report statistical significance with approximate randomisation for the first three metrics with MultEval (Clark et al., 2011). 5.1 Baselines We train a text-only phrase-based SMT (PBSMT) system and a text-only NMT model for comparison (English→German and German→English). Our PBSMT baseline is built with Moses and uses a 5–gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995). It is trained on the English→German (German→English) descriptions of the M30kT , whereas its LM is trained on the German (English) descriptions only. We use minimum error"
P17-1175,Q14-1006,0,0.0182663,"iability of the image descriptions generated with their model. Finally, we use the time-dependent image context vector it as an additional input to a modified version of REC2 (§2.2), which now computes the final hidden state st using the hidden state proposal s0t , and the time-dependent source and image context vectors ct and it , as in eq. (9): zt = σ(Wzsrc ct + Wzimg it + Uz s0j ), rt = σ(Wrsrc ct + Wrimg it + Ur s0j ), st = tanh(W src ct + W img it + rt (U s0t )), st = (1 − zt ) st + zt s0t . (9) Data The Flickr30k data set contains 30k images and 5 descriptions in English for each image (Young et al., 2014). In this work, we use the Multi30k dataset (Elliott et al., 2016), which consists of two multilingual expansions of the original Flickr30k: one with translated data and another one with comparable data, henceforth referred to as M30kT and M30kC , respectively. For each of the 30k images in the Flickr30k, the M30kT has one of the English descriptions manually translated into German by a professional translator. Training, validation and test sets contain 29k, 1,014 and 1k images respectively, each accompanied by a sentence pair (the original English sentence and its translation into German). Fo"
P17-1175,N16-1004,0,0.0992276,"al. (2015) put forward a model to generate multilingual descriptions of images by learning and transferring features between two independent, non-attentive neural image description models.5 Venugopalan et al. (2015) introduced a model trained end-to-end to generate textual descriptions of open-domain videos from the video frames based on the sequence-to-sequence framework. Finally, Xu et al. (2015) introduced the first attention-based IDG model where an attentive decoder learns to attend to different parts of an image as it generates its description in natural language. In the context of NMT, Zoph and Knight (2016) introduced a multi-source attention-based NMT model trained to translate a pair of sentences in two different source languages into a target language, and reported considerable improvements over a single-source baseline. Dong et al. (2015) proposed a multi-task learning approach where a model is trained to translate from one source language into multiple target languages. Firat et al. (2016) put forward a multi-way model trained to translate between many different source and target languages. Instead of one attention mechanism per language pair as in Dong et al. (2015), which would lead to a"
P17-1175,W16-2346,0,0.415827,"Missing"
P17-1175,P16-1008,0,0.0194855,"vements over the best text-only baseline in parentheses. Results are significantly better than the NMT baseline († ) and the SMT baseline (‡ ) with p &lt; 0.01 (no pre-training) or p &lt; 0.05 (when pre-training either on the back-translated M30kC or WMT’15 corpora). metrics, i.e. METEOR and chrF3, whereas NMT is better at precision-oriented ones, i.e. BLEU4. This is somehow expected, since the attention mechanism in NMT (Bahdanau et al., 2015) does not explicitly take attention weights from previous time steps into account, an thus lacks the notion of source coverage as in SMT (Koehn et al., 2003; Tu et al., 2016). We note that these ideas are complementary and incorporating coverage into model NMTSRC+IMG could lead to more improvements, especially in recall-oriented metrics. Nonetheless, our doubly-attentive model shows consistent gains in both precision- and recall-oriented metrics in comparison to the text-only NMT baseline, i.e. it is significantly better according to BLEU4, METEOR and TER (p &lt; 0.01), and it also improves chrF3 by +2.1. In comparison to the PBSMT baseline, our proposed model still significantly improves according to both BLEU4 and TER (p &lt; 0.01), also increasing METEOR by +0.7 but"
P18-1138,E17-2029,0,0.0229563,"es of sequence-to-sequence architecture (Cho et al., 2014; Sutskever et al., 2014) motivated investigation in dialogue systems that can effectively learn to generate a response sequence given the previous utterance sequence (Shang et al., 2015; Sordoni et al., 2015b; Vinyals and Le, 2015). The model is trained to minimize the negative log-likelihood of the training data. Despite the current progress, the lack of response diversity is a notorious problem, where the model inherently tends to generate short, general responses in spite of different inputs. Li et al. (2016a); Serban et al. (2017); Cao and Clark (2017) suggested that theses boring responses are common in training data and shorter responses are more likely to be given a higher likelihood. To tackle the problem, Li et al. (2016a) introduced a maximum mutual information training objective. Serban et al. (2017), Cao and Clark (2017) and Chen et al. (2018) used latent variables to introduce stochasticity to enhance the response diversity. Vijayakumar et al. (2016),Shao et al. (2017) and Li et al. (2016b) recognized that the greedy search decoding process, especially beam-search with a wide beam size, leads the short responses possess higher like"
P18-1138,D14-1179,0,0.0292992,"Missing"
P18-1138,W17-5506,0,0.153075,"rst analyzed, followed by retrieving related facts from knowledge bases (KBs), and finally the answers are generated.The facts are usually presented in the form of “subject-relationobject” triplets, where the subject and object are entities. With the aid of knowledge triplets, neural generative question answering systems are capable of answering facts related inquiries (Yin et al., 2016; Zhu et al., 2017; He et al., 2017a), WH questions in particular, like “who is Yao Ming’s wife ?”. Although answering enquiries is essential for dialogue systems, especially for task-oriented dialogue systems (Eric et al., 2017), it is still far behind a natural knowledge grounded dialogue system, which should be able to understand the facts involved in current dialogue session (socalled facts matching), as well as diffuse them to other similar entities for knowledge-based chitchats (i.e. entity diffusion): 1) facts matching: in dialogue systems, matching utterances to exact facts is much harder than explicit factoid inquiries answering. Though some utterances are facts related inquiries, whose subjects and relations can be easily recognized, for some utterances, the subjects and relations are elusive, which leads th"
P18-1138,P17-1019,0,0.448474,"bases ( e.g., the freebase (Google, 2013) and DBpedia (Lehmann et al., 2017) ) are leveraged. A related application of knowledge bases is question answering, where the given questions are first analyzed, followed by retrieving related facts from knowledge bases (KBs), and finally the answers are generated.The facts are usually presented in the form of “subject-relationobject” triplets, where the subject and object are entities. With the aid of knowledge triplets, neural generative question answering systems are capable of answering facts related inquiries (Yin et al., 2016; Zhu et al., 2017; He et al., 2017a), WH questions in particular, like “who is Yao Ming’s wife ?”. Although answering enquiries is essential for dialogue systems, especially for task-oriented dialogue systems (Eric et al., 2017), it is still far behind a natural knowledge grounded dialogue system, which should be able to understand the facts involved in current dialogue session (socalled facts matching), as well as diffuse them to other similar entities for knowledge-based chitchats (i.e. entity diffusion): 1) facts matching: in dialogue systems, matching utterances to exact facts is much harder than explicit factoid inquiries"
P18-1138,W18-2605,0,0.0371289,"Missing"
P18-1138,N16-1014,0,0.0685156,"abulary cases. 4 Related Work The successes of sequence-to-sequence architecture (Cho et al., 2014; Sutskever et al., 2014) motivated investigation in dialogue systems that can effectively learn to generate a response sequence given the previous utterance sequence (Shang et al., 2015; Sordoni et al., 2015b; Vinyals and Le, 2015). The model is trained to minimize the negative log-likelihood of the training data. Despite the current progress, the lack of response diversity is a notorious problem, where the model inherently tends to generate short, general responses in spite of different inputs. Li et al. (2016a); Serban et al. (2017); Cao and Clark (2017) suggested that theses boring responses are common in training data and shorter responses are more likely to be given a higher likelihood. To tackle the problem, Li et al. (2016a) introduced a maximum mutual information training objective. Serban et al. (2017), Cao and Clark (2017) and Chen et al. (2018) used latent variables to introduce stochasticity to enhance the response diversity. Vijayakumar et al. (2016),Shao et al. (2017) and Li et al. (2016b) recognized that the greedy search decoding process, especially beam-search with a wide beam size,"
P18-1138,P15-1152,0,0.222259,"Missing"
P18-1138,N15-1020,0,0.0921078,"Missing"
P18-1138,W16-0106,0,0.23812,"puters, different kinds of knowledge bases ( e.g., the freebase (Google, 2013) and DBpedia (Lehmann et al., 2017) ) are leveraged. A related application of knowledge bases is question answering, where the given questions are first analyzed, followed by retrieving related facts from knowledge bases (KBs), and finally the answers are generated.The facts are usually presented in the form of “subject-relationobject” triplets, where the subject and object are entities. With the aid of knowledge triplets, neural generative question answering systems are capable of answering facts related inquiries (Yin et al., 2016; Zhu et al., 2017; He et al., 2017a), WH questions in particular, like “who is Yao Ming’s wife ?”. Although answering enquiries is essential for dialogue systems, especially for task-oriented dialogue systems (Eric et al., 2017), it is still far behind a natural knowledge grounded dialogue system, which should be able to understand the facts involved in current dialogue session (socalled facts matching), as well as diffuse them to other similar entities for knowledge-based chitchats (i.e. entity diffusion): 1) facts matching: in dialogue systems, matching utterances to exact facts is much har"
P19-1139,D15-1075,0,0.0415293,"nington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin’ in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly impossible to extract the fine-grained relations, such as composer and autho"
P19-1139,D18-1021,1,0.795851,"Missing"
P19-1139,P17-1149,0,0.0267538,"token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3.1 We denote a token sequence as {w1 , . . . , wn } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1 , . . . , em }, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be aligned to an entity in"
P19-1139,P18-1224,0,0.0426992,"low ··· wn(i 1) 1962 (i 1) e1 (i 1) e2 Entity Input Bob Dylan Blowin’ in the Wind Token Input Bob Dylan wrote Blowin’ in the Wind in 1962 (a) Model Achitecture (b) Aggregator Figure 2: The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this"
P19-1139,P18-1009,0,0.0439758,"to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3. As most datasets do not have entity annotations, we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs. 4.3 Entity Typing Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1. We compare our model with the following baseline models for entity typing: NFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER. As this paper focuses on comparing the general language representation abilities of various n"
P19-1139,N19-1423,0,0.626127,"e Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004. Figure 1: An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results"
P19-1139,I05-5002,0,0.0118207,"ovide more information for relation classification than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller training set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small datasets, that is, ERNIE"
P19-1139,D18-1247,1,0.937706,"ty Input Bob Dylan Blowin’ in the Wind Token Input Bob Dylan wrote Blowin’ in the Wind in 1962 (a) Model Achitecture (b) Aggregator Figure 2: The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora a"
P19-1139,D18-1514,1,0.936476,"ty Input Bob Dylan Blowin’ in the Wind Token Input Bob Dylan wrote Blowin’ in the Wind in 1962 (a) Model Achitecture (b) Aggregator Figure 2: The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora a"
P19-1139,P18-1031,0,0.153256,"olume One is_a Bob Dylan Writer Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004. Figure 1: An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language represent"
P19-1139,P16-1200,1,0.845415,"from each class for the training set, and sample 200 instances for the development and test respectively. There are 80 classes in FewRel, and there are 42 classes (including a special relation “no relation”) in TACRED. We compare our model with the following baseline models for relation classification: CNN. With a convolution layer, a max-pooling layer, and a non-linear activation layer, CNN gets the output sentence embedding, and then feeds it into a relation classifier. To better capture the position of head and tail entities, position embeddings are introduced into CNN (Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017; Han et al., 2018b). PA-LSTM. Zhang et al. (2017) propose PALSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation. C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to"
P19-1139,Q15-1023,0,0.119712,"Missing"
P19-1139,P18-1136,0,0.0322388,"oken Input Bob Dylan wrote Blowin’ in the Wind in 1962 (a) Model Achitecture (b) Aggregator Figure 2: The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language represen"
P19-1139,W03-0419,0,0.686759,"Missing"
P19-1139,W16-1313,0,0.0292746,"nd its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1. We compare our model with the following baseline models for entity typing: NFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER. As this paper focuses on comparing the general language representation abilities of various neural models, we thus do not use the hand-craft features in this work. UFET. For Open Entity, we add a new hybrid model UFET (Choi et al., 2018) for comparison. UFET is proposed with the Open Entity dataset, which uses a Bi-LSTM for context representation instead of two Bi-LSTMs separated by entity mentions in NFGEC. Besides NFGEC and UFET, we also report th"
P19-1139,D13-1170,0,0.0050581,"anguage models can provide more information for relation classification than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller training set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small"
P19-1139,speer-havasi-2012-representing,0,0.0818577,"Missing"
P19-1139,D15-1174,0,0.032581,"token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3.1 We denote a token sequence as {w1 , . . . , wn } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1 , . . . , em }, where m is the length of the entity sequence. Note that m is not equal to n in most"
P19-1139,P18-1076,0,0.0381481,"T). Radford et al. (2018) propose a generative pre-trained Transformer (Vaswani et al., 2017) (GPT) to learn language representations. Devlin et al. (2019) propose a deep bidirectional model with multiplelayer Transformers (BERT), which achieves the state-of-the-art results for various NLP tasks. Though both feature-based and fine-tuning language representation models have achieved great success, they ignore the incorporation of knowledge information. As demonstrated in recent work, injecting extra knowledge information can significantly enhance original models, such as reading comprehension (Mihaylov and Frank, 2018; Zhong et al., 2018), machine translation (Zaremoodi et al., 2018), natural language 1442 Token Output (i) Token Output Aggregator Information Fusion (i) e1 Entity Output e2 (i) (i) (i) w2 w1 w3 ··· wn(i) e1 ··· w˜n(i) e˜1 (i) e2 (i) e˜2 (i) Entity Output Aggregator K-Encoder Mx Information Fusion Multi-Head Attention Multi-Head Attention (i) (i) e˜2 e˜1 Feed Forward (i) (i) (i) w˜3 w˜2 w˜1 Transformer T-Encoder Nx (i) w4 (i) w˜4 (i) Entity Input Multi-Head Attention Multi-Head Attention Multi-Head Attention (i 1) Token Input w1 bob (i 1) w2 dylan (i 1) w3 wrote (i 1) w4 blow ··· wn(i 1) 1962"
P19-1139,P10-1040,0,0.0598698,"cific NLP tasks. These pre-training approaches can be divided into two classes, i.e., feature-based approaches and finetuning approaches. The early work (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings. Different from the above-mentioned featurebased language approaches only using the pretrained language representations as input features, Dai and Le (2015) train auto-encoders on unlabeled text, and then use the pre-trained model architecture and parameters as a starting point for other specific NLP models. Inspired by Dai and Le (2015)"
P19-1139,D14-1162,0,0.102807,"rresponding author: Z.Liu(liuzy@tsinghua.edu.cn) os er Song Book r tho au Chronicles: Volume One is_a Bob Dylan Writer Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004. Figure 1: An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015)"
P19-1139,P17-1161,0,0.0313441,"(liuzy@tsinghua.edu.cn) os er Song Book r tho au Chronicles: Volume One is_a Bob Dylan Writer Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004. Figure 1: An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classifica"
P19-1139,N18-1202,0,0.102763,"ture-based approaches and finetuning approaches. The early work (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings. Different from the above-mentioned featurebased language approaches only using the pretrained language representations as input features, Dai and Le (2015) train auto-encoders on unlabeled text, and then use the pre-trained model architecture and parameters as a starting point for other specific NLP models. Inspired by Dai and Le (2015), more pre-trained language representation models for fine-tuning have been proposed. How"
P19-1139,D16-1264,0,0.274955,"e representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin’ in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly imp"
P19-1139,W18-5446,0,0.172541,") and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin’ in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly impossible to extract the fine-grained relations, such as composer and author on the relation classification task. For th"
P19-1139,D14-1167,0,0.0268165,"input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3.1 We denote a token sequence as {w1 , . . . , wn } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1 , . . . , em }, where m is the length of the entity sequence. Note that m i"
P19-1139,N18-1101,0,0.0178587,"ion than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller training set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small datasets, that is, ERNIE is better on CoLA and RTE, but worse on STS-"
P19-1139,D17-1187,0,0.0321162,"or the training set, and sample 200 instances for the development and test respectively. There are 80 classes in FewRel, and there are 42 classes (including a special relation “no relation”) in TACRED. We compare our model with the following baseline models for relation classification: CNN. With a convolution layer, a max-pooling layer, and a non-linear activation layer, CNN gets the output sentence embedding, and then feeds it into a relation classifier. To better capture the position of head and tail entities, position embeddings are introduced into CNN (Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017; Han et al., 2018b). PA-LSTM. Zhang et al. (2017) propose PALSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation. C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to these three base"
P19-1139,D18-1121,1,0.825023,"Missing"
P19-1139,E17-1055,0,0.0423233,"Missing"
P19-1139,K16-1025,0,0.0384676,"concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3.1 We denote a token sequence as {w1 , . . . , wn } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1 , . . . , em }, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be align"
P19-1139,P18-2104,0,0.0250317,"er (Vaswani et al., 2017) (GPT) to learn language representations. Devlin et al. (2019) propose a deep bidirectional model with multiplelayer Transformers (BERT), which achieves the state-of-the-art results for various NLP tasks. Though both feature-based and fine-tuning language representation models have achieved great success, they ignore the incorporation of knowledge information. As demonstrated in recent work, injecting extra knowledge information can significantly enhance original models, such as reading comprehension (Mihaylov and Frank, 2018; Zhong et al., 2018), machine translation (Zaremoodi et al., 2018), natural language 1442 Token Output (i) Token Output Aggregator Information Fusion (i) e1 Entity Output e2 (i) (i) (i) w2 w1 w3 ··· wn(i) e1 ··· w˜n(i) e˜1 (i) e2 (i) e˜2 (i) Entity Output Aggregator K-Encoder Mx Information Fusion Multi-Head Attention Multi-Head Attention (i) (i) e˜2 e˜1 Feed Forward (i) (i) (i) w˜3 w˜2 w˜1 Transformer T-Encoder Nx (i) w4 (i) w˜4 (i) Entity Input Multi-Head Attention Multi-Head Attention Multi-Head Attention (i 1) Token Input w1 bob (i 1) w2 dylan (i 1) w3 wrote (i 1) w4 blow ··· wn(i 1) 1962 (i 1) e1 (i 1) e2 Entity Input Bob Dylan Blowin’ in the Wind Token"
P19-1139,D18-1009,0,0.0262803,"including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin’ in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly impossible to extract the"
P19-1139,D15-1203,0,0.231948,"rom Devlin et al. (2019). The overall pre-training loss is the sum of the dEA, MLM and NSP loss. to fine-tune ERNIE for relation classification is to apply the pooling layer to the final output embeddings of the given entity mentions, and represent the given entity pair with the concatenation of their mention embeddings for classification. In this paper, we design another method, which modifies the input token sequence by adding two mark tokens to highlight entity mentions. These extra mark tokens play a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015). Then, we also take the [CLS] token embedding for classification. Note that we design different tokens [HD] and [TL] for head entities and tail entities respectively. The specific fine-tuning procedure for entity typing is a simplified version of relation classification. As previous typing models make full use of both context embeddings and entity mention embeddings (Shimaoka et al., 2016; Yaghoobzadeh and Sch¨utze, 2017; Xin et al., 2018), we argue that the modified input sequence with the mention mark token [ENT] can guide ERNIE to combine both context information and entity mention informa"
P19-1139,D18-1244,0,0.0362932,"n classification: CNN. With a convolution layer, a max-pooling layer, and a non-linear activation layer, CNN gets the output sentence embedding, and then feeds it into a relation classifier. To better capture the position of head and tail entities, position embeddings are introduced into CNN (Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017; Han et al., 2018b). PA-LSTM. Zhang et al. (2017) propose PALSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation. C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to these three baselines, we also finetune BERT with the same input format introduced in Section 3.5 for fair comparison. 1447 Model MNLI-(m/mm) 392k QQP 363k QNLI 104k SST-2 67k BERTBASE 84.6/83.4 71.2 - 93.5 ERNIE 84.0/83.2 71.2 91.3 93.5 Model CoLA 8.5k STS-B 5.7k MRPC 3.5k RTE 2.5k BERTB"
P19-1139,D17-1004,0,0.0498373,"labels more precisely. In summary, ERNIE effectively reduces the noisy label challenge in FIGER, which is a widely-used distantly supervised entity typing dataset, by injecting the information from KGs. Besides, ERNIE also outperforms the baselines on Open Entity which has gold annotations. Relation Classification Relation classification aims to determine the correct relation between two entities in a given sentence, which is an important knowledge-driven NLP task. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FewRel (Han et al., 2018c) and TACRED (Zhang et al., 2017). The statistics of these two datasets are shown in Table 4. As the original experimental setting of FewRel is few-shot learning, we rearrange the FewRel dataset for the common relation classification setting. Specifically, we sample 100 instances from each class for the training set, and sample 200 instances for the development and test respectively. There are 80 classes in FewRel, and there are 42 classes (including a special relation “no relation”) in TACRED. We compare our model with the following baseline models for relation classification: CNN. With a convolution layer, a max-pooling lay"
P19-1139,W07-1401,0,\N,Missing
P19-1332,D17-1158,0,0.132886,"ed DNPG in two settings of domain transfer: 1) Quora dataset as the source domain and WikiAnswers as the target domain, denoted as Quora→WikiAnswers, and 2) in reverse as WikiAnswers→Quora. For the baseline models, in addition to the pointer-generator network and the Transformer model with copy mechanism (denoted as Transformer+Copy), we use 3409 Table 8: Human Evaluation in WikiAnswers→Quora Models Figure 4: Left: Training of language model in the source domain; Right: RL training of separator in the target domain. the shallow fusion (Gulcehre et al., 2015) and the multi-task learning (MTL) (Domhan and Hieber, 2017) that harness the non-parallel data in the target domain for adaptation. For fair comparisons, we use the Transformer+Copy as the base model for shallow fusion and implement a variant of MTL with copy mechanism (denoted as MTL+Copy). Table 5 shows performances of the models in two settings. DNPG achieves better performance over the pointer-generator and Transformer-based model, and has the competitive performance with MTL+Copy, which accesses target domain for training. With a fine-tuned separator, Adapted DNPG outperforms other models significantly on Quora→WikiAnswers. When it comes to WikiA"
P19-1332,P16-1154,0,0.0502561,"(zl = 1)αt,l 0 αt,l = PL . l=1 P (zl = 1)αt,l (7) We also restrict the decoder at z level only access the position l : zl = z at encoder in the same way. Figure 3: Aggregator. Figure 2: Attention: phrase-level self-attention (upper) and sentence-level self-attention (lower). Model Capacity: We choose a larger capacity for the phrase-level Transformer over the sentence-level Transformer. The intuition behind is that lexical/phrasal paraphrases generally contain more long-tail expressions than the sentential ones. In addition, the phrase-level Transformer is equipped with the copying mechanism (Gu et al., 2016). Thus, the probability of generating the target word yt by the m-decoder0 is: aggregator, which combines the outputs from the m-decoders. More precisely, the aggregator first decides the probability of the next word being at each granularity. The previous word yt−1 and the context vectors c0 and c1 given by m-decoder0 and m-decoder1 , are fed into a LSTM to make the prediction: vt = LSTM([Wc [c0 ; c1 ; yt−1 ]; vt−1 ]) P (zt |y1:t−1 , X) = GS(Wv vt , τ ), (9) where vt is the hidden state of the LSTM. Then, jointly with the probabilities computed by mdecoders, we can make the final prediction o"
P19-1332,N18-1170,0,0.0979685,"uality of generated paraphrases. Prakash et al. (2016) and Ma et al. (2018) adjust the network architecture for larger capacity. Cao et al. (2017) and Wang et al. (2018) utilize external resources, in other words, phrase dictionary and semantic annotations. Li et al. (2018) reinforce the paraphrase generator by a learnt reward function. Although achieving state-of-the-art performances, none of the above work considers the paraphrase patterns at different levels of granularity. Moreover, their models can generate the paraphrase in a neither interpretable nor a fine-grained controllable way. In Iyyer et al. (2018)’s work, the model is trained to produce a paraphrase of the sentence with a given syntax. In this work, we consider automatically learning controllable and interpretable paraphrasing operations from the corpus. This is also the first work to consider scalable unsupervised domain adaptation for neural paraphrase generation. 4.2 Controllable and Interpretable Text Generation There is extensive attention on controllable neural sequence generation and its interpretation. A line of research is based on variational auto-encoder (VAE), which captures the implicit (Gupta et al., 2017; Li et al., 2017"
P19-1332,D17-1222,0,0.0233645,"r et al. (2018)’s work, the model is trained to produce a paraphrase of the sentence with a given syntax. In this work, we consider automatically learning controllable and interpretable paraphrasing operations from the corpus. This is also the first work to consider scalable unsupervised domain adaptation for neural paraphrase generation. 4.2 Controllable and Interpretable Text Generation There is extensive attention on controllable neural sequence generation and its interpretation. A line of research is based on variational auto-encoder (VAE), which captures the implicit (Gupta et al., 2017; Li et al., 2017) or explicit information (Hu et al., 2017; Liao et al., 2018) via latent representations. Another approach is to integrate probabilistic graphical model, e.g., hidden semi-Markov model (HSMM) into neural network (Wiseman et al., 2018; Dai et al., 2016). In these works, neural templates are learned as a sequential composition of segments controlled by the latent states, and be used for language modeling and data-totext generation. Unfortunately, it is non-trivial to adapt this approach to the Seq2Seq learning framework to extract templates from both the Domain Adaptation for Seq2Seq Learning Do"
P19-1332,D18-1421,1,0.78789,"er performance when adapting to a new domain. 1 what is the reason of $x → what makes $x happen world war II → the second world war Introduction Paraphrases are texts that convey the same meaning using different wording. Paraphrase generation is an important technique in natural language processing (NLP), which can be applied in various downstream tasks such as information retrieval, semantic parsing, and dialogue systems. Neural sequence-to-sequence (Seq2Seq) models have demonstrated the superior performances on generating paraphrases given a sentence (Prakash et al., 2016; Cao et al., 2017; Li et al., 2018; Ma et al., 2018). All of the existing works learn to paraphrase by mapping a sequence to another, with each word processed and generated in a uniform way. This work is motivated by a commonly observed phenomenon that the paraphrase of a sentence is usually composed of multiple paraphrasing patterns at different levels of granularity, e.g., Specifically, the blue part is the sentence-level pattern, which can be expressed as a pair of sentence templates, where $x can be any fragment of text. The green part is the phrase-level pattern, which is a pair of phrases. Table 1 shows more examples of"
P19-1332,D18-1420,0,0.0287124,"araphrase of the sentence with a given syntax. In this work, we consider automatically learning controllable and interpretable paraphrasing operations from the corpus. This is also the first work to consider scalable unsupervised domain adaptation for neural paraphrase generation. 4.2 Controllable and Interpretable Text Generation There is extensive attention on controllable neural sequence generation and its interpretation. A line of research is based on variational auto-encoder (VAE), which captures the implicit (Gupta et al., 2017; Li et al., 2017) or explicit information (Hu et al., 2017; Liao et al., 2018) via latent representations. Another approach is to integrate probabilistic graphical model, e.g., hidden semi-Markov model (HSMM) into neural network (Wiseman et al., 2018; Dai et al., 2016). In these works, neural templates are learned as a sequential composition of segments controlled by the latent states, and be used for language modeling and data-totext generation. Unfortunately, it is non-trivial to adapt this approach to the Seq2Seq learning framework to extract templates from both the Domain Adaptation for Seq2Seq Learning Domain adaptation for neural paraphrase generation is under-exp"
P19-1332,N18-1018,0,0.276965,"en adapting to a new domain. 1 what is the reason of $x → what makes $x happen world war II → the second world war Introduction Paraphrases are texts that convey the same meaning using different wording. Paraphrase generation is an important technique in natural language processing (NLP), which can be applied in various downstream tasks such as information retrieval, semantic parsing, and dialogue systems. Neural sequence-to-sequence (Seq2Seq) models have demonstrated the superior performances on generating paraphrases given a sentence (Prakash et al., 2016; Cao et al., 2017; Li et al., 2018; Ma et al., 2018). All of the existing works learn to paraphrase by mapping a sequence to another, with each word processed and generated in a uniform way. This work is motivated by a commonly observed phenomenon that the paraphrase of a sentence is usually composed of multiple paraphrasing patterns at different levels of granularity, e.g., Specifically, the blue part is the sentence-level pattern, which can be expressed as a pair of sentence templates, where $x can be any fragment of text. The green part is the phrase-level pattern, which is a pair of phrases. Table 1 shows more examples of paraphrase pairs s"
P19-1332,J03-1002,0,0.0165174,"generating the final paraphrases. The prediction of the token at t-th position is determined by the 2.5 Learning of Separator and Aggregator The proposed model can be trained end-to-end by maximizing the conditional probability (3). However, learning from scratch may not be informative for the separator and aggregator to disentangle the paraphrase patterns in an optimal way. Thus we induce weak supervision to guide the training of the model. We construct the supervision based on a heuristic that long-tail expressions contain more rare words. To this end, we first use the word alignment model (Och and Ney, 2003) to establish the links between the words in the sentence pairs from the paraphrase corpus. Then we assign the label z ∗ = 0 (phrase-level) to n (randomly sampled from {1, 2, 3}) pairs of aligned phrases that contain most rare words. The rest of the words are labeled as z ∗ = 1 (sentence-level). 3406 We train the model with explicit supervision at the beginning, with the following loss function: L= T X log P (yt |y1:t−1 , X)+ t=1 L T X X ∗ λ( log P (zl |X) + log P (zt∗ |y1:t−1 , X)) t=1 l=1 (10) where λ is the hyper-parameter controlling the weight of the explicit supervision. In experiments,"
P19-1332,P02-1040,0,0.104063,"ons, and attention with 9 heads. We use early stopping to prevent the problem of over-fitting. We train the DNPG with Adam optimizer (Kingma and Ba, 2014). We set the learning rate as 5e − 4, τ as 1 and λ as 1 at first, and then decrease them to 1e − 4, 0.9 and 1e − 2 after 3 epochs. We set the hyper-parameters of models and optimization in all other baseline models to remain the same in their original works. We implement our model with PyTorch (Paszke et al., 2017). 3.2 Interpretable Paraphrase Generation First, we evaluate our model quantitatively in terms of automatic metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), which have been widely used in previous works on paraphrase generation. In addition, we include iBLEU (Sun and Zhou, 2012), which penalizes repeating the source sentence in its paraphrase. We use the same hyper-parameter in their original work. We compare DNPG with four existing neural-based models: ResidualLSTM (Prakash et al., 2016), VAE-SVG-eq (Gupta et al., 2017), pointer-generator (See et al., 2017) and the Transformer (Vaswani et al., 2017), the latter two of which have been reported as the state-of-the-art models in Li et al. (2018) and Wang et al. (2018) respective"
P19-1332,C16-1275,0,0.183563,"rt neural models, and significantly better performance when adapting to a new domain. 1 what is the reason of $x → what makes $x happen world war II → the second world war Introduction Paraphrases are texts that convey the same meaning using different wording. Paraphrase generation is an important technique in natural language processing (NLP), which can be applied in various downstream tasks such as information retrieval, semantic parsing, and dialogue systems. Neural sequence-to-sequence (Seq2Seq) models have demonstrated the superior performances on generating paraphrases given a sentence (Prakash et al., 2016; Cao et al., 2017; Li et al., 2018; Ma et al., 2018). All of the existing works learn to paraphrase by mapping a sequence to another, with each word processed and generated in a uniform way. This work is motivated by a commonly observed phenomenon that the paraphrase of a sentence is usually composed of multiple paraphrasing patterns at different levels of granularity, e.g., Specifically, the blue part is the sentence-level pattern, which can be expressed as a pair of sentence templates, where $x can be any fragment of text. The green part is the phrase-level pattern, which is a pair of phras"
P19-1332,P17-1099,0,0.123412,"el with PyTorch (Paszke et al., 2017). 3.2 Interpretable Paraphrase Generation First, we evaluate our model quantitatively in terms of automatic metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), which have been widely used in previous works on paraphrase generation. In addition, we include iBLEU (Sun and Zhou, 2012), which penalizes repeating the source sentence in its paraphrase. We use the same hyper-parameter in their original work. We compare DNPG with four existing neural-based models: ResidualLSTM (Prakash et al., 2016), VAE-SVG-eq (Gupta et al., 2017), pointer-generator (See et al., 2017) and the Transformer (Vaswani et al., 2017), the latter two of which have been reported as the state-of-the-art models in Li et al. (2018) and Wang et al. (2018) respectively. For a fair comparison, we also include a Transformer model with copy mechanism. Table 4 shows the performances of the models, indicating that DNPG achieves competitive performance in terms of all the automatic metrics among all the models. In particular, the DNPG has similar performance with the vanilla Transformer model on Quora dataset, while significantly performs better on WikiAnswers. The reason maybe that the DNPG"
P19-1332,D17-1127,0,0.060833,"nother approach is to integrate probabilistic graphical model, e.g., hidden semi-Markov model (HSMM) into neural network (Wiseman et al., 2018; Dai et al., 2016). In these works, neural templates are learned as a sequential composition of segments controlled by the latent states, and be used for language modeling and data-totext generation. Unfortunately, it is non-trivial to adapt this approach to the Seq2Seq learning framework to extract templates from both the Domain Adaptation for Seq2Seq Learning Domain adaptation for neural paraphrase generation is under-explored. To our best knowledge, Su and Yan (2017)’s work is the only one on this topic. They utilize the pre-trained word embedding and include all the words in both domains to vocabulary, which is tough to scale. Meanwhile, we notice that there is a considerable amount of work on domain adaptation for neural machine translation, another classic sequence-to-sequence learning task. However, most of them require parallel data in the target domain (Wang et al., 2017a,b). In this work, we consider unsupervised domain adaptation, which is more challenging, and there are only two works that are applicable. Gulcehre et al. (2015) use the language m"
P19-1332,P12-2008,0,0.317305,"a, 2014). We set the learning rate as 5e − 4, τ as 1 and λ as 1 at first, and then decrease them to 1e − 4, 0.9 and 1e − 2 after 3 epochs. We set the hyper-parameters of models and optimization in all other baseline models to remain the same in their original works. We implement our model with PyTorch (Paszke et al., 2017). 3.2 Interpretable Paraphrase Generation First, we evaluate our model quantitatively in terms of automatic metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), which have been widely used in previous works on paraphrase generation. In addition, we include iBLEU (Sun and Zhou, 2012), which penalizes repeating the source sentence in its paraphrase. We use the same hyper-parameter in their original work. We compare DNPG with four existing neural-based models: ResidualLSTM (Prakash et al., 2016), VAE-SVG-eq (Gupta et al., 2017), pointer-generator (See et al., 2017) and the Transformer (Vaswani et al., 2017), the latter two of which have been reported as the state-of-the-art models in Li et al. (2018) and Wang et al. (2018) respectively. For a fair comparison, we also include a Transformer model with copy mechanism. Table 4 shows the performances of the models, indicating th"
P19-1332,P17-2089,0,0.0526964,"Missing"
P19-1426,W04-1013,0,0.0135768,"d select an oracle word oracle to simulate the context word. The oracle yj−1 word should be a word similar to the ground truth or a synonym. Using different strategies will prooracle . One option is duce a different oracle word yj−1 that word-level greedy search could be employed to output the oracle word of each step, which is called Word-level Oracle (called WO). Besides, we can further optimize the oracle by enlarging the search space with beam search and then reranking the candidate translations with a sentencelevel metric, e.g. BLEU (Papineni et al., 2002), GLEU (Wu et al., 2016), ROUGE (Lin, 2004), etc, the selected translation is called oracle sentence, the words in the translation are Sentence-level Oracle (denoted as SO). Word-Level Oracle For the {j−1}-th decoding step, the direct way to select the word-level oracle is to pick the word with the highest probability from the word distribution Pj−1 drawn by Equation (9), which is shown in Figure 2. The predicted score in oj−1 is the value before the softmax operation. In practice, we can acquire more robust word-level oracles by introducing the Gumbel-Max technique (Gumbel, 1954; Maddison et al., 2014), which provides a simple and eff"
P19-1426,P02-1040,0,0.104092,"o the ground truth word yj−1 predict yj , thus, we could select an oracle word oracle to simulate the context word. The oracle yj−1 word should be a word similar to the ground truth or a synonym. Using different strategies will prooracle . One option is duce a different oracle word yj−1 that word-level greedy search could be employed to output the oracle word of each step, which is called Word-level Oracle (called WO). Besides, we can further optimize the oracle by enlarging the search space with beam search and then reranking the candidate translations with a sentencelevel metric, e.g. BLEU (Papineni et al., 2002), GLEU (Wu et al., 2016), ROUGE (Lin, 2004), etc, the selected translation is called oracle sentence, the words in the translation are Sentence-level Oracle (denoted as SO). Word-Level Oracle For the {j−1}-th decoding step, the direct way to select the word-level oracle is to pick the word with the highest probability from the word distribution Pj−1 drawn by Equation (9), which is shown in Figure 2. The predicted score in oj−1 is the value before the softmax operation. In practice, we can acquire more robust word-level oracles by introducing the Gumbel-Max technique (Gumbel, 1954; Maddison et"
P19-1426,P16-1162,0,0.357348,"Missing"
P19-1426,D18-1510,1,0.715141,"Missing"
P19-1426,P16-1159,0,0.340677,"ntence-level oracle to relieve the overcorrection problem and neither the noise perturbations on the predicted distribution. Another direction of attempts is the sentencelevel training with the thinking that the sentencelevel metric, e.g., BLEU, brings a certain degree of flexibility for generation and hence is more robust to mitigate the exposure bias problem. To avoid the problem of exposure bias, Ranzato et al. (2015) presented a novel algorithm Mixed Incremental Cross-Entropy Reinforce (MIXER) for sequence-level training, which directly optimized the sentence-level BLEU used at inference. Shen et al. (2016) introduced the Minimum Risk Training (MRT) into the end-to-end NMT model, which optimized model parameters by minimizing directly the expected loss with respect to arbitrary evaluation metrics, e.g., sentence-level BLEU. Shao et al. (2018) proposed to eliminate the exposure bias through a probabilistic n-gram matching objective, which trains NMT NMT under the greedy decoding strategy. 5 Experiments 5.1 For Zh→En, the training dataset consists of 1.25M sentence pairs extracted from LDC corpora1 . We choose the NIST 2002 (MT02) dataset as the validation set, which has 878 sentences, and the NIS"
P19-1426,P16-1008,0,0.105283,"odel parameters with batch size setting to 80. Moreover, the learning rate is adjusted by adadelta optimizer (Zeiler, 2012) with ρ=0.95 and =1e-6. Dropout is applied on the output layer with dropout rate being 0.5. For Transformer model, we train base model with 1 We carry out experiments on the NIST Chinese→English (Zh→En) and the WMT’14 English→German (En→De) translation tasks. Settings These sentence pairs are mainly extracted from LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 2 http://www.statmt.org/wmt14/ translation-task.html 4338 Systems Tu et al. (2016) Shen et al. (2016) Zhang et al. (2017) this work Architecture MT03 MT04 MT05 Existing end-to-end NMT systems Coverage 33.69 38.05 35.01 MRT 37.41 39.87 37.45 Distortion 37.93 40.40 36.81 Our end-to-end NMT systems RNNsearch 37.93 40.53 36.65 + SS-NMT 38.82 41.68 37.28 + MIXER 38.70 40.81 37.59 + OR-NMT 40.40‡†? 42.63‡†? 38.87‡†? Transformer 46.89 47.88 47.40 + word oracle 47.42 48.34 47.89 + sentence oracle 48.31∗ 49.40∗ 48.72∗ MT06 Average 34.83 36.80 35.77 35.40 37.88 37.73 35.80 37.98 38.38 38.44‡ 46.66 47.34 48.45∗ 37.73 38.94 38.87 40.09 47.21 47.75 48.72 Table 1: Case-insensitive BLEU s"
P19-1426,1983.tc-1.13,0,0.438724,"Missing"
P19-1426,P17-1140,1,0.844345,"nhance the overcorrection recovery capacity. For the sentencelevel oracle selection, we set the beam size to be 3, set τ =0.5 in Equation (11) and µ=12 for the decay function in Equation (15). OR-NMT is the abbreviation of NMT with Overcorrection Recovery. 3 https://github.com/pytorch/fairseq Results on Zh→En Translation Results on the RNNsearch As shown in Table 1, Tu et al. (2016) propose to model coverage in RNN-based NMT to improve the adequacy of translations. Shen et al. (2016) propose minimum risk training (MRT) for NMT to directly optimize model parameters with respect to BLEU scores. Zhang et al. (2017) model distortion to enhance the attention model. Compared with them, our baseline system RNNsearch 1) outperforms previous shallow RNN-based NMT system equipped with the coverage model (Tu et al., 2016); and 2) achieves competitive performance with the MRT (Shen et al., 2016) and the Distortion (Zhang et al., 2017) on the same datasets. We hope that the strong shallow baseline system used in this work makes the evaluation convincing. We also compare with the other two related methods that aim at solving the exposure bias problem, including the scheduled sampling (Bengio et al., 2015) (SS-NMT)"
P19-1426,P05-1066,0,\N,Missing
P19-1571,D10-1115,0,0.327112,"2016). In the field of NLP, SC has proved effective in many tasks including language model∗ Indicates equal contribution † Work done during internship at Tsinghua University ‡ Corresponding author (1) where f is the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of hu"
P19-1571,D12-1050,0,0.364815,"p at Tsinghua University ‡ Corresponding author (1) where f is the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926). It is believed that the meanings of all the 1 This formula only applies to two-word MWEs but can be easily exte"
P19-1571,W00-1213,0,0.185735,"ole of external knowledge in SC. Zhu et al. (2016) incorporate prior sentimental knowledge into LSTM models, aiming to improve sentiment analysis performance of sentences. To the best our knowledge, there is no work trying to take account of general linguistic knowledge in SC, especially for the MWE representation learning task. 5.2 Sememes and HowNet HowNet, as the most well-known sememe KB, has attracted wide research attention. Previous work applies the sememe knowledge of HowNet to various NLP applications, such as word similarity computation (Liu and Li, 2002), word sense disambiguation (Gan and Wong, 2000; Zhang et al., 2005; Duan et al., 2007), sentiment analysis (Zhu et al., 2006; Dang and Zhang, 2010; Fu et al., 2013), word representation learning (Niu et al., 2017), language modeling (Gu et al., 2018), lexicon expansion (Zeng et al., 2018) and semantic rationality evaluation (Liu et al., 2018). To tackle the challenge of high cost of annotating sememes for new words, Xie et al. (2017) propose the task of automatic sememe prediction to facilitate sememe annotation. And they also propose two simple but effective models. Jin et al. 5713 (2018) further incorporate Chinese character information"
P19-1571,W13-0112,0,0.0153263,"the development of distributional semantics, vector-based SC modeling has been extensively studied in recent years. Most existing work concentrates on using better compositionality functions. Mitchell and Lapata (2008) first make a detailed comparison of several simple compositionality functions including addition and element-wise multiplication. Then various complicated models are proposed in succession, such as vector-matrix models (Baroni and Zamparelli, 2010; Socher et al., 2012), matrix-space models (Yessenalina and Cardie, 2011; Grefenstette and Sadrzadeh, 2011) and tensor-based models (Grefenstette et al., 2013; Van de Cruys et al., 2013; Socher et al., 2013b). There are also some works trying to integrate combination rules into semantic composition models (Blacoe and Lapata, 2012; Zhao et al., 2015; Kober et al., 2016; Weir et al., 2016). But few works explore the role of external knowledge in SC. Zhu et al. (2016) incorporate prior sentimental knowledge into LSTM models, aiming to improve sentiment analysis performance of sentences. To the best our knowledge, there is no work trying to take account of general linguistic knowledge in SC, especially for the MWE representation learning task. 5.2 Seme"
P19-1571,D11-1129,0,0.357139,"C has proved effective in many tasks including language model∗ Indicates equal contribution † Work done during internship at Tsinghua University ‡ Corresponding author (1) where f is the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926)."
P19-1571,D18-1493,1,0.890044,"es of constituents are shared with the constituents’ corresponding MWE. words can be composed of a limited set of sememes, which is similar to the idea of semantic primes (Wierzbicka, 1996). HowNet (Dong and Dong, 2003) is a widely acknowledged sememe knowledge base (KB), which defines about 2,000 sememes and uses them to annotate over 100,000 Chinese words together with their English translations. Sememes and HowNet have been successfully utilized in a variety of NLP tasks including sentiment analysis (Dang and Zhang, 2010), word representation learning (Niu et al., 2017), language modeling (Gu et al., 2018), etc. In this paper, we argue that sememes are beneficial to modeling SC2 . To verify this, we first design a simple SC degree (SCD) measurement experiment and find that the SCDs of MWEs computed by simple sememe-based formulae are highly correlated with human judgment. This result shows that sememes can finely depict meanings of MWEs and their constituents, and capture the semantic relations between the two sides. Therefore, we believe that sememes are appropriate for modeling SC and can improve the performance of SC-related tasks like MWE representation learning. We propose two sememe-incor"
P19-1571,P18-1227,1,0.920458,"MWE p, its training loss is: Lp = kpc − pr k22 , (9) where pc ∈ Rd is the embedding of p obtained by our SC models , i.e., previous p, and pr ∈ Rd is the corresponding reference embedding, which might be obtained by regarding the MWE as a whole and applying word representation learning methods. And the overall loss function is as follows: L= X p∈Pt Lp + λX kθk22 , 2 (10) θ∈Θ where Pt is the training set, Θ refers to the parameter set including Wc and Wa , and λ is the regularization parameter. 5709 Training for MWE Sememe Prediction Sememe prediction is a well-defined task (Xie et al., 2017; Jin et al., 2018; Qi et al., 2018), aimed at selecting appropriate sememes for unannotated words or phrases from the set of all the sememes. Existing works model sememe prediction as a multi-label classification problem, where sememes are regarded as the labels of words and phrases. For doing MWE sememe prediction, we employ a single-layer perceptron as the classifier: (Pennington et al., 2014) on the Sogou-T corpus4 . We also utilize pretrained sememe embeddings obtained from the results of a sememe-based word representation learning model5 (Niu et al., 2017). And we build a dataset consisting of 51,034 Chin"
P19-1571,D16-1175,0,0.405979,"the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926). It is believed that the meanings of all the 1 This formula only applies to two-word MWEs but can be easily extended to longer MWEs. In fact, we also focus on modeling SC"
P19-1571,O02-2003,1,0.5746,"eir et al., 2016). But few works explore the role of external knowledge in SC. Zhu et al. (2016) incorporate prior sentimental knowledge into LSTM models, aiming to improve sentiment analysis performance of sentences. To the best our knowledge, there is no work trying to take account of general linguistic knowledge in SC, especially for the MWE representation learning task. 5.2 Sememes and HowNet HowNet, as the most well-known sememe KB, has attracted wide research attention. Previous work applies the sememe knowledge of HowNet to various NLP applications, such as word similarity computation (Liu and Li, 2002), word sense disambiguation (Gan and Wong, 2000; Zhang et al., 2005; Duan et al., 2007), sentiment analysis (Zhu et al., 2006; Dang and Zhang, 2010; Fu et al., 2013), word representation learning (Niu et al., 2017), language modeling (Gu et al., 2018), lexicon expansion (Zeng et al., 2018) and semantic rationality evaluation (Liu et al., 2018). To tackle the challenge of high cost of annotating sememes for new words, Xie et al. (2017) propose the task of automatic sememe prediction to facilitate sememe annotation. And they also propose two simple but effective models. Jin et al. 5713 (2018) fu"
P19-1571,D11-1014,0,0.0408112,"-based word representation learning model5 (Niu et al., 2017). And we build a dataset consisting of 51,034 Chinese MWEs, each of which and its two constituents are annotated with sememes in HowNet and have pretrained word embeddings simultaneously. We randomly split the dataset into training, validation and test sets in the ratio of 8 : 1 : 1. ˆ p = σ(Ws · p), y Baseline Methods We choose several typical SC models as the baseline methods, including: (1) ADD and MUL, the simple additive and elementwise multiplicative models (Mitchell and Lapata, 2008); (2) RAE, the recursive autoencoder model (Socher et al., 2011); (3) RNTN, the recursive neural tensor network (Socher et al., 2013b); (4) TIM, the tensor index model (Zhao et al., 2015); and (5) SCAS-S, the ablated version of our SCAS model which removes sememe knowledge6 . These baseline methods range from the simplest additive model to complicated tensor-based model, all of which take no knowledge into consideration. (11) ˆ p ∈ R|S |, Ws ∈ R|S|×d and σ is the sigwhere y ˆ p , demoid function. [ˆ yp ]i , the i-th element of y notes the predicted score of i-th sememe, where the higher the score is, the more probable the sememe is selected. And Ws = [s1 ,"
P19-1571,D13-1170,0,0.606441,"ao Qi1∗, Junjie Huang2∗†, Chenghao Yang3† , Zhiyuan Liu1 , Xiao Chen4 , Qun Liu4 , Maosong Sun1‡ 1 Department of Computer Science and Technology, Tsinghua University Institute for Artificial Intelligence, Tsinghua University State Key Lab on Intelligent Technology and Systems, Tsinghua University 2 School of ASEE, Beihang University 3 Software College, Beihang Unviersity 4 Huawei Noah’s Ark Lab qfc17@mails.tsinghua.edu.cn, {hjj1997,alanyang}@buaa.edu.cn {liuzy,sms}@tsinghua.edu.cn, {chen.xiao2,qun.liu}@huawei.com Abstract ing (Mitchell and Lapata, 2009), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), syntactic parsing (Socher et al., 2013a), etc. Most literature on SC pays attention to using vector-based distributional models of semantics to learn representations of multiword expressions (MWEs), i.e., embeddings of phrases or compounds. Mitchell and Lapata (2008) conduct a pioneering work in which they introduce a general framework to formulate this task: Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to mod"
P19-1571,N13-1134,0,0.0453151,"Missing"
P19-1571,J16-4006,0,0.170424,"hor (1) where f is the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926). It is believed that the meanings of all the 1 This formula only applies to two-word MWEs but can be easily extended to longer MWEs. In fact, we also"
P19-1571,W13-3512,0,0.121261,"Missing"
P19-1571,P11-1015,0,0.0702493,"eme Knowledge Fanchao Qi1∗, Junjie Huang2∗†, Chenghao Yang3† , Zhiyuan Liu1 , Xiao Chen4 , Qun Liu4 , Maosong Sun1‡ 1 Department of Computer Science and Technology, Tsinghua University Institute for Artificial Intelligence, Tsinghua University State Key Lab on Intelligent Technology and Systems, Tsinghua University 2 School of ASEE, Beihang University 3 Software College, Beihang Unviersity 4 Huawei Noah’s Ark Lab qfc17@mails.tsinghua.edu.cn, {hjj1997,alanyang}@buaa.edu.cn {liuzy,sms}@tsinghua.edu.cn, {chen.xiao2,qun.liu}@huawei.com Abstract ing (Mitchell and Lapata, 2009), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), syntactic parsing (Socher et al., 2013a), etc. Most literature on SC pays attention to using vector-based distributional models of semantics to learn representations of multiword expressions (MWEs), i.e., embeddings of phrases or compounds. Mitchell and Lapata (2008) conduct a pioneering work in which they introduce a general framework to formulate this task: Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositiona"
P19-1571,P08-1028,0,0.646777,"and Systems, Tsinghua University 2 School of ASEE, Beihang University 3 Software College, Beihang Unviersity 4 Huawei Noah’s Ark Lab qfc17@mails.tsinghua.edu.cn, {hjj1997,alanyang}@buaa.edu.cn {liuzy,sms}@tsinghua.edu.cn, {chen.xiao2,qun.liu}@huawei.com Abstract ing (Mitchell and Lapata, 2009), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), syntactic parsing (Socher et al., 2013a), etc. Most literature on SC pays attention to using vector-based distributional models of semantics to learn representations of multiword expressions (MWEs), i.e., embeddings of phrases or compounds. Mitchell and Lapata (2008) conduct a pioneering work in which they introduce a general framework to formulate this task: Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to model SC while few works consider external knowledge in models. In this paper, we verify the effectiveness of sememes, the minimum semantic units of human languages, in modeling SC by a confirmatory experiment. Furthermore, we make the first attempt to incorporate sememe kn"
P19-1571,D11-1016,0,0.0347472,"del can take advantage of sememes. 5 5.1 Related Work Semantic Compositionality Based on the development of distributional semantics, vector-based SC modeling has been extensively studied in recent years. Most existing work concentrates on using better compositionality functions. Mitchell and Lapata (2008) first make a detailed comparison of several simple compositionality functions including addition and element-wise multiplication. Then various complicated models are proposed in succession, such as vector-matrix models (Baroni and Zamparelli, 2010; Socher et al., 2012), matrix-space models (Yessenalina and Cardie, 2011; Grefenstette and Sadrzadeh, 2011) and tensor-based models (Grefenstette et al., 2013; Van de Cruys et al., 2013; Socher et al., 2013b). There are also some works trying to integrate combination rules into semantic composition models (Blacoe and Lapata, 2012; Zhao et al., 2015; Kober et al., 2016; Weir et al., 2016). But few works explore the role of external knowledge in SC. Zhu et al. (2016) incorporate prior sentimental knowledge into LSTM models, aiming to improve sentiment analysis performance of sentences. To the best our knowledge, there is no work trying to take account of general lin"
P19-1571,D09-1045,0,0.0970384,"Missing"
P19-1571,P17-1187,1,0.86977,"tion formulae and examples. Bold sememes of constituents are shared with the constituents’ corresponding MWE. words can be composed of a limited set of sememes, which is similar to the idea of semantic primes (Wierzbicka, 1996). HowNet (Dong and Dong, 2003) is a widely acknowledged sememe knowledge base (KB), which defines about 2,000 sememes and uses them to annotate over 100,000 Chinese words together with their English translations. Sememes and HowNet have been successfully utilized in a variety of NLP tasks including sentiment analysis (Dang and Zhang, 2010), word representation learning (Niu et al., 2017), language modeling (Gu et al., 2018), etc. In this paper, we argue that sememes are beneficial to modeling SC2 . To verify this, we first design a simple SC degree (SCD) measurement experiment and find that the SCDs of MWEs computed by simple sememe-based formulae are highly correlated with human judgment. This result shows that sememes can finely depict meanings of MWEs and their constituents, and capture the semantic relations between the two sides. Therefore, we believe that sememes are appropriate for modeling SC and can improve the performance of SC-related tasks like MWE representation"
P19-1571,D14-1162,0,0.0942188,") θ∈Θ where Pt is the training set, Θ refers to the parameter set including Wc and Wa , and λ is the regularization parameter. 5709 Training for MWE Sememe Prediction Sememe prediction is a well-defined task (Xie et al., 2017; Jin et al., 2018; Qi et al., 2018), aimed at selecting appropriate sememes for unannotated words or phrases from the set of all the sememes. Existing works model sememe prediction as a multi-label classification problem, where sememes are regarded as the labels of words and phrases. For doing MWE sememe prediction, we employ a single-layer perceptron as the classifier: (Pennington et al., 2014) on the Sogou-T corpus4 . We also utilize pretrained sememe embeddings obtained from the results of a sememe-based word representation learning model5 (Niu et al., 2017). And we build a dataset consisting of 51,034 Chinese MWEs, each of which and its two constituents are annotated with sememes in HowNet and have pretrained word embeddings simultaneously. We randomly split the dataset into training, validation and test sets in the ratio of 8 : 1 : 1. ˆ p = σ(Ws · p), y Baseline Methods We choose several typical SC models as the baseline methods, including: (1) ADD and MUL, the simple additive a"
P19-1571,D18-1033,1,0.906717,"ng loss is: Lp = kpc − pr k22 , (9) where pc ∈ Rd is the embedding of p obtained by our SC models , i.e., previous p, and pr ∈ Rd is the corresponding reference embedding, which might be obtained by regarding the MWE as a whole and applying word representation learning methods. And the overall loss function is as follows: L= X p∈Pt Lp + λX kθk22 , 2 (10) θ∈Θ where Pt is the training set, Θ refers to the parameter set including Wc and Wa , and λ is the regularization parameter. 5709 Training for MWE Sememe Prediction Sememe prediction is a well-defined task (Xie et al., 2017; Jin et al., 2018; Qi et al., 2018), aimed at selecting appropriate sememes for unannotated words or phrases from the set of all the sememes. Existing works model sememe prediction as a multi-label classification problem, where sememes are regarded as the labels of words and phrases. For doing MWE sememe prediction, we employ a single-layer perceptron as the classifier: (Pennington et al., 2014) on the Sogou-T corpus4 . We also utilize pretrained sememe embeddings obtained from the results of a sememe-based word representation learning model5 (Niu et al., 2017). And we build a dataset consisting of 51,034 Chinese MWEs, each of"
P19-1571,P13-1045,0,0.687064,"ao Qi1∗, Junjie Huang2∗†, Chenghao Yang3† , Zhiyuan Liu1 , Xiao Chen4 , Qun Liu4 , Maosong Sun1‡ 1 Department of Computer Science and Technology, Tsinghua University Institute for Artificial Intelligence, Tsinghua University State Key Lab on Intelligent Technology and Systems, Tsinghua University 2 School of ASEE, Beihang University 3 Software College, Beihang Unviersity 4 Huawei Noah’s Ark Lab qfc17@mails.tsinghua.edu.cn, {hjj1997,alanyang}@buaa.edu.cn {liuzy,sms}@tsinghua.edu.cn, {chen.xiao2,qun.liu}@huawei.com Abstract ing (Mitchell and Lapata, 2009), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), syntactic parsing (Socher et al., 2013a), etc. Most literature on SC pays attention to using vector-based distributional models of semantics to learn representations of multiword expressions (MWEs), i.e., embeddings of phrases or compounds. Mitchell and Lapata (2008) conduct a pioneering work in which they introduce a general framework to formulate this task: Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to mod"
P19-1571,N16-1106,0,0.372405,"WE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926). It is believed that the meanings of all the 1 This formula only applies to two-word MWEs but can be easily extended to longer MWEs. In fact, we also focus on modeling SC for two-word MWEs in this paper because they are the most co"
P19-1571,D12-1110,0,0.539642,"ks including language model∗ Indicates equal contribution † Work done during internship at Tsinghua University ‡ Corresponding author (1) where f is the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926). It is believed that t"
S12-1073,W09-1201,0,0.0722852,"Missing"
S12-1073,P10-1110,0,0.0125594,"on As mentioned, we employ three single dependency parsers to generate respect dependency structure. To further improve the accuracy of dependency structure construction, we blend the syntactic outputs and find a better dependency structure. In the followings, we will first introduce the details of our strategy for dependency structure construction. 2.1 Parsers We implement three transition-based dependency parsers with three different parsing algorithms: Nivre’s arc standard, Nivre’s arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Liang’s dynamic algorithm(Huang and Sagae, 2010). We use these algorithms for several reasons: first, they are easy to implement and their reported performance are approaching to state-of-the-art. Second, their outputs are projective, which is consistent with given corpus. 2.2 Parser Combination We use the similar method presented in Hall et al. (2011) to advance the accuracy of parses. The parses of each sentence are combined into a weighted directed graph. The left procedure is similar to traditional graph-based dependency parsing except that the number of edges in our system is smaller since we reserve best edges predicted by three singl"
S12-1073,W03-3017,0,0.132905,"Missing"
S12-1073,W04-0308,0,0.0123171,"detailed information of our system, and report several experimental results. 2 System Description As mentioned, we employ three single dependency parsers to generate respect dependency structure. To further improve the accuracy of dependency structure construction, we blend the syntactic outputs and find a better dependency structure. In the followings, we will first introduce the details of our strategy for dependency structure construction. 2.1 Parsers We implement three transition-based dependency parsers with three different parsing algorithms: Nivre’s arc standard, Nivre’s arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Liang’s dynamic algorithm(Huang and Sagae, 2010). We use these algorithms for several reasons: first, they are easy to implement and their reported performance are approaching to state-of-the-art. Second, their outputs are projective, which is consistent with given corpus. 2.2 Parser Combination We use the similar method presented in Hall et al. (2011) to advance the accuracy of parses. The parses of each sentence are combined into a weighted directed graph. The left procedure is similar to traditional graph-based dependency parsing exce"
S12-1073,W08-2121,0,0.0965106,"Missing"
S12-1073,S12-1050,0,0.0214967,"In this year’s Semantic Evaluation Task, the organizers hold a task for Chinese Semantic Dependency Parsing. The semantic dependency parsing (SDP) is a kind of dependency parsing. It builds a dependency structure for a sentence and labels the semantic relation between a head and its modifier. The semantic relations are different from syntactic relations. They are position independent, e.g., the patient can be before or behind a predicate. On the other hand, their grains are finer than syntactic relations, e.g., the syntactic subject can be agent or experiencer. Readers can refer to (Wanxiang Che, 2012) for detailed introduction. Different from most methods proposed in CoNLL-2008 1 and 2009 2 , in which some researchers build a joint model to simultaneously generate dependency structure and its syntactic relations (Surdeanu et al., 2008; Hajiˇc et al., 2009), here, we first employ several parsers to generate dependency structure and then propose a method to combine their outputs. After that, we label relation between each head and its modifier via the traversal of this refined parse tree. The reason why we use a pipeline model while not a joint model is that the number of semantic relations"
S12-1108,H05-1079,0,0.0376177,"y treat the input expressions simply as surface strings, they may operate on syntactic or semantic representations of the input expressions, or on representations combining information from different 1 http://edits.fbk.eu/ 715 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 715–720, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics levels. Logic-based approach is to map the language expressions to logical meaning representations, and then rely on logical entailment checks, possibly by invoking theorem provers (Rinaldi et al., 2003; Bos & Markert, 2005; Tatu & Moldovan, 2005, 2007). An alternative to use logical meaning representations is to start by mapping each word of the input language expressions to a vector that shows how strongly the word co-occurs with particular other words in corpora (Lin, 1998b), possibly also taking into account syntactic information, for example requiring that the co-occurring words participate in particular syntactic dependencies (Pad´o & Lapata, 2007). Several textual entailment recognizing methods operate directly on the input surface strings. For example, they compute the string edit distance (Levenshtein,"
S12-1108,P05-1033,0,0.0209569,"ramework of our system, where a machine translation model is employed to translate foreign language into English, since original EDITS could only deal with the text in the same language pairs. 716 In the following of this section, we will describe the translation module and configuration of EDITS in details. Figure 1: The framework of our system. 2.1 Machine Translation Recently, machine translation has attracted intensive attention and has been well studied in natural language community. Effective models, such as Phrase-Based model (Koehn et al., 2003), Hierarchical Phrase-Based model (HPB) (Chiang, 2005), and Syntax-Based (Liu et al., 2006) model have been proposed to improve the translation quality. However, since current translation models require parallel corpus to extract translation rules, while parallel corpus on some language pairs such as Italian-English and Spanish-English are hard to obtain, therefore, we could use Google Translation Toolkit (GTT) to generate translation. Specifically, WMT 2 released some bilingual corpus for training, thus we use some portion to train a French-English translation engine using hierarchical phrase-based model. We also exploit system combination techn"
S12-1108,W07-1401,0,0.0260318,"nt (TE) recognition under a new dimension (cross-linguality), and within a new challenging application scenario (content synchronization) Readers can refer to M. Negri et al. 2012.s., for more detailed introduction. 1 Textual entailment, on the other hand, recognize, generate, or extract pairs of natural language expressions, and infer that if one element is true, whether the other element is also true. Several methods are proposed by previous researchers. There have been some workshops on textual entailment in recent years. The recognizing textual entailment challenges (Bar-Haim et al. 2006; Giampiccolo, Magnini, Dagan, & Dolan, 2007; Giampiccolo, Dang, Magnini, Dagan, & Dolan, 2008), currently in the 7th year, provide additional significant thrust. Consequently, there are a large number of published articles, proposed methods, and resources related to textual entailment. A special issue on textual entailment was also recently published, and its editorial provides a brief overview of textual entailment methods (Dagan, Dolan, Magnini, & Roth, 2009). Textual entailment recognizers judge whether or not two given language expressions constitute a correct textual entailment pair. Different methods may operate at different leve"
S12-1108,P10-4008,0,0.330664,"Missing"
S12-1108,W07-1407,0,0.133475,"the input language expressions to a vector that shows how strongly the word co-occurs with particular other words in corpora (Lin, 1998b), possibly also taking into account syntactic information, for example requiring that the co-occurring words participate in particular syntactic dependencies (Pad´o & Lapata, 2007). Several textual entailment recognizing methods operate directly on the input surface strings. For example, they compute the string edit distance (Levenshtein, 1966) of the two input strings, the number of their common words, or combinations of several string similarity measures (Malakasiotis & Androutsopoulos, 2007). Dependency grammar parsers (Melcuk, 1987; Kubler, McDonald, & Nivre, 2009) are popular in textual entailment research. However, cross-lingual textual entailment brings some problems on past algorithms. On the other hand, many methods can’t be applied to it directly. In this paper, we propose a translation based method for cross-lingual textual entailment, which has been described in Mehdad et al. 2010. First, we translate one part of the text, which termed as “t1” and written in one language, into English, which termed as “t2”. Then, we use EDITS, an open source package, to recognize entailm"
S12-1108,N10-1045,0,0.238488,", they compute the string edit distance (Levenshtein, 1966) of the two input strings, the number of their common words, or combinations of several string similarity measures (Malakasiotis & Androutsopoulos, 2007). Dependency grammar parsers (Melcuk, 1987; Kubler, McDonald, & Nivre, 2009) are popular in textual entailment research. However, cross-lingual textual entailment brings some problems on past algorithms. On the other hand, many methods can’t be applied to it directly. In this paper, we propose a translation based method for cross-lingual textual entailment, which has been described in Mehdad et al. 2010. First, we translate one part of the text, which termed as “t1” and written in one language, into English, which termed as “t2”. Then, we use EDITS, an open source package, to recognize entailment relations between two parts. Large-scale experiments are conducted on four language pairs, French-English, Spanish-English, Italian-English and GermanEnglish. Although our method achieves promising results reported by organizers, it is still far from perfect compared to other participants. The remainder of this paper is organized as follows. We describe our system framework in section 2. We report e"
S12-1108,S12-1053,0,0.0420748,"idirectional prediction, thus we exchange the hypothesis and test to detect entailment in another direction. Experimental results show that our method achieves promising results but not perfect results compared to other participants. 1 Introduction In Cross-Lingual Textual Entailment task (CLTE) of 2012, the organizers hold a task for CrossLingual Textual Entailment. The Cross-Lingual Textual Entailment task addresses textual entailment (TE) recognition under a new dimension (cross-linguality), and within a new challenging application scenario (content synchronization) Readers can refer to M. Negri et al. 2012.s., for more detailed introduction. 1 Textual entailment, on the other hand, recognize, generate, or extract pairs of natural language expressions, and infer that if one element is true, whether the other element is also true. Several methods are proposed by previous researchers. There have been some workshops on textual entailment in recent years. The recognizing textual entailment challenges (Bar-Haim et al. 2006; Giampiccolo, Magnini, Dagan, & Dolan, 2007; Giampiccolo, Dang, Magnini, Dagan, & Dolan, 2008), currently in the 7th year, provide additional significant thrust. Consequently, ther"
S12-1108,D11-1062,0,0.213452,"Missing"
S12-1108,J07-2002,0,0.0279302,"Missing"
S12-1108,N03-1017,0,0.00756406,"Missing"
S12-1108,W03-1604,0,0.0226145,". For example, they may treat the input expressions simply as surface strings, they may operate on syntactic or semantic representations of the input expressions, or on representations combining information from different 1 http://edits.fbk.eu/ 715 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 715–720, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics levels. Logic-based approach is to map the language expressions to logical meaning representations, and then rely on logical entailment checks, possibly by invoking theorem provers (Rinaldi et al., 2003; Bos & Markert, 2005; Tatu & Moldovan, 2005, 2007). An alternative to use logical meaning representations is to start by mapping each word of the input language expressions to a vector that shows how strongly the word co-occurs with particular other words in corpora (Lin, 1998b), possibly also taking into account syntactic information, for example requiring that the co-occurring words participate in particular syntactic dependencies (Pad´o & Lapata, 2007). Several textual entailment recognizing methods operate directly on the input surface strings. For example, they compute the string edit di"
S12-1108,W07-1404,0,0.0509293,"Missing"
S12-1108,P06-1077,1,0.857523,"achine translation model is employed to translate foreign language into English, since original EDITS could only deal with the text in the same language pairs. 716 In the following of this section, we will describe the translation module and configuration of EDITS in details. Figure 1: The framework of our system. 2.1 Machine Translation Recently, machine translation has attracted intensive attention and has been well studied in natural language community. Effective models, such as Phrase-Based model (Koehn et al., 2003), Hierarchical Phrase-Based model (HPB) (Chiang, 2005), and Syntax-Based (Liu et al., 2006) model have been proposed to improve the translation quality. However, since current translation models require parallel corpus to extract translation rules, while parallel corpus on some language pairs such as Italian-English and Spanish-English are hard to obtain, therefore, we could use Google Translation Toolkit (GTT) to generate translation. Specifically, WMT 2 released some bilingual corpus for training, thus we use some portion to train a French-English translation engine using hierarchical phrase-based model. We also exploit system combination technique (A Rosti et al., 2007) to improv"
S12-1108,H05-1047,0,\N,Missing
S12-1108,P07-1040,0,\N,Missing
S12-1108,P11-1134,0,\N,Missing
W02-1117,J97-4004,0,0.0447196,"Missing"
W02-1117,J96-3004,0,0.0910491,"Missing"
W03-1709,Y98-1021,0,\N,Missing
W03-1709,W02-1808,0,\N,Missing
W03-1709,C02-1055,0,\N,Missing
W03-1709,W02-1817,1,\N,Missing
W03-1709,C02-1080,0,\N,Missing
W03-1709,W02-1815,0,\N,Missing
W03-1709,C02-1012,0,\N,Missing
W03-1709,P97-1041,0,\N,Missing
W03-1709,J00-3004,0,\N,Missing
W03-1730,W02-1817,1,\N,Missing
W07-0706,2005.iwslt-1.8,0,0.0933759,"Missing"
W07-0706,W06-1606,0,0.375434,"ng Correspondence A dependency treelet string correspondence π is a triple &lt; D, S, A &gt; which describes a translation pair &lt; D, S &gt; and their alignment A, where D is the dependency treelet on the source side and S is the translation string on the target side. &lt; D, S &gt; must be consistent with the word alignment M of the corresponding sentence pair ∀(i, j) ∈ M, i ∈ D ↔ j ∈ S A treelet is defined to be any connected subgraph, which is similar to the definition in (Quirk et al., 2005). Treelet is more representatively flexible than subtree which is widely used in models based on phrase structures (Marcu et al., 2006; Liu et al., 2006). The most important distinction between the treelet in (Quirk et al., 2005) and ours is that we allow variables at positions of subnodes. In our definition, the root node must be lexicalized but the subnodes can be replaced with a wild card. The target counterpart of a wildcard node in S is also replaced with a wild card. The wildcards introduced in this way generalize DTSC to match dependency structures with the same head word but with different modifiers or arguments. Another unique feature of our DTSC is that we allow target strings with gaps between words or wildcards."
W07-0706,2004.tmi-1.5,0,0.136619,"− LM (slr ) (3) where LM is the logarithm of the language model probability. We only need to compute the increment of the language model score: 4LM = LM (srl slr ) − LM (srl ) − LM (slr ) for each node n of the input tree T , in bottom-up order do Get all matched DTSCs rooted at n for each matched DTSC π do for each wildcard node n∗ in π do Substitute the corresponding wildcard on the target side with translations from the stack of n∗ end for for each uncovered node n@ by π do Attach the translations from the stack of n@ to the target side at the attaching point end for end for end for (4) 44 Melamed (2004) also used a similar way to integrate the language model. 5 Decoding Our decoding algorithm is similar to the bottom-up chart parsing. The distinction is that the input is a tree rather than a string and therefore the chart is indexed by nodes of the tree rather than spans of the string. Also, several other tree-based decoding algorithms introduced by Eisner (2003), Quirk et al. (2005) and Liu et al. (2006) can be classified as the chart-style parsing algorithm too. Our decoding algorithm is shown in Figure 4. Given an input dependency tree, firstly we generate the bottom-up order by postorder"
W07-0706,C04-1090,0,0.377464,", generalization, and handling discontinuous phrases which are very desirable for machine translation. We finally evaluate our current implementation of a simplified version of DTSC for statistical machine translation. 1 Introduction Over the last several years, various statistical syntaxbased models were proposed to extend traditional In this paper, we propose a novel model based on dependency structures: Dependency Treelet String Correspondence Model (DTSC). The DTSC model maps source dependency structures to target strings. It just needs a source language parser. In contrast to the work by Lin (2004) and by Quirk et al. (2005), the DTSC model does not need to generate target language dependency structures using source structures and word alignments. On the source side, we extract treelets which are any connected subgraphs and consistent with word alignments. While on the target side, we allow the aligned target sequences to be generalized and discontinuous by introducing variables and gaps. The variables on the target side are aligned to the corresponding variables of treelets, while gaps between words or variables are corresponding to the uncovered nodes which are not included by treelet"
W07-0706,P06-1077,1,0.950964,"dependency treelet string correspondence π is a triple &lt; D, S, A &gt; which describes a translation pair &lt; D, S &gt; and their alignment A, where D is the dependency treelet on the source side and S is the translation string on the target side. &lt; D, S &gt; must be consistent with the word alignment M of the corresponding sentence pair ∀(i, j) ∈ M, i ∈ D ↔ j ∈ S A treelet is defined to be any connected subgraph, which is similar to the definition in (Quirk et al., 2005). Treelet is more representatively flexible than subtree which is widely used in models based on phrase structures (Marcu et al., 2006; Liu et al., 2006). The most important distinction between the treelet in (Quirk et al., 2005) and ours is that we allow variables at positions of subnodes. In our definition, the root node must be lexicalized but the subnodes can be replaced with a wild card. The target counterpart of a wildcard node in S is also replaced with a wild card. The wildcards introduced in this way generalize DTSC to match dependency structures with the same head word but with different modifiers or arguments. Another unique feature of our DTSC is that we allow target strings with gaps between words or wildcards. Since source treele"
W07-0706,P03-1021,0,0.0595132,"Missing"
W07-0706,P00-1056,0,0.206099,"Missing"
W07-0706,P05-1034,0,0.657821,"and handling discontinuous phrases which are very desirable for machine translation. We finally evaluate our current implementation of a simplified version of DTSC for statistical machine translation. 1 Introduction Over the last several years, various statistical syntaxbased models were proposed to extend traditional In this paper, we propose a novel model based on dependency structures: Dependency Treelet String Correspondence Model (DTSC). The DTSC model maps source dependency structures to target strings. It just needs a source language parser. In contrast to the work by Lin (2004) and by Quirk et al. (2005), the DTSC model does not need to generate target language dependency structures using source structures and word alignments. On the source side, we extract treelets which are any connected subgraphs and consistent with word alignments. While on the target side, we allow the aligned target sequences to be generalized and discontinuous by introducing variables and gaps. The variables on the target side are aligned to the corresponding variables of treelets, while gaps between words or variables are corresponding to the uncovered nodes which are not included by treelets. To complete the translat"
W07-0706,W06-1608,0,0.0319377,"Missing"
W07-0706,P06-1066,1,0.90218,"Missing"
W07-0706,I05-1007,1,0.798107,"Missing"
W07-0706,P05-1033,0,0.550552,"Missing"
W07-0706,P05-1067,0,0.207272,"Missing"
W07-0706,P03-2041,0,0.441956,"ing wildcard on the target side with translations from the stack of n∗ end for for each uncovered node n@ by π do Attach the translations from the stack of n@ to the target side at the attaching point end for end for end for (4) 44 Melamed (2004) also used a similar way to integrate the language model. 5 Decoding Our decoding algorithm is similar to the bottom-up chart parsing. The distinction is that the input is a tree rather than a string and therefore the chart is indexed by nodes of the tree rather than spans of the string. Also, several other tree-based decoding algorithms introduced by Eisner (2003), Quirk et al. (2005) and Liu et al. (2006) can be classified as the chart-style parsing algorithm too. Our decoding algorithm is shown in Figure 4. Given an input dependency tree, firstly we generate the bottom-up order by postorder transversal. This order guarantees that any subnodes of node n have been translated before node n is done. For each node n in the bottom-up order, all matched DTSCs rooted at n are found, and a stack is also built for it to store the candidate translations. A DTSC π is said to match the input dependency subtree T rooted at n if and only if there is a treelet roote"
W07-0706,H05-1095,0,\N,Missing
W07-0706,zhang-etal-2004-interpreting,0,\N,Missing
W09-2907,W03-1812,0,0.0444888,"automatic word alignment, errors in word alignment would further propagate to the SMT and hurt SMT performance. 2.1 Automatic Extraction of MWEs In the past two decades, many different approaches on automatic MWE identification were reported. In general, those approaches can be classified into three main trends: (1) statistical approaches (Pantel and Lin, 2001; Piao et • We conduct experiments on domain-specific corpus. For one thing, domain-specific 2 http://www.statmt.org/moses/ 48 al., 2005), (2) syntactic approaches (Fazly and Stevenson, 2006; Bannard, 2007), and (3) semantic approaches (Baldwin et al., 2003; Cruys and Moir´on, 2007). Syntax-based and semantic-based methods achieve high precision, but syntax or semantic analysis has to be introduced as preparing step, so it is difficult to apply them to domains with few syntactical or semantic annotation. Statistical approaches only consider frequency information, so they can be used to obtain MWEs from bilingual corpora without deeper syntactic or semantic analysis. Most statistical measures only take two words into account, so it not easy to extract MWEs containing three or more than three words. Log Likelihood Ratio (LLR) has been proved a goo"
W09-2907,W07-1101,0,0.119712,"component words. Stanford university launched a MWE project1 , in which different qualities of MWE were presented. For bilingual multiword expression (BiMWE), we define a bilingual phrase as a bilingual MWE if (1) the source phrase is a MWE in source language; (2) the source phrase and the target phrase must be translated to each other exactly, i.e. there is no additional (boundary) word in target phrase which cannot find the corresponding word in source phrase, and vice versa. In recent years, many useful methods have been proposed to extract MWEs or BiMWEs automatically (Piao et al., 2005; Bannard, 2007; Fazly and Stevenson, 2006). Since MWE usually constrains possible senses of a polysemous word in context, they can be used in many NLP applications such as information retrieval, question answering, word sense disambiguation and so on. For machine translation, Piao et al. (2005) have noted that the issue of MWE identification and accurate interpretation from source to target language remained an unsolved problem for existing MT systems. This problem is more severe when MT systems are used to translate domain-specific texts, since they may include technical terminology as well as more general"
W09-2907,J93-2003,0,0.0183166,"l language processing tasks. However, how to use them to improve performance of statistical machine translation (SMT) is not well studied. This paper presents a simple yet effective strategy to extract domain bilingual multiword expressions. In addition, we implement three methods to integrate bilingual MWEs to Moses, the state-ofthe-art phrase-based machine translation system. Experiments show that bilingual MWEs could improve translation performance significantly. 1 Introduction Phrase-based machine translation model has been proved a great improvement over the initial wordbased approaches (Brown et al., 1993). Recent syntax-based models perform even better than phrase-based models. However, when syntaxbased models are applied to new domain with few syntax-annotated corpus, the translation performance would decrease. To utilize the robustness of phrases and make up the lack of syntax or semantic information in phrase-based model for domain translation, we study domain bilingual multiword expressions and integrate them to the existing phrase-based model. A multiword expression (MWE) can be considered as word sequence with relatively fixed structure representing special meanings. There is no uniform"
W09-2907,W02-1801,0,0.172545,"hieve high precision, but syntax or semantic analysis has to be introduced as preparing step, so it is difficult to apply them to domains with few syntactical or semantic annotation. Statistical approaches only consider frequency information, so they can be used to obtain MWEs from bilingual corpora without deeper syntactic or semantic analysis. Most statistical measures only take two words into account, so it not easy to extract MWEs containing three or more than three words. Log Likelihood Ratio (LLR) has been proved a good statistical measurement of the association of two random variables (Chang et al., 2002). We adopt the idea of statistical approaches, and propose a new algorithm named LLR-based Hierarchical Reducing Algorithm (HRA for short) to extract MWEs with arbitrary lengths. To illustrate our algorithm, firstly we define some useful items. In the following definitions, we assume the given sentence is “A B C D E”. list contains N units, of course. The final set of MWEs, S, is initialized to empty set. After initialization, the algorithm will enter an iterating loop with two steps: (1) select the two adjacent units with maximum score in L, naming U1 and U2 ; and (2) reduce U1 and U2 in L, a"
W09-2907,W02-1001,0,0.002314,"better use of the resources in hand could also enhance the quality of SMT system. We use “Baseline+BiMWE” to represent this method. 2. For a given MWE, find the bilingual sentence pairs where the source language sentences include the MWE. 3. Extract the candidate translations of the MWE from the above sentence pairs according to the algorithm described by Och (2002). After the above procedure, we have already extracted all possible candidate translations of a given MWE. The next step is to distinguish right candidates from wrong candidates. We construct perceptron-based classification model (Collins, 2002) to solve the problem. We design two groups of features: translation features, which describe the mutual translating chance between source phrase and target phrase, and the language features, which refer to how well a candidate is a reasonable translation. The translation features include: (1) the logarithm of source-target translation probability; (2) the logarithm of targetsource translation probability; (3) the logarithm of source-target lexical weighting; (4) the logarithm of target-source lexical weighting; and (5) the logarithm of the phrase pair’s LLR (Dunning, 1993). The first four fea"
W09-2907,W07-1104,0,0.0289521,"Missing"
W09-2907,J93-1003,0,0.0257739,"assification model (Collins, 2002) to solve the problem. We design two groups of features: translation features, which describe the mutual translating chance between source phrase and target phrase, and the language features, which refer to how well a candidate is a reasonable translation. The translation features include: (1) the logarithm of source-target translation probability; (2) the logarithm of targetsource translation probability; (3) the logarithm of source-target lexical weighting; (4) the logarithm of target-source lexical weighting; and (5) the logarithm of the phrase pair’s LLR (Dunning, 1993). The first four features are exactly the same as the four translation probabilities used in traditional phrase-based system (Koehn et al., 2003). The language features include: (1) the left entropy of the target phrase (Luo and Sun, 2003); (2) the right entropy of the target phrase; (3) the first word of the target phrase; (4) the last word of the target phrase; and (5) all words in the target phrase. 5 3.2 New Feature for Bilingual MWEs Lopez and Resnik (2006) once pointed out that better feature mining can lead to substantial gain in translation quality. Inspired by this idea, we append one"
W09-2907,P02-1040,0,0.0989801,"Missing"
W09-2907,C04-1114,0,0.0620022,"em into the training corpus, and retrain the model using GIZA++. By increasing the occurrences of bilingual MWEs, which are good phrases, we expect that the alignment would be modified and the probability estimation would be more reasonable. Wu et al. (2008) also used this method to perform domain adaption for SMT. Different from their approach, in which bilingual MWEs are extracted from additional corpus, we extract bilingual MWEs from the original training set. The fact that additional resources can improve the domain-specific SMT performance was proved by many researchers (Wu et al., 2008; Eck et al., 2004). However, our method shows that making better use of the resources in hand could also enhance the quality of SMT system. We use “Baseline+BiMWE” to represent this method. 2. For a given MWE, find the bilingual sentence pairs where the source language sentences include the MWE. 3. Extract the candidate translations of the MWE from the above sentence pairs according to the algorithm described by Och (2002). After the above procedure, we have already extracted all possible candidate translations of a given MWE. The next step is to distinguish right candidates from wrong candidates. We construct"
W09-2907,E06-1043,0,0.0290626,"s. Stanford university launched a MWE project1 , in which different qualities of MWE were presented. For bilingual multiword expression (BiMWE), we define a bilingual phrase as a bilingual MWE if (1) the source phrase is a MWE in source language; (2) the source phrase and the target phrase must be translated to each other exactly, i.e. there is no additional (boundary) word in target phrase which cannot find the corresponding word in source phrase, and vice versa. In recent years, many useful methods have been proposed to extract MWEs or BiMWEs automatically (Piao et al., 2005; Bannard, 2007; Fazly and Stevenson, 2006). Since MWE usually constrains possible senses of a polysemous word in context, they can be used in many NLP applications such as information retrieval, question answering, word sense disambiguation and so on. For machine translation, Piao et al. (2005) have noted that the issue of MWE identification and accurate interpretation from source to target language remained an unsolved problem for existing MT systems. This problem is more severe when MT systems are used to translate domain-specific texts, since they may include technical terminology as well as more general fixed expressions and idiom"
W09-2907,C96-1009,0,0.0606959,"Missing"
W09-2907,W03-1803,0,0.0267191,"5) have noted that the issue of MWE identification and accurate interpretation from source to target language remained an unsolved problem for existing MT systems. This problem is more severe when MT systems are used to translate domain-specific texts, since they may include technical terminology as well as more general fixed expressions and idioms. Although some MT systems may employ a machine-readable bilingual dictionary of MWE, it is time-consuming and inefficient to obtain this resource manually. Therefore, some researchers have tried to use automatically extracted bilingual MWEs in SMT. Tanaka and Baldwin (2003) described an approach of noun-noun compound machine translation, but no significant comparison was presented. Lambert and Banchs (2005) presented a method in which bilingual MWEs were used to modify the word alignment so as to improve the SMT quality. In their work, a bilingual MWE in training corpus was grouped as Multiword expressions (MWEs) have been proved useful for many natural language processing tasks. However, how to use them to improve performance of statistical machine translation (SMT) is not well studied. This paper presents a simple yet effective strategy to extract domain bilin"
W09-2907,J07-3002,0,0.0115354,"e methods to apply bilingual MWEs in SMT system. Section 4 presents the experimental results. Section 5 draws conclusions and describes the future work. Since this paper mainly focuses on the application of BiMWE in SMT, we only give a brief introduction on monolingual and bilingual MWE extraction. • Instead of improving translation indirectly by improving the word alignment quality, we directly target at the quality of translation. Some researchers have argued that large gains of alignment performance under many metrics only led to small gains in translation performance (Ayan and Dorr, 2006; Fraser and Marcu, 2007). 2 Bilingual Multiword Expression Extraction Besides the above differences, there are some advantages of our approaches: In this section we describe our approach of bilingual MWE extraction. In the first step, we obtain monolingual MWEs from the Chinese part of parallel corpus. After that, we look for the translation of the extracted MWEs from parallel corpus. • In our method, automatically extracted MWEs are used as additional resources rather than as phrase-table filter. Since bilingual MWEs are extracted according to noisy automatic word alignment, errors in word alignment would further pr"
W09-2907,N03-1017,0,0.0411699,"ranslating chance between source phrase and target phrase, and the language features, which refer to how well a candidate is a reasonable translation. The translation features include: (1) the logarithm of source-target translation probability; (2) the logarithm of targetsource translation probability; (3) the logarithm of source-target lexical weighting; (4) the logarithm of target-source lexical weighting; and (5) the logarithm of the phrase pair’s LLR (Dunning, 1993). The first four features are exactly the same as the four translation probabilities used in traditional phrase-based system (Koehn et al., 2003). The language features include: (1) the left entropy of the target phrase (Luo and Sun, 2003); (2) the right entropy of the target phrase; (3) the first word of the target phrase; (4) the last word of the target phrase; and (5) all words in the target phrase. 5 3.2 New Feature for Bilingual MWEs Lopez and Resnik (2006) once pointed out that better feature mining can lead to substantial gain in translation quality. Inspired by this idea, we append one feature into bilingual phrase table to indicate that whether a bilingual phrase contains bilingual MWEs. In other words, if the source language"
W09-2907,C08-1125,0,0.0925757,"Missing"
W09-2907,2005.mtsummit-posters.11,0,0.461395,"em for existing MT systems. This problem is more severe when MT systems are used to translate domain-specific texts, since they may include technical terminology as well as more general fixed expressions and idioms. Although some MT systems may employ a machine-readable bilingual dictionary of MWE, it is time-consuming and inefficient to obtain this resource manually. Therefore, some researchers have tried to use automatically extracted bilingual MWEs in SMT. Tanaka and Baldwin (2003) described an approach of noun-noun compound machine translation, but no significant comparison was presented. Lambert and Banchs (2005) presented a method in which bilingual MWEs were used to modify the word alignment so as to improve the SMT quality. In their work, a bilingual MWE in training corpus was grouped as Multiword expressions (MWEs) have been proved useful for many natural language processing tasks. However, how to use them to improve performance of statistical machine translation (SMT) is not well studied. This paper presents a simple yet effective strategy to extract domain bilingual multiword expressions. In addition, we implement three methods to integrate bilingual MWEs to Moses, the state-ofthe-art phrase-bas"
W09-2907,W06-2402,0,0.621208,"ccumulation”. Every word of this term represents a special meaning and cannot be understood literally or without this context. These terms are difficult to be translated even for humans, let alone machine translation. So, treating these terms as MWEs and applying them in SMT system have practical significance. one unique token before training alignment models. They reported that both alignment quality and translation accuracy were improved on a small corpus. However, in their further study, they reported even lower BLEU scores after grouping MWEs according to part-of-speech on a large corpus (Lambert and Banchs, 2006). Nonetheless, since MWE represents liguistic knowledge, the role and usefulness of MWE in full-scale SMT is intuitively positive. The difficulty lies in how to integrate bilingual MWEs into existing SMT system to improve SMT performance, especially when translating domain texts. In this paper, we implement three methods that integrate domain bilingual MWEs into a phrasebased SMT system, and show that these approaches improve translation quality significantly. The main difference between our methods and Lambert and Banchs’ work is that we directly aim at improving the SMT performance rather th"
W09-2907,2006.amta-papers.11,0,0.0174462,"ithm of source-target lexical weighting; (4) the logarithm of target-source lexical weighting; and (5) the logarithm of the phrase pair’s LLR (Dunning, 1993). The first four features are exactly the same as the four translation probabilities used in traditional phrase-based system (Koehn et al., 2003). The language features include: (1) the left entropy of the target phrase (Luo and Sun, 2003); (2) the right entropy of the target phrase; (3) the first word of the target phrase; (4) the last word of the target phrase; and (5) all words in the target phrase. 5 3.2 New Feature for Bilingual MWEs Lopez and Resnik (2006) once pointed out that better feature mining can lead to substantial gain in translation quality. Inspired by this idea, we append one feature into bilingual phrase table to indicate that whether a bilingual phrase contains bilingual MWEs. In other words, if the source language phrase contains a MWE (as substring) and http://www.fjoch.com/GIZA++.html 50 In the chemical industry domain, table 2 gives the detail information of the data. In this experiment, 59466 bilingual MWEs are extracted. the target language phrase contains the translation of the MWE (as substring), the feature value is 1, ot"
W09-2907,P06-1002,0,\N,Missing
W09-2907,P05-1066,0,\N,Missing
W09-3803,P09-1059,1,0.917794,"with a different annotation standard to the desired target parser, with the supervision by a target corpus annotated in the desired standard. Furthermore, instead of a hand-annotated one, a projected treebank derived from a bilingual corpus is used as the source corpus. This benefits the resource-scarce languages which haven’t different handannotated treebanks. Experiments show that the target parser gains significant improvement over the baseline parser trained on the target corpus only, when the target corpus is smaller. 1 Introduction Automatic annotation adaptation for sequence labeling (Jiang et al., 2009) aims to enhance a tagger with one annotation standard by transferring knowledge from a source corpus annotated in another standard. It would be valuable to adapt this strategy to parsing, since for some languages there are also several treebanks with different annotation standards, such as Chomskian-style Penn Treebank (Marcus et al., 1993) and HPSG LinGo Redwoods Treebank (Oepen et al., 2002) for English. However, we are not content with conducting annotation adaptation between existing different treebanks, because it would be more valuable to boost the parsers also for the resource-scarce l"
W09-3803,D09-1106,1,0.747991,"s is, the more significant improvement can be obtained. For example, 5 Experiments The source corpus for annotation adaptation, that is, the projected Chinese treebank, is derived from 5.6 millions LDC Chinese-English sentence pairs. The Chinese side of the bilingual corpus is wordsegmented and POS-tagged by an implementation of (Jiang et al., 2008), and the English sentences are parsed by an implementation of (McDonald and Pereira, 2006) which is instead trained on WSJ section of Penn English Treebank (Marcus et al., 1993). The alignment matrixes for sentence pairs are obtained according to (Liu et al., 2009). The English trees are then projected across to Chinese using the algorithm in section 2. Out of these projected trees, we only select 500 thousands with word count l s.t. 6 ≤ l ≤ 100 and with projecting confidence c = C(TC |TE , A)1/l s.t. c ≥ 0.35. While for the target corpus, we take Penn Chinese Treebank (CTB) 1.0 and CTB 5.0 (Xue et al., 2005) respectively, and follow the traditional corpus splitting: chapters 271-300 for testing, chapters 301-325 for development, and else for training. We adopt the 2nd-order MST model (McDonald et al., 2005) as the target parser for better performance,"
W09-3803,C02-1003,0,0.296037,"Missing"
W09-3803,J93-2004,0,0.0375212,"ndannotated treebanks. Experiments show that the target parser gains significant improvement over the baseline parser trained on the target corpus only, when the target corpus is smaller. 1 Introduction Automatic annotation adaptation for sequence labeling (Jiang et al., 2009) aims to enhance a tagger with one annotation standard by transferring knowledge from a source corpus annotated in another standard. It would be valuable to adapt this strategy to parsing, since for some languages there are also several treebanks with different annotation standards, such as Chomskian-style Penn Treebank (Marcus et al., 1993) and HPSG LinGo Redwoods Treebank (Oepen et al., 2002) for English. However, we are not content with conducting annotation adaptation between existing different treebanks, because it would be more valuable to boost the parsers also for the resource-scarce languages, rather than only for the resource-rich ones that already have several treebanks. Although hand-annotated treebanks are costly and scarce, it is not difficult for many languages to collect large numbers of bilingual sentence-pairs aligned to English. According to the word alignment, the English parses can be projected across 2 Error"
W09-3803,D08-1017,0,0.02667,"d the target parser, so the guide features should be defined on dependency edges in accordance with the edge-factored property of MST models. In the decoding procedure of the target parser, the degree of a dependency edge being supported can be adjusted by the relationship between this edge’s head and modifier in the intermediate parsing result of the source parser. The most intuitionistic relationship is whether the dependency between head and modifier exists in this intermediate result. Such a bi-valued relationship is similar to that in the stacking method for combining dependency parsers (Martins et al., 2008; Nivre and McDonald, 2008). The guide features are then defined as this relationship itself as well as its combinations with the lexical features of MST models. Furthermore, in order to explore more detailed knowledge from the source parser, we redefine the relationship as a four-valued variable which covers the following situations: parentchild, child-parent, siblings and else. With the guide features, the parameter tuning procedure of the target parser will automatically learn the regularity of using the source parser’s intermediate result to guide its decision making. (1) TC C(TC |TE , A)"
W09-3803,E06-1011,0,0.0618714,"parser, is trained on the target corpus with guide features extracted from the source parser’s parsing results. For testing, a token sequence is first parsed by the source parser to obtain an intermediate parsing result with the source annotation standard, and then parsed by the target parser with the guide features extracted from the intermediate parsing result to obtain the final result. The design of the guide features is the most important, and is specific to the parsing algorithm of the target parser. In this work we adopt the maximum spanning tree (MST) algorithm (McDonald et al., 2005; McDonald and Pereira, 2006) for both the source and the target parser, so the guide features should be defined on dependency edges in accordance with the edge-factored property of MST models. In the decoding procedure of the target parser, the degree of a dependency edge being supported can be adjusted by the relationship between this edge’s head and modifier in the intermediate parsing result of the source parser. The most intuitionistic relationship is whether the dependency between head and modifier exists in this intermediate result. Such a bi-valued relationship is similar to that in the stacking method for combini"
W09-3803,P05-1012,0,0.226911,"ser, called the target parser, is trained on the target corpus with guide features extracted from the source parser’s parsing results. For testing, a token sequence is first parsed by the source parser to obtain an intermediate parsing result with the source annotation standard, and then parsed by the target parser with the guide features extracted from the intermediate parsing result to obtain the final result. The design of the guide features is the most important, and is specific to the parsing algorithm of the target parser. In this work we adopt the maximum spanning tree (MST) algorithm (McDonald et al., 2005; McDonald and Pereira, 2006) for both the source and the target parser, so the guide features should be defined on dependency edges in accordance with the edge-factored property of MST models. In the decoding procedure of the target parser, the degree of a dependency edge being supported can be adjusted by the relationship between this edge’s head and modifier in the intermediate parsing result of the source parser. The most intuitionistic relationship is whether the dependency between head and modifier exists in this intermediate result. Such a bi-valued relationship is similar to that in th"
W09-3803,P08-1108,0,0.0203806,"o the guide features should be defined on dependency edges in accordance with the edge-factored property of MST models. In the decoding procedure of the target parser, the degree of a dependency edge being supported can be adjusted by the relationship between this edge’s head and modifier in the intermediate parsing result of the source parser. The most intuitionistic relationship is whether the dependency between head and modifier exists in this intermediate result. Such a bi-valued relationship is similar to that in the stacking method for combining dependency parsers (Martins et al., 2008; Nivre and McDonald, 2008). The guide features are then defined as this relationship itself as well as its combinations with the lexical features of MST models. Furthermore, in order to explore more detailed knowledge from the source parser, we redefine the relationship as a four-valued variable which covers the following situations: parentchild, child-parent, siblings and else. With the guide features, the parameter tuning procedure of the target parser will automatically learn the regularity of using the source parser’s intermediate result to guide its decision making. (1) TC C(TC |TE , A) can be factorized into each"
W09-3803,C02-2025,0,0.0213072,"parser gains significant improvement over the baseline parser trained on the target corpus only, when the target corpus is smaller. 1 Introduction Automatic annotation adaptation for sequence labeling (Jiang et al., 2009) aims to enhance a tagger with one annotation standard by transferring knowledge from a source corpus annotated in another standard. It would be valuable to adapt this strategy to parsing, since for some languages there are also several treebanks with different annotation standards, such as Chomskian-style Penn Treebank (Marcus et al., 1993) and HPSG LinGo Redwoods Treebank (Oepen et al., 2002) for English. However, we are not content with conducting annotation adaptation between existing different treebanks, because it would be more valuable to boost the parsers also for the resource-scarce languages, rather than only for the resource-rich ones that already have several treebanks. Although hand-annotated treebanks are costly and scarce, it is not difficult for many languages to collect large numbers of bilingual sentence-pairs aligned to English. According to the word alignment, the English parses can be projected across 2 Error-Tolerant Tree-Projecting Algorithm Previous works mak"
W09-3803,W02-1001,0,0.0676866,"Missing"
W09-3803,D09-1086,0,0.0414749,"in another standard. We can define the purpose of the automatic annotation adaptation for dependency parsing in the same way. 4 Related Works Many works have been devoted to obtain parsing knowledge from word aligned bilingual cor26 Model source parser target parser baseline parser pora. (L¨u et al., 2002) learns Chinese bracketing knowledge via ITG alignment; (Hwa et al., 2005) and (Ganchev et al., 2009) induces dependency grammar via projection from aligned English, where some filtering is used to reduce the noise and some hand-designed rules to handle language heterogeneity. Just recently, Smith and Eisner (2009) gave an idea similar to ours. They perform dependency projection and annotation adaptation with Quasi-Synchronous Grammar (QG) Features. Although both related to projection and annotation, there are still important differences between these two works. First, we design an error-tolerant alignment-matrix-based tree-projecting algorithm to perform whole-tree projection, while they resort to QG features to score local configurations of aligned source and target trees. Second, their adaptation emphasizes to transform a tree from one annotation standard to another, while our adaptation emphasizes t"
W09-3803,P09-1042,0,0.235533,"more valuable to boost the parsers also for the resource-scarce languages, rather than only for the resource-rich ones that already have several treebanks. Although hand-annotated treebanks are costly and scarce, it is not difficult for many languages to collect large numbers of bilingual sentence-pairs aligned to English. According to the word alignment, the English parses can be projected across 2 Error-Tolerant Tree-Projecting Algorithm Previous works making use of projected corpus usually adopt the direct-mapping method for structure projection (Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009), where 25 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 25–28, c Paris, October 2009. 2009 Association for Computational Linguistics some filtering is needed to eliminate the inaccurate or conflicting labels or dependency edges. Here we propose a more robust algorithm for dependency tree projection. According to the alignment matrix, this algorithm dynamically searches the projected Chinese dependency tree which has the largest consistency with the corresponding English tree. We briefly introduce the alignment matrix before describing our projecting al"
W09-3803,W05-1506,0,0.0370989,"sing the source parser’s intermediate result to guide its decision making. (1) TC C(TC |TE , A) can be factorized into each dependency edge x → y in TC , that is to say Y C(TC |TE , A) = Ce (x → y|TE , A) (2) x→y∈TC We can obtain Ce by simple accumulation across all possible alignments Ce (x → y|TE , A) X Ax,x′ × Ay,y′ × δ(x′ , y ′ |TE ) (3) = 1≤x′ ,y ′ ≤|E| where δ(x′ , y ′ |TE ) is a 0-1 function that equals 1 only if x′ → y ′ exists in TE . The searching procedure, argmax operation in equation 1, can be effectively solved by a simple, bottom-up dynamic algorithm with cube-pruning speed-up (Huang and Chiang, 2005). We omit the detailed algorithm here due to space restrictions. 3 Annotation Adaptation for Dependency Parsing The automatic annotation adaptation strategy for sequence labeling (Jiang et al., 2009) aims to strengthen a tagger trained on a corpus annotated in one annotation standard with a larger assistant corpus annotated in another standard. We can define the purpose of the automatic annotation adaptation for dependency parsing in the same way. 4 Related Works Many works have been devoted to obtain parsing knowledge from word aligned bilingual cor26 Model source parser target parser baselin"
W09-3803,N01-1026,0,0.0341324,"ng different treebanks, because it would be more valuable to boost the parsers also for the resource-scarce languages, rather than only for the resource-rich ones that already have several treebanks. Although hand-annotated treebanks are costly and scarce, it is not difficult for many languages to collect large numbers of bilingual sentence-pairs aligned to English. According to the word alignment, the English parses can be projected across 2 Error-Tolerant Tree-Projecting Algorithm Previous works making use of projected corpus usually adopt the direct-mapping method for structure projection (Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009), where 25 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 25–28, c Paris, October 2009. 2009 Association for Computational Linguistics some filtering is needed to eliminate the inaccurate or conflicting labels or dependency edges. Here we propose a more robust algorithm for dependency tree projection. According to the alignment matrix, this algorithm dynamically searches the projected Chinese dependency tree which has the largest consistency with the corresponding English tree. We briefly introduce the alignment m"
W09-3803,P02-1050,0,0.0623112,"of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {jiangwenbin, liuqun}@ict.ac.cn Abstract to their translations, and the projected trees can be leveraged to boost parsing. Many efforts are devoted to the research on projected treebanks, such as (L¨u et al., 2002), (Hwa et al., 2005) and (Ganchev et al., 2009), etc. Considering the fact that a projected treebank partially inherits the English annotation standard, some hand-written rules are designed to deal with the divergence between languages such as in (Hwa et al., 2002). However, it will be more valuable and interesting to adapt this divergence automatically and boost the existing parsers with this projected treebank. In this paper, we investigate the automatic annotation adaptation strategy for Chinese dependency parsing, where the source corpus for adaptation is a projected treebank derived from a bilingual corpus aligned to English with word alignment and English trees. We also propose a novel, errortolerant tree-projecting algorithm, which dynamically searches the project Chinese tree that has the largest consistency with the corresponding English tree,"
W09-3803,P08-1102,1,\N,Missing
W11-1911,P05-1022,0,0.0603318,"Missing"
W11-1911,P08-1067,0,0.0477932,"Missing"
W11-1911,P04-1018,0,0.370236,"andidates among all constituents from both given parse tree and packed forest. The packed forest is a compact representation of all parse trees for a given sentence. Readers can refer to (Mi et al., 2008) for detailed definitions. Once the mentions are identified, the left step is to group mentions referring to same object into similar entity. This problem can be viewed as binary classification problem of determining whether each mention pairs corefer. We use a Maximum Entropy classifier to predict the possibility that two mentions refer to the similar entity. And mainly following the work of Luo et al. (2004), we use a beam search algorithm based on Bell Tree to obtain the global optimal classification. As this is the first time we participate competition of coreference resolution, we mainly concentrate on developing fault tolerant capability of our system while omitting feature engineering and other helpful technologies. 2 Mention Detection The first step of the coreference resolution tries to recognize occurrences of mentions in documents. Note that we recognize mention boundaries only on development and test set while generating training 76 Proceedings of the 15th Conference on Computational Na"
W11-1911,P08-1023,1,0.901686,"Missing"
W11-1911,P10-1142,0,0.0187597,"we use the L-BFGS parameter estimation algorithm with gaussian prior smoothing (Chen and Rosenfeld, 1999). We set the gaussian prior to 2 and train the model in 100 iterations. • [A][B][C][D], [A][B][CD] 3.1 Creation of Entities This stage aims to create the mentions detected in the first stage into entities, according to the prediction of classifier. One simple method is to use a greedy algorithm, by comparing each mention to its previous mentions and refer to the one that has the highest probability. In principle, this algorithm is too greedy and sometimes results in unreasonable partition (Ng, 2010). To address this problem, we follow the literature (Luo et al., 2004) and propose to use beam search to find global optimal partition. Intuitively, creation of entities can be casted as partition problem. And the number of partitions equals the Bell Number (Bell, which has a ∑ 1934), kn “closed” formula B(n) = 1e ∞ . k=0 k! Clearly, this number is very huge when n is large, enumeration of all partitions is impossible, so we instead designing a beam search algorithm to find the best partition. Formally, the task is to optimize the following objective, yˆ = arg max ϕ∈P ∑ P rob(e) (1) e∈ϕ ∑ pos("
W11-1911,W11-1901,0,0.0939802,"Missing"
W11-1911,J01-4004,0,0.0947592,"ence, intuitively, we 77 3 Determining Coreference This stage is to determine which mentions belong to the same entity. We train a Maximum Entropy classifier (Le, 2004) to decide whether two mentions are coreferent. We use the method proposed by Soon, et al.’s to generate the training instances, where a positive instance is formed between current mention Mj and its closest preceding antecedent Mi , and a negative instance is created by paring Mj with each of the intervening mentions, Mi+1 , Mi+2 ,...,Mj−1 . We use the following features to train our classifier. Features in Soon et al.’s work (Soon et al., 2001) Lexical features IS PREFIX: whether the string of one mention is prefix of the other; IS SUFFIX: whether the string of one mention is suffix of the other; ACRONYM: whether one mention is the acronym of the other; Distance features SENT DIST: distance between the sentences containing the two mentions; MEN DIST: number of mentions between two mentions; Grammatical features IJ PRONOUN: whether both mentions are pronoun; I NESTED: whether mention i is nested in another mention; J NESTED: whether mention j is nested in another mention; Syntax features HEAD: whether the heads of two mentions have t"
W11-1911,D08-1022,0,\N,Missing
W12-4506,N03-1017,0,0.00340471,". Thus, Clearly, another problem is how to compute p(i, j) for each edge between entity and mention candidate. This problem could be casted as how to compute similarity of phrases across multiple languages. Formally, given an English phrases we1 , .., wen and a Chinese phrase wc1 , .., wcm , the problem is how to compute the similar score S between them. Although we could compute lexical, syntactic or semantic similar score to obtain accurate similarity, here for simplicity, we just compute the lexical similarity using the phrase table extracted by a phrased-based machine translation decoder (Koehn et al., 2003). Phrase table is a rich resource that contains probability score for phrase in one language translated into another language, thus we could design a dynamic algorithm shown in Algorithm 1 to compute the similar score. Equation in line 5 is used to reserve highest similar score for its sub-phrases, and p(i, i + j) is the similar score between sub-phrases wi , .., wi+j and its translation. When we compute the score of the sub-phrases wi , .., wi+j , we literately pick one pti from P T and check whether wc1 , .., wcm involves pti ’s target side, if that we record its score until we obtain a high"
W12-4506,W11-1902,0,0.0580909,"e and Arabic into English, second is coreference resolution for English, last is the projection of coreferring entities. Since the first step is clear that we extract sentences from Chinese and Arabic documents and translate them into English using Google Translator, hence in this section we will mainly describe the configuration of our English resolver and details of projection method. 2.1 English Resolver In last year’s evaluation task, the Standford Natural Language Processing Group ranked the first position and they also open their toolkit for research community, namely Standford CoreNLP (Lee et al., 2011) 1 , better yet, their toolkit is optimized for CoNLL task. Thus we could use their toolkit as our English resolver and concentrate on bettering the projection of coreferring entities. 1 http://nlp.stanford.edu/software/ corenlp.shtml 72 Figure 2: A minimum cost and maximum flow structure is used to solve the problem that mapping coreferring entities into each mention candidates with highest probability. We use the basic running script that is “java -cp joda-time.jar:stanford-corenlp.jar:stanfordcorenlp-models.jar:xom.jar -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -filelist filelist.txt”"
W12-4506,J03-1002,0,0.00334523,"hrase into the coreferring entities as the English resolve finding with the probability 0.6. 3 Experiments 3.1 English Results In this section, we will report our experimental results in details. We use Standford CoreNLP toolkit to generate results for English. Table 1 lists the Fscore obtained on developing set. 3.2 Chinese and Arabic Results As last section mentioned, we first translate Chinese and Arabic into English and then use CoreNLP to resolve coreference on English. To obtain high translation quality, we use Google Translator Toolkit 2 . And to compute similarity score, we run Giza++(Och and Ney, 2003) 3 , an open source toolkit for word alignment, to perform word alignment. For Chinese, we use 1 million bilingual corpus provided by NIST MT evaluation task to extract phrase table, and for Arabic its size is 2 million. Note that, we extract phrase table from English to Chinese and Arabic with maximum phrase length 10. The reason is that our algorithm check English phrase whose length is less than 10 tokens. To compare our results, we also use CoreNLP to generate results for Chinese and Arabic. Since CoreNLP use some syntactic knowledge to resolving coreference, it can also output coreferring"
W12-4506,W12-4501,0,0.0596353,"12. This is an extension of the CoNLL2011 shared task and would involve automatic anaphoric mention detection and coreference resolution across three languages – English, Chinese and Arabic – using OntoNotes v5.0 corpus, given predicted information on the syntax, proposition, word sense and named entity layers. Automatic identification of coreferring entities and events in text has been an uphill battle for several decades, partly because it can require world knowledge which is not well-defined and partly owing to the lack of substantial annotated data. For more details, readers can refer to (Pradhan et al., 2012). Before this year’s task, researchers proposed two typical novel methods to address the problem of natural language processing across multiple languages: projection and joint learning (Rahman and Ng, 2012). Specific to this year’s coreference resolution task, for projection based method, we could first develop a strong resolver or utilize a publicly available system on English, and translate other languages into English, eventually, we could project the coreferring entities resolved on English back into other language sides. Generally, a projection method is easier to develop since it doesn’t"
W12-4506,N12-1090,0,0.0318405,"v5.0 corpus, given predicted information on the syntax, proposition, word sense and named entity layers. Automatic identification of coreferring entities and events in text has been an uphill battle for several decades, partly because it can require world knowledge which is not well-defined and partly owing to the lack of substantial annotated data. For more details, readers can refer to (Pradhan et al., 2012). Before this year’s task, researchers proposed two typical novel methods to address the problem of natural language processing across multiple languages: projection and joint learning (Rahman and Ng, 2012). Specific to this year’s coreference resolution task, for projection based method, we could first develop a strong resolver or utilize a publicly available system on English, and translate other languages into English, eventually, we could project the coreferring entities resolved on English back into other language sides. Generally, a projection method is easier to develop since it doesn’t need sentence alignment across multiple languages. Thus, in this year’s task, we investigate a translation based model to resolve coreference on English, Chinese and Arabic. The whole process is illustrate"
W12-5704,2003.mtsummit-systems.1,0,0.693557,"tands for the number of nulls inserted, and Len is the length of the current path. ν ,µ and ε are the corresponding weights of each feature. A beam search algorithm is employed to find the best path. 3 Experimental Setup 3.1 Data We participate in the ML4HMT-12 shared task ES-EN. Participants are given a development bilingual data set aligned at the sentence level. Each &quot;bilingual sentence&quot; contains: 1) the source sentence, 2) the target (reference) sentence and 3) the corresponding multiple output translations from four systems, based on different MT approaches (Ramırez-Sánchez et al., 2006; Alonso and Thurmair, 2003; Koehn et al., 2007). The output has been annotated with 40 system-internal meta-data information derived from the translation process of each of the systems. In this work we use 1000 sentence pair from the 10K development set to tune the system parameters and all the 3003 sentence pairs in the test set to run the test. 3.2 Backbone Selection Equation 1 describes the traditional backbone selection. However in this work we heuristically set Lucy RBMT (Alonso and Thurmair, 2003) output as the backbone. Our motivations are that: 1) Lucy’s output tends to be more grammatical than Moses or other M"
W12-5704,2010.amta-papers.16,1,0.887417,"Missing"
W12-5704,D09-1115,1,0.886062,"sing (Henderson and Brill, 1999) and speech recognition (Fiscus, 1997). In late the 90s, the speech recognition community produced a confusion network-based system combination approach, spreading instantly to SMT community as well. The traditional system combination approach employs confusion networks which are built by the monolingual alignment which induces sentence similarity. Confusion networks are compact graph-based structures representing multiple hypothesises (Bangalore et al., 2001). It is noted that there are several generalized forms of confusion networks as well. One is a lattice (Feng et al., 2009) and the other is a translation forest (Watanabe and Sumita, 2011). The former employs lattices that can describe arbitrary mappings in hypothesis alignments. A lattice is more general than a confusion network. By contrast, a confusion forest exploits syntactic similarity between individual outputs. Up to now, various state-of-the-art alignment methods have been developed including IndirectHMM (He et al., 2008; Du and Way, 2010) which is a statistical-model-based method, and TER which is a metric-based method which uses an edit distance. In this work we focus on the IHMM method. The main probl"
W12-5704,D08-1011,0,0.100136,"aph-based structures representing multiple hypothesises (Bangalore et al., 2001). It is noted that there are several generalized forms of confusion networks as well. One is a lattice (Feng et al., 2009) and the other is a translation forest (Watanabe and Sumita, 2011). The former employs lattices that can describe arbitrary mappings in hypothesis alignments. A lattice is more general than a confusion network. By contrast, a confusion forest exploits syntactic similarity between individual outputs. Up to now, various state-of-the-art alignment methods have been developed including IndirectHMM (He et al., 2008; Du and Way, 2010) which is a statistical-model-based method, and TER which is a metric-based method which uses an edit distance. In this work we focus on the IHMM method. The main problem of IHMM is that there are numerous one-to-many and one-to-null cases in the alignment results. This alignment noise significantly affects the confusion network construction and the decoding process. In this work, in addition to the IHMM alignment, we also incorporate alignment meta information extracted from an RBMT system to help the decoding process. The other crucial factor is the backbone selection whic"
W12-5704,W99-0623,0,0.0607824,"oduction This paper describes a new extension to our system combination module in Dublin City University for the participation in the system combination task in the ML4HMT-2012 workshop. We incorporate alignment meta information to the alignment module when building a confusion network. Given multiple translation outputs, a system combination strategy aims at finding the best translations, either by choosing one sentence or generating a new translation from fragments originated from individual systems(Banerjee et al., 2010). Combination methods have been widely used in fields such as parsing (Henderson and Brill, 1999) and speech recognition (Fiscus, 1997). In late the 90s, the speech recognition community produced a confusion network-based system combination approach, spreading instantly to SMT community as well. The traditional system combination approach employs confusion networks which are built by the monolingual alignment which induces sentence similarity. Confusion networks are compact graph-based structures representing multiple hypothesises (Bangalore et al., 2001). It is noted that there are several generalized forms of confusion networks as well. One is a lattice (Feng et al., 2009) and the other"
W12-5704,P07-2045,0,0.00511754,"ls inserted, and Len is the length of the current path. ν ,µ and ε are the corresponding weights of each feature. A beam search algorithm is employed to find the best path. 3 Experimental Setup 3.1 Data We participate in the ML4HMT-12 shared task ES-EN. Participants are given a development bilingual data set aligned at the sentence level. Each &quot;bilingual sentence&quot; contains: 1) the source sentence, 2) the target (reference) sentence and 3) the corresponding multiple output translations from four systems, based on different MT approaches (Ramırez-Sánchez et al., 2006; Alonso and Thurmair, 2003; Koehn et al., 2007). The output has been annotated with 40 system-internal meta-data information derived from the translation process of each of the systems. In this work we use 1000 sentence pair from the 10K development set to tune the system parameters and all the 3003 sentence pairs in the test set to run the test. 3.2 Backbone Selection Equation 1 describes the traditional backbone selection. However in this work we heuristically set Lucy RBMT (Alonso and Thurmair, 2003) output as the backbone. Our motivations are that: 1) Lucy’s output tends to be more grammatical than Moses or other MT systems; 2) accordi"
W12-5704,J03-1002,0,0.0115978,"put. After obtaining the backbone, all other hypotheses are aligned to it. The alignment strategies include IHMM, TER, etc. Note that during the word alignment word reordering and ‘null’ insertion are performed, which is usually called normalization. The confusion network, which can be constructed directly from the normalized alignment is given in Figure 2b, in which case H1 is chosen as the backbone. 2.2 Indirect HMM Alignment In this work we implement the IHMM (He et al., 2008). IHMM is a refined version of HMM alignment (Vogel et al., 1996) which is widely used in bilingual word alignment (Och and Ney, 2003). Let B = (b1 , ..., bJ ) denote the J words in the backbone sentence, H = (h1 , ..., h I ) denote one of the hypothesis, and A = (a1 , ..., aJ ) denote the alignment of each backbone word to the hypothesis word. We use Equation 2 to compute the alignment probability of each word pair. In Equation 2, d represents the distortion model and p denotes the word similarity model. P(H|B) = X Y A j=1...J [d(a j |a j−1 , I)p(h j |ba j )] 39 (2) In order to handle the words which are aligned to an empty word, we also insert a null associated with each backbone word. We follow (Vogel et al., 1996) and us"
W12-5704,2006.tc-1.12,0,0.689469,"of the current path, Nnul ls stands for the number of nulls inserted, and Len is the length of the current path. ν ,µ and ε are the corresponding weights of each feature. A beam search algorithm is employed to find the best path. 3 Experimental Setup 3.1 Data We participate in the ML4HMT-12 shared task ES-EN. Participants are given a development bilingual data set aligned at the sentence level. Each &quot;bilingual sentence&quot; contains: 1) the source sentence, 2) the target (reference) sentence and 3) the corresponding multiple output translations from four systems, based on different MT approaches (Ramırez-Sánchez et al., 2006; Alonso and Thurmair, 2003; Koehn et al., 2007). The output has been annotated with 40 system-internal meta-data information derived from the translation process of each of the systems. In this work we use 1000 sentence pair from the 10K development set to tune the system parameters and all the 3003 sentence pairs in the test set to run the test. 3.2 Backbone Selection Equation 1 describes the traditional backbone selection. However in this work we heuristically set Lucy RBMT (Alonso and Thurmair, 2003) output as the backbone. Our motivations are that: 1) Lucy’s output tends to be more gramma"
W12-5704,N07-1029,0,0.138427,"ull cases in the alignment results. This alignment noise significantly affects the confusion network construction and the decoding process. In this work, in addition to the IHMM alignment, we also incorporate alignment meta information extracted from an RBMT system to help the decoding process. The other crucial factor is the backbone selection which also affects the combination results. The backbone determines the word order in the final output. Backbone selection is often done by Minimum Bayes Risk (MBR) decoding which selects a hypothesis with minimum average distance among all hypotheses (Rosti et al., 2007a,b). In this work we heuristically choose an RBMT output as the backbone due to its (expected) overall grammatically well-formed output and better human evaluation results. We report our results and provide a comparison with traditional confusion-network-based network approach. The remainder of the paper is organized as follows: We will review the state-of-the-art system combination framework based on confusion networks in Section 2. We describe our experimental setup, how we extract the alignment information from meta-data and how we use it in Section 3. The results and analysis are also giv"
W12-5704,P07-1040,0,0.122071,"ull cases in the alignment results. This alignment noise significantly affects the confusion network construction and the decoding process. In this work, in addition to the IHMM alignment, we also incorporate alignment meta information extracted from an RBMT system to help the decoding process. The other crucial factor is the backbone selection which also affects the combination results. The backbone determines the word order in the final output. Backbone selection is often done by Minimum Bayes Risk (MBR) decoding which selects a hypothesis with minimum average distance among all hypotheses (Rosti et al., 2007a,b). In this work we heuristically choose an RBMT output as the backbone due to its (expected) overall grammatically well-formed output and better human evaluation results. We report our results and provide a comparison with traditional confusion-network-based network approach. The remainder of the paper is organized as follows: We will review the state-of-the-art system combination framework based on confusion networks in Section 2. We describe our experimental setup, how we extract the alignment information from meta-data and how we use it in Section 3. The results and analysis are also giv"
W12-5704,C96-2141,0,0.304371,"(1) H∈∇ The backbone is used to decide the word order of the final output. After obtaining the backbone, all other hypotheses are aligned to it. The alignment strategies include IHMM, TER, etc. Note that during the word alignment word reordering and ‘null’ insertion are performed, which is usually called normalization. The confusion network, which can be constructed directly from the normalized alignment is given in Figure 2b, in which case H1 is chosen as the backbone. 2.2 Indirect HMM Alignment In this work we implement the IHMM (He et al., 2008). IHMM is a refined version of HMM alignment (Vogel et al., 1996) which is widely used in bilingual word alignment (Och and Ney, 2003). Let B = (b1 , ..., bJ ) denote the J words in the backbone sentence, H = (h1 , ..., h I ) denote one of the hypothesis, and A = (a1 , ..., aJ ) denote the alignment of each backbone word to the hypothesis word. We use Equation 2 to compute the alignment probability of each word pair. In Equation 2, d represents the distortion model and p denotes the word similarity model. P(H|B) = X Y A j=1...J [d(a j |a j−1 , I)p(h j |ba j )] 39 (2) In order to handle the words which are aligned to an empty word, we also insert a null asso"
W12-5704,P11-1125,0,0.0120179,"iscus, 1997). In late the 90s, the speech recognition community produced a confusion network-based system combination approach, spreading instantly to SMT community as well. The traditional system combination approach employs confusion networks which are built by the monolingual alignment which induces sentence similarity. Confusion networks are compact graph-based structures representing multiple hypothesises (Bangalore et al., 2001). It is noted that there are several generalized forms of confusion networks as well. One is a lattice (Feng et al., 2009) and the other is a translation forest (Watanabe and Sumita, 2011). The former employs lattices that can describe arbitrary mappings in hypothesis alignments. A lattice is more general than a confusion network. By contrast, a confusion forest exploits syntactic similarity between individual outputs. Up to now, various state-of-the-art alignment methods have been developed including IndirectHMM (He et al., 2008; Du and Way, 2010) which is a statistical-model-based method, and TER which is a metric-based method which uses an edit distance. In this work we focus on the IHMM method. The main problem of IHMM is that there are numerous one-to-many and one-to-null"
W13-2222,W12-3141,0,0.0151486,"cess to obtain the best weights based on the n-best lists, (Repeat the decoding / MERT process several iterations. Then, we obtain the best weights for a particular class.) For the test phase, 1. Separate each class of the test set (keep the original index and new index in the allocated separated dataset). 2. Suppose the test sentence belongs to cluster i, run the decoder of cluster i. 3. Repeat the previous step until all the test sentences are decoded. 3.3 Context ID Context ID semantics is used through the reranking of the n-best list in a MERT process (Schwenk, 2007; Schwenk et al., 2012; Le et al., 2012). 2-layer ngram-HMM LM is a two layer version of the 1-layer ngram-HMM LM (Blunsom and Cohn, 2011) which is a nonparametric 1. For each label c ∈ {1, . . . C}, sample a distribution over word-types φc ∼ Dirichlet(·|β) 2. For each document d ∈ {1, . . . , D} (a) Sample a distribution over its observed labels θd ∼ Dirichlet(·|α) (b) For each word i ∈ {1, . . . , NdW } 1 Currently, we do not have a definite recommendation on this. It needs to be studied more deeply. 180 Bayesian method using hierarchical Pitman-Yor prior. In the 2-layer LM, the hidden sequence of the first layer becomes the input"
W13-2222,P11-1087,0,0.0148513,"ess several iterations. Then, we obtain the best weights for a particular class.) For the test phase, 1. Separate each class of the test set (keep the original index and new index in the allocated separated dataset). 2. Suppose the test sentence belongs to cluster i, run the decoder of cluster i. 3. Repeat the previous step until all the test sentences are decoded. 3.3 Context ID Context ID semantics is used through the reranking of the n-best list in a MERT process (Schwenk, 2007; Schwenk et al., 2012; Le et al., 2012). 2-layer ngram-HMM LM is a two layer version of the 1-layer ngram-HMM LM (Blunsom and Cohn, 2011) which is a nonparametric 1. For each label c ∈ {1, . . . C}, sample a distribution over word-types φc ∼ Dirichlet(·|β) 2. For each document d ∈ {1, . . . , D} (a) Sample a distribution over its observed labels θd ∼ Dirichlet(·|α) (b) For each word i ∈ {1, . . . , NdW } 1 Currently, we do not have a definite recommendation on this. It needs to be studied more deeply. 180 Bayesian method using hierarchical Pitman-Yor prior. In the 2-layer LM, the hidden sequence of the first layer becomes the input to the higher layer of inputs. Note that such an architecture comes from the Restricted Boltzmann"
W13-2222,J03-1002,0,0.00551079,"model (Teh, 2006). We used a blocked inference for inference. The performance of 2-layer LM is shown in Table 3. 4 P projection layer 2−layer conditional RBM language model Figure 3: Figure shows the three kinds of contextdependent LM. The upper-side shows continuousspace language model (Schwenk, 2007). The lower-left shows ours, i.e. the 2-layer ngramHMM LM. The lower-right shows the 2-layer conditional Restricted Boltzmann Machine LM (Taylor and Hinton, 2009). Experimental Settings We used Moses (Koehn et al., 2007) for PBSMT and HPBSMT systems in our experiments. The GIZA++ implementation (Och and Ney, 2003) of IBM Model 4 is used as the baseline for word alignment: Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. For phrase extraction the grow-diag-final heuristics described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. We then perform MERT process (Och, 2003) which optimizes the BLEU metric, while a 5-gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002). For the HPBSMT system, the chart-ba"
W13-2222,P03-1021,0,0.0155701,"icted Boltzmann Machine LM (Taylor and Hinton, 2009). Experimental Settings We used Moses (Koehn et al., 2007) for PBSMT and HPBSMT systems in our experiments. The GIZA++ implementation (Och and Ney, 2003) of IBM Model 4 is used as the baseline for word alignment: Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. For phrase extraction the grow-diag-final heuristics described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. We then perform MERT process (Och, 2003) which optimizes the BLEU metric, while a 5-gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002). For the HPBSMT system, the chart-based decoder of Moses (Koehn et al., 2007) is used. Most of the procedures are identical with the PBSMT systems except the rule extraction process (Chiang, 2005). The procedures to handle three kinds of semantics are implemented using the already mentioned algorithm. We use libSVM (Chang and Lin, 2011), and Mallet (McCallum, 2002) for Latent Dirichlet Allocation (LDA) (Blei et al., 2003). For the corpus"
W13-2222,P05-1033,0,0.0965582,"HMM, 3 iterations of Model 3, and 3 iterations of Model 4. For phrase extraction the grow-diag-final heuristics described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. We then perform MERT process (Och, 2003) which optimizes the BLEU metric, while a 5-gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002). For the HPBSMT system, the chart-based decoder of Moses (Koehn et al., 2007) is used. Most of the procedures are identical with the PBSMT systems except the rule extraction process (Chiang, 2005). The procedures to handle three kinds of semantics are implemented using the already mentioned algorithm. We use libSVM (Chang and Lin, 2011), and Mallet (McCallum, 2002) for Latent Dirichlet Allocation (LDA) (Blei et al., 2003). For the corpus, we used all the resources provided for the translation task at WMT13 for language model, that is parallel corpora (Europarl V7 (Koehn, 2005), Common Crawl corpus, UN corpus, and News Commentary) and monolingual corpora (Europarl V7, News Commentary, and News Crawl from 2007 to 2012). Experimental results are shown in Table 2. The left-most column (sem"
W13-2222,W10-4006,1,0.861831,"utput attached) to predict their BLEU scores for PBSMT and HPBSMT. Our motivation came from the comparison of a sequential learning system and a parser-based system. The typical decoder of the former is a Figure 1: Figure shows the difference of sentencebased performance between PBSMT and HPBSMT systems. Relation of Complexity of Source Sentence and Performance of HPBSMT and PBSMT It is interesting to note that PBSMT tends to be better than HPBSMT for European language pairs as the recent WMT workshop shows, while HPBSMT shows often better performance for distant language pairs such as EN-JP (Okita et al., 2010b) 178 Viterbi decoder while that of the latter is a CockeYounger-Kasami (CYK) decoder (Younger, 1967). The capability of these two systems provides an intuition about the difference of PBSMT and HPBSMT: the CYK decoder-based system has some capability to handle syntactic constructions while the Viterbi decoder-based system has only the capability of learning a sequence. For exties caused by inserted clauses, coordination, long Multiword Expressions, and parentheses, while the sequential learning system does not (This is since this is what the aim of the context-free grammar-based system is.)"
W13-2222,de-marneffe-etal-2006-generating,0,0.0353283,"Missing"
W13-2222,W12-5706,1,0.901057,"Missing"
W13-2222,N03-1017,0,0.00761191,"Missing"
W13-2222,W12-5705,1,0.900608,"Missing"
W13-2222,P07-2045,0,0.00934318,"and Gi is a base measure. Note that these terms belong to the hierarchical Pitman-Yor language model (Teh, 2006). We used a blocked inference for inference. The performance of 2-layer LM is shown in Table 3. 4 P projection layer 2−layer conditional RBM language model Figure 3: Figure shows the three kinds of contextdependent LM. The upper-side shows continuousspace language model (Schwenk, 2007). The lower-left shows ours, i.e. the 2-layer ngramHMM LM. The lower-right shows the 2-layer conditional Restricted Boltzmann Machine LM (Taylor and Hinton, 2009). Experimental Settings We used Moses (Koehn et al., 2007) for PBSMT and HPBSMT systems in our experiments. The GIZA++ implementation (Och and Ney, 2003) of IBM Model 4 is used as the baseline for word alignment: Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. For phrase extraction the grow-diag-final heuristics described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. We then perform MERT process (Och, 2003) which optimizes the BLEU metric, while a 5-gram language model is derived with Kneser-Ney smoothing"
W13-2222,W12-5707,1,0.909208,"that the sentence is to be translated by genre IDsensitive MT systems, again based on semantics on a sentence level. The context-dependent LM can be interpreted as supplying the local context to a word, capturing semantics on a word level. The architecture presented in this paper is substantially different from multi-engine system combination. Although the system has multiple paths, only one path is chosen at decoding when processing unseen data. Note that standard multi-engine system combination using these three semantics has been presented before (Okita et al., 2012b; Okita et al., 2012a; Okita, 2012). This paper also compares the two approaches. The remainder of this paper is organized as follows. Section 2 describes the motivation for our approach. In Section 3, we describe our proposed systems, while in Section 4 we describe the experimental results. We conclude in Section 5. This paper describes shallow semantically-informed Hierarchical Phrase-based SMT (HPBSMT) and Phrase-Based SMT (PBSMT) systems developed at Dublin City University for participation in the translation task between EN-ES and ES-EN at the Workshop on Statistical Machine Translation (WMT 13). The system uses PBSMT and"
W13-2222,W04-3250,0,0.0242566,") and monolingual corpora (Europarl V7, News Commentary, and News Crawl from 2007 to 2012). Experimental results are shown in Table 2. The left-most column (sem-inform) shows our results. The sem-inform made a improvement of 0.8 BLEU points absolute compared to the PBSMT results in EN-ES, while the standard system combination lost 0.1 BLEU points absolute compared to the single worst. For ES-EN, the sem-inform made an improvement of 0.7 BLEU points absolute compared to the PBSMT results. These improvements over both of PBSMT and HPBSMT are statistically significant by a paired bootstrap test (Koehn, 2004). 5 Conclusion This paper describes shallow semanticallyinformed HPBSMT and PBSMT systems developed at Dublin City University for participation in the translation task at the Workshop on Statistical Machine Translation (WMT 13). Our system has 181 EN-ES BLEU BLEU(11b) BLEU-cased BLEU-cased(11b) NIST Meteor WER PER ES-EN BLEU BLEU(11b) BLEU-cased BLEU-cased(11b) NIST Meteor WER PER sem-inform 30.3 30.3 29.0 29.0 7.91 0.580 53.7 41.3 sem-inform 31.1 31.1 29.7 29.7 7.87 0.615 54.8 41.3 PBSMT 29.5 29.5 28.4 28.4 7.74 0.579 55.4 42.4 PBSMT 30.4 30.4 29.1 29.1 7.79 0.612 55.4 41.8 HPBSMT 28.2 28.2 2"
W13-2222,2005.mtsummit-papers.11,0,0.00611639,"d with SRILM (Stolcke, 2002). For the HPBSMT system, the chart-based decoder of Moses (Koehn et al., 2007) is used. Most of the procedures are identical with the PBSMT systems except the rule extraction process (Chiang, 2005). The procedures to handle three kinds of semantics are implemented using the already mentioned algorithm. We use libSVM (Chang and Lin, 2011), and Mallet (McCallum, 2002) for Latent Dirichlet Allocation (LDA) (Blei et al., 2003). For the corpus, we used all the resources provided for the translation task at WMT13 for language model, that is parallel corpora (Europarl V7 (Koehn, 2005), Common Crawl corpus, UN corpus, and News Commentary) and monolingual corpora (Europarl V7, News Commentary, and News Crawl from 2007 to 2012). Experimental results are shown in Table 2. The left-most column (sem-inform) shows our results. The sem-inform made a improvement of 0.8 BLEU points absolute compared to the PBSMT results in EN-ES, while the standard system combination lost 0.1 BLEU points absolute compared to the single worst. For ES-EN, the sem-inform made an improvement of 0.7 BLEU points absolute compared to the PBSMT results. These improvements over both of PBSMT and HPBSMT are s"
W13-2222,P07-1040,0,0.0564181,"Missing"
W13-2222,J10-4005,0,0.0147593,". predictQEScore() predictGenreID() predictContextID(wordi , wordi−1 ) If we assume that this is one major difference between these two systems, the complexity of the input sentence will correlate with the difference of translation quality of these two systems. In this subsection, we assume that this is one major difference of these two systems and that the complexity of the input sentence will correlate with the difference of translation quality of these two systems. Based on these assumptions, we build a regressor Table 1: Decoding algorithm: the main algorithm of PBSMT and HPBSMT are from (Koehn, 2010). The modification is related to predictQEScore(), predictGenreID(), and predictContextID(). ample, the (context-free) grammar-based system has the capability of handling various difficul179 for each system for a given input sentence where in a training phase we supply the BLEU score measured using the training set. One remark is that the BLEU score which we predict is only meaningful in a relative manner since we actually generate a translation output in preparation phase (there is a dependency to the mean of BLEU score in the training set). Nevertheless, this is still meaningful as a relativ"
W13-2222,W12-2702,0,0.0199701,"lists, run a MERT process to obtain the best weights based on the n-best lists, (Repeat the decoding / MERT process several iterations. Then, we obtain the best weights for a particular class.) For the test phase, 1. Separate each class of the test set (keep the original index and new index in the allocated separated dataset). 2. Suppose the test sentence belongs to cluster i, run the decoder of cluster i. 3. Repeat the previous step until all the test sentences are decoded. 3.3 Context ID Context ID semantics is used through the reranking of the n-best list in a MERT process (Schwenk, 2007; Schwenk et al., 2012; Le et al., 2012). 2-layer ngram-HMM LM is a two layer version of the 1-layer ngram-HMM LM (Blunsom and Cohn, 2011) which is a nonparametric 1. For each label c ∈ {1, . . . C}, sample a distribution over word-types φc ∼ Dirichlet(·|β) 2. For each document d ∈ {1, . . . , D} (a) Sample a distribution over its observed labels θd ∼ Dirichlet(·|α) (b) For each word i ∈ {1, . . . , NdW } 1 Currently, we do not have a definite recommendation on this. It needs to be studied more deeply. 180 Bayesian method using hierarchical Pitman-Yor prior. In the 2-layer LM, the hidden sequence of the first layer"
W13-2222,P06-1124,0,0.0130387,"tput layer. The generative model for this is shown below. ¯ t ∼ F (φ¯s ) (1) ht |h t wt |ht ∼ F (φst ) wi |w1:i−1 ∼ PY(di , θi , Gi ) discrete representation N continuous−space language model [Schwenk, 2007] hidden layer N output layer probability estimation neural network 2nd hidden layer 2nd RBM 1st hidden layer 1st RBM output layer (2) ngram language model output layer ngram language model (3) 2−layer ngram−HMM language model where α is a concentration parameter, θ is a strength parameter, and Gi is a base measure. Note that these terms belong to the hierarchical Pitman-Yor language model (Teh, 2006). We used a blocked inference for inference. The performance of 2-layer LM is shown in Table 3. 4 P projection layer 2−layer conditional RBM language model Figure 3: Figure shows the three kinds of contextdependent LM. The upper-side shows continuousspace language model (Schwenk, 2007). The lower-left shows ours, i.e. the 2-layer ngramHMM LM. The lower-right shows the 2-layer conditional Restricted Boltzmann Machine LM (Taylor and Hinton, 2009). Experimental Settings We used Moses (Koehn et al., 2007) for PBSMT and HPBSMT systems in our experiments. The GIZA++ implementation (Och and Ney, 2003"
W13-2227,P10-2041,0,0.156028,"Missing"
W13-2227,D11-1033,0,0.177558,"Missing"
W13-2227,N07-1029,0,0.0348966,"gaword. Each LM is trained with the SRILM toolkit, before interpolating all the LMs according to their weights obtained by minimizing the perplexity on the tuning set (WMT2011 and WMT2012 test sets). As SRILM can only interpolate 10 LMs, we first interpolate a LM with Europarl, News Commentary, News Crawl (20072012, each year individually, 6 separate parts), then we interpolate a new LM with this interpolated LM and LDC Gigawords (we kept the Gigaword subsets separated according to the news sources as distributed by LDC, which leads to 7 corpus). We also use a word-level combination strategy (Rosti et al., 2007) to combine the three translation hypotheses. To combine these systems, we first use the Minimum Bayes-Risk (MBR) (Kumar and Byrne, 2004) decoder to obtain the 5 best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al., 2000). We then use IHMM (He et al., 2008) to choose the backbone build the CN and finally search for and generate the best translation. We tune the system parameters on development set with Simple-Simplex algorithm. The parameters for system weights are set equal. Other parameters like language model, length penalty and combination coefficient are"
W13-2227,P96-1041,0,0.0728056,"scribed. We investigate the use of linguistic information to select parallel data. In Section 3, we present the systems built for the French-English pair in both di2 Setting Our setup uses the M OSES toolkit, version 1.0 (Koehn et al., 2007). We use a pipeline with the phrase-based decoder with standard parameters, unless noted otherwise. The decoder uses cube pruning (-cube-pruning-pop-limit 2000 -s 2000), MBR (-mbr-size 800 -mbr-scale 1) and monotone at punctuation reordering. Individual language models (LMs), 5-gram and smoothed using a simplified version of the improved Kneser-Ney method (Chen and Goodman, 1996), are built for each monolingual corpus using IRSTLM 5.80.01 (Federico et al., 2008). These LMs are then interpolated with IRSTLM using the test set of WMT11 as the development set. Finally, the interpolated LMs are merged into one LM preserving the weights using SRILM (Stolcke, 2002). We use all the parallel corpora available for this language pair: Europarl (EU), News Commentary (NC), United Nations (UN) and Common Crawl (CC). Regarding monolingual corpora, we use the freely available monolingual corpora (EuIntroduction 1 Spanish-English http://www.nclt.dcu.ie/mt/ http://www.prompsit.com/ 21"
W13-2227,P10-4002,0,0.0227851,"rther experiments are still required to determine the minimum sample size needed to outperform both the in-domain system and the combination of the two translation models. Finally, for the German-English language pair, we presents our exploitation of long ordering problem. We compared two hierarchical models with one phrase-based model, and we also use a system combination strategy to further improve 4.2.2 Three baseline systems We use the data set up described by the former subsection and build up three baseline systems, namely PB M OSES (phrase-based), Hiero M OSES (hierarchical) and C DEC (Dyer et al., 2010). The motivation of choosing Hierarchical Models is to address the German-English’s long reorder problem. We want to test the performance of C DEC and Hiero M OSES and choose the best. PB M OSES is used as our benchmark. The three results obtained on the development and test sets for the three baseline system and the system combination are shown in the Table 6. PB M OSES Hiero M OSES C DEC Combination System Combination Test 24.0 24.4 24.4 24.8 Table 6: BLEU scores obtained by our systems on the development and test sets for the German to English translation task. From the Table 6 we can see t"
W13-2227,W13-2803,1,0.864471,"Missing"
W13-2227,D11-1020,1,0.829215,"Comb. 30.0 30.8 29.8 58.9 29.9 30.4 29.3 59.3 29.7 29.6 28.7 61.8 29.6 29.4 28.5 62.0 4 German-English In this section we describe our work on German to English subtask. Firstly we describe the Dependency tree to string method which we tried but unfortunately failed due to short of time. Secondly we discuss the baseline system and the preprocessing we performed. Thirdly a system combination method is described. 4.1 Dependency Tree to String Method Our original plan was to address the long distance reordering problem in German-English translation. We use Xie’s Dependency tree to string method(Xie et al., 2011) which obtains good results on Chinese to English translation and exhibits good performance at long distance reordering as our decoder. We use Stanford dependency parser4 to parse the English side of the data and Mate-Tool5 for the German side. The first set of experiments did not lead to encouraging results and due to insufficient time, we decide to switch to other decoders, based on statistical phrase-based and hierarchical approaches. Table 5: BLEU and TER scores obtained by our systems. BLEUdev is the score obtained on the development set given by MERT, while BLEU, BLEUcased and TER are ob"
W13-2227,W08-0509,0,0.0846195,"Missing"
W13-2227,D08-1011,0,0.0262708,"News Crawl (20072012, each year individually, 6 separate parts), then we interpolate a new LM with this interpolated LM and LDC Gigawords (we kept the Gigaword subsets separated according to the news sources as distributed by LDC, which leads to 7 corpus). We also use a word-level combination strategy (Rosti et al., 2007) to combine the three translation hypotheses. To combine these systems, we first use the Minimum Bayes-Risk (MBR) (Kumar and Byrne, 2004) decoder to obtain the 5 best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al., 2000). We then use IHMM (He et al., 2008) to choose the backbone build the CN and finally search for and generate the best translation. We tune the system parameters on development set with Simple-Simplex algorithm. The parameters for system weights are set equal. Other parameters like language model, length penalty and combination coefficient are chosen when we see a good improvement on development set. 5 Development 22.0 22.1 22.5 23.0 Conclusion This paper presented a set of experiments conducted on Spanish-English, French-English and German-English language pairs. For the SpanishEnglish pair, we have explored the use of linguisti"
W13-2227,P07-2045,0,\N,Missing
W13-2227,N04-1022,0,\N,Missing
W13-2256,W07-0411,0,0.0974486,"Missing"
W13-2256,P08-1007,0,0.0888122,"Missing"
W13-2256,W11-2105,0,0.0139993,"ard BLEU as their primary evaluation metric to develop and compare MT systems. However, BLEU only employs the lexical information and cannot adequately reflect the structural level similarity. Translation Error Rate (TER) (Snover et al., 2006) measures the number of edits required to change the hypothesis into one of the references. METEOR (Lavie and Agarwal, 2007), which defines loose unigram matching between the hypothesis and the references with the help of stemming and Wordnet-looking-up, is also a lexical based method and achieves the first-class humanevaluation-correlation score. AMBER (Chen and Kuhn, 2011; Chen et al., 2012) incorporates recall, extra penalties and some text processing variants on the basis of BLEU. The main weakness of all the above lexical based methods is that they cannot adequately reflect the structural level similarity. To overcome the weakness of the lexical based methods, many syntactic based metrics were proposed. Liu and Gildea (2005) proposed STM, a constituent tree based approach, and HWCM, a dependency tree based approach. In this paper, we attempt to overcome the shortcoming of the syntactic based methods and propose a novel dependency based MT evaluation metric."
W13-2256,W07-0714,0,0.207221,"Missing"
W13-2256,W12-3104,0,0.0130795,"mary evaluation metric to develop and compare MT systems. However, BLEU only employs the lexical information and cannot adequately reflect the structural level similarity. Translation Error Rate (TER) (Snover et al., 2006) measures the number of edits required to change the hypothesis into one of the references. METEOR (Lavie and Agarwal, 2007), which defines loose unigram matching between the hypothesis and the references with the help of stemming and Wordnet-looking-up, is also a lexical based method and achieves the first-class humanevaluation-correlation score. AMBER (Chen and Kuhn, 2011; Chen et al., 2012) incorporates recall, extra penalties and some text processing variants on the basis of BLEU. The main weakness of all the above lexical based methods is that they cannot adequately reflect the structural level similarity. To overcome the weakness of the lexical based methods, many syntactic based metrics were proposed. Liu and Gildea (2005) proposed STM, a constituent tree based approach, and HWCM, a dependency tree based approach. In this paper, we attempt to overcome the shortcoming of the syntactic based methods and propose a novel dependency based MT evaluation metric. The proposed metric"
W13-2256,P02-1040,0,0.0864131,"side. Experimental results show that our method achieves higher correlation with human judgments than BLEU, TER, HWCM and METEOR at both sentence and system level for all of the four language pairs in WMT 2010. 1 Introduction As we know that the hypothesis is potentially noisy, and these errors expand through the parsing process. Thus the power of syntactic information could be considerably weakened. Automatic evaluation plays a more important role in the evolution of machine translation. At the earliest stage, the automatic evaluation metrics only use the lexical information, in which, BLEU (Papineni et al., 2002) is the most popular one. BLEU is simple and effective. Most of the researchers regard BLEU as their primary evaluation metric to develop and compare MT systems. However, BLEU only employs the lexical information and cannot adequately reflect the structural level similarity. Translation Error Rate (TER) (Snover et al., 2006) measures the number of edits required to change the hypothesis into one of the references. METEOR (Lavie and Agarwal, 2007), which defines loose unigram matching between the hypothesis and the references with the help of stemming and Wordnet-looking-up, is also a lexical b"
W13-2256,W07-0734,0,0.0659736,"role in the evolution of machine translation. At the earliest stage, the automatic evaluation metrics only use the lexical information, in which, BLEU (Papineni et al., 2002) is the most popular one. BLEU is simple and effective. Most of the researchers regard BLEU as their primary evaluation metric to develop and compare MT systems. However, BLEU only employs the lexical information and cannot adequately reflect the structural level similarity. Translation Error Rate (TER) (Snover et al., 2006) measures the number of edits required to change the hypothesis into one of the references. METEOR (Lavie and Agarwal, 2007), which defines loose unigram matching between the hypothesis and the references with the help of stemming and Wordnet-looking-up, is also a lexical based method and achieves the first-class humanevaluation-correlation score. AMBER (Chen and Kuhn, 2011; Chen et al., 2012) incorporates recall, extra penalties and some text processing variants on the basis of BLEU. The main weakness of all the above lexical based methods is that they cannot adequately reflect the structural level similarity. To overcome the weakness of the lexical based methods, many syntactic based metrics were proposed. Liu an"
W13-2256,C10-2175,0,0.438985,"Missing"
W13-2256,W05-0904,0,\N,Missing
W14-3303,J03-1002,0,0.00990406,"ble at http://github.com/bicici/FDAOptimization. 2.2 Language Model Data Selection Results We run ParFDA5 SMT experiments for all language pairs in both directions in the WMT14 translation task (Bojar et al., 2014), which include English-Czech (en-cs), English-German (en-de), English-French (en-fr), English-Hindi (en-hi), and English-Russian (en-ru). We true-case all of the corpora, use 150-best lists during tuning, set the LM order to a value between 7 and 10 for all language pairs, and train the LM using SRILM (Stolcke, 2002). We set the maximum sentence length filter to 126 and for GIZA++ (Och and Ney, 2003), Parallel FDA5 Parallel FDA5 (ParFDA5) is presented in Algorithm 1, which first shuffles the training sentences, U and runs individual FDA5 models on the multiple splits from which equal number of sentences, 1 (Cormen et al., 2009), question 6.5-9. Merging k sorted lists into one sorted list using a min-heap for k-way merging. 2 Unless the translation is a verbatim copy of the source. 60 S→T en-cs en-cs cs-en cs-en en-de en-de de-en de-en en-fr en-fr fr-en fr-en en-hi en-hi hi-en hi-en en-ru en-ru ru-en ru-en Data C ParFDA5 C ParFDA5 C ParFDA5 C ParFDA5 C ParFDA5 C ParFDA5 C ParFDA5 C ParFDA5"
W14-3303,W13-2206,1,0.833312,"Missing"
W14-3303,W14-3332,0,0.151794,"e the diversity. Algorithm 1: Parallel FDA5 Input: Parallel training sentences U, test set features F, and desired number of training instances N . Output: Subset of the parallel sentences to be used as the training data L ⊆ U. 1 U ← shuffle(U) 2 U , M ← split(U, N ) 3 L ← {} 4 foreach Ui ∈ U do 5 hLi , si i ← FDA5(Ui , F, M ) 6 L ← L ∪ hLi , si i 7 L ← merge(L) 2.3 We select the LM training data with ParFDA5 based on the following observation (Bic¸ici, 2013): 3.22 BLEU points compared to random selection. FDA5 is also used for selecting the training set in the WMT14 medical translation task (Calixto et al., 2014) and the tuning set in the WMT14 German-English translation task (Li et al., 2014). FDA5 has 5 parameters that effect the instance scores based on the three formulas used: No word not appearing in the training set can appear in the translation. It is impossible for an SMT system to translate a word unseen in the training corpus nor can it translate it with a word not found in the target side of the training set 2 . Thus we are only interested in correctly ordering the words appearing in the training corpus and collecting the sentences that contain them for building the LM. At the same time, a"
W14-3303,P07-2045,0,0.0130258,"Deployment of Accurate Statistical Machine Translation Systems Ergun Bic¸ici Qun Liu Andy Way Centre for Next Generation Localisation Centre for Next Generation Localisation Centre for Next Generation Localisation School of Computing School of Computing School of Computing Dublin City University Dublin City University Dublin City University ergun.bicici@computing.dcu.ie qliu@computing.dcu.ie away@computing.dcu.ie Abstract Parallel FDA5 runs separate FDA5 models on randomized subsets of the training data and combines the selections afterwards. We run parallel FDA5 SMT experiments using Moses (Koehn et al., 2007) in all language pairs in WMT14 (Bojar et al., 2014) and obtain SMT performance close to the top constrained Moses systems training using all of the training material. Parallel FDA5 allows rapid prototyping of SMT systems for a given target domain or task and can be very useful for MT in target domains with limited resources or in disaster and crisis situations (Lewis et al., 2011). We use parallel FDA5, an efficiently parameterized and optimized parallel implementation of feature decay algorithms for fast deployment of accurate statistical machine translation systems, taking only about half a"
W14-3303,W11-2164,0,0.0270653,"ting.dcu.ie away@computing.dcu.ie Abstract Parallel FDA5 runs separate FDA5 models on randomized subsets of the training data and combines the selections afterwards. We run parallel FDA5 SMT experiments using Moses (Koehn et al., 2007) in all language pairs in WMT14 (Bojar et al., 2014) and obtain SMT performance close to the top constrained Moses systems training using all of the training material. Parallel FDA5 allows rapid prototyping of SMT systems for a given target domain or task and can be very useful for MT in target domains with limited resources or in disaster and crisis situations (Lewis et al., 2011). We use parallel FDA5, an efficiently parameterized and optimized parallel implementation of feature decay algorithms for fast deployment of accurate statistical machine translation systems, taking only about half a day for each translation direction. We build Parallel FDA5 Moses SMT systems for all language pairs in the WMT14 translation task and obtain SMT performance close to the top Moses systems with an average of 3.49 BLEU points difference using significantly less resources for training and development. 1 2 2.1 Introduction Parallel FDA5 for Instance Selection FDA5 FDA is developed mai"
W14-3303,W14-3314,1,0.812886,"Missing"
W14-3303,W11-2131,1,\N,Missing
W14-3303,W14-3302,0,\N,Missing
W14-3314,P11-1105,0,0.06307,"nfiguration for submitted system will be presented. And we conclude in the last section. Introduction On the German-English translation task of WMT 2014, we submitted a system which is built with Moses phrase-based model (Koehn et al., 2007). For system training, we use all provided German-English parallel data, and conducted several pre-processing steps to clean the data. In addition, in order to improve the translation quality, we adopted some popular techniques, including three Lexicalized Reordering Models (Axelrod et al., 2005; Galley and Manning, 2008), a 9gram Operation Sequence Model (Durrani et al., 2011) and Language Model interpolation on several datasets. And then we use system combination on several systems with different settings to produce the final outputs. Our phrase-based systems are tuned with k-best MIRA (Cherry and Foster, 2012) on development set. We set the maximum iteration to be 25. The Language Models in our systems are trained with SRILM (Stolcke, 2002). We trained 2 Pre-processing We use all the training data for German-English translation, including Europarl, News Commentary and Common Crawl. The first thing we noticed is that some Non-German and Non-English sentences are i"
W14-3314,N13-1073,0,0.0337443,"ases, are designed to select the right translation. But different with (He et al., 2008), we use sparse features to model the context. And instead of using syntactic POS, we adopt independent POS-like features: cluster ID of word. In our experiment mkcls was used to cluster words into 50 groups. And all features are generalized to cluster ID. 4 • Multi-alignment Selection: We also try to use multi-alignment selection (Tu et al., 2012) to generate a ”better” alignment from three alignmens: MGIZA++ with function growdiag-final-and, SyMGIZA++ with function grow-diag-final-and and fast alignment (Dyer et al., 2013). Although this method show comparable or better result on development set, it fails on test set. Since we build a few systems with different setting on Moses phrase-based model, a straightforward thinking is to obtain the better translation from several different translation systems. So we use system combination (Heafield and Lavie, 2010) on the 1-best outputs of three systems (indicated with ∗ in table 4). And this results in our best system so far, as shown in Table 4. In our final submission, this result is taken as primary. Submission Based on our preliminary experiments in the section ab"
W14-3314,D08-1089,0,0.526005,"s task and the experiment results. In section 4, our final configuration for submitted system will be presented. And we conclude in the last section. Introduction On the German-English translation task of WMT 2014, we submitted a system which is built with Moses phrase-based model (Koehn et al., 2007). For system training, we use all provided German-English parallel data, and conducted several pre-processing steps to clean the data. In addition, in order to improve the translation quality, we adopted some popular techniques, including three Lexicalized Reordering Models (Axelrod et al., 2005; Galley and Manning, 2008), a 9gram Operation Sequence Model (Durrani et al., 2011) and Language Model interpolation on several datasets. And then we use system combination on several systems with different settings to produce the final outputs. Our phrase-based systems are tuned with k-best MIRA (Cherry and Foster, 2012) on development set. We set the maximum iteration to be 25. The Language Models in our systems are trained with SRILM (Stolcke, 2002). We trained 2 Pre-processing We use all the training data for German-English translation, including Europarl, News Commentary and Common Crawl. The first thing we notice"
W14-3314,W08-0509,0,0.030245,"selected by FDA is better than the baseline tuned with all the development data. 3.2 The Operation Sequence Model (OSM) (Durrani et al., 2011) explains the translation procedure as a linear sequence of operations which generates source and target sentences in parallel. Durrani et al. (2011) defined four translation operations: Generate(X,Y), Continue Source Concept, Generate Source Only (X) and Generate Identical, as well as three reordering operations: Insert Gap, Jump Back(W) and Jump Forward. These operations are described as follows. In this section, alignment model is trained by MGIZA++ (Gao and Vogel, 2008) with grow-diag-final-and heuristic function. And other settings are mostly default values in Moses. 3.1 Operation Sequence Model Lexicalized Reordering Model • Generate(X,Y) make the words in Y and the first word in X added to target and source string respectively. German and English have different word order which brings a challenge in German-English machine translation. In our system, we adopt three Lexicalized Reordering Models (LRMs) for addressing this problem. They are word-based LRM (wLRM), phrase-based LRM (pLRM) and hierarchal LRM (hLRM). • Continue Source Concept adds the word in th"
W14-3314,2005.iwslt-1.8,0,0.0531592,"we have tried for this task and the experiment results. In section 4, our final configuration for submitted system will be presented. And we conclude in the last section. Introduction On the German-English translation task of WMT 2014, we submitted a system which is built with Moses phrase-based model (Koehn et al., 2007). For system training, we use all provided German-English parallel data, and conducted several pre-processing steps to clean the data. In addition, in order to improve the translation quality, we adopted some popular techniques, including three Lexicalized Reordering Models (Axelrod et al., 2005; Galley and Manning, 2008), a 9gram Operation Sequence Model (Durrani et al., 2011) and Language Model interpolation on several datasets. And then we use system combination on several systems with different settings to produce the final outputs. Our phrase-based systems are tuned with k-best MIRA (Cherry and Foster, 2012) on development set. We set the maximum iteration to be 25. The Language Models in our systems are trained with SRILM (Stolcke, 2002). We trained 2 Pre-processing We use all the training data for German-English translation, including Europarl, News Commentary and Common Crawl"
W14-3314,C08-1041,1,0.816799,"gressively. • SyMGIZA++: Better alignment could lead to better translation. So we carry out some experiments on SyMGIZA++ aligner (JunczysDowmunt and Sza, 2012), which modifies the original IBM/GIZA++ word alignment models to allow to update the symmetrized models between chosen iterations of the original training algorithms. Experiment shows this new alignment improves translation quality. • Sparse Features: For each source phrase, there is usually more than one corresponding translation option. Each different translation may be optimal in different contexts. Thus in our systems, similar to (He et al., 2008) which proposed a Maximum Entropy-based rule selection for the hierarchical phrasebased model, features which describe the context of phrases, are designed to select the right translation. But different with (He et al., 2008), we use sparse features to model the context. And instead of using syntactic POS, we adopt independent POS-like features: cluster ID of word. In our experiment mkcls was used to cluster words into 50 groups. And all features are generalized to cluster ID. 4 • Multi-alignment Selection: We also try to use multi-alignment selection (Tu et al., 2012) to generate a ”better” a"
W14-3314,P96-1041,0,0.259229,"Missing"
W14-3314,D07-1091,0,0.0381084,"· · oJ ) is: p(O) = J Y p(oj |oj−n+1 · · · oj−1 ) (1) j=1 where n indicates the number of previous operations used. In this paper we train a 9-gram OSM on training data and integrate this model directly into loglinear framework (OSM is now available to use in Moses). Our experiment shows OSM improves our system by about 0.8 BLEU (see Table 2). 3.3 3.4 Other Tries In addition to the techniques mentioned above, we also try some other approaches. Unfortunately all of these methods described in this section are non-effective in our experiments. The results are shown in Table 2. • Factored Model (Koehn and Hoang, 2007): We tried to integrate a target POS factored model into our system with a 9-gram POS language model to address the problem of word selection and word order. But experiment doesn’t show improvement. The English POS is from Stanford POS Tagger (Toutanova et al., 2003). Language Model Interpolation In our baseline, Language Model (LM) is trained on all the monolingual data provided. In this section, we try to build a large language model by including data from English Gigaword fifth edition (only taking partial data with size of 1.6G), English side of UN corpus and English side of 109 French-Eng"
W14-3314,E03-1076,0,0.139113,"Missing"
W14-3314,P07-2045,0,0.00785319,"detection: percentage of filtered out sentences a 5-gram model with Kneser-Ney discounting (Chen and Goodman, 1996). In the next sections, we will describe our system in detail. In section 2, we will explain our preprocessing steps on corpus. Then in section 3, we will describe some techniques we have tried for this task and the experiment results. In section 4, our final configuration for submitted system will be presented. And we conclude in the last section. Introduction On the German-English translation task of WMT 2014, we submitted a system which is built with Moses phrase-based model (Koehn et al., 2007). For system training, we use all provided German-English parallel data, and conducted several pre-processing steps to clean the data. In addition, in order to improve the translation quality, we adopted some popular techniques, including three Lexicalized Reordering Models (Axelrod et al., 2005; Galley and Manning, 2008), a 9gram Operation Sequence Model (Durrani et al., 2011) and Language Model interpolation on several datasets. And then we use system combination on several systems with different settings to produce the final outputs. Our phrase-based systems are tuned with k-best MIRA (Cher"
W14-3314,W13-2221,0,0.0180035,"d the results are shown in Table 4. to minimize perplexity measured on development set. We try to split the training data in two ways. One is according to data source, resulting in three subsets: Europarl, News Commentary and Common Crawl. Another one is to use data selection. We use FDA to select 200K sentence pairs as in-domain data and the rest as out-domain data. Unfortunately both experiments failed. In Table 2, we only report results of phrase table combination on FDA-based data sets. • Development Set Selection: Instead of using FDA which is dependent on test set, we use the method of (Nadejde et al., 2013) to select tuning set from newstest 2008-2013 for the final system. We only keep 2K sentences which have more than 30 words and higher BLEU score. The experiment result is shown in Table 4 ( The system is indicated as Baseline). • OSM Interpolation: Since OSM in our system could be taken as a special language model, we try to use the idea of interpolation similar with language model to make OSM adapted to some data. Training data are splitted into two subsets with FDA. We train 9-gram OSM on each subsets and interpolate them according to OSM trained on the development set. • Pre-processing: In"
W14-3314,E12-1055,0,0.0274196,"line, Language Model (LM) is trained on all the monolingual data provided. In this section, we try to build a large language model by including data from English Gigaword fifth edition (only taking partial data with size of 1.6G), English side of UN corpus and English side of 109 French-English corpus. Instead of training a single model on all data, we interpolate language models trained on each subset (monolingual data provided is splitted into three parts: News 20072013, Europarl and News Commentary) by tuning • Translation Model Combination: In this experiment, we try to use the method of (Sennrich, 2012) to combine phrase tables or reordering tables from different subsets of data 138 final submission. And the results are shown in Table 4. to minimize perplexity measured on development set. We try to split the training data in two ways. One is according to data source, resulting in three subsets: Europarl, News Commentary and Common Crawl. Another one is to use data selection. We use FDA to select 200K sentence pairs as in-domain data and the rest as out-domain data. Unfortunately both experiments failed. In Table 2, we only report results of phrase table combination on FDA-based data sets. •"
W14-3314,N03-1033,0,0.00982984,"). Our experiment shows OSM improves our system by about 0.8 BLEU (see Table 2). 3.3 3.4 Other Tries In addition to the techniques mentioned above, we also try some other approaches. Unfortunately all of these methods described in this section are non-effective in our experiments. The results are shown in Table 2. • Factored Model (Koehn and Hoang, 2007): We tried to integrate a target POS factored model into our system with a 9-gram POS language model to address the problem of word selection and word order. But experiment doesn’t show improvement. The English POS is from Stanford POS Tagger (Toutanova et al., 2003). Language Model Interpolation In our baseline, Language Model (LM) is trained on all the monolingual data provided. In this section, we try to build a large language model by including data from English Gigaword fifth edition (only taking partial data with size of 1.6G), English side of UN corpus and English side of 109 French-English corpus. Instead of training a single model on all data, we interpolate language models trained on each subset (monolingual data provided is splitted into three parts: News 20072013, Europarl and News Commentary) by tuning • Translation Model Combination: In this"
W14-3314,C12-2122,1,0.756888,"Missing"
W14-3314,N12-1047,0,\N,Missing
W14-3325,bojar-etal-2014-hindencorp,0,0.146073,"Missing"
W14-3325,P96-1041,0,0.232301,"including data from the English Gigaword fifth edition, the English side of the UN corpus, the English side of the 109 French–English corpus and the English side of the Hindi–English parallel data provided by the organisers. We interpolate language models trained using each dataset, with the monolingual data provided split into three parts (news 2007-2013, Europarl (?) and news commentary) and the weights tuned to minimize perplexity on the target side of the devset. The language models in our systems are trained with SRILM (Stolcke, 2002). We train a 5-gram model with Kneser-Ney discounting (Chen and Goodman, 1996). 3.4 (2) (3) For the Hindi-to-English translation task, we use part-of-speech (PoS) tags4 of the source phrase and the neighbouring words as the contextual feature, owing to the fact that supertaggers are readily available only for English. We use a memory-based machine learning (MBL) classifier (TRIBL: (Daelemans, 2005))5 that is able to estimate P(ˆ ek |fˆk , CI(fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations. Thus, we derive the feature ˆ mbl defined in Equation (2). In addition to h ˆ mbl , h 4 In order to obtain PoS tags"
W14-3325,P11-1105,0,0.0954256,"anslation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). 1 http://en.wikipedia.org/wiki/UTF-8 http://en.wikipedia.org/wiki/WX_notation 3 The acronyms stand for (Left|Right) (Round|Square|Curly) Bracket. 2 215 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 215–220, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics problem, where a source phrase with given additional context information is classified into a distribution over possible target phrases. The size of this distribution needs to be limited, and would ideally omit irrelevant target ph"
W14-3325,D08-1089,0,0.0630626,"tokens on either side and Introduction This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi-English translation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). 1 http://en.wikipedia.org/wiki/UTF-8 http://en.wikipedia.org/wiki/WX_notation 3 The acronyms stand for (Left|Right) (Round|Square|Curly) Bracket. 2 215 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 215–220, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics problem, where a source phrase with given additional context information is classified into a distribution over"
W14-3325,2005.iwslt-1.8,0,0.0390493,"ntaining more than 80 tokens on either side and Introduction This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi-English translation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). 1 http://en.wikipedia.org/wiki/UTF-8 http://en.wikipedia.org/wiki/WX_notation 3 The acronyms stand for (Left|Right) (Round|Square|Curly) Bracket. 2 215 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 215–220, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics problem, where a source phrase with given additional context information is classifie"
W14-3325,N03-1017,0,0.0101267,"tp://en.wikipedia.org/wiki/UTF-8 http://en.wikipedia.org/wiki/WX_notation 3 The acronyms stand for (Left|Right) (Round|Square|Curly) Bracket. 2 215 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 215–220, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics problem, where a source phrase with given additional context information is classified into a distribution over possible target phrases. The size of this distribution needs to be limited, and would ideally omit irrelevant target phrase translations that the standard PB-SMT (Koehn et al., 2003) approach would normally include. Following Haque et al. (2011), we derive a context-informed feature ˆ mbl that is expressed as the conditional probabilh ity of the target phrase eˆk given the source phrase fˆk and its context information (CI), as in (2): sentence pairs with length difference larger than 3 times. 3 3.1 Techniques Deployed Combination of Various Lexical Reordering Model (LRM) Clearly, Hindi and English have quite different word orders, so we adopt three lexical reordering models to address this problem. They are wordbased LRM and phrase-based LRM, which mainly focus on local r"
W14-3325,I08-2099,0,0.075816,"Missing"
W14-3325,P07-2045,0,0.0107113,"-, -LSB-, LCB-, -RRB-, -RSB-, and -RCB-.3 For consistency, those character sequences in the training data were replaced by the corresponding brackets. For English – both monolingual and the target side of the bilingual data – we perform tokenization, normalization of punctuation, and truecasing. For parallel training data, we filter sentences pairs containing more than 80 tokens on either side and Introduction This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi-English translation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). 1 http://en.wikipedia.org/wiki/UTF-8 http://en.wikipedia.org"
W14-3325,bojar-etal-2010-data,0,0.0144511,"experimental results and resultant discussion. This paper describes the DCU-Lingo24 submission to WMT 2014 for the HindiEnglish translation task. We exploit miscellaneous methods in our system, including: Context-Informed PB-SMT, OOV Word Conversion (OWC), MultiAlignment Combination (MAC), Operation Sequence Model (OSM), Stemming Align and Normal Phrase Extraction (SANPE), and Language Model Interpolation (LMI). We also describe various preprocessing steps we tried for Hindi in this task. 1 2 Pre-processing Steps We use all the training data provided for Hindi– English translation. Following Bojar et al. (2010), we apply a number of normalisation methods on the Hindi corpus. The HindEnCorp parallel corpus compiles several sources of parallel data. We observe that the source-side (Hindi) of the TIDES data source contains font-related noise, i.e. many Hindi sentences are a mixture of two different encodings: UTF-81 and WX2 notations. We prepared a WX-to-UTF-8 font conversion script for Hindi which converts all WX encoded characters into UTF-8, thus removing all WX encoding appearing in the TIDES data. We also observe that a portion of the English training corpus contained the following bracketlike seq"
W14-3325,W04-3230,0,0.0852863,"Missing"
W14-3325,N06-1014,0,0.117013,"Missing"
W14-3325,J03-1002,0,0.00674523,"anguage model, and use a continuous space representation to project words to each other on the basis of a small amount of correspondences in the dictionary. The following algorithm shows the steps involved: 1. Prepare the monolingual source and target sentences. Multi-Alignment Combination (MAC) Word alignment is a critical component of MT systems. Various methods for word alignment have been proposed, and different models can produce signicantly different outputs. For example, Tu et al. (2012) demonstrates that the alignment agreement between the two best-known alignment tools, namely Giza++(Och and Ney, 2003) and 2. Prepare the dictionary which consists of U entries of source and target sentences comprising non-stop-words. 3. Train the neural network language model on the source side and obtain the real vectors of X dimensions for each word. 6 Suffixes were separated and completely removed from the training data. 7 217 http://code.google.com/p/berkeleyaligner/ 4. Train the neural network language model on the target side and obtain the real vectors of X dimensions for each word. 4.2 We employ a standard Moses PB-SMT model as our baseline. The Hindi side is preprocessed but unstemmed. We use Giza++"
W14-3325,P03-1021,0,0.0686331,"For consistency, those character sequences in the training data were replaced by the corresponding brackets. For English – both monolingual and the target side of the bilingual data – we perform tokenization, normalization of punctuation, and truecasing. For parallel training data, we filter sentences pairs containing more than 80 tokens on either side and Introduction This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi-English translation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). 1 http://en.wikipedia.org/wiki/UTF-8 http://en.wikipedia.org/wiki/WX_notation 3 The acronyms"
W14-3325,W14-3329,1,0.619071,", word stemming on the Hindi side usually results in too many English words being aligned to one stemmed Hindi word, i.e. we encounter the problem of phrase overextraction. Therefore, we conduct word alignment with the stemmed version of Hindi, and then at the phrase extraction step, we replace the stemmed form with the original Hindi form. 3.8 OOV Word Conversion Method Our algorithm for OOV word conversion uses the recently developed zero-shot learning (Palatucci et al., 2009) using neural network language modelling (Bengio et al., 2000; Mikolov et al., 2013). The same technique is used in (Okita et al., 2014). This method requires neither parallel nor comparable corpora, but rather two monolingual corpora. In our context, we prepare two monolingual corpora on both sides, which are neither parallel nor comparable, and a small amount of already known correspondences between words on the source and target sides (henceforth, we refer to this as the ‘dictionary’). Then, we train both sides with the neural network language model, and use a continuous space representation to project words to each other on the basis of a small amount of correspondences in the dictionary. The following algorithm shows the"
W14-3325,C12-2122,1,0.901635,"Missing"
W14-3329,2005.mtsummit-papers.11,0,0.0124348,"term = {eterm1 , . . . , etermn } and Fterm = {fterm1 , . . . , ftermn }. We search for possible alignment links between the term-pair only when they co-occur in the same sentence. One obvious advantage of this approach is the computational complexity which is fairly low. Note that the result of (Okita et al., 2010) shows that the frequency-based approach of (Kupiec, 1993) worked well for NTCIR patent terminology (Fujii et al., 2010), which otherwise would have been difficult to capture via the traditional SMT/GIZA++ method. In contrast, however, this did not work well on the Europarl corpus (Koehn, 2005). 240 However, even among European languages, this mechanism makes it possible to find possible translation counterparts for a given term. In this query task, we did this only for the French-to-English direction and only for words containing accented characters (by rule-based conversion). 2.3 Terminology Dictionaries Terminology dictionaries themselves are obviously among the most important resources for bilingual term-pairs. In this medical query translation subtask, two corpora are provided for this purpose: (i) Unified Medical Language System corpus (UMLS corpus),1 and (ii) Wiki entries.2 2"
W14-3329,J10-4005,0,0.0130232,"small subset of sentences includes a few terms, such that term-pairs become sparse with regard to sentences. Term-pairs can be found rather easily if a candidate term-pair cooccurs on the source and the target sides and on the condition that the items in the term-pair actually correspond with one another. 2.1 Translation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). This condition can be easily checked in various ways. One way is to translate the source side of the targeted pairs with the alignment option in the Moses decoder (Koehn et al., 2007), which we did in this evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation mod"
W14-3329,P93-1003,0,0.0136536,"is evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extract term-pairs, which are the common focus in this medical query translation task. The classical algorithm of (Kupiec, 1993) used in (Okita et al., 2010) counts the statistics of terminology c(etermi , ftermj |st ) on the source and the target sides which jointly occur in a sentence st after detecting candidate terms via POS tagging, which are then summed up over the entire P corpus N t=1 c(etermi , ftermj |st ). Then, the algorithm adjusts the length of etermi and ftermj . It can be said that this algorithm captures termpairs which occur rather frequently. However, this We assume two predefined sets of terms at the outset, Eterm = {eterm1 , . . . , etermn } and Fterm = {fterm1 , . . . , ftermn }. We search for pos"
W14-3329,J93-2003,0,0.023956,". Accordingly, if our aim changes to capture only those less frequent pairs, the situation changes dramatically. The number of terms we need to capture is considerably decreased. Many sentences do not include any terminology at all, and only a relatively small subset of sentences includes a few terms, such that term-pairs become sparse with regard to sentences. Term-pairs can be found rather easily if a candidate term-pair cooccurs on the source and the target sides and on the condition that the items in the term-pair actually correspond with one another. 2.1 Translation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). This condition can be easily checked in various ways. One way is to translate the source side of the targeted pairs with the alignment option in the Moses decoder (Koehn et al., 2007), which we did in this evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pair"
W14-3329,N13-1090,0,0.12842,"i) “What can we do if the terminology does not occur in a corpus?” These two problems require computationally quite different approaches than what is usually done in the standard statistical approach. Furthermore, the medical query task in WMT14 provides a wide range of corpora: parallel and monolingual corpora, as well as dictionaries. These two interesting aspects motivate our extraction methods which we present in this section, including one relatively new Machine Learning algorithm of zero-shot learning arising from recent developments in the neural network community (Bengio et al., 2000; Mikolov et al., 2013b). It is possible to approach this in a reverse manner: “less frequent pairs can be outstanding term candidates”. Accordingly, if our aim changes to capture only those less frequent pairs, the situation changes dramatically. The number of terms we need to capture is considerably decreased. Many sentences do not include any terminology at all, and only a relatively small subset of sentences includes a few terms, such that term-pairs become sparse with regard to sentences. Term-pairs can be found rather easily if a candidate term-pair cooccurs on the source and the target sides and on the condi"
W14-3329,P09-1104,0,0.0127763,"nslation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). This condition can be easily checked in various ways. One way is to translate the source side of the targeted pairs with the alignment option in the Moses decoder (Koehn et al., 2007), which we did in this evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extract term-pairs, which are the common focus in this medical query translation task. The classical algorithm of (Kupiec, 1993) used in (Okita et al., 2010) counts the statistics of terminology c(etermi , ftermj |st ) on the source a"
W14-3329,J03-1002,0,0.0203194,"o translate the source side of the targeted pairs with the alignment option in the Moses decoder (Koehn et al., 2007), which we did in this evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extract term-pairs, which are the common focus in this medical query translation task. The classical algorithm of (Kupiec, 1993) used in (Okita et al., 2010) counts the statistics of terminology c(etermi , ftermj |st ) on the source and the target sides which jointly occur in a sentence st after detecting candidate terms via POS tagging, which are then summed up over the entire P corpus N t=1 c(etermi , ftermj |st ). Then, the algorithm adjusts the length of etermi and ftermj . It can be said that this algorithm captures termpairs which occur rather frequently. However, this We assume two"
W14-3329,P03-1021,0,0.108968,"Missing"
W14-3329,N03-1017,0,0.0289087,"Missing"
W14-3329,P07-2045,0,0.0230485,"rs on the source and the target sides and on the condition that the items in the term-pair actually correspond with one another. 2.1 Translation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). This condition can be easily checked in various ways. One way is to translate the source side of the targeted pairs with the alignment option in the Moses decoder (Koehn et al., 2007), which we did in this evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extract term-pairs, which are the common focus in this medical query translation task. The clas"
W14-3329,W10-4006,1,0.807417,"h a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). This condition can be easily checked in various ways. One way is to translate the source side of the targeted pairs with the alignment option in the Moses decoder (Koehn et al., 2007), which we did in this evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extract term-pairs, which are the common focus in this medical query translation task. The classical algorithm of (Kupiec, 1993) used in (Okita et al., 2010) counts the statistics of terminology c(etermi , ftermj |st ) on the source and the target sides which jointly occur in a sentence st after detecting candidate terms via POS tagging, which are then summed up over the"
W14-3329,W04-3250,0,0.0737557,"Missing"
W14-3329,P02-1040,0,0.0884738,"Missing"
W14-3329,H05-1059,0,0.0191079,"Missing"
W14-3329,2013.tc-1.12,1,0.769408,"Missing"
W14-3355,W07-0714,0,0.0379215,"Missing"
W14-3355,P02-1040,0,0.0911543,"Missing"
W14-3355,J10-4005,0,0.0585986,"Missing"
W14-3355,W11-2131,0,0.0431015,"Missing"
W14-3355,2006.amta-papers.25,0,0.101685,"Missing"
W14-3355,P08-1007,0,0.0495787,"Missing"
W14-3355,W13-2256,1,0.676534,"Missing"
W14-3355,W11-2107,0,0.0522041,"Missing"
W14-3355,C14-1193,1,0.813347,"Missing"
W14-3355,C10-2175,0,0.0290084,"Missing"
W14-3355,W07-0734,0,0.0719742,"Missing"
W14-3355,W05-0904,0,0.0897203,"Missing"
W14-3355,W07-0411,0,0.0431701,"Missing"
W14-4014,W08-0336,0,0.118081,"e NIST 2002 as the development set to tune weights, and NIST 2004 (MT04) and NIST 2005 (MT05) as the test data to evaluate the systems. Table 1 provides a summary of the Chinese–English corpus. The German–English training corpus is from WMT 2014, including Europarl V7 and News Commentary. News-test 2011 is taken as the development set, while News-test 2012 (test12) and News-test 2013 (test13) are our test sets. Table 2 provides a summary of the German–English corpus. 5.2 MT05 33.91 33.79 5.3 Baseline Chinese–English In the Chinese–English translation task, the Stanford Chinese word segmenter (Chang et al., 2008) is used to segment Chinese sentences into words. The Stanford dependency parser (Chang et al., 2009) parses a Chinese sentence into the projective dependency tree. For both language pairs, we filter sentence pairs longer than 80 words and keep the length ratio less than or equal to 3. English sentences are tokenized with scripts in Moses. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and (Koehn et al., 2003). We use SRILM (Stolcke, 2002) to 5 In this paper, the use of phrasal rules is similar to that of the HPB model, so they can be hand"
W14-4014,W09-2307,0,0.178172,"test data to evaluate the systems. Table 1 provides a summary of the Chinese–English corpus. The German–English training corpus is from WMT 2014, including Europarl V7 and News Commentary. News-test 2011 is taken as the development set, while News-test 2012 (test12) and News-test 2013 (test13) are our test sets. Table 2 provides a summary of the German–English corpus. 5.2 MT05 33.91 33.79 5.3 Baseline Chinese–English In the Chinese–English translation task, the Stanford Chinese word segmenter (Chang et al., 2008) is used to segment Chinese sentences into words. The Stanford dependency parser (Chang et al., 2009) parses a Chinese sentence into the projective dependency tree. For both language pairs, we filter sentence pairs longer than 80 words and keep the length ratio less than or equal to 3. English sentences are tokenized with scripts in Moses. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and (Koehn et al., 2003). We use SRILM (Stolcke, 2002) to 5 In this paper, the use of phrasal rules is similar to that of the HPB model, so they can be handled by Moses directly. 128 Systems Moses HPB D2S +pseudo-forest +sub-structural rules +pseudo-forest"
W14-4014,P96-1041,0,0.223319,"Missing"
W14-4014,P99-1065,0,0.058104,"Missing"
W14-4014,W13-3710,0,0.0271875,"Missing"
W14-4014,W02-1039,0,0.189689,"Missing"
W14-4014,P03-1021,0,0.0691294,"Missing"
W14-4014,W06-3601,0,0.0254702,"hnology Chinese Academy of Sciences, Beijing, China {liangyouli,away,qliu}@computing.dcu.ie junxie@ict.ac.cn † Abstract semantic information and has the best inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be helpful to translation. In recent years, dependency structure has been widely used in SMT. For example, Shen et al. (2010) present a string-to-dependency model by using the dependency fragments of the neighbouring words on the target side, which makes it easier to integrate a dependency language model. However such string-to-tree systems run slowly in cubic time (Huang et al., 2006). Another example is the treelet approach (Menezes and Quirk, 2005; Quirk et al., 2005), which uses dependency structure on the source side. Xiong et al. (2007) extend the treelet approach to allow dependency fragments with gaps. As the treelet is defined as an arbitrary connected sub-graph, typically both substitution and insertion operations are adopted for decoding. However, as translation rules based on the treelets do not encode enough reordering information directly, another heuristic or separate reordering model is usually needed to decide the best target position of the inserted words."
W14-4014,J03-1002,0,0.0230289,"Missing"
W14-4014,P02-1040,0,0.0894681,"el before (XJ) and after (D2S) dependency tree being transformed. Systems are trained on a selected 1.2M Chinese–English corpus. Table 1: Chinese–English corpus. For the English dev and test sets, words counts are averaged across 4 references. corpus train dev test12 test13 sentences 2,037,209 3,003 3,003 3,000 words(de) 52,671,991 72,661 72,603 63,412 train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (Och, 2003) is used to tune weights. Caseinsensitive BLEU (Papineni et al., 2002) is used to evaluate the translation results. Bootstrap resampling (Koehn, 2004) is also performed to compute statistical significance with 1000 iterations. We implement the baseline Dep2Str model in Moses with methods described in this paper, which is denoted as D2S. The first experiment we do is to sanity check our implementation. Thus we take a separate system (denoted as XJ) for comparison which implements the Dep2Str model based on (Xie et al., 2011). As shown in Table 3, using the transformation of dependency trees, the Dep2Str model implemented in Moses (D2S) is comparable with the stan"
W14-4014,P05-1034,0,0.182818,"u.ie junxie@ict.ac.cn † Abstract semantic information and has the best inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be helpful to translation. In recent years, dependency structure has been widely used in SMT. For example, Shen et al. (2010) present a string-to-dependency model by using the dependency fragments of the neighbouring words on the target side, which makes it easier to integrate a dependency language model. However such string-to-tree systems run slowly in cubic time (Huang et al., 2006). Another example is the treelet approach (Menezes and Quirk, 2005; Quirk et al., 2005), which uses dependency structure on the source side. Xiong et al. (2007) extend the treelet approach to allow dependency fragments with gaps. As the treelet is defined as an arbitrary connected sub-graph, typically both substitution and insertion operations are adopted for decoding. However, as translation rules based on the treelets do not encode enough reordering information directly, another heuristic or separate reordering model is usually needed to decide the best target position of the inserted words. Different from these works, Xie et al. (2011) present a dependency-to-string (Dep2Str)"
W14-4014,W04-3250,0,0.0311272,"selected 1.2M Chinese–English corpus. Table 1: Chinese–English corpus. For the English dev and test sets, words counts are averaged across 4 references. corpus train dev test12 test13 sentences 2,037,209 3,003 3,003 3,000 words(de) 52,671,991 72,661 72,603 63,412 train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (Och, 2003) is used to tune weights. Caseinsensitive BLEU (Papineni et al., 2002) is used to evaluate the translation results. Bootstrap resampling (Koehn, 2004) is also performed to compute statistical significance with 1000 iterations. We implement the baseline Dep2Str model in Moses with methods described in this paper, which is denoted as D2S. The first experiment we do is to sanity check our implementation. Thus we take a separate system (denoted as XJ) for comparison which implements the Dep2Str model based on (Xie et al., 2011). As shown in Table 3, using the transformation of dependency trees, the Dep2Str model implemented in Moses (D2S) is comparable with the standard implementation (XJ). In the rest of this section, we describe experiments w"
W14-4014,H01-1014,0,0.061167,"= {2 → 2} where “H1” denotes the position of the head word is 1, “R1” indicates the first right dependent of the head word, “X” is the general label for the target side and φ is the set of alignments (the index-correspondences between s and t). The format has been described in detail at http://www.statmt.org/ moses/?n=Moses.SyntaxTutorial. Guohui For the internal node “ 国会” in the HD fragment Guohui Xuanju “(国会) 选举”, we create two constituent nodes 125 In addition, our transformation is different from other works which transform a dependency tree into a constituent tree (Collins et al., 1999; Xia and Palmer, 2001). In this paper, the produced constituent tree still preserves dependency relations between words, and the phrasal structure is directly derived from the dependency structure without refinement. Accordingly, the constituent tree may not be a linguistically well-formed syntactic structure. However, it is not a problem for our model, because in this paper what matters is the dependency structure which has already been encoded into the (ill-formed) constituent tree. 4 smart/JJ She/PRP is/VBZ very/RB smart/JJ + She/PRP is/VBZ very/RB Figure 4: An example of decomposition on a headdependent fragmen"
W14-4014,N03-1017,0,0.0792329,"Missing"
W14-4014,D11-1020,1,0.942012,"let approach (Menezes and Quirk, 2005; Quirk et al., 2005), which uses dependency structure on the source side. Xiong et al. (2007) extend the treelet approach to allow dependency fragments with gaps. As the treelet is defined as an arbitrary connected sub-graph, typically both substitution and insertion operations are adopted for decoding. However, as translation rules based on the treelets do not encode enough reordering information directly, another heuristic or separate reordering model is usually needed to decide the best target position of the inserted words. Different from these works, Xie et al. (2011) present a dependency-to-string (Dep2Str) model, which extracts head-dependent (HD) rules from word-aligned source dependency trees and target strings. As this model specifies reordering information in the HD rules, during translation only the substitution operation is needed, because words are reordered simultaneously with the rule being applied. Meng et al. (2013) and Xie et al. (2014) extend the model by augmenting HD rules with the help of either constituent tree or fixed/float structure (Shen et al., 2010). Augmented rules are created by the combination of two or more nodes in Dependency"
W14-4014,2005.mtsummit-ebmt.13,0,0.78139,"i,away,qliu}@computing.dcu.ie junxie@ict.ac.cn † Abstract semantic information and has the best inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be helpful to translation. In recent years, dependency structure has been widely used in SMT. For example, Shen et al. (2010) present a string-to-dependency model by using the dependency fragments of the neighbouring words on the target side, which makes it easier to integrate a dependency language model. However such string-to-tree systems run slowly in cubic time (Huang et al., 2006). Another example is the treelet approach (Menezes and Quirk, 2005; Quirk et al., 2005), which uses dependency structure on the source side. Xiong et al. (2007) extend the treelet approach to allow dependency fragments with gaps. As the treelet is defined as an arbitrary connected sub-graph, typically both substitution and insertion operations are adopted for decoding. However, as translation rules based on the treelets do not encode enough reordering information directly, another heuristic or separate reordering model is usually needed to decide the best target position of the inserted words. Different from these works, Xie et al. (2011) present a dependenc"
W14-4014,C14-1209,1,0.89908,"lets do not encode enough reordering information directly, another heuristic or separate reordering model is usually needed to decide the best target position of the inserted words. Different from these works, Xie et al. (2011) present a dependency-to-string (Dep2Str) model, which extracts head-dependent (HD) rules from word-aligned source dependency trees and target strings. As this model specifies reordering information in the HD rules, during translation only the substitution operation is needed, because words are reordered simultaneously with the rule being applied. Meng et al. (2013) and Xie et al. (2014) extend the model by augmenting HD rules with the help of either constituent tree or fixed/float structure (Shen et al., 2010). Augmented rules are created by the combination of two or more nodes in Dependency structure provides grammatical relations between words, which have shown to be effective in Statistical Machine Translation (SMT). In this paper, we present an open source module in Moses which implements a dependency-to-string model. We propose a method to transform the input dependency tree into a corresponding constituent tree for reusing the tree-based decoder in Moses. In our experi"
W14-4014,D13-1108,1,0.913216,"Missing"
W14-4014,W07-0706,1,0.884483,"ter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be helpful to translation. In recent years, dependency structure has been widely used in SMT. For example, Shen et al. (2010) present a string-to-dependency model by using the dependency fragments of the neighbouring words on the target side, which makes it easier to integrate a dependency language model. However such string-to-tree systems run slowly in cubic time (Huang et al., 2006). Another example is the treelet approach (Menezes and Quirk, 2005; Quirk et al., 2005), which uses dependency structure on the source side. Xiong et al. (2007) extend the treelet approach to allow dependency fragments with gaps. As the treelet is defined as an arbitrary connected sub-graph, typically both substitution and insertion operations are adopted for decoding. However, as translation rules based on the treelets do not encode enough reordering information directly, another heuristic or separate reordering model is usually needed to decide the best target position of the inserted words. Different from these works, Xie et al. (2011) present a dependency-to-string (Dep2Str) model, which extracts head-dependent (HD) rules from word-aligned source"
W14-4014,P08-1023,1,0.907825,"Missing"
W14-4014,P05-1013,0,0.453403,"Missing"
W14-4014,J10-4005,0,\N,Missing
W14-4014,P07-2045,0,\N,Missing
W15-3005,W14-3303,1,0.877323,"Missing"
W15-3005,W13-2206,1,0.885723,"Missing"
W15-3005,P07-2045,0,0.0118755,"eployment of Accurate Statistical Machine Translation Systems, Benchmarks, and Statistics Ergun Bic¸ici Qun Liu Andy Way ADAPT Research Center ADAPT Research Center ADAPT Research Center School of Computing School of Computing School of Computing Dublin City University, Ireland Dublin City University, Ireland Dublin City University, Ireland ergun.bicici@computing.dcu.ie qliu@computing.dcu.ie away@computing.dcu.ie Abstract randomized subsets of the training data and combines the selections afterwards. FDA5 is available at http://github.com/bicici/FDA. We run ParFDA SMT experiments using Moses (Koehn et al., 2007) in all language pairs in WMT15 (Bojar et al., 2015) and obtain SMT performance close to the top constrained Moses systems. ParFDA allows rapid prototyping of SMT systems for a given target domain or task. We use ParFDA for selecting parallel training data and LM data for building SMT systems. We select the LM training data with ParFDA based on the following observation (Bic¸ici, 2013): We build parallel FDA5 (ParFDA) Moses statistical machine translation (SMT) systems for all language pairs in the workshop on statistical machine translation (Bojar et al., 2015) (WMT15) translation task and ob"
W15-3005,J03-1002,0,0.0147758,"2013; Bic¸ici et al., 2014) runs separate FDA5 (Bic¸ici and Yuret, 2015) models on 2 Results We run ParFDA SMT experiments for all language pairs in both directions in the WMT15 translation task (Bojar et al., 2015), which include English-Czech (en-cs), English-German (en-de), English-Finnish (en-fi), English-French (en-fr), and English-Russian (en-ru). We truecase all of the corpora, set the maximum sentence length to 126, use 150-best lists during tuning, set the LM order to a value in [7, 10] for all language pairs, and train the LM using SRILM (Stolcke, 2002) with -unk option. For GIZA++ (Och and Ney, 2003), max-fertility is set to 10, with the number of iterations set to 7,3,5,5,7 for IBM models 1,2,3,4, and the HMM model, and 70 word 74 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 74–78, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. S→T en-cs en-cs cs-en cs-en en-de en-de de-en de-en en-fi en-fi fi-en fi-en en-fr en-fr fr-en fr-en en-ru en-ru ru-en ru-en Data C ParFDA C ParFDA C ParFDA C ParFDA C ParFDA C ParFDA C ParFDA C ParFDA C ParFDA C ParFDA #word S (M) 253.8 49.0 224.1 42.0 116.3 37.6 109.8 33.3 52.8 37.2 37.9 25"
W15-3005,P09-2087,1,0.85904,"Missing"
W15-3005,W15-3001,0,\N,Missing
W15-3035,W15-3001,0,0.153862,"ed learning models. We develop RTM models for each WMT15 QET (QET15) subtask and obtain improvements over QET14 results. RTMs achieve top performance in QET15 ranking 1st in document- and sentence-level prediction tasks and 2nd in word-level prediction task. 1 • with extended learning models including bayesian ridge regression (Tan et al., 2015), which did not obtain better performance than support vector regression in training results (Section 2.2). We present top results with Referential Translation Machines (Bic¸ici, 2015; Bic¸ici and Way, 2014) at quality estimation task (QET15) in WMT15 (Bojar et al., 2015). RTMs pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bic¸ici and Yuret, 2015) as interpretants for reaching shared semantics. RTMs use Machine Translation Performance Prediction (MTPP) System (Bic¸ici et al., 2013; Bic¸ici, 2015), which is a state-of-the-art performance predictor of translation even without using the translation by using only the source. We use ParFDA for selecting the interpretants (Bic¸ici et al., 2015; Bic¸ici and Yuret, 2015) and build an MTPP model. MTPP derives"
W15-3035,2009.eamt-1.5,0,0.0365708,"for Computational Linguistics. 2.1 We present results using support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Sch¨olkopf, 2004) for sentence and document translation prediction tasks and Global Linear Models (GLM) (Collins, 2002) with dynamic learning (GLMd) (Bic¸ici, 2013; Bic¸ici and Way, 2014) for word-level translation performance prediction. We also use these learning models after a feature subset selection (FS) with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009), or PLS after FS (FS+PLS). GLM relies on Viterbi decoding, perceptron learning, and flexible feature definitions. GLMd extends the GLM framework by parallel perceptron training (McDonald et al., 2010) and dynamic learning with adaptive weight updates in the perceptron learning algorithm: Figure 1: RTM depiction. Task Task 1 (en-es) Task 2 (en-es) Task 3 (en-de) Task 3 (de-en) Train 12271 12271 800 800 Test 1817 1817 415 415 Table 1: Number of sentences in different tasks. 2 w = w + α (Φ(xi , yi ) − Φ(xi , ˆy)) , (1) where Φ returns a global representation for instance i and the weights are up"
W15-3035,W12-3102,0,0.0987149,"Missing"
W15-3035,W02-1001,0,0.0614078,"ed ParFDA instance selection model (Bic¸ici et al., 2015) allowing better language models (LM) in which similarity judgments are made to be built with improved optimization and selection of the LM data, 304 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 304–308, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. 2.1 We present results using support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Sch¨olkopf, 2004) for sentence and document translation prediction tasks and Global Linear Models (GLM) (Collins, 2002) with dynamic learning (GLMd) (Bic¸ici, 2013; Bic¸ici and Way, 2014) for word-level translation performance prediction. We also use these learning models after a feature subset selection (FS) with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009), or PLS after FS (FS+PLS). GLM relies on Viterbi decoding, perceptron learning, and flexible feature definitions. GLMd extends the GLM framework by parallel perceptron training (McDonald et al., 2010) and dynamic learning with adaptive weight"
W15-3035,S15-2015,0,0.052246,"Missing"
W15-3035,P07-2045,0,0.00372441,"error relative to the absolute error of a predictor always predicting the target mean assuming that target mean is known (Bic¸ici, 2015). RTM test performance on various tasks sorted according to MRAER can help identify which tasks and subtasks may require more work. DeltaAvg (Callison-Burch et al., Instance selection for the training set and the language model (LM) corpus is handled by ParFDA (Bic¸ici et al., 2015), whose parameters are optimized for each translation task. LM are trained using SRILM (Stolcke, 2002). We tokenize and truecase all of the corpora using code released with Moses (Koehn et al., 2007) 1 . Table 1 lists the number of sentences in the training and test sets for each task. 1 RTM Prediction Models and Optimization mosesdecoder/scripts/ 305 Task Task1 Task3 Translation en-es en-es en-de en-de de-en de-en Model FS SVR FS+PLS SVR FS SVR SVR FS SVR FS+PLS SVR r 0.355 0.362 0.517 0.503 0.479 0.391 MAE 0.1387 0.1389 0.0737 0.0765 0.0473 0.0515 RAE 0.895 0.896 0.734 0.761 0.738 0.804 MAER 0.782 0.784 0.289 0.307 0.267 0.288 MRAER 0.821 0.824 0.678 0.737 0.665 0.81 Table 2: Training performance of the top 2 individual RTM models prepared for different tasks. 2.0 1.8 learning rate 1.6"
W15-3035,W07-0734,0,0.47327,"Estimation Task We participate in all of the three subtasks of the quality estimation task (QET) (Bojar et al., 2015), which include English to Spanish (en-es), English to German (en-de), and German to English (deen) translation directions. There are three subtasks: sentence-level prediction (Task 1), wordlevel prediction (Task 2), and document-level prediction (Task 3). Task 1 is about predicting HTER (human-targeted translation edit rate) (Snover et al., 2006) scores of sentence translations, Task 2 is about binary classification of word-level quality, and Task 3 is about predicting METEOR (Lavie and Agarwal, 2007) scores of document translations. f (x) = (loga b − 1)x2 + 1 (2) Learning rate curve for a = 0.5 and b = 1.0 is provided in Figure 2: 2.2 Training Results We use mean absolute error (MAE), relative absolute error (RAE), root mean squared error (RMSE), and correlation (r) as well as relative MAE (MAER) and relative RAE (MRAER) to evaluate (Bic¸ici, 2015; Bic¸ici, 2013). MAER is mean absolute error relative to the magnitude of the target and MRAER is mean absolute error relative to the absolute error of a predictor always predicting the target mean assuming that target mean is known (Bic¸ici, 20"
W15-3035,2006.amta-papers.25,0,0.0613992,"learning rate updates the weight values with weights in the range [a, b] using the following function taking error rate as the input: RTM in the Quality Estimation Task We participate in all of the three subtasks of the quality estimation task (QET) (Bojar et al., 2015), which include English to Spanish (en-es), English to German (en-de), and German to English (deen) translation directions. There are three subtasks: sentence-level prediction (Task 1), wordlevel prediction (Task 2), and document-level prediction (Task 3). Task 1 is about predicting HTER (human-targeted translation edit rate) (Snover et al., 2006) scores of sentence translations, Task 2 is about binary classification of word-level quality, and Task 3 is about predicting METEOR (Lavie and Agarwal, 2007) scores of document translations. f (x) = (loga b − 1)x2 + 1 (2) Learning rate curve for a = 0.5 and b = 1.0 is provided in Figure 2: 2.2 Training Results We use mean absolute error (MAE), relative absolute error (RAE), root mean squared error (RMSE), and correlation (r) as well as relative MAE (MAER) and relative RAE (MRAER) to evaluate (Bic¸ici, 2015; Bic¸ici, 2013). MAER is mean absolute error relative to the magnitude of the target an"
W15-3035,S15-2010,1,\N,Missing
W15-3035,W13-2242,1,\N,Missing
W15-3035,N10-1069,0,\N,Missing
W15-3035,W14-3339,1,\N,Missing
W15-3053,P08-1007,0,0.0609156,"he reference. The lexicon-based metrics can only use lexical information, such as BLEU (Papineni et al., 2002), NIST(Doddington, 2002) and METEOR (Lavie and Agarwal, 2007). To evaluate the hypothesis on syntactic level, some researchers proposed the syntax-based metrics. Liu and Gildea (2005) proposed a constituent-tree-based metric STM and a dependency-tree-based metric HWCM. The syntax-based metric proposed by Owczarzak et al (2007) uses the Lexical-Functional Grammar (LFG) dependency tree. Some metrics introduce the syntactic information on the basis of lexical information, such as MAXSIM (Chan and Ng, 2008) and the metric proposed by Zhu et al. (2010). These metrics evaluate the syntactic similarity by comparing the sub-structures ex1 Combined metrics directly use the scores of many kinds of metrics, such as BLEU, TER, METEOR and some syntaxbased metrics. For the metrics using different kinds of information types (lexicon, syntax and semantic information) as features, we still think they are single metrics, because they don’t use the score of other metrics. 417 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 417–421, c Lisboa, Portugal, 17-18 September 2015. 2015 Asso"
W15-3053,W14-3347,0,0.0199008,"ing data used to train the weight of each single metric are the English-targeted language pairs in WMT 2012 and WMT 2013. 4.2 Baseline 4.4 The baselines are the widely-used lexicon-based metrics, such as BLEU5 , TER6 and METEOR7 . In addition, according to the published results of WMT 2014, we also give the correlation of the metric with the best performance on average, DISCOTK-PARTY-TUNED (Joty et al., 2014), which is a combined metric including many kinds of other metrics. For fairness, we also give the result of the metric with the best performance on average in the single metrics, VERTA-W(Comelles and Atserias, 2014) on system level and BEER (Stanojevic and Sima’an, 2014) on sentence level respectively. For our combined metric, to evaluate the effect of adding DPMF, REDp and ENTFp, we also give the correlation of the metric only combining the single metrics in Asiya. 4.3 (1) Sentence Level Correlation To further evaluate the performance of DPMF and DPMFcomb , we carry out the experiments on sentence level. On sentence level, Kendall’s τ correlation coefficient is used. τ is calculated using the following equation. τ= num con pairs − num dis pairs num con pairs + num dis pairs num con pairs is the number o"
W15-3053,W14-3352,0,0.0408733,"Missing"
W15-3053,W07-0734,0,0.0404166,"introduce more information, we combine the new metric with many other metrics. The combined metric obtains state-of-theart performance on both system level evaluation and sentence level evaluation on WMT 2014. 1 Introduction Automatic evaluation metrics play an important role in machine translation research. At present, most of the automatic evaluation metrics evaluate the translation quality by comparing the similarity between the hypothesis and the reference. The lexicon-based metrics can only use lexical information, such as BLEU (Papineni et al., 2002), NIST(Doddington, 2002) and METEOR (Lavie and Agarwal, 2007). To evaluate the hypothesis on syntactic level, some researchers proposed the syntax-based metrics. Liu and Gildea (2005) proposed a constituent-tree-based metric STM and a dependency-tree-based metric HWCM. The syntax-based metric proposed by Owczarzak et al (2007) uses the Lexical-Functional Grammar (LFG) dependency tree. Some metrics introduce the syntactic information on the basis of lexical information, such as MAXSIM (Chan and Ng, 2008) and the metric proposed by Zhu et al. (2010). These metrics evaluate the syntactic similarity by comparing the sub-structures ex1 Combined metrics direc"
W15-3053,W05-0904,0,0.36469,"erformance on both system level evaluation and sentence level evaluation on WMT 2014. 1 Introduction Automatic evaluation metrics play an important role in machine translation research. At present, most of the automatic evaluation metrics evaluate the translation quality by comparing the similarity between the hypothesis and the reference. The lexicon-based metrics can only use lexical information, such as BLEU (Papineni et al., 2002), NIST(Doddington, 2002) and METEOR (Lavie and Agarwal, 2007). To evaluate the hypothesis on syntactic level, some researchers proposed the syntax-based metrics. Liu and Gildea (2005) proposed a constituent-tree-based metric STM and a dependency-tree-based metric HWCM. The syntax-based metric proposed by Owczarzak et al (2007) uses the Lexical-Functional Grammar (LFG) dependency tree. Some metrics introduce the syntactic information on the basis of lexical information, such as MAXSIM (Chan and Ng, 2008) and the metric proposed by Zhu et al. (2010). These metrics evaluate the syntactic similarity by comparing the sub-structures ex1 Combined metrics directly use the scores of many kinds of metrics, such as BLEU, TER, METEOR and some syntaxbased metrics. For the metrics using"
W15-3053,2007.tmi-papers.15,0,0.0438102,"pation in WMT2015 Metrics Task Hui Yu† Qingsong Ma† Xiaofeng Wu‡ Qun Liu‡† † Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences ‡ ADAPT Centre, School of Computing, Dublin City University {yuhui,maqingsong}@ict.ac.cn {xiaofengwu}@computing.dcu.ie {liuqun}@ict.ac.cn Abstract tracted from the trees of hypothesis and reference. To avoid parsing the hypothesis in order to prevent translation error propagation, some researchers propose a kind of syntax-based evaluation metric which only uses the tree of reference, ˆ such as BLEUATRE (Mehay and Brew, 2007) and RED (Yu et al., 2014). The syntax-based metrics either use the substructures of both the reference and the hypothesis tree, or only use that on the reference side. Therefore, for these metrics, sub-structures designed by human are required. In this paper, we propose a novel dependency-parsing-model-based metric in the view of dependency tree generation, which completely avoids this human involvement. A dependency parsing model is trained by the reference dependency tree, through which we can obtain the dependency tree of the hypothesis and the corresponding score. The syntactic similarity"
W15-3053,P02-1040,0,0.101627,"ystem level and is comparable with it on sentence level. To introduce more information, we combine the new metric with many other metrics. The combined metric obtains state-of-theart performance on both system level evaluation and sentence level evaluation on WMT 2014. 1 Introduction Automatic evaluation metrics play an important role in machine translation research. At present, most of the automatic evaluation metrics evaluate the translation quality by comparing the similarity between the hypothesis and the reference. The lexicon-based metrics can only use lexical information, such as BLEU (Papineni et al., 2002), NIST(Doddington, 2002) and METEOR (Lavie and Agarwal, 2007). To evaluate the hypothesis on syntactic level, some researchers proposed the syntax-based metrics. Liu and Gildea (2005) proposed a constituent-tree-based metric STM and a dependency-tree-based metric HWCM. The syntax-based metric proposed by Owczarzak et al (2007) uses the Lexical-Functional Grammar (LFG) dependency tree. Some metrics introduce the syntactic information on the basis of lexical information, such as MAXSIM (Chan and Ng, 2008) and the metric proposed by Zhu et al. (2010). These metrics evaluate the syntactic similari"
W15-3053,W14-3354,0,0.0840861,"Missing"
W15-3053,C14-1193,1,0.934309,"Hui Yu† Qingsong Ma† Xiaofeng Wu‡ Qun Liu‡† † Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences ‡ ADAPT Centre, School of Computing, Dublin City University {yuhui,maqingsong}@ict.ac.cn {xiaofengwu}@computing.dcu.ie {liuqun}@ict.ac.cn Abstract tracted from the trees of hypothesis and reference. To avoid parsing the hypothesis in order to prevent translation error propagation, some researchers propose a kind of syntax-based evaluation metric which only uses the tree of reference, ˆ such as BLEUATRE (Mehay and Brew, 2007) and RED (Yu et al., 2014). The syntax-based metrics either use the substructures of both the reference and the hypothesis tree, or only use that on the reference side. Therefore, for these metrics, sub-structures designed by human are required. In this paper, we propose a novel dependency-parsing-model-based metric in the view of dependency tree generation, which completely avoids this human involvement. A dependency parsing model is trained by the reference dependency tree, through which we can obtain the dependency tree of the hypothesis and the corresponding score. The syntactic similarity between the hypothesis an"
W15-3053,C10-2175,0,\N,Missing
W15-3055,N10-1080,0,0.59863,"Translation (SMT) is modeled as a weighted combination of several features. Tuning in SMT refers to learning a set of optimized weights, which minimize a defined translation error on a tuning set. Typically, the error is measured by an automatic evaluation metric. Thanks to its simplicity and language independence, BLEU (Papineni et al., 2002) has served as the optimization objective since the 2000s. Although various lexical metrics, such as TER (Snover et al., 2006) and METEOR (Lavie and Denkowski, 2009) etc., have been proposed, none of them can truly replace BLEU in a phrase-based system (Cer et al., 2010). However, BLEU has no proficiency to deal with synonyms, paraphrases, and syntactic equivalent etc. (Callison-Burch et al., 2006). In addition, as a lexical and n-gram-based metric, BLEU may be not suitable for optimization in a syntax-based model. BLEU BLEU is the most widely used metric in SMT. It is lexical-based and language-independent. BLEU scores a hypothesis by combining n-gram precisions over reference translations with a length penalty. A n-gram precision pn is calculated separately for different n-gram lengths. BLEU combines these precisions using a geometric mean. The resulting sc"
W15-3055,D08-1024,0,0.0282309,"ere distri and distci are relative distances between ith word and (i + 1)th word in the reference and hypothesis, respectively. If a fixed/floating structure is matched, p(d, c) = 1. 430 by the decoding model, ⊕ represents the accumulation of potentially non-decomposable sentential errors, which then produces a document-level evaluation score. 3.2 Train  Eval. BLEU MERT METEOR RED BLEU MIRA METEOR RED MIRA MIRA is an online large margin learning algorithm (Crammer and Singer, 2003). Its application to MT decoding model tuning was firstly explored by Watanabe et al. (2007) and then refined by Chiang et al. (2008) and Cherry and Foster (2012). The MIRA we use tries to separate a “fear” derivation d− (x, y) from a “hope” one d+ (x, y) by a margin propositional to their metric difference (Chiang et al., 2008). The two derivations are defined as follows: d+ (x, y) = argmax w · Φ(d) − δy (d) d − d (x, y) = argmax w · Φ(d) + δy (d) d RED 19.91 20.02 19.97 20.02 20.05 20.02 includes 6,003 sentence pairs in total4 . English sentences are parsed into dependency structures by Stanford parser (Marneffe et al., 2006). Czech sentences are parsed by a Perl implementation5 of the MST parser (McDonald et al., 2005)."
W15-3055,P05-1033,0,0.443598,"Missing"
W15-3055,D07-1080,0,0.0609237,"Missing"
W15-3055,P07-2045,0,0.0116946,"Missing"
W15-3055,C14-1193,1,0.87987,"Missing"
W15-3055,W05-0904,0,0.0394396,"esources are language-dependent. Besides, METEOR is unigram-based and thus has a lack of incorporating syntactic structures. • Exact: match words that have the same word form. • Stem: match words whose stems are identical. • Synonym: match words when they are defined as synonyms in the WordNet database2 . 2.3 • Paraphrase: match a phrase pair when they are listed as paraphrases in a paraphrase table. RED Instead of collecting n-grams from word sequences as in BLEU, RED extracts n-grams according to a dependency structure of a reference, called dep-ngrams, which have two types: headword chain (Liu and Gildea, 2005) and fixed/floating structures (Shen et al., 2010). A headword chain is a sequence of words which corresponds to a path in a dependency tree, while a fixed/floating structure covers a sequence of contiguous words. Figure 1 shows an example of different types of dep-ngrams. A F mean score is separately calculated for each different dep-ngram lengths. Then, they are linearly combined as follows: Typically, there is more than one possible alignment. In METEOR, a final alignment is obtained by beam search in the entire alignment space. Given the final alignment, METEOR calculates a unigram precisi"
W15-3055,de-marneffe-etal-2006-generating,0,0.0685091,"Missing"
W15-3055,P05-1012,0,0.0759577,"by Chiang et al. (2008) and Cherry and Foster (2012). The MIRA we use tries to separate a “fear” derivation d− (x, y) from a “hope” one d+ (x, y) by a margin propositional to their metric difference (Chiang et al., 2008). The two derivations are defined as follows: d+ (x, y) = argmax w · Φ(d) − δy (d) d − d (x, y) = argmax w · Φ(d) + δy (d) d RED 19.91 20.02 19.97 20.02 20.05 20.02 includes 6,003 sentence pairs in total4 . English sentences are parsed into dependency structures by Stanford parser (Marneffe et al., 2006). Czech sentences are parsed by a Perl implementation5 of the MST parser (McDonald et al., 2005). 4.1 (16) Metrics Setting As described in Section 2.1, we use the standard BLEU parameters6 . We use METEOR 1.47 in our experiments with default optimized parameters. Specifically, for Czech to English translation, we adopt all four lexical matching strategies with parameter values: α = 0.85, β = 0.2, γ = 0.6, δ = 0.75 and wi = 1.0, 0.6, 0.8, 0.6. For English to Czech translation, we use two lexical matching strategies, including exact and paraphrase, with parameter values: α = 0.95, β = 0.2, γ = 0.6, δ = 0.8 and wi = 1.0, 0.4. In RED, we use all four matchers in the Czech– English task while"
W15-3055,P03-1021,0,0.0332493,"ute of Computing Technology Chinese Academy of Sciences, China {liangyouli,qliu}@computing.dcu.ie yuhui@ict.ac.cn Abstract In this paper, we integrate a reference dependency-based MT evaluation metric, RED1 (Yu et al., 2014), into the hierarchical phrasebased model (Chiang, 2005) in Moses (Koehn et al., 2007). In doing so, we explore whether a syntax-based translation system will perform better when it is optimized towards a syntaxbased evaluation criteria. We compare RED with two other evaluation metrics, BLEU and METEOR (Section 2). Two tuning algorithms are used (Section 3). They are MERT (Och, 2003), MIRA (Cherry and Foster, 2012). Experiments are conducted on Czech–English and English–Czech translation (Section 4). In this paper, we describe our submission to WMT 2015 Tuning Task. We integrate a dependency-based MT evaluation metric, RED, to Moses and compare it with BLEU and METEOR in conjunction with two tuning methods: MERT and MIRA. Experiments are conducted using hierarchical phrase-based models on Czech–English and English–Czech tasks. Our results show that MIRA performs better than MERT in most cases. Using RED performs similarly to METEOR when tuning is performed using MIRA. We"
W15-3055,P02-1040,0,0.0950324,"ask and 6/9 on the Czech– English task. 1 2 Evaluation Metrics An evaluation metric, which has a higher correlation with human judgments, may be used to train a better system. In this paper, we compare three metrics: BLEU, METEOR, and RED. Introduction 2.1 Statistical Machine Translation (SMT) is modeled as a weighted combination of several features. Tuning in SMT refers to learning a set of optimized weights, which minimize a defined translation error on a tuning set. Typically, the error is measured by an automatic evaluation metric. Thanks to its simplicity and language independence, BLEU (Papineni et al., 2002) has served as the optimization objective since the 2000s. Although various lexical metrics, such as TER (Snover et al., 2006) and METEOR (Lavie and Denkowski, 2009) etc., have been proposed, none of them can truly replace BLEU in a phrase-based system (Cer et al., 2010). However, BLEU has no proficiency to deal with synonyms, paraphrases, and syntactic equivalent etc. (Callison-Burch et al., 2006). In addition, as a lexical and n-gram-based metric, BLEU may be not suitable for optimization in a syntax-based model. BLEU BLEU is the most widely used metric in SMT. It is lexical-based and langua"
W15-3055,2006.amta-papers.25,0,0.0372379,"udgments, may be used to train a better system. In this paper, we compare three metrics: BLEU, METEOR, and RED. Introduction 2.1 Statistical Machine Translation (SMT) is modeled as a weighted combination of several features. Tuning in SMT refers to learning a set of optimized weights, which minimize a defined translation error on a tuning set. Typically, the error is measured by an automatic evaluation metric. Thanks to its simplicity and language independence, BLEU (Papineni et al., 2002) has served as the optimization objective since the 2000s. Although various lexical metrics, such as TER (Snover et al., 2006) and METEOR (Lavie and Denkowski, 2009) etc., have been proposed, none of them can truly replace BLEU in a phrase-based system (Cer et al., 2010). However, BLEU has no proficiency to deal with synonyms, paraphrases, and syntactic equivalent etc. (Callison-Burch et al., 2006). In addition, as a lexical and n-gram-based metric, BLEU may be not suitable for optimization in a syntax-based model. BLEU BLEU is the most widely used metric in SMT. It is lexical-based and language-independent. BLEU scores a hypothesis by combining n-gram precisions over reference translations with a length penalty. A n"
W15-3055,W09-0441,0,0.0277502,"re content words in a hypothesis and a reference, hf and rf are functions words in a hypothesis and a reference, respectively. Then the precision and recall are combined as in Equation (4). 2 (4) To consider differences in word order, a penalty is calculated on the basis of the total number (m) of matched words and the number (ch) of chunks. A chunk is defined as a sequence of matches, which are contiguous and have identical word order. The penalty is formulated as in Equation (5): METEOR P = P ·R α · P + (1 − α) · R i wi RED = N X wn · F meann (7) n=1 Inspired by other metrics, such as TERp (Snover et al., 2009) and METEOR, RED integrates some resources as follows: • Stem and synonym: used to align words. This increases the possibility of matching a dep-ngram. Different matchers are assigned https://wordnet.princeton.edu/ 429 saw saw ant I with where with magnifier an scoredep = a (b) saw ant with ant I (c) (d) 3 Figure 1: An illustration of dep-ngrams. (a) is a dependency tree, (b) is a headword chain, (c) is a fixed structure and (d) is a floating structure. • Paraphrase: used for extracting paraphrasengrams. In this case, RED ignores the dependency structure of a reference. A paraphrasengram has a"
W15-3055,E06-1032,0,\N,Missing
W15-3055,J10-4005,0,\N,Missing
W15-3055,N12-1047,0,\N,Missing
W15-4911,2013.iwslt-evaluation.20,0,0.0154076,"s a commercial rule-based engine for English–Farsi translation. It contains 1.5 million words in its database and includes speciﬁc dictionaries for 33 different ﬁelds of science. Another English–Farsi MT system was developed by the Iran Supreme Council of Information.4 Postchi5 is a bidirectional system listed among the EuroMatrix6 systems for the Farsi language. These systems are not terribly robust or precise examples of Farsi SMT and are usually the by-products of research or commercial projects. The only system that has ofﬁcially been reported for the purpose of Farsi SMT is FBK’s system (Bertoldi et al., 2013). It was tested on a publicly available dataset and from this viewpoint is the most important system for our purposes.7 2.2 Parallel Corpora for Farsi SMT The ﬁrst attempts at generating Farsi–English parallel corpora are documented in the Shiraz project (Zajac et al., 2000). The authors constructed a corpus of 3000 parallel sentences, which were translated manually from monolingual online Farsi doc3 http://mabnasoft.com/english/parstrans/index.htm http://www.machinetranslation.ir/ 5 http://www.postchi.com/ 6 http://matrix.statmt.org/resources/pair?l1=fa&l2=en#pair 7 However other Farsi MT eng"
W15-4911,2012.eamt-1.60,0,0.0397015,"Missing"
W15-4911,erjavec-2010-multext,0,0.0282231,"nces, which were translated manually from monolingual online Farsi doc3 http://mabnasoft.com/english/parstrans/index.htm http://www.machinetranslation.ir/ 5 http://www.postchi.com/ 6 http://matrix.statmt.org/resources/pair?l1=fa&l2=en#pair 7 However other Farsi MT engines like the Shiraz system (Amtrup et al., 2000) or that of Mohaghegh (2012) use their own in-house datasets. As we are not able to replicate them we do not include them in our comparisons. 4 83 uments at New Mexico State University. More recently Qasemizadeh et al. (2007) participated in the Farsi part of MULTEXT-EAST8 project (Erjavec, 2010) and developed about 6000 sentences. There is also a corpus available in ELRA9 consisting of about 3,500,000 English and Farsi words aligned at sentence level (about 100,000 sentences). This is a mixed domain dataset including a variety of text types such as art, law, culture, literature, poetry, proverbs, religion etc. PEN (Parallel English–Persian News corpus) is another small corpus (Farajian, 2011) generated semi-automatically. It includes almost 30,000 sentences. Farajian developed a method to ﬁnd similar sentence pairs and for quality assurance used Google Translate.10 All these corpora"
W15-4911,N03-1017,0,0.0397858,"Missing"
W15-4911,P07-2045,0,0.0310231,"scuss the problems with Mizan in Section 4.1 and perform error analysis on the output translations, where it is used as the SMT training data. In the second part using TEP and TEP++ we carry out monolingual translation between SF and FF (SF2FF) and discuss some use-cases for this type of translation task. Finally in the last part we show how SF2FF boosts the SMT quality for Farsi and report our results on the IWSLT–2013 dataset providing a comparison with FBK’s system. 4.1 Mizan, TEP and TEP++ To test the performance of our engines, they were trained using Mizan, TEP and TEP++. We used Moses (Koehn et al., 2007) with the default conﬁguration for phrase-based translation. For the language modeling, SRILM (Stolcke and others, 2002) was used. The evaluation measure is BLEU (Papineni et al., 2002) and to tune the models, we applied MERT (Och, 2003). Table 1 summarizes our experimental results for the Mizan dataset. We evaluated with two types of language models, 3gram (LM3) and 5-gram (LM5). Numbers for both before and after tuning are reported. For all experiments training, tuning and test sets were selected randomly from the main corpus. The size of the test set is 1,000 and the tuning set is 2000 sent"
W15-4911,W04-3250,0,0.590562,"Missing"
W15-4911,P03-1021,0,0.0511009,"nd discuss some use-cases for this type of translation task. Finally in the last part we show how SF2FF boosts the SMT quality for Farsi and report our results on the IWSLT–2013 dataset providing a comparison with FBK’s system. 4.1 Mizan, TEP and TEP++ To test the performance of our engines, they were trained using Mizan, TEP and TEP++. We used Moses (Koehn et al., 2007) with the default conﬁguration for phrase-based translation. For the language modeling, SRILM (Stolcke and others, 2002) was used. The evaluation measure is BLEU (Papineni et al., 2002) and to tune the models, we applied MERT (Och, 2003). Table 1 summarizes our experimental results for the Mizan dataset. We evaluated with two types of language models, 3gram (LM3) and 5-gram (LM5). Numbers for both before and after tuning are reported. For all experiments training, tuning and test sets were selected randomly from the main corpus. The size of the test set is 1,000 and the tuning set is 2000 sentences. Training set sizes are reported in tables. For all experiments BLEU scores for Google Translate are reported as a baseline. LM3 LM5 Google Translate Training set Corpus EN–FA Before After 8.24 10.47 8.54 10.53 FA–EN Before After 1"
W15-4911,P02-1040,0,0.0915203,"we carry out monolingual translation between SF and FF (SF2FF) and discuss some use-cases for this type of translation task. Finally in the last part we show how SF2FF boosts the SMT quality for Farsi and report our results on the IWSLT–2013 dataset providing a comparison with FBK’s system. 4.1 Mizan, TEP and TEP++ To test the performance of our engines, they were trained using Mizan, TEP and TEP++. We used Moses (Koehn et al., 2007) with the default conﬁguration for phrase-based translation. For the language modeling, SRILM (Stolcke and others, 2002) was used. The evaluation measure is BLEU (Papineni et al., 2002) and to tune the models, we applied MERT (Och, 2003). Table 1 summarizes our experimental results for the Mizan dataset. We evaluated with two types of language models, 3gram (LM3) and 5-gram (LM5). Numbers for both before and after tuning are reported. For all experiments training, tuning and test sets were selected randomly from the main corpus. The size of the test set is 1,000 and the tuning set is 2000 sentences. Training set sizes are reported in tables. For all experiments BLEU scores for Google Translate are reported as a baseline. LM3 LM5 Google Translate Training set Corpus EN–FA Bef"
W15-4911,I13-1144,0,0.0380416,"Missing"
W16-0602,N12-1047,0,0.0147445,"DE–EN) training corpus (2M+ sentence pairs) is from WMT 2014. GBMT is our graphbased translation system and GSM adds the graph segmentation model into GBMT. DTU extends the PB model by allowing source discontinuous phrases (Galley and Manning, 2010). All systems are implemented in Moses (Koehn et al., 2007). 11 System PBMT DTU GBMT GSM ZH–EN NIST04 NIST05 32.8 31.4 33.4∗ 31.5 33.7∗+ 31.7 33.8∗+ 32.0∗+ DE–EN WMT12 WMT13 19.6 21.9 19.8∗ 22.3∗ ∗ 19.8 22.4∗ ∗+ 20.3 22.9∗+ Table 1: BLEU (Papineni et al., 2002) scores for all systems on two datasets. Each score is the average score over three MIRA (Cherry and Foster, 2012) runs (Clark et al., 2011). ∗ means a system is significantly better than PBMT at p ≤ 0.01. + means a system is significantly better than DTU at p ≤ 0.01. System DTU GBMT # Rules ZH–EN DE–EN 224M+ 352M+ 99M+ 153M+ Table 2: The number of rules in DTU and GBMT. Table 1 shows our main results. Our system GBMT is better than PBMT as measured by all three metrics across all testsets. This improvement is reasonable as GBMT allows discontinuous phrases which can reduce data sparsity and handle longdistance relations (Galley and Manning, 2010). Since phrases from syntactic structures are fewer in numb"
W16-0602,N13-1003,0,0.0178214,"define a set of sparse features to explicitly model a graph segmentation. Given previous subgraphs, for each node in a current subgraph, we extract the following features:     0  C     in n .w n.w × P × × 0 out n .c n.c   H where n.w and n.c are the word and class of a current node n, and n0 is a node connected to n. C, P , and H denote that n0 is in the current subgraph or the last previous subgraph or other previous subgraphs, respectively. in and out denote that an edge is an in-coming edge or out-going edge of n. In this paper we lexicalize only on the top-100 frequent words (Cherry, 2013). In addition, we group source words into 50 classes by using mkcls. 3 Experiments and Results Our Chinese–English (ZH–EN) training corpus contains 1.5M+ sentence pairs from LDC. Our German– English (DE–EN) training corpus (2M+ sentence pairs) is from WMT 2014. GBMT is our graphbased translation system and GSM adds the graph segmentation model into GBMT. DTU extends the PB model by allowing source discontinuous phrases (Galley and Manning, 2010). All systems are implemented in Moses (Koehn et al., 2007). 11 System PBMT DTU GBMT GSM ZH–EN NIST04 NIST05 32.8 31.4 33.4∗ 31.5 33.7∗+ 31.7 33.8∗+ 32"
W16-0602,P05-1033,0,0.136346,"raphs. Experiments on Chinese–English and German–English tasks show that our system is significantly better than the phrase-based model. By explicitly modeling the graph segmentation, our system gains further improvement. 1 Introduction One significant weakness of conventional phrasebased (PB) models (Koehn et al., 2003) is that it only uses continuous phrases and thus cannot learn generalizations, such as French ne. . . pas to English not (Galley and Manning, 2010). Although using tree structures is believed to be a promising way to solve this problem by learning either translation patterns (Chiang, 2005; Galley et al., 2004; Liu et al., 2006) or treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007), handling non-syntactic phrases is still a big challenge. In this paper, we propose a graph-based translation model which translates a graph into a string by segmenting the graph into subgraphs. Each subgraph is connected and may cover discontinuous phrases. Experiments show that our model is significantly better than the PB model. Explicitly modeling the graph segmentation further improves our system. 10 held Juxing FIFA FIFA World Cup Shijiebei 2010 2010Nian in Zai successfu"
W16-0602,P11-2031,0,0.0130664,"tence pairs) is from WMT 2014. GBMT is our graphbased translation system and GSM adds the graph segmentation model into GBMT. DTU extends the PB model by allowing source discontinuous phrases (Galley and Manning, 2010). All systems are implemented in Moses (Koehn et al., 2007). 11 System PBMT DTU GBMT GSM ZH–EN NIST04 NIST05 32.8 31.4 33.4∗ 31.5 33.7∗+ 31.7 33.8∗+ 32.0∗+ DE–EN WMT12 WMT13 19.6 21.9 19.8∗ 22.3∗ ∗ 19.8 22.4∗ ∗+ 20.3 22.9∗+ Table 1: BLEU (Papineni et al., 2002) scores for all systems on two datasets. Each score is the average score over three MIRA (Cherry and Foster, 2012) runs (Clark et al., 2011). ∗ means a system is significantly better than PBMT at p ≤ 0.01. + means a system is significantly better than DTU at p ≤ 0.01. System DTU GBMT # Rules ZH–EN DE–EN 224M+ 352M+ 99M+ 153M+ Table 2: The number of rules in DTU and GBMT. Table 1 shows our main results. Our system GBMT is better than PBMT as measured by all three metrics across all testsets. This improvement is reasonable as GBMT allows discontinuous phrases which can reduce data sparsity and handle longdistance relations (Galley and Manning, 2010). Since phrases from syntactic structures are fewer in number but more reliable (Koeh"
W16-0602,N04-1035,0,0.0481273,"ents on Chinese–English and German–English tasks show that our system is significantly better than the phrase-based model. By explicitly modeling the graph segmentation, our system gains further improvement. 1 Introduction One significant weakness of conventional phrasebased (PB) models (Koehn et al., 2003) is that it only uses continuous phrases and thus cannot learn generalizations, such as French ne. . . pas to English not (Galley and Manning, 2010). Although using tree structures is believed to be a promising way to solve this problem by learning either translation patterns (Chiang, 2005; Galley et al., 2004; Liu et al., 2006) or treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007), handling non-syntactic phrases is still a big challenge. In this paper, we propose a graph-based translation model which translates a graph into a string by segmenting the graph into subgraphs. Each subgraph is connected and may cover discontinuous phrases. Experiments show that our model is significantly better than the PB model. Explicitly modeling the graph segmentation further improves our system. 10 held Juxing FIFA FIFA World Cup Shijiebei 2010 2010Nian in Zai successfully Chenggong South A"
W16-0602,N03-1017,0,0.141924,"away,qliu}@computing.dcu.ie Abstract In this paper, we propose a graph-based translation model which takes advantage of discontinuous phrases. The model segments a graph which combines bigram and dependency relations into subgraphs and produces translations by combining translations of these subgraphs. Experiments on Chinese–English and German–English tasks show that our system is significantly better than the phrase-based model. By explicitly modeling the graph segmentation, our system gains further improvement. 1 Introduction One significant weakness of conventional phrasebased (PB) models (Koehn et al., 2003) is that it only uses continuous phrases and thus cannot learn generalizations, such as French ne. . . pas to English not (Galley and Manning, 2010). Although using tree structures is believed to be a promising way to solve this problem by learning either translation patterns (Chiang, 2005; Galley et al., 2004; Liu et al., 2006) or treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007), handling non-syntactic phrases is still a big challenge. In this paper, we propose a graph-based translation model which translates a graph into a string by segmenting the graph into subgrap"
W16-0602,P07-2045,0,0.00860634,"ming edge or out-going edge of n. In this paper we lexicalize only on the top-100 frequent words (Cherry, 2013). In addition, we group source words into 50 classes by using mkcls. 3 Experiments and Results Our Chinese–English (ZH–EN) training corpus contains 1.5M+ sentence pairs from LDC. Our German– English (DE–EN) training corpus (2M+ sentence pairs) is from WMT 2014. GBMT is our graphbased translation system and GSM adds the graph segmentation model into GBMT. DTU extends the PB model by allowing source discontinuous phrases (Galley and Manning, 2010). All systems are implemented in Moses (Koehn et al., 2007). 11 System PBMT DTU GBMT GSM ZH–EN NIST04 NIST05 32.8 31.4 33.4∗ 31.5 33.7∗+ 31.7 33.8∗+ 32.0∗+ DE–EN WMT12 WMT13 19.6 21.9 19.8∗ 22.3∗ ∗ 19.8 22.4∗ ∗+ 20.3 22.9∗+ Table 1: BLEU (Papineni et al., 2002) scores for all systems on two datasets. Each score is the average score over three MIRA (Cherry and Foster, 2012) runs (Clark et al., 2011). ∗ means a system is significantly better than PBMT at p ≤ 0.01. + means a system is significantly better than DTU at p ≤ 0.01. System DTU GBMT # Rules ZH–EN DE–EN 224M+ 352M+ 99M+ 153M+ Table 2: The number of rules in DTU and GBMT. Table 1 shows our main r"
W16-0602,P06-1077,1,0.69701,"sh and German–English tasks show that our system is significantly better than the phrase-based model. By explicitly modeling the graph segmentation, our system gains further improvement. 1 Introduction One significant weakness of conventional phrasebased (PB) models (Koehn et al., 2003) is that it only uses continuous phrases and thus cannot learn generalizations, such as French ne. . . pas to English not (Galley and Manning, 2010). Although using tree structures is believed to be a promising way to solve this problem by learning either translation patterns (Chiang, 2005; Galley et al., 2004; Liu et al., 2006) or treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007), handling non-syntactic phrases is still a big challenge. In this paper, we propose a graph-based translation model which translates a graph into a string by segmenting the graph into subgraphs. Each subgraph is connected and may cover discontinuous phrases. Experiments show that our model is significantly better than the PB model. Explicitly modeling the graph segmentation further improves our system. 10 held Juxing FIFA FIFA World Cup Shijiebei 2010 2010Nian in Zai successfully Chenggong South Africa Nanfei Figure"
W16-0602,2005.mtsummit-ebmt.13,0,0.0186176,"w that our system is significantly better than the phrase-based model. By explicitly modeling the graph segmentation, our system gains further improvement. 1 Introduction One significant weakness of conventional phrasebased (PB) models (Koehn et al., 2003) is that it only uses continuous phrases and thus cannot learn generalizations, such as French ne. . . pas to English not (Galley and Manning, 2010). Although using tree structures is believed to be a promising way to solve this problem by learning either translation patterns (Chiang, 2005; Galley et al., 2004; Liu et al., 2006) or treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007), handling non-syntactic phrases is still a big challenge. In this paper, we propose a graph-based translation model which translates a graph into a string by segmenting the graph into subgraphs. Each subgraph is connected and may cover discontinuous phrases. Experiments show that our model is significantly better than the PB model. Explicitly modeling the graph segmentation further improves our system. 10 held Juxing FIFA FIFA World Cup Shijiebei 2010 2010Nian in Zai successfully Chenggong South Africa Nanfei Figure 1: An example of constructing a grap"
W16-0602,P02-1040,0,0.0963065,"Missing"
W16-0602,P05-1034,0,0.0704469,"ificantly better than the phrase-based model. By explicitly modeling the graph segmentation, our system gains further improvement. 1 Introduction One significant weakness of conventional phrasebased (PB) models (Koehn et al., 2003) is that it only uses continuous phrases and thus cannot learn generalizations, such as French ne. . . pas to English not (Galley and Manning, 2010). Although using tree structures is believed to be a promising way to solve this problem by learning either translation patterns (Chiang, 2005; Galley et al., 2004; Liu et al., 2006) or treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007), handling non-syntactic phrases is still a big challenge. In this paper, we propose a graph-based translation model which translates a graph into a string by segmenting the graph into subgraphs. Each subgraph is connected and may cover discontinuous phrases. Experiments show that our model is significantly better than the PB model. Explicitly modeling the graph segmentation further improves our system. 10 held Juxing FIFA FIFA World Cup Shijiebei 2010 2010Nian in Zai successfully Chenggong South Africa Nanfei Figure 1: An example of constructing a graph for a Chinese sent"
W16-0602,W07-0706,1,0.783162,"n the phrase-based model. By explicitly modeling the graph segmentation, our system gains further improvement. 1 Introduction One significant weakness of conventional phrasebased (PB) models (Koehn et al., 2003) is that it only uses continuous phrases and thus cannot learn generalizations, such as French ne. . . pas to English not (Galley and Manning, 2010). Although using tree structures is believed to be a promising way to solve this problem by learning either translation patterns (Chiang, 2005; Galley et al., 2004; Liu et al., 2006) or treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007), handling non-syntactic phrases is still a big challenge. In this paper, we propose a graph-based translation model which translates a graph into a string by segmenting the graph into subgraphs. Each subgraph is connected and may cover discontinuous phrases. Experiments show that our model is significantly better than the PB model. Explicitly modeling the graph segmentation further improves our system. 10 held Juxing FIFA FIFA World Cup Shijiebei 2010 2010Nian in Zai successfully Chenggong South Africa Nanfei Figure 1: An example of constructing a graph for a Chinese sentence. Each node inclu"
W16-0602,N10-1140,0,\N,Missing
W16-3403,W14-4001,0,0.0172345,"d in document-level MT to disambiguate the word selection. Tran et al. (2014) used bilingual word embeddings to compute the semantic similarity of phrases. To extend the application of text embedding beyond single words, Gao et al. (2013) proposed learning embeddings for source and target phrases by training a network to maximize the sentence-level BLEU score. Costa-jussa et al. (2014) worked at the sentence-level and incorporated the source side information into the decoding phase by finding the similarities between phrases and source embeddings. Some other models re-scored the phrase table (Alkhouli et al., 2014) or generated new phrase pairs in order to address the OOV word problem (Zhao et al., 2014). Our network makes use of some ideas from existing models, but also extends the information available to the embedding model. We train embeddings in the joint space using both source and target side information simultaneously, using a model which is similar to that of Devlin et al. (2014) and Passban et al. (2015b). Similar to Gao et al. (2013) we make embeddings for phrases and sentences and add their similarity as feature functions to the SMT model. 3 Proposed Method In order to train our bilingual em"
W16-3403,W14-3306,0,0.0133549,"he parallel training data.The work by Garcia and Tiedemann (2014) is another model follows that the same paradigm. However, machine translation (MT) is more than word-level translation. In Mart´ınez et al. (2015) word embeddings were used in document-level MT to disambiguate the word selection. Tran et al. (2014) used bilingual word embeddings to compute the semantic similarity of phrases. To extend the application of text embedding beyond single words, Gao et al. (2013) proposed learning embeddings for source and target phrases by training a network to maximize the sentence-level BLEU score. Costa-jussa et al. (2014) worked at the sentence-level and incorporated the source side information into the decoding phase by finding the similarities between phrases and source embeddings. Some other models re-scored the phrase table (Alkhouli et al., 2014) or generated new phrase pairs in order to address the OOV word problem (Zhao et al., 2014). Our network makes use of some ideas from existing models, but also extends the information available to the embedding model. We train embeddings in the joint space using both source and target side information simultaneously, using a model which is similar to that of Devli"
W16-3403,P14-1129,0,0.338501,"2014) worked at the sentence-level and incorporated the source side information into the decoding phase by finding the similarities between phrases and source embeddings. Some other models re-scored the phrase table (Alkhouli et al., 2014) or generated new phrase pairs in order to address the OOV word problem (Zhao et al., 2014). Our network makes use of some ideas from existing models, but also extends the information available to the embedding model. We train embeddings in the joint space using both source and target side information simultaneously, using a model which is similar to that of Devlin et al. (2014) and Passban et al. (2015b). Similar to Gao et al. (2013) we make embeddings for phrases and sentences and add their similarity as feature functions to the SMT model. 3 Proposed Method In order to train our bilingual embedding model, we start by creating a large bilingual corpus. Each line of the corpus may include: – a source or target sentence, – a source or target phrase, – a concatenation of a phrase pair (source and target phrases which are each other’s translation), – a tuple of source and target words (each other’s translation). Sentences of the bilingual corpus are taken from the SMT t"
W16-3403,W14-4015,0,0.0216153,"source embedding space into the target space. The transformation function was approximated using a small set of word pairs extracted using an unsupervised alignment model trained with a parallel corpus. This approach allows the construction of a word-level translation engine with very large monolingual data and only a small number of bilingual word pairs. The cross-lingual transformation mechanism allows the engine to search for translations for OOV (out-of-vocabulary) words by consulting a monolingual index which contains words that were not observed in the parallel training data.The work by Garcia and Tiedemann (2014) is another model follows that the same paradigm. However, machine translation (MT) is more than word-level translation. In Mart´ınez et al. (2015) word embeddings were used in document-level MT to disambiguate the word selection. Tran et al. (2014) used bilingual word embeddings to compute the semantic similarity of phrases. To extend the application of text embedding beyond single words, Gao et al. (2013) proposed learning embeddings for source and target phrases by training a network to maximize the sentence-level BLEU score. Costa-jussa et al. (2014) worked at the sentence-level and incorp"
W16-3403,P14-6007,0,0.025054,"detail. In Section 4, experimental results are reported. We also have a separate section to discuss different aspects of embeddings and the model. Finally, in the last section we present our conclusions along with some avenues for future work. 2 Background Several models such as He et al. (2008), Liu et al. (2008) and Shen et al. (2009) studied the use of contextual information for statistical machine translation (SMT). The idea is to go beyond the phrase level and enhance the phrase representation by taking surrounding phrases into account. This line of research is referred as discourse SMT (Hardmeier, 2014; Meyer, 2014). Because NNs can provide distributed representations for words and phrases, they are ideally suited to the task of comparing semantic similarity. Unsupervised models such as Word2Vec2 (Mikolov et al., 2013a) or Paragraph Vectors (Le & Mikolov, 2014) have shown that distributional information is often enough to learn high-quality word and sentence embeddings. A large body of recent work has evaluated the use of embeddings in machine translation. A successful usecase was reported in (Mikolov et al., 2013b). They separately 1 2 Although the features contributed by the language mode"
W16-3403,C08-1041,1,0.788524,"formation using the neural features. We search in the source and target spaces and retrieve the closest constituent to the phrase pair in our bilingual embedding space. The structure of the paper is as follows. Section 2 gives an overview of related work. Section 3 explains our pipeline and the network architecture in detail. In Section 4, experimental results are reported. We also have a separate section to discuss different aspects of embeddings and the model. Finally, in the last section we present our conclusions along with some avenues for future work. 2 Background Several models such as He et al. (2008), Liu et al. (2008) and Shen et al. (2009) studied the use of contextual information for statistical machine translation (SMT). The idea is to go beyond the phrase level and enhance the phrase representation by taking surrounding phrases into account. This line of research is referred as discourse SMT (Hardmeier, 2014; Meyer, 2014). Because NNs can provide distributed representations for words and phrases, they are ideally suited to the task of comparing semantic similarity. Unsupervised models such as Word2Vec2 (Mikolov et al., 2013a) or Paragraph Vectors (Le & Mikolov, 2014) have shown that"
W16-3403,W04-3250,0,0.199529,"sed MERT (Och, 2003) for tuning. In the experiments we trained 5-gram language models on the monolingual parts of the bilingual corpora using SRILM (Stolcke et al., 2002). We used BLEU (Papineni et al., 2002) as the evaluation metric. We added our features to the phrase table and tuned the translation models. Table 2 shows the impact of each feature. We also estimated the translation quality in the presence of the all features (we run MERT for each row of Table 2). Bold numbers are statistically significant according to the results of paired bootstrap re-sampling with p=0.05 for 1000 samples (Koehn, 2004). Arrows indicate whether the new features increased or decreased the quality over the baseline. 4 http://www.statmt.org/europarl/ 136 Passban et al. Table 2. Impact of the proposed features. Feature Baseline sp2tp sp2sm sp2tm tp2tm tp2sm sm2tm All En–Fa 21.03 21.46 21.32 21.40 20.40 21.93 21.18 21.84 ↑↓ 0.00 0.43 ↑ 0.29 ↑ 0.37 ↑ 0.63 ↓ 0.90 ↑ 0.15 ↑ 0.81 ↑ Fa–En 29.21 29.71 29.74 29.56 29.56 29.26 30.08 30.26 ↑↓ En–Cz ↑↓ 0.00 28.35 0.00 0.50 ↑ 28.72 0.37 ↑ 0.53 ↑ 28.30 0.05 ↓ 0.35 ↑ 28.52 0.17 ↑ 0.35 ↑ 28.00 0.35 ↓ 0.05 ↑ 28.94 0.59↑ 0.87 ↑ 28.36 0.01 ↑ 1.05 ↑ 29.01 0.66 ↑ Cz–En 39.63 40.34 3"
W16-3403,2005.mtsummit-papers.11,0,0.00908734,"cab. Fig. 2. Network architecture. The input document is S = w1 w2 w3 w4 w5 w6 and the target word is w3 . 4 Experimental Results We evaluated our new features on two language pairs: En–Fa and En–Cz. Both Farsi and Czech are morphologically rich languages; therefore, translation to/from these languages can be more difficult than it is for languages where words tend to be discrete semantic units. Farsi is also a low-resource language, so we are interested in working with these pairs. For the En–Fa pair we used the TEP++ corpus (Passban et al., 2015a) and for Czech we used the Europarl4 corpus (Koehn, 2005). TEP++ is a collection of 600,000 parallel sentences. We used 1000 and 2000 sentences for testing and tuning, respectively and the rest of the corpus for training. From the Czech dataset we selected the same number of sentences for training, testing and tuning. The baseline system is a PBSMT engine built using Moses (Koehn et al., 2007) with the default configuration. We used MERT (Och, 2003) for tuning. In the experiments we trained 5-gram language models on the monolingual parts of the bilingual corpora using SRILM (Stolcke et al., 2002). We used BLEU (Papineni et al., 2002) as the evaluati"
W16-3403,P07-2045,0,0.0103536,"Missing"
W16-3403,D08-1010,1,0.729984,"e neural features. We search in the source and target spaces and retrieve the closest constituent to the phrase pair in our bilingual embedding space. The structure of the paper is as follows. Section 2 gives an overview of related work. Section 3 explains our pipeline and the network architecture in detail. In Section 4, experimental results are reported. We also have a separate section to discuss different aspects of embeddings and the model. Finally, in the last section we present our conclusions along with some avenues for future work. 2 Background Several models such as He et al. (2008), Liu et al. (2008) and Shen et al. (2009) studied the use of contextual information for statistical machine translation (SMT). The idea is to go beyond the phrase level and enhance the phrase representation by taking surrounding phrases into account. This line of research is referred as discourse SMT (Hardmeier, 2014; Meyer, 2014). Because NNs can provide distributed representations for words and phrases, they are ideally suited to the task of comparing semantic similarity. Unsupervised models such as Word2Vec2 (Mikolov et al., 2013a) or Paragraph Vectors (Le & Mikolov, 2014) have shown that distributional info"
W16-3403,P03-1021,0,0.0122874,"Farsi is also a low-resource language, so we are interested in working with these pairs. For the En–Fa pair we used the TEP++ corpus (Passban et al., 2015a) and for Czech we used the Europarl4 corpus (Koehn, 2005). TEP++ is a collection of 600,000 parallel sentences. We used 1000 and 2000 sentences for testing and tuning, respectively and the rest of the corpus for training. From the Czech dataset we selected the same number of sentences for training, testing and tuning. The baseline system is a PBSMT engine built using Moses (Koehn et al., 2007) with the default configuration. We used MERT (Och, 2003) for tuning. In the experiments we trained 5-gram language models on the monolingual parts of the bilingual corpora using SRILM (Stolcke et al., 2002). We used BLEU (Papineni et al., 2002) as the evaluation metric. We added our features to the phrase table and tuned the translation models. Table 2 shows the impact of each feature. We also estimated the translation quality in the presence of the all features (we run MERT for each row of Table 2). Bold numbers are statistically significant according to the results of paired bootstrap re-sampling with p=0.05 for 1000 samples (Koehn, 2004). Arrows"
W16-3403,P02-1038,0,0.00763426,"rch problem where the score at each step of exploration is formulated as a log-linear model (Koehn, 2010). For each candidate phrase, the set of features is combined with a set of learned weights to find the best target counterpart of the provided source sentence. Because an exhaustive search of the candidate space is not computationally feasible, the space is typically pruned via some heuristic search, such as beam search (Koehn, 2010). The discriminative log-linear model allows the incorporation of arbitrary context-dependent and context-independent features. Thus, features such as those in Och and Ney (2002) or Chiang et al. (2009) can be combined to improve translation performance. The standard baseline bilingual features included in Moses (Koehn et al., 2007) by default are: the phrase translation 130 Passban et al. probability φ(e|f ), inverse phrase translation probability φ(f |e), direct lexical weighting lex(e|f ) and inverse lexical weighting lex(f |e).1 The scores in the phrase table are computed directly from the co-occurrence of aligned phrases in training corpora. A large body of recent work evaluates the hypothesis that co-occurrence information alone cannot capture contextual informa"
W16-3403,P02-1040,0,0.0971356,"sed the Europarl4 corpus (Koehn, 2005). TEP++ is a collection of 600,000 parallel sentences. We used 1000 and 2000 sentences for testing and tuning, respectively and the rest of the corpus for training. From the Czech dataset we selected the same number of sentences for training, testing and tuning. The baseline system is a PBSMT engine built using Moses (Koehn et al., 2007) with the default configuration. We used MERT (Och, 2003) for tuning. In the experiments we trained 5-gram language models on the monolingual parts of the bilingual corpora using SRILM (Stolcke et al., 2002). We used BLEU (Papineni et al., 2002) as the evaluation metric. We added our features to the phrase table and tuned the translation models. Table 2 shows the impact of each feature. We also estimated the translation quality in the presence of the all features (we run MERT for each row of Table 2). Bold numbers are statistically significant according to the results of paired bootstrap re-sampling with p=0.05 for 1000 samples (Koehn, 2004). Arrows indicate whether the new features increased or decreased the quality over the baseline. 4 http://www.statmt.org/europarl/ 136 Passban et al. Table 2. Impact of the proposed features. Feat"
W16-3403,W15-4911,1,0.558797,"nce-level and incorporated the source side information into the decoding phase by finding the similarities between phrases and source embeddings. Some other models re-scored the phrase table (Alkhouli et al., 2014) or generated new phrase pairs in order to address the OOV word problem (Zhao et al., 2014). Our network makes use of some ideas from existing models, but also extends the information available to the embedding model. We train embeddings in the joint space using both source and target side information simultaneously, using a model which is similar to that of Devlin et al. (2014) and Passban et al. (2015b). Similar to Gao et al. (2013) we make embeddings for phrases and sentences and add their similarity as feature functions to the SMT model. 3 Proposed Method In order to train our bilingual embedding model, we start by creating a large bilingual corpus. Each line of the corpus may include: – a source or target sentence, – a source or target phrase, – a concatenation of a phrase pair (source and target phrases which are each other’s translation), – a tuple of source and target words (each other’s translation). Sentences of the bilingual corpus are taken from the SMT training corpus. According"
W16-3403,D09-1008,0,0.0168738,"search in the source and target spaces and retrieve the closest constituent to the phrase pair in our bilingual embedding space. The structure of the paper is as follows. Section 2 gives an overview of related work. Section 3 explains our pipeline and the network architecture in detail. In Section 4, experimental results are reported. We also have a separate section to discuss different aspects of embeddings and the model. Finally, in the last section we present our conclusions along with some avenues for future work. 2 Background Several models such as He et al. (2008), Liu et al. (2008) and Shen et al. (2009) studied the use of contextual information for statistical machine translation (SMT). The idea is to go beyond the phrase level and enhance the phrase representation by taking surrounding phrases into account. This line of research is referred as discourse SMT (Hardmeier, 2014; Meyer, 2014). Because NNs can provide distributed representations for words and phrases, they are ideally suited to the task of comparing semantic similarity. Unsupervised models such as Word2Vec2 (Mikolov et al., 2013a) or Paragraph Vectors (Le & Mikolov, 2014) have shown that distributional information is often enough"
W16-3403,D15-1167,0,0.0347918,"e level similarities. Retrieved instances are semantically related to the given queries. Table 3. The top 10 most similar vectors for the given English query. Recall that the retrieved vectors could belong to words, phrases or sentences in either English or Farsi and word or phrase pairs. The items that were originally in Farsi have been translated into English, and are indicated with italics. Query 1 2 3 4 5 6 7 8 9 10 sadness <apprehension, nervous> emotion <ill,sick> pain <money,money> benignity <may he was punished,punished harshly> is really gonna hurt i know tom ’ s dying <bitter,angry> Tang et al. (2015) proposed that a sentence embedding could be generated by averaging/concatenating embeddings of the words in that sentence. In our case the model by Tang et al. was not as beneficial as ours for both Farsi and Czech. As an example if the sp2tp is computed using their model, it degrades the En–Fa direction’s BLEU from 21.03 to 20.97 and its improvement for the Fa–En direction is only +0.11 points (almost 5 times less than ours). Our goal is not to compare our model to that of Tang et al.. We only performed a simple comparison on the most important feature to see the difference. Furthermore, acc"
W16-3403,D14-1175,0,0.0133728,"ranslation engine with very large monolingual data and only a small number of bilingual word pairs. The cross-lingual transformation mechanism allows the engine to search for translations for OOV (out-of-vocabulary) words by consulting a monolingual index which contains words that were not observed in the parallel training data.The work by Garcia and Tiedemann (2014) is another model follows that the same paradigm. However, machine translation (MT) is more than word-level translation. In Mart´ınez et al. (2015) word embeddings were used in document-level MT to disambiguate the word selection. Tran et al. (2014) used bilingual word embeddings to compute the semantic similarity of phrases. To extend the application of text embedding beyond single words, Gao et al. (2013) proposed learning embeddings for source and target phrases by training a network to maximize the sentence-level BLEU score. Costa-jussa et al. (2014) worked at the sentence-level and incorporated the source side information into the decoding phase by finding the similarities between phrases and source embeddings. Some other models re-scored the phrase table (Alkhouli et al., 2014) or generated new phrase pairs in order to address the"
W16-3403,N09-1025,0,\N,Missing
W16-3403,N15-1176,0,\N,Missing
W16-3406,P96-1041,0,0.113892,"reordering ability. The two syntax-based systems are: HPB, the hierarchical phrase-based model in Moses with default configurations, and D2S, an improved dependency-to-string model which has been implemented in Moses (Li et al., 2014b).8 We add the TM features in Li et al. (2014a) to phrase-based systems and our TM features to syntax-based systems. Word alignment is performed by GIZA++ (Och and Ney, 2004) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the target side of our training corpus with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune weights. BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006) are used for evaluation.9 5.3 Results and Discussion Table 2 accounts for the results obtained for all our experiments. As may be observed, all our baselines are already pretty high and thus improvements are harder to obtain. 7 8 9 Unfortunately, due to confidentiality agreements the data used in these experiments cannot be publicly released. http://computing.dcu.ie/ liangyouli/dep2str.zip https://github.com/jhclark/multeval 172 Li et a"
W16-3406,N12-1047,0,0.0163712,"sed systems are: HPB, the hierarchical phrase-based model in Moses with default configurations, and D2S, an improved dependency-to-string model which has been implemented in Moses (Li et al., 2014b).8 We add the TM features in Li et al. (2014a) to phrase-based systems and our TM features to syntax-based systems. Word alignment is performed by GIZA++ (Och and Ney, 2004) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the target side of our training corpus with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune weights. BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006) are used for evaluation.9 5.3 Results and Discussion Table 2 accounts for the results obtained for all our experiments. As may be observed, all our baselines are already pretty high and thus improvements are harder to obtain. 7 8 9 Unfortunately, due to confidentiality agreements the data used in these experiments cannot be publicly released. http://computing.dcu.ie/ liangyouli/dep2str.zip https://github.com/jhclark/multeval 172 Li et al. Table 2. Metric scores for all syst"
W16-3406,P05-1033,0,0.657359,"ns such as the translation of the English call. . . off into the Spanish cancelar cannot be learned. In this paper, we explore the possibility of using a run-time TM combination on syntax-based MT. Syntax-based MT learns translation rules which can be easily extrapolated to new sentences by allowing non-terminals. In our approach, for each applied translation rule during decoding, we identify a corresponding pattern in the TM and then extract sparse features which are subsequently added to our system. In our experiments, the TM combination is done on the hierarchical phrase-based (HPB) model (Chiang, 2005) and the dependency-to-string (D2S) model (Xie et al., 2011; Li et al., 2014b). The experimental results on real English–Spanish data3 show that syntax-based models produce significantly better translations than phrase-based models. After adding the TM features, the syntax-based models are further significantly improved. 2 TMs in SMT Combining TMs and SMT together has been explored in different ways in recent years. He et al. (2010a) presented a recommendation system which used a Support Vector Machine (Cortes and Vapnik, 1995) binary classifier to select a translation from the outputs of a TM"
W16-3406,P11-2031,0,0.0560528,"Missing"
W16-3406,W11-2107,0,0.018169,"ations, and D2S, an improved dependency-to-string model which has been implemented in Moses (Li et al., 2014b).8 We add the TM features in Li et al. (2014a) to phrase-based systems and our TM features to syntax-based systems. Word alignment is performed by GIZA++ (Och and Ney, 2004) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the target side of our training corpus with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune weights. BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006) are used for evaluation.9 5.3 Results and Discussion Table 2 accounts for the results obtained for all our experiments. As may be observed, all our baselines are already pretty high and thus improvements are harder to obtain. 7 8 9 Unfortunately, due to confidentiality agreements the data used in these experiments cannot be publicly released. http://computing.dcu.ie/ liangyouli/dep2str.zip https://github.com/jhclark/multeval 172 Li et al. Table 2. Metric scores for all systems on English–Spanish. Each score is the average score over three MIRA runs (Clark et al."
W16-3406,D08-1089,0,0.0351604,"t that those segments were HTML addresses that did not require a translation and would have added noise to our data. Inline tags were not treated specifically and were maintained in the data. Once our data was cleaned, we randomly split it into training, development and test. Table 1 summarizes the size of our data in terms of number of sentences and running words. 5.2 Settings In our experiments, we build four baselines. The two phrase-based baselines are: PB, the phrase-based model in Moses with default configurations, and PBLR, the phrasebased model, adding three lexical reordering models (Galley and Manning, 2008) to improve its reordering ability. The two syntax-based systems are: HPB, the hierarchical phrase-based model in Moses with default configurations, and D2S, an improved dependency-to-string model which has been implemented in Moses (Li et al., 2014b).8 We add the TM features in Li et al. (2014a) to phrase-based systems and our TM features to syntax-based systems. Word alignment is performed by GIZA++ (Och and Ney, 2004) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the target side of our training corpus with modified Kneser-N"
W16-3406,P10-1064,0,0.0501812,"Missing"
W16-3406,C10-2043,0,0.0446743,"Missing"
W16-3406,P07-2045,0,0.00790841,"Missing"
W16-3406,N03-1017,0,0.0358308,"ials, they are believed to be useful for Statistical Machine Translation (SMT). The combination of TM and SMT (henceforth referred as “TM combination”) has been explored in many ways and it has shown to improve translation quality. Unlike the well-known pipeline approaches (Koehn and Senellart, 2010; Ma et al., 2011), which use a TM combination at sentence-level, run-time TM combination (namely, combining the TM and SMT during decoding) can make a better use of the matched sub-sentences (Wang et al., 2013; Li et al., 2014a). Such run-time combination has been explored on Phrase-Based (PB) MT (Koehn et al., 2003). However, PBMT systems making use of TMs only take into consideration continuous segments and thus generalizations such as the translation of the English call. . . off into the Spanish cancelar cannot be learned. In this paper, we explore the possibility of using a run-time TM combination on syntax-based MT. Syntax-based MT learns translation rules which can be easily extrapolated to new sentences by allowing non-terminals. In our approach, for each applied translation rule during decoding, we identify a corresponding pattern in the TM and then extract sparse features which are subsequently a"
W16-3406,2010.jec-1.4,0,0.362178,"nce and the one stored in the TM to enhance the post-editing task. Different coloring schemes are used to highlight changes and additions to the source text in the TM to help the translator spot quicker the post-edits needed. As TMs can help produce high quality and 166 Li et al. consistent translations for repetitive materials, they are believed to be useful for Statistical Machine Translation (SMT). The combination of TM and SMT (henceforth referred as “TM combination”) has been explored in many ways and it has shown to improve translation quality. Unlike the well-known pipeline approaches (Koehn and Senellart, 2010; Ma et al., 2011), which use a TM combination at sentence-level, run-time TM combination (namely, combining the TM and SMT during decoding) can make a better use of the matched sub-sentences (Wang et al., 2013; Li et al., 2014a). Such run-time combination has been explored on Phrase-Based (PB) MT (Koehn et al., 2003). However, PBMT systems making use of TMs only take into consideration continuous segments and thus generalizations such as the translation of the English call. . . off into the Spanish cancelar cannot be learned. In this paper, we explore the possibility of using a run-time TM co"
W16-3406,2014.amta-researchers.19,1,0.604082,"help produce high quality and 166 Li et al. consistent translations for repetitive materials, they are believed to be useful for Statistical Machine Translation (SMT). The combination of TM and SMT (henceforth referred as “TM combination”) has been explored in many ways and it has shown to improve translation quality. Unlike the well-known pipeline approaches (Koehn and Senellart, 2010; Ma et al., 2011), which use a TM combination at sentence-level, run-time TM combination (namely, combining the TM and SMT during decoding) can make a better use of the matched sub-sentences (Wang et al., 2013; Li et al., 2014a). Such run-time combination has been explored on Phrase-Based (PB) MT (Koehn et al., 2003). However, PBMT systems making use of TMs only take into consideration continuous segments and thus generalizations such as the translation of the English call. . . off into the Spanish cancelar cannot be learned. In this paper, we explore the possibility of using a run-time TM combination on syntax-based MT. Syntax-based MT learns translation rules which can be easily extrapolated to new sentences by allowing non-terminals. In our approach, for each applied translation rule during decoding, we identify"
W16-3406,W14-4014,1,0.73714,"help produce high quality and 166 Li et al. consistent translations for repetitive materials, they are believed to be useful for Statistical Machine Translation (SMT). The combination of TM and SMT (henceforth referred as “TM combination”) has been explored in many ways and it has shown to improve translation quality. Unlike the well-known pipeline approaches (Koehn and Senellart, 2010; Ma et al., 2011), which use a TM combination at sentence-level, run-time TM combination (namely, combining the TM and SMT during decoding) can make a better use of the matched sub-sentences (Wang et al., 2013; Li et al., 2014a). Such run-time combination has been explored on Phrase-Based (PB) MT (Koehn et al., 2003). However, PBMT systems making use of TMs only take into consideration continuous segments and thus generalizations such as the translation of the English call. . . off into the Spanish cancelar cannot be learned. In this paper, we explore the possibility of using a run-time TM combination on syntax-based MT. Syntax-based MT learns translation rules which can be easily extrapolated to new sentences by allowing non-terminals. In our approach, for each applied translation rule during decoding, we identify"
W16-3406,P11-1124,0,0.0337589,"Missing"
W16-3406,P02-1038,0,0.129899,"s , j s ] and [it , j t ] for [i, j]; s = words in span [is , j s ] and replacing phrases covered by h[isk , jks ]i with non-terminals; t = words in span [it , j t ] and replacing phrases covered by [itk , jkt ] with non-terminals; R = hs, t, ai, a indicates mappings between non-terminals in s and t; where the underlined element denotes the leaf node. Variables in the Dep2Str model are constrained either by words (like x1 :selection) or Part-of-Speech tags (like x1 :NN). 4 TM Combination Method Inspired by Li et al. (2014a), who directly add sparse features to the log-linear framework of SMT (Och and Ney, 2002) to combine a TM with the PB model, in this paper we extract sparse features for each applied rule during decoding and directly add them to our syntax-based SMT systems. These features can be jointly trained with other features to maximize translation quality measured by BLEU (Papineni et al., 2002). Given an input sentence in our test set, our approach starts from retrieving the most similar sentence from a TM.6 The similarity is measured by the so-called fuzzy match score. Concretely, we use the word-based string-edit distance in Equation (2) (Koehn and Senellart, 2010) to compute the fuzzy"
W16-3406,J04-4002,0,0.0372323,"wo phrase-based baselines are: PB, the phrase-based model in Moses with default configurations, and PBLR, the phrasebased model, adding three lexical reordering models (Galley and Manning, 2008) to improve its reordering ability. The two syntax-based systems are: HPB, the hierarchical phrase-based model in Moses with default configurations, and D2S, an improved dependency-to-string model which has been implemented in Moses (Li et al., 2014b).8 We add the TM features in Li et al. (2014a) to phrase-based systems and our TM features to syntax-based systems. Word alignment is performed by GIZA++ (Och and Ney, 2004) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the target side of our training corpus with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune weights. BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006) are used for evaluation.9 5.3 Results and Discussion Table 2 accounts for the results obtained for all our experiments. As may be observed, all our baselines are already pretty high and thus improvements are harder to obtain."
W16-3406,P02-1040,0,0.095457,"nd t; where the underlined element denotes the leaf node. Variables in the Dep2Str model are constrained either by words (like x1 :selection) or Part-of-Speech tags (like x1 :NN). 4 TM Combination Method Inspired by Li et al. (2014a), who directly add sparse features to the log-linear framework of SMT (Och and Ney, 2002) to combine a TM with the PB model, in this paper we extract sparse features for each applied rule during decoding and directly add them to our syntax-based SMT systems. These features can be jointly trained with other features to maximize translation quality measured by BLEU (Papineni et al., 2002). Given an input sentence in our test set, our approach starts from retrieving the most similar sentence from a TM.6 The similarity is measured by the so-called fuzzy match score. Concretely, we use the word-based string-edit distance in Equation (2) (Koehn and Senellart, 2010) to compute the fuzzy match score between the input sentence and the TM instance. F =1− edit distance(input, tm source) max( |input |, |tm source |) (2) During the calculation of the fuzzy match score, we also obtain a sequence of operations, including insertion, match, substitution and deletion, which are useful for fin"
W16-3406,2015.mtsummit-wptp.4,1,0.80479,"Missing"
W16-3406,2006.amta-papers.25,0,0.0202991,"cy-to-string model which has been implemented in Moses (Li et al., 2014b).8 We add the TM features in Li et al. (2014a) to phrase-based systems and our TM features to syntax-based systems. Word alignment is performed by GIZA++ (Och and Ney, 2004) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the target side of our training corpus with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune weights. BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006) are used for evaluation.9 5.3 Results and Discussion Table 2 accounts for the results obtained for all our experiments. As may be observed, all our baselines are already pretty high and thus improvements are harder to obtain. 7 8 9 Unfortunately, due to confidentiality agreements the data used in these experiments cannot be publicly released. http://computing.dcu.ie/ liangyouli/dep2str.zip https://github.com/jhclark/multeval 172 Li et al. Table 2. Metric scores for all systems on English–Spanish. Each score is the average score over three MIRA runs (Clark et al., 2011). ∗ means a system is be"
W16-3406,P13-1002,0,0.0718779,"needed. As TMs can help produce high quality and 166 Li et al. consistent translations for repetitive materials, they are believed to be useful for Statistical Machine Translation (SMT). The combination of TM and SMT (henceforth referred as “TM combination”) has been explored in many ways and it has shown to improve translation quality. Unlike the well-known pipeline approaches (Koehn and Senellart, 2010; Ma et al., 2011), which use a TM combination at sentence-level, run-time TM combination (namely, combining the TM and SMT during decoding) can make a better use of the matched sub-sentences (Wang et al., 2013; Li et al., 2014a). Such run-time combination has been explored on Phrase-Based (PB) MT (Koehn et al., 2003). However, PBMT systems making use of TMs only take into consideration continuous segments and thus generalizations such as the translation of the English call. . . off into the Spanish cancelar cannot be learned. In this paper, we explore the possibility of using a run-time TM combination on syntax-based MT. Syntax-based MT learns translation rules which can be easily extrapolated to new sentences by allowing non-terminals. In our approach, for each applied translation rule during deco"
W16-3406,D11-1020,1,0.921705,"into the Spanish cancelar cannot be learned. In this paper, we explore the possibility of using a run-time TM combination on syntax-based MT. Syntax-based MT learns translation rules which can be easily extrapolated to new sentences by allowing non-terminals. In our approach, for each applied translation rule during decoding, we identify a corresponding pattern in the TM and then extract sparse features which are subsequently added to our system. In our experiments, the TM combination is done on the hierarchical phrase-based (HPB) model (Chiang, 2005) and the dependency-to-string (D2S) model (Xie et al., 2011; Li et al., 2014b). The experimental results on real English–Spanish data3 show that syntax-based models produce significantly better translations than phrase-based models. After adding the TM features, the syntax-based models are further significantly improved. 2 TMs in SMT Combining TMs and SMT together has been explored in different ways in recent years. He et al. (2010a) presented a recommendation system which used a Support Vector Machine (Cortes and Vapnik, 1995) binary classifier to select a translation from the outputs of a TM and an SMT system. He et al. (2010b) extended this work by"
W17-1715,W10-3704,0,0.0790441,"Missing"
W17-1715,2005.mtsummit-papers.11,0,0.0102402,"idation experiment on training data was conducted using each template against each of the 15 languages. For each language, the best performing template (regardless of the language family for which it was developed) was chosen for the final challenge, in which the CRF++ system was trained using that selected template on the full training data for the language, and the prediction output was generated from the blind test set provided. FS3 was chosen for Greek, Spanish, French, Slovenian and Turkish, whilst FS4 was chosen for Swedish and FS5 for the rest of the languages. 3.2 4.1 We use Europarl (Koehn, 2005) as third-party corpus, because it is large and contains most languages addressed in this Shared Task. It does not contain Farsi, Maltese and Turkish, which are therefore excluded from this part of the process. For each of the 12 remaining languages, we use only the monolingual Europarl corpus, and we tokenise it using the generic tokeniser provided by the organisers.6 Offical Evaluation 4.2 Table 1 shows, under “crf”, the F1 scores for each of the VMWE categories in the competition: ID (low-compositional verbal idiomatic expressions), IReflV (reflexive verbs), LVC (light verb constructions),"
W17-1715,W06-1620,0,0.0411464,"MWEs helps the word alignment of verbs, and of Boukobza and Rappoport (2009) who used sentence surface features based on the canonical form of VMWEs. In addition, Sun et al. (2013) applied a Hidden SemiCRF model to capture latent semantics from Chinese microblogging posts; Hosseini et al. (2016) used double-chained CRF for minimal semantic units detection in SemEval task. And Bar et al. (2014) discussed that syntactic construction classes are helpful for verb-noun and verb-particle MWE identification. Schneider et al. (2014) also used a sequence tagger to annotate MWEs, including VMWEs, while Blunsom and Baldwin (2006) and Vincze et al. (2011) have used CRF taggers for identifying contiguous MWEs. In relation to our open-track approach, Attia et al. (2010) exploited large corpora to identify Arabic MWEs, and Legrand and Collobert (2016) applied fixed-size continuous vector representations for various length of phrases and chunks in the MWE identification task. Constant et al. (2012) used a re-ranker for MWEs in an n-best parser. 3 so we felt that we were unlikely to obtain good results with them. It should be noted that of these 15 languages, four (Czech, Farsi, Maltese and Romanian) were provided without s"
W17-1715,W16-1810,0,0.0271058,"Missing"
W17-1715,D09-1049,0,0.0351625,"loped, such as combining statistical and symbolic methods (Sag et al., 2001), single and multi-prototype word embeddings (Salehi et al., 2015), integrating MWE identification within larger NLP tasks such as parsing (Green et al., 2011; Green et al., 2013; Constant et al., 2012) and machine translation (Tsvetkov and Wintner, 2010; Salehi et al., 2014a; Salehi et al., 2014b). More directly related to our closed-track approach are works such as that of Venkatapathy and Joshi (2006), who showed that information about the degree of compositionality of MWEs helps the word alignment of verbs, and of Boukobza and Rappoport (2009) who used sentence surface features based on the canonical form of VMWEs. In addition, Sun et al. (2013) applied a Hidden SemiCRF model to capture latent semantics from Chinese microblogging posts; Hosseini et al. (2016) used double-chained CRF for minimal semantic units detection in SemEval task. And Bar et al. (2014) discussed that syntactic construction classes are helpful for verb-noun and verb-particle MWE identification. Schneider et al. (2014) also used a sequence tagger to annotate MWEs, including VMWEs, while Blunsom and Baldwin (2006) and Vincze et al. (2011) have used CRF taggers fo"
W17-1715,W11-1309,0,0.0179557,"imilarity, Cosine similarity with or without IDF weights). The main difficulty in representing a predicted sequence as a fixed set of features is that each sentence can contain any number of MWEs, and each MWE can contain any number of words. We opted for “summarising” any non-fixed number of features with three statistics: minimum, mean and maximum. For instance, the similarity scores Unofficial Open Track: Semantic Re-Ranking We implemented an optional post-processing stage intended to improve the performance of our CRF-based method using a distributional semantics approach (Sch¨utze, 1998; Maldonado and Emms, 2011). Intuitively, the goal is to assess the likeliness of a given candidate MWE, and then, based on such features for all the candidate MWEs in a sentence, to select the most likely predicted sequence among a set of 10 potential sequences. This part of the system receives the output produced by CRF++ in the form of the 10 most likely predictions for every sentence. For every such set of 10 predicted sequences, context vectors are 6 Discrepancies are to be expected between the tokenisation of the Shared Task corpus (language-specific) and the one performed on Europarl (generic). 7 There are multip"
W17-1715,P12-1022,0,0.140972,"etails, feature templates, code and experiment instructions: https://github.com/alfredomg/ADAPT-MWE17 3 Official results: http://bit.ly/2krOu05 http://www.parseme.eu 114 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 114–120, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics have been developed, such as combining statistical and symbolic methods (Sag et al., 2001), single and multi-prototype word embeddings (Salehi et al., 2015), integrating MWE identification within larger NLP tasks such as parsing (Green et al., 2011; Green et al., 2013; Constant et al., 2012) and machine translation (Tsvetkov and Wintner, 2010; Salehi et al., 2014a; Salehi et al., 2014b). More directly related to our closed-track approach are works such as that of Venkatapathy and Joshi (2006), who showed that information about the degree of compositionality of MWEs helps the word alignment of verbs, and of Boukobza and Rappoport (2009) who used sentence surface features based on the canonical form of VMWEs. In addition, Sun et al. (2013) applied a Hidden SemiCRF model to capture latent semantics from Chinese microblogging posts; Hosseini et al. (2016) used double-chained CRF for"
W17-1715,D11-1067,0,0.148211,"Missing"
W17-1715,J13-1009,0,0.338776,"Missing"
W17-1715,D14-1189,0,0.0245028,"com/alfredomg/ADAPT-MWE17 3 Official results: http://bit.ly/2krOu05 http://www.parseme.eu 114 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 114–120, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics have been developed, such as combining statistical and symbolic methods (Sag et al., 2001), single and multi-prototype word embeddings (Salehi et al., 2015), integrating MWE identification within larger NLP tasks such as parsing (Green et al., 2011; Green et al., 2013; Constant et al., 2012) and machine translation (Tsvetkov and Wintner, 2010; Salehi et al., 2014a; Salehi et al., 2014b). More directly related to our closed-track approach are works such as that of Venkatapathy and Joshi (2006), who showed that information about the degree of compositionality of MWEs helps the word alignment of verbs, and of Boukobza and Rappoport (2009) who used sentence surface features based on the canonical form of VMWEs. In addition, Sun et al. (2013) applied a Hidden SemiCRF model to capture latent semantics from Chinese microblogging posts; Hosseini et al. (2016) used double-chained CRF for minimal semantic units detection in SemEval task. And Bar et al. (2014) d"
W17-1715,E14-1050,0,0.0303661,"com/alfredomg/ADAPT-MWE17 3 Official results: http://bit.ly/2krOu05 http://www.parseme.eu 114 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 114–120, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics have been developed, such as combining statistical and symbolic methods (Sag et al., 2001), single and multi-prototype word embeddings (Salehi et al., 2015), integrating MWE identification within larger NLP tasks such as parsing (Green et al., 2011; Green et al., 2013; Constant et al., 2012) and machine translation (Tsvetkov and Wintner, 2010; Salehi et al., 2014a; Salehi et al., 2014b). More directly related to our closed-track approach are works such as that of Venkatapathy and Joshi (2006), who showed that information about the degree of compositionality of MWEs helps the word alignment of verbs, and of Boukobza and Rappoport (2009) who used sentence surface features based on the canonical form of VMWEs. In addition, Sun et al. (2013) applied a Hidden SemiCRF model to capture latent semantics from Chinese microblogging posts; Hosseini et al. (2016) used double-chained CRF for minimal semantic units detection in SemEval task. And Bar et al. (2014) d"
W17-1715,W15-3103,0,0.0137599,"P-1, P, P+1, P+2, P-1/P, P/P+1. • FS5: FS4, L/HP. Each template summary above consists of a name (FS3, FS4 or FS5) and a list of feature Official Closed Track: CRF Labelling We decided to model the problem of VMWE identification as a sequence labelling and classification problem. We operationalise our solution through CRFs (Lafferty et al., 2001), which encode relationships between observations in a sequence. We implemented our solution using the CRF++4 system. CRFs have been successfully applied to such sequence-sensitive NLP tasks as segmentation, named-entity recognition (Han et al., 2013; Han et al., 2015) and part-of-speech tagging. Our team attempted 15 out of the 18 languages involved in the Shared Task. The data for the languages we did not attempt (Bulgarian, Hebrew and Lithuanian) lacked morpho-syntactic information, 4 Features 5 https://taku910.github.io/crfpp/ 115 Actual templates are on GitHub. See footnote 2. computed for each candidate MWE, using a large third-party corpus. A set of features based on these context vectors is computed for each predicted sequence. These features are then fed to a supervised regression algorithm, which predicts a score for every predicted sequence; the"
W17-1715,N15-1099,0,0.0481095,"Missing"
W17-1715,S16-1143,0,0.02915,"., 2011; Green et al., 2013; Constant et al., 2012) and machine translation (Tsvetkov and Wintner, 2010; Salehi et al., 2014a; Salehi et al., 2014b). More directly related to our closed-track approach are works such as that of Venkatapathy and Joshi (2006), who showed that information about the degree of compositionality of MWEs helps the word alignment of verbs, and of Boukobza and Rappoport (2009) who used sentence surface features based on the canonical form of VMWEs. In addition, Sun et al. (2013) applied a Hidden SemiCRF model to capture latent semantics from Chinese microblogging posts; Hosseini et al. (2016) used double-chained CRF for minimal semantic units detection in SemEval task. And Bar et al. (2014) discussed that syntactic construction classes are helpful for verb-noun and verb-particle MWE identification. Schneider et al. (2014) also used a sequence tagger to annotate MWEs, including VMWEs, while Blunsom and Baldwin (2006) and Vincze et al. (2011) have used CRF taggers for identifying contiguous MWEs. In relation to our open-track approach, Attia et al. (2010) exploited large corpora to identify Arabic MWEs, and Legrand and Collobert (2016) applied fixed-size continuous vector representa"
W17-1715,J98-1004,0,0.0399133,"Missing"
W17-1715,C10-2144,0,0.0273999,"nstructions: https://github.com/alfredomg/ADAPT-MWE17 3 Official results: http://bit.ly/2krOu05 http://www.parseme.eu 114 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 114–120, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics have been developed, such as combining statistical and symbolic methods (Sag et al., 2001), single and multi-prototype word embeddings (Salehi et al., 2015), integrating MWE identification within larger NLP tasks such as parsing (Green et al., 2011; Green et al., 2013; Constant et al., 2012) and machine translation (Tsvetkov and Wintner, 2010; Salehi et al., 2014a; Salehi et al., 2014b). More directly related to our closed-track approach are works such as that of Venkatapathy and Joshi (2006), who showed that information about the degree of compositionality of MWEs helps the word alignment of verbs, and of Boukobza and Rappoport (2009) who used sentence surface features based on the canonical form of VMWEs. In addition, Sun et al. (2013) applied a Hidden SemiCRF model to capture latent semantics from Chinese microblogging posts; Hosseini et al. (2016) used double-chained CRF for minimal semantic units detection in SemEval task. An"
W17-1715,W06-1204,0,0.048231,"shop on Multiword Expressions (MWE 2017), pages 114–120, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics have been developed, such as combining statistical and symbolic methods (Sag et al., 2001), single and multi-prototype word embeddings (Salehi et al., 2015), integrating MWE identification within larger NLP tasks such as parsing (Green et al., 2011; Green et al., 2013; Constant et al., 2012) and machine translation (Tsvetkov and Wintner, 2010; Salehi et al., 2014a; Salehi et al., 2014b). More directly related to our closed-track approach are works such as that of Venkatapathy and Joshi (2006), who showed that information about the degree of compositionality of MWEs helps the word alignment of verbs, and of Boukobza and Rappoport (2009) who used sentence surface features based on the canonical form of VMWEs. In addition, Sun et al. (2013) applied a Hidden SemiCRF model to capture latent semantics from Chinese microblogging posts; Hosseini et al. (2016) used double-chained CRF for minimal semantic units detection in SemEval task. And Bar et al. (2014) discussed that syntactic construction classes are helpful for verb-noun and verb-particle MWE identification. Schneider et al. (2014)"
W17-1715,R11-1040,0,0.0171274,"f verbs, and of Boukobza and Rappoport (2009) who used sentence surface features based on the canonical form of VMWEs. In addition, Sun et al. (2013) applied a Hidden SemiCRF model to capture latent semantics from Chinese microblogging posts; Hosseini et al. (2016) used double-chained CRF for minimal semantic units detection in SemEval task. And Bar et al. (2014) discussed that syntactic construction classes are helpful for verb-noun and verb-particle MWE identification. Schneider et al. (2014) also used a sequence tagger to annotate MWEs, including VMWEs, while Blunsom and Baldwin (2006) and Vincze et al. (2011) have used CRF taggers for identifying contiguous MWEs. In relation to our open-track approach, Attia et al. (2010) exploited large corpora to identify Arabic MWEs, and Legrand and Collobert (2016) applied fixed-size continuous vector representations for various length of phrases and chunks in the MWE identification task. Constant et al. (2012) used a re-ranker for MWEs in an n-best parser. 3 so we felt that we were unlikely to obtain good results with them. It should be noted that of these 15 languages, four (Czech, Farsi, Maltese and Romanian) were provided without syntactic dependency infor"
W17-4717,W17-4758,0,0.0835228,"ese submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negative) predicted values prior to the calculation of the HTER score. The two submissions use the baseline features and the EnglishGerman submission also uses features from (Avramidis, 2017a). JXNU (T1): The JXNU submissions use features extracted from a neural network, including embedding features and cross-entropy features of the source sentences and their machine translations. The sentence embedding features are extracted through global average pooling from word embedding, which are trained using the WORD 2 VEC toolkit. The sentence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus."
W17-4717,W17-4772,0,0.0310859,"Missing"
W17-4717,W17-4759,0,0.0327025,"Missing"
W17-4717,L16-1356,1,0.770826,"or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method. They report results considering the word and its context versus the word in isolation, as well as variants with and without the gold labels at training time. Finally, for the phrase-level task, SHEF made use of predictions generated by BMAPS for task 2 and the phrase labelling approaches in (Blain et al., 2016). These approaches use the number of BAD word-level predictions in a phrase: an optimistic version labels the phrase as OK if at least half of the words in it are predicted to be OK, and a superpessimistic version labels the phrase as BAD if any word is in is predicted to be BAD. UHH (T1): The UHH-STK submission is based on sequence and tree kernels applied on the source and target input data for predicting the HTER score. The kernels use a backtranslation of the MT output into the source language as an additional input data representation. Further hand-crafted features were deﬁned in the form"
W17-4717,W17-4760,1,0.841464,"Missing"
W17-4717,W17-4755,1,0.0393895,"nd Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair"
W17-4717,W07-0718,1,0.696979,") • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), part"
W17-4717,W08-0309,1,0.537902,"Missing"
W17-4717,W10-1703,1,0.603181,"Missing"
W17-4717,W12-3102,1,0.508067,"Missing"
W17-4717,W17-4761,0,0.0349831,"Missing"
W17-4717,W17-4723,0,0.0362034,"Missing"
W17-4717,W17-4724,1,0.839009,"Missing"
W17-4717,W11-2103,1,0.744276,"Missing"
W17-4717,W17-4773,1,0.839713,"Missing"
W17-4717,W15-3025,1,0.905105,"Instead of predicting the HTER score, the systems attempted to predict the number of each of the four postediting operations (add, replace, shift, delete) at the sentence level. However, this did not lead to positive results. In future editions of the task, we plan to make this detailed post-editing information available again and suggest clear ways of using it. 5 Automatic Post-editing Task The WMT shared task on MT automatic postediting (APE), this year at its third round at WMT, aims to evaluate systems for the automatic correction of errors in a machine translated text. As pointed out by (Chatterjee et al., 2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. The third round of the APE task proposed to parti"
W17-4717,W17-4718,1,0.0662673,"builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair. Chinese allowed us to co-operate with an ongoing evaluation campaign on Asian languages org"
W17-4717,W17-4725,0,0.0391077,"Missing"
W17-4717,W08-0509,0,0.00859402,"l post-editors) and WMT17 DE-EN (German-English, pharmacological domain, professional post-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁci"
W17-4717,W17-4726,0,0.0387381,"Missing"
W17-4717,W13-2305,1,0.899702,"(RR) approach, so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer absolute quality. For example, RR can be used to discover which systems perform better than others, but RR does not provide any information about the absolute quality of system translations, i.e. it provides no information about how far a given system is from producing perfect output according to a human user. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and last year’s evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with RR and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established last year (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for English-to-Russian (th"
W17-4717,E14-1047,1,0.508986,"Missing"
W17-4717,N15-1124,1,0.658439,"Missing"
W17-4717,W13-0805,0,0.0140095,"ely contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets released in the three rounds 24 We used phrase-based MT systems trained with generic and in-domain parallel training data, leveraging prereordering techniques (Herrmann et al., 2013), and taking advantage of POS and word class-based language models. 25 For both language directions, the source sentences and reference translations were provided by TAUS (https://www.taus.net/). 197 EN-DE Train (23,000) Dev (1,000) Test (2,000) DE-EN Train (25,000) Dev (1,000) Test (2,000) SRC Tokens TGT PE SRC Types TGT PE SRC Lemmas TGT PE 384448 17827 65120 403306 19355 69812 411246 19763 71483 18220 2931 8061 27382 3333 9765 31652 3506 10502 10946 1922 2626 21959 2686 3976 25550 2806 4282 437833 17578 35087 453096 18130 36082 456163 18313 36480 29745 4426 6987 19866 3583 5391 19172 3642 5"
W17-4717,W17-4775,0,0.0513454,"Missing"
W17-4717,W17-4730,1,0.829861,"Missing"
W17-4717,W17-4731,0,0.029579,"Missing"
W17-4717,W17-4727,0,0.046917,"Missing"
W17-4717,W16-2378,0,0.0869978,"a manuallyrevised version of the target, done by professional translators. Test data consists of (source, target) pairs having similar characteristics of those in the training set. Human post-edits of the test target instances were left apart to measure system performance. English-German data were drawn from the Information Technology (IT) domain. Training and test sets respectively contain 11,000 and 2,000 triplets. The data released for the 2016 round of the task (15,000 instances) and the artiﬁcially generated post-editing triplets (4 million instances) used by last year’s winning system (Junczys-Dowmunt and Grundkiewicz, 2016) were also provided as additional training material. German-English data were drawn from the Pharmacological domain. Training and development sets respectively contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets rele"
W17-4717,W11-2123,0,0.00943973,"-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT o"
W17-4717,I17-1013,0,0.0334345,"Missing"
W17-4717,W16-2384,0,0.0137431,"features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma a"
W17-4717,W17-4763,0,0.0300648,"ence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamb"
W17-4717,W04-3250,1,0.380171,"age28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to t"
W17-4717,P07-2045,1,0.0149362,"t with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to the task, which represented the common backbone of APE systems before the spread of neural solutions. The system is based on Moses (Koehn et al., 2007); translation and reordering models were estimated following the Moses protocol with default setup using 27 http://www.cs.umd.edu/˜snover/tercom/ https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 28 5.2 Participants Seven teams participated in the English-German task by submitting a total of ﬁfteen runs. Two of them also participated in the German-English task with ﬁve submitted runs. Participants are listed in Table 27, and a short description of their systems is provided in the following. Adam Mickiewicz University. AMU’s (ENDE) participation explores and"
W17-4717,C14-1017,0,0.0141812,"ord embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained from three matrices corresponding to the training data, the development set and a “truth” matrix between them, which is built from the word alignments and the gold labels to indicate which lexical items form a pair, and whether or not their lexical relation is OK or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method."
W17-4717,Q17-1015,0,0.0112193,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W16-2387,0,0.0126259,"the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisation algorithm. We note th"
W17-4717,W17-4764,0,0.0178514,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W06-3114,1,0.104699,"g (Bojar et al., 2017b) • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news trans"
W17-4717,P03-1021,0,0.144477,"ed better than monolingual models. The code for these models is freely available.15 DCU (T2): DCU’s submission is an ensemble of neural MT systems with different input factors, designed to jointly tackle both the automatic post-editing and word-level QE. 15 https://github.com/patelrajnath/rnn4nlp 186 Word-level features which have proven effective for QE, such as part-of-speech tags and dependency labels are included as input factors to NMT systems. NMT systems using different input representations are ensembled together in a log-linear model which is tuned for the F1 -mult metric using MERT (Och, 2003). The output of the ensemble is a pseudo-reference that is then TER aligned with the original MT to obtain OK/BAD tags for each word in the MT hypothesis. DFKI (T1): These submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negativ"
W17-4717,W15-3037,0,0.0138062,"n in the source side of the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisatio"
W17-4717,P16-1160,0,0.0330991,"t attention (looking at information anywhere in the source sequence during decoding) and hard monotonic attention (looking at one encoder state at a time from left to right, thus being more conservative and faithful to the original input), which are combined in different ways in the case of multi-source models. The artiﬁcial data provided by JunczysDowmunt and Grundkiewicz (2016) are used to boost performance by increasing the size of the corpus used for training. Univerzita Karlova v Praze. CUNI’s (EN-DE) system is based on the character-to-character neural network architecture described in (Lee et al., 2016). This architecture was compared with the standard neural network architecture proposed by Bahdanau et al. (2014) which uses byte-pair encoding (Sennrich et al., 2015) for generating translation tokens. During the experiments, two setups have been compared for each architecture: i) a single encoder with SRC and MT sentences concatenated, and ii) a two-encoder system, where each SRC and MT sentence is fed to a separate encoder. The submitted system uses the two-encoder architecture with a character-level encoder and decoder. The initial state of the decoder is a weighted combination of the ﬁnal"
W17-4717,W17-4733,0,0.0368107,"Missing"
W17-4717,W17-4765,1,0.786172,"Missing"
W17-4717,L16-1582,1,0.768758,"T 183 translations labelled with task-speciﬁc labels. Participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same fo"
W17-4717,C16-1241,0,0.0353857,"Missing"
W17-4717,W14-3342,0,0.0181229,"lity estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a sentence), the length of a sentence might inﬂuence the probability of a word being wrong. • Target token, its left and right contexts of 1 word. • Source word aligne"
W17-4717,P16-2046,0,0.0171489,"Missing"
W17-4717,W16-2379,0,0.0228966,"Missing"
W17-4717,P02-1040,0,0.119222,"Missing"
W17-4717,W16-2389,0,0.038207,"Missing"
W17-4717,W17-4736,0,0.0268357,"Missing"
W17-4717,W17-4737,0,0.0383776,"Missing"
W17-4717,W17-4738,0,0.0427074,"Missing"
W17-4717,W14-3301,1,0.655758,"Missing"
W17-4717,W16-2391,1,0.761669,"target sentences into sequences of character embeddings, and then passes them through a series of deep parallel stacked convolution/max pooling layers. The baseline features are provided through a multi-layer perceptron, 187 and then concatenated with the characterlevel information. Finally, the concatenation is passed onto another multi-layer perceptron and the very last layer outputs HTER values. The two submissions differ in the the use of standard (CNN+BASE-Single) and multi-task learning (CNN+BASE-Multi) for training. The QUEST-EMB submission follows the word embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained"
W17-4717,W16-2323,1,0.349233,"theses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target information in order to increase robustness and precision of the automatic corrections. The n-best hypotheses produced by 200 this ensemble are further re-ranked using features based on the edit distance between the original MT output and each APE hypothesis, as well as other statistical models (n-gram language model and operation sequence mod"
W17-4717,P16-1159,0,0.0156896,"different input factors, designed to jointly tackle both the APE task and the Word-Level QE task. Word-Level features which have proven effective for QE, such as word-alignments, partof-speech tags, and dependency labels, are included as input factors to neural machine translation systems, which are trained to output PostEdited MT hypotheses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target inf"
W17-4717,2006.amta-papers.25,0,0.817721,"trast to last year, we also provide datasets for two language pairs. The structure used for the data have been the same since WMT15. Each data instance consists of (i) a source sentence, (ii) its automatic translation into the target language, (iii) the manually post-edited version of the automatic translation, (iv) a free reference translation of the source sentence. Post-edits are used to extract labels for the 4.4 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the proportion of their words that need to be ﬁxed. HTER (Snover et al., 2006b) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version. Labels HTER labels were computed using the TERCOM tool16 with default settings (tokenised, case insensitive, exact matching only), with scores capped to 1. 188 16 http://www.cs.umd.edu/˜snover/tercom/ Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: with an edit operation: insertion, deletion, substitution or no edit (correct word). We mark each edited word as BAD, and the remainingn as OK. • Scoring: Pears"
W17-4717,W17-4756,0,0.0529328,"Missing"
W17-4717,P15-4020,1,0.738308,"Missing"
W17-4717,P13-4014,1,0.634274,"Missing"
W17-4717,W17-4720,1,0.830567,"Missing"
W17-4717,W17-4776,0,0.0596806,"Missing"
W17-4717,W17-4777,1,0.832756,"Missing"
W17-4717,W17-4742,0,0.0342495,"Missing"
W17-4717,W17-4744,0,0.0607458,"Missing"
W17-4717,C00-2137,0,0.0420443,"ch edited word as BAD, and the remainingn as OK. • Scoring: Pearson’s r correlation score (primary metric, ofﬁcial score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). Evaluation Analogously to the last year’s task, the primary evaluation metric is the multiplication of F1 -scores for the OK and BAD classes, denoted as F1 -mult. Unlike previously used F1 BAD score this metric is not biased towards “pessimistic” labellings. We also report F1 -scores for individual classes for completeness. We test the signiﬁcance of the results using randomisation tests (Yeh, 2000) with Bonferroni correction (Abdi, 2007). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical signiﬁcance on Pearson r was computed using the William’s test.17 Results Tables 13 and 14 summarise the results for Task 1 on German–English and English– German datasets, respectively, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems for the ranking variant. The top three systems are the same for both datasets, and the ranking of systems according to their performance is similar for"
W17-4717,W17-4745,1,0.825998,"Missing"
W17-4717,P16-1147,0,0.0102821,"Missing"
W17-4745,1983.tc-1.13,0,0.597045,"Missing"
W17-4745,Q16-1027,0,0.0421022,"network, we use the linear associative unit (LAU) to ensure fluent gradient propagation. The LAU is computed as rt = σ(Wxr xt + Whr ht−1 ), zt = σ(Wxz xt + Whz ht−1 ), gt = σ(Wsg xt + Whg ht−1 ), e ht = tanh((1 − rt ) Wxh xt + Whh (rt ht−1 )), ht = ((1 − zt ) ht−1 + zt e ht ) (1 − gt ) + gt (6) where W∗ is the weight matrices, xt is the input at time t and ht−1 is the hidden states at time t − 1. The LAU allows the input linearly forward propagates in a certain scale to acquire fluent gradient back propagation. It works like residual connections (He et al., 2016) and fast-forward connections (Zhou et al., 2016) and makes build deep network possible. Our encoder is a 4 layers LAU network where forward LAU and backward LAU are alternately stacked. The general architecture of our systems is shown in Figure 1. 3 Data Preprocessing For English ↔ Chinese news translation task, WMT 2017 provides tree parts of data: News Commentary v12, UN Parallel Corpus V1.0 and CWMT Corpus. We used all corpora to train our translation systems. For English sentences, the Moses tokenization script2 is employed to execute the tokenization processing. For Chinese sentences, we used our in-house word segmentor called ”PBCLAS”"
W17-4745,W14-4012,0,0.103666,"Missing"
W17-4745,D14-1179,0,0.0435451,"Missing"
W17-4745,D13-1176,0,0.0309473,"the attention mechanism. We employ the Gated Recurrent Unit (GRU) with the linear associative connection to build deep encoder and address the unknown words with the dictionary replace approach. The dictionaries are extracted from the parallel training data with unsupervised word alignment method. In the decoding procedure, the translation probabilities of the target word from different models are averagely combined as the ensemble strategy. In this paper, we introduce our systems from data preprocessing to post-editing in details. 1 2 System Description The neural machine translation model (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014) aims to capture the translation knowledge through training a neural network in the end-to-end style. Our systems are built on the RNNSearch neural machine translation model. Formally, given a source sentence x = x1 , ..., xm and a target sentence y = y1 , ..., yn , NMT models the translation probability as P (y|x) = n Y t=1 P (yt |y&lt;t , x), (1) where y&lt;t = y1 , ..., yt−1 . The generation probability of yt is Introduction P (yt |y&lt;t , x) = g(yt−1 , ct , st ), We build the Neural Machine Translation systems CASICT-DCU for WMT17 English ↔ Chinese news"
W17-4745,P16-1162,0,0.0668449,"onference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 428–431 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics (4) Encoder h1 y1 h2 y2 hm yn ... Softmax 3.1 Ct ... St  1 y t 1 Attention ... ... S~t 1 Decoder ... ... &lt;s&gt; x1 x2 ... y1 ... yn-1 xm Figure 1: The general architecture of our systems. where f (·) is a recurrent function. We adopt a varietal attention mechanism1 in our system which is implemented as e st = f1 (st−1 , yt−1 ), αt,j = align(e st , hj ), Vocabulary Our systems are based on the words rather than sub-words (Sennrich et al., 2016; Wu et al., 2016). For our system is serially trained on the single GPU with restricted memory space, the source vocabulary size is set to 100,000 and the target vocabulary size is set to 50,000. The words that out of the vocabulary are represented by the ”UNK” symbol. where f1 (·) and f2 (·) are recurrent functions. To construct deep network, we use the linear associative unit (LAU) to ensure fluent gradient propagation. The LAU is computed as rt = σ(Wxr xt + Whr ht−1 ), zt = σ(Wxz xt + Whz ht−1 ), gt = σ(Wsg xt + Whg ht−1 ), e ht = tanh((1 − rt ) Wxh xt + Whh (rt ht−1 )), ht = ((1 − zt ) ht"
W17-4745,P17-1013,1,0.836524,"1 , ..., yn , NMT models the translation probability as P (y|x) = n Y t=1 P (yt |y&lt;t , x), (1) where y&lt;t = y1 , ..., yt−1 . The generation probability of yt is Introduction P (yt |y&lt;t , x) = g(yt−1 , ct , st ), We build the Neural Machine Translation systems CASICT-DCU for WMT17 English ↔ Chinese news translation task. Our systems are based on the encoder-decoder model with the attention mechanism, which is also known as the RNNSearch model (Bahdanau et al., 2015). To construct the deep RNN network, we employ the Gated Recurrent Unit (Cho et al., 2014b) with the linear associative connection (Wang et al., 2017) to ensure the fluent gradient propagation. Adadelta (Zeiler, 2012) algorithm is used to optimize the parameters and stochastic gradient descent algorithm with small learning rate is used in the fine-tuning stage. We extract dictionaries from parallel training data with the unsupervised method to address the unknown words in target translation according to the word alignment vector. During the decoding, the ensemble strategy is (2) where g(·) is a softmax regression function, yt−1 is the newly translated target word and st is the hidden states of decoder which represents the translation status"
W17-4747,W16-2360,0,0.0145735,"ce, and a model is trained to translate sequences in the source language into corresponding sequences in the target language while taking the image into consideration. We use the three models introduced in Calixto et al. (2017b), which integrate global image features extracted using a pre-trained convolutional neural network into NMT (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state. We are inspired by the recent success of multimodal NMT models applied to the translation of image descriptions (Huang et al., 2016; Calixto 1 http://www.robots.ox.ac.uk/˜vgg/ research/very_deep/ 440 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 440–444 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics 2.1 IMG2W : Image as source words where Wf and Wb are multi-modal projection matrices that project the image features d into the encoder forward and backward hidden states dimensionality, respectively, and bf and bb are bias → − ← − vectors. h init and h init are directly used as the forward and backward RNN initial hidden states, res"
W17-4747,P07-2045,0,0.00723246,"lish sentence and its gold-standard translations into German and into French. Lang. Model en–de en–de NMT baseline Ensemble en–fr en–fr NMT baseline Ensemble BLEU4 ↑ METEOR↑ TER↓ 18.7 26.4 (↑ 7.7) 37.6 46.8 (↑ 9.2) 66.1 54.5 (↓ 11.6) 35.1 44.5 (↑ 9.4) 55.8 64.1 (↑ 8.3) 45.8 35.2 (↓ 10.6) Table 2: Results for the MSCOCO 2017 English– German and English–French test sets. All models are trained on the original M30kT training data. Ensemble uses four multi-modal models, all trained independently: two models IMGD , one model IMGE , and one model IMG2W . We use the scripts in the Moses SMT Toolkit (Koehn et al., 2007) to normalise, lowercase and tokenize English, German and French descriptions and we also convert space-separated tokens into subwords (Sennrich et al., 2016). The subword models are trained jointly for English–German descriptions and separately for English–French descriptions using the English-German and EnglishFrench WMT 2015 data (Bojar et al., 2015). English–German models have a final vocabulary of 74K English and 81K German subword tokens, and English–French models 82K English and 82K French subword tokens. If sentences in English, German or French are longer than 80 tokens, they are disc"
W17-4747,P17-1175,1,0.898506,"GRU) (Cho et al., 2014) is used as the encoder. The final annotation vector for a given source position i is the concatenation of forward and back→ − ← − ward RNN hidden states, hi = hi ; hi . We use the publicly available pre-trained convolution neural network VGG191 of Simonyan and Zisserman (2014) to extract global image feature vectors for all images. These features are the 4096D activations of the penultimate fullyconnected layer FC7, henceforth referred to as q. We now describe the three multi-modal NMT models used in our experiments. For a detailed explanation about these models, see Calixto et al. (2017b). Introduction In this paper we report on our application of three different multi-modal neural machine translation (NMT) systems to translate image descriptions. We use encoder–decoder attentive multi-modal NMT models where each training example consists of one source variable-length sequence, one image, and one target variable-length sequence, and a model is trained to translate sequences in the source language into corresponding sequences in the target language while taking the image into consideration. We use the three models introduced in Calixto et al. (2017b), which integrate global i"
W17-4747,D17-1105,1,0.914387,"GRU) (Cho et al., 2014) is used as the encoder. The final annotation vector for a given source position i is the concatenation of forward and back→ − ← − ward RNN hidden states, hi = hi ; hi . We use the publicly available pre-trained convolution neural network VGG191 of Simonyan and Zisserman (2014) to extract global image feature vectors for all images. These features are the 4096D activations of the penultimate fullyconnected layer FC7, henceforth referred to as q. We now describe the three multi-modal NMT models used in our experiments. For a detailed explanation about these models, see Calixto et al. (2017b). Introduction In this paper we report on our application of three different multi-modal neural machine translation (NMT) systems to translate image descriptions. We use encoder–decoder attentive multi-modal NMT models where each training example consists of one source variable-length sequence, one image, and one target variable-length sequence, and a model is trained to translate sequences in the source language into corresponding sequences in the target language while taking the image into consideration. We use the three models introduced in Calixto et al. (2017b), which integrate global i"
W17-4747,P02-1040,0,0.100648,"for the English– German and third best for the English–French lanFinally, we use the 29K entries in the M30kT training set for training our models, and the 1, 014 entries in the M30kT development set for model selection, early stopping the training procedure in case the model stops improving BLEU scores on this development set. We evaluate our English–German models on three held-out test sets, the Multi30k 2016/2017 and the MSCOCO 2017 test sets, and our English–French models on the Multi30k 2017 and the MSCOCO 2017 test sets. We evaluate translation quality quantitatively in terms of BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and TER (Snover et al., 2006). 442 Multi30k 2016 (English→German) Ensemble? NMTSRC+IMG 1 IMGD IMGD + IMGE IMGD + IMGE + IMG2W IMGD + IMGE + IMG2W + IMGD × × BLEU4 ↑ METEOR↑ TER↓ 39.0 37.3 56.8 55.1 40.6 42.8 40.1 (↑ 1.1) 41.0 (↑ 2.0) 41.3 (↑ 2.3) 58.5 (↑ 1.7) 58.9 (↑ 2.1) 59.2 (↑ 2.4) 40.7 (↑ 0.1) 39.7 (↓ 0.9) 39.5 (↓ 1.1) 1 This model is pre-trained on the English–German WMT 2015 (Bojar et al., 2015), consisting of ∼4.3M sentence pairs. Table 3: Results for the best model of Calixto et al. (2017a), which is pre-trained on the English–German WMT 2015 (Boja"
W17-4747,D14-1179,0,0.0152562,"Missing"
W17-4747,W14-3348,0,0.0489481,"rd best for the English–French lanFinally, we use the 29K entries in the M30kT training set for training our models, and the 1, 014 entries in the M30kT development set for model selection, early stopping the training procedure in case the model stops improving BLEU scores on this development set. We evaluate our English–German models on three held-out test sets, the Multi30k 2016/2017 and the MSCOCO 2017 test sets, and our English–French models on the Multi30k 2017 and the MSCOCO 2017 test sets. We evaluate translation quality quantitatively in terms of BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and TER (Snover et al., 2006). 442 Multi30k 2016 (English→German) Ensemble? NMTSRC+IMG 1 IMGD IMGD + IMGE IMGD + IMGE + IMG2W IMGD + IMGE + IMG2W + IMGD × × BLEU4 ↑ METEOR↑ TER↓ 39.0 37.3 56.8 55.1 40.6 42.8 40.1 (↑ 1.1) 41.0 (↑ 2.0) 41.3 (↑ 2.3) 58.5 (↑ 1.7) 58.9 (↑ 2.1) 59.2 (↑ 2.4) 40.7 (↑ 0.1) 39.7 (↓ 0.9) 39.5 (↓ 1.1) 1 This model is pre-trained on the English–German WMT 2015 (Bojar et al., 2015), consisting of ∼4.3M sentence pairs. Table 3: Results for the best model of Calixto et al. (2017a), which is pre-trained on the English–German WMT 2015 (Bojar et al., 2015), and different combi"
W17-4747,2006.amta-papers.25,0,0.12588,"ally, we use the 29K entries in the M30kT training set for training our models, and the 1, 014 entries in the M30kT development set for model selection, early stopping the training procedure in case the model stops improving BLEU scores on this development set. We evaluate our English–German models on three held-out test sets, the Multi30k 2016/2017 and the MSCOCO 2017 test sets, and our English–French models on the Multi30k 2017 and the MSCOCO 2017 test sets. We evaluate translation quality quantitatively in terms of BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and TER (Snover et al., 2006). 442 Multi30k 2016 (English→German) Ensemble? NMTSRC+IMG 1 IMGD IMGD + IMGE IMGD + IMGE + IMG2W IMGD + IMGE + IMG2W + IMGD × × BLEU4 ↑ METEOR↑ TER↓ 39.0 37.3 56.8 55.1 40.6 42.8 40.1 (↑ 1.1) 41.0 (↑ 2.0) 41.3 (↑ 2.3) 58.5 (↑ 1.7) 58.9 (↑ 2.1) 59.2 (↑ 2.4) 40.7 (↑ 0.1) 39.7 (↓ 0.9) 39.5 (↓ 1.1) 1 This model is pre-trained on the English–German WMT 2015 (Bojar et al., 2015), consisting of ∼4.3M sentence pairs. Table 3: Results for the best model of Calixto et al. (2017a), which is pre-trained on the English–German WMT 2015 (Bojar et al., 2015), and different combinations of multi-modal models,"
W17-4747,W16-3210,0,0.100775,"Missing"
W17-4768,W16-2302,1,0.654182,"Missing"
W17-4768,W07-0718,0,0.0185925,"h correlation with human assessment. DPMFcomb includes default metrics provided by Asiya MT evaluation toolkit (Gim´enez and M`arquez, 2010), as well as three other metrics, namely ENTF (Yu et al., 2015c), REDp (Yu et al., 2014) and DPMF (Yu et al., 2015b). Over the past two years of WMT metrics tasks, DPMFcomb has achieved the best performance for evaluation of MT of to-English language pairs. Therefore, human RR only provides relative differences in quality of a given 5 hypotheses rather than the overall absolute quality of hypotheses. Besides, the low inter-annotator agreement level in RR (Callison-Burch et al., 2007) has been a longlasting issue in MT human evaluation. The ability and the reliability of RR raise our concern whether the capability of the model trained with RR as the golden standard may be limited. Fortunately, a new emerged evaluation approach, direct assessment (DA) (Graham et al., 2013), has been proven more reliable for evaluation of metrics and was recently adopted as the official human evaluation in WMT17. DA produces absolute quality scores of hypotheses, by measuring to what extend the hypothesis adequately expresses the meaning of the reference translation, through a 1-100 continuo"
W17-4768,P08-1007,0,0.0233044,"Technology, University of Chinese Academy of Sciences 2 ADAPT Centre, School of Computing, Dublin City University maqingsong@ict.ac.cn, graham.yvette@gmail.com wangshugen@ict.ac.cn, qun.liu@dcu.ie Abstract Meteor (Denkowski and Lavie, 2014) and GTM (Melamed et al., 2003) catch different kinds of matches, ROUGE (Lin and Och, 2004) captures common subsequences, WER (Nießen et al., 2000), PER (Tillmann et al., 1997) and TER (Snover et al., 2009) compute the post-editing distance between the hypothesis and the reference translation. Syntactic based metrics mainly use shallow syntactic structures (Chan and Ng, 2008; Zhu et al., 2010), dependency tree structures or constituent tree structures (Owczarzak et al., 2007; Liu and Gildea, 2005). Semantic measures (Lo et al., 2012) and discourse similarity based metrics (Guzm´an et al., 2014) have also been proposed. Different metrics evaluate similarity between hypotheses and reference translations from various perspectives, each of which has pros and cons. One straightforward and effective method to take advantage of the merits of existing metrics is to combine quality scores assigned by these metrics, like DPMFcomb (Yu et al., 2015a). In WMT15 and WMT16 Metr"
W17-4768,N15-1124,1,0.323772,"usions in section 4. 2 2.2 Although RR reflects the quality of hypotheses to some extent, it has two obvious defects. Firstly, RR provides relative ranks of the given competing MT hypotheses, which only reflects relative differences in quality rather than the absolute quality of hypotheses. On the other hand, RR suffers from low inter-annotator agreement levels. As a result, the capability of the model trained with RR as the golden standard could be limited. However, DA with carefully design of criteria (Graham et al., 2013) produces highly reliable overall quality scores for each hypothesis (Graham et al., 2015). In addition, since DA has replaced RR as the official human evaluation in the news domain in WMT17, more DA data would become available in the coming years. These motivate our new combined metric, specially designed based on DA, rather than RR, named as Blend, which means it is a metric that can blend advantages of arbitrary metrics in a combined metric that has a high correlation with human assessment. Our metric follows the basic formulation of DPMFcomb. However, since DA is an absolute quality judgment, which is different from RR, the Metrics 2.1 Review of DPMFcomb DPMFcomb utilizes human"
W17-4768,W13-2305,1,0.922921,"s, DPMFcomb has achieved the best performance for evaluation of MT of to-English language pairs. Therefore, human RR only provides relative differences in quality of a given 5 hypotheses rather than the overall absolute quality of hypotheses. Besides, the low inter-annotator agreement level in RR (Callison-Burch et al., 2007) has been a longlasting issue in MT human evaluation. The ability and the reliability of RR raise our concern whether the capability of the model trained with RR as the golden standard may be limited. Fortunately, a new emerged evaluation approach, direct assessment (DA) (Graham et al., 2013), has been proven more reliable for evaluation of metrics and was recently adopted as the official human evaluation in WMT17. DA produces absolute quality scores of hypotheses, by measuring to what extend the hypothesis adequately expresses the meaning of the reference translation, through a 1-100 continuous rating scale that facilitates reliable quality control of crowd-sourcing. Large numbers of repeat human assessments per translation are standardized and then combined into a mean score as the final quality score of the MT hypothesis. The recent development in human evaluation of MT motivat"
W17-4768,P04-1077,0,0.0500749,"Missing"
W17-4768,W16-2342,0,0.100343,"6 .637 .640 Table 5: Segment-level Pearson correlation of Blend.lex incorporating 4 other metrics for to-English language pairs on WMT16, where “avg” denotes the average Pearson correlation of all language pairs. 3.3 Trade-off between Performance and Efficiency Blend.all, and even that of Blend.lex. Since syntactic and semantic based metrics are usually complex, and the performance of Blend.lex is comparable with that of Blend.all, Blend can operate effectively with only incorporating the default lexical based metrics from Asiya toolkit. We further add 4 other metrics to Blend.lex., CharacTer(Wang et al., 2016), a novel characterbased metric; BEER(Stanojevi´c and Sima’an, 2015), a metric combining different kinds of features; DPMF and ENTF, which proved to be effective. All of these 4 metrics are convenient to use. Table 5 shows Blend.lex+4 (.640) achieves better performance than that of Blend.lex (.632), and is very close to that of Blend.all (.641) as shown in Table 3. Hence, we submit Blend.lex+4 to WMT17 Metrics task for to-English language pairs, since it provides a good trade-off between performance and efficiency for Blend. It is convenient for Blend to combine arbitrary metrics in order to a"
W17-4768,W05-0904,0,0.0373706,"ong@ict.ac.cn, graham.yvette@gmail.com wangshugen@ict.ac.cn, qun.liu@dcu.ie Abstract Meteor (Denkowski and Lavie, 2014) and GTM (Melamed et al., 2003) catch different kinds of matches, ROUGE (Lin and Och, 2004) captures common subsequences, WER (Nießen et al., 2000), PER (Tillmann et al., 1997) and TER (Snover et al., 2009) compute the post-editing distance between the hypothesis and the reference translation. Syntactic based metrics mainly use shallow syntactic structures (Chan and Ng, 2008; Zhu et al., 2010), dependency tree structures or constituent tree structures (Owczarzak et al., 2007; Liu and Gildea, 2005). Semantic measures (Lo et al., 2012) and discourse similarity based metrics (Guzm´an et al., 2014) have also been proposed. Different metrics evaluate similarity between hypotheses and reference translations from various perspectives, each of which has pros and cons. One straightforward and effective method to take advantage of the merits of existing metrics is to combine quality scores assigned by these metrics, like DPMFcomb (Yu et al., 2015a). In WMT15 and WMT16 Metrics tasks, DPMFcomb was the best metric on average for toEnglish language pairs (Stanojevi´c et al., 2015; Bojar et al., 2016"
W17-4768,W15-3053,1,0.944823,"ow syntactic structures (Chan and Ng, 2008; Zhu et al., 2010), dependency tree structures or constituent tree structures (Owczarzak et al., 2007; Liu and Gildea, 2005). Semantic measures (Lo et al., 2012) and discourse similarity based metrics (Guzm´an et al., 2014) have also been proposed. Different metrics evaluate similarity between hypotheses and reference translations from various perspectives, each of which has pros and cons. One straightforward and effective method to take advantage of the merits of existing metrics is to combine quality scores assigned by these metrics, like DPMFcomb (Yu et al., 2015a). In WMT15 and WMT16 Metrics tasks, DPMFcomb was the best metric on average for toEnglish language pairs (Stanojevi´c et al., 2015; Bojar et al., 2016). DPMFcomb incorporates lexical, syntactic and semantic based metrics, using ranking SVM1 to train parameters of each metric score and achieves a high correlation with human evaluation. Human evaluations in terms of relative ranking (RR) accumulated in WMT Metrics tasks are adopted to generate training data and to guide the training process. Human relative ranking is carried out by ranking the quality of 5 MT hypotheses of the same source segm"
W17-4768,W12-3129,0,0.0494463,"gshugen@ict.ac.cn, qun.liu@dcu.ie Abstract Meteor (Denkowski and Lavie, 2014) and GTM (Melamed et al., 2003) catch different kinds of matches, ROUGE (Lin and Och, 2004) captures common subsequences, WER (Nießen et al., 2000), PER (Tillmann et al., 1997) and TER (Snover et al., 2009) compute the post-editing distance between the hypothesis and the reference translation. Syntactic based metrics mainly use shallow syntactic structures (Chan and Ng, 2008; Zhu et al., 2010), dependency tree structures or constituent tree structures (Owczarzak et al., 2007; Liu and Gildea, 2005). Semantic measures (Lo et al., 2012) and discourse similarity based metrics (Guzm´an et al., 2014) have also been proposed. Different metrics evaluate similarity between hypotheses and reference translations from various perspectives, each of which has pros and cons. One straightforward and effective method to take advantage of the merits of existing metrics is to combine quality scores assigned by these metrics, like DPMFcomb (Yu et al., 2015a). In WMT15 and WMT16 Metrics tasks, DPMFcomb was the best metric on average for toEnglish language pairs (Stanojevi´c et al., 2015; Bojar et al., 2016). DPMFcomb incorporates lexical, syn"
W17-4768,N03-2021,0,0.0245431,"Missing"
W17-4768,C14-1193,1,0.846426,"porated as features in the form of metric scores attributed to the same hypotheses, with relative ranks as the gold standard to guide SVM-rank to learn parameters for features. When testing, the predicted ranking scores produced by DPMFcomb reflect the quality of hypotheses. DPMFcomb allows the combination of the advantages of a set of arbitrary metrics resulting in a metric with a high correlation with human assessment. DPMFcomb includes default metrics provided by Asiya MT evaluation toolkit (Gim´enez and M`arquez, 2010), as well as three other metrics, namely ENTF (Yu et al., 2015c), REDp (Yu et al., 2014) and DPMF (Yu et al., 2015b). Over the past two years of WMT metrics tasks, DPMFcomb has achieved the best performance for evaluation of MT of to-English language pairs. Therefore, human RR only provides relative differences in quality of a given 5 hypotheses rather than the overall absolute quality of hypotheses. Besides, the low inter-annotator agreement level in RR (Callison-Burch et al., 2007) has been a longlasting issue in MT human evaluation. The ability and the reliability of RR raise our concern whether the capability of the model trained with RR as the golden standard may be limited."
W17-4768,niessen-etal-2000-evaluation,0,0.0405302,": a Novel Combined MT Metric Based on Direct Assessment — CASICT-DCU submission to WMT17 Metrics Task Qingsong Ma1 Yvette Graham2 Shugen Wang1 Qun Liu2,1 1 Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, University of Chinese Academy of Sciences 2 ADAPT Centre, School of Computing, Dublin City University maqingsong@ict.ac.cn, graham.yvette@gmail.com wangshugen@ict.ac.cn, qun.liu@dcu.ie Abstract Meteor (Denkowski and Lavie, 2014) and GTM (Melamed et al., 2003) catch different kinds of matches, ROUGE (Lin and Och, 2004) captures common subsequences, WER (Nießen et al., 2000), PER (Tillmann et al., 1997) and TER (Snover et al., 2009) compute the post-editing distance between the hypothesis and the reference translation. Syntactic based metrics mainly use shallow syntactic structures (Chan and Ng, 2008; Zhu et al., 2010), dependency tree structures or constituent tree structures (Owczarzak et al., 2007; Liu and Gildea, 2005). Semantic measures (Lo et al., 2012) and discourse similarity based metrics (Guzm´an et al., 2014) have also been proposed. Different metrics evaluate similarity between hypotheses and reference translations from various perspectives, each of w"
W17-4768,C10-2175,0,0.0203279,"ity of Chinese Academy of Sciences 2 ADAPT Centre, School of Computing, Dublin City University maqingsong@ict.ac.cn, graham.yvette@gmail.com wangshugen@ict.ac.cn, qun.liu@dcu.ie Abstract Meteor (Denkowski and Lavie, 2014) and GTM (Melamed et al., 2003) catch different kinds of matches, ROUGE (Lin and Och, 2004) captures common subsequences, WER (Nießen et al., 2000), PER (Tillmann et al., 1997) and TER (Snover et al., 2009) compute the post-editing distance between the hypothesis and the reference translation. Syntactic based metrics mainly use shallow syntactic structures (Chan and Ng, 2008; Zhu et al., 2010), dependency tree structures or constituent tree structures (Owczarzak et al., 2007; Liu and Gildea, 2005). Semantic measures (Lo et al., 2012) and discourse similarity based metrics (Guzm´an et al., 2014) have also been proposed. Different metrics evaluate similarity between hypotheses and reference translations from various perspectives, each of which has pros and cons. One straightforward and effective method to take advantage of the merits of existing metrics is to combine quality scores assigned by these metrics, like DPMFcomb (Yu et al., 2015a). In WMT15 and WMT16 Metrics tasks, DPMFcomb"
W17-4768,P02-1040,0,0.116198,"on of metrics incorporated in Blend, in order to find a trade-off between performance and efficiency. 1 Introduction Automatic machine translation evaluation (AMTE) has received much attention in recent years, with the aim of providing quick and stable measurements of the performance of machine translation (MT) systems. Various metrics for AMTE have been proposed and most operate via computation of the similarity between the MT hypothesis and the reference translation. However, different metrics focus on different perspectives in terms of measuring similarity. For lexical based metrics, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) count n-gram co-occurrence, 1 598 http://www.cs.cornell.edu/People/tj/svm light/svm rank.html Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 598–603 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics WMT15 WMT16 cs-en 500 560 de-en 500 560 fi-en 500 560 ro-en − 560 ru-en 500 560 tr-en − 560 en-ru 500 560 Table 1: The number of sampled DA data for each language pair in WMT15 and WMT16. cess, metrics are incorporated as features in the form of metric scores attributed to the same"
W17-4768,W09-0441,0,0.023715,"ASICT-DCU submission to WMT17 Metrics Task Qingsong Ma1 Yvette Graham2 Shugen Wang1 Qun Liu2,1 1 Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, University of Chinese Academy of Sciences 2 ADAPT Centre, School of Computing, Dublin City University maqingsong@ict.ac.cn, graham.yvette@gmail.com wangshugen@ict.ac.cn, qun.liu@dcu.ie Abstract Meteor (Denkowski and Lavie, 2014) and GTM (Melamed et al., 2003) catch different kinds of matches, ROUGE (Lin and Och, 2004) captures common subsequences, WER (Nießen et al., 2000), PER (Tillmann et al., 1997) and TER (Snover et al., 2009) compute the post-editing distance between the hypothesis and the reference translation. Syntactic based metrics mainly use shallow syntactic structures (Chan and Ng, 2008; Zhu et al., 2010), dependency tree structures or constituent tree structures (Owczarzak et al., 2007; Liu and Gildea, 2005). Semantic measures (Lo et al., 2012) and discourse similarity based metrics (Guzm´an et al., 2014) have also been proposed. Different metrics evaluate similarity between hypotheses and reference translations from various perspectives, each of which has pros and cons. One straightforward and effective m"
W17-4768,W15-3031,0,0.0524792,"Missing"
W17-4768,W15-3050,0,0.454035,"Missing"
W17-4768,W14-3348,0,\N,Missing
W18-3405,D14-1179,0,0.059328,"Missing"
W18-3405,P11-2031,0,0.0194503,"nd Ghahramani, 2016), dropout to the encoder bidirectional RNN and decoder RNN using the same mask in all time steps are also applied. The models are trained for 25 epochs using Adam (Kingma and Ba, 2015) with learning rate 0.002 and mini-batches of size 40, where each training instance consists of one English sentence, one Hindi sentence and one image. Finally, we evaluate translation quality quantitatively in terms of BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) and report statistical significance for the metrics using approximate randomisation computed with MultEval (Clark et al., 2011). 7 7.1 BLEU 21.6 22.7 23.3 24.2(↑ 0.9) 23.9 METEOR 29.6 30.2 29.7 30.7(↑ 1) 29.9 Table 5: Evaluation metrics scores Hi-En translation systems before and after applying the image features on manually curated dev data. Bold numbers indicate that improvements are statistically significant compared to NMTtext with p = 0.05 Results and Analysis Figure 2: Example from the Flickr30k dataset Quantitative Analysis We develop the following five systems - We see from the results that the text-only NMT model outperforms phrase based SMT model in terms of BLEU score. Our results indicate that incorporatin"
W18-3405,abdelali-etal-2014-amara,0,0.0191814,"and Enr (M anlT rans) into Hindi while the other speaker verified the same. The examples of manually translated descriptions are shown in Table 4. • Finally, we translate Hir (Manl.Test.Trans.) into English and measure the performance with reference to Enr (Manl.Test.Trans.) Data Hic − Enc : In order to generate the synthetic data by means of back-translation, we use the general domain IITB English-Hindi Corpus to train a PBSMT system. The corpus is a compilation of parallel corpora collected from a various existing sources such as OPUS (Tiedemann, 2012), HindEn (Bojar et al., 2014b) and TED (Abdelali et al., 2014) as well as corpora developed at the Center for Indian Language Technology, IIT-B 1 over the years (Kunchukuttan et al., 2017). Hit (Syn.Trans) : We divide the English descriptions of Flickr30k dataset consisting of 158,915 sentences into training, development and test sets. The training dataset (Ent (M anl.T rans)) contains 1 Images CNN PBSMT System • Visual input may provide right-angled information that is free of the natural language ambiguities and can serve as extraneous information to textual features for machine translation in multimodal scenarios. This motivates us to extract deep vis"
W18-3405,W14-3348,0,0.0112735,"er and decoder RNNs inputs and recurrent connections, and before the readout operation in the decoder RNN was applied. Following (Gal and Ghahramani, 2016), dropout to the encoder bidirectional RNN and decoder RNN using the same mask in all time steps are also applied. The models are trained for 25 epochs using Adam (Kingma and Ba, 2015) with learning rate 0.002 and mini-batches of size 40, where each training instance consists of one English sentence, one Hindi sentence and one image. Finally, we evaluate translation quality quantitatively in terms of BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) and report statistical significance for the metrics using approximate randomisation computed with MultEval (Clark et al., 2011). 7 7.1 BLEU 21.6 22.7 23.3 24.2(↑ 0.9) 23.9 METEOR 29.6 30.2 29.7 30.7(↑ 1) 29.9 Table 5: Evaluation metrics scores Hi-En translation systems before and after applying the image features on manually curated dev data. Bold numbers indicate that improvements are statistically significant compared to NMTtext with p = 0.05 Results and Analysis Figure 2: Example from the Flickr30k dataset Quantitative Analysis We develop the following five systems - We see from the result"
W18-3405,J95-3006,0,0.524107,"n answering (Antol et al., 2015), etc. However, neural machine translation (NMT), which is an inherently data-dependent procedure, continues to be a challenging problem in the context of lowresourced and out-of-domain settings (Koehn and Knowles, 2017). In other words, there is a concern that the model will perform poorly with languages having limited resources, especially in comparison with well-resourced major languages. Although English(En) and Hindi(Hi) languages belong to the same family (IndoEuropean), they differ significantly in terms of word order, syntax and morphological structure (Bharati et al., 1995). While English maintains a Subject-Verb-Object (SVO) template, Hindi follows a Subject-Object-Verb (SOV) convention. Moreover, compared to English, Hindi has a more complex inflection system, where nouns, verbs and adjectives are inflected according to number, gender and case. These issues, combined with the data scarcity problem, makes Hi→En machine translation a challenging task. Bilingual corpora, which are an important component for machine translation systems, suffer from the problem of data scarcity when one of the languages is resource-poor. To achieve better quality translation, a pot"
W18-3405,bojar-etal-2014-hindencorp,0,0.0542774,"e datasets End (M anl.T rans) and Enr (M anlT rans) into Hindi while the other speaker verified the same. The examples of manually translated descriptions are shown in Table 4. • Finally, we translate Hir (Manl.Test.Trans.) into English and measure the performance with reference to Enr (Manl.Test.Trans.) Data Hic − Enc : In order to generate the synthetic data by means of back-translation, we use the general domain IITB English-Hindi Corpus to train a PBSMT system. The corpus is a compilation of parallel corpora collected from a various existing sources such as OPUS (Tiedemann, 2012), HindEn (Bojar et al., 2014b) and TED (Abdelali et al., 2014) as well as corpora developed at the Center for Indian Language Technology, IIT-B 1 over the years (Kunchukuttan et al., 2017). Hit (Syn.Trans) : We divide the English descriptions of Flickr30k dataset consisting of 158,915 sentences into training, development and test sets. The training dataset (Ent (M anl.T rans)) contains 1 Images CNN PBSMT System • Visual input may provide right-angled information that is free of the natural language ambiguities and can serve as extraneous information to textual features for machine translation in multimodal scenarios. Thi"
W18-3405,P17-1175,1,0.926197,"roblem (Cho et al., 2014a). Vinyals et al. (2015) proposed an IDG model that uses a vector, encoding the image as input based on the sequence-to-sequence framework. Specia et al. (2016) introduced a shared task to investigate the role of images in Multi-modal MT. Similarly, Huang et al. (2016) introduced a model to associate textual and visual features extracted with the VGG19 network for translation tasks (Simonyan and Zisserman, 2014). Elliott et al. (2015) generated multilingual image descriptions using image features transferred from separate non-attentive neural image description models. Calixto et al. (2017a) carried out experiments to incorporate spatial visual information into NMT using a separate visual attention mechanism. Although these approaches have demonstrated the plausibility of multilingual natural language processing with multiple modalities, they rely exclusively on the availability of a large three-way parallel corpus (bilingual captions corresponding to the image) as training data. Having enough parallel corpora is a big challenge in NMT and it is very unlikely to have millions of parallel sentences for every language pair. Therefore, quite a few attempts have been made to build"
W18-3405,W16-2360,0,0.101852,"g English-image parallel corpus. We used these datasets to build our image description translation system by adopting state-of-theart MNMT models. Our results show that it is possible to train a MNMT system for low-resource language pairs through the use of synthetic data and that such a system can benefit from image features. 1 Introduction Recent years have witnessed a surge in application of multimodal neural models as a sequence to sequence learning problem (Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b) for solving different tasks such as machine translations (Huang et al., 2016), image and video description generation (Karpathy and Fei-Fei, 2015; 33 Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP, pages 33–42 c Melbourne, Australia July 19, 2018. 2018 Association for Computational Linguistics 2 Related Work We are inspired by the recent successes of using visual inputs for translation tasks (see Section 2 for relevant studies). For translating image descriptions, given both the source image and it’s description, it can be seen that both modalities can bring more useful information for generating the target language description. With the g"
W18-3405,D13-1176,0,0.0282295,"th a synthetic training dataset and a manually curated development/test dataset for Hindi based on an existing English-image parallel corpus. We used these datasets to build our image description translation system by adopting state-of-theart MNMT models. Our results show that it is possible to train a MNMT system for low-resource language pairs through the use of synthetic data and that such a system can benefit from image features. 1 Introduction Recent years have witnessed a surge in application of multimodal neural models as a sequence to sequence learning problem (Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b) for solving different tasks such as machine translations (Huang et al., 2016), image and video description generation (Karpathy and Fei-Fei, 2015; 33 Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP, pages 33–42 c Melbourne, Australia July 19, 2018. 2018 Association for Computational Linguistics 2 Related Work We are inspired by the recent successes of using visual inputs for translation tasks (see Section 2 for relevant studies). For translating image descriptions, given both the source image and it’s description, it can be seen that both modal"
W18-3405,P16-1009,0,0.506611,"al machine translation, we take the first steps towards applying MNMT methods for Hi→En translation. Our contributions in this study are as follows: There has been some previous work on using visual context in tasks involving both neural machine translation (NMT) and image description generation (IDG) that explicitly uses an encoder-decoder framework as an instantiation of the sequence to sequence (seq2seq) learning problem (Cho et al., 2014a). Vinyals et al. (2015) proposed an IDG model that uses a vector, encoding the image as input based on the sequence-to-sequence framework. Specia et al. (2016) introduced a shared task to investigate the role of images in Multi-modal MT. Similarly, Huang et al. (2016) introduced a model to associate textual and visual features extracted with the VGG19 network for translation tasks (Simonyan and Zisserman, 2014). Elliott et al. (2015) generated multilingual image descriptions using image features transferred from separate non-attentive neural image description models. Calixto et al. (2017a) carried out experiments to incorporate spatial visual information into NMT using a separate visual attention mechanism. Although these approaches have demonstrate"
W18-3405,W16-2346,0,0.158796,"Missing"
W18-3405,P17-4012,0,0.078641,"ed. If the sentences in English or Hindi are longer than 80 tokens, they are discarded. To measure the performance of the system, we also translate the Enr testset into Hir both manually and automatically. We also perform Hindi→English (Hi→En) translation using a PBSMT system with the general domain Hic − Enc corpus. We use the News Crawl articles 2016 from the WMT17 4 as additional English monolingual corpora to train the 4-gram language model. This contain roughly 20 million sentence for English. (Table 3). To build our Multi-modal NMT systems we use OpenNMT-py (the pytorch port of OpenNMT (Klein et al., 2017)) following the settings of Calixto et al. (2017b) which implements the encoder as a bi-directional RNN with GRU, one 1024D single-layer forward RNN and one 1024D single-layer backward RNN. Throughout the experiments, the models are parameterised using 620D source and target word embeddings, and both are trained jointly with the model. All non-recurrent matrices are initialised by sampling from a Gaussian distribution (µ = 0, σ = 0.01), reIMGD : Image for decoder initialization A new single-layer feed-forward neural network is used for incorporating an image into the decoder. Originally, the i"
W18-3405,P07-2045,0,0.0106163,"ings similar to that of (Kunchukuttan et al., 2017) to develop Hit . They used the news stories from the WMT 2014 English-Hindi shared task (Bojar et al., 2014a) as the development(dev) and test corpora which we concatenate together to create our dev set. The training and dev corpora consist of 1,492,827 and 3,207 sentence segments respectively. We used the HindMono corpus (Bojar et al., 2014b) which contains roughly 45 million sentences to build our language model in Hindi. The corpus statistics are shown in Table.1 and Table.2. For training the Hic − Enc corpus, we use the Moses SMT system (Koehn et al., 2007) . We use the SRILM toolkit (Stolcke, 2002) for building a language model and GIZA++ (Och and Ney, 2000) with the grow-diag-final-and heuristic for extracting phrases from Hic − Enc .The trained system is tuned using Minimum Error Rate Training (Och, 2003). For other parameters of Moses, default values are used. If the sentences in English or Hindi are longer than 80 tokens, they are discarded. To measure the performance of the system, we also translate the Enr testset into Hir both manually and automatically. We also perform Hindi→English (Hi→En) translation using a PBSMT system with the gene"
W18-3405,W17-3204,0,0.0226208,"Dublin, Ireland Mohammed Hasanuzzaman ADAPT Centre School of Computing Dublin City University Dublin, Ireland Qun Liu ADAPT Centre School of Computing Dublin City University Dublin, Ireland koel.chowdhury@adaptcentre.ie mohammed.hasanuzzaman@adaptcentre.ie qun.liu@adaptcentre.ie Abstract Kiros et al., 2014; Donahue et al., 2015; Venugopalan et al., 2014), visual question answering (Antol et al., 2015), etc. However, neural machine translation (NMT), which is an inherently data-dependent procedure, continues to be a challenging problem in the context of lowresourced and out-of-domain settings (Koehn and Knowles, 2017). In other words, there is a concern that the model will perform poorly with languages having limited resources, especially in comparison with well-resourced major languages. Although English(En) and Hindi(Hi) languages belong to the same family (IndoEuropean), they differ significantly in terms of word order, syntax and morphological structure (Bharati et al., 1995). While English maintains a Subject-Verb-Object (SVO) template, Hindi follows a Subject-Object-Verb (SOV) convention. Moreover, compared to English, Hindi has a more complex inflection system, where nouns, verbs and adjectives are"
W18-3405,tiedemann-2012-parallel,0,0.0308971,"One of them translated the datasets End (M anl.T rans) and Enr (M anlT rans) into Hindi while the other speaker verified the same. The examples of manually translated descriptions are shown in Table 4. • Finally, we translate Hir (Manl.Test.Trans.) into English and measure the performance with reference to Enr (Manl.Test.Trans.) Data Hic − Enc : In order to generate the synthetic data by means of back-translation, we use the general domain IITB English-Hindi Corpus to train a PBSMT system. The corpus is a compilation of parallel corpora collected from a various existing sources such as OPUS (Tiedemann, 2012), HindEn (Bojar et al., 2014b) and TED (Abdelali et al., 2014) as well as corpora developed at the Center for Indian Language Technology, IIT-B 1 over the years (Kunchukuttan et al., 2017). Hit (Syn.Trans) : We divide the English descriptions of Flickr30k dataset consisting of 158,915 sentences into training, development and test sets. The training dataset (Ent (M anl.T rans)) contains 1 Images CNN PBSMT System • Visual input may provide right-angled information that is free of the natural language ambiguities and can serve as extraneous information to textual features for machine translation"
W18-3405,P03-1021,0,0.01385,"ing and dev corpora consist of 1,492,827 and 3,207 sentence segments respectively. We used the HindMono corpus (Bojar et al., 2014b) which contains roughly 45 million sentences to build our language model in Hindi. The corpus statistics are shown in Table.1 and Table.2. For training the Hic − Enc corpus, we use the Moses SMT system (Koehn et al., 2007) . We use the SRILM toolkit (Stolcke, 2002) for building a language model and GIZA++ (Och and Ney, 2000) with the grow-diag-final-and heuristic for extracting phrases from Hic − Enc .The trained system is tuned using Minimum Error Rate Training (Och, 2003). For other parameters of Moses, default values are used. If the sentences in English or Hindi are longer than 80 tokens, they are discarded. To measure the performance of the system, we also translate the Enr testset into Hir both manually and automatically. We also perform Hindi→English (Hi→En) translation using a PBSMT system with the general domain Hic − Enc corpus. We use the News Crawl articles 2016 from the WMT17 4 as additional English monolingual corpora to train the 4-gram language model. This contain roughly 20 million sentence for English. (Table 3). To build our Multi-modal NMT sy"
W18-3405,P02-1040,0,0.101819,"(in all MNMT models), in the encoder and decoder RNNs inputs and recurrent connections, and before the readout operation in the decoder RNN was applied. Following (Gal and Ghahramani, 2016), dropout to the encoder bidirectional RNN and decoder RNN using the same mask in all time steps are also applied. The models are trained for 25 epochs using Adam (Kingma and Ba, 2015) with learning rate 0.002 and mini-batches of size 40, where each training instance consists of one English sentence, one Hindi sentence and one image. Finally, we evaluate translation quality quantitatively in terms of BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) and report statistical significance for the metrics using approximate randomisation computed with MultEval (Clark et al., 2011). 7 7.1 BLEU 21.6 22.7 23.3 24.2(↑ 0.9) 23.9 METEOR 29.6 30.2 29.7 30.7(↑ 1) 29.9 Table 5: Evaluation metrics scores Hi-En translation systems before and after applying the image features on manually curated dev data. Bold numbers indicate that improvements are statistically significant compared to NMTtext with p = 0.05 Results and Analysis Figure 2: Example from the Flickr30k dataset Quantitative Analysis We develop the followin"
W18-3405,D16-1160,0,0.13535,"into NMT using a separate visual attention mechanism. Although these approaches have demonstrated the plausibility of multilingual natural language processing with multiple modalities, they rely exclusively on the availability of a large three-way parallel corpus (bilingual captions corresponding to the image) as training data. Having enough parallel corpora is a big challenge in NMT and it is very unlikely to have millions of parallel sentences for every language pair. Therefore, quite a few attempts have been made to build NMT systems for low-resource language pairs (Sennrich et al., 2016; Zhang and Zong, 2016) which focused on building NMT systems in a low-resource scenario. They incorporated huge monolingual corpus in the source or target side. Gulcehre et al. (2017) proposed two alternative methods to integrate monolingual data on target side, namely shallow fusion and deep fusion. In shallow fusion, the top K hypotheses (produced by NMT) at each time step t are re-scored using the weighted sum of the scores given by the NMT(trained on parallel data) and a recurrent neural network based language model (RNNLM). Whereas in deep fusion, hidden states obtained at each time step t of RNNLM and NMT are"
W18-3405,I08-1067,0,0.0273197,"s in translation quality between the SMT and NMT system, at least in terms of evaluation metrics. These results are not necessarily surprising given that the grammatical syntax between the two languages is poorly represented in the synthetic Hindi training data. In addition to this, Hindi as a language presents many of the well-known issues that NMT currently struggles with (resource sparsity, rich morphology and complex inflection structure). An approach worth considering to address the divergence in word order of the En-Hi language pair is the prereordering approach such as the one taken by Ramanathan et al. (2008) to build stronger baseline systems. We will also investigate if incorporating local, spatial-preserving image features can provide more cues to an NMT model as an extension of this work. English reference: two young children are on sand. Manual Source: दो छोटे ब े रेत पर ह। NMT: two little kids are on the sand. MNMT: two small children are on sand. For this particular example, the overall meaning of the source description has been correctly preserved into the target side description for the outputs generated by both models. However, if we closely look into each of the example, we note the dif"
W18-3405,D16-1163,0,0.0338264,"orpus for the image descriptions translation task, we translate the English descriptions of Flickr30k dataset (referred to as En (Manl.Trans.)) into Hindi, using a PBSMT system. We take motivation for using the PBSMT system over NMT from the work carried out by Kunchukuttan et al. (2017). For Hi →En translation, their system achieves better results with PBSMT over NMT when trained on the same corpus. A few other popular approaches in this area involve using a method called transfer learning which focuses on sharing parameters, such as source side word-embeddings across related language pairs. Zoph et al. (2016) focus on training a model on high resource language pair and then using learned parameters to train the low resource language pair. However, it requires selecting closely related high and low resource language pairs. So this approach might not work if the language pairs are distant. • We divide the En (Manl.Trans.) into training, validation and test set and call these as Ent (Manl.Train.Trans.), End (Manl.Dev.Trans.) and Enr (Manl.Test.Trans.), respectively. We translate the Ent (Manl.Train.Trans) into Hindi using the PBSMT system and call these as Hit (Syn.Train.Trans).We manually translate"
W18-6556,W07-0734,0,0.182448,"Missing"
W18-6556,P17-4012,0,0.04076,"to be more consistent but when viewed over many hundreds of samples this can be dry and repetitive. In most cases the baseline model appears to have Source sequence pub more than £30 5 out of 5 Target sequence star Prices start Table 3: Example pair used for training the additional word generator 3 Results & Discussion Experiments The data set was tokenized using the NLTK port of the moses tokenizer with aggressive hyphen splitting. For each DA a custom start and stop token was added to the source sequence; e.g. name start The Vaults name end The models used were from the OpenNMT-py library (Klein et al., 2017). Our model architecture contains 2 layers of bidirectional recurrent neural networks (RNN) with long short-term memory (LSTM) cells (Hochreiter and Schmidhuber, 1997). We use 500 hidden units for the encoder and decoder layer, and 500 units for the word vectors which are learned jointly across the whole model. We add dropout of 0.3 applied between the LSTM stacks. The models are trained using Adam (Kingma and Ba, 2014) with learning rate 0.001 and learn459 Model Additional words - temperature 1.1 Additional words - temperature 1.0 Additional words - temperature 0.9 Baseline Additional words -"
W18-6556,P16-1094,0,0.0153131,"ystems have followed a rule-based approach (Reiter and Dale, 2000). While robust, these systems are noted to generate repetitive and stilted output, which can 457 Proceedings of The 11th International Natural Language Generation Conference, pages 457–462, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics et al., 2017) and content-introducing text generation (Mou et al., 2016). Other approaches to controllable text generation have focused on more abstract inputs. Language models which generate text about a specific topic, product, person, sentiment (Li et al., 2016; Tang et al., 2016; Fan et al., 2017; Dong et al., 2017). methodology to maximize both the quality of collected utterances as well as their naturalness and variety (Novikova et al., 2016). Wei et al. (2017) note that neural networks learning from highly unaligned datasets have trouble choosing between equally plausible outputs and tend towards short and less meaningful outputs. They suggest that the number of plausible outputs can be decreased by providing additional information to the model. In Table 1 we augment the meaning representation (MR) with a novel dialogue act (DA) containing addit"
W18-6556,P17-1014,0,0.0215579,"ubmission does not rank highly using automated metrics, qualitative investigation of generated utterances suggests the use of additional information in neural network NLG systems to be a promising research direction. 1 Table 1: Utterance generated with a novel dialogue act containing additional words make interacting with rule based systems a tedious experience (Wen et al., 2015). Data driven models using deep neural networks have achieved state-of-the-art results in many NLG tasks/datasets such as RoboCup, Weathergov, SF Hotels/Restaurants and AMR-to-text (Mei et al., 2016; Wen et al., 2016; Konstas et al., 2017). However Sharma et al. (2017) notes that high performance on datasets such as Wen et al. (2015)’s SF Restaurant indicates they no longer pose a sufficient challenge and that the community ought to progress to using larger and more complex datasets. Two new crowd sourced datasets, each containing tens of thousands of examples and focusing on complex sentence structures, have been recently released; WebNLG and E2E (Colin et al., 2016; Novikova et al., 2017). This paper focuses on the E2E dataset which was created using a new Introduction Natural Language Generation (NLG) is a broad field, rangi"
W18-6556,W04-1013,0,0.0229501,"d to the model choosing more conservative outputs. We use values of 0.9 to 1.1, to encourage the generation of a more diverse set of additional words. ing rate decay of 0.5 applied after 8 epochs. The models were trained for 10 epochs and the best performing checkpoint on the development set was chosen. The exploration and choice of hyperparameters was aided by the use of Bayesian hyperparameter optimization platform SigOpt (2014). 4 We report results using automated evaluation metrics; BLEU (Papineni et al., 2002), NIST (Przybocki et al., 2009), METEOR (Lavie and Agarwal, 2007), and ROUGE-L (Lin, 2004). Table 4 shows the performance of the baseline relative to our models using both sample additional words and those extracted from target sentences, these are the gold standard additional words. The baseline model is TGen, a sequence-to-sequence model with attention (Duˇsek and Jurˇc´ıcˇ ek, 2016). The model using extracted additional words performs better in almost all metrics. The poor performance of models using sampled words versus gold standard words highlights an issue with the generation of additional words. These results maintain their relative ranking in the test set as shown in Table"
W18-6556,W04-1000,0,0.458522,"Missing"
W18-6556,N16-1086,0,0.0544563,"and control of outputs. While our submission does not rank highly using automated metrics, qualitative investigation of generated utterances suggests the use of additional information in neural network NLG systems to be a promising research direction. 1 Table 1: Utterance generated with a novel dialogue act containing additional words make interacting with rule based systems a tedious experience (Wen et al., 2015). Data driven models using deep neural networks have achieved state-of-the-art results in many NLG tasks/datasets such as RoboCup, Weathergov, SF Hotels/Restaurants and AMR-to-text (Mei et al., 2016; Wen et al., 2016; Konstas et al., 2017). However Sharma et al. (2017) notes that high performance on datasets such as Wen et al. (2015)’s SF Restaurant indicates they no longer pose a sufficient challenge and that the community ought to progress to using larger and more complex datasets. Two new crowd sourced datasets, each containing tens of thousands of examples and focusing on complex sentence structures, have been recently released; WebNLG and E2E (Colin et al., 2016; Novikova et al., 2017). This paper focuses on the E2E dataset which was created using a new Introduction Natural Language"
W18-6556,C16-1316,0,0.0226439,"ne translation (Sutskever et al., 2014), or require significant abstractive reasoning, as in summarization or datato-text tasks (See and Manning, 2017; Wiseman et al., 2017). Traditionally NLG systems have followed a rule-based approach (Reiter and Dale, 2000). While robust, these systems are noted to generate repetitive and stilted output, which can 457 Proceedings of The 11th International Natural Language Generation Conference, pages 457–462, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics et al., 2017) and content-introducing text generation (Mou et al., 2016). Other approaches to controllable text generation have focused on more abstract inputs. Language models which generate text about a specific topic, product, person, sentiment (Li et al., 2016; Tang et al., 2016; Fan et al., 2017; Dong et al., 2017). methodology to maximize both the quality of collected utterances as well as their naturalness and variety (Novikova et al., 2016). Wei et al. (2017) note that neural networks learning from highly unaligned datasets have trouble choosing between equally plausible outputs and tend towards short and less meaningful outputs. They suggest that the numb"
W18-6556,W17-5525,0,0.0688269,"Missing"
W18-6556,D15-1199,0,0.176335,"Missing"
W18-6556,P02-1040,0,0.102169,"perature. Higher values of temperature lead to more diverse outputs. Temperature values close to 0 lead to the model choosing more conservative outputs. We use values of 0.9 to 1.1, to encourage the generation of a more diverse set of additional words. ing rate decay of 0.5 applied after 8 epochs. The models were trained for 10 epochs and the best performing checkpoint on the development set was chosen. The exploration and choice of hyperparameters was aided by the use of Bayesian hyperparameter optimization platform SigOpt (2014). 4 We report results using automated evaluation metrics; BLEU (Papineni et al., 2002), NIST (Przybocki et al., 2009), METEOR (Lavie and Agarwal, 2007), and ROUGE-L (Lin, 2004). Table 4 shows the performance of the baseline relative to our models using both sample additional words and those extracted from target sentences, these are the gold standard additional words. The baseline model is TGen, a sequence-to-sequence model with attention (Duˇsek and Jurˇc´ıcˇ ek, 2016). The model using extracted additional words performs better in almost all metrics. The poor performance of models using sampled words versus gold standard words highlights an issue with the generation of additio"
W18-6556,D17-1239,0,0.0232316,"w Introduction Natural Language Generation (NLG) is a broad field, ranging from text-to-text translation to experiments in computational poetry (Gatt and Krahmer, 2018). Whether the task is to summarize, translate, or entertain, a core challenge is doing so in a manner that is compatible with human needs and preferences. Formally, NLG systems aim to create utterances from a set of abstract inputs. These inputs can be closely aligned, e.g. machine translation (Sutskever et al., 2014), or require significant abstractive reasoning, as in summarization or datato-text tasks (See and Manning, 2017; Wiseman et al., 2017). Traditionally NLG systems have followed a rule-based approach (Reiter and Dale, 2000). While robust, these systems are noted to generate repetitive and stilted output, which can 457 Proceedings of The 11th International Natural Language Generation Conference, pages 457–462, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics et al., 2017) and content-introducing text generation (Mou et al., 2016). Other approaches to controllable text generation have focused on more abstract inputs. Language models which generate text about a specific topic, product"
W18-6556,W14-3301,0,0.0976834,"Missing"
W18-6556,P17-1099,0,0.125566,"was created using a new Introduction Natural Language Generation (NLG) is a broad field, ranging from text-to-text translation to experiments in computational poetry (Gatt and Krahmer, 2018). Whether the task is to summarize, translate, or entertain, a core challenge is doing so in a manner that is compatible with human needs and preferences. Formally, NLG systems aim to create utterances from a set of abstract inputs. These inputs can be closely aligned, e.g. machine translation (Sutskever et al., 2014), or require significant abstractive reasoning, as in summarization or datato-text tasks (See and Manning, 2017; Wiseman et al., 2017). Traditionally NLG systems have followed a rule-based approach (Reiter and Dale, 2000). While robust, these systems are noted to generate repetitive and stilted output, which can 457 Proceedings of The 11th International Natural Language Generation Conference, pages 457–462, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics et al., 2017) and content-introducing text generation (Mou et al., 2016). Other approaches to controllable text generation have focused on more abstract inputs. Language models which generate text about a"
W19-2307,N19-1234,1,0.789446,"is not straightforward to pass the gradients through the discrete output words of the generator. A latent code based solution for this problem, ARAE, was proposed in Kim et al. (2017), where a latent representation of the text is derived using an autoencoder and the manifold of this representation is learned via adversarial training of a generator. Another version of the ARAE method which proposes updating the encoder based on discriminator loss function was introduced in (Spinks and Moens, 2018). GagnonMarchand et al. (2019) introduced a self-attention based GAN architecture to the ARAE and Haidar et al. (2019) explore a hybrid approach generating both a latent representation and the text itself. multi-lingual embeddings to learn the aligned latent representation of two languages and a GAN that can generate this latent space. Particularly, our contributions are as follows: • We introduce a GAN model, Bilingual-GAN, which can generate parallel sentences in two languages concurrently. • Bilingual-GAN can match the latent distribution of the encoder of an attention based NMT model. • We explore the ability to generate parallel sentences when using only monolingual corpora. 2 2.1 Related Work Latent spa"
W19-2307,2005.mtsummit-papers.11,0,0.355907,"Missing"
W19-2307,Q17-1010,0,0.607023,"sentences from different languages into a shared latent representation and using back-translation (Sennrich et al., 2015a) to provide a pseudo supervision. Lample et al. (2017) use a word by word translation dictionary learned in an unsupervised way (Conneau et al., 2017b) as part of their back-translation along with an adversarial loss to enforce language independence in latent representations. Lample et al. (2018) improves this by removing these two elements and instead use Byte Pair Encoding (BPE) sub-word tokenization (Sennrich et al., 2015b) with joint embeddings learned using FastText (Bojanowski et al., 2017), so that the sentences are embedded in a common space. Artetxe et al. (2017) uses online back translation and cross-lingual embeddings to embed sentences in a shared space. They also decouple the decoder so that one is used per language. 2.2 3 Methodology The Bilingual-GAN comprises of a translation module and a text generation module. The complete architecture is illustrated in Figure 1. Figure 1: The complete architecture for our unsupervised bilingual text generator (Bilingual-GAN) 3.1 Translation Unit The translation system is a sequence-to-sequence model with an encoder and a decoder ext"
W19-2307,P07-2045,0,0.00681297,"ets have been used for our experimentation. The Europarl dataset is part of the WMT 2014 parallel corpora (Koehn, 2005) and contains a little more than 2 millions French-English aligned sentences. The Multi30k dataset is used for image captioning (Elliott et al., 2017) and consists of 29k images and their captions. We only use the French and English paired captions. As preprocessing steps on the Europarl dataset, we removed sentences longer than 20 words and those with a ratio of number of words between translations is bigger than 1.5. Then, we tokenize the sentence using the Moses tokenizer (Koehn et al., 2007). For the Multi30k dataset, we use the supplied tokenized version of the dataset with no further processing. For the BPE experiments, we use the sentencepiece subword tokenizer by Google 1 . Consequentially, the decoder also predicts subword tokens. This results in a common embeddings table for both languages since English and French share the same subwords. The BPE was trained on the training corpora that we created. For the training, validation and test splits, we used 200k, after filtering, randomly chosen sentences from the Europarl dataset for training and 40k sentences for testing. When"
W19-2307,W17-4718,0,0.0150617,"ration. Once the GAN is trained, the generator code can be decoded in either language using the pretrained decoder of the NMT system. 4 Experiments This section presents the different experiments we did, on both translation and bilingual text generation, and the datasets we worked on. 58 4.1 Datasets as vocabulary. The Europarl and the Multi30k datasets have been used for our experimentation. The Europarl dataset is part of the WMT 2014 parallel corpora (Koehn, 2005) and contains a little more than 2 millions French-English aligned sentences. The Multi30k dataset is used for image captioning (Elliott et al., 2017) and consists of 29k images and their captions. We only use the French and English paired captions. As preprocessing steps on the Europarl dataset, we removed sentences longer than 20 words and those with a ratio of number of words between translations is bigger than 1.5. Then, we tokenize the sentence using the Moses tokenizer (Koehn et al., 2007). For the Multi30k dataset, we use the supplied tokenized version of the dataset with no further processing. For the BPE experiments, we use the sentencepiece subword tokenizer by Google 1 . Consequentially, the decoder also predicts subword tokens."
W19-2307,J82-2005,0,0.542013,"Missing"
W19-2307,D17-1230,0,0.19525,"Missing"
W19-2307,P02-1040,0,0.103692,"on and one output layer of one cell with Sigmoid activation. We used Adam with a β1 of 0.5, a β2 of 0.999, and a learning rate of 0.0003 to train the encoder and the decoder whereas we used RMSProp with a learning rate of 0.0005 to train the discriminator. Most of the specifications here were taken from Lample et al. (2017). NTG Unit The Generator and Discriminator are trained using Adam with a a β1 of 0.5, a β2 of 0.999, and a learning rate of 0.0001. 4.3 Quantitative Evaluation Metrics Corpus-level BLEU We use the BLEU-N scores to evaluate the fluency of the generated sentences according to Papineni et al. (2002), BLEU-N = BP · exp( N X wn log(pn )) (3) n=1 where pn is the probability of n-gram and wn = 1 n . The results is described in Table 3. Here, we set BP to 1 as there is no reference length like in machine translation. For the evaluations, we generated 40 000 sentences for the model trained on Europarl and 1 000 on the model trained on Multi30k. Perplexity is also used to evaluate the fluency of the generated sentences. For the perplexity evaluations, we generated 100 000 and 10 000 sentences for the Europarl and the Multi30k datasets respectively. The forward and reverse perplexities of the LM"
W19-2307,N18-5014,0,0.111728,"ging due to the discrete nature of text. Consequently, back-propagation would not be feasible for discrete outputs and it is not straightforward to pass the gradients through the discrete output words of the generator. A latent code based solution for this problem, ARAE, was proposed in Kim et al. (2017), where a latent representation of the text is derived using an autoencoder and the manifold of this representation is learned via adversarial training of a generator. Another version of the ARAE method which proposes updating the encoder based on discriminator loss function was introduced in (Spinks and Moens, 2018). GagnonMarchand et al. (2019) introduced a self-attention based GAN architecture to the ARAE and Haidar et al. (2019) explore a hybrid approach generating both a latent representation and the text itself. multi-lingual embeddings to learn the aligned latent representation of two languages and a GAN that can generate this latent space. Particularly, our contributions are as follows: • We introduce a GAN model, Bilingual-GAN, which can generate parallel sentences in two languages concurrently. • Bilingual-GAN can match the latent distribution of the encoder of an attention based NMT model. • We"
W19-2307,1983.tc-1.13,0,0.240423,"Missing"
W19-2307,N18-1122,0,0.116518,"Missing"
W19-2307,P17-1096,0,0.0997451,"rds in either language. Inspired by this bilingual paradigm, the success of attention based neural machine translation (NMT) and the potential of Generative Adversarial Networks (GANs) for text generation we propose Bilingual-GAN, an agent capable of deriving a shared latent space between two languages, and then generating from that space in either language. 55 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 55–64 c Minneapolis, Minnesota, USA, June 6, 2019. 2019 Association for Computational Linguistics tion (Wu et al., 2017; Yang et al., 2017a), dialogue models (Li et al., 2017), question answering (Yang et al., 2017b), and natural language generation (Gulrajani et al., 2017; Kim et al., 2017). However, applying GAN in NLP is challenging due to the discrete nature of text. Consequently, back-propagation would not be feasible for discrete outputs and it is not straightforward to pass the gradients through the discrete output words of the generator. A latent code based solution for this problem, ARAE, was proposed in Kim et al. (2017), where a latent representation of the text is derived using an autoencoder and the manifold of this"
W19-2307,D18-1549,0,\N,Missing
W19-2307,P16-1162,0,\N,Missing
W19-2307,P16-1009,0,\N,Missing
W19-5420,P16-1162,0,0.218647,"Missing"
W19-5420,tian-etal-2014-um,0,0.0154872,"s 3 Corpus OOD Parallel BT UM-Corpus Wikipedia UFAL EMEA Medline 6 Pubmed Total Parallel Corpora In this section, we present the parallel corpora used to train and evaluate translation models. The statistics of the data used is shown in Table 2. The OOD parallel corpora are collected from a number of sources. In addition to WMT parallel corpora for the news translation task, we also gather data from OPUS.1 For English–Chinese tasks, we also include in-house data. The data generated by back-translating WMT monolingual corpus is named as “BT” data. Data from other sources such as the UM-Corpus (Tian et al., 2014) and Wikipedia are also included. The in-domain data is from WMT biomedical translation shared task website.2 More specifically, the in-domain data are gathered from the following sources (shown in Table 2): EN–FR 66.33M 818K 2.81M 1.09M 55K 613K 71.72M EN–DE 22.28M 24.19M 2.46M 3.04M 1.11M 29K 53.11M Table 2: Corpora statistics in the numbers of sentence pairs after cleaning. • The EMEA corpus (Tiedemann, 2012). The EMEA corpus encompasses biomedical documents from the European Medicines Agency (EMEA). This corpus is a major component of in-domain training data. Figure 1: Data Processing Pipe"
W19-5420,tiedemann-2012-parallel,0,0.352154,"Missing"
Y10-1009,P08-1023,1,0.829903,"Missing"
Y10-1009,D08-1022,1,\N,Missing
Y10-1009,P07-1089,1,\N,Missing
Y10-1009,P06-1077,1,\N,Missing
Y10-1009,P09-1063,1,\N,Missing
Y10-1009,D08-1010,1,\N,Missing
Y10-1009,C08-1041,1,\N,Missing
Y10-1009,C10-1080,1,\N,Missing
