2020.paclic-1.15,Generation and Evaluation of Concept Embeddings Via Fine-Tuning Using Automatically Tagged Corpus,2020,-1,-1,4,1,15816,kanako komiya,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.paclic-1.32,Evaluation of Pretrained {BERT} Model by Using Sentence Clustering,2020,-1,-1,5,0,15866,naoki shibayama,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.paclic-1.46,Composing Word Vectors for {J}apanese Compound Words Using Bilingual Word Embeddings,2020,-1,-1,4,0,15890,teruo hirabayashi,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.bucc-1.4,Automatic Creation of Correspondence Table of Meaning Tags from Two Dictionaries in One Language Using Bilingual Word Embedding,2020,-1,-1,4,0,15890,teruo hirabayashi,Proceedings of the 13th Workshop on Building and Using Comparable Corpora,0,"In this paper, we show how to use bilingual word embeddings (BWE) to automatically create a corresponding table of meaning tags from two dictionaries in one language and examine the effectiveness of the method. To do this, we had a problem: the meaning tags do not always correspond one-to-one because the granularities of the word senses and the concepts are different from each other. Therefore, we regarded the concept tag that corresponds to a word sense the most as the correct concept tag corresponding the word sense. We used two BWE methods, a linear transformation matrix and VecMap. We evaluated the most frequent sense (MFS) method and the corpus concatenation method for comparison. The accuracies of the proposed methods were higher than the accuracy of the random baseline but lower than those of the MFS and corpus concatenation methods. However, because our method utilized the embedding vectors of the word senses, the relations of the sense tags corresponding to concept tags could be examined by mapping the sense embeddings to the vector space of the concept tags. Also, our methods could be performed when we have only concept or word sense embeddings whereas the MFS method requires a parallel corpus and the corpus concatenation method needs two tagged corpora."
Y18-1005,Domain Adaptation for Sentiment Analysis using Keywords in the Target Domain as the Learning Weight,2018,0,1,2,0.952381,10894,jing bai,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
Y18-1068,Domain Adaptation Using a Combination of Multiple Embeddings for Sentiment Analysis,2018,0,0,1,1,15818,hiroyuki shinnou,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
Y18-1072,Fine-tuning for Named Entity Recognition Using Part-of-Speech Tagging,2018,0,0,4,0,27556,masaya suzuki,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
W18-3408,Investigating Effective Parameters for Fine-tuning of Word Embeddings Using Only a Small Corpus,2018,0,1,2,1,15816,kanako komiya,Proceedings of the Workshop on Deep Learning Approaches for Low-Resource {NLP},0,"Fine-tuning is a popular method to achieve better performance when only a small target corpus is available. However, it requires tuning of a number of metaparameters and thus it might carry risk of adverse effect when inappropriate metaparameters are used. Therefore, we investigate effective parameters for fine-tuning when only a small target corpus is available. In the current study, we target at improving Japanese word embeddings created from a huge corpus. First, we demonstrate that even the word embeddings created from the huge corpus are affected by domain shift. After that, we investigate effective parameters for fine-tuning of the word embeddings using a small target corpus. We used perplexity of a language model obtained from a Long Short-Term Memory network to assess the word embeddings input into the network. The experiments revealed that fine-tuning sometimes give adverse effect when only a small target corpus is used and batch size is the most important parameter for fine-tuning. In addition, we confirmed that effect of fine-tuning is higher when size of a target corpus was larger."
L18-1162,All-words Word Sense Disambiguation Using Concept Embeddings,2018,0,2,5,0,29674,rui suzuki,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
Y17-1052,{J}apanese all-words {WSD} system using the {K}yoto Text Analysis {T}ool{K}it,2017,10,1,1,1,15818,hiroyuki shinnou,"Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation",0,None
Y16-2009,Supervised Word Sense Disambiguation with Sentences Similarities from Context Word Embeddings,2016,2,2,2,0,33288,shoma yamaki,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers",0,None
W16-1708,Comparison of Annotating Methods for Named Entity Corpora,2016,6,3,5,1,15816,kanako komiya,Proceedings of the 10th Linguistic Annotation Workshop held in conjunction with {ACL} 2016 ({LAW}-X 2016),0,None
Y15-2025,Learning under Covariate Shift for Domain Adaptation for Word Sense Disambiguation,2015,21,3,1,1,15818,hiroyuki shinnou,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation: Posters",0,"We show that domain adaptation for word sense disambiguation (WSD) satisfies the assumption of covariate shift, and then solve it by learning under covariate shift. Learning under covariate shift has two key points: (1) calculation of the weight of an instance and (2) weighted learning. For the first point, we employ unconstrained least squares importance fitting (uLSIF), which models the probability density ratio of the source domain against a target domain directly. Additionally, we propose weight only to the particular instance and using a linear kernel rather than a Gaussian kernel in uLSIF. For the second point, we employ a support vector machine (SVM) rather than the maximum entropy method (ME) that is commonly employed in weighted learning. Three corpora in the Balanced Corpus of Contemporary Written Japanese (BCCWJ) and 16 target words were used in our experiment. The experimental results show that the proposed method demonstrates the highest average precision."
Y15-2026,Unsupervised Domain Adaptation for Word Sense Disambiguation using Stacked Denoising Autoencoder,2015,12,3,2,0,36282,kazuhei kouno,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation: Posters",0,"In this paper, we propose an unsupervised domain adaptation for Word Sense Disambiguation (WSD) using Stacked Denoising Autoencoder (SdA). SdA is an unsupervised learning method of obtaining the abstract feature set of input data using Neural Network. The abstract feature set absorbs the difference of domains, and thus SdA can solve a problem of domain adaptation. However, SdA does not always cope with any problems of domain adaptation. Especially, difficulty of domain adaptation for WSD depends on the combination of a source domain, a target domain and a target word. As a result, any method of domain adaptation for WSD has adverse effect for a part of the problem, Therefore, we defined the similarity between two domains, and judge whether we use SdA or not through this similarity. This approach avoids an adverse effect of SdA. In the experiments, we have used three domains from the Balanced Corpus of Contemporary Written Japanese and 16 target words. In comparison with baseline, our method has got higher average accuracies for all combinations of two domains. Furthermore, we have obtained better results against conventional domain adaptation methods."
Y15-1005,Surrounding Word Sense Model for {J}apanese All-words Word Sense Disambiguation,2015,15,1,5,1,15816,kanako komiya,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"This paper proposes a surrounding word sense model (SWSM) that uses the distribution of word senses that appear near ambiguous words for unsupervised all-words word sense disambiguation in Japanese. Although it was inspired by the topic model, ambiguous Japanese words tend to have similar topics since coarse semantic polysemy is less likely to occur than that in Western languages as Japanese uses Chinese characters, which are ideograms. We thus propose a model that uses the distribution of word senses that appear near ambiguous words: SWSM. We embedded the concept dictionary of an Electronic Dictionary Research (EDR) electronic dictionary in the system and used the Japanese Corpus of EDR for the experiments, which demonstrated that SWSM outperformed a system with a random baseline and a system that used a topic model called Dirichlet Allocation with WORDNET (LDAWN), especially when there were high levels of entropy for the word sense distribution of ambiguous words."
Y15-1057,Hybrid Method of Semi-supervised Learning and Feature Weighted Learning for Domain Adaptation of Document Classification,2015,19,3,1,1,15818,hiroyuki shinnou,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"In regard to document classification, semisupervised learning using the Naive Bayes method and EM algorithm was a great success, and we refer to this method as NBEM in this paper. Although NBEM is also effective for domain adaption of document classification, there is still room for improvement because NBEM does not employ valuable information for this task, that is the difference between source domain and target domain. Here, according to the similarity between the label distribution of the feature on source domain and the estimated label distribution of the feature on target domain, we set the weight on the features to reconstruct the training data. We use this reconstructed training data to perform document classification by NBEM. As a result of experiment by using a part of 20 Newsgroups, the effect of this method was confirmed."
R15-1039,Domain Adaptation with Filtering for Named Entity Extraction of {J}apanese Anime-Related Words,2015,12,0,5,1,15816,kanako komiya,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"We developed a system to extract Japanese anime-related words, i.e., Japanese NEs (named entities) in the anime-related domain. Since the NEs in the area, such as the titles of anime or the names of characters, were domain-specific, we started by building a tagged corpus and then used it for the experiments. We examined to see if the existing corpora were useful to improve the results. The experiments conducted using Conditional Random Fields showed that the effect of domain adaptation varied according to the genres of the corpora, but the filtering of the source data not only reduced the time for training but also assisted in the domain adaptation work."
Y13-1043,Use of Combined Topic Models in Unsupervised Domain Adaptation for Word Sense Disambiguation,2013,14,0,2,0,40484,shinya kunii,"Proceedings of the 27th Pacific Asia Conference on Language, Information, and Computation ({PACLIC} 27)",0,"Topic models can be used in an unsupervised domain adaptation for Word Sense Disambiguation (WSD). In the domain adaptation task, three types of topic models are available: (1) a topic model constructed from the source domain corpus: (2) a topic model constructed from the target domain corpus, and (3) a topic model constructed from both domains. Basically, three topic features made from each topic model are added to the normal feature used for WSD. By using the extended features, SVM learns and solves WSD. However, the topic features constructed from source domain have weights describing the similarity between the source corpus and the entire corpus because the topic features made from the source domain can reduce the accuracy of WSD. In six transitions of domain adaptation using three domains, we conducted experiments by varying the combination of topic features, and show the effectiveness of the proposed method."
sasaki-shinnou-2012-detection,Detection of Peculiar Word Sense by Distance Metric Learning with Labeled Examples,2012,10,1,2,0.666049,14383,minoru sasaki,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"For natural language processing on machines, resolving such peculiar usages would be particularly useful in constructing a dictionary and dataset for word sense disambiguation. Hence, it is necessary to develop a method to detect such peculiar examples of a target word from a corpus. Note that, hereinafter, we define a peculiar example as an instance in which the target word or phrase has a new meaning.In this paper, we proposed a new peculiar example detection method using distance metric learning from labeled example pairs. In this method, first, distance metric learning is performed by large margin nearest neighbor classification for the training data, and new training data points are generated using the distance metric in the original space. Then, peculiar examples are extracted using the local outlier factor, which is a density-based outlier detection method, from the updated training and test data. The efficiency of the proposed method was evaluated on an artificial dataset and the Semeval-2010 Japanese WSD task dataset. The results showed that the proposed method has the highest number of properly detected instances and the highest F-measure value. This shows that the label information of training data is effective for density-based peculiar example detection. Moreover, an experiment on outlier detection using a classification method such as SVM showed that it is difficult to apply the classification method to outlier detection."
shinnou-sasaki-2010-detection,Detection of Peculiar Examples using {LOF} and One Class {SVM},2010,6,1,1,1,15818,hiroyuki shinnou,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper proposes the method to detect peculiar examples of the target word from a corpus. In this paper we regard following examples as peculiar examples: (1) a meaning of the target word in the example is new, (2) a compound word consisting of the target word in the example is new or very technical. The peculiar example is regarded as an outlier in the given example set. Therefore we can apply many methods proposed in the data mining domain to our task. In this paper, we propose the method to combine the density based method, Local Outlier Factor (LOF), and One Class SVM, which are representative outlier detection methods in the data mining domain. In the experiment, we use the Whitepaper text in BCCWJ as the corpus, and 10 noun words as target words. Our method improved precision and recall of LOF and One Class SVM. And we show that our method can detect new meanings by using the noun `midori (green)'. The main reason of un-detections and wrong detection is that similarity measure of two examples is inadequacy. In future, we must improve it."
shinnou-sasaki-2008-ping,Ping-pong Document Clustering using {NMF} and Linkage-Based Refinement,2008,8,3,1,1,15818,hiroyuki shinnou,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper proposes a ping-pong document clustering method using NMF and the linkage based refinement alternately, in order to improve the clustering result of NMF. The use of NMF in the ping-pong strategy can be expected effective for document clustering. However, NMF in the ping-pong strategy often worsens performance because NMF often fails to improve the clustering result given as the initial values. Our method handles this problem with the stop condition of the ping-pong process. In the experiment, we compared our method with the k-means and NMF by using 16 document data sets. Our method improved the clustering result of NMF significantly."
shinnou-sasaki-2008-spectral,Spectral Clustering for a Large Data Set by Reducing the Similarity Matrix Size,2008,7,32,1,1,15818,hiroyuki shinnou,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Spectral clustering is a powerful clustering method for document data set. However, spectral clustering needs to solve an eigenvalue problem of the matrix converted from the similarity matrix corresponding to the data set. Therefore, it is not practical to use spectral clustering for a large data set. To overcome this problem, we propose the method to reduce the similarity matrix size. First, using k-means, we obtain a clustering result for the given data set. From each cluster, we pick up some data, which are near to the central of the cluster. We take these data as one data. We call this data set as ÂcommitteeÂ. Data except for committees remain one data. For these data, we construct the similarity matrix. Definitely, the size of this similarity matrix is reduced so much that we can perform spectral clustering using the reduced similarity matrix."
shinnou-sasaki-2008-division,Division of Example Sentences Based on the Meaning of a Target Word Using Semi-Supervised Clustering,2008,9,3,1,1,15818,hiroyuki shinnou,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this paper, we describe a system that divides example sentences (data set) into clusters, based on the meaning of the target word, using a semi-supervised clustering technique. In this task, the estimation of the cluster number (the number of the meaning) is critical. Our system primarily concentrates on this aspect. First, a user assigns the system an initial cluster number for the target word. The system then performs general clustering on the data set to obtain small clusters. Next, using constraints given by the user, the system integrates these clusters to obtain the final clustering result. Our system performs this entire procedure with high precision and requiring only a few constraints. In the experiment, we tested the system for 12 Japanese nouns used in the SENSEVAL2 Japanese dictionary task. The experiment proved the effectiveness of our system. In the future, we will improve sentence similarity measurements."
Y07-1045,Refinement of Document Clustering by Using {NMF},2007,0,0,1,1,15818,hiroyuki shinnou,"Proceedings of the 21st Pacific Asia Conference on Language, Information and Computation",0,None
P07-2020,Ensemble document clustering using weighted hypergraph generated by {NMF},2007,15,9,1,1,15818,hiroyuki shinnou,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"In this paper, we propose a new ensemble document clustering method. The novelty of our method is the use of Non-negative Matrix Factorization (NMF) in the generation phase and a weighted hypergraph in the integration phase. In our experiment, we compared our method with some clustering methods. Our method achieved the best results."
shinnou-sasaki-2004-semi,Semi-supervised Learning by Fuzzy Clustering and Ensemble Learning,2004,5,0,1,1,15818,hiroyuki shinnou,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper proposes a semi-supervised learning method using Fuzzy clustering to solve word sense disambiguation problems. Furthermore, we reduce side effects of semi-supervised learning by ensemble learning. We set classes for labeled instances. The -th labeled instance is used as the prototype of the -th class. By using Fuzzy clustering for unlabeled instances, prototypes are moved to more suitable positions. We can classify a test instance by the Nearest Neighbor (k-NN) with the moved prototypes. Moreover, to reduce side effects of semi-supervised learning, we use the ensemble learning combined the k-NN with initial labeled instances, which is initial prototype, and the k-NN with prototypes moved by Fuzzy clustering."
sasaki-shinnou-2004-information,Information Retrieval System Using Latent Contextual Relevance,2004,8,0,2,0.606061,14383,minoru sasaki,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"When the relevance feedback, which is one of the most popular information retrieval model, is used in an information retrieval system, a related word is extracted based on the first retrival result. Then these words are added into the original query, and retrieval is performed again using updated query. Generally, Using such query expansion technique, retrieval performance using the query expansion falls in comparison with the performance using the original query. As the cause, there is a few synonyms in the thesaurus and although some synonyms are added to the query, the same documents are retireved as a result. In this paper, to solve the problem over such related words, we propose latent context relevance in consideration of the relevance between query and each index words in the document set."
W03-0406,Unsupervised learning of word sense disambiguation rules by estimating an optimum iteration number in the {EM} algorithm,2003,7,18,1,1,15818,hiroyuki shinnou,Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003,0,"In this paper, we improve an unsupervised learning method using the Expectation-Maximization (EM) algorithm proposed by Nigam et al. for text classification problems in order to apply it to word sense disambiguation (WSD) problems. The improved method stops the EM algorithm at the optimum iteration number. To estimate that number, we propose two methods. In experiments, we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2. The score of our method is a match for the best public score of this task. Furthermore, our methods were confirmed to be effective also for verb WSD problems."
shinnou-2002-learning,"Learning of word sense disambiguation rules by Co-training, checking co-occurrence of features",2002,10,2,1,1,15818,hiroyuki shinnou,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"In this paper, we propose a method to improve Co-training and apply it to word sense disambiguation problems. Co-training is an unsupervised learning method to overcome the problem that labeled training data is fairly expensive to obtain. Co-training is theoretically promising, but it requires two feature sets with the conditional independence assumption. This assumption is too rigid. In fact there is no choice but to use incomplete feature sets, and then the accuracy of learned rules reaches a limit. In this paper, we check co-occurrence between two feature sets to avoid such undesirable situation when we add unlabeled instances to training data. In experiments, we applied our method to word sense disambiguation problems for the three Japanese words xe2x80x98koexe2x80x99, xe2x80x98toppuxe2x80x99 and xe2x80x98kabexe2x80x99 and demonstrated that it improved Co-training."
shinnou-ikeya-2000-extraction,Extraction of Unknown Words Using the Probability of Accepting the Kanji Character Sequence as One Word,2000,0,2,1,1,15818,hiroyuki shinnou,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,None
E99-1024,Detection of {J}apanese Homophone Errors by a Decision List Including a Written Word as a Default Evidence,1999,7,3,1,1,15818,hiroyuki shinnou,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In this paper, we propose a practical method to detect Japanese homophone errors in Japanese texts. It is very important to detect homophone errors in Japanese revision systems because Japanese texts suffer from homophone errors frequently. In order to detect homophone errors, we have only to solve the homophone problem. We can use the decision list to do it because the homophone problem is equivalent to the word sense disambiguation problem. However, the homophone problem is different from the word sense disambiguation problem because the former can use the written word but the latter cannot. In this paper, we incorporate the written word into the original decision list by obtaining the identifying strength of the written word. The improved decision list can raise the F-measure of error detection."
W98-1120,A Decision Tree Method for Finding and Classifying Names in {J}apanese Texts,1998,0,108,3,0,1087,satoshi sekine,Sixth Workshop on Very Large Corpora,0,None
shinnou-1998-revision,Revision of morphological analysis errors through the person name construction model,1998,8,0,1,1,15818,hiroyuki shinnou,Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"In this paper, we present the method to automatically revise morphological analysis errors caused by unregistered person names. In order to detect and revise their errors, we propose the Person Name Construction Model for kanji characters composing Japanese names. Our method has the advantage of not using context information, like a suffix, to recognize person names, thus making our method a useful one. Through the experiment, we show that our proposed model is effective."
Y96-1019,Finding a Deficiency of a Meaning in a Bunrui-goi-hyou Entry by Using Corpora,1996,5,0,1,1,15818,hiroyuki shinnou,"Proceedings of the 11th Pacific Asia Conference on Language, Information and Computation",0,"This paper presents a method to automatically find meanings which should be but are not entered in thesaurus. In this paper, we use Bunrui-goi-hyou as the thesaurus. To find the noun n with the meaning lacking in Bunrui-goi-hyou, we applies our presented method which extracts idioms from a corpus. We use the clue that many idioms with the noun n are extracted through that method. We have experimented with a corpus which consists of 5 years' worth of articles from a Japanese economic newspaper. As a result, we found 177 types of lacking meanings. Furthermore, our method can find not only a deficiency of a meaning but also meanings which are not used or are specifically used in the corpus domain."
C96-2205,Redefining similarity in a thesaurus by using corpora,1996,5,2,1,1,15818,hiroyuki shinnou,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,None
