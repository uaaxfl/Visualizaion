2020.codi-1.11,Analyzing Neural Discourse Coherence Models,2020,-1,-1,4,1,21809,youmna farag,Proceedings of the First Workshop on Computational Approaches to Discourse,0,"In this work, we systematically investigate how well current models of coherence can capture aspects of text implicated in discourse organisation. We devise two datasets of various linguistic alterations that undermine coherence and test model sensitivity to changes in syntax and semantics. We furthermore probe discourse embedding space and examine the knowledge that is encoded in representations of coherence. We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further. Finally, we make our datasets publicly available as a resource for researchers to use to test discourse coherence models."
W19-6404,Active Learning for Financial Investment Reports,2019,-1,-1,2,0,4297,sian gooding,Proceedings of the Second Financial Narrative Processing Workshop (FNP 2019),0,None
W19-4406,The {BEA}-2019 Shared Task on Grammatical Error Correction,2019,0,15,4,1,10020,christopher bryant,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"This paper reports on the BEA-2019 Shared Task on Grammatical Error Correction (GEC). As with the CoNLL-2014 shared task, participants are required to correct all types of errors in test data. One of the main contributions of the BEA-2019 shared task is the introduction of a new dataset, the Write{\&}Improve+LOCNESS corpus, which represents a wider range of native and learner English levels and abilities. Another contribution is the introduction of tracks, which control the amount of annotated data available to participants. Systems are evaluated in terms of ERRANT F{\_}0.5, which allows us to report a much wider range of performance statistics. The competition was hosted on Codalab and remains open for further submissions on the blind test set."
N19-1261,Automatic learner summary assessment for reading comprehension,2019,0,0,3,1,984,menglin xia,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,Automating the assessment of learner summary provides a useful tool for assessing learner reading comprehension. We present a summarization task for evaluating non-native reading comprehension and propose three novel approaches to automatically assess the learner summaries. We evaluate our models on two datasets we created and show that our models outperform traditional approaches that rely on exact word match on this task. Our best model produces quality assessments close to professional examiners.
W18-0529,Language Model Based Grammatical Error Correction without Annotated Training Data,2018,0,13,2,1,10020,christopher bryant,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (â¼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research."
W18-0536,The Effect of Adding Authorship Knowledge in Automated Text Scoring,2018,0,1,5,0.5,8091,meng zhang,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Some language exams have multiple writing tasks. When a learner writes multiple texts in a language exam, it is not surprising that the quality of these texts tends to be similar, and the existing automated text scoring (ATS) systems do not explicitly model this similarity. In this paper, we suggest that it could be useful to include the other texts written by this learner in the same exam as extra references in an ATS system. We propose various approaches of fusing information from multiple tasks and pass this authorship knowledge into our ATS model on six different datasets. We show that this can positively affect the model performance at a global level."
N18-1024,Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input,2018,21,2,3,1,21809,youmna farag,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"We demonstrate that current state-of-the-art approaches to Automated Essay Scoring (AES) are not well-suited to capturing adversarially crafted input of grammatical but incoherent sequences of sentences. We develop a neural model of local coherence that can effectively learn connectedness features between sentences, and propose a framework for integrating and jointly training the local coherence model with a state-of-the-art AES model. We evaluate our approach against a number of baselines and experimentally demonstrate its effectiveness on both the AES task and the task of flagging adversarial input, further contributing to the development of an approach that strengthens the validity of neural essay scoring models."
W17-5016,An Error-Oriented Approach to Word Embedding Pre-Training,2017,18,1,3,1,21809,youmna farag,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We propose a novel word embedding pre-training approach that exploits writing errors in learners{'} scripts. We compare our method to previous models that tune the embeddings based on script scores and the discrimination between correct and corrupt word contexts in addition to the generic commonly-used embeddings pre-trained on large corpora. The comparison is achieved by using the aforementioned models to bootstrap a neural network that learns to predict a holistic score for scripts. Furthermore, we investigate augmenting our model with error corrections and monitor the impact on performance. Our results show that our error-oriented approach outperforms other comparable ones which is further demonstrated when training on more data. Additionally, extending the model with corrections provides further performance gains when data sparsity is an issue."
W17-5032,Artificial Error Generation with Machine Translation and Syntactic Patterns,2017,16,3,4,0.583208,2501,marek rei,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Shortage of available training data is holding back progress in the area of automated error detection. This paper investigates two alternative methods for artificially generating writing errors, in order to create additional resources. We propose treating error generation as a machine translation task, where grammatically correct text is translated to contain errors. In addition, we explore a system for extracting textual patterns from an annotated corpus, which can then be used to insert errors into grammatically correct sentences. Our experiments show that the inclusion of artificially generated errors significantly improves error detection accuracy on both FCE and CoNLL 2014 datasets."
P17-1074,Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction,2017,8,29,3,1,10020,christopher bryant,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to a new, dataset-agnostic, rule-based framework. This not only facilitates error type evaluation at different levels of granularity, but can also be used to reduce annotator workload and standardise existing GEC datasets. Human experts rated the automatic edits as {``}Good{''} or {``}Acceptable{''} in at least 95{\%} of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carry out a detailed error type analysis for the first time."
W16-0502,Text Readability Assessment for Second Language Learners,2016,32,18,3,1,984,menglin xia,Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper addresses the task of readability assessment for the texts aimed at second language (L2) learners. One of the major challenges in this task is the lack of significantly sized level-annotated data. For the present work, we collected a dataset of CEFR-graded texts tailored for learners of English as an L2 and investigated text readability assessment for both native and L2 learners. We applied a generalization method to adapt models trained on larger native corpora to estimate text readability for learners, and explored domain adaptation and self-learning techniques to make use of the native data to improve system performance on the limited L2 data. In our experiments, the best performing model for readability on learner texts achieves an accuracy of 0.797 and PCC of $0.938$."
W16-0510,Unsupervised Modeling of Topical Relevance in {L}2 Learner Text,2016,31,6,3,1,28668,ronan cummins,Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"The automated scoring of second-language (L2) learner text along various writing dimensions is an increasingly active research area. In this paper, we focus on determining the topical relevance of an essay to the prompt that elicited it. Given the burden involved in manually assigning scores for use in training supervised prompt-relevance models, we develop unsupervised models and show that they correlate well with human judgements. We show that expanding prompts using topically-related words, via pseudo-relevance modelling, is beneficial and outperforms other distributional techniques. Finally, we incorporate our prompt-relevance models into a supervised essay scoring system that predicts a holistic score and show that it improves its performance."
W16-0530,Candidate re-ranking for {SMT}-based grammatical error correction,2016,29,10,2,1,1828,zheng yuan,Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,None
P16-1075,Constrained Multi-Task Learning for Automated Essay Scoring,2016,26,9,3,1,28668,ronan cummins,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,This is the author accepted manuscript. The final version is available from Association for Computational Linguistics at http://acl2016.org/index.php?article_id=71.
N16-1042,Grammatical error correction using neural machine translation,2016,32,52,2,1,1828,zheng yuan,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This paper presents the first study using neural machine translation (NMT) for grammatical error correction (GEC). We propose a twostep approach to handle the rare word problem in NMT, which has been proved to be useful and effective for the GEC task. Our best NMTbased system trained on the CLC outperforms our SMT-based system when testing on the publicly available FCE test set. The same system achieves an F0.5 score of 39.90% on the CoNLL-2014 shared task test set, outperforming the state-of-the-art and demonstrating that the NMT-based GEC system generalises effectively."
C16-1079,Automatic Extraction of Learner Errors in {ESL} Sentences Using Linguistically Enhanced Alignments,2016,11,11,3,1,21029,mariano felice,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We propose a new method of automatically extracting learner errors from parallel English as a Second Language (ESL) sentences in an effort to regularise annotation formats and reduce inconsistencies. Specifically, given an original and corrected sentence, our method first uses a linguistically enhanced alignment algorithm to determine the most likely mappings between tokens, and secondly employs a rule-based function to decide which alignments should be merged. Our method beats all previous approaches on the tested datasets, achieving state-of-the-art results for automatic error extraction."
W15-0627,Using Learner Data to Improve Error Correction in Adjective{--}Noun Combinations,2015,27,0,2,1,4298,ekaterina kochmar,Proceedings of the Tenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper presents a novel approach to error correction in content words in learner writing focussing on adjectivexe2x80x90noun (AN) combinations. We show how error patterns can be used to improve the performance of the error correction system, and demonstrate that our approach is capable of suggesting an appropriate correction within the top two alternatives in half of the cases and within top 10 alternatives in 71% of the cases, performing with an MRR of 0.5061. We then integrate our error correction system with a state-of-the-art content word error detection system and discuss the results."
N15-1060,Towards a standard evaluation method for grammatical error detection and correction,2015,14,18,2,1,21029,mariano felice,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present a novel evaluation method for grammatical error correction that addresses problems with previous approaches and scores systems in terms of improvement on the original text. Our method evaluates corrections at the token level using a globally optimal alignment between the source, a system hypothesis, and a reference. Unlike the M 2 Scorer, our method provides scores for both detection and correction and is sensitive to different types of edit operations."
W14-1701,The {C}o{NLL}-2014 Shared Task on Grammatical Error Correction,2014,29,179,3,0,7314,hwee ng,Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task,0,"The CoNLL-2014 shared task was devoted to grammatical error correction of all error types. In this paper, we give the task definition, present the data sets, and describe the evaluation metric and scorer used in the shared task. We also give an overview of the various approaches adopted by the participating teams, and present the evaluation results. Compared to the CoNLL2013 shared task, we have introduced the following changes in CoNLL-2014: (1) A participating system is expected to detect and correct grammatical errors of all types, instead of just the five error types in CoNLL-2013; (2) The evaluation metric was changed from F1 to F0.5, to emphasize precision over recall; and (3) We have two human annotators who independently annotated the test essays, compared to just one human annotator in CoNLL-2013."
W14-1608,Looking for Hyponyms in Vector Space,2014,33,25,2,1,2501,marek rei,Proceedings of the Eighteenth Conference on Computational Natural Language Learning,0,"The task of detecting and generating hyponyms is at the core of semantic understanding of language, and has numerous practical applications. We investigate how neural network embeddings perform on this task, compared to dependency-based vector space models, and evaluate a range of similarity measures on hyponym generation. A new asymmetric similarity measure and a combination approach are described, both of which significantly improve precision. We release three new datasets of lexical vector representations trained on the BNC and our evaluation dataset for hyponym generation."
C14-1164,Detecting Learner Errors in the Choice of Content Words Using Compositional Distributional Semantics,2014,27,7,2,1,4298,ekaterina kochmar,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We describe a novel approach to error detection in adjectivexe2x80x90noun combinations. We present and release a new dataset of annotated errors where the examples are extracted from learner texts and annotated with error types. We show how compositional distributional semantic approaches can be applied to discriminate between correct and incorrect word combinations from learner data. Finally, we show how the output of the compositional distributional semantic models can be used as features in a classifier yielding good precision and accuracy."
R13-1047,Capturing Anomalies in the Choice of Content Words in Compositional Distributional Semantic Space,2013,25,7,2,1,4298,ekaterina kochmar,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"We are grateful to Cambridge ESOL, a division of Cambridge Assessment, and Cambridge University Press for supporting this research and for granting us access to the CLC for research purposes."
N13-1040,Parser lexicalisation through self-learning,2013,23,4,2,1,2501,marek rei,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We describe a new self-learning framework for parser lexicalisation that requires only a plain-text corpus of in-domain text. The method first creates augmented versions of dependency graphs by applying a series of modifications designed to directly capture higherorder lexical path dependencies. Scores are assigned to each edge in the graph using statistics from an automatically parsed background corpus. As bilexical dependencies are sparse, a novel directed distributional word similarity measure is used to smooth edge score estimates. Edge scores are then combined into graph scores and used for reranking the topn analyses found by the unlexicalised parser. The approach achieves significant improvements on WSJ and biomedical text over the unlexicalised baseline parser, which is originally trained on a subset of the Brown corpus."
W12-2004,Modeling coherence in {ESOL} learner texts,2012,37,18,2,1,2500,helen yannakoudakis,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"To date, few attempts have been made to develop new methods and validate existing ones for automatic evaluation of discourse coherence in the noisy domain of learner texts. We present the first systematic analysis of several methods for assessing coherence under the framework of automated assessment (AA) of learner free-text responses. We examine the predictive power of different coherence models by measuring the effect on performance when combined with an AA system that achieves competitive results, but does not use discourse coherence features, which are also strong indicators of a learner's level of attainment. Additionally, we identify new techniques that outperform previously developed ones and improve on the best published result for AA on a publically-available dataset of English learner free-text examination scripts."
W12-2028,{HOO} 2012 Error Recognition and Correction Shared Task: {C}ambridge {U}niversity Submission Report,2012,22,7,3,1,4298,ekaterina kochmar,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"Previous work on automated error recognition and correction of texts written by learners of English as a Second Language has demonstrated experimentally that training classifiers on error-annotated ESL text generally outperforms training on native text alone and that adaptation of error correction models to the native language (L1) of the writer improves performance. Nevertheless, most extant models have poor precision, particularly when attempting error correction, and this limits their usefulness in practical applications requiring feedback.n n We experiment with various feature types, varying quantities of error-corrected data, and generic versus L1-specific adaptation to typical errors using Naive Bayes (NB) classifiers and develop one model which maximizes precision. We report and discuss the results for 8 models, 5 trained on the HOO data and 3 (partly) on the full error-coded Cambridge Learner Corpus, from which the HOO data is drawn."
W12-0206,Automating Second Language Acquisition Research: Integrating Information Visualisation and Machine Learning,2012,26,5,2,1,2500,helen yannakoudakis,Proceedings of the {EACL} 2012 Joint Workshop of {LINGVIS} {\\&} {UNCLH},0,"We demonstrate how data-driven approaches to learner corpora can support Second Language Acquisition research when integrated with visualisation tools. We present a visual user interface supporting the investigation of a set of linguistic features discriminating between pass and fail 'English as a Second or Other Language' exam scripts. The system displays directed graphs to model interactions between features and supports exploratory search over a set of learner scripts. We illustrate how the interface can support the investigation of the co-occurrence of many individual features, and discuss how such investigations can shed light on understanding the linguistic abilities that characterise different levels of attainment and, more generally, developmental aspects of learner grammars."
W11-0202,Unsupervised Entailment Detection between Dependency Graph Fragments,2011,22,2,2,1,2501,marek rei,Proceedings of {B}io{NLP} 2011 Workshop,0,"Entailment detection systems are generally designed to work either on single words, relations or full sentences. We propose a new task -- detecting entailment between dependency graph fragments of any type -- which relaxes these restrictions and leads to much wider entailment discovery. An unsupervised framework is described that uses intrinsic similarity, multi-level extrinsic similarity and the detection of negation and hedged language to assign a confidence score to entailment relations between two fragments. The final system achieves 84.1% average precision on a data set of entailment examples from the biomedical domain."
P11-1019,A New Dataset and Method for Automatically Grading {ESOL} Texts,2011,27,200,2,1,2500,helen yannakoudakis,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We demonstrate how supervised discriminative machine learning techniques can be used to automate the assessment of 'English as a Second or Other Language' (ESOL) examination scripts. In particular, we use rank preference learning to explicitly model the grade relationships between scripts. A number of different features are extracted and ablation tests are used to investigate their contribution to overall performance. A comparison between regression and rank preference models further supports our method. Experimental results on the first publically available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus. Finally, using a set of 'outlier' texts, we test the validity of our model and identify cases where the model's scores diverge from that of a human examiner."
W10-3008,Combining Manual Rules and Supervised Learning for Hedge Cue and Scope Detection,2010,9,12,2,1,2501,marek rei,Proceedings of the Fourteenth Conference on Computational Natural Language Learning {--} Shared Task,0,"Hedge cues were detected using a supervised Conditional Random Field (CRF) classifier exploiting features from the RASP parser. The CRF's predictions were filtered using known cues and unseen instances were removed, increasing precision while retaining recall. Rules for scope detection, based on the grammatical relations of the sentence and the part-of-speech tag of the cue, were manually-developed. However, another supervised CRF classifier was used to refine these predictions. As a final step, scopes were constructed from the classifier output using a small set of post-processing rules. Development of the system revealed a number of issues with the annotation scheme adopted by the organisers."
W10-2809,Active Learning for Constrained {D}irichlet Process Mixture Models,2010,-1,-1,3,0.606061,7746,andreas vlachos,Proceedings of the 2010 Workshop on {GE}ometrical Models of Natural Language Semantics,0,None
N10-2001,{C}amtology: Intelligent Information Access for Science,2010,6,0,1,1,21810,ted briscoe,Proceedings of the {NAACL} {HLT} 2010 Demonstration Session,0,"We describe a novel semantic search engine for scientific literature. The Camtology system allows for sentence-level searches of PDF files and combines text and image searches, thus facilitating the retrieval of information present in tables and figures. It allows the user to generate complex queries for search terms that are related through particular grammatical/semantic relations in an intuitive manner. The system uses Grid processing to parallelise the analysis of large numbers of papers."
W09-1405,Biomedical Event Extraction without Training Data,2009,5,24,4,0.606061,7746,andreas vlachos,Proceedings of the {B}io{NLP} 2009 Workshop Companion Volume for Shared Task,0,"We describe our system for the BioNLP 2009 event detection task. It is designed to be as domain-independent and unsupervised as possible. Nevertheless, the precisions achieved for single theme event classes range from 75% to 92%, while maintaining reasonable recall. The overall F-scores achieved were 36.44% and 30.80% on the development and the test sets respectively."
andersen-etal-2008-bnc,The {BNC} Parsed with {RASP}4{UIMA},2008,7,26,3,0.833333,24155,oistein andersen,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We have integrated the RASP system with the UIMA framework (RASP4UIMA) and used this to parse the XML-encoded version of the British National Corpus (BNC). All original annotation is preserved, and parsing information, mainly in the form of grammatical relations, is added in an XML format. A few specific adaptations of the system to give better results with the BNC are discussed briefly. The RASP4UIMA system is publicly available and can be used to parse other corpora or document collections, and the final parsed version of the BNC will be deposited with the Oxford Text Archive."
C08-1033,Statistical Anaphora Resolution in Biomedical Texts,2008,77,43,2,0.625,45422,caroline gasperin,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper presents a probabilistic model for resolution of non-pronominal anaphora in biomedical texts. The model seeks to find the antecedents of anaphoric expressions, both coreferent and associative ones, and also to identify discourse-new expressions. We consider only the noun phrases referring to biomedical entities. The model reaches state-of-the art performance: 56--69% precision and 54--67% recall on coreferent cases, and reasonable performance on different classes of associative cases."
W07-2203,Semi-supervised Training of a Statistical Parser from Unlabeled Partially-bracketed Data,2007,27,8,2,1,45784,rebecca watson,Proceedings of the Tenth International Conference on Parsing Technologies,0,"We compare the accuracy of a statistical parse ranking model trained from a fully-annotated portion of the Susanne treebank with one trained from unlabeled partially-bracketed sentences derived from this treebank and from the Penn Treebank. We demonstrate that confidence-based semi-supervised techniques similar to self-training outperform expectation maximization when both are constrained by partial bracketing. Both methods based on partially-bracketed training data outperform the fully supervised technique, and both can, in principle, be applied to any statistical parser whose output is consistent with such partial-bracketing. We also explore tuning the model to a different domain and the effect of in-domain data in the semi-supervised training processes."
P07-1115,"A System for Large-Scale Acquisition of Verbal, Nominal and Adjectival Subcategorization Frames from Corpora",2007,17,47,2,0.833333,30715,judita preiss,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"This paper describes the first system for large-scale acquisition of subcategorization frames (SCFs) from English corpus data which can be used to acquire comprehensive lexicons for verbs, nouns and adjectives. The system incorporates an extensive rulebased classifier which identifies 168 verbal, 37 adjectival and 31 nominal frames from grammatical relations (GRs) output by a robust parser. The system achieves state-ofthe-art performance on all three sets."
P07-1125,Weakly Supervised Learning for Hedge Classification in Scientific Literature,2007,17,131,2,0,44651,ben medlock,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We investigate automatic classification of speculative language (xe2x80x98hedgingxe2x80x99), in biomedical text using weakly supervised machine learning. Our contributions include a precise description of the task with annotation guidelines, analysis and discussion, a probabilistic weakly supervised learning model, and experimental evaluation of the methods presented. We show that hedge classification is feasible using weakly supervised ML, and point toward avenues for future research."
D07-1130,Adapting the {RASP} System for the {C}o{NLL}07 Domain-Adaptation Task,2007,14,5,2,1,45784,rebecca watson,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We describe our submission to the domain adaptation track of the CoNLL07 shared task in the open class for systems using external resources. Our main finding was that it was very difficult to map from the annotation scheme used to prepare training and development data to one that could be used to effectively train and adapt the RASP system unlexicalized parse ranking model. Nevertheless, we were able to demonstrate a significant improvement in performance utilizing bootstrapping over the PBIOTB data."
P06-4020,The Second Release of the {RASP} System,2006,9,324,1,1,21810,ted briscoe,Proceedings of the {COLING}/{ACL} 2006 Interactive Presentation Sessions,0,"We describe the new release of the RASP (robust accurate statistical parsing) system, designed for syntactic annotation of free text. The new version includes a revised and more semantically-motivated output representation, an enhanced grammar and part-of-speech tagger lexicon, and a more flexible and semi-supervised training method for the structural parse ranking model. We evaluate the released version on the WSJ using a relational evaluation scheme, and describe how the new release allows users to enhance performance using (in-domain) lexical information."
P06-2006,Evaluating the Accuracy of an Unlexicalized Statistical Parser on the {PARC} {D}ep{B}ank,2006,18,58,1,1,21810,ted briscoe,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"We evaluate the accuracy of an unlexicalized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank. We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manually-constructed treebanks. This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure. The comparison of systems using DepBank is not straightforward, so we extend and validate DepBank and highlight a number of representation and scoring issues for relational evaluation schemes."
korhonen-etal-2006-large,A Large Subcategorization Lexicon for Natural Language Processing Applications,2006,25,73,3,0.812272,7440,anna korhonen,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We introduce a large computational subcategorizationlexicon which includes subcategorization frame (SCF) and frequencyinformation for 6,397 English verbs. This extensive lexicon was acquiredautomatically from five corpora and the Web using the current version of the comprehensive subcategorization acquisition system of Briscoe and Carroll (1997). The lexicon is provided freely for research use, along with a script which can be used to filter and build sub-lexicons suited for different natural languageprocessing (NLP) purposes. Documentation is also provided whichexplains each sub-lexicon option and evaluates its accuracy."
W05-1517,Efficient Extraction of Grammatical Relations,2005,17,15,3,1,45784,rebecca watson,Proceedings of the Ninth International Workshop on Parsing Technology,0,"We present a novel approach for applying the Inside-Outside Algorithm to a packed parse forest produced by a unification-based parser. The approach allows a node in the forest to be assigned multiple inside and outside probabilities, enabling a set of 'weighted GRs' to be computed directly from the forest. The approach improves on previous work which either loses efficiency by unpacking the parse forest before extracting weighted GRs, or places extra constraints on which nodes can be packed, leading to less compact forests. Our experiments demonstrate substantial increases in parser accuracy and throughput for weighted GR output."
P05-1076,Automatic Acquisition of Adjectival Subcategorization from Corpora,2005,17,6,3,0,50910,jeremy yallop,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,This paper describes a novel system for acquiring adjectival subcategorization frames (SCFs) and associated frequency information from English corpus data. The system incorporates a decision-tree classifier for 30 SCF types which tests for the presence of grammatical relations (GRs) in the output of a robust statistical parser. It uses a powerful pattern-matching language to classify GRs into frames hierarchically in a way that mirrors inheritance-based lexica. The experiments show that the system is able to detect SCF types with 70% precision and 66% recall rate. A new tool for linguistic annotation of SCFs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition.
W04-2606,Extended Lexical-Semantic Classification of {E}nglish Verbs,2004,22,45,2,0.812272,7440,anna korhonen,Proceedings of the Computational Lexical Semantics Workshop at {HLT}-{NAACL} 2004,0,"Lexical-semantic verb classifications have proved useful in supporting various natural language processing (NLP) tasks. The largest and the most widely deployed classification in English is Levin's (1993) taxonomy of verbs and their classes. While this resource is attractive in being extensive enough for some NLP use, it is not comprehensive. In this paper, we present a substantial extension to Levin's taxonomy which incorporates 57 novel classes for verbs not covered (comprehensively) by Levin. We also introduce 106 novel diathesis alternations, created as a side product of constructing the new classes. We demonstrate the utility of our novel classes by using them to support automatic subcategorization acquisition and show that the resulting extended classification has extensive coverage over the English verb lexicon."
preiss-etal-2004-anaphoric,Can Anaphoric Definite Descriptions be Replaced by Pronouns?,2004,7,3,3,0.833333,30715,judita preiss,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We investigate the hypothesis that a pronoun is used in discourse, when its antecedent is in the focus of the discourse. We create a corpus of xe2x80x98replaced anaphoric definite descriptionsxe2x80x99, where occurrences of definite descriptions are replaced with a corresponding pronoun. We use the Lappin and Leass (1994) anaphora resolution algorithm on the new corpus, and obtain a much lower performance than when the corpus only contains genuine pronouns, thus supporting the hypothesis."
W03-2601,Intermediate Parsing for Anaphora Resolution? Implementing the Lappin and Leass non-coreference filters,2003,0,1,2,0.784314,30715,judita preiss,Proceedings of the 2003 {EACL} Workshop on The Computational Treatment of Anaphora,0,None
preiss-etal-2002-subcategorization,Subcategorization Acquisition as an Evaluation Method for {WSD},2002,16,8,3,0.784314,30715,judita preiss,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"Evaluation of word sense disambiguation (WSD) systems is often based on machine-readable dictionaries (MRDs). Such evaluation typically employs a set of fine-grained dictionary senses and considers them all to be equally important. In this paper, we propose a novel evaluation method for WSD systems in the context of automatic subcategorization acquisition. Building on an extant subcategorization acquisition system, we show that the system would benefit from WSD and propose modifications which allow it to make use of WSD. The enhanced subcategorization acquisition system can then be used as a task-based evaluation method for WSD systems where both the notion of sense and the sensexe2x80x99s relevance to the evaluation process is determined by the application itself."
briscoe-carroll-2002-robust,Robust Accurate Statistical Annotation of General Text,2002,28,251,1,1,21810,ted briscoe,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"We describe a robust accurate domain-independent approach to statistical parsing incorporated into the new release of the ANLT toolkit, and publicly available as a research tool. The system has been used to parse many well known corpora in order to produce data for lexical acquisition efforts; it has also been used as a component in an open-domain question answering project. The performance of the system is competitive with that of statistical parsers using highly lexicalised parse selection models. However, we plan to extend the system to improve parse coverage, depth and accuracy."
C02-1013,High Precision Extraction of Grammatical Relations,2002,25,43,2,0,17923,john carroll,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,A parsing system returning analyses in the form of sets of grammatical relations can obtain high precision if it hypothesises a particular relation only when it is certain that the relation is correct. We operationalise this technique---in a statistical parser using a manually-developed wide-coverage grammar of English---by only returning relations that form part of all analyses licensed by the grammar. We observe an increase in precision from 75% to over 90% (at the cost of a reduction in recall) on a test corpus of naturally-occurring text.
W01-1808,High Precision Extraction of Grammatical Relations,2001,0,14,2,0,53741,john carrol,Proceedings of the Seventh International Workshop on Parsing Technologies,0,None
J99-4002,Lexical rules in constraint based grammars,1999,59,54,1,1,21810,ted briscoe,Computational Linguistics,0,"Lexical rules have been used to cover a very diverse range of phenomena in constraint-based grammars. Examination of the full range of rules proposed shows that Carpenter's (1991) postulated upper bound on the length of list-valued attributes such as SUBCAT in the lexicon cannot be maintained, leading to unrestricted generative capacity in constraint-based formalisms utilizing HPSG-style lexical rules. We argue that it is preferable to subdivide such rules into a class of semiproductive lexically governed genuinely lexical rules, and a class of fully productive unary syntactic rules.We develop a restricted approach to lexical rules in a typed default feature structure (TDFS) framework (Lascarides et al. 1995; Lascarides and Copestake 1999), which has enough expressivity to state, for example, rules of verb diathesis alternation, but which does not allow arbitrary manipulation of list-valued features. An interpretation of such lexical rules within a probabilistic version of a TDFS-based linguistic (lexical and grammatical) theory allows us to capture the semiproductive nature of genuinely lexical rules, steering an intermediate course between fully generative or purely abbreviatory rules.We illustrate the utility of this approach with a treatment of dative constructions within a linguistic framework that borrows insights from the constraint-based theories: HPSG, UCG, (Zeevat, Klein, and Calder 1987) and construction grammar (Goldberg 1995). We end by outlining how our approach to lexical rules allows for a treatment of passive and recursive affixation, which are generally assumed to require unrestricted list manipulation operations."
W98-1114,Can Subcategorisation Probabilities Help a Statistical Parser,1998,30,55,3,0,17923,john carroll,Sixth Workshop on Very Large Corpora,0,None
W97-1010,Learning Stochastic Categorial Grammars,1997,16,33,2,0,34710,miles osborne,{C}o{NLL}97: Computational Natural Language Learning,0,None
P97-1054,Co-Evolution of Language and of the Language Acquisition Device,1997,17,9,1,1,21810,ted briscoe,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"A new account of parameter setting during grammatical acquisition is presented in terms of Generalized Categorial Grammar embedded in a default inheritance hierarchy, providing a natural partial ordering on the setting of parameters. Experiments show that several expermentally effective learners can be defined in this framework. Evolutionary simulations suggest that a learner with default initial settings for parameters will emerge, provided that learning is memory limited and the environment of linguistic adaptation contains an appropriate language."
A97-1052,Automatic Extraction of Subcategorization from Corpora,1997,29,244,1,1,21810,ted briscoe,Fifth Conference on Applied Natural Language Processing,0,"We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount1."
W96-0303,Controlling the Application of Lexical Rules,1996,16,13,1,1,21810,ted briscoe,Breadth and Depth of Semantic Lexicons,0,"In this paper, we describe an item-familiarity account of the semi-productivity of morphological and lexical rules, and illustrate how it can be applied to practical issues which arise when building large scale lexical knowledge bases which utilize lexical rules. Our approach assumes that attested uses of derived words and senses are explicitly recorded, but that productive use of lexical rules is also possible, though controlled by probabilities associated with rule application. We discuss how the necessary probabilities and estimates of lexical rule productivity may be acquired from corpora."
W96-0209,Apportioning Development Effort in a Probabilistic {LR} Parsing System Through Evaluation,1996,22,26,2,0.281028,17923,john carroll,Conference on Empirical Methods in Natural Language Processing,0,"We describe an implemented system for robust domain-independent syntactic parsing of English, using a unification-based grammar of part-ofspeech and punctuation labels coupled with a probabilistic LR parser. We present evaluations of the systemxe2x80x99s performance along several different dimensions; these enable us to assess the contribution that each individual part is making to the success of the system as a whole, and thus prioritise the effort to be devoted to its further enhancement. Currently, the system is able to parse around 80% of sentences in a substantial corpus of general text containing a number of distinct genres. On a random sample of 250 such sentences the system has a mean crossing bracket rate of 0.71 and recall and precision of 83% and 84% respectively when evaluated against manually-disambiguated analyses."
1995.iwpt-1.8,Developing and Evaluating a Probabilistic {LR} Parser of Part-of-Speech and Punctuation Labels,1995,17,53,1,1,21810,ted briscoe,Proceedings of the Fourth International Workshop on Parsing Technologies,0,"We describe an approach to robust domain-independent syntactic parsing of unrestricted naturally-occurring (English) input. The technique involves parsing sequences of part-of-speech and punctuation labels using a unification-based grammar coupled with a probabilistic LR parser. We describe the coverage of several corpora using this grammar and report the results of a parsing experiment using probabilities derived from bracketed training data. We report the first substantial experiments to assess the contribution of punctuation to deriving an accurate syntactic analysis, by parsing identical texts both with and without naturally-occurring punctuation marks."
J93-1002,Generalized Probabilistic {LR} Parsing of Natural Language (Corpora) with Unification-Based Grammars,1993,68,191,1,1,21810,ted briscoe,Computational Linguistics,0,"We describe work toward the construction of a very wide-coverage probabilistic parsing system for natural language (NL), based on LR parsing techniques. The system is intended to rank the large number of syntactic analyses produced by NL grammars according to the frequency of occurrence of the individual rules deployed in each analysis. We discuss a fully automatic procedure for constructing an LR parse table from a unification-based grammar formalism, and consider the suitability of alternative LALR(1) parse table construction methods for large grammars. The parse table is used as the basis for two parsers; a user-driven interactive system that provides a computationally tractable and labor-efficient method of supervised training of the statistical information required to drive the probabilistic parser. The latter is constructed by associating probabilities with the LR parse table directly. This technique is superior to parsers based on probabilistic lexical tagging or probabilistic context-free grammar because it allows for a more context-dependent probabilistic language model, as well as use of a more linguistically adequate grammar formalism. We compare the performance of an optimized variant of Tomita's (1987) generalized LR parsing algorithm to an (efficiently indexed and optimized) chart parser. We report promising results of a pilot study training on 150 noun definitions from the Longman Dictionary of Contemporary English (LDOCE) and retesting on these plus a further 55 definitions. Finally, we discuss limitations of the current system and possible extensions to deal with lexical (syntactic and semantic) frequency of occurrence."
J93-1014,Book Reviews: Corpus Linguistics and the Automatic Analysis of {E}nglish,1993,-1,-1,1,1,21810,ted briscoe,Computational Linguistics,0,None
1992.tmi-1.1,Translation equivalence and lexicalization in the {ACQUILEX} {LKB},1992,-1,-1,2,0,49115,antonio sanfilippo,Proceedings of the Fourth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
W91-0209,Lexical Operations in a Unification-based Framework,1991,0,82,2,0,6790,ann copestake,Lexical Semantics and Knowledge Representation,0,We consider lexical operations and their representation in a unification based lexicon and the role of lexical semantic information. We describe a unified treatment of the linguistic aspects of sense extension and derivational morphological processes which delimit the range of possible coercions between lexemes and give a preliminary account of how default interpretations may arise.
C90-2008,Enjoy the Paper: Lexicology,1990,7,0,1,1,21810,ted briscoe,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,None
P89-1011,Lexical Access in Connected Speech Recognition,1989,27,10,1,1,21810,ted briscoe,27th Annual Meeting of the Association for Computational Linguistics,1,This paper addresses two issues concerning lexical access in connected speech recognition: 1) the nature of the pre-lexical representation used to initiate lexical lookup 2) the points at which lexical look-up is triggered off this representation. The results of an experiment are reported which was designed to evaluate a number of access strategies proposed in the literature in conjunction with several plausible pre-lexical representations of the speech input. The experiment also extends previous work by utilising a dictionary database containing a realistic rather than illustrative English vocabulary.
E89-1035,The Syntactic Regularity of {E}nglish Noun Phrases,1989,6,13,3,0,57811,lita taylor,Fourth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Approximately, 10,000 naturally occurring noun phrases taken from the LOB corpus were used firstly, to evaluate the NP component of the Alvey ANLT grammar (Grover et al., 1987, 1989) and secondly, to retest Sampson's (1987a) claim that this data provide evidence for the lack of a clear-cut distinction between grammatical and 'deviant' examples. The examples were sorted and classified on the basis of the lexical and syntactic analysis undertaken as part of the LOB corpus project (Sampson, 1987b). Tokens of each resulting type were parsed using the ANLT grammar and the results analysed to determine the success rate of the parses and the generality of the rules employed."
C88-1012,Software Support for Practical Grammar Development,1988,20,19,3,0.666667,57635,bran boguraev,{C}oling {B}udapest 1988 Volume 1: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Even though progress in theoretical linguistics does not necessarily rely on the construction of working programs, a large proportion of current research in syntactic theory is facilitated by suitable computational tools. However, when natural language processing applications seek to draw on the results from new developments in theories of grammar, not only the nature of the tools has to change, but they face the challenge of reconciling the seemingly contradictory requirements of notational perspicuity and efficiency of performance. In this paper, we present a comparison and an evaluation of a number of software systems for grammar development, and argue that they are inadequate as practical tools for building wide-coverage grammars. We discuss a number of factors characteristic of this task, demonstrate how they influence the design of a suitable software environment, and describe the implementation of a system which has supported efficient development of a large computational grammar of English."
P87-1027,The Derivation of a Grammatically Indexed Lexicon from the Longman Dictionary of Contemporary {E}nglish,1987,8,64,2,0,57635,bran boguraev,25th Annual Meeting of the Association for Computational Linguistics,1,"We describe a methodology and associated software system for the construction of a large lexicon from an existing machine-readable (published) dictionary. The lexicon serves as a component of an English morphological and syntactic analyser and contains entries with grammatical definitions compatible with the word and sentence grammar employed by the analyser. We describe a software system with two integrated components. One of these is capable of extracting syntactically rich, theory-neutral lexical templates from a suitable machine-readable source. The second supports interactive and semi-automatic generation and testing of target lexical entries in order to derive a sizeable, accurate and consistent lexicon from the source dictionary which contains partial (and occasionally in-accurate) information. Finally, we evaluate the utility of the Longman Dictionary of Contemporary English as a suitable source dictionary for the target lexicon."
J87-3002,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of {LDOCE},1987,24,65,2,0,57635,bran boguraev,Computational Linguistics,0,"This article focusses on the derivation of large lexicons for natural language processing. We describe the development of a dictionary support environment linking a restructured version of the Longman Dictionary of Contemporary English to natural language processing systems. The process of restructuring the information in the machine readable version of the dictionary is discussed. The Longman grammar code system is used to construct 'theory neutral' lexical entries. We demonstrate how such lexical entries can be put to practical use by linking up the system described here with the experimental PATR-II grammar development environment. Finally, we offer an evaluation of the utility of the grammar coding system for use by automatic natural language parsing systems."
E87-1011,A Multi-Purpose Interface to an On-line Dictionary,1987,9,8,3,0,37086,branimir boguraev,Third Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We argue that there are two qualitatively different modes of using a machine-readable dictionary in the context of research in computational linguistics: batch processing of the source with the purpose of collating information for subsequent use by a natural language application, and placing the dictionary on-line in an environment which supports fast interactive access to data selected on the basis of a number of linguistic constraints. While it is the former mode of dictionary use which is characteristic of most computational linguistics work to date, it is the latter which has the potential of making maximal use of the information typically found in a machine-readable dictionary. We describe the mounting of the machine-readable source of the Longman Dictionary of Contemporary English on a single user workstation to make it available as a development tool for a number of research projects."
E87-1035,Deterministic Parsing and Unbounded Dependencies,1987,15,0,1,1,21810,ted briscoe,Third Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper assesses two new approaches to deterministic parsing with respect to the analysis of unbounded dependencies (UDs). UDs in English are highly locally (and often globally) ambiguous. Several researchers have argued that the difficulty of UDs undermines the programme of deterministic parsing. However, their conclusion is based on critiques of various versions of the Marcus parser which represents only one of many possible approaches to deterministic parsing. We examine the predictions made by a LR(1) deterministic parser and the Lexicat deterministic parser concerning the analysis of UDs. The LR(1) technique is powerful enough to resolve the local ambiguities we examine. However, the Lexicat model provides a more psychologically plausible account of the parsing of UDs, which also offers a unified account of the resolution of local and global ambiguities in these constructions."
E85-1025,Towards a Dictionary Support Environment for Realtime Parsing,1985,13,18,3,0,41856,hiyan alshawi,Second Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,In this article we describe research on the development of large dictionaries for natural language processing. We detail the development of a dictionary support environment linking a restructured version of the Longman Dictionary of Contemporary English to natural language processing systems. We describe the process of restructuring the information in the dictionary and our use of the Longman grammar code system to construct dictionary entries for the PATR-II parsing system and our use of the Longman word definitions for automated word sense classification.
