1997.mtsummit-plenaries.13,W97-0411,0,0.0359081,"Missing"
1997.mtsummit-plenaries.13,W97-0405,0,0.0181888,"Missing"
1997.mtsummit-plenaries.13,C86-1155,0,0.061115,"Missing"
2004.eamt-1.15,binnenpoorte-etal-2002-field,0,0.156081,"Missing"
2008.eamt-1.21,W07-0712,0,0.0229164,"the second project, mainly carried out in 2007, involving at the research side Copenhagen Business School and the University of Copenhagen and on the business side Inter-Set, a medium-sized language service provider (LSP) with subsidiaries in other countries. The Open Source Moses MT system [1] was used both for training the SMT system and for translation. MOSES is currently mainly supported under the EuroMatrix project, funded by the European Commission. The language model was 150 12th EAMT conference, 22-23 September 2008, Hamburg, Germany trained using the language modelling toolkit IRSTLM [2]. The language models were trained with order 5. The maximum length of phrases in the phrase tables was set to 5. Domain Issues in SMT The assumption that there would be a productivity gain using SMT was the prime motivation factor for the LSP to investigate the potential of SMT systems. One of the issues to be considered in this context was the handling of subject domains. In an ideal world, all users involved with translation of technical documents would apply the same large-scale general subject classification system such as Lenoch [3]. From an SMT point of view the advantages of a consiste"
2008.eamt-1.21,P02-1040,0,0.0750122,"8, Hamburg, Germany Table 1. Training and test material for English-&gt;Danish SMT system. Subdomains Training Training Training Development Development words sentences sentence test set test set (Danish) av. length Words Sentences A:Camcorders 262,138 24,897 10.5 3,263 250 B:Software 1,609,943 73,517 21.9 7,282 250 C:DVD 136,683 12,991 10.5 2,416 250 D:Printers 144,379 14,657 9.9 1,989 250 E:Mobile phones 127,701 8,740 14.6 3,216 250 2,280,844 134,802 16.9 18,166 1,250 Total To do a quick evaluation of the system it was decided to score the translation output with the two automatic metrics BLEU [5] and TER [6]. As can be seen the subdomain with the best score is B, the second best score is found for sub-domain E, where a high figure for BLEU and a low figure for TER state that the translation output is closer to the reference, cf. table 2 below. Table 2. Translation quality in terms of BLEU and TER scores for 5 sub-domains. Development test data: A:Camcorders B:Software C:DVD D:Printers E:Mobile phones Total BLEU 0.5517 0.7564 0.4766 0.6539 0.6713 0.6818 TER 33.17 16.81 37.69 23.71 24.82 24.72 Based on this evaluation, the sub-domains B, E and D seemed to the best candidates to focus on"
2008.eamt-1.21,2006.amta-papers.25,0,0.205872,"Germany Table 1. Training and test material for English-&gt;Danish SMT system. Subdomains Training Training Training Development Development words sentences sentence test set test set (Danish) av. length Words Sentences A:Camcorders 262,138 24,897 10.5 3,263 250 B:Software 1,609,943 73,517 21.9 7,282 250 C:DVD 136,683 12,991 10.5 2,416 250 D:Printers 144,379 14,657 9.9 1,989 250 E:Mobile phones 127,701 8,740 14.6 3,216 250 2,280,844 134,802 16.9 18,166 1,250 Total To do a quick evaluation of the system it was decided to score the translation output with the two automatic metrics BLEU [5] and TER [6]. As can be seen the subdomain with the best score is B, the second best score is found for sub-domain E, where a high figure for BLEU and a low figure for TER state that the translation output is closer to the reference, cf. table 2 below. Table 2. Translation quality in terms of BLEU and TER scores for 5 sub-domains. Development test data: A:Camcorders B:Software C:DVD D:Printers E:Mobile phones Total BLEU 0.5517 0.7564 0.4766 0.6539 0.6713 0.6818 TER 33.17 16.81 37.69 23.71 24.82 24.72 Based on this evaluation, the sub-domains B, E and D seemed to the best candidates to focus on. For sub-do"
2008.eamt-1.21,W07-0718,0,0.0140308,"For the system developer and for system tuning the goal is to have automatic metrics that give reproducible results and save users from doing expensive post-editing tasks in each iteration of system improvements. Two automatic evaluation metrics were used: BLEU metric [5] and Translation Edit Rate, TER [6] 152 12th EAMT conference, 22-23 September 2008, Hamburg, Germany There has been much focus on evaluation of SMT and MT-systems in the last decades [7], [8], [9]. For a brief overview of other currently used evaluation metrics used for SMT and MT and recent experiences within the field, see [10] and [11]. The two selected metrics were chosen because they are easy for the developer to apply, given a translation reference. It has been argued that an increase/decrease in the value of the BLEU score does not guarantee a better/worse translation quality [13]. But nevertheless, the metric is still widely used to measure development improvements in systems. TER is calculated as the ratio of edits (insertions, deletions and substitutions of single words as well as shifts of word sequences) compared to the average number of words in the references. TER is stated to correlate reasonably well w"
2008.eamt-1.21,W08-0309,0,0.0189902,"system developer and for system tuning the goal is to have automatic metrics that give reproducible results and save users from doing expensive post-editing tasks in each iteration of system improvements. Two automatic evaluation metrics were used: BLEU metric [5] and Translation Edit Rate, TER [6] 152 12th EAMT conference, 22-23 September 2008, Hamburg, Germany There has been much focus on evaluation of SMT and MT-systems in the last decades [7], [8], [9]. For a brief overview of other currently used evaluation metrics used for SMT and MT and recent experiences within the field, see [10] and [11]. The two selected metrics were chosen because they are easy for the developer to apply, given a translation reference. It has been argued that an increase/decrease in the value of the BLEU score does not guarantee a better/worse translation quality [13]. But nevertheless, the metric is still widely used to measure development improvements in systems. TER is calculated as the ratio of edits (insertions, deletions and substitutions of single words as well as shifts of word sequences) compared to the average number of words in the references. TER is stated to correlate reasonably well with human"
2008.eamt-1.21,W07-0733,0,0.0469288,"Missing"
2008.eamt-1.21,E06-1032,0,0.0261338,"metric [5] and Translation Edit Rate, TER [6] 152 12th EAMT conference, 22-23 September 2008, Hamburg, Germany There has been much focus on evaluation of SMT and MT-systems in the last decades [7], [8], [9]. For a brief overview of other currently used evaluation metrics used for SMT and MT and recent experiences within the field, see [10] and [11]. The two selected metrics were chosen because they are easy for the developer to apply, given a translation reference. It has been argued that an increase/decrease in the value of the BLEU score does not guarantee a better/worse translation quality [13]. But nevertheless, the metric is still widely used to measure development improvements in systems. TER is calculated as the ratio of edits (insertions, deletions and substitutions of single words as well as shifts of word sequences) compared to the average number of words in the references. TER is stated to correlate reasonably well with human judgements [6]. TER values will be in the range from 0 (translated sentence is exactly like the reference) to in principle more that 100, e.g. if the reference sentence consists of only a few words whereas the translation output contains too many words"
2008.eamt-1.21,H94-1024,0,\N,Missing
2008.eamt-1.21,P07-2045,0,\N,Missing
2020.lrec-1.417,broeder-etal-2008-building,0,0.0816072,"Missing"
2020.lrec-1.417,L18-1515,1,0.568782,"Missing"
2020.lrec-1.417,L18-1210,1,0.800222,"Missing"
2020.lrec-1.417,hinrichs-krauwer-2014-clarin,0,0.0177308,"nfrastructures. It is strongly rooted in the humanities and the field of Natural Language Processing (NLP) and has the mission to create and maintain an infrastructure to support the sharing, use and sustainable availability of language data and tools for research in the humanities and social sciences (SSH) 1 See also https://www.clarin.eu/fair Figure 1: Map of CLARIN members, observers, and participating centres by February 2020. and beyond.2 Since its early days, the CLARIN consortium has aimed at building both a technical infrastructure and a sustainable organisation (Broeder et al., 2008; Hinrichs and Krauwer, 2014) while adhering to the interoperability paradigm at a range of levels. CLARIN has always operated in line with the European agenda for Open Science3 and it can be seen as an adopter of the FAIR data principles avant la lettre (de Jong et al., 2018). The focus on interoperability can be further illustrated along several dimensions, including the ambition to address the challenge of overcoming the obstacles stem2 See https://www.clarin.eu/content/vision-and-strategy See the 2016 Background note on Open Science available at https://ec.europa.eu/research/openscience/pdf/openaccess/ background note"
2020.lrec-1.417,P10-4005,0,0.0419607,"ort for scholarly workflows will be described in more detail. 2.1. In an infrastructure for research data both static resources, i.e. the data, and tools to process the data, are in place. The tools of a distributed infrastructure must be accessible from different locations. Interoperability ensures that linguistic tools can be combined with language data in a common processing pipeline. In CLARIN, web services have been put in place to encapsulate these tools and combine them in a common serviceoriented architecture. The first CLARIN activity in this field led to the development of WebLicht (Hinrichs et al., 2010). The web-based linguistic chaining tool provides an environment that allows the processing of textually given resources in a pipeline architecture. WebLicht has been applied in different processing tasks and for resources of different languages (Schmidt et al., 2016; C¸o¨ ltekin, 2015). More recently, a tool has been developed (Zinn, 2016) to provide guidance on which service is recommended for which data, known as the Language Resource Switchboard.8 The basic assumption behind the Switchboard is the focus on achieving a fairly basic but well-tested and robust level of interoperability, based"
2020.lrec-1.417,van-uytvanck-etal-2012-semantic,1,0.860288,"Missing"
andersen-etal-2012-creation,levy-andrew-2006-tregex,0,\N,Missing
andersen-etal-2012-creation,N10-1086,0,\N,Missing
andersen-etal-2012-creation,C10-1120,0,\N,Missing
andersen-etal-2012-creation,E06-1011,0,\N,Missing
andersen-etal-2012-creation,petrov-etal-2012-universal,0,\N,Missing
C96-2170,C96-1079,0,\N,Missing
calzolari-etal-2004-enabler,binnenpoorte-etal-2002-field,0,\N,Missing
henriksen-etal-2014-encompassing,pedersen-etal-2010-merging,1,\N,Missing
henriksen-etal-2014-encompassing,W14-0147,0,\N,Missing
henriksen-etal-2014-encompassing,W13-5616,1,\N,Missing
L16-1072,arranz-hamon-2012-way,0,0.0607119,"Missing"
L16-1072,fersoe-monachini-2004-elra,0,0.114129,"Missing"
L16-1072,2012.eamt-1.38,0,0.0424726,"Missing"
L18-1515,broeder-etal-2008-building,0,0.633208,"Missing"
L18-1515,L18-1210,1,0.860668,"Missing"
L18-1515,hinrichs-krauwer-2014-clarin,0,0.0971141,"rastructure with data centres (nodes) across Europe and beyond. The activities in CLARIN basically take place at two levels. One is the central level: the Board of Directors and the technical, communication, and administrative staff. The other is the national level: in each member country, the 1 http://www.clarin.eu national activities have by far the largest volume. Previous CLARIN activities have been described in multiple publications that have been disseminated through many channels. At LREC 2014 an overview of language resources, tools, and services on offer through CLARIN was presented (Hinrichs and Krauwer, 2014). The current paper focuses on the overall vision and in particular the adherence to the principles of FAIR and Responsible Data Science. 1.1. A Bit of History In the preparatory phase 2008–2011 CLARIN was funded by the European Commission. It was established as a European Research Infrastructure Consortium (ERIC)2 in 2012; the basic funding comes from the member countries. When it was established in 2012 CLARIN had nine members, and in 2017 it has grown to 19 members and two observers. Additionally, CLARIN has a special agreement with Carnegie Mellon University in the USA. This growth, which"
L18-1515,J16-3007,1,0.884286,"Missing"
L18-1515,trilsbeek-etal-2008-grid,0,0.069527,"Missing"
L18-1515,van-uytvanck-etal-2012-semantic,1,0.911593,"Missing"
maegaard-2004-nemlar-arabic,binnenpoorte-etal-2002-field,0,\N,Missing
maegaard-etal-2006-blark,binnenpoorte-etal-2002-field,0,\N,Missing
maegaard-etal-2006-blark,yaseen-etal-2006-building,0,\N,Missing
maegaard-etal-2006-blark,maegaard-2004-nemlar-arabic,1,\N,Missing
maegaard-etal-2006-blark,2004.eamt-1.15,1,\N,Missing
maegaard-etal-2006-mulinco,W00-1434,0,\N,Missing
maegaard-etal-2006-mulinco,W02-1501,0,\N,Missing
maegaard-etal-2008-medar,hamon-etal-2006-cesta,0,\N,Missing
maegaard-etal-2008-medar,yaseen-etal-2006-building,0,\N,Missing
maegaard-etal-2008-medar,maegaard-2004-nemlar-arabic,1,\N,Missing
maegaard-etal-2008-medar,2004.eamt-1.15,1,\N,Missing
maegaard-etal-2008-medar,maegaard-etal-2006-blark,1,\N,Missing
maegaard-etal-2010-cooperation,yaseen-etal-2006-building,0,\N,Missing
maegaard-etal-2010-cooperation,maegaard-etal-2006-blark,1,\N,Missing
