2004.iwslt-papers.3,W03-0301,0,0.062964,"ocessing, such as bilingual dictionaries extraction or transfer rules learning. However, it is in the context of statistical machine translation where it becomes particularly crucial. As an essential block in the learning process of current statistical translation models (single-word or phrase-based, conditionalor joined-probability based), its correct production has a sound correlation with translation quality [1]. This relevance has been corresponded by many previous works on the matter, including a shared task in the frame of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts [2]. Several competing systems were presented and evaluated against a manual reference using AER, the most widely used evaluation measure. Among the wide range of approaches presented, two basic trends should be highlighted. On the other side of the spectrum, we find the approach based on word cooccurrences and link probabilities, presented in [10]. Its relative simplicity, its flexibility to introduce more knowledge sources, its symmetry and its promising results [2] make it appealing despite its dependence on empirical data and tuning strategies. However, the most important disadvantage of this"
2004.iwslt-papers.3,C96-2141,0,0.435176,"Missing"
2004.iwslt-papers.3,W99-0604,0,0.110404,"Missing"
2004.iwslt-papers.3,N03-1017,0,0.0373062,"Missing"
2004.iwslt-papers.3,W02-1012,0,0.0356565,"target sentence to positions in source sentence, it is strictly asymmetric, generating one-to-many word alignments that do not account for many translation phenomena. This effect has been tackled by several kinds of symmetrization heuristics (all of them linguistically blind), in search of a strategy to provide posterior phrase-based translation systems with the most accurate possible source. Moreover, the complexity of IBM models and their overload of parameters to estimate turn it very hard to introduce linguistic information into this setting in a reasonable way (some efforts being done in [9]). This paper introduces a phrase alignment strategy that seeks phrase and word links in two stages using cooccurrence measures and linguistic information. On a first stage, the algorithm finds high-precision links involving a linguistically-derived set of phrases, leaving word alignment to be performed in a second phase. Experiments have been carried out for an English-Spanish parallel corpus, and we show how phrase cooccurrence measures convey a complementary information to word cooccurrences, and a stronger evidence of a good alignment. Alignment Error Rate (AER) results are presented, bein"
2004.iwslt-papers.3,P03-1012,0,0.0281238,"correct production has a sound correlation with translation quality [1]. This relevance has been corresponded by many previous works on the matter, including a shared task in the frame of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts [2]. Several competing systems were presented and evaluated against a manual reference using AER, the most widely used evaluation measure. Among the wide range of approaches presented, two basic trends should be highlighted. On the other side of the spectrum, we find the approach based on word cooccurrences and link probabilities, presented in [10]. Its relative simplicity, its flexibility to introduce more knowledge sources, its symmetry and its promising results [2] make it appealing despite its dependence on empirical data and tuning strategies. However, the most important disadvantage of this approach is the one-to-one constrain, producing high precision alignments with low recall, what can represent a severe limitation to its use in practical translation systems. We present in this paper an alignment strategy that is also based on bilingual cooccurrences, but aims at finding phrase-to-phrase alignment by using linguistic knowledge"
2004.iwslt-papers.3,W02-1018,0,0.0440718,"rases. Typically, initial Giza-based alignments are generated and a symmetrization strategy is followed to obtain the core alignment from which bilingual phrases are built. These in turn are fed to the statistical translation model for estimation. However, current symmetrization strategies lack linguistic knowledge to decide ambiguities, and the translation model faces the task of learning translation probabilities from a noisy source. To face this problem, we present an alignment strategy that generates directly a phrase alignment from corpus cooccurrence counts. In contrast to previous work [11], we do so by generating very high-confidence links between phrases before proceeding onto word alignment. This search for phrase links is limited to a small adequate set of possible phrases. In the following subsections our proposal is described in detail. Table 1: Examples of φ2 between words and phrases. por favor Association or cooccurrence measures extracted from parallel corpora give strong evidence of so-called translation equivalence [12], or simply alignment adequacy, between a pair of phrases or words. Among these measures we find Dice-score, φ2 score and some others, offering a simi"
2004.iwslt-papers.3,J03-1002,0,0.0116221,"subwords or phrases) in a given parallel corpus, detecting which words from each language are connected together in a given situation. This has many applications in natural language processing, such as bilingual dictionaries extraction or transfer rules learning. However, it is in the context of statistical machine translation where it becomes particularly crucial. As an essential block in the learning process of current statistical translation models (single-word or phrase-based, conditionalor joined-probability based), its correct production has a sound correlation with translation quality [1]. This relevance has been corresponded by many previous works on the matter, including a shared task in the frame of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts [2]. Several competing systems were presented and evaluated against a manual reference using AER, the most widely used evaluation measure. Among the wide range of approaches presented, two basic trends should be highlighted. On the other side of the spectrum, we find the approach based on word cooccurrences and link probabilities, presented in [10]. Its relative simplicity, its flexibility to introduce more knowled"
2004.iwslt-papers.3,N03-2017,0,0.0297579,"d include regular expressions such as numbers, dates or times of the day (that could also be classified) or even collocations and phrasal verbs. As this selection is language-dependent, every language will 109 define its own adequate rules. sented in [10]. Basically, an initial alignment is generated using word cooccurrence measures, from which link probabilities are estimated. Then, a best first search is performed, following an heuristic function based on the global aligned sentence link probabilities. The search is further improved with a syntactic constrain (also called cohesion constrain [15]) and can introduce features on the links, such as a dependence on adjacent links. Our implementation allows certain positions to be prohibited, so that previous phrase alignment is fixed, although its links also compute in link probability estimation at each iteration. Given the enormous space of possible word alignments to choose from, the heuristic function becomes the key to efficiency, so long as it is correctly defined. Basic parameters are: If no linguistic knowledge is available, statistical procedures can also be used to obtain a set of possible phrases. For example, we can select the"
2004.iwslt-papers.3,A00-1031,0,0.0489206,"ment strategy proposed against state-of-the-art alignments. Lmean 7.6 7.3 3.2. Preprocessing All preprocessing that has been carried out is described as follows: 4.1. Verb groups • Normalization of contracted forms for English (ie. wouldn’t = would not, we’ve = we have) and Spanish (del = de el) Verb groups detection rules include 14 rules for English language and just 6 for Spanish, which usually employs declined verb forms omitting thus personal pronouns and using thus a single word. Verb groups rules have detected a total of: • English data has been tagged using freely-available TnT tagger [16], and base forms have been obtained using wnmorph, included in the WordNet package [17]. • 1156 verb groups in English, classified into 238 different verb lemmas • Spanish data has been tagged using maco+ and relax package already mentioned. This software also generates a lemma or base form for each input word. • 658 verb groups in Spanish, classified into 188 different verb lemmas The classification of these phrases produces an efficient reduction of the cooccurrence table from 0.35M to 0.33M, we do not compute cooccurrence counts for all words internal to the phrase, but just for the lemma o"
2004.iwslt-papers.3,H92-1116,0,0.0518371,"ssing All preprocessing that has been carried out is described as follows: 4.1. Verb groups • Normalization of contracted forms for English (ie. wouldn’t = would not, we’ve = we have) and Spanish (del = de el) Verb groups detection rules include 14 rules for English language and just 6 for Spanish, which usually employs declined verb forms omitting thus personal pronouns and using thus a single word. Verb groups rules have detected a total of: • English data has been tagged using freely-available TnT tagger [16], and base forms have been obtained using wnmorph, included in the WordNet package [17]. • 1156 verb groups in English, classified into 238 different verb lemmas • Spanish data has been tagged using maco+ and relax package already mentioned. This software also generates a lemma or base form for each input word. • 658 verb groups in Spanish, classified into 188 different verb lemmas The classification of these phrases produces an efficient reduction of the cooccurrence table from 0.35M to 0.33M, we do not compute cooccurrence counts for all words internal to the phrase, but just for the lemma of its head verb. The results of the phrase alignment with these phrases are shown in th"
2004.iwslt-papers.3,P00-1056,0,0.146299,"Missing"
2004.iwslt-papers.3,H91-1026,0,\N,Missing
2005.iwslt-1.23,2002.tmi-tutorials.2,0,0.0759388,"ared under monotone conditions. We finally report comparative results in terms of translation accuracy, computation time and memory size. Results show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1. Introduction From the initial word-based translation models [1], research on statistical machine translation has been strongly boosted. At the end of the last decade the use of context in the translation model (phrase-based approach) lead to a clear improvement in translation quality ( [2], [3], [4]). Nowadays the introduction of some reordering abilities is of crucial importance for some language pairs and is an important focus of research in the area of SMT. In parallel to the phrase-based approach, the ngrambased approach [5] also introduces the word context in the translation model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phras"
2005.iwslt-1.23,P01-1067,0,0.0561396,"under monotone conditions. We finally report comparative results in terms of translation accuracy, computation time and memory size. Results show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1. Introduction From the initial word-based translation models [1], research on statistical machine translation has been strongly boosted. At the end of the last decade the use of context in the translation model (phrase-based approach) lead to a clear improvement in translation quality ( [2], [3], [4]). Nowadays the introduction of some reordering abilities is of crucial importance for some language pairs and is an important focus of research in the area of SMT. In parallel to the phrase-based approach, the ngrambased approach [5] also introduces the word context in the translation model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, w"
2005.iwslt-1.23,W02-1018,0,0.0292349,"monotone conditions. We finally report comparative results in terms of translation accuracy, computation time and memory size. Results show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1. Introduction From the initial word-based translation models [1], research on statistical machine translation has been strongly boosted. At the end of the last decade the use of context in the translation model (phrase-based approach) lead to a clear improvement in translation quality ( [2], [3], [4]). Nowadays the introduction of some reordering abilities is of crucial importance for some language pairs and is an important focus of research in the area of SMT. In parallel to the phrase-based approach, the ngrambased approach [5] also introduces the word context in the translation model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, where"
2005.iwslt-1.23,N04-1033,0,0.415123,"tuples methods. As can be seen,to produce the source sentence, the extracted unfolded tuples must be reordered. It is not the case of the target sentence, as it can be produced in order using both sequence of units. Figure 1 shows the bilingual units extracted using the extract-tuples and extract-unfold-tuples methods, for a given word-to-word aligned sentence pair. 2.2. Phrase-based Translation Model The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modification in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any gi"
2005.iwslt-1.23,J04-4002,0,0.0562728,"t is not the case of the target sentence, as it can be produced in order using both sequence of units. Figure 1 shows the bilingual units extracted using the extract-tuples and extract-unfold-tuples methods, for a given word-to-word aligned sentence pair. 2.2. Phrase-based Translation Model The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modification in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provid"
2005.iwslt-1.23,W05-0827,1,0.833763,"t can be produced in order using both sequence of units. Figure 1 shows the bilingual units extracted using the extract-tuples and extract-unfold-tuples methods, for a given word-to-word aligned sentence pair. 2.2. Phrase-based Translation Model The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modification in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [16] as the"
2005.iwslt-1.23,P05-1032,0,0.0181694,"he target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modification in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [16] as the probability of reappearence of larger phrases decreases. In our system we considered two length limits. We first extract all the phrases of length X or less (usually X equal to 3 or 4). Then, we also add phrases up to length Y (Y greater than X) if they cannot be generated by smaller phrases. Basically, we select additional phrases with source words that otherwise would be missed because"
2005.iwslt-1.23,N03-1017,0,0.0564353,"gth in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [16] as the probability of reappearence of larger phrases decreases. In our system we considered two length limits. We first extract all the phrases of length X or less (usually X equal to 3 or 4). Then, we also add phrases up to length Y (Y greater than X) if they cannot be generated by smaller phrases. Basically, we select additional phrases with source words that otherwise would be missed because of cross or long alignments [14]. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frecuency. N (f, e) P (f |e) = N (e) (3) where N(f,e) means t"
2005.iwslt-1.23,N04-1021,0,0.198833,"s is approximated by the product of word 3-gram probabilities: p(Tk ) ≈ k Y p(wn |wn−2 , wn−1 ) (4) n=1 where Tk refers to the partial translation hypothesis and wn to the nth word in it. As default language model feature, we use a standard word-based trigram language model generated with smoothing Kneser-Ney and interpolation of higher and lower order ngrams (by using SRILM [17]). • The following two feature functions correspond to a forward and backwards lexicon models. These models provides lexicon translation probabilities for each tuple based on the word-to-word IBM model 1 probabilities [18]. These lexicon models are computed according to the following equation: p((t, s)n ) = J X I Y 1 pIBM 1 (tin |sjn ) (I + 1)J j=1 i=0 (5) where sjn and tin are the j th and ith words in the source and target sides of tuple (t, s)n , being J and I the corresponding total number words in each side of it. For computing the forward lexicon model, IBM model 1 probabilities from GIZA++ [19] source-to-target alignments are used. In the case of the backwards lexicon model, GIZA++ target-to-source alignments are used instead. • The last feature in common we consider corresponds to a word penalty model."
2005.iwslt-1.23,2005.mtsummit-papers.37,1,0.848127,"model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, where the sequential composition of the phrases source words corresponds to the source sentence reordered. This procedure poses additional difficulties when applied to the ngram-based approach, because the characteristics of the ngram-based translation model. Despite of this, recent works ( [7], [8]) have shown how applying a reordering schema in the training process the ngram-based approach can also take advantage of the distortion capabilities. In this paper we study the differences and similarities of both approaches (ngram-based and phrase-based), focusing on the translation model, where the translation context is differently taken into account. We also investigate the differences in the translation (bilingual) units (tuples and phrases) and show efficiency results in terms of computation time and memory size for both systems. We have extended the comparison in [6] to a Chinese"
2005.iwslt-1.23,W05-0831,0,0.0547079,"l, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, where the sequential composition of the phrases source words corresponds to the source sentence reordered. This procedure poses additional difficulties when applied to the ngram-based approach, because the characteristics of the ngram-based translation model. Despite of this, recent works ( [7], [8]) have shown how applying a reordering schema in the training process the ngram-based approach can also take advantage of the distortion capabilities. In this paper we study the differences and similarities of both approaches (ngram-based and phrase-based), focusing on the translation model, where the translation context is differently taken into account. We also investigate the differences in the translation (bilingual) units (tuples and phrases) and show efficiency results in terms of computation time and memory size for both systems. We have extended the comparison in [6] to a Chinese to En"
2005.iwslt-1.23,P02-1038,0,0.234488,"els taken into account in the log-linear combination of features (see equation 1), and the bilingual units extraction methods (namely tuples and phrases). In section 3 is discussed the decoder used in both systems (MARIE) [9], giving details of pruning and reordering techniques. The comparison framework, experiments and results are shown in section 4, while conclusions are detailed in section 5. 2. Modeling Alternatively to the classical source channel approach, statistical machine translation models directly the posterior probability p(eI1 |f1J ) as a log-linear combination of feature models [10], based on the maximum entropy framework, as shown in [11]. This simplifies the introduction of several additional models explaining the translation process, as the search becomes: arg max{exp( eI1 X λi hi (e, f ))} (1) i where the feature functions hi are the system models (translation model, language model, reordering model, ...), and the λi weights are typically optimized to maximize a scoring function on a development set. The Translation Model is based on bilingual units (here called tuples and phrases). A bilingual unit consists of two monolingual fragments, where each one is supposed to"
2005.iwslt-1.23,J96-1002,0,0.0712527,"atures (see equation 1), and the bilingual units extraction methods (namely tuples and phrases). In section 3 is discussed the decoder used in both systems (MARIE) [9], giving details of pruning and reordering techniques. The comparison framework, experiments and results are shown in section 4, while conclusions are detailed in section 5. 2. Modeling Alternatively to the classical source channel approach, statistical machine translation models directly the posterior probability p(eI1 |f1J ) as a log-linear combination of feature models [10], based on the maximum entropy framework, as shown in [11]. This simplifies the introduction of several additional models explaining the translation process, as the search becomes: arg max{exp( eI1 X λi hi (e, f ))} (1) i where the feature functions hi are the system models (translation model, language model, reordering model, ...), and the λi weights are typically optimized to maximize a scoring function on a development set. The Translation Model is based on bilingual units (here called tuples and phrases). A bilingual unit consists of two monolingual fragments, where each one is supposed to be the translation of its counterpart. During training, t"
2005.iwslt-1.23,takezawa-etal-2002-toward,0,0.0468191,"sary trade-off between quality and efficiency. 4. Comparison 4.1. Evaluation Framework Figure 2: Search graph corresponding to a source sentence with four words. Details of constraints are given in following sections. The search loops expanding available hypotheses. The expansion proceeds incrementally starting in the group of lists covering 1 source word, ending with the group of lists covering J − 1 source words (J is the size in words of the source sentence). See [9] for further details. Experiments have been carried out using two databases: the EPPS database (Spanish-English) and the BTEC [20] database (Chinese-English). The BTEC is a small corpus translation task, used in the IWSLT’04 spoken language campaign1. Table 1 shows the main statistics of the used data, namely number of sentences, words, vocabulary, and mean sentence lengths for each language. The EPPS data set corresponds to the parliamentary session transcriptions of the European Parliament and is currently available at the Parliament’s website (http://www.euro parl.eu.int/). In the case of the results presented here, we have used the version of the EPPS data that was made available by RWTH Aachen University through the"
2005.iwslt-1.23,2005.iwslt-1.24,1,0.680304,"ated to the corpus size). Similar accuracy results in all tasks are reached for the baseline configurations. When upgrading the systems with additional features, slight differences appear. Although improvements added by each feature depends on the task and system, similar performances are reached in the best system’s configurations. Under reordering conditions, the ngram-based system seems to take advantage of the unfolding method applied in training, outperforming the phrase-based system. However, last results obtained for the IWSLT’05 show an opposite behaviour of both systems, see [22] and [23]. We can conclude that both approaches have a similar performance in terms of translation quality. The slight differences seen in the experiments are related to how the systems take advantage of each feature model and to the current system’s implementation. In terms of the memory size and computation time, the ngram-based system has obtained consistently better results. This indicates how even though using a smaller vocabulary of bilingual units, it has been more efficiently built and managed. The last characteristic becomes of great importance when working with large databases. 6. Acknowledgm"
2005.iwslt-1.25,2005.mtsummit-papers.36,1,0.794738,"m (s, t) to a logarithmic scaling of the probabilities of each model. Following this approach, the translation system described in this paper implements a log-linear combination of one translation model and four additional feature models. In contrast with standard phrase-based approaches, our translation model is expressed in tuples as bilingual units. Given a word alignment, tuples define a unique and monotonic segmentation of each bilingual sentence, building up a much smaller set of units than with phrases and allowing N-gram estimation to account for the history of the translation process [5, 6]. This approach has its origins in SMT by using finite state transducers [7, 8, 9]. The organization of the paper is as follows. Section 2 describes in detail the tuple n-gram translation model, while section 3 introduces the additional features used in the system. Section 4 provides a brief overview of the decoding tool and search strategy used. Next, sections 5 and 6 report and discuss results on IWSLT’05 Chineseto-English and Arabic-to-English tracks, respectively. Finally, Section 7 concludes and outlines future research lines. 2. The Tuple N-gram translation model The tuple N-gram transla"
2005.iwslt-1.25,2004.iwslt-evaluation.14,1,0.888051,"this approach, the translation system described in this paper implements a log-linear combination of one translation model and four additional feature models. In contrast with standard phrase-based approaches, our translation model is expressed in tuples as bilingual units. Given a word alignment, tuples define a unique and monotonic segmentation of each bilingual sentence, building up a much smaller set of units than with phrases and allowing N-gram estimation to account for the history of the translation process [5, 6]. This approach has its origins in SMT by using finite state transducers [7, 8, 9]. The organization of the paper is as follows. Section 2 describes in detail the tuple n-gram translation model, while section 3 introduces the additional features used in the system. Section 4 provides a brief overview of the decoding tool and search strategy used. Next, sections 5 and 6 report and discuss results on IWSLT’05 Chineseto-English and Arabic-to-English tracks, respectively. Finally, Section 7 concludes and outlines future research lines. 2. The Tuple N-gram translation model The tuple N-gram translation model is a language model of a particular language composed by bilingual unit"
2005.iwslt-1.25,N04-1033,0,0.0439184,"referred to as tuples. This model approximates the joint probability between source and target languages by using N-grams as described by the following equation: p(sJ1 , tI1 ) = · · · = K Y i=1 p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (2) (3) where (s, t)i refers to the ith tuple of a given bilingual sentence pair, which is segmented into K tuples. It is important to notice that, since both languages are linked up in tuples, the context information provided by this translation model is bilingual. Tuples are extracted from a word-to-word aligned corpus according to the following constraints [10]: • a monotonic segmentation of each bilingual sentence pair is produced • no word inside the tuple is aligned to words outside the tuple • no smaller tuples can be extracted without violating the previous constraints As a consequence of these constraints, only one segmentation is possible for a given parallel sentence pair and a word alignment. Usually, automatic word-to-word alignments are generated in both source-to-target and target-to-source directions by using GIZA++ [11], and tuples are usually extracted from the union set of alignments. However, in section 5 results are also reported w"
2005.iwslt-1.25,P00-1056,0,0.157829,"on model is bilingual. Tuples are extracted from a word-to-word aligned corpus according to the following constraints [10]: • a monotonic segmentation of each bilingual sentence pair is produced • no word inside the tuple is aligned to words outside the tuple • no smaller tuples can be extracted without violating the previous constraints As a consequence of these constraints, only one segmentation is possible for a given parallel sentence pair and a word alignment. Usually, automatic word-to-word alignments are generated in both source-to-target and target-to-source directions by using GIZA++ [11], and tuples are usually extracted from the union set of alignments. However, in section 5 results are also reported when extracting tuples with the alignment from sourceto-target direction. Figure 1 presents a simple example illustrating the tuple extraction process. I would like NULL NULL quisiera ir t1 t2 t3 to have a comer un helado t4 a t5 huge ice−cream gigante t6 Figure 1: Example of tuple extraction from an aligned bilingual sentence pair. Once tuples have been extracted, the tuple vocabulary can be pruned by using histogram counts, thus keeping the N most frequent tuples sharing the s"
2005.iwslt-1.25,2005.mtsummit-papers.37,1,0.781011,"uentiality contraint may lead to an unpractical tuple length and excessive amount of embedded words. In this case, it is more reasonable to allow for a certain reordering in the training data. This means that the tuples are broken into smaller tuples, and these are sequenced in the order of the target words. In order not to lose the information on the correct order, the decoder performs then a reordered search, which is guided by the N-gram model of the unfolded tuples and the additional feature models. On the other hand, the tuple unfolding process highly reduces the effect of embedded words [12]. Figure 2 shows an example of tuple unfolding compared to the monotonic extraction. The unfolding technique produces a different bilingual N-gram language model with reordered source words. Usually, this feature function is accompanied by a word penalty model. This model introduces a sentence length penalty in order to compensate the system’s preference for short target sentences, caused by the presence of the previous target language model. This penalization depends on the total number of words contained in the partial translation hypothesis, and it is computed as follows: pW P (tk ) = exp(n"
2005.iwslt-1.25,2005.iwslt-1.23,1,0.865304,"nments are used. In the case of the backward lexicon model, GIZA++ target-to-source alignments are used instead. 4. N-gram based Decoding • a target-to-source lexicon model 3.1. Target language model The first of these feature functions is a standard target language model, estimated as an N-gram over the target words, as expressed by this equation: k Y J X I Y 1 p(tin |sjn ) (I + 1)J j=1 i=0 For decoding given the combination of models presented above, we used MARIE, a decoder implemeting a beam search strategy with distortion (or reordering) capabilities developed at the TALP Research Center [13]. For efficient pruning of the search space, several pruning techniques are used, such as: (5) • Threshold pruning: Hypotheses with lower scores than a certain threshold are eliminated. where tk refers to the partial translation hypothesis and wn to the nth word in it. Although this model could be trained from a larger monolingual data set, this has not been done for IWSLT’05 experiments, which use as target text the same amount of data used as parallel text. As with the tuple translation model, the SRI Language Modeling toolkit was used. • Histogram pruning: Only the K-best ranked hypotheses"
2005.iwslt-1.25,2002.tmi-tutorials.2,0,0.0510661,"the target language is a possible translation of a given sentence s in the source language, and the main difference between two translation hypotheses is a probability assigned to each, which is to be learned from a bilingual corpus. The first SMT systems were based on the noisy channel approach on a word-based basis, modeling the translation of a target language sentence t given a source language sentence t as a translation model probability p(s|t) times a target language model probability p(t) [1]. Recently, word-based translation models have been replaced by phrase-based translation models [2, 3], which are estimated from aligned bilingual corpora by using relative frequencies. On the other hand, according to the maximum entropy framework [4], we can define the translation hypothesis t given a source sentence s, as the target sentence maximizing a log-linear combination of feature functions, as described in the following equation: tˆI1 = arg max tI1 ( M X λm hm (sJ1 , tI1 ) m=1 ) (1) where λm correspond to the weighting coefficients of the log-linear combination, and the feature functions hm (s, t) to a logarithmic scaling of the probabilities of each model. Following this approach, t"
2005.iwslt-1.25,N03-1017,0,0.0257197,"the target language is a possible translation of a given sentence s in the source language, and the main difference between two translation hypotheses is a probability assigned to each, which is to be learned from a bilingual corpus. The first SMT systems were based on the noisy channel approach on a word-based basis, modeling the translation of a target language sentence t given a source language sentence t as a translation model probability p(s|t) times a target language model probability p(t) [1]. Recently, word-based translation models have been replaced by phrase-based translation models [2, 3], which are estimated from aligned bilingual corpora by using relative frequencies. On the other hand, according to the maximum entropy framework [4], we can define the translation hypothesis t given a source sentence s, as the target sentence maximizing a log-linear combination of feature functions, as described in the following equation: tˆI1 = arg max tI1 ( M X λm hm (sJ1 , tI1 ) m=1 ) (1) where λm correspond to the weighting coefficients of the log-linear combination, and the feature functions hm (s, t) to a logarithmic scaling of the probabilities of each model. Following this approach, t"
2005.iwslt-1.25,P96-1041,0,0.0830768,"), in order to separate articles from words. Note that this process is neither guided by tagging information (it does not use any tagging software) nor complete (several other Arabic particles are usually attached to words). However, it already produces a significative vocabulary reduction, leading to improved performance. During word alignment, IBM model 1 tables are used directly to compute the lexicon feature. Finally, in order to learn the target and the tuples language models we used SRILM [14]. All models were learnt using interpolation of higher and lower order n-grams with Knesser-Ney [15] smoothing. 5.3. Development work Several configurations were tested on the development set optimizing BLEU, namely baseline and three alternatives. Results are shown in table 3. The baseline configuration system is built using: • The union alignment [11] to extract unfolded tuples and the intersection to solve embedded words. • All source-nulled tuples are linked the the target word of the next tuple. • The order of the target and the translation Ngram language models is set to 4 and 3, respectively. • The reordering parameters of the decoder are fixed to m = 5 and j = 3 for the Chinese-to-En"
2005.iwslt-1.25,J96-1002,0,0.0198229,"s a probability assigned to each, which is to be learned from a bilingual corpus. The first SMT systems were based on the noisy channel approach on a word-based basis, modeling the translation of a target language sentence t given a source language sentence t as a translation model probability p(s|t) times a target language model probability p(t) [1]. Recently, word-based translation models have been replaced by phrase-based translation models [2, 3], which are estimated from aligned bilingual corpora by using relative frequencies. On the other hand, according to the maximum entropy framework [4], we can define the translation hypothesis t given a source sentence s, as the target sentence maximizing a log-linear combination of feature functions, as described in the following equation: tˆI1 = arg max tI1 ( M X λm hm (sJ1 , tI1 ) m=1 ) (1) where λm correspond to the weighting coefficients of the log-linear combination, and the feature functions hm (s, t) to a logarithmic scaling of the probabilities of each model. Following this approach, the translation system described in this paper implements a log-linear combination of one translation model and four additional feature models. In con"
2005.iwslt-1.25,W05-0823,1,0.827145,"m (s, t) to a logarithmic scaling of the probabilities of each model. Following this approach, the translation system described in this paper implements a log-linear combination of one translation model and four additional feature models. In contrast with standard phrase-based approaches, our translation model is expressed in tuples as bilingual units. Given a word alignment, tuples define a unique and monotonic segmentation of each bilingual sentence, building up a much smaller set of units than with phrases and allowing N-gram estimation to account for the history of the translation process [5, 6]. This approach has its origins in SMT by using finite state transducers [7, 8, 9]. The organization of the paper is as follows. Section 2 describes in detail the tuple n-gram translation model, while section 3 introduces the additional features used in the system. Section 4 provides a brief overview of the decoding tool and search strategy used. Next, sections 5 and 6 report and discuss results on IWSLT’05 Chineseto-English and Arabic-to-English tracks, respectively. Finally, Section 7 concludes and outlines future research lines. 2. The Tuple N-gram translation model The tuple N-gram transla"
2005.iwslt-1.25,2005.iwslt-1.24,0,0.0195904,"s are correlated for both dev and test sets in the zh2en task, both improving in the primary run. However, they are incorrelated for both dev and test sets in the ar2en task. 6. Discussion When studying the test results, we can note that the Chinese test set seems to be ’easier’ to translate than the development (obtaining higher scores), whereas the effect is opposite in the case of Arabic. This behaviour could easily be explained by the nature of the data. However, when comparing the two TALP systems which competed in the same tracks and under the same conditions (TALP-Ngram and TALP-Phrase [17]), a surprisingly different behaviour between development and test can be found. Regarding development results, the TALPNgram system improves the performance of the TALPPhrase system (table 6) in the Chinese-to-English task (0.384 &gt; 0.373), while it achieves the same score in the Arabic-to-English task (0.573 ≈ 0.572), both measured in BLEU. However, regarding the test set, the TALPNgram system is clearly beaten by the TALP-Phrase system in both tasks (0.444 &lt; 0.452 in Chinese-to-English, and 0.533 &lt; 0.573 in Arabic-to-English). Experiments have been conducted in order to find out the reason e"
2005.mtsummit-papers.36,N01-1018,0,0.0681459,"ation of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is learned from training data. The translation system described in this paper implements a translation model based on the ﬁnite-state perspective, (de Gispert and Mari˜ no, 2002) and (de Gispert et al., 2004), which is used along with a log-linear combination of four additional feature functions (Crego et al., 2005). The implemented translation model, which is referred to as tuple n-gram model, diﬀers from the well known phrase-model approach (Koehn et al., 2003) in two basic issues."
2005.mtsummit-papers.36,J96-1002,0,0.115652,"entioned in the introduction, the translation system presented here implements a log-linear combination of feature functions 1 In the present version of the system, target words aligned to NULL are always attached to the following word. Further work in this area is proposed in the last section. along with the tuple n-gram model. This section describes the log-linear model and each of the four speciﬁc feature functions that are used. Finally, a brief description of the customized decoding tool that is used is presented. 3.1 Log-Linear Model Framework According to the maximum entropy framework (Berger et al., 1996), the corresponding translation hypothesis T , for a given source sentence S, is deﬁned by the target sentence that maximizes a log-linear combination of feature functions hi (S, T ), as described in the following equation:  argmax λi hi (S, T ) (2) T i where the λi ’s constitute the weighting coeﬃcients of the log-linear combination and the feature function hi (S, T ) corresponds to a logarithmic scaling of the ith -model probabilities. These weights are computed via an optimization procedure which maximizes the translation BLEU (Papineni et al., 2002) over a given development set. This opti"
2005.mtsummit-papers.36,J90-2002,0,0.523079,"Missing"
2005.mtsummit-papers.36,J93-2003,0,0.0465928,"ementation of translation algorithms based on statistical methods (Brown et al., 1990) and (1993). The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is l"
2005.mtsummit-papers.36,N04-1033,0,0.157603,"Missing"
2005.mtsummit-papers.36,2005.iwslt-1.23,1,0.857305,"Missing"
2005.mtsummit-papers.36,2004.iwslt-evaluation.14,1,0.687498,"Missing"
2005.mtsummit-papers.36,P05-2012,1,0.78658,"Missing"
2005.mtsummit-papers.36,N03-1017,0,0.067546,"The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is learned from training data. The translation system described in this paper implements a translation"
2005.mtsummit-papers.36,P02-1038,0,0.221853,"otivated by the development of computer resources needed to allow the implementation of translation algorithms based on statistical methods (Brown et al., 1990) and (1993). The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of"
2005.mtsummit-papers.36,N04-1021,0,0.0290838,"length penalization in order to compensate the system preference for short target sentences caused by the presence of the previous target language model. This penalization depends on the total number of words contained in the partial translation hypothesis, and it is computed as follows: wp(Tk ) = exp(number of words in Tk ) (4) where, again, Tk refers to the partial translation hypothesis. The fourth and ﬁfth feature functions correspond to a forward and backward lexicon models. These models provide IBM 1 translation probabilities for each tuple based on the IBM 1 lexical parameters p(t|s) (Och et al., 2004). These lexicon models are computed according to the following equation: pIBM 1 ((t, s)n ) = I J   1 p(tin |sjn ) (5) (I + 1)J j=1 i=0 (3) where sjn and tin are the j th and ith words in the source and target sides of tuple (t, s)n , being J and I the corresponding total number words in each side of it. For computing the forward lexicon model, IBM model 1 lexical parameters from GIZA++ source-to-target alignments are used. In the case of the backward lexicon model, GIZA++ target-to-source alignments are used instead. where Tk refers to the partial translation hypothesis and wn to the nth wor"
2005.mtsummit-papers.36,P02-1040,0,0.115781,"cording to the maximum entropy framework (Berger et al., 1996), the corresponding translation hypothesis T , for a given source sentence S, is deﬁned by the target sentence that maximizes a log-linear combination of feature functions hi (S, T ), as described in the following equation:  argmax λi hi (S, T ) (2) T i where the λi ’s constitute the weighting coeﬃcients of the log-linear combination and the feature function hi (S, T ) corresponds to a logarithmic scaling of the ith -model probabilities. These weights are computed via an optimization procedure which maximizes the translation BLEU (Papineni et al., 2002) over a given development set. This optimization is performed by using an in-house developed optimization algorithm, which is based on a simplex method (Press et al., 2002). 3.2 Implemented Feature Functions The proposed translation system implements a total of ﬁve feature functions: • tuple 3-gram model, • target language model, • word penalty model, • source-to-target lexicon model, and • target-to-source lexicon model. The ﬁrst of these functions is the tuple 3-gram model, which was already described in the previous section. The second feature function implemented is a target language model"
2005.mtsummit-papers.36,2002.tmi-tutorials.2,0,0.0433398,"al., 1990) and (1993). The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is learned from training data. The translation system described in this paper"
2005.mtsummit-papers.36,P00-1056,0,\N,Missing
2005.mtsummit-papers.37,J96-1002,0,0.0120462,"rce sentence f1J is transformed into (or generates) a target sentence eI1 by means of a stochastic process. The translation of a source sentence f1J can be formulated as the search of the target sentence eI1 that maximizes the conditional probability p(eI1 |f1J ), which can be rewritten using the Bayes rule as:  arg max eI1 p(f1J |eI1 ) · p(eI1 )  (1) Alternatively to this classical source channel approach, the posterior probability p(eI1 |f1J ) can be modeled directly as a log-linear combination of feature models (Och and Ney, H., 2002), based on the maximum entropy framework, as shown in (Berger et al., 1996). This simpliﬁes the introduction of several additional models explaining the translation process, as the search becomes:  arg max{exp( eI1 λi hi (e, f ))} (2) i where the feature functions hi are the system models (translation model, language model, reordering model, ...), and the λi weights are typically optimized to maximize a scoring function on a development set. In this work a combination of 4 feature models is used, which include: • a translation Ngram tuple-based model Figure 1: Tuples extraction from a pair of word aligned source sentences. 2.1 Translation model As for the Translatio"
2005.mtsummit-papers.37,N04-1033,0,0.160951,"Missing"
2005.mtsummit-papers.37,2005.iwslt-1.23,1,0.8677,"Missing"
2005.mtsummit-papers.37,2004.iwslt-evaluation.14,1,0.855497,"Missing"
2005.mtsummit-papers.37,koen-2004-pharaoh,0,0.0499024,"means that the target sentence is generated by translating parts of the source sentence in a non-sequential fashion. However, this search can cause a combinatory explosion of the search graph if arbitrary reorderings are allowed, being an NP-hard problem (Knight, 1999). This issue has been tackled in previous work for a phrase-based SMT approach, typically reducing the combinatory explosion of the search space by using a distortion model that penalizes the longest reorderings, which are only allowed if well supported by the other feature models involved in the search (Och and Ney, H., 2004), (Koehn, 2004). In this paper we deal with the question of reordering for an Ngram-based SMT approach with two complementary strategies, namely reordered search and tuple unfolding. These strategies interact to improve translation quality in a Chinese to English task. On the one hand, we introduce reordering capabilities into an Ngram-based decoder. The decoder then performs a reordered search over the source sentence, and combines a translation tuples Ngram model, a target language model, a word penalty and a word distance model. Interestingly, even though the translation units are learnt sequentially, its"
2005.mtsummit-papers.37,P00-1056,0,0.333674,"Missing"
2005.mtsummit-papers.37,P02-1038,0,0.132368,"Missing"
2005.mtsummit-papers.37,J04-4002,0,0.160354,"Missing"
2005.mtsummit-papers.37,2001.mtsummit-papers.68,0,0.0211217,"of the whole training corpus, and reﬁned the links by the union of both alignment directions (Och and Ney, H., 2004). Afterwards we segmented the bilingual sentence pairs of the training set, extracting translation units (tuples) using the extract-tuples method described in (Crego et al., 2004). To train the Ngram models, we used the SRILM toolkit (Stolcke, 2002). The type of discounting algorithm used was the modiﬁed 1 Kneser-Ney combining higher and lower order estimates via interpolation. The weights λi for the log-linear combination of models were set in order to minimize the BLEU score (Papineni et al., 2001) on the development set, using the simplex (Nelder and Mead, 1965) algorithm. All the experiments were performed on a Pentium IV (Xeon 3.06GHz), with 4Gb of RAM memory. www.slt.atr.jp/IWSLT2004 287 5.3 Results Conﬁg. baseline tpl.mon tpl.reo utpl.mon utpl.reo BLEU 28.85 33.14 36.33 33.16 37.82 mWER 53.42 51.5 49.68 50.16 47.31 mPER 42.89 41.53 41.41 41.25 39.9 time 11 333 12 354 Table 3: Translation quality (using BLEU, mWER and mPER) and eﬃciency (measured in decoding time) results. The ﬁrst row shows the results of the baseline TALP system. Table 3 shows the results in BLEU, mWER and mPER, a"
2006.amta-papers.4,J96-1002,0,0.152165,"problem is classified NP-complete (Knight, 1999), while polynomial time search algorithms can be obtained under monotone conditions. The first SMT systems introducing reordering capabilities were founded on the brute force of computers, aiming at finding the best hypothesis through traversing a fully reordered graph (the whole permutations of source-side words are allowed in the search). This approach is computationally very expensive, even for very short input sentences. Therefore, different distance-based reordering constraints must be used to make the search feasible: ITG (Wu, 1996), IBM (Berger et al., 1996), Local (Kanthak et al., 2005), MaxJumps (Crego et al., 2005), etc. The use of these constraints implies a necessary balance between translation accuracy and efficiency. Typically, a distance-based reordering model is used in the search to penalize longer reorderings, only allowed when well supported by the rest of models. Obviously, this model does not follow any property of language. Lexicalized reordering models, which use distance of words seen in train to score reorderings in search, (Koehn et al., 2005) , (Kumar and Byrne, 2005) have also been introduced. A main criticism to this brute f"
2006.amta-papers.4,A00-1031,0,0.033872,"in both sides of an instance (dif < 4). Table 1: TC-Star English-Spanish Parallel corpus statistics. pute the alignments, five iterations for models IBM1 and HMM, and three iterations for models IBM3 and IBM4, were performed. Then, a tuple set for each translation direction was extracted from the union set of alignments. The resulting tuple vocabularies were pruned considering the N best translations for each tuple source-side (N = 30 for the English-to-Spanish and N = 20 for the Spanish-to-English). The English side of the training corpus was POS tagged using the freely available TNT tagger (Brants, 2000), for the Spanish side we used the freely available Freeling (Carreras et al., 2004). Only the first two characters of each tag were used for the Spanish side, aiming at achieving a higher level of generalization. We used the SRI Language modelling toolkit (Stolcke, 2002) to compute the three Ngram language models, using respectively 4, 5 and 5 as ngram orders for the translation, target and tagged target models. Once the models were computed, sets of optimal log-linear coefficients were estimated for each translation direction and system configuration using an in-house implementation of the w"
2006.amta-papers.4,carreras-etal-2004-freeling,0,0.0240478,"Missing"
2006.amta-papers.4,P05-1033,0,0.0121688,"main criticism to this brute force approach is that it does not make use of any linguistic information, while in linguistic theory, reorderings between linguistic phrases in different language pairs are well described. Lately, some SMT systems have introduced linguistic information in order to tackle the reordering 29 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 29-36, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas problem: • Reordering encoded within translation units in form of hierarchical units (Chiang, 2005), or phrases with gaps (Simard et al., 2005). • Word order monotonization (in train and/or test). Consisting of learning reorderings into the source side to achieve a similar word order to that of the target side (Collins et al., 2005), (Xia and McCord, 2004). We found specially interesting the work in (Xia and McCord, 2004), where reorderings are applied following a set of patterns which are automatically learned using lexical, syntactical and morphological information (words, parse trees, and POS tags). In test, a monotone search is applied after reordering the source words using the learnt"
2006.amta-papers.4,P05-1066,0,0.0703591,"some SMT systems have introduced linguistic information in order to tackle the reordering 29 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 29-36, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas problem: • Reordering encoded within translation units in form of hierarchical units (Chiang, 2005), or phrases with gaps (Simard et al., 2005). • Word order monotonization (in train and/or test). Consisting of learning reorderings into the source side to achieve a similar word order to that of the target side (Collins et al., 2005), (Xia and McCord, 2004). We found specially interesting the work in (Xia and McCord, 2004), where reorderings are applied following a set of patterns which are automatically learned using lexical, syntactical and morphological information (words, parse trees, and POS tags). In test, a monotone search is applied after reordering the source words using the learnt patterns. In this work we follow a similar strategy to learn reordering patterns but aiming at reducing the search graph. Our goal is double. On the one hand we add some linguistic information to the problem of guessing which reorderin"
2006.amta-papers.4,W05-0831,0,0.0449427,"plete (Knight, 1999), while polynomial time search algorithms can be obtained under monotone conditions. The first SMT systems introducing reordering capabilities were founded on the brute force of computers, aiming at finding the best hypothesis through traversing a fully reordered graph (the whole permutations of source-side words are allowed in the search). This approach is computationally very expensive, even for very short input sentences. Therefore, different distance-based reordering constraints must be used to make the search feasible: ITG (Wu, 1996), IBM (Berger et al., 1996), Local (Kanthak et al., 2005), MaxJumps (Crego et al., 2005), etc. The use of these constraints implies a necessary balance between translation accuracy and efficiency. Typically, a distance-based reordering model is used in the search to penalize longer reorderings, only allowed when well supported by the rest of models. Obviously, this model does not follow any property of language. Lexicalized reordering models, which use distance of words seen in train to score reorderings in search, (Koehn et al., 2005) , (Kumar and Byrne, 2005) have also been introduced. A main criticism to this brute force approach is that it does"
2006.amta-papers.4,J99-4005,0,0.0800766,"-toSpanish). Results are presented regarding translation accuracy (using human and automatic evaluations) and computational efficiency, showing significant improvements in translation quality for both translation directions at a very low computational cost. 1 Introduction In statistical machine translation, the use of reordering strategies allows for an important improvement in translation accuracy, specially when translating between language pairs with high disparity in word order. On the other hand, when arbitrary word reorderings are permitted, the search problem is classified NP-complete (Knight, 1999), while polynomial time search algorithms can be obtained under monotone conditions. The first SMT systems introducing reordering capabilities were founded on the brute force of computers, aiming at finding the best hypothesis through traversing a fully reordered graph (the whole permutations of source-side words are allowed in the search). This approach is computationally very expensive, even for very short input sentences. Therefore, different distance-based reordering constraints must be used to make the search feasible: ITG (Wu, 1996), IBM (Berger et al., 1996), Local (Kanthak et al., 2005"
2006.amta-papers.4,2005.iwslt-1.8,0,0.0223291,"eordering constraints must be used to make the search feasible: ITG (Wu, 1996), IBM (Berger et al., 1996), Local (Kanthak et al., 2005), MaxJumps (Crego et al., 2005), etc. The use of these constraints implies a necessary balance between translation accuracy and efficiency. Typically, a distance-based reordering model is used in the search to penalize longer reorderings, only allowed when well supported by the rest of models. Obviously, this model does not follow any property of language. Lexicalized reordering models, which use distance of words seen in train to score reorderings in search, (Koehn et al., 2005) , (Kumar and Byrne, 2005) have also been introduced. A main criticism to this brute force approach is that it does not make use of any linguistic information, while in linguistic theory, reorderings between linguistic phrases in different language pairs are well described. Lately, some SMT systems have introduced linguistic information in order to tackle the reordering 29 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 29-36, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas problem: • Reordering encoded w"
2006.amta-papers.4,H05-1021,0,0.0187456,"ust be used to make the search feasible: ITG (Wu, 1996), IBM (Berger et al., 1996), Local (Kanthak et al., 2005), MaxJumps (Crego et al., 2005), etc. The use of these constraints implies a necessary balance between translation accuracy and efficiency. Typically, a distance-based reordering model is used in the search to penalize longer reorderings, only allowed when well supported by the rest of models. Obviously, this model does not follow any property of language. Lexicalized reordering models, which use distance of words seen in train to score reorderings in search, (Koehn et al., 2005) , (Kumar and Byrne, 2005) have also been introduced. A main criticism to this brute force approach is that it does not make use of any linguistic information, while in linguistic theory, reorderings between linguistic phrases in different language pairs are well described. Lately, some SMT systems have introduced linguistic information in order to tackle the reordering 29 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 29-36, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas problem: • Reordering encoded within translation units in"
2006.amta-papers.4,2005.mtsummit-papers.36,1,0.899477,"Missing"
2006.amta-papers.4,2005.eamt-1.25,0,0.0445366,". In test, a monotone search is applied after reordering the source words using the learnt patterns. In this work we follow a similar strategy to learn reordering patterns but aiming at reducing the search graph. Our goal is double. On the one hand we add some linguistic information to the problem of guessing which reorderings must be applied (achieving generalization power through using POS tags instead of words). On the other hand, the final decision about reordering is taken in decoding time, when all the information is available (not just reordering patterns but the whole SMT models). In (Matusov et al., 2005) a similar work can be found, where search graphs are restricted without linguistic motivation but using monotonic sequences seen in training. The paper is organized as follows. In section 2 we review the translation system used in this work. Section 3 introduces the reordering framework proposed, giving details of the method used to extract reordering patterns, and how reorderings are supplied to the decoder in form of a reordering graph. Section 4 presents the experiments conducted to test the efectiveness of using the new reordering framework. Finally, Conclusion and further work are outlin"
2006.amta-papers.4,H05-1095,0,0.0184552,"pproach is that it does not make use of any linguistic information, while in linguistic theory, reorderings between linguistic phrases in different language pairs are well described. Lately, some SMT systems have introduced linguistic information in order to tackle the reordering 29 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 29-36, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas problem: • Reordering encoded within translation units in form of hierarchical units (Chiang, 2005), or phrases with gaps (Simard et al., 2005). • Word order monotonization (in train and/or test). Consisting of learning reorderings into the source side to achieve a similar word order to that of the target side (Collins et al., 2005), (Xia and McCord, 2004). We found specially interesting the work in (Xia and McCord, 2004), where reorderings are applied following a set of patterns which are automatically learned using lexical, syntactical and morphological information (words, parse trees, and POS tags). In test, a monotone search is applied after reordering the source words using the learnt patterns. In this work we follow a similar s"
2006.amta-papers.4,P96-1021,0,0.0901638,"tted, the search problem is classified NP-complete (Knight, 1999), while polynomial time search algorithms can be obtained under monotone conditions. The first SMT systems introducing reordering capabilities were founded on the brute force of computers, aiming at finding the best hypothesis through traversing a fully reordered graph (the whole permutations of source-side words are allowed in the search). This approach is computationally very expensive, even for very short input sentences. Therefore, different distance-based reordering constraints must be used to make the search feasible: ITG (Wu, 1996), IBM (Berger et al., 1996), Local (Kanthak et al., 2005), MaxJumps (Crego et al., 2005), etc. The use of these constraints implies a necessary balance between translation accuracy and efficiency. Typically, a distance-based reordering model is used in the search to penalize longer reorderings, only allowed when well supported by the rest of models. Obviously, this model does not follow any property of language. Lexicalized reordering models, which use distance of words seen in train to score reorderings in search, (Koehn et al., 2005) , (Kumar and Byrne, 2005) have also been introduced. A mai"
2006.amta-papers.4,C04-1073,0,0.109977,"ntroduced linguistic information in order to tackle the reordering 29 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 29-36, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas problem: • Reordering encoded within translation units in form of hierarchical units (Chiang, 2005), or phrases with gaps (Simard et al., 2005). • Word order monotonization (in train and/or test). Consisting of learning reorderings into the source side to achieve a similar word order to that of the target side (Collins et al., 2005), (Xia and McCord, 2004). We found specially interesting the work in (Xia and McCord, 2004), where reorderings are applied following a set of patterns which are automatically learned using lexical, syntactical and morphological information (words, parse trees, and POS tags). In test, a monotone search is applied after reordering the source words using the learnt patterns. In this work we follow a similar strategy to learn reordering patterns but aiming at reducing the search graph. Our goal is double. On the one hand we add some linguistic information to the problem of guessing which reorderings must be applied (achi"
2006.iwslt-evaluation.17,W05-0820,0,0.0116848,", namely from Arabic, Chinese, Italian and Japanese into English for the open data track, thoroughly explaining all language-related preprocessing and optimization schemes. 1. Introduction Rooted in the Finite-State Transducers approach to SMT [1, 2] and estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternative to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3]. This is specially true when dealing with pairs of languages with a relatively similar word order [4, 5]. Given the language pairs involved in this year’s evaluation, efforts have been focused on improving the word reordering strategies for Ngram-based SMT. Specifically, a novel reordering strategy based on extending the search graph with automatically-extracted reordering patterns is explored. Results are very promising while keeping computational expenses at a similar level of monotone search. Additionally, a novel tuple segmentation strategy based on the entropy of Part-Of-Speech distributions was used with slight improvements in model estimation. This paper is organized as follows. Section 2"
2006.iwslt-evaluation.17,W06-3114,0,0.0159274,", namely from Arabic, Chinese, Italian and Japanese into English for the open data track, thoroughly explaining all language-related preprocessing and optimization schemes. 1. Introduction Rooted in the Finite-State Transducers approach to SMT [1, 2] and estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternative to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3]. This is specially true when dealing with pairs of languages with a relatively similar word order [4, 5]. Given the language pairs involved in this year’s evaluation, efforts have been focused on improving the word reordering strategies for Ngram-based SMT. Specifically, a novel reordering strategy based on extending the search graph with automatically-extracted reordering patterns is explored. Results are very promising while keeping computational expenses at a similar level of monotone search. Additionally, a novel tuple segmentation strategy based on the entropy of Part-Of-Speech distributions was used with slight improvements in model estimation. This paper is organized as follows. Section 2"
2006.iwslt-evaluation.17,2005.mtsummit-papers.36,1,0.472143,"g patterns. Section 4 focuses on tuple segmentation strategies, and contrasts the criterion on IBM model 1 probabilities from 2005 with a novel criterion based on Part-Of-Speech entropy distributions. Later on, Section 5 reports on all experiments carried out from Arabic, Chinese, Italian and Japanese into English for IWSLT 2006. Finally, Section 6 sums up the main conclusions from the paper and discusses future research lines. 2. 2005 system review The TALP Ngram-based SMT system performs a log-linear combination of a translation model and additional feature functions (see further details in [6, 7]). In contrast to phrasebased models, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. This way it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(sJ1 , tI1 ) = K Y p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (1) i=1 where (s, t)i refers to the ith tuple of a sentence pair being segmented into K tuples. A detailed comparison between Ngram-based and phrase-based SMT can be found in [8]. 2.1. Tuple extraction Given a certain word-aligned parallel"
2006.iwslt-evaluation.17,2005.iwslt-1.23,1,0.868702,"l and additional feature functions (see further details in [6, 7]). In contrast to phrasebased models, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. This way it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(sJ1 , tI1 ) = K Y p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (1) i=1 where (s, t)i refers to the ith tuple of a sentence pair being segmented into K tuples. A detailed comparison between Ngram-based and phrase-based SMT can be found in [8]. 2.1. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints 116 3. Word ordering strategies 2.2. Feature functions As additional feature functions to better guide the translation process, the system incorporates a target language model, a word bonus model and two lexicon models. The target language model is estimated"
2006.iwslt-evaluation.17,N04-1033,0,0.120187,"ated as a standard n-gram model of a bilingual language expressed in tuples. This way it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(sJ1 , tI1 ) = K Y p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (1) i=1 where (s, t)i refers to the ith tuple of a sentence pair being segmented into K tuples. A detailed comparison between Ngram-based and phrase-based SMT can be found in [8]. 2.1. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints 116 3. Word ordering strategies 2.2. Feature functions As additional feature functions to better guide the translation process, the system incorporates a target language model, a word bonus model and two lexicon models. The target language model is estimated as a standard ngram over the target words, as follows: pLM (tk ) ≈ k Y p(wn |wn−N +1 , ..., wn−1 ) (2) n=1 where tk refers to the par"
2006.iwslt-evaluation.17,2005.mtsummit-papers.37,1,0.788658,"n |wn−N +1 , ..., wn−1 ) (2) n=1 where tk refers to the partial hypothesis and wn to the nth word in it. Usually, this feature is accompanied by a word bonus model based on sentence length, compensating the target language model preference for short sentences (in number of target words). This bonus depends on the number of target words in the partial hypothesis, denoted as: pW P (tk ) = exp(number of words in tk ) When dealing with pairs of languages with non-monotonic word order, a certain reordering strategy is required. Apart from that, tuples need to be extracted by an unfolding technique [11]. This means that the tuples are broken into smaller tuples, and these are sequenced in the order of the target words. In order not to lose the information on the correct order, the decoder performs then a reordered search (or a monotone search extended with reordering paths), which is guided by the n-gram model of the unfolded tuples and the additional feature models. Figure 1 shows an example of tuple unfolding compared to the monotonic extraction. The unfolding technique produces a different bilingual n-gram language model with reordered source words. (3) where tk refers to the partial hypo"
2006.iwslt-evaluation.17,A00-1031,0,0.0103336,"Language-dependent preprocessing For all language pairs, training sentences were split by using final dots on both sides of the bilingual text (when the number of dots was equal), increasing the number of sentences and reducing its length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. English Table 1: Arabic→English corpus statistics. sent. it en it it it it wrds 155k 166k 5,193 2,807 5,978 5,767 refs. 1 7 16 7 7 Table 2: Chinese→English corpus statistics. English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [15] and lemmatization using wnmorph, included in the WordNet package [16]. The English Penn Treebank Tag Set used contains 36 different tags. 5.2.2. Arabic Following a similar approach to that in [17], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [18], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronom"
2006.iwslt-evaluation.17,N06-2013,0,0.0357882,"mber of sentences and reducing its length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. English Table 1: Arabic→English corpus statistics. sent. it en it it it it wrds 155k 166k 5,193 2,807 5,978 5,767 refs. 1 7 16 7 7 Table 2: Chinese→English corpus statistics. English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [15] and lemmatization using wnmorph, included in the WordNet package [16]. The English Penn Treebank Tag Set used contains 36 different tags. 5.2.2. Arabic Following a similar approach to that in [17], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [18], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. The Arabic Treebank tag set used contains 20 different tags. Corpora statistics for all language pairs can be found in 2 Ver"
2006.iwslt-evaluation.17,P05-1071,0,0.0218458,"n it it it it wrds 155k 166k 5,193 2,807 5,978 5,767 refs. 1 7 16 7 7 Table 2: Chinese→English corpus statistics. English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [15] and lemmatization using wnmorph, included in the WordNet package [16]. The English Penn Treebank Tag Set used contains 36 different tags. 5.2.2. Arabic Following a similar approach to that in [17], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [18], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. The Arabic Treebank tag set used contains 20 different tags. Corpora statistics for all language pairs can be found in 2 Version 119 2.0. Linguistic Data Consortium Catalog: LDC2004L02. 5.2.3. Chinese official Chinese preprocessing included resegmentation and POStagging. These tasks were done by using ICTCLAS [19]. Resultan"
2006.iwslt-evaluation.17,W03-1730,0,0.0274114,"Missing"
2006.iwslt-evaluation.17,2004.iwslt-evaluation.14,1,\N,Missing
2006.iwslt-evaluation.17,E99-1010,0,\N,Missing
2006.iwslt-evaluation.17,J96-1002,0,\N,Missing
2006.iwslt-evaluation.17,W05-0823,1,\N,Missing
2006.iwslt-evaluation.17,N07-2022,1,\N,Missing
2006.iwslt-evaluation.17,2005.iwslt-1.1,0,\N,Missing
2006.iwslt-evaluation.17,J06-4004,1,\N,Missing
2006.iwslt-evaluation.17,N03-1017,0,\N,Missing
2006.iwslt-evaluation.17,J03-1002,0,\N,Missing
2006.iwslt-evaluation.17,2006.iwslt-evaluation.1,0,\N,Missing
2006.iwslt-evaluation.17,2005.iwslt-1.24,1,\N,Missing
2006.iwslt-evaluation.17,2006.iwslt-papers.2,1,\N,Missing
2006.iwslt-evaluation.17,atserias-etal-2006-freeling,0,\N,Missing
2006.iwslt-evaluation.17,2005.iwslt-1.11,0,\N,Missing
2006.iwslt-evaluation.17,P00-1056,0,\N,Missing
2006.iwslt-evaluation.17,2006.iwslt-papers.5,1,\N,Missing
2006.iwslt-evaluation.18,2005.iwslt-1.24,1,0.787344,"TALP phrase-based statistical machine translation system, enriched with the statistical machine reordering technique. We also report the combination of this system and the TALP-tuple, the n-gram-based statistical machine translation system. We report the results for all the tasks (Chinese, Arabic, Italian and Japanese to English) in the framework of the third evaluation campaign of the International Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report th"
2006.iwslt-evaluation.18,W06-1609,1,0.921033,"e combination of this system and the TALP-tuple, the n-gram-based statistical machine translation system. We report the results for all the tasks (Chinese, Arabic, Italian and Japanese to English) in the framework of the third evaluation campaign of the International Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to Eng"
2006.iwslt-evaluation.18,W06-3125,1,0.836838,"ional Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to English. 2. Description of the TALP-phrase System 2.1. Phrase-based Model The basic idea of phrase-based translation is to segment the given source sentence into units (here called phrases), then translate each phrase and finally compose the target sentence from th"
2006.iwslt-evaluation.18,P02-1038,0,0.0577834,"re consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. 2.2. Feature functions The baseline phrase-based system implements a log-linear combination of four feature functions, which are described as follows. • The translation model is estimated with relative frequencies. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency in both directions. (1) The feature functions, hm , and weights, λi , are typically optimized to maximize the scoring function [4]. Two basic issues differentiate the n-gram-based system from the phrase-based system: the bilingual units are extracted from a monotonic segmentation of the training data; the unit probabilities are based on a standard back-off language model rather than directly on relative frequencies. In both systems, the introduction of reordering capabilities is crucial for certain language pairs. This paper is organized as follows. Section 2 describes the TALP-phrase system, with particular emphasis on a new reordering technique: the statistical machine reordering approach. In Section 3, we combine the"
2006.iwslt-evaluation.18,J04-4002,0,0.0302302,"X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to English. 2. Description of the TALP-phrase System 2.1. Phrase-based Model The basic idea of phrase-based translation is to segment the given source sentence into units (here called phrases), then translate each phrase and finally compose the target sentence from these phrase translations. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [5]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. 2.2. Feature functions The baseline phrase-based system implements a log-linear combination of four feature functions, which are described as follows. • The translation model is estimated with relative frequencies. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequen"
2006.iwslt-evaluation.18,2005.iwslt-1.6,0,0.0322379,"using the GIZA++ tool [7]. During word alignment, we used 50 classes per language. We aligned both translation directions and combined the two alignments with the union operation. train dev4 dev123 test ASRtest • Word classes (which were used to help the aligner and to perform the SMR process) were determined using “mkcls”, a tool freely-available with GIZA++. • The language model was estimated using the SRILM toolkit [8]. • The decoder was MARIE [9]. • The optimization tool used for computing log-linear weights was based on the simplex method [6]. Following the consensus strategy proposed in [10], the objective function was set to 100 · BLEU + 4 · N IST . 489 500 500 500 voc. 9.7k 9.6k 1,096 909 1,292 1,311 slen. 6.7 7.0 11.2 6.0 11.7 11.6 refs. 1 7 16 7 7 Corpus statistics for all language pairs can be found in Tables 1, 2, 3 and 4, respectively, where number of sentences, running words, vocabulary, sentence length and human references are shown. sent. train dev4 dev123 test ASRtest Experiments were carried out for all tasks of the IWSLT06 evaluation (Zh2En, Jp2En, Ar2En and It2En) using the BTEC Corpus provided for the open data track1 . it en it it it it 24.6k 489 500 500 500 wrds"
2006.iwslt-evaluation.18,A00-1031,0,0.0229282,"while randomly selecting 500 sentences from developments 1, 2 and 3 (around 160 sentences from each) to build an internal test set (dev123). zh en zh zh zh zh 4.4. Language-dependent preprocessing For all language pairs, training sentences were split by using full stops on both sides of the bilingual text (when the number of stops was equal), increasing the number of sentences and reducing their length. Specific preprocessing for each language is detailed in the respective section below. 4.4.1. English English preprocessing includes part-of-speech tagging using the freely-available TnT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly p"
2006.iwslt-evaluation.18,N06-2013,0,0.0411805,"ing their length. Specific preprocessing for each language is detailed in the respective section below. 4.4.1. English English preprocessing includes part-of-speech tagging using the freely-available TnT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. search (with m = 5 and j = 3) for all tasks and for all systems (with or without SMR technique); except for the Italian to E"
2006.iwslt-evaluation.18,P05-1071,0,0.0382995,"nT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. search (with m = 5 and j = 3) for all tasks and for all systems (with or without SMR technique); except for the Italian to English task where a monotonic search was used. The primary system of each task is that which had the best performance in the internal test. In all tasks, the SMR improved the results in the internal te"
2006.iwslt-evaluation.18,W03-1730,0,0.0516051,"Missing"
2006.iwslt-evaluation.18,atserias-etal-2006-freeling,0,0.0138517,"for the internal test set (specially, for the Arabic and Japanese tasks). The higher the number of unknown words, the worse the SMR output and, consequently, the quality of translation. Here, a possible solution would be to predict word classes for unknown words in order to avoid their bad influence in the SMR output. 4.4.3. Chinese Set development test evaluation Chinese preprocessing included re-segmentation and POStagging. These tasks were performed using ICTCLAS [15]. 4.4.4. Italian Italian was POS-tagged and lemmatized using the freelyavailable FreeLing morpho-syntactic analysis package [16]. Additionally, Italian contracted prepositions were separated into preposition + article, for example ’alla’→’a la’, ’degli’→’di gli’ or ’dallo’→’da lo’. 4.4.5. Japanese When dealing with Japanese, one has to come up with new methods for overcoming the absence of delimiters between words. We addressed this issue by word segmentation using the freely available JUMAN tool [17] version 5.1. This tool was also used for POS-tagging of the Japanese text. 4.5. Results In Table 6 we show the results for all the TALP systems that participated in the IWSLT 2006: the TALP-phrase, the TALP-tuple and the"
2006.iwslt-evaluation.18,2004.iwslt-evaluation.8,0,\N,Missing
2007.iwslt-1.26,2006.iwslt-papers.2,1,0.882609,"4]. Efforts have been focused on improving translation according to human evaluation by further developing different stages of the SMT system: alignment and rescoring. As in previous years, we aligned the training corpus using Giza++ software. However, instead of keeping the default parameters, we performed a minimum translation error training procedure to adjust Giza++ smoothing parameters to the task. This procedure had been successful with an alignment system based on discriminative training [5]. For the rescoring we incorporate a neural network language model as previously experienced in [6]. The neural network language model mainly is able to produce a better generalization in the translation system. This paper is organized as follows. Section 2 briefly reviews last year’s system, including tuple definition and extraction, translation model and feature functions, decoding tool and reordering and optimization criterion. Section 3 describes the alignment translation-minimum-error training procedure. Section 4 focuses on rescoring using a neural language model (NNLM). Next, Section 5 reports on all experiments carried out from Arabic and Chinese into English for IWSLT 2007. Finally"
2007.iwslt-1.26,N04-1033,0,0.0623584,"ls, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. In this way, it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(S, T ) = K Y p((˜ s, t˜)k |(˜ s, t˜)k−N +1 , ..., (˜ s, t˜)k−1 ) (1) k=1 where s refers to source, t to target, and (˜ s, t˜)k to the k th tuple of a given bilingual sentence pair segmented in K tuples. 2.2. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints However, when dealing with pairs of languages with nonmonotonic word order, a certain reordering strategy is required to extract more reusable units (less sparse). Hence, we allow the source words to be reordered before extracting translation units from training sentence pairs by following the word-to-word alignments. The unfolding technique is fully described in [10]. Figure 1 shows an ex"
2007.iwslt-1.26,2005.mtsummit-papers.37,1,0.856996,"following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints However, when dealing with pairs of languages with nonmonotonic word order, a certain reordering strategy is required to extract more reusable units (less sparse). Hence, we allow the source words to be reordered before extracting translation units from training sentence pairs by following the word-to-word alignments. The unfolding technique is fully described in [10]. Figure 1 shows an example of tuple unfolding compared to the monotonic extraction. The unfolding technique produces a different bilingual n-gram language model with reordered source words. where tn refers to the nth word in the partial translation hypothesis T . Usually, this feature is accompanied by a word bonus model based on sentence length, compensating the target language model preference for short sentences (in number of target words). This bonus depends on the number of target words in the partial hypothesis, denoted as: pW P (T ) = exp(number of words in T ). The third and fourth fe"
2007.iwslt-1.26,2006.iwslt-papers.5,1,0.867108,"ChineseEnglish task, a secondary run was performed with a rescoring module, as described in Sections 4 and 5.3.2. 2.5. Feature Weights Optimization To tune the weight of each feature function in the SMT system, we used the Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm [12]. SPSA is a stochastic implementation of the conjugate gradient method which requires only two evaluations of the objective function in each iteration, regardless of the dimension of the optimization problem. It was observed to be more robust than the Downhill Simplex method when tuning SMT coefficients [13]. The SPSA procedure is in the general recursive stochastic approximation form: ˆ k+1 = λ ˆ k − ak g ˆk ) ˆk (λ λ (5) lation tuples (as no word within a tuple can be linked to a word out of it [9]). Starting from the monotonic graph, each sequence of input POS tags fulfilling a source-side rewrite rule implies the addition of a reordering arc (which encodes the reordering detailed in the target-side of the rule). Figure 2 shows how three rewrite rules applied over an input sentence extend the search graph given the reordering patterns that match the source POS tag sequence 1 . ˆ k ) is the esˆ"
2007.iwslt-1.26,N07-2022,1,0.823419,"o phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3, 4]. Efforts have been focused on improving translation according to human evaluation by further developing different stages of the SMT system: alignment and rescoring. As in previous years, we aligned the training corpus using Giza++ software. However, instead of keeping the default parameters, we performed a minimum translation error training procedure to adjust Giza++ smoothing parameters to the task. This procedure had been successful with an alignment system based on discriminative training [5]. For the rescoring we incorporate a neural network language model as previously experienced in [6]. The neural network language model mainly is able to produce a better generalization in the translation system. This paper is organized as follows. Section 2 briefly reviews last year’s system, including tuple definition and extraction, translation model and feature functions, decoding tool and reordering and optimization criterion. Section 3 describes the alignment translation-minimum-error training procedure. Section 4 focuses on rescoring using a neural language model (NNLM). Next, Section 5"
2007.iwslt-1.26,N06-2013,0,0.076004,"fluency and METEOR is well correlated to adequacy [4], we supposed that adding all references was beneficial to monolingual language models but not to the bilingual language model. Table 2: Chinese→English corpus statistics. 5.2. Data Preprocessing For all language pairs, training sentences were split by using final dots on both sides of the bilingual text (when the number of dots was equal), increasing the number of sentences and reducing its length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. Arabic Following a similar approach to that in [16], we used the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available"
2007.iwslt-1.26,W03-1730,0,0.018621,"ts length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. Arabic Following a similar approach to that in [16], we used the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal"
2007.iwslt-1.26,A00-1031,0,0.0277074,"he MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results i"
2007.iwslt-1.26,E99-1010,0,0.0816899,"ion produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results in all cases, and the best combination for the system with pattern-based reordering was the union6 . At the end, the best alignment configuration for our baseline system was obtained with Giza++ software, running respectively 5, 5, 3 and 3 iterations of models 1, HMM, 3 and 4, using English stems and 50"
2007.iwslt-1.26,J03-1002,0,0.00791937,"y available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results in all cases, and the best combination for the system with pattern-based reordering was the union6 . At the end, the best alignment configuration for our baseline system was obtained with Giza++ software, running respectively 5, 5, 3 and 3 iterations of models 1, HMM, 3 and 4, using English stems and 50 classes and taking the union of source-target and target-source alignments. Table 3 show results for the new features of this year’s system. We optimized the foll"
2007.mtsummit-papers.16,J90-2002,0,0.364965,"d on the BTEC corpus (Chinese to English task) Results are presented regarding translation accuracy and computational efficiency, showing significant improvements in translation quality at a reasonable computational cost. 1 Introduction In the statistical machine translation (SMT) community, it is widely accepted the need for structural information to account for mappings between the different language pairs. These mappings offer a greater potential to learn generalizations about relationships between languages than flat-structured models, such as the word-based IBM models of the early 1990s (Brown et al., 1990) or the more recent phrasebased models (Zens et al., 2002; Marcu and Wong, 2002; Koehn et al., 2003), which to date remain widely used. The need for structural information is specially relevant when handling language pairs with very different word order (such as Chinese-English), because the flatstructured models fail to derive generalizations from the training corpus. Several alternatives have been proposed in the recent years to boost the use of syntactic information in SMT systems. They range from those aiming at monotonizing the word order of the considered language pairs by means of a set"
2007.mtsummit-papers.16,P05-1033,0,0.0864727,"ased system), what allows to further improve translation accuracy by avoiding some of the errors performed in the monotonization preprocessing step. (Collins et al., 2005) proposes a set of hand-crafted reordering rules for a German-English translation task. The second group (syntax-directed) has gained many adepts in the last few years because of the significant improvements made by exploiting the power of synchronous rewriting systems. These systems employ source and/or target dependency (Quirk et al., 2005; Langlais and Gotti, 2006) or constituent trees, which can be formally syntax-based (Chiang, 2005; Watanabe et al., 2006) or linguistically syntaxbased (Yamada and Knight, 2002; Wu, 1997; Marcu et al., 2006). A main criticism to the first group is that it has shown a relatively good performance when tackling language pairs with reduced reordering needs (such as Spanish-English or FrenchEnglish). On the other hand, syntax-directed systems show a main weakness on their poor efficiency results, recently overrided by the apparition of new decoders, which show significant improvements when handling with syntactically divergent language pairs, under large-scale data translation tasks. An exampl"
2007.mtsummit-papers.16,P05-1066,0,0.36496,"Missing"
2007.mtsummit-papers.16,W06-1609,0,0.0535677,"alternatives have been proposed in the recent years to boost the use of syntactic information in SMT systems. They range from those aiming at monotonizing the word order of the considered language pairs by means of a set of linguistically-based reordering patterns (though, reducing the reordering needs in the overall search), to others considering translation as a synchronous parsing process, where reorderings introduced in the overall search are syntactically motivated. Among the first group (word order monotonization) we can find (Xia and McCord, 2004), (Collins et al., 2005), (Costa-juss` a and Fonollosa, 2006) or (Popovic and Ney, 2006). They modify the source language word order before decoding in order to acquire the word order of the target language. Then, the reordered source sentence is sent to a standard phrasebased decoder to be translated under monotonic conditions. In (Crego and Mari˜ no, 2006) the same idea is enhanced by coupling reordering and decoding (using an N -gram-based system), what allows to further improve translation accuracy by avoiding some of the errors performed in the monotonization preprocessing step. (Collins et al., 2005) proposes a set of hand-crafted reordering rules"
2007.mtsummit-papers.16,P07-2054,1,0.747839,"Missing"
2007.mtsummit-papers.16,2005.iwslt-1.23,1,0.868481,"Missing"
2007.mtsummit-papers.16,2005.mtsummit-papers.37,1,0.897802,"Missing"
2007.mtsummit-papers.16,N03-1017,0,0.0632272,"Missing"
2007.mtsummit-papers.16,W06-3106,0,0.0168258,"6) the same idea is enhanced by coupling reordering and decoding (using an N -gram-based system), what allows to further improve translation accuracy by avoiding some of the errors performed in the monotonization preprocessing step. (Collins et al., 2005) proposes a set of hand-crafted reordering rules for a German-English translation task. The second group (syntax-directed) has gained many adepts in the last few years because of the significant improvements made by exploiting the power of synchronous rewriting systems. These systems employ source and/or target dependency (Quirk et al., 2005; Langlais and Gotti, 2006) or constituent trees, which can be formally syntax-based (Chiang, 2005; Watanabe et al., 2006) or linguistically syntaxbased (Yamada and Knight, 2002; Wu, 1997; Marcu et al., 2006). A main criticism to the first group is that it has shown a relatively good performance when tackling language pairs with reduced reordering needs (such as Spanish-English or FrenchEnglish). On the other hand, syntax-directed systems show a main weakness on their poor efficiency results, recently overrided by the apparition of new decoders, which show significant improvements when handling with syntactically diverg"
2007.mtsummit-papers.16,W02-1018,0,0.0227728,"translation accuracy and computational efficiency, showing significant improvements in translation quality at a reasonable computational cost. 1 Introduction In the statistical machine translation (SMT) community, it is widely accepted the need for structural information to account for mappings between the different language pairs. These mappings offer a greater potential to learn generalizations about relationships between languages than flat-structured models, such as the word-based IBM models of the early 1990s (Brown et al., 1990) or the more recent phrasebased models (Zens et al., 2002; Marcu and Wong, 2002; Koehn et al., 2003), which to date remain widely used. The need for structural information is specially relevant when handling language pairs with very different word order (such as Chinese-English), because the flatstructured models fail to derive generalizations from the training corpus. Several alternatives have been proposed in the recent years to boost the use of syntactic information in SMT systems. They range from those aiming at monotonizing the word order of the considered language pairs by means of a set of linguistically-based reordering patterns (though, reducing the reordering n"
2007.mtsummit-papers.16,W06-1606,0,0.0452889,"Missing"
2007.mtsummit-papers.16,J06-4004,1,0.904196,"Missing"
2007.mtsummit-papers.16,popovic-ney-2006-pos,0,0.0324834,"posed in the recent years to boost the use of syntactic information in SMT systems. They range from those aiming at monotonizing the word order of the considered language pairs by means of a set of linguistically-based reordering patterns (though, reducing the reordering needs in the overall search), to others considering translation as a synchronous parsing process, where reorderings introduced in the overall search are syntactically motivated. Among the first group (word order monotonization) we can find (Xia and McCord, 2004), (Collins et al., 2005), (Costa-juss` a and Fonollosa, 2006) or (Popovic and Ney, 2006). They modify the source language word order before decoding in order to acquire the word order of the target language. Then, the reordered source sentence is sent to a standard phrasebased decoder to be translated under monotonic conditions. In (Crego and Mari˜ no, 2006) the same idea is enhanced by coupling reordering and decoding (using an N -gram-based system), what allows to further improve translation accuracy by avoiding some of the errors performed in the monotonization preprocessing step. (Collins et al., 2005) proposes a set of hand-crafted reordering rules for a German-English trans"
2007.mtsummit-papers.16,takezawa-etal-2002-toward,0,0.0183881,"e rule extraction. Each sequence of source words is hypothesized as reordering sequence. Hence, for each sequence, its parse subtree is identified and seeked in the training set of reordering rules (the successively pruned versions of the subtree are also searched). If the subtree (or pruned subtrees) exist in the set of rules, the input graph will be extended with a new reordering path, following the word order indicated in the reordering rule. 4 Experiments In this section we report on the experimental work carried out in this paper. 4.1 Experimental framework We have used the BTEC2 corpus (Takezawa et al., 2002) from Chinese to English. It consists exactly of the corpus used in the IWSLT 2006 evaluation campaign as training and development sets. We have used two disjoint sets of the official Development set to build our Dev and Test sets. Table 1 shows the main statistics of the used data, namely number of sentences, words, vocabulary, average sentence length and number of references for each language. The training data was preprocessed by using standard tools for tokenizing and filtering. Word-to-word alignments were computed using GIZA++3 . The union of both alignment directions was used as startin"
2007.mtsummit-papers.16,P06-1098,0,0.0244065,"what allows to further improve translation accuracy by avoiding some of the errors performed in the monotonization preprocessing step. (Collins et al., 2005) proposes a set of hand-crafted reordering rules for a German-English translation task. The second group (syntax-directed) has gained many adepts in the last few years because of the significant improvements made by exploiting the power of synchronous rewriting systems. These systems employ source and/or target dependency (Quirk et al., 2005; Langlais and Gotti, 2006) or constituent trees, which can be formally syntax-based (Chiang, 2005; Watanabe et al., 2006) or linguistically syntaxbased (Yamada and Knight, 2002; Wu, 1997; Marcu et al., 2006). A main criticism to the first group is that it has shown a relatively good performance when tackling language pairs with reduced reordering needs (such as Spanish-English or FrenchEnglish). On the other hand, syntax-directed systems show a main weakness on their poor efficiency results, recently overrided by the apparition of new decoders, which show significant improvements when handling with syntactically divergent language pairs, under large-scale data translation tasks. An example of such a system can b"
2007.mtsummit-papers.16,J97-3002,0,0.0794855,"ors performed in the monotonization preprocessing step. (Collins et al., 2005) proposes a set of hand-crafted reordering rules for a German-English translation task. The second group (syntax-directed) has gained many adepts in the last few years because of the significant improvements made by exploiting the power of synchronous rewriting systems. These systems employ source and/or target dependency (Quirk et al., 2005; Langlais and Gotti, 2006) or constituent trees, which can be formally syntax-based (Chiang, 2005; Watanabe et al., 2006) or linguistically syntaxbased (Yamada and Knight, 2002; Wu, 1997; Marcu et al., 2006). A main criticism to the first group is that it has shown a relatively good performance when tackling language pairs with reduced reordering needs (such as Spanish-English or FrenchEnglish). On the other hand, syntax-directed systems show a main weakness on their poor efficiency results, recently overrided by the apparition of new decoders, which show significant improvements when handling with syntactically divergent language pairs, under large-scale data translation tasks. An example of such a system can be found in (Marcu et al., 2006). Similar to (Crego and Mari˜ no,"
2007.mtsummit-papers.16,C04-1073,0,0.108736,"to derive generalizations from the training corpus. Several alternatives have been proposed in the recent years to boost the use of syntactic information in SMT systems. They range from those aiming at monotonizing the word order of the considered language pairs by means of a set of linguistically-based reordering patterns (though, reducing the reordering needs in the overall search), to others considering translation as a synchronous parsing process, where reorderings introduced in the overall search are syntactically motivated. Among the first group (word order monotonization) we can find (Xia and McCord, 2004), (Collins et al., 2005), (Costa-juss` a and Fonollosa, 2006) or (Popovic and Ney, 2006). They modify the source language word order before decoding in order to acquire the word order of the target language. Then, the reordered source sentence is sent to a standard phrasebased decoder to be translated under monotonic conditions. In (Crego and Mari˜ no, 2006) the same idea is enhanced by coupling reordering and decoding (using an N -gram-based system), what allows to further improve translation accuracy by avoiding some of the errors performed in the monotonization preprocessing step. (Collins"
2007.mtsummit-papers.16,P02-1039,0,0.0267621,"avoiding some of the errors performed in the monotonization preprocessing step. (Collins et al., 2005) proposes a set of hand-crafted reordering rules for a German-English translation task. The second group (syntax-directed) has gained many adepts in the last few years because of the significant improvements made by exploiting the power of synchronous rewriting systems. These systems employ source and/or target dependency (Quirk et al., 2005; Langlais and Gotti, 2006) or constituent trees, which can be formally syntax-based (Chiang, 2005; Watanabe et al., 2006) or linguistically syntaxbased (Yamada and Knight, 2002; Wu, 1997; Marcu et al., 2006). A main criticism to the first group is that it has shown a relatively good performance when tackling language pairs with reduced reordering needs (such as Spanish-English or FrenchEnglish). On the other hand, syntax-directed systems show a main weakness on their poor efficiency results, recently overrided by the apparition of new decoders, which show significant improvements when handling with syntactically divergent language pairs, under large-scale data translation tasks. An example of such a system can be found in (Marcu et al., 2006). Similar to (Crego and"
2007.mtsummit-papers.16,2002.tmi-tutorials.2,0,0.064188,"presented regarding translation accuracy and computational efficiency, showing significant improvements in translation quality at a reasonable computational cost. 1 Introduction In the statistical machine translation (SMT) community, it is widely accepted the need for structural information to account for mappings between the different language pairs. These mappings offer a greater potential to learn generalizations about relationships between languages than flat-structured models, such as the word-based IBM models of the early 1990s (Brown et al., 1990) or the more recent phrasebased models (Zens et al., 2002; Marcu and Wong, 2002; Koehn et al., 2003), which to date remain widely used. The need for structural information is specially relevant when handling language pairs with very different word order (such as Chinese-English), because the flatstructured models fail to derive generalizations from the training corpus. Several alternatives have been proposed in the recent years to boost the use of syntactic information in SMT systems. They range from those aiming at monotonizing the word order of the considered language pairs by means of a set of linguistically-based reordering patterns (though, red"
2009.eamt-1.10,W99-0604,0,0.125254,"Missing"
2009.eamt-1.10,H05-1095,0,0.514304,"hierarchical system that derives translations in two steps, so as to mitigate the computational impact resulting from the intersection of a probabilistic synchronous CFG and and the n-gram language model. Firstly, a CYK-style decoding considering first-best chart item approximations is used to generate an hypergraph of target language derivations. In the second step, a detailed exploration of the previous hypergraph is performed. The language model is used to drive the second step search process and to recover from search errors made during the first step. Related Work We follow the work in (Simard et al., 2005), which, to the best of our knowledge is the first MT system that within a left-to-right decoding approach, introduces the idea of phrases with gaps. A main limitation of their work arised from the difficulties of left-to-right decoders to handle gaps in the target side, again because of the non-monotonic generation of the target. Such gaps are to be filled in further steps of the search, thus, increasing the complexity of decoding and at the same time that hindering the use of the target language model. Such translation units are more naturally used under systems employing parsing techniques"
2009.eamt-1.10,J93-2003,0,0.0326071,"we aim at capturing the benefits of the higher generalization power shown by hierarchical systems. On the other hand, we want to avoid the computational burden of decoding based on parsing techniques, which among other drawbacks, make difficult the introduction of the required target language model costs. Our experiments show slight but consistent improvements for Chinese-toEnglish machine translation. Accuracy results are competitive with those achieved by a state-of-the-art phrasebased system. 1 Introduction don′ t want : ne veux pas Work in SMT has evolved from the traditional word-based (Brown et al., 1993) to the current phrase-based (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) and hierarchicalbased (Melamed, 2004; Chiang, 2007) translation models. Phrase-based and hierarchical systems are also characterized by the underlying formal device employed to produce translations (Knight, 2008): finite-state transducers (FST) on the one hand, and tree transducers In contrast, under hierarchical systems, it is possible to obtain the right generalization, decomposing the previous pattern as: X → don′ t Y : ne Y pas Y → want : veux 1 This example is only used for illustrative purposes. The co"
2009.eamt-1.10,takezawa-etal-2002-toward,0,0.0332215,"iments 4.2 In this section, we give details regarding the evaluation framework and report on the experimental work carried out to evaluate the improvements. 4.1 Accuracy results are reported for different configurations in table 2. System configurations consist of: base for which translation units do not introduce the ability to split source words into multiple tokens, and +split where the previous technique is used. The POS configuration employs POS tags in the source side of the reordering rules while +SYN employs both POS tag and syntactic rules. Evaluation Framework We have used the BTEC (Takezawa et al., 2002) corpus focusing on translations from Chinese to English. It consists of the data made available for the IWSLT 2007 evaluation campaign. Some statistics regarding the corpora used, namely number of sentences, words, vocabulary, average sentence length and number of references per language are shown in table 1. Sent Words Train en 377k 40k zh 354k Tune / Test (zh) tune 506 3,564 tst2 500 3,608 tst3 506 3,889 tst4 489 5,476 tst5 500 5,846 tst6 489 3,325 Voc Avg 11k 9,6k 9.5 8.9 871 921 916 1,094 1,292 864 7 7.22 7.69 11.2 11.69 6.8 Set tst2 tst3 tst4 tst5 tst6 base POS +SYN 47.25 48.15 55.82 56."
2009.eamt-1.10,J07-2003,0,0.882404,"utational burden of decoding based on parsing techniques, which among other drawbacks, make difficult the introduction of the required target language model costs. Our experiments show slight but consistent improvements for Chinese-toEnglish machine translation. Accuracy results are competitive with those achieved by a state-of-the-art phrasebased system. 1 Introduction don′ t want : ne veux pas Work in SMT has evolved from the traditional word-based (Brown et al., 1993) to the current phrase-based (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) and hierarchicalbased (Melamed, 2004; Chiang, 2007) translation models. Phrase-based and hierarchical systems are also characterized by the underlying formal device employed to produce translations (Knight, 2008): finite-state transducers (FST) on the one hand, and tree transducers In contrast, under hierarchical systems, it is possible to obtain the right generalization, decomposing the previous pattern as: X → don′ t Y : ne Y pas Y → want : veux 1 This example is only used for illustrative purposes. The contracted form don’t is not a real issue as most tokenizers split the form as do not, thus solving the alignment problem. c 2009 European A"
2009.eamt-1.10,N07-1063,0,0.033169,"ose translation. More recently, (Watanabe et al., 2006) presents a hierarchical system in which the target sentence is generated in left-to-right order, thus enabling a straightforward integration of the n-gram language models during search. The authors employ a top-down strategy to parse the foreign language side, using a synchronous grammar having a GNF2 -like structure. This means that the target side body of each translation rule takes the form bβ, where b is a string of terminal symbols and β a (possibly empty) string of non-terminals. This ensures that the target is built monotonously. (Venugopal et al., 2007) present a hierarchical system that derives translations in two steps, so as to mitigate the computational impact resulting from the intersection of a probabilistic synchronous CFG and and the n-gram language model. Firstly, a CYK-style decoding considering first-best chart item approximations is used to generate an hypergraph of target language derivations. In the second step, a detailed exploration of the previous hypergraph is performed. The language model is used to drive the second step search process and to recover from search errors made during the first step. Related Work We follow the"
2009.eamt-1.10,P07-2054,1,0.904734,"Missing"
2009.eamt-1.10,2007.mtsummit-papers.16,1,0.9267,"Missing"
2009.eamt-1.10,P06-1098,0,0.0414791,"discontinuous constituents, a major difference between FST- and CFG-based approaches to translation, has to do with the size of the search space, or more precisely with the kind of pruning that takes place to make the search feasible. As previously outlined, when considering the use of translation units with gaps under the left-to-right decoding approach, the main difficulty arises motivated by the appearance of discontinuities in the output side. In this work, we make use of an input word lattice to naturally avoid this problem, allowing to monotonically compose translation. More recently, (Watanabe et al., 2006) presents a hierarchical system in which the target sentence is generated in left-to-right order, thus enabling a straightforward integration of the n-gram language models during search. The authors employ a top-down strategy to parse the foreign language side, using a synchronous grammar having a GNF2 -like structure. This means that the target side body of each translation rule takes the form bβ, where b is a string of terminal symbols and β a (possibly empty) string of non-terminals. This ensures that the target is built monotonously. (Venugopal et al., 2007) present a hierarchical system t"
2009.eamt-1.10,2007.mtsummit-papers.29,0,0.0323727,"ied on top of the sentence abcd (s). As it can be seen, the resulting word lattice contains the path of the original sentence s : abcd, as well as the additional paths appeared by the composition of reordering rules: τ1 (s) : cab, τ2 (s) : ba and τ2 (τ1 (s)) : cba. Part-of-speech (POS) and syntactic information are used to increase the generalization 3 Translation units with gaps In this section we give details of the gappy translation units introduced in this work. 3.1 Split rules and reordering Some phrase-based systems have been able to introduce some levels of syntactical information. In (Habash, 2007) the author employs automatically learned syntactic reordering rules to preprocess the input, aiming at solving the reordering problem, before passing the reordered input to a phrase-based decoder for Arabic-English translation. However, this kind of systems cannot produce the translation 68 ings and split words are introduced in the source sentence only, motivating the use of a word lattice. During training, the alignment is entirely monotonized before extracting tuples, only keeping those one-to-many and many-toone alignments where the tokens on the many are contiguous; when this is not the"
2009.eamt-1.10,2002.tmi-tutorials.2,0,0.03956,"r shown by hierarchical systems. On the other hand, we want to avoid the computational burden of decoding based on parsing techniques, which among other drawbacks, make difficult the introduction of the required target language model costs. Our experiments show slight but consistent improvements for Chinese-toEnglish machine translation. Accuracy results are competitive with those achieved by a state-of-the-art phrasebased system. 1 Introduction don′ t want : ne veux pas Work in SMT has evolved from the traditional word-based (Brown et al., 1993) to the current phrase-based (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) and hierarchicalbased (Melamed, 2004; Chiang, 2007) translation models. Phrase-based and hierarchical systems are also characterized by the underlying formal device employed to produce translations (Knight, 2008): finite-state transducers (FST) on the one hand, and tree transducers In contrast, under hierarchical systems, it is possible to obtain the right generalization, decomposing the previous pattern as: X → don′ t Y : ne Y pas Y → want : veux 1 This example is only used for illustrative purposes. The contracted form don’t is not a real issue as most tokenizers split"
2009.eamt-1.10,N03-1017,0,0.195799,"ical systems. On the other hand, we want to avoid the computational burden of decoding based on parsing techniques, which among other drawbacks, make difficult the introduction of the required target language model costs. Our experiments show slight but consistent improvements for Chinese-toEnglish machine translation. Accuracy results are competitive with those achieved by a state-of-the-art phrasebased system. 1 Introduction don′ t want : ne veux pas Work in SMT has evolved from the traditional word-based (Brown et al., 1993) to the current phrase-based (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) and hierarchicalbased (Melamed, 2004; Chiang, 2007) translation models. Phrase-based and hierarchical systems are also characterized by the underlying formal device employed to produce translations (Knight, 2008): finite-state transducers (FST) on the one hand, and tree transducers In contrast, under hierarchical systems, it is possible to obtain the right generalization, decomposing the previous pattern as: X → don′ t Y : ne Y pas Y → want : veux 1 This example is only used for illustrative purposes. The contracted form don’t is not a real issue as most tokenizers split the form as do not, t"
2009.eamt-1.10,P07-2045,0,0.00718213,"ces per language are shown in table 1. Sent Words Train en 377k 40k zh 354k Tune / Test (zh) tune 506 3,564 tst2 500 3,608 tst3 506 3,889 tst4 489 5,476 tst5 500 5,846 tst6 489 3,325 Voc Avg 11k 9,6k 9.5 8.9 871 921 916 1,094 1,292 864 7 7.22 7.69 11.2 11.69 6.8 Set tst2 tst3 tst4 tst5 tst6 base POS +SYN 47.25 48.15 55.82 56.88 15.72 16.82 15.89 16.32 29.56 30.81 +split POS +SYN 47.42 48.39 56.44 57.17 16.48 17.08 16.34 16.89 29.81 31.67 Moses 48.14 55.95 18.06 15.91 31.76 Refs Table 2: Accuracy results measured using the BLEU score. 1 The last column shows accuracy results obtained by Moses (Koehn et al., 2007), a stateof-the-art phrase-based SMT system. It is worth saying that the Moses system was built using the same data sets and alignments that were used for our system (Moses performs lexicalized reordering with a maximum reordering distance of 8 words). In this case, we run a different optimization for each of the system configurations. BLEU confidence intervals range depending on the test set approximately from ±2.0 to ±3.0 points BLEU. As it can be seen, the system built using the +split technique obtains higher accuracy results than the baseline one (base), in all test 16 16 16 7 7 6 Table 1"
2009.eamt-1.10,W03-1730,0,0.0126589,"built using the same data sets and alignments that were used for our system (Moses performs lexicalized reordering with a maximum reordering distance of 8 words). In this case, we run a different optimization for each of the system configurations. BLEU confidence intervals range depending on the test set approximately from ±2.0 to ±3.0 points BLEU. As it can be seen, the system built using the +split technique obtains higher accuracy results than the baseline one (base), in all test 16 16 16 7 7 6 Table 1: BTEC Corpus (Chinese-to-English). Chinese words were segmented by means of the ICTCLAS (Zhang et al., 2003) tagger/segmenter. Word alignments were computed for the training data in the original word order, using GIZA++3 . The grow-final-diagand heuristic is used to refine the alignments 3 Results 4 5 www.fjoch.com/GIZA++ 71 nlp.stanford.edu/downloads/lex-parser.shtml www.speech.sri.com/projects/srilm 5 sets and for both reordering rule configurations (POS and +SYN). Conclusions and Further Work In this paper, we have presented an extension to a bilingual n-gram translation system in which we allow translation units with gaps. The use of word lattices allowed us to introduce the concept of gappy tra"
2009.eamt-1.10,W06-3119,0,0.0394058,"s not a real issue as most tokenizers split the form as do not, thus solving the alignment problem. c 2009 European Association for Machine Translation. Proceedings of the 13th Annual Conference of the EAMT, pages 66–73, Barcelona, May 2009 66 tion they use. We mainly differentiate here between translation units that are formally syntax-based, like those appearing in (Chiang, 2007), which employ non-terminal categories without linguistic motivation, working as placeholders to be filled by words in further translation steps; and hierarchical units that are more linguistically motivated, as in (Zollmann and Venugopal, 2006). This ability to capture better generalization comes at a double price: translation as parsing is typically cubic with respect to the source sentence length; furthermore, in this formalism, target constituent are no longer produced monotonically from left-to-right, thus rendering the application of the language model score difficult (Chiang, 2007). This example also suggests that hierarchical rules tend to be less sparse, given that the holistic unit in the phrase-based (PB) model is divided into two smaller, more reusable, rules. Notice that, in this specific case, the rich morphology of Fre"
2009.eamt-1.10,J06-4004,1,0.931134,"Missing"
2009.eamt-1.10,C08-1144,0,0.102713,"Missing"
2009.eamt-1.10,P04-1083,0,0.0610688,"avoid the computational burden of decoding based on parsing techniques, which among other drawbacks, make difficult the introduction of the required target language model costs. Our experiments show slight but consistent improvements for Chinese-toEnglish machine translation. Accuracy results are competitive with those achieved by a state-of-the-art phrasebased system. 1 Introduction don′ t want : ne veux pas Work in SMT has evolved from the traditional word-based (Brown et al., 1993) to the current phrase-based (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) and hierarchicalbased (Melamed, 2004; Chiang, 2007) translation models. Phrase-based and hierarchical systems are also characterized by the underlying formal device employed to produce translations (Knight, 2008): finite-state transducers (FST) on the one hand, and tree transducers In contrast, under hierarchical systems, it is possible to obtain the right generalization, decomposing the previous pattern as: X → don′ t Y : ne Y pas Y → want : veux 1 This example is only used for illustrative purposes. The contracted form don’t is not a real issue as most tokenizers split the form as do not, thus solving the alignment problem. c"
2009.jeptalnrecital-court.28,P05-1048,0,0.295677,"Missing"
2009.jeptalnrecital-court.28,W06-2606,0,0.0423906,"Missing"
2009.jeptalnrecital-court.28,2008.amta-srw.3,0,0.0259168,"Missing"
2009.jeptalnrecital-court.28,2005.mtsummit-papers.11,0,0.0623989,"Missing"
2009.jeptalnrecital-court.28,N03-1017,0,0.0219395,"Missing"
2009.jeptalnrecital-court.28,W09-0407,0,0.055301,"Missing"
2009.jeptalnrecital-court.28,J06-4004,1,0.880029,"Missing"
2009.jeptalnrecital-court.28,2009.jeptalnrecital-long.16,1,0.785303,"Missing"
2009.jeptalnrecital-court.28,P04-1063,0,0.0604008,"Missing"
2009.jeptalnrecital-court.28,2001.mtsummit-papers.46,0,0.14582,"Missing"
2009.jeptalnrecital-court.28,P02-1040,0,0.076177,"Missing"
2009.jeptalnrecital-court.28,N07-1029,0,0.0454675,"Missing"
2009.jeptalnrecital-court.28,2008.amta-srw.6,0,0.022181,"Missing"
2009.jeptalnrecital-court.28,N04-1023,0,0.0770885,"Missing"
2009.jeptalnrecital-court.28,P07-1108,0,0.0239088,"Missing"
2009.jeptalnrecital-court.28,W08-0309,0,\N,Missing
2010.iwslt-evaluation.13,W10-1704,1,0.845666,"is first described in Section 2, while Section 3 reports our work on Turkish pre-processing and on the use of continuous space language models. 2. TALK task 2.1. n-code SMT system 1. Introduction LIMSI took part in the IWSLT 2010 evaluation for two different tasks: Talk and BTEC. The goal of the new Talk task is to translate public speeches on a variety of topics, from English to French. Since the allowed training data includes the parallel corpora distributed by the ACL 2010 Workshop on Statistical Machine Translation (WMT), our starting system is the one submitted to the evaluation campaign [1]. We enhanced our inhouse n-code SMT system with an additional reordering model which is estimated as a standard n-gram language model over generalized translation units (partof-speech in the described experiments). In order to add more closely related training data, the use of Wikipedia as an additionnal source of monolingual text for the target language model was also evaluated. For the BTEC task, the LIMSI participated in the Turkish to English translation track with a system based on the open source Moses system [2]. The linguistic discrepancies between these two languages appear both Our"
2010.iwslt-evaluation.13,P07-2045,0,0.00317713,"(WMT), our starting system is the one submitted to the evaluation campaign [1]. We enhanced our inhouse n-code SMT system with an additional reordering model which is estimated as a standard n-gram language model over generalized translation units (partof-speech in the described experiments). In order to add more closely related training data, the use of Wikipedia as an additionnal source of monolingual text for the target language model was also evaluated. For the BTEC task, the LIMSI participated in the Turkish to English translation track with a system based on the open source Moses system [2]. The linguistic discrepancies between these two languages appear both Our in-house n-code SMT system implements the bilingual n-gram approach to statistical Machine Translation [3]. A translation hypothesis t given a source sentence s is defined as the sentence which maximizes a linear combination of feature functions: tˆI1 = arg max tI1 ( M X m=1 λm hm (sJ1 , tI1 ) ) , (1) where sJ1 and tI1 respectively denote the source and the target sentences, and λm is the weight associated with the feature function hm . The most important feature is the log-score of the translation model based on biling"
2010.iwslt-evaluation.13,J06-4004,1,0.810779,"standard n-gram language model over generalized translation units (partof-speech in the described experiments). In order to add more closely related training data, the use of Wikipedia as an additionnal source of monolingual text for the target language model was also evaluated. For the BTEC task, the LIMSI participated in the Turkish to English translation track with a system based on the open source Moses system [2]. The linguistic discrepancies between these two languages appear both Our in-house n-code SMT system implements the bilingual n-gram approach to statistical Machine Translation [3]. A translation hypothesis t given a source sentence s is defined as the sentence which maximizes a linear combination of feature functions: tˆI1 = arg max tI1 ( M X m=1 λm hm (sJ1 , tI1 ) ) , (1) where sJ1 and tI1 respectively denote the source and the target sentences, and λm is the weight associated with the feature function hm . The most important feature is the log-score of the translation model based on bilingual units called tuples. The probability assigned to a sentence pair by the translation model is estimated by using the n-gram assumption: p(sJ1 , tI1 ) = K Y k=1 p((s, t)k |(s, t)k"
2010.iwslt-evaluation.13,P03-1021,0,0.0080791,"Figure 1: Tuple extraction from a sentence pair. The resulting sequence of tuples (1) is further refined to avoid NULL words in source side of the tuples (2). Once the whole bilingual training data is segmented into tuples, n-gram language model probabilities can be estimated. In this example, note that the English source words perfect and translations have been reordered in the final tuple segmentation, while the French target words are kept in their original order. In addition to the translation model, eleven feature functions are optimally combined using a discriminative training framework [4]: a target-language model; four lexicon models; two lexicalized reordering models [5] aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuplebonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correpond to the relative frequencies of All the available textual corpora are processed and normalized using in-house tools. Previous experiments revealed that using better normalizatio"
2010.iwslt-evaluation.13,N04-4026,0,0.0409215,"is further refined to avoid NULL words in source side of the tuples (2). Once the whole bilingual training data is segmented into tuples, n-gram language model probabilities can be estimated. In this example, note that the English source words perfect and translations have been reordered in the final tuple segmentation, while the French target words are kept in their original order. In addition to the translation model, eleven feature functions are optimally combined using a discriminative training framework [4]: a target-language model; four lexicon models; two lexicalized reordering models [5] aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuplebonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correpond to the relative frequencies of All the available textual corpora are processed and normalized using in-house tools. Previous experiments revealed that using better normalization tools provides a significant reward in BLEU . The downside is the need to post-proc"
2010.iwslt-evaluation.13,C10-2023,1,0.883793,"Missing"
2010.iwslt-evaluation.13,W07-0704,1,0.827751,"Missing"
2010.iwslt-evaluation.13,P10-1047,0,0.0285202,"Missing"
2010.iwslt-evaluation.13,2009.iwslt-evaluation.2,0,0.0280005,"Missing"
2010.iwslt-evaluation.13,2009.iwslt-evaluation.6,0,0.0375883,"Missing"
2010.iwslt-evaluation.13,P06-1001,0,0.0377565,"Missing"
2010.iwslt-evaluation.13,popovic-ney-2004-towards,0,0.0520656,"Missing"
2010.iwslt-evaluation.13,H05-1085,0,0.0623309,"Missing"
2010.iwslt-evaluation.13,P08-1087,0,0.0340659,"Missing"
2010.iwslt-evaluation.13,P07-1017,0,0.0300031,"Missing"
2010.iwslt-evaluation.13,J04-2003,0,0.0867503,"Missing"
2010.iwslt-evaluation.13,corston-oliver-gamon-2004-normalizing,0,0.068379,"Missing"
2010.iwslt-evaluation.13,2005.mtsummit-papers.11,0,0.0224187,"Missing"
2010.iwslt-evaluation.13,E06-1006,0,0.0399082,"Missing"
2010.iwslt-evaluation.13,N04-4015,0,0.0798783,"Missing"
2010.iwslt-evaluation.13,2001.mtsummit-papers.45,0,0.0857026,"Missing"
2010.iwslt-evaluation.13,D10-1076,1,0.891217,"Missing"
2010.iwslt-evaluation.13,W06-3102,1,\N,Missing
2010.iwslt-evaluation.13,2009.iwslt-evaluation.5,0,\N,Missing
2010.iwslt-papers.12,E06-1005,1,0.922753,"arget language pair. The disadvantage of this approach is that both the translation into the pivot language, and the translation into the target language are error-prone – and typically, these errors add up. As a result, on comparable training resources, we can expect the translation quality of a pivot system to be significantly lower that the quality of a “direct” system1 . Since Kay [2] has first predicted the usefulness of multilingual resources, several approaches have been proposed to utilize resources and data available in more than two languages for MT. Multi-source machine translation [3, 4, 5] denotes techniques to translate documents which are available in two or more source languages. One approach that has recently been shown to be very effective [4, 6] is to use individual bilingual MT systems to translate the source documents independently of each other into one document each in the target language, and then to use MT system combination (ibid.) to generate a consensus translation out of these different target translations. In this paper, we will investigate to what extent 1 Within this paper, we use the term “direct system” to denote a (statistical) MT system that has been trai"
2010.iwslt-papers.12,E09-1082,0,0.0772212,"arget language pair. The disadvantage of this approach is that both the translation into the pivot language, and the translation into the target language are error-prone – and typically, these errors add up. As a result, on comparable training resources, we can expect the translation quality of a pivot system to be significantly lower that the quality of a “direct” system1 . Since Kay [2] has first predicted the usefulness of multilingual resources, several approaches have been proposed to utilize resources and data available in more than two languages for MT. Multi-source machine translation [3, 4, 5] denotes techniques to translate documents which are available in two or more source languages. One approach that has recently been shown to be very effective [4, 6] is to use individual bilingual MT systems to translate the source documents independently of each other into one document each in the target language, and then to use MT system combination (ibid.) to generate a consensus translation out of these different target translations. In this paper, we will investigate to what extent 1 Within this paper, we use the term “direct system” to denote a (statistical) MT system that has been trai"
2010.iwslt-papers.12,W09-0407,1,0.812949,"e – and typically, these errors add up. As a result, on comparable training resources, we can expect the translation quality of a pivot system to be significantly lower that the quality of a “direct” system1 . Since Kay [2] has first predicted the usefulness of multilingual resources, several approaches have been proposed to utilize resources and data available in more than two languages for MT. Multi-source machine translation [3, 4, 5] denotes techniques to translate documents which are available in two or more source languages. One approach that has recently been shown to be very effective [4, 6] is to use individual bilingual MT systems to translate the source documents independently of each other into one document each in the target language, and then to use MT system combination (ibid.) to generate a consensus translation out of these different target translations. In this paper, we will investigate to what extent 1 Within this paper, we use the term “direct system” to denote a (statistical) MT system that has been trained on a bilingual corpus between source and target language, and does not utilize any pivot or bridge languages. 299 Proceedings of the 7th International Workshop o"
2010.iwslt-papers.12,D07-1005,0,0.143443,"Missing"
2010.iwslt-papers.12,N06-1003,0,0.0986054,"Missing"
2010.iwslt-papers.12,N07-1061,0,0.498978,"ta are available. Unfortunately, if one wants to be able to translate from many possible source languages into many possible target languages, separate MT systems for each possible pair of source and target language have to be trained, on bilingual data in this specific language pair. Quite often this is not possible, especially where rare or unrelated languages are involved. Significant amounts of bilingual in-domain training data may be unavailable; the number of systems to train and to tune may be too high. One approach to overcome this problem has been proposed e.g. by Utiyama and Isahara [1]: A third, more frequent language is utilized as a pivot or bridge language. Ideally, sufficient bilingual language resources are available for both the pair of source and pivot language, and for the pair of pivot and target language. The final translation is then obtained by going via the bridge language, either by generating full translations of the source sentence in this bridge language, or by using the bilingual data to build translation models for the source–target language pair. The disadvantage of this approach is that both the translation into the pivot language, and the translation i"
2010.iwslt-papers.12,2001.mtsummit-papers.46,1,0.565201,"arget language pair. The disadvantage of this approach is that both the translation into the pivot language, and the translation into the target language are error-prone – and typically, these errors add up. As a result, on comparable training resources, we can expect the translation quality of a pivot system to be significantly lower that the quality of a “direct” system1 . Since Kay [2] has first predicted the usefulness of multilingual resources, several approaches have been proposed to utilize resources and data available in more than two languages for MT. Multi-source machine translation [3, 4, 5] denotes techniques to translate documents which are available in two or more source languages. One approach that has recently been shown to be very effective [4, 6] is to use individual bilingual MT systems to translate the source documents independently of each other into one document each in the target language, and then to use MT system combination (ibid.) to generate a consensus translation out of these different target translations. In this paper, we will investigate to what extent 1 Within this paper, we use the term “direct system” to denote a (statistical) MT system that has been trai"
2010.iwslt-papers.12,W09-2503,1,0.895966,"Missing"
2010.iwslt-papers.12,D10-1064,1,0.878502,"Missing"
2010.iwslt-papers.12,P07-1092,0,0.267577,"Missing"
2010.iwslt-papers.12,J06-4004,1,0.89411,"1000 sentences from the common subset (same sentences for all languages). 301 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 MT Sys 1' Src ... MT Sys m' Piv 1 ... MT Sys 1'' Piv m MT Sys m'' ... Direct MT Sys Hyp 1 ... GIZA++alignment Hyp m Reordering Network generation, weighting, rescoring Consensus Translation Hyp m+1 Figure 1: Structure of the multipivot system 5.2. Translation engines The translation engine for these experiments implements the n-gram-based approach to statistical machine translation detailed by Marino et al. [21]. The overall translation accuracy is comparable to state-of-the-art phrase-based translation engines such as the MOSES system [22]. In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram language model of (source,target) pairs [23]. Training such a model requires to reorder source sentences so as to match the target word order. Reordering hypotheses are computed before decoding takes place via a stochastic finite-state automaton that builds a lattice with the most promising hypotheses according to a set of rewrite rules previously co"
2010.iwslt-papers.12,P09-1018,0,0.147907,"Missing"
2010.iwslt-papers.12,P07-2045,0,0.00550476,"en Language Translation Paris, December 2nd and 3rd, 2010 MT Sys 1' Src ... MT Sys m' Piv 1 ... MT Sys 1'' Piv m MT Sys m'' ... Direct MT Sys Hyp 1 ... GIZA++alignment Hyp m Reordering Network generation, weighting, rescoring Consensus Translation Hyp m+1 Figure 1: Structure of the multipivot system 5.2. Translation engines The translation engine for these experiments implements the n-gram-based approach to statistical machine translation detailed by Marino et al. [21]. The overall translation accuracy is comparable to state-of-the-art phrase-based translation engines such as the MOSES system [22]. In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram language model of (source,target) pairs [23]. Training such a model requires to reorder source sentences so as to match the target word order. Reordering hypotheses are computed before decoding takes place via a stochastic finite-state automaton that builds a lattice with the most promising hypotheses according to a set of rewrite rules previously collected from the training bi-texts using the word alignments. In addition to the bilingual n-gram model, our SMT system implements"
2010.iwslt-papers.12,2008.amta-srw.6,0,0.0340832,"Missing"
2010.iwslt-papers.12,eisele-2006-parallel,0,0.0566411,"Missing"
2010.iwslt-papers.12,J03-1002,1,0.0201737,"puts from different systems was first shown to produce superior results in automatic speech recognition (ASR). Voting schemes like the ROVER approach of Fiscus [16] create confusion networks (CNs) from the output of different ASR systems for the same audio input. The consensus recognition hypothesis is generated by weighted majority voting. This approach has later been adapted to MT as well [17]. In this paper, we follow the approach of Matusov et al [4, 18]: An unsupervised monolingual word alignment is trained between all pairs of hypotheses for each source sentence using the GIZA++ toolkit [19]. These alignments are then used to reorder all individual hypotheses to one selected (“primary”) hypothesis, which defines the word order in the consensus translation. A CN is then generated from these reordered hypotheses. As there is no obvious way to determine the best primary hypothesis, separate CNs are generated for all possible primary hypotheses, which are then combined 300 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 Table 1: Corpus statistics for the experimental setup. to a single word lattice. This lattice is then r"
2010.iwslt-papers.12,2005.mtsummit-papers.11,0,0.0506804,"k Words 13.4k 13.5k 14.0k 14.6k 10.1k 16.1k 14.1k 14.3k 14.2k 14.5k 12.7k Dev Test Voc. OOV Words Voc. OOV 3.2k 104 25.9k 5.1k 226 3.5k 120 26.0k 5.5k 245 2.8k 39 27.2k 4.0k 63 3.3k 56 28.6k 5.0k 88 4.3k 244 19.6k 7.1k 407 3.2k 47 31.5k 4.8k 87 3.9k 72 27.2k 6.2k 159 3.4k 61 28.1k 5.1k 99 3.1k 76 27.5k 4.8k 162 3.4k 49 28.3k 5.2k 118 3.3k 116 24.5k 5.2k 226 5. Experimental setup 5.1. Training and Development data For experimenting with our approach, we built translation systems to serve as direct or pivot systems using a phrase-based MT engine for several language pairs of the Europarl corpus [20], which is available in 11 languages: Danish (da), German (de), English (en) , Spanish (es), Finnish (fi), French (fr), Greek (el), Italian (it), Dutch (nl), Portuguese (pt) and Swedish (sv). We also decided to study three source–target language pairs, two for which translation accuracy, as measured by automatic metrics, is moderate, (de–en) and (fr–de), and one for which translation accuracy, is much higher. (fr–en). This allowed us to check whether the improvements provided by our method carry over even in situations where the baseline is high; conversely, it also allows us to assess whether"
2010.iwslt-papers.12,J04-2004,0,0.0336595,"gnment Hyp m Reordering Network generation, weighting, rescoring Consensus Translation Hyp m+1 Figure 1: Structure of the multipivot system 5.2. Translation engines The translation engine for these experiments implements the n-gram-based approach to statistical machine translation detailed by Marino et al. [21]. The overall translation accuracy is comparable to state-of-the-art phrase-based translation engines such as the MOSES system [22]. In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram language model of (source,target) pairs [23]. Training such a model requires to reorder source sentences so as to match the target word order. Reordering hypotheses are computed before decoding takes place via a stochastic finite-state automaton that builds a lattice with the most promising hypotheses according to a set of rewrite rules previously collected from the training bi-texts using the word alignments. In addition to the bilingual n-gram model, our SMT system implements eight additional models which are linearly combined following a discriminative modeling framework [24]: two lexicalized reordering models [25], which attempt to"
2010.iwslt-papers.12,P02-1038,1,0.508083,"ned using a n-gram language model of (source,target) pairs [23]. Training such a model requires to reorder source sentences so as to match the target word order. Reordering hypotheses are computed before decoding takes place via a stochastic finite-state automaton that builds a lattice with the most promising hypotheses according to a set of rewrite rules previously collected from the training bi-texts using the word alignments. In addition to the bilingual n-gram model, our SMT system implements eight additional models which are linearly combined following a discriminative modeling framework [24]: two lexicalized reordering models [25], which attempt to model the orientation of the current translation unit according to the previous as well as the ordering of the next unit with respect to the current unit, a target-language model which provides information about the target language structure and fluency; two lexicon models, which constitute complementary translation models computed for each given tuple; a ‘weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which are used in order to compensate for the system preference for short translations."
2010.iwslt-papers.12,N04-4026,0,0.0237976,"urce,target) pairs [23]. Training such a model requires to reorder source sentences so as to match the target word order. Reordering hypotheses are computed before decoding takes place via a stochastic finite-state automaton that builds a lattice with the most promising hypotheses according to a set of rewrite rules previously collected from the training bi-texts using the word alignments. In addition to the bilingual n-gram model, our SMT system implements eight additional models which are linearly combined following a discriminative modeling framework [24]: two lexicalized reordering models [25], which attempt to model the orientation of the current translation unit according to the previous as well as the ordering of the next unit with respect to the current unit, a target-language model which provides information about the target language structure and fluency; two lexicon models, which constitute complementary translation models computed for each given tuple; a ‘weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which are used in order to compensate for the system preference for short translations. For this study, we used 3-gram bilingual"
2010.iwslt-papers.12,P96-1041,0,0.0557653,"ccording to the previous as well as the ordering of the next unit with respect to the current unit, a target-language model which provides information about the target language structure and fluency; two lexicon models, which constitute complementary translation models computed for each given tuple; a ‘weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which are used in order to compensate for the system preference for short translations. For this study, we used 3-gram bilingual tuple and 3-gram target language models built using Kneser-Ney smoothing [26]; training was performed with the SRI language modeling toolkit [27]. After preprocessing the corpora with standard tokenization tools, word-to-word GIZA++ [19] alignments are performed in both directions, followed by the growdiag-final-and heuristic [28]. 5.3. Experiments The two principal research questions we wanted to answer with the experiments for this paper were: Can we use the multi-pivot approach instead of a direct source– target system, with comparable translation scores? And secondly, can we use the multi-pivot approach to improve the output of an existing direct system? The former"
2010.iwslt-papers.12,2005.iwslt-1.8,0,0.0213956,"slation models computed for each given tuple; a ‘weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which are used in order to compensate for the system preference for short translations. For this study, we used 3-gram bilingual tuple and 3-gram target language models built using Kneser-Ney smoothing [26]; training was performed with the SRI language modeling toolkit [27]. After preprocessing the corpora with standard tokenization tools, word-to-word GIZA++ [19] alignments are performed in both directions, followed by the growdiag-final-and heuristic [28]. 5.3. Experiments The two principal research questions we wanted to answer with the experiments for this paper were: Can we use the multi-pivot approach instead of a direct source– target system, with comparable translation scores? And secondly, can we use the multi-pivot approach to improve the output of an existing direct system? The former question is most relevant for the “matrix” scenario sketched in Section 3, where we have a large number of possible source and target languages, and do not want to build separate translation systems for each individual pair – either because we want to sa"
2010.iwslt-papers.12,2009.mtsummit-papers.7,0,\N,Missing
2011.iwslt-evaluation.15,2011.iwslt-evaluation.16,1,0.746987,"led in the Quaero program is 1 http://www.quaero.org spoken language translation (SLT). In this work, the 2011 project-internal evaluation campaign on SLT is described. The campaign focuses on the language pair German-French in both directions, and both human and automatic transcripts of the spoken text are considered as input data. The automatic transcripts were produced by the Rover combination of single-best output of the best submission from each of the three sites participating in the internal 2010 automatic speech recognition (ASR) evaluation, which is described in an accompanying paper [1]. The campaign was designed and conducted by DGA and compares the different approaches taken by the four participating partners RWTH, KIT, LIMSI and SYSTRAN. In addition to publicly available data, monolingual and bilingual corpora collected in the Quaero program were used for training and evaluating the systems. The approaches to machine translation taken by the partners differ substantially. KIT, LIMSI and RWTH apply statistical techniques to perform the task, whereas SYSTRAN uses their commercial rule-based translation engine. KIT makes use of a phrase-based decoder augmented with partof-sp"
2011.iwslt-evaluation.15,P02-1040,0,0.0815552,"s been built from the test sets of the previous years. • arte.tv 2.3. Metrics and Scoring The corpora used to evaluate this task have been built from French and German (manual) transcriptions extracted from the test set used in the previous year’s Quaero evaluation campaign of ASR [1]. These transcriptions come from recordings of broadcast shows. The transcriptions were resegmented manually by the human translators into sentences. Indeed the time-based segmentation, traditionally used for ASR purposes, induced translation issues in the previous 2 http://www.statmt.org/wmt10/ The B LEU-4 score [3] and the Translation Edit Rate (T ER) [4] were chosen as the evaluation metrics for machine translation in Quaero program. B LEU measures the closeness of a candidate translation to one or several reference translations by counting the number of n-grams in the system output that also occur in the reference translation. T ER is an error measure for machine translation that measures the number of edits required to change a system output into one of the references. T ER is defined as the minimum number of 115 edits needed to change a hypothesis so that it matches one of the references, normalized"
2011.iwslt-evaluation.15,2006.amta-papers.25,0,0.0225424,"evious years. • arte.tv 2.3. Metrics and Scoring The corpora used to evaluate this task have been built from French and German (manual) transcriptions extracted from the test set used in the previous year’s Quaero evaluation campaign of ASR [1]. These transcriptions come from recordings of broadcast shows. The transcriptions were resegmented manually by the human translators into sentences. Indeed the time-based segmentation, traditionally used for ASR purposes, induced translation issues in the previous 2 http://www.statmt.org/wmt10/ The B LEU-4 score [3] and the Translation Edit Rate (T ER) [4] were chosen as the evaluation metrics for machine translation in Quaero program. B LEU measures the closeness of a candidate translation to one or several reference translations by counting the number of n-grams in the system output that also occur in the reference translation. T ER is an error measure for machine translation that measures the number of edits required to change a system output into one of the references. T ER is defined as the minimum number of 115 edits needed to change a hypothesis so that it matches one of the references, normalized by the average length of the references."
2011.iwslt-evaluation.15,J05-4003,0,0.0172208,"asing variant and change the case as required to be able to translate it. Some of the available data contains a lot of noise. The Giga corpus, for example, includes a large amount of noise such as non-standardized HTML characters. Also, the Bookshop and Presseurop corpora contain truncated lines, which do not match its aligned translation sentence. These noisy pairs potentially degrade the quality of the translation model. The special filtering was applied to the Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built"
2011.iwslt-evaluation.15,2007.tmi-papers.21,0,0.0128424,"ot match its aligned translation sentence. These noisy pairs potentially degrade the quality of the translation model. The special filtering was applied to the Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words"
2011.iwslt-evaluation.15,W09-0435,1,0.688116,"Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using"
2011.iwslt-evaluation.15,P07-2045,0,0.00836467,"generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We ad"
2011.iwslt-evaluation.15,W11-2145,1,0.808022,"oolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingua"
2011.iwslt-evaluation.15,W11-2124,1,0.82441,"g the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grain"
2011.iwslt-evaluation.15,W05-0836,1,0.88503,"ng model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grained linguistic information than the TreeTagger, whose output is used for POS-based reordering. French-German For French to German we also used long-range POS based reordering"
2011.iwslt-evaluation.15,C08-1098,0,0.0239782,"kit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grained linguistic information than the TreeTagger, whose output is used for POS-based reordering. French-German For French to German we also used long-range POS based reordering rules and lattice phrase extraction. Using the POS-based language model led to a big improvement. 3.2. LIMSI LIMSI’s participation in Quaero 2011 evaluation campaign was focused on the translation of German from and into French. The adaptation of our text translation system to speech inputs is mostly performed in preprocessing, aimed at removing dysflu"
2011.iwslt-evaluation.15,N04-4026,0,0.0164881,"slation system based on bilingual n-grams. N-code overview N-code’s translation model implements a stochastic finite-state transducer (FST) trained using an n-gram model (source,target) pairs. The training requires source-side sentence reorderings to match the target word order, also performed by a stochastic FST reordering model, which uses POS information to generalize reordering patterns beyond lexical regularities. Complementary to the translation model, ten more features are used in a linear scoring function: a target-language model; four lexicon models; two lexicalized reordering models [16] to predict the orientation of the next translation unit; a weak distancebased distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the standard ones in phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights, estimated from the automatically generated word alignments. The weights associated to features are found using the minimum error rate training procedure [17] on the development set. The decoding is beam-search-"
2011.iwslt-evaluation.15,P03-1021,0,0.0157097,"ur lexicon models; two lexicalized reordering models [16] to predict the orientation of the next translation unit; a weak distancebased distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the standard ones in phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights, estimated from the automatically generated word alignments. The weights associated to features are found using the minimum error rate training procedure [17] on the development set. The decoding is beam-search-based on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder as word lattices [18]. German-French Part-of-speech information for German 3 http://lia.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagg"
2011.iwslt-evaluation.15,P10-1052,1,0.744516,"the development set. The decoding is beam-search-based on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder as word lattices [18]. German-French Part-of-speech information for German 3 http://lia.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the r"
2011.iwslt-evaluation.15,D09-1022,1,0.784658,"al machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 100-best lists on B LEU. • pbt with additional models dwl and triplets • pbt with additional model triplets With the system combination of all different systems, we got an improvement in B LEU and in T ER compared to the best single system of both tasks. 3.4. SYSTRAN The German and French data submitted by SYSTRAN were obtained by the SYSTRAN baseline engine, being traditionally classified as a rule-based system. However, over the decades, its devel"
2011.iwslt-evaluation.15,2010.iwslt-papers.6,0,0.0142765,"a.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the reordering models we selected the monotone-swap-discontinuous (MSD) model. Language models Large 4-gram language models were trained on all the available data as described in [22]. Additionally, SOUL, a neuronal language model was used to rescore the n-best hypotheses. These models were trained following the methodology of [23] and used for rescoring n-best lists. We used 10-gram history size (differences with 6"
2011.iwslt-evaluation.15,C00-2162,1,0.678983,"sed tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the reordering models we selected the monotone-swap-discontinuous (MSD) model. Language models Large 4-gram language models were trained on all the available data as described in [22]. Additionally, SOUL, a neuronal language model was used to rescore the n-best hypotheses. These models were trained following the methodology of [23] and used for rescoring n-best lists. We used 10-gram history size (differences with 6-gram were insignificant). Using the neural language model led to (small but consistent) improvements in all tasks. With the help of system combination, we combined the hypoth"
2011.iwslt-evaluation.15,2008.iwslt-papers.8,1,0.818685,"the pipeline was unchanged as compared to text translations. For the Quaero 2011 evaluation RWTH utilized state-ofthe-art phrase-based and hierarchical translation systems as well as our in-house system combination framework. GIZA [24] was employed to train word alignments, all language models were created with the SRILM toolkit [11] and are standard 4-gram language models with interpolated modified Kneser-Ney smoothing. The phrase-based statistical machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 1"
2011.iwslt-evaluation.15,E06-1005,1,0.810679,"ents, all language models were created with the SRILM toolkit [11] and are standard 4-gram language models with interpolated modified Kneser-Ney smoothing. The phrase-based statistical machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 100-best lists on B LEU. • pbt with additional models dwl and triplets • pbt with additional model triplets With the system combination of all different systems, we got an improvement in B LEU and in T ER compared to the best single system of both tasks. 3.4. SYSTRAN The Ger"
2011.iwslt-evaluation.15,J03-1002,1,\N,Missing
2011.iwslt-evaluation.15,W11-2135,1,\N,Missing
2017.jeptalnrecital-court.27,P02-1023,0,0.18376,"Missing"
2017.jeptalnrecital-court.27,E12-1016,0,0.0293633,"Missing"
2017.jeptalnrecital-court.27,D13-1176,0,0.103714,"Missing"
2017.jeptalnrecital-court.27,P17-4012,1,0.83894,"Missing"
2017.jeptalnrecital-court.27,W11-2132,1,0.795463,"Missing"
2017.jeptalnrecital-court.27,D07-1036,0,0.0972007,"Missing"
2017.jeptalnrecital-court.27,2015.iwslt-evaluation.11,0,0.0979969,"Missing"
2017.jeptalnrecital-court.27,D15-1166,0,0.0657391,"Missing"
2017.jeptalnrecital-court.27,P10-2041,0,0.108924,"Missing"
2017.jeptalnrecital-court.27,P03-1021,0,0.0842472,"Missing"
2017.jeptalnrecital-court.27,P02-1040,0,0.0994922,"Missing"
2017.jeptalnrecital-court.27,N16-1005,0,0.0424935,"Missing"
2017.jeptalnrecital-court.27,P16-1009,0,0.0608435,"Missing"
2017.jeptalnrecital-court.27,W10-1759,0,0.0550544,"Missing"
2017.jeptalnrecital-court.27,I11-1148,0,0.0263053,"Missing"
2017.jeptalnrecital-court.27,tiedemann-2012-parallel,0,0.0169973,"Missing"
2017.jeptalnrecital-court.27,2014.amta-researchers.15,1,0.864363,"Missing"
2017.jeptalnrecital-court.27,C16-1170,0,0.0372666,"Missing"
2020.acl-main.144,P19-1294,0,0.0463637,"y to guide translation of a given sentence. Similar to our work, (Farajian et al., 2017; Li et al., 2018) retrieve similar sentences from the training data to dynamically adapt individual input sentences. To compute similarity, the first work uses n-gram matches, the second includes dense vector representations. In (Xu et al., 2019) the same approach is followed but authors consider for adaptation a bunch of semantically related input sentences to reduce adaptation time. Our approach combines source and target words within a same sentence - the same type of approach has also been proposed by (Dinu et al., 2019) for introduction of terminology translation. Last, we can also compare the extra-tokens appended in augmented sentences as “side constraints” activating different translation paths on the same spirit than the work done by (Sennrich et al., 2016a; Kobus et al., 2017) for controlling translation. 6 Conclusions and Further Work This paper explores augmentation methods for boosting Neural Machine Translation performance by using similar translations. Based on “neural fuzzy repair” technique, we introduce tighter integration of fuzzy matches informing neural network of source and target and propos"
2020.acl-main.144,N13-1073,0,0.0682232,"tence s, max(q) returns the longest n-gram in the set q and ∣r∣ is the length of the n-gram r. For N gram matching retrieval we also use our in-house open-sourced toolkit. ⎧ ⎫ t ∈ tk ∶ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ∃s ∈ S ∣ (s, t) ∈ A T =⎨ ⎬ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ∧ ∀s ∉ S ∣ (s, t) ∉ A ⎩ ⎭ where A is the set of word alignments between words in sk and tk and S is the LCS (Longest Common Subsequence) set of words in sk and s. The LCS is computed as a by-product of the edit distance (Paterson and Danˇc´ık, 1994). S is found as a sub-product of computing fuzzy or n-gram matches. Word alignments are per6 formed by fast align (Dyer et al., 2013). Figure 1 illustrates the alignments and LCS words between input sentences and their corresponding fuzzy (top) and N -gram (bottom) matches. dure le vol ? »» »» N M (si , sj ) = »»»»max({S(si ) ∩ S(sj )})»»»» »» »» discuss an algorithm capable of identifying the set of target words T ∈ tk that are related to words of the input sentence s. Thus, we define the set T as: dure un 3 N -gram Matching We define the N -gram matching score N M (si , sj ) between si and sj : Figure 1: English-French TM entries with corresponding word alignments (right) and LCS of words with the input sentence (left). M"
2020.acl-main.144,W17-4713,0,0.146587,"ting with more general notions of similar sentences and techniques to inject fuzzy matches. The use of similar sentences to improve translation models has been explored at scale in (Schwenk et al., 2019), where the authors use multilingual sentence embeddings to retrieve pairs of similar sentences and train models uniquely with such sentences. In (Niehues et al., 2016), input sentences are augmented with pre-translations performed by a phrase-based MT system. In our approach, similar sentence translations are provided dynamically to guide translation of a given sentence. Similar to our work, (Farajian et al., 2017; Li et al., 2018) retrieve similar sentences from the training data to dynamically adapt individual input sentences. To compute similarity, the first work uses n-gram matches, the second includes dense vector representations. In (Xu et al., 2019) the same approach is followed but authors consider for adaptation a bunch of semantically related input sentences to reduce adaptation time. Our approach combines source and target words within a same sentence - the same type of approach has also been proposed by (Dinu et al., 2019) for introduction of terminology translation. Last, we can also compa"
2020.acl-main.144,E14-1022,0,0.164442,"a sets and domains show consistent accuracy improvements. To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation. 1 Introduction For decades, the localization industry has been proposing Fuzzy Matching technology in CAT tools allowing the human translator to visualize one or several fuzzy matches from translation memory when translating a sentence leading to higher productivity and consistency (Yamada, 2011). Hence, even though the concept of fuzzy match scores is not standardized and differs between CAT tools (Bloodgood and Strauss, 2014), translators generally accept discounted translation 1 rate for sentences with ”high” fuzzy matches . With improving machine translation technology 1 https://signsandsymptomsoftranslation. com/2015/03/06/fuzzy-matches/. and training of models on translation memories, machine translated output has been progressively introduced as a substitute for fuzzy matches when no sufficiently “good” fuzzy match is found and proved to also increase translator productivity given appropriate post-editing environment (Plitt and Masselot, 2010). These two technologies are entirely different in their finality -"
2020.acl-main.144,P19-1175,0,0.256078,"Missing"
2020.acl-main.144,C18-1111,0,0.0465017,"s like use of fuzzy matches in SMT decoding (Koehn and Senellart, 2010; Wang et al., 2013), adaptive machine translation (Zaretskaya et al., 2015) or “fuzzy match repairing” (Ortega et al., 2016). With Neural Machine Translation (NMT), the integration of Fuzzy Matching is less obvious since NMT does not keep nor build a database of aligned sequences and does not explicitly use n-gram language models for decoding. The only obvious and important use of translation memory is to use them to train an NMT model from scratch or to adapt a generic translation model to a specific domain (fine-tuning) (Chu and Wang, 2018). While some works propose architecture changes (Zhang et al., 2018) or decoding constraints (Gu et al., 2018); a recent work (Bult´e and Tezcan, 2019; Bult´e et al., 2018) has proposed a simple and elegant framework where, like for human translation, translation of fuzzy matches are presented simultaneously with source sentence and the network learns to use this additional information. Even though this method has showed huge gains in quality, it also opens 1580 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1580–1590 c July 5 - 10, 2020. 2020 As"
2020.acl-main.144,P17-4012,1,0.820079,"RC); Localisation files (GNOME, KDE4 and Ubuntu) and Manual texts (PHP). Detailed statistics about these are provided in Appendix A. We randomly split the corpora by keeping 500 sentences for validation, 1, 000 sentences for testing and the rest for training. All data is preprocessed using the Open9 NMT tokenizer (conservative mode). We train a 32K joint byte-pair encoding (BPE) (Sennrich et al., 2016b) and use a joint vocabulary for both source and target. Our NMT model follows the state-of-the-art Transformer base architecture (Vaswani et al., 10 2017) implemented in the OpenNMT-tf toolkit (Klein et al., 2017). Further configuration details are given in Appendix B. 3.2 TM Retrieval We perform fuzzy matching, ignoring exact matches, and keep the single best match if F M (si , sj ) ≥ 0.6 with no approximation. Similarly, the largest N -gram match is used for each test sentence with a threshold N M (si , sj ) ≥ 5. A similarity threshold EM (si , sj ) ≥ 0.8 is also employed when retrieving similar sentences using distributed representations. The EM model is trained on the source training data with default fasttext params on 200 dimension, and 20 epochs. Algorithm FM NM EM Indexing (s) 546 546 181+342 R"
2020.acl-main.144,kobus-etal-2017-domain,1,0.876661,"e second includes dense vector representations. In (Xu et al., 2019) the same approach is followed but authors consider for adaptation a bunch of semantically related input sentences to reduce adaptation time. Our approach combines source and target words within a same sentence - the same type of approach has also been proposed by (Dinu et al., 2019) for introduction of terminology translation. Last, we can also compare the extra-tokens appended in augmented sentences as “side constraints” activating different translation paths on the same spirit than the work done by (Sennrich et al., 2016a; Kobus et al., 2017) for controlling translation. 6 Conclusions and Further Work This paper explores augmentation methods for boosting Neural Machine Translation performance by using similar translations. Based on “neural fuzzy repair” technique, we introduce tighter integration of fuzzy matches informing neural network of source and target and propose extension to similar translations retrieved 1587 from their distributed representations. We show that the different types of similar translations and model fine-tuning provide complementary information to the neural model outperforming consistently and significantl"
2020.acl-main.144,2010.jec-1.4,1,0.861121,"Masselot, 2010). These two technologies are entirely different in their finality - indeed, for a given source sentence, fuzzy matching is just a database retrieval and scoring technique always returning a pair of source and target segments, while machine translation is actually building an original translation. However, with Statistical Machine Translation, the two technologies are sharing the same simple idea about managing and retrieving optimal combination of longest translated n-grams and this property led to the development of several techniques like use of fuzzy matches in SMT decoding (Koehn and Senellart, 2010; Wang et al., 2013), adaptive machine translation (Zaretskaya et al., 2015) or “fuzzy match repairing” (Ortega et al., 2016). With Neural Machine Translation (NMT), the integration of Fuzzy Matching is less obvious since NMT does not keep nor build a database of aligned sequences and does not explicitly use n-gram language models for decoding. The only obvious and important use of translation memory is to use them to train an NMT model from scratch or to adapt a generic translation model to a specific domain (fine-tuning) (Chu and Wang, 2018). While some works propose architecture changes (Zh"
2020.acl-main.144,L18-1146,0,0.311686,"notions of similar sentences and techniques to inject fuzzy matches. The use of similar sentences to improve translation models has been explored at scale in (Schwenk et al., 2019), where the authors use multilingual sentence embeddings to retrieve pairs of similar sentences and train models uniquely with such sentences. In (Niehues et al., 2016), input sentences are augmented with pre-translations performed by a phrase-based MT system. In our approach, similar sentence translations are provided dynamically to guide translation of a given sentence. Similar to our work, (Farajian et al., 2017; Li et al., 2018) retrieve similar sentences from the training data to dynamically adapt individual input sentences. To compute similarity, the first work uses n-gram matches, the second includes dense vector representations. In (Xu et al., 2019) the same approach is followed but authors consider for adaptation a bunch of semantically related input sentences to reduce adaptation time. Our approach combines source and target words within a same sentence - the same type of approach has also been proposed by (Dinu et al., 2019) for introduction of terminology translation. Last, we can also compare the extra-token"
2020.acl-main.144,2015.iwslt-evaluation.11,0,0.0451284,"ble 3 (2 block), the best com+ + bination of matches is achieved by ⊕(FM ,EM ) further boosting the performance of previous con+ + figurations. It is only surpassed by ⊖(FM ,EM ) in two test sets by a slight margin. Fine Tuning Results so far evaluate the ability of NMT models to integrate similar sentences. However, we have run our comparisons over a “generic” model built from a heterogeneous training data set while it is well known that these models do not achieve best performance on homogeneous test sets. Thus, we now assess the capability of our augmentation methods to enhance fine-tuned (Luong and Manning, 2015) models, a well known technique that is commonly used in domain adaptation scenarios obtaining state-of-the-art results. Table 3 illustrates the results of the model configurations previously described after fine-tuning the models towards each test set domain. Thus, building 7 fine-tuned models for each configuration. Note that similar sentences (matches) are retrieved from the same in-domain data sets used for fine tuning. As 1586 ⊕(FM ,EM ) How long does a cold last ? ∥ Combien de temps dure le vol ? ∥ Combien de temps dure un vaccin ? S S S S S S SR T T T T R R TE E E E E E E E + + + + Figu"
2020.acl-main.144,C16-1172,0,0.11237,"Missing"
2020.acl-main.144,2016.amta-researchers.3,0,0.0802957,"Missing"
2020.acl-main.144,N18-1049,0,0.0467461,"de temps dure le vol?]. Even though the TM entry may be of great help when translating the input sentence, it receives a low 5 score (1 − 12 = 0.583) because of the multiple insertion/deletion operations needed. We thus introduce a second lexicalised similarity measure that focuses on finding the longest of n-gram overlap between sentences. 1581 2 https://github.com/systran/FuzzyMatch Distributed Representations The current research on sentence similarity measures has made tremendous advances thanks to distributed word representations computed by neural nets. In this 4 work, we use sent2vec (Pagliardini et al., 2018) to generate sentence embeddings. The network implements a simple but efficient unsupervised objective to train distributed representations of sentences. The authors claim that the algorithm performs state-of-the-art sentence representations on multiple benchmark tasks in particular for unsupervised similarity evaluation. We define the similarity score EM (si , sj ) between sentences si and sj via cosine similarity of their distributed representations hi and hj : ? last cold a does long How hi ⋅ hj ∣∣hi ∣∣ × ∣∣hj ∣∣ where ∣∣h∣∣ denotes the magnitude of vector h. To implement fast retrieval bet"
2020.acl-main.144,N16-1005,0,0.159905,"subtitles (TED); Parallel sentences extracted from Wikipedia (Wiki); Documentation from the European Central Bank (ECB); Documents from the European Medicines Agency (EMEA); Legislative texts of the European Union (JRC); Localisation files (GNOME, KDE4 and Ubuntu) and Manual texts (PHP). Detailed statistics about these are provided in Appendix A. We randomly split the corpora by keeping 500 sentences for validation, 1, 000 sentences for testing and the rest for training. All data is preprocessed using the Open9 NMT tokenizer (conservative mode). We train a 32K joint byte-pair encoding (BPE) (Sennrich et al., 2016b) and use a joint vocabulary for both source and target. Our NMT model follows the state-of-the-art Transformer base architecture (Vaswani et al., 10 2017) implemented in the OpenNMT-tf toolkit (Klein et al., 2017). Further configuration details are given in Appendix B. 3.2 TM Retrieval We perform fuzzy matching, ignoring exact matches, and keep the single best match if F M (si , sj ) ≥ 0.6 with no approximation. Similarly, the largest N -gram match is used for each test sentence with a threshold N M (si , sj ) ≥ 5. A similarity threshold EM (si , sj ) ≥ 0.8 is also employed when retrieving s"
2020.acl-main.144,P13-1002,0,0.356951,"technologies are entirely different in their finality - indeed, for a given source sentence, fuzzy matching is just a database retrieval and scoring technique always returning a pair of source and target segments, while machine translation is actually building an original translation. However, with Statistical Machine Translation, the two technologies are sharing the same simple idea about managing and retrieving optimal combination of longest translated n-grams and this property led to the development of several techniques like use of fuzzy matches in SMT decoding (Koehn and Senellart, 2010; Wang et al., 2013), adaptive machine translation (Zaretskaya et al., 2015) or “fuzzy match repairing” (Ortega et al., 2016). With Neural Machine Translation (NMT), the integration of Fuzzy Matching is less obvious since NMT does not keep nor build a database of aligned sequences and does not explicitly use n-gram language models for decoding. The only obvious and important use of translation memory is to use them to train an NMT model from scratch or to adapt a generic translation model to a specific domain (fine-tuning) (Chu and Wang, 2018). While some works propose architecture changes (Zhang et al., 2018) or"
2020.acl-main.144,N18-1120,0,0.217504,"10; Wang et al., 2013), adaptive machine translation (Zaretskaya et al., 2015) or “fuzzy match repairing” (Ortega et al., 2016). With Neural Machine Translation (NMT), the integration of Fuzzy Matching is less obvious since NMT does not keep nor build a database of aligned sequences and does not explicitly use n-gram language models for decoding. The only obvious and important use of translation memory is to use them to train an NMT model from scratch or to adapt a generic translation model to a specific domain (fine-tuning) (Chu and Wang, 2018). While some works propose architecture changes (Zhang et al., 2018) or decoding constraints (Gu et al., 2018); a recent work (Bult´e and Tezcan, 2019; Bult´e et al., 2018) has proposed a simple and elegant framework where, like for human translation, translation of fuzzy matches are presented simultaneously with source sentence and the network learns to use this additional information. Even though this method has showed huge gains in quality, it also opens 1580 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1580–1590 c July 5 - 10, 2020. 2020 Association for Computational Linguistics many questions. In this work"
2020.acl-main.144,P16-1162,0,0.494194,"subtitles (TED); Parallel sentences extracted from Wikipedia (Wiki); Documentation from the European Central Bank (ECB); Documents from the European Medicines Agency (EMEA); Legislative texts of the European Union (JRC); Localisation files (GNOME, KDE4 and Ubuntu) and Manual texts (PHP). Detailed statistics about these are provided in Appendix A. We randomly split the corpora by keeping 500 sentences for validation, 1, 000 sentences for testing and the rest for training. All data is preprocessed using the Open9 NMT tokenizer (conservative mode). We train a 32K joint byte-pair encoding (BPE) (Sennrich et al., 2016b) and use a joint vocabulary for both source and target. Our NMT model follows the state-of-the-art Transformer base architecture (Vaswani et al., 10 2017) implemented in the OpenNMT-tf toolkit (Klein et al., 2017). Further configuration details are given in Appendix B. 3.2 TM Retrieval We perform fuzzy matching, ignoring exact matches, and keep the single best match if F M (si , sj ) ≥ 0.6 with no approximation. Similarly, the largest N -gram match is used for each test sentence with a threshold N M (si , sj ) ≥ 5. A similarity threshold EM (si , sj ) ≥ 0.8 is also employed when retrieving s"
2020.acl-main.144,tiedemann-2012-parallel,0,0.0586845,"the duration of flu symptoms ? How long does a cold last ? ∥ Quelle est la dur´ee de la grippe ? S S S S S S SE E E E E E E E E ∗ FM As a variant of FM , we now mark target words which are not related to the input sentence in an attempt to help the network identify those target Figure 2: Input sentence augmented with different TM # ∗ + matches: FM (Bult´e and Tezcan, 2019), FM , FM and + EM . 7 The original paper uses ‘@@@’ as break token. We made sure that ∥ was not part of the vocabulary. 1583 3 Experimental Framework 3.1 Corpora and Evaluation 8 We used the following corpora in this work (Tiedemann, 2012): Proceedings of the European Parliament (EPPS); News Commentaries (NEWS); TED talk subtitles (TED); Parallel sentences extracted from Wikipedia (Wiki); Documentation from the European Central Bank (ECB); Documents from the European Medicines Agency (EMEA); Legislative texts of the European Union (JRC); Localisation files (GNOME, KDE4 and Ubuntu) and Manual texts (PHP). Detailed statistics about these are provided in Appendix A. We randomly split the corpora by keeping 500 sentences for validation, 1, 000 sentences for testing and the rest for training. All data is preprocessed using the Open9"
2020.acl-main.144,W15-4920,0,0.0436122,"ber of overlaps between the sentences taken into account. The latter counts on the generalisation F M (si , sj ) = 1 − ED(si , sj ) max(∣si ∣, ∣sj ∣) where ED(si , sj ) is the Edit Distance between si and sj , and ∣s∣ is the length of s. Many variants have been proposed to compute the edit distance, generally performed on normalized sentences (ignoring for instance case, number, punctuation, space or inline tags differences that are typically handled at a later stage). Also, IDF and stemming techniques are used to give more weight on significant words or less weight on morphological variants (Vanallemeersch and Vandeghinste, 2015; Bloodgood and Strauss, 2014). Since we did not find an efficient TM fuzzy match library, we implemented an efficient and parameterizable algorithm in C++ based on suffixarray (Manber and Myers, 1993) that we open2 sourced . Fuzzy matching offers a great performance under large overlapping conditions. However, in some cases, sentences with large overlaps may receive low F M scores. Consider for instance the input: [How long does the flight arriving in Paris from Barcelona last?] and the TM entry of our previous example: [How long does the flight last?] ↝ [Combien de temps dure le vol?]. Even"
2020.coling-main.348,P19-1175,0,0.0318136,"Missing"
2020.coling-main.348,W17-4716,0,0.0867319,"Vilar, 2018; Susanto et al., 2020) attempt to reduce the computational problem caused by using multiple beams in the inference, a well known weakness of this approach. Similar to the previous approach, constrained decoding does not consider target context when inserting translation terms, as it sets the target form and then produces a target context that fits this constraint. However, in a more realistic scenario, a source term may have multiple translation term inflections among which the MT engine should on-the-fly select the best one depending on the source and target context. Previously, Chatterjee et al. (2017) proposed a guide mechanism to enhance an NMT network with the ability to prioritize translation options presented in the form of XML annotations of source words. The mechanism is applied at every inference time-step, where the beam search is influenced with external suggestions coming from the attention model. Similarly, Zhang et al. (2018) exploit a search engine to retrieve sentence pairs whose source sides are similar with the input sentence, from which they collect translation pieces. Then, the NMT model is modified to give an additional bonus to output sentences that contain the collecte"
2020.coling-main.348,W19-5402,0,0.0201923,"e-case where terminology is used in a system trained on generic data only. 1 Introduction High out-of-the-box quality for Neural Machine Translation (Bojar et al., 2016) has boosted the adoption of automatic translation by the industry and invigorated the research and development on domain adaption and integration of technology in human translation workflows. For instance, combination with translation memories (Bult´e and Tezcan, 2019; Xu et al., 2020), terminology handling (Hasler et al., 2018; Dinu et al., 2019), interactive translation (Peris and Casacuberta, 2019), post-editing modelling (Chatterjee et al., 2019) or dynamic adaptation (Farajian et al., 2017) are all different techniques to make machine translation part of real-life localization workflow. In this work, we focus on integrating terminology as a quick way to dynamically specialize a translation to a specific domain. Terminology is a key high quality asset maintained by language specialists as part of a translation project: it is a way to guarantee language consistency, certify translation accuracy and define constraints to human translation. Terminologists are putting a lot of effort to describe terms, including their morphology, their sy"
2020.coling-main.348,P18-1198,0,0.0126742,"a specific domain in a specific context. Terminology resources with all their sophistication have been the core building bricks and a continuous challenge to acquire in volume (Senellart et al., 2003) for rule-based engines. At the other extreme, they have been reduced to corpus or aligned “phrases” (Schwenk et al., 2008) for Statistical Machine Translation approaches, missing most of their intrinsic linguistic properties. In contrast, Neural Machine Translation operates on word and sentence representations in a continuous space so, on one hand, has access to deep actual linguistic knowledge (Conneau et al., 2018) and demonstrates a huge ability to generalize. But on the other hand, results are more difficult to interpret (Koehn and Knowles, 2017), and subsequently the translation process is far more complicated to control. Therefore, as for several other linguistic annotations, the challenge is how terminological information can be “passed” to the model. In this work, we extend existing work on terminology adaptation, show similarity with translation memory, and propose a new approach and new benchmark through a well-defined evaluation framework focusing on actual application of terminology and not ju"
2020.coling-main.348,N18-1119,0,0.0159609,"ained decoding enforces translation terms as decoding constraints applied at inference. Among others, Hokamp and Liu (2017) introduced grid beam search (GBS), an algorithm which employs a separate beam for each lexical constraint (translation term) aiming at ensuring the apparition of each given constraint in the translation hypothesis. The algorithm explores all possible constraints at each time-step, making sure not to generate a constraint that has already been generated in previous timesteps. The approach generates all the constraints in the final output. Other works (Hasler et al., 2018; Post and Vilar, 2018; Susanto et al., 2020) attempt to reduce the computational problem caused by using multiple beams in the inference, a well known weakness of this approach. Similar to the previous approach, constrained decoding does not consider target context when inserting translation terms, as it sets the target form and then produces a target context that fits this constraint. However, in a more realistic scenario, a source term may have multiple translation term inflections among which the MT engine should on-the-fly select the best one depending on the source and target context. Previously, Chatterjee e"
2020.coling-main.348,W08-0313,1,0.696081,"hich these terms apply, etc. From a human perspective, even though presentation and usage of dictionaries have evolved from ontology (as found in paper dictionary) to corpus-based presentation, looking up terms in a dictionary is the ultimate point of reference for validating the correct term for a specific domain in a specific context. Terminology resources with all their sophistication have been the core building bricks and a continuous challenge to acquire in volume (Senellart et al., 2003) for rule-based engines. At the other extreme, they have been reduced to corpus or aligned “phrases” (Schwenk et al., 2008) for Statistical Machine Translation approaches, missing most of their intrinsic linguistic properties. In contrast, Neural Machine Translation operates on word and sentence representations in a continuous space so, on one hand, has access to deep actual linguistic knowledge (Conneau et al., 2018) and demonstrates a huge ability to generalize. But on the other hand, results are more difficult to interpret (Koehn and Knowles, 2017), and subsequently the translation process is far more complicated to control. Therefore, as for several other linguistic annotations, the challenge is how terminolog"
2020.coling-main.348,2003.mtsummit-papers.46,1,0.276529,"inologists are putting a lot of effort to describe terms, including their morphology, their syntax, the semantic context in which these terms apply, etc. From a human perspective, even though presentation and usage of dictionaries have evolved from ontology (as found in paper dictionary) to corpus-based presentation, looking up terms in a dictionary is the ultimate point of reference for validating the correct term for a specific domain in a specific context. Terminology resources with all their sophistication have been the core building bricks and a continuous challenge to acquire in volume (Senellart et al., 2003) for rule-based engines. At the other extreme, they have been reduced to corpus or aligned “phrases” (Schwenk et al., 2008) for Statistical Machine Translation approaches, missing most of their intrinsic linguistic properties. In contrast, Neural Machine Translation operates on word and sentence representations in a continuous space so, on one hand, has access to deep actual linguistic knowledge (Conneau et al., 2018) and demonstrates a huge ability to generalize. But on the other hand, results are more difficult to interpret (Koehn and Knowles, 2017), and subsequently the translation process"
2020.coling-main.348,P16-1162,0,0.0398528,"terminologies with different translations according to the domain as can be seen in the next examples: accordance (noun) ECB/NEWS: conformidad (noun) EMEA: acuerdo (noun) EPPS/JRC: arreglo (noun) 4.3 move (verb) COMM: migrar (verb) ECB: pasar (verb) KDE4: mover (verb) NEWS: ascender (verb) move (noun) JRC: decisi´on (noun) KDE4: movimiento (noun) NEWS: maniobra (noun) Neural Machine Translation Our NMT models follow the state-of-the-art Transformer architecture described in Vaswani et al. (2017) implemented in the OpenNMT-tf13 toolkit. Before learning, we train a 32K joint byte-pair encoding (Sennrich et al., 2016) not applying on introduced placeholders. Note that all models are learnt using a joint source and target vocabulary and shared word embeddings to allow the injection of target words in the source stream. This is only required by one configuration but it enables a fair comparison and does not harm the rest of models. Additional details of our translation networks are given in Appendix A. 4.4 Experiments We evaluate the following configurations: • app: the target inflected term is appended to the source term. We use an additional parallel stream (factor) to indicate if each word is a term to in"
2020.ngt-1.25,D18-1045,0,0.0128552,"l. (2020) which shows that reducing the number of decoder layers can improve decoding speed at a very limited accuracy cost. We also keep the approach of running the models with a custom C++ runtime. This year we present CTranslate21 , an optimized and production-grade 1 https://github.com/OpenNMT/ CTranslate2 2 Teacher-student training We train our systems using a teacher-student approach (Kim and Rush, 2016). First, a large model (the teacher) is trained on all available bilingual data, including synthetic data such as backtranslations of monolingual target sentences (Sennrich et al., 2016; Edunov et al., 2018) and translations of monolingual source sentences (Zhang and Zong, 2016). Model ensembles are also typically used to build stronger teacher systems. Then, a small model (the student) is trained by means of minimizing the loss between the student and teacher systems with the goal of distilling the knowledge of the teacher (Kim and Rush, 2016; Zhang et al., 2018) into a smaller model with comparable accuracy results. Crego and Senellart (2016) show that student models can even outperform to some extent their teacher counterparts. Knowledge distillation is an effective approach to reduce the mode"
2020.ngt-1.25,D16-1139,0,0.0244369,"d Translation 2020 efficiency shared task. For WNMT 2018, we explored training and optimizations of small LSTM translation models combined with a customized runtime (Senellart et al., 2018). While this resulted in interesting decoding speed, there was still room for improvements in terms of quality, memory usage, and overall efficiency. For this 2020 edition, we focus on the standard Transformer architecture (Vaswani et al., 2017) that is now commonly used in production machine translation systems. Similar to our first participation, we train smaller models using the teacherstudent technique (Kim and Rush, 2016). We experiment with several encoder and decoder sizes following the work by Hongfei et al. (2020) which shows that reducing the number of decoder layers can improve decoding speed at a very limited accuracy cost. We also keep the approach of running the models with a custom C++ runtime. This year we present CTranslate21 , an optimized and production-grade 1 https://github.com/OpenNMT/ CTranslate2 2 Teacher-student training We train our systems using a teacher-student approach (Kim and Rush, 2016). First, a large model (the teacher) is trained on all available bilingual data, including synthet"
2020.ngt-1.25,P17-4012,1,0.518532,"the accuracy and efficiency results achieved by the submitted models. This paper describes the OpenNMT submissions to the WNGT 2020 efficiency shared task. We explore training and acceleration of Transformer models with various sizes that are trained in a teacher-student setup. We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies. By combining additional optimizations and parallelization techniques, we create small, efficient, and highquality neural machine translation models. 1 Introduction This paper describes the OpenNMT (Klein et al., 2017) submissions to the Workshop on Neural Generation and Translation 2020 efficiency shared task. For WNMT 2018, we explored training and optimizations of small LSTM translation models combined with a customized runtime (Senellart et al., 2018). While this resulted in interesting decoding speed, there was still room for improvements in terms of quality, memory usage, and overall efficiency. For this 2020 edition, we focus on the standard Transformer architecture (Vaswani et al., 2017) that is now commonly used in production machine translation systems. Similar to our first participation, we train"
2020.ngt-1.25,P12-3005,0,0.113224,"Missing"
2020.ngt-1.25,W19-5333,0,0.0250952,"Missing"
2020.ngt-1.25,W18-6301,0,0.0193471,"sentences. We set the sampling weights of the selected data (a), (b), and (c) to 5, 2, and 2 respectively. That is, we consider a larger number of sentences synthesized from the English part of the bilingual data than from ParaCrawl or from the monolingual English data set. We use the OpenNMT-tf5 toolkit to train our student systems. Training is run on a single NVIDIA Tesla V100 GPU with an effective batch size of 25,000 tokens for the early epochs. Just before the final release, we train 10 additional epochs with a larger batch size by increasing the gradient update delay by a factor of 16 (Ott et al., 2018). Figure 1 shows the comparison with a larger batch size. We achieve an additional 0.1 to 0.2 BLEU using this 2 http://matrix.statmt.org/ http://statmt.org/wmt19/ translation-task.html 3 212 4 Due to the long decoding time of the teacher system, the English monolingual data was partially translated. The final data pool used for training consists of: (a) 7.4M bilingual data, (b) 26.1M ParaCrawl data, and (c) 127M English monolingual data. 5 https://github.com/OpenNMT/OpenNMT-tf Transformer Base (4:3 2xFFN) (6:3) (4:3) NEnc 6 4 6 4 NDec 6 3 3 3 h 8 8 8 8 dmodel 512 256 256 256 df f 2048 2048 102"
2020.ngt-1.25,P02-1040,0,0.106003,"Missing"
2020.ngt-1.25,D16-1160,0,0.0159878,"prove decoding speed at a very limited accuracy cost. We also keep the approach of running the models with a custom C++ runtime. This year we present CTranslate21 , an optimized and production-grade 1 https://github.com/OpenNMT/ CTranslate2 2 Teacher-student training We train our systems using a teacher-student approach (Kim and Rush, 2016). First, a large model (the teacher) is trained on all available bilingual data, including synthetic data such as backtranslations of monolingual target sentences (Sennrich et al., 2016; Edunov et al., 2018) and translations of monolingual source sentences (Zhang and Zong, 2016). Model ensembles are also typically used to build stronger teacher systems. Then, a small model (the student) is trained by means of minimizing the loss between the student and teacher systems with the goal of distilling the knowledge of the teacher (Kim and Rush, 2016; Zhang et al., 2018) into a smaller model with comparable accuracy results. Crego and Senellart (2016) show that student models can even outperform to some extent their teacher counterparts. Knowledge distillation is an effective approach to reduce the model size, thus lowering memory and computation requirements. 2.1 Teacher s"
2020.ngt-1.25,W18-6319,0,0.020918,"Missing"
2020.ngt-1.25,W18-2715,1,0.850979,"at are trained in a teacher-student setup. We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies. By combining additional optimizations and parallelization techniques, we create small, efficient, and highquality neural machine translation models. 1 Introduction This paper describes the OpenNMT (Klein et al., 2017) submissions to the Workshop on Neural Generation and Translation 2020 efficiency shared task. For WNMT 2018, we explored training and optimizations of small LSTM translation models combined with a customized runtime (Senellart et al., 2018). While this resulted in interesting decoding speed, there was still room for improvements in terms of quality, memory usage, and overall efficiency. For this 2020 edition, we focus on the standard Transformer architecture (Vaswani et al., 2017) that is now commonly used in production machine translation systems. Similar to our first participation, we train smaller models using the teacherstudent technique (Kim and Rush, 2016). We experiment with several encoder and decoder sizes following the work by Hongfei et al. (2020) which shows that reducing the number of decoder layers can improve deco"
2020.ngt-1.25,P16-1009,0,0.0259392,"he work by Hongfei et al. (2020) which shows that reducing the number of decoder layers can improve decoding speed at a very limited accuracy cost. We also keep the approach of running the models with a custom C++ runtime. This year we present CTranslate21 , an optimized and production-grade 1 https://github.com/OpenNMT/ CTranslate2 2 Teacher-student training We train our systems using a teacher-student approach (Kim and Rush, 2016). First, a large model (the teacher) is trained on all available bilingual data, including synthetic data such as backtranslations of monolingual target sentences (Sennrich et al., 2016; Edunov et al., 2018) and translations of monolingual source sentences (Zhang and Zong, 2016). Model ensembles are also typically used to build stronger teacher systems. Then, a small model (the student) is trained by means of minimizing the loss between the student and teacher systems with the goal of distilling the knowledge of the teacher (Kim and Rush, 2016; Zhang et al., 2018) into a smaller model with comparable accuracy results. Crego and Senellart (2016) show that student models can even outperform to some extent their teacher counterparts. Knowledge distillation is an effective appro"
2020.wmt-1.63,N19-1423,0,0.00803713,"f concatenating previous and current sentences was explored by Tiedemann and Scherrer (2017) further evaluated by Bawden et al. (2018) in the context of tackling discourse phenomena. Our work employs force decoding to allow including true translations in the decoder targetside. Thus, avoiding the error propagation problem (Ranzato et al., 2016) of longer sequences in auto-regressive models. Bapna and Firat (2019) propose a neural MT model that incorporates retrieved neighbours relying on local phrase level similarities. Using deep pre-trained models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Le et al., 2020; Conneau and Lample, 2019) to compute contextualized sentence representations has become common fashion in recent works (Feng et al., 2020; Chang et al., 2020). However, deep models suffer from computation complexity when applied onthe-fly for inference. We propose an extension of sent2vec (Pagliardini et al., 2018) to compute sentence representations that also inherits from the computationally efficient bilinear models (Mikolov et al., 2013a,b; Pennington et al., 2014). Similar to our work, Farajian et al. (2017) and Li et al. (2018) retrieve similar sentence to dynamically"
2020.wmt-1.63,P19-1294,0,0.0266447,"ed: for instance, in Rosenfeld et al. (2018), the authors introduce a cue about the presence of a certain class of object in an image that significantly improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Following up on previous work (Bulte and Tez"
2020.wmt-1.63,N19-1191,0,0.0170632,"based MT system. Our work, in contrast, integrates similar sentences in both source and target sides and employs similar translations found in parallel as well as monolingual data sets. A similar strategy of concatenating previous and current sentences was explored by Tiedemann and Scherrer (2017) further evaluated by Bawden et al. (2018) in the context of tackling discourse phenomena. Our work employs force decoding to allow including true translations in the decoder targetside. Thus, avoiding the error propagation problem (Ranzato et al., 2016) of longer sequences in auto-regressive models. Bapna and Firat (2019) propose a neural MT model that incorporates retrieved neighbours relying on local phrase level similarities. Using deep pre-trained models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Le et al., 2020; Conneau and Lample, 2019) to compute contextualized sentence representations has become common fashion in recent works (Feng et al., 2020; Chang et al., 2020). However, deep models suffer from computation complexity when applied onthe-fly for inference. We propose an extension of sent2vec (Pagliardini et al., 2018) to compute sentence representations that also inherits from t"
2020.wmt-1.63,N18-1118,0,0.0172585,"xplored in Schwenk et al. (2019b), where the authors use multilingual sentence embeddings to retrieve pairs of similar sentences and train models uniquely with such sentences. Previously, Niehues et al. (2016) augmented input sentences with pre-translations generated by a phrase-based MT system. Our work, in contrast, integrates similar sentences in both source and target sides and employs similar translations found in parallel as well as monolingual data sets. A similar strategy of concatenating previous and current sentences was explored by Tiedemann and Scherrer (2017) further evaluated by Bawden et al. (2018) in the context of tackling discourse phenomena. Our work employs force decoding to allow including true translations in the decoder targetside. Thus, avoiding the error propagation problem (Ranzato et al., 2016) of longer sequences in auto-regressive models. Bapna and Firat (2019) propose a neural MT model that incorporates retrieved neighbours relying on local phrase level similarities. Using deep pre-trained models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Le et al., 2020; Conneau and Lample, 2019) to compute contextualized sentence representations has become common f"
2020.wmt-1.63,P19-1175,0,0.129039,"improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Following up on previous work (Bulte and Tezcan, 2019; Xu et al., 2020), we consider similar translations as external cues that can influence the translation process. We push this concept further: a) b"
2020.wmt-1.63,W17-4713,0,0.366859,"NVIDIA V100 GPU. We limit the length of training sentences to 300 BPE tokens (Sennrich et al., 2016c) in both source and target sides to enable the integration of similar sentences. We use a joint BPE-vocabulary of size 32K for both source and target texts. Inference is performed with a beam size of 5 using CTranslate210 , a custom C++ runtime inference engine for OpenNMT models that enables fast CPU decoding and also implements prefix decoding. For evaluation, we report BLEU (Papineni et al., 2002) scores computed by detokenized case-sensitive multi-bleu.perl11 . We re-implement the work of Farajian et al. (2017) as a contrastive model that we denote µadapt. Note that we only experiment with the basic version of this work, where the closest neighbours of the input sentence are first retrieved from the memory and then used to fine-tune a generic model during 15 additional iterations with a fixed learning rate of 0.0005; the fine-tuned model is then used to produce the translation of the given input sentence. In addition, Farajian et al. (2017) include a variant where learning rate and number of epochs are dynamically adapted considering sentence similarity. Adaptation is run on a sentenceby-sentence ba"
2020.wmt-1.63,P17-4012,1,0.0302657,"he French-side of the WikiMatrix data (Schwenk et al., 2019a). We randomly split the parallel corpora by keeping 500 sentences for validation, 1, 000 sentences for testing and the rest for training. All data is preprocessed using the OpenNMT tokenizer6 (conservative mode). Sentence Retrieval To identify similar translations using distributed representations, we use the faiss8 search toolkit (Johnson et al., 2019) through its Python API with exact FlatIP index. Translation Our NMT models rely on the Transformer base architecture of Vaswani et al. (2017), implemented in the OpenNMT-tf9 toolkit (Klein et al., 2017). We use the standard setting of Transformers for all experiments: size of word embedding: 512; size of hidden layers: 512; size of inner feed-forward layer: 2, 048; number of heads: 8; number of layers in the encoder or in the decoder: 6. In the tgt1 +STU scheme, token (508 cells) and STU (4 4 Freely available from http://opus.nlpl.eu http://data.statmt.org/news-crawl/ 6 https://github.com/OpenNMT/Tokenizer 5 519 7 Optimization experiments on a held-out development set are carried out for both models. 8 https://github.com/facebookresearch/ faiss 9 https://github.com/OpenNMT/OpenNMT-tf cells)"
2020.wmt-1.63,2016.amta-researchers.9,0,0.008848,"-the-fly priming compares to micro-adaptation (fine-tuning). Finally, we 1 https://github.com/jmcrego/cbon 516 Proceedings of the 5th Conference on Machine Translation (WMT), pages 516–527 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics show that our priming approach can also be used with monolingual data, providing a scenario where NMT can be effectively helped by large amounts of available data. Our proposal does not require to change the NMT architectures or algorithms, relying solely on input preprocessing and on prefix (forced) decoding (Santy et al., 2019; Knowles and Koehn, 2016), a feature already implemented in many NMT toolkits. The remainder of the paper is organized as follows: Section 2 gives details regarding our priming approach. The experimental framework is presented in Section 3. Results and discussion are respectively in Sections 4 and 5. We review related work in Section 6 and conclude in Section 7. 2 S2V: we use sent2vec3 (Pagliardini et al., 2018) to generate sentence embeddings. The network implements a simple but efficient unsupervised objective to train distributed representations for sentences. The model is based on efficient matrix factor (bilinear"
2020.wmt-1.63,kobus-etal-2017-domain,1,0.851148,"s been broadly studied: for instance, in Rosenfeld et al. (2018), the authors introduce a cue about the presence of a certain class of object in an image that significantly improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Following up on previous"
2020.wmt-1.63,2010.jec-1.4,1,0.473968,"arding the corpora used in this work4 (Tiedemann, 2012). Statistics are computed after splitting off punctuation. Corpus EPPS NEWS WIKI ECB EMEA JRC GNOME KDE4 WIKI NEWS #Sents (K) System Configurations Lmean English French Vocab (K) English Parallel Corpora 1,992.8 27.7 32.0 129.5 315.3 25.3 31.7 90.5 749.0 25.9 23.5 527.5 174.1 28.6 33.8 45.3 336.8 16.8 20.3 62.8 475.2 30.1 34.5 81.0 51.9 9.6 11.6 19.0 163.9 9.1 12.4 48.7 Monolingual Corpora 6,426.8 24.1 83,567.8 25.5 - French 149.2 96.7 506.6 53.5 68.9 83.5 21.6 64.7 1,626.3 3,444.1 Similarity For fuzzy matching FM we follow several works (Koehn and Senellart, 2010; Bulte and Tezcan, 2019; Xu et al., 2020) and keep the n-best matches when FM(s1 , s2 ) ≥ 0.5 with no approximation. Concerning S2V, the model is trained with default options during 20 epochs using all training data. We use an embedding dimension of 300 cells. Regarding CBON, we learn models using also the entire training data during one epoch (∼50,000 iterations). Similarly to S2V we use 10 negative samples per positive word to approximate the softmax, a batch size of 2k examples, and embedding size of 300 cells. We build CBON models using 3-grams and 4-grams to enable a comparison with sent"
2020.wmt-1.63,2020.lrec-1.302,0,0.0229122,"Missing"
2020.wmt-1.63,L18-1146,0,0.064128,"Missing"
2020.wmt-1.63,C16-1172,0,0.0170125,"Missing"
2020.wmt-1.63,N18-1049,0,0.0112408,"helped by large amounts of available data. Our proposal does not require to change the NMT architectures or algorithms, relying solely on input preprocessing and on prefix (forced) decoding (Santy et al., 2019; Knowles and Koehn, 2016), a feature already implemented in many NMT toolkits. The remainder of the paper is organized as follows: Section 2 gives details regarding our priming approach. The experimental framework is presented in Section 3. Results and discussion are respectively in Sections 4 and 5. We review related work in Section 6 and conclude in Section 7. 2 S2V: we use sent2vec3 (Pagliardini et al., 2018) to generate sentence embeddings. The network implements a simple but efficient unsupervised objective to train distributed representations for sentences. The model is based on efficient matrix factor (bilinear) models (Mikolov et al., 2013a,b; Pennington et al., 2014). Borrowing the notations of Pagliardini et al. (2018), training the model is formalized as an optimization problem: min U ,V This section describes our framework for priming neural MT with similar translations. We follow the work by (Bulte and Tezcan, 2019; Xu et al., 2020) and build a translation model that incorporates similar"
2020.wmt-1.63,P02-1040,0,0.121238,"000 and update the learning rate for every 8 iterations. Models are optimised during 300K iterations, using a single NVIDIA V100 GPU. We limit the length of training sentences to 300 BPE tokens (Sennrich et al., 2016c) in both source and target sides to enable the integration of similar sentences. We use a joint BPE-vocabulary of size 32K for both source and target texts. Inference is performed with a beam size of 5 using CTranslate210 , a custom C++ runtime inference engine for OpenNMT models that enables fast CPU decoding and also implements prefix decoding. For evaluation, we report BLEU (Papineni et al., 2002) scores computed by detokenized case-sensitive multi-bleu.perl11 . We re-implement the work of Farajian et al. (2017) as a contrastive model that we denote µadapt. Note that we only experiment with the basic version of this work, where the closest neighbours of the input sentence are first retrieved from the memory and then used to fine-tune a generic model during 15 additional iterations with a fixed learning rate of 0.0005; the fine-tuned model is then used to produce the translation of the given input sentence. In addition, Farajian et al. (2017) include a variant where learning rate and nu"
2020.wmt-1.63,D14-1162,0,0.118332,"many NMT toolkits. The remainder of the paper is organized as follows: Section 2 gives details regarding our priming approach. The experimental framework is presented in Section 3. Results and discussion are respectively in Sections 4 and 5. We review related work in Section 6 and conclude in Section 7. 2 S2V: we use sent2vec3 (Pagliardini et al., 2018) to generate sentence embeddings. The network implements a simple but efficient unsupervised objective to train distributed representations for sentences. The model is based on efficient matrix factor (bilinear) models (Mikolov et al., 2013a,b; Pennington et al., 2014). Borrowing the notations of Pagliardini et al. (2018), training the model is formalized as an optimization problem: min U ,V This section describes our framework for priming neural MT with similar translations. We follow the work by (Bulte and Tezcan, 2019; Xu et al., 2020) and build a translation model that incorporates similar translations from a translation memory (TM) to boost translation accuracy. In this work, TMs are parallel corpora containing translations falling in the same domain as test sentences. We first describe the methods employed in this work to compute sentence similarity."
2020.wmt-1.63,N18-1202,0,0.0171998,"monolingual data sets. A similar strategy of concatenating previous and current sentences was explored by Tiedemann and Scherrer (2017) further evaluated by Bawden et al. (2018) in the context of tackling discourse phenomena. Our work employs force decoding to allow including true translations in the decoder targetside. Thus, avoiding the error propagation problem (Ranzato et al., 2016) of longer sequences in auto-regressive models. Bapna and Firat (2019) propose a neural MT model that incorporates retrieved neighbours relying on local phrase level similarities. Using deep pre-trained models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Le et al., 2020; Conneau and Lample, 2019) to compute contextualized sentence representations has become common fashion in recent works (Feng et al., 2020; Chang et al., 2020). However, deep models suffer from computation complexity when applied onthe-fly for inference. We propose an extension of sent2vec (Pagliardini et al., 2018) to compute sentence representations that also inherits from the computationally efficient bilinear models (Mikolov et al., 2013a,b; Pennington et al., 2014). Similar to our work, Farajian et al. (2017) and Li et al. (2018"
2020.wmt-1.63,D19-3018,0,0.0124582,"by analyzing how on-the-fly priming compares to micro-adaptation (fine-tuning). Finally, we 1 https://github.com/jmcrego/cbon 516 Proceedings of the 5th Conference on Machine Translation (WMT), pages 516–527 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics show that our priming approach can also be used with monolingual data, providing a scenario where NMT can be effectively helped by large amounts of available data. Our proposal does not require to change the NMT architectures or algorithms, relying solely on input preprocessing and on prefix (forced) decoding (Santy et al., 2019; Knowles and Koehn, 2016), a feature already implemented in many NMT toolkits. The remainder of the paper is organized as follows: Section 2 gives details regarding our priming approach. The experimental framework is presented in Section 3. Results and discussion are respectively in Sections 4 and 5. We review related work in Section 6 and conclude in Section 7. 2 S2V: we use sent2vec3 (Pagliardini et al., 2018) to generate sentence embeddings. The network implements a simple but efficient unsupervised objective to train distributed representations for sentences. The model is based on efficie"
2020.wmt-1.63,2021.eacl-main.115,0,0.0345079,"Missing"
2020.wmt-1.63,W17-2619,0,0.0145288,"language (French in this work) and translate each sentence back into English to obtain synthetic parallel data. Similar to back-translation experiments in Sennrich et al. (2016b), we only use original (human-crafted) target-language data. We expect this to add less noise than incorporating synthetic target-language data into the NMT input. Once translated into English, the various priming approaches identify similar synthetic sentences and injects both the synthetic source and original target in the NMT input stream. Note that crosslingual sentence embedding models exist (Sabet et al., 2019; Schwenk and Douze, 2017; Conneau and Lample, 2019) but our preliminary experiments using these tools did not show satisfactory results. Thus, we exploit large collections of French texts for the News and Wikipedia domains (as detailed in Table 1) that we translate into English to enable similarity retrieval. Table 4 reports BLEU scores obtained by our best performing network CBON following the s+t5 scheme. The supplementary number of similar sentences (468 input sentences have similar translations) collected for the WIKI domain over parallel and mono521 lingual12 corpora (par+mon) yields an improvement of 2 BLEU poi"
2020.wmt-1.63,N16-1005,0,0.152513,"mputer vision priming has been broadly studied: for instance, in Rosenfeld et al. (2018), the authors introduce a cue about the presence of a certain class of object in an image that significantly improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Fol"
2020.wmt-1.63,P16-1009,0,0.160789,"mputer vision priming has been broadly studied: for instance, in Rosenfeld et al. (2018), the authors introduce a cue about the presence of a certain class of object in an image that significantly improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Fol"
2020.wmt-1.63,P16-1162,0,0.215406,"mputer vision priming has been broadly studied: for instance, in Rosenfeld et al. (2018), the authors introduce a cue about the presence of a certain class of object in an image that significantly improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Fol"
2020.wmt-1.63,tiedemann-2012-parallel,0,0.0135836,"is section gives learning/inference details of the various systems used in this work. Corpora We experiment with the English-French language pair and data originating from eight domains, corresponding to texts from three European institutions: the European Parliament (EPPS), the European Medicines Agency (EMEA) and the European Central Bank (ECB); Legislative texts of the European Union (JRC); IT-domain corpora corresponding to KDE4 and GNOME; News Commentaries (NEWS); and parallel sentences extracted from Wikipedia (WIKI). Table 1 contains statistics regarding the corpora used in this work4 (Tiedemann, 2012). Statistics are computed after splitting off punctuation. Corpus EPPS NEWS WIKI ECB EMEA JRC GNOME KDE4 WIKI NEWS #Sents (K) System Configurations Lmean English French Vocab (K) English Parallel Corpora 1,992.8 27.7 32.0 129.5 315.3 25.3 31.7 90.5 749.0 25.9 23.5 527.5 174.1 28.6 33.8 45.3 336.8 16.8 20.3 62.8 475.2 30.1 34.5 81.0 51.9 9.6 11.6 19.0 163.9 9.1 12.4 48.7 Monolingual Corpora 6,426.8 24.1 83,567.8 25.5 - French 149.2 96.7 506.6 53.5 68.9 83.5 21.6 64.7 1,626.3 3,444.1 Similarity For fuzzy matching FM we follow several works (Koehn and Senellart, 2010; Bulte and Tezcan, 2019; Xu e"
2020.wmt-1.63,W17-4811,0,0.174236,"sed in this paper also addresses the unrelated word problem, at a much reduced computational cost. It considers both sides of similar translations (sk and tk ). Training streams take the form: src: tgt: sk ◦ ... ◦ s2 ◦ s1 ◦ s tk ◦ ... ◦ t2 ◦ t1 ◦ t In inference, target-side similar translations tk are used by the model as a target prefix. The initial steps of the beam search use the given prefix tk ◦ ... ◦ t2 ◦ t1 ◦ in forced decoding mode, returning to a regular beam search after the last ◦ token is generated. A similar strategy of concatenating previous and current sentences was explored by Tiedemann and Scherrer (2017) in the context of handling discourse phenomena. However, since we use true translation as prefixes, our strategy does not suffer from exposure bias (Ranzato et al., 2016) and the subsequent error propagation problem. Continuing on our running example, during inference the model receives: input: prefix: measles vaccin ◦ pertussis vaccin vaccin contre la rougeole ◦ the encoder embeds the input stream, and forcedecodes the target prefix, before starting the translation generation. Note that during beam search, the decoder has thus access both to all input tokens (sk and s) as well as to similar"
2020.wmt-1.63,2020.acl-main.144,1,0.145304,"n performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Following up on previous work (Bulte and Tezcan, 2019; Xu et al., 2020), we consider similar translations as external cues that can influence the translation process. We push this concept further: a) by proposing a nove"
2020.wmt-1.72,2020.acl-main.688,0,0.0315866,"c domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of interest, one then adapts the pre-trained model by resuming training with the sole in-domain corpus. It is a conjecture that the pretrained model constitutes a better initialization than a random one, especially when adaptation data is scarce. Indeed, studies of transfer learning for NMT such as Artetxe et al. (2020); Aji et al. (2020) have confirmed this claim in extensive experiments. Full fine-tuning, that adapts all the parameters of a baseline model usually significantly improves the quality of the NMT for the chosen domain. However, it also yields large losses in translation quality for other domains, a phenomenon referred to as “catastrophic forgetting” in the neural network literature (McCloskey and Cohen, 1989). Therefore, a fully fine-tuned model is only useful to one target domain. As the number of domains to handle grows, training, and maintaining a separate model for each task can quickly become tedious and res"
2020.wmt-1.72,2020.acl-main.421,0,0.0420588,"g NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of interest, one then adapts the pre-trained model by resuming training with the sole in-domain corpus. It is a conjecture that the pretrained model constitutes a better initialization than a random one, especially when adaptation data is scarce. Indeed, studies of transfer learning for NMT such as Artetxe et al. (2020); Aji et al. (2020) have confirmed this claim in extensive experiments. Full fine-tuning, that adapts all the parameters of a baseline model usually significantly improves the quality of the NMT for the chosen domain. However, it also yields large losses in translation quality for other domains, a phenomenon referred to as “catastrophic forgetting” in the neural network literature (McCloskey and Cohen, 1989). Therefore, a fully fine-tuned model is only useful to one target domain. As the number of domains to handle grows, training, and maintaining a separate model for each task can quickly bec"
2020.wmt-1.72,2010.amta-papers.16,0,0.0611561,"Missing"
2020.wmt-1.72,D19-1165,0,0.370734,"baseline model usually significantly improves the quality of the NMT for the chosen domain. However, it also yields large losses in translation quality for other domains, a phenomenon referred to as “catastrophic forgetting” in the neural network literature (McCloskey and Cohen, 1989). Therefore, a fully fine-tuned model is only useful to one target domain. As the number of domains to handle grows, training, and maintaining a separate model for each task can quickly become tedious and resource-expensive. Several recent studies (e.g. (Vilar, 2018; Wuebker et al., 2018; Michel and Neubig, 2018; Bapna and Firat, 2019)) have proposed more lightweight schemes to perform domain adaptation, while also preserving the value of pre-trained models. Our main inspiration is the latter work, whose proposal relies on small adapter components that are plugged in each hidden layer. These adapters are trained only with the in-domain data, keeping the pre-trained model frozen. Because these additional 617 Proceedings of the 5th Conference on Machine Translation (WMT), pages 617–628 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics adapters are very small compared to the size of the baseline mo"
2020.wmt-1.72,W17-4712,0,0.0824316,"e them to full fine-tuning on the one hand, and to two variants of the residual adapter architecture on the other hand. The reference methods included in our experiments are the following: • a system using “domain control” (Kobus et al., 2017). In this approach, domain information is introduced either as an additional token for each source sentence (DC-Tag) or in the form of a supplementary feature for each word (DC-Feat); • a system using lexicalized domain representations (Pham et al., 2019): word embeddings are composed of a generic and a domainspecific part (LDR); • the three proposals of Britz et al. (2017). TTM is a feature-based approach where the domain tag is introduced as an extra word on the target side. The training uses reference tags and inference is performed with predicted tags, just like for regular target words. DM is a multi-task learner where a domain classifier is trained on top of the MT encoder, so as to make it aware of domain differences; ADM is the adversarial version of DM, pushing the encoder towards learning domain-independent source representations. These methods only use domain labels in training. Model / Domain Mixed FT-Full DC-Tag DC-Feat LDR TTM DM ADM Res-Adap Res-A"
2020.wmt-1.72,W17-4713,0,0.0169783,"cation or domain normalization on the source or target side. A contribution of this study is an adversarial training scheme to normalize representations across domains and make the combination of multiple data sources more effective. Similar techniques (parameter sharing, automatic domain classification/normalization) are at play in Zeng et al. (2018): in this work, the lower layers of the MT use auxiliary classification tasks to disentangle domain-specific from domain-agnostic representations. These representations are first processed separately, then merged to compute the final translation. Farajian et al. (2017); Li et al. (2018) are two recent representatives of the instance-based approach: for each test sentence, a small adaptation corpus is collected based on similarity measures and used to fine-tune a mix-domain model. As shown in the former work, also adapting the training regime on a per sentence basis is crucial to make these techniques really effective. Finally, note that a distinct evolution of the residual adapter model of Bapna and Firat (2019) is presented in Sharaf et al. (2020), where meta-learning techniques are used to make fine-tuning more effective in a standard domain-adaptation se"
2020.wmt-1.72,N09-1068,0,0.0896083,"Missing"
2020.wmt-1.72,2012.eamt-1.60,0,0.0125413,"tal settings 3.1 Data and metrics We perform our experiments with two translation pairs involving multiple domains: English-French (En→Fr) and English-German (En→De). For the former pair, we use texts3 initially from 6 domains, corresponding to the following data sources: the UFAL Medical corpus V1.0 ( M E D )4 , the European Central Bank corpus ( B A N K ) (Tiedemann, 2012); The JRC-Acquis Communautaire corpus ( L A W ) (Steinberger et al., 2006), documentations for KDE, Ubuntu, GNOME and PHP from Opus collection (Tiedemann, 2009), collectively merged in a I T -domain, Ted Talks ( T A L K ) (Cettolo et al., 2012), and the Koran ( R E L ). Complementary experiments also use v12 of the News Commentary corpus ( N E W S ). Corpus statistics are in Table 1. En→De is a much larger task, for which we use corpora distributed for the News task of WMT205 including: European Central Bank corpus ( B A N K ), European Economic and Social Committee corpus ( E C O ), European Medicines Agency corpus ( M E D )6 , Press Release Database of European Commission corpus, News Commentary v15 corpus, Common Crawl corpus ( N E W S ), Europarl v10 ( G O V ), Tilde MODEL - czechtourism ( T O U R )7 , Paracrawl and Wikipedia Ma"
2020.wmt-1.72,2016.amta-researchers.10,0,0.019201,"lark et al. (2012); Sennrich et al. (2013); Huck et al. (2015)) or domains containing several topics (Eidelman et al., 2012; Hasler et al., 2014). Two main strategies emerge: feature-based methods, where domain labels are integrated through supplementary features; and instance-based methods, involving a measure of similarity between train and test domains. The former approach has also been adapted to NMT: Kobus et al. (2017); Tars and Fishel (2018) use an additional domain feature in an RNN model, in the form of an extra domain-token or of additional domain-features associated with each word. Chen et al. (2016) apply domain control on the target side, using a topic vector to describe the 623 Model / Domain Mixed Res-Adap Res-Adap(2,4,6) Res-Adap(6) Res-Adap(4) Res-Adap(2) Res-Adap-WD Res-Adap-LR MED LAW BANK TALK IT REL AVG PARAMS 37.3 37.3 37.7 37.7 37.9 37.8 37.2 37.4 54.6 57.9 57 55.8 55.6 55.5 56.0 56.1 50.1 53.9 53 51.5 51.7 51.4 52.9 51.8 33.5 33.8 33.3 33.9 33.7 34 33.4 33.3 43.2 46.7 45 43.6 44.4 43.8 46.0 45.0 77.5 90.2 90 89.2 88.7 86.7 90.6 89.7 49.4 53.3 52.7 51.9 52 51.5 52.7 52.2 65M/0 65M/12M 65M/6M 65M/2M 65M/2M 65M/2M 65M/12M 65M/12M Table 4: Translation performance of various fine-"
2020.wmt-1.72,C18-1111,0,0.0172823,"and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of interest, one then adapts the pre-trained model by resuming training with the sole in-domain corpus. It is a conjecture that the pretrained model con"
2020.wmt-1.72,1983.tc-1.13,0,0.339315,"Missing"
2020.wmt-1.72,D08-1072,0,0.169703,"er regularization, which penalizes the output of the adapters, corresponding to the following objective: X 1 (− log(P (y|x)) #(x, y) x,y X +λ kADAP(i) (hi (x, y))k2 ) ¯= L i∈{1,..,6}⊗{enc,dec} Finally, another independent design choice relates to the training strategy for adapters. A first option is to generalize supervised domain adaptation to multi-domain adaptation and to proceed in two steps: (a) train a generic model with all the available data; (b) train each adapter layer with domain-specific data, keeping the generic model parameters unchanged. Another strategy is to adopt the view of Dredze and Crammer (2008), where the multi-domain setting is viewed as an instance of multi-task learning (Caruana, 1997) with each domain corresponding to a specific task. This suggests training all the parameters from scratch, as we would do in a multi-task mode. The generic parameters will still depend on all the available data, while each adapter will only be trained with the corresponding in-domain data. Figure 1: Highway residual adapter network 2.3 Gated Residual Adapters The basic architecture presented above rests on a rather simplistic view of “domains” as made of well-separated and unrelated pieces of texts"
2020.wmt-1.72,P13-2119,0,0.0200228,"apter model and open perspective to also make adapted models more robust to label domain errors. 1 Introduction Owing to multiple improvements, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parall"
2020.wmt-1.72,P12-2023,0,0.0258425,"Crammer, 2008; Finkel and Manning, 2009). It is thus no wonder that the design of multi-domain systems has been proposed for many tasks. In this short survey, we exclusively focus on machine translation; it is likely that similar methods (parameter sharing, instance selection/weighting, adversarial training, etc) have also been proposed for other tasks. Early approaches to multi-domain MT were proposed for statistical MT, either considering multiple data sources (eg. Banerjee et al. (2010); Clark et al. (2012); Sennrich et al. (2013); Huck et al. (2015)) or domains containing several topics (Eidelman et al., 2012; Hasler et al., 2014). Two main strategies emerge: feature-based methods, where domain labels are integrated through supplementary features; and instance-based methods, involving a measure of similarity between train and test domains. The former approach has also been adapted to NMT: Kobus et al. (2017); Tars and Fishel (2018) use an additional domain feature in an RNN model, in the form of an extra domain-token or of additional domain-features associated with each word. Chen et al. (2016) apply domain control on the target side, using a topic vector to describe the 623 Model / Domain Mixed R"
2020.wmt-1.72,E14-1035,0,0.0224033,"and Manning, 2009). It is thus no wonder that the design of multi-domain systems has been proposed for many tasks. In this short survey, we exclusively focus on machine translation; it is likely that similar methods (parameter sharing, instance selection/weighting, adversarial training, etc) have also been proposed for other tasks. Early approaches to multi-domain MT were proposed for statistical MT, either considering multiple data sources (eg. Banerjee et al. (2010); Clark et al. (2012); Sennrich et al. (2013); Huck et al. (2015)) or domains containing several topics (Eidelman et al., 2012; Hasler et al., 2014). Two main strategies emerge: feature-based methods, where domain labels are integrated through supplementary features; and instance-based methods, involving a measure of similarity between train and test domains. The former approach has also been adapted to NMT: Kobus et al. (2017); Tars and Fishel (2018) use an additional domain feature in an RNN model, in the form of an extra domain-token or of additional domain-features associated with each word. Chen et al. (2016) apply domain control on the target side, using a topic vector to describe the 623 Model / Domain Mixed Res-Adap Res-Adap(2,4,6"
2020.wmt-1.72,2015.mtsummit-papers.19,0,0.0200802,"common scenario in natural language processing (Dredze and Crammer, 2008; Finkel and Manning, 2009). It is thus no wonder that the design of multi-domain systems has been proposed for many tasks. In this short survey, we exclusively focus on machine translation; it is likely that similar methods (parameter sharing, instance selection/weighting, adversarial training, etc) have also been proposed for other tasks. Early approaches to multi-domain MT were proposed for statistical MT, either considering multiple data sources (eg. Banerjee et al. (2010); Clark et al. (2012); Sennrich et al. (2013); Huck et al. (2015)) or domains containing several topics (Eidelman et al., 2012; Hasler et al., 2014). Two main strategies emerge: feature-based methods, where domain labels are integrated through supplementary features; and instance-based methods, involving a measure of similarity between train and test domains. The former approach has also been adapted to NMT: Kobus et al. (2017); Tars and Fishel (2018) use an additional domain feature in an RNN model, in the form of an extra domain-token or of additional domain-features associated with each word. Chen et al. (2016) apply domain control on the target side, us"
2020.wmt-1.72,D13-1176,0,0.0376761,"odel intact and adaptable to multiple domains. In this paper, we conduct a thorough analysis of the adapter model in the context of a multidomain machine translation task. We contrast multiple implementations of this idea using two language pairs. Our main conclusions are that residual adapters provide a fast and cheap method for supervised multi-domain adaptation; our two variants prove as effective as the original adapter model and open perspective to also make adapted models more robust to label domain errors. 1 Introduction Owing to multiple improvements, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (201"
2020.wmt-1.72,P17-4012,1,0.831901,"Missing"
2020.wmt-1.72,kobus-etal-2017-domain,1,0.929226,"to its size; we then sample a batch of 12,288 tokens that is used to update the shared parameters and the parameters of the corresponding adapter. Models for En→De are larger and rely on embeddings as well as hidden layers of size 1024; each 621 3.3 Multi-domain systems In this section, we evaluate several proposals from the literature on multi-domain adaptation and compare them to full fine-tuning on the one hand, and to two variants of the residual adapter architecture on the other hand. The reference methods included in our experiments are the following: • a system using “domain control” (Kobus et al., 2017). In this approach, domain information is introduced either as an additional token for each source sentence (DC-Tag) or in the form of a supplementary feature for each word (DC-Feat); • a system using lexicalized domain representations (Pham et al., 2019): word embeddings are composed of a generic and a domainspecific part (LDR); • the three proposals of Britz et al. (2017). TTM is a feature-based approach where the domain tag is introduced as an extra word on the target side. The training uses reference tags and inference is performed with predicted tags, just like for regular target words. D"
2020.wmt-1.72,W17-3204,0,0.0198096,"lso make adapted models more robust to label domain errors. 1 Introduction Owing to multiple improvements, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of"
2020.wmt-1.72,L18-1146,0,0.0348477,"zation on the source or target side. A contribution of this study is an adversarial training scheme to normalize representations across domains and make the combination of multiple data sources more effective. Similar techniques (parameter sharing, automatic domain classification/normalization) are at play in Zeng et al. (2018): in this work, the lower layers of the MT use auxiliary classification tasks to disentangle domain-specific from domain-agnostic representations. These representations are first processed separately, then merged to compute the final translation. Farajian et al. (2017); Li et al. (2018) are two recent representatives of the instance-based approach: for each test sentence, a small adaptation corpus is collected based on similarity measures and used to fine-tune a mix-domain model. As shown in the former work, also adapting the training regime on a per sentence basis is crucial to make these techniques really effective. Finally, note that a distinct evolution of the residual adapter model of Bapna and Firat (2019) is presented in Sharaf et al. (2020), where meta-learning techniques are used to make fine-tuning more effective in a standard domain-adaptation setting. 5 tional co"
2020.wmt-1.72,2015.iwslt-evaluation.11,0,0.0287615,"4; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of interest, one then adapts the pre-trained model by resuming training with the sole in-domain corpus. It is a conjecture that the pretrained model constitutes a better initialization than a rand"
2020.wmt-1.72,2020.ngt-1.5,0,0.0261148,"Missing"
2020.wmt-1.72,steinberger-etal-2006-jrc,0,0.0761424,"ility P (k|hL [t]) of domain k as the value for zk (hL [t]). Training gated residual adapters thus comprises three steps, instead of two for the baseline version: 3 Experimental settings 3.1 Data and metrics We perform our experiments with two translation pairs involving multiple domains: English-French (En→Fr) and English-German (En→De). For the former pair, we use texts3 initially from 6 domains, corresponding to the following data sources: the UFAL Medical corpus V1.0 ( M E D )4 , the European Central Bank corpus ( B A N K ) (Tiedemann, 2012); The JRC-Acquis Communautaire corpus ( L A W ) (Steinberger et al., 2006), documentations for KDE, Ubuntu, GNOME and PHP from Opus collection (Tiedemann, 2009), collectively merged in a I T -domain, Ted Talks ( T A L K ) (Cettolo et al., 2012), and the Koran ( R E L ). Complementary experiments also use v12 of the News Commentary corpus ( N E W S ). Corpus statistics are in Table 1. En→De is a much larger task, for which we use corpora distributed for the News task of WMT205 including: European Central Bank corpus ( B A N K ), European Economic and Social Committee corpus ( E C O ), European Medicines Agency corpus ( M E D )6 , Press Release Database of European Co"
2020.wmt-1.72,P18-2050,0,0.0197571,"all the parameters of a baseline model usually significantly improves the quality of the NMT for the chosen domain. However, it also yields large losses in translation quality for other domains, a phenomenon referred to as “catastrophic forgetting” in the neural network literature (McCloskey and Cohen, 1989). Therefore, a fully fine-tuned model is only useful to one target domain. As the number of domains to handle grows, training, and maintaining a separate model for each task can quickly become tedious and resource-expensive. Several recent studies (e.g. (Vilar, 2018; Wuebker et al., 2018; Michel and Neubig, 2018; Bapna and Firat, 2019)) have proposed more lightweight schemes to perform domain adaptation, while also preserving the value of pre-trained models. Our main inspiration is the latter work, whose proposal relies on small adapter components that are plugged in each hidden layer. These adapters are trained only with the in-domain data, keeping the pre-trained model frozen. Because these additional 617 Proceedings of the 5th Conference on Machine Translation (WMT), pages 617–628 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics adapters are very small compared to the"
2020.wmt-1.72,D18-1103,0,0.0223531,"Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of interest, one then adapts the pre-trained model by resuming training with the sole in-domain corpus. It is a conjecture that the pretrained model constitutes a better initialization than a random one, especially whe"
2020.wmt-1.72,P02-1040,0,0.109675,"luding: European Central Bank corpus ( B A N K ), European Economic and Social Committee corpus ( E C O ), European Medicines Agency corpus ( M E D )6 , Press Release Database of European Commission corpus, News Commentary v15 corpus, Common Crawl corpus ( N E W S ), Europarl v10 ( G O V ), Tilde MODEL - czechtourism ( T O U R )7 , Paracrawl and Wikipedia Matrix ( W E B ). Statistics are in Table 2. We randomly select in each corpus a development and a test set of 1,000 lines each and keep the rest for training.8 Development sets help choose the best model according to the average BLEU score (Papineni et al., 2002).9 1. learn a generic model with mixed corpora from multiple domains. 3.2 2. train a domain classifier on top of the encoder and decoder; during this step, the parameters of the generic model are frozen. This model computes the posterior domain probability P (k|hL [t]) for each word wt , based on the representation computed by the last layer. 3. train the parameters of adapters with indomain data separately for each domain, while freezing all the other parameters. 2 The term “word” is employed here by mere convenience, as systems only manipulate sub-lexical BPE units; furthermore, the values o"
2020.wmt-1.72,tiedemann-2012-parallel,0,0.0211147,"the encoder (resp. decoder) as input and use the posterior probability P (k|hL [t]) of domain k as the value for zk (hL [t]). Training gated residual adapters thus comprises three steps, instead of two for the baseline version: 3 Experimental settings 3.1 Data and metrics We perform our experiments with two translation pairs involving multiple domains: English-French (En→Fr) and English-German (En→De). For the former pair, we use texts3 initially from 6 domains, corresponding to the following data sources: the UFAL Medical corpus V1.0 ( M E D )4 , the European Central Bank corpus ( B A N K ) (Tiedemann, 2012); The JRC-Acquis Communautaire corpus ( L A W ) (Steinberger et al., 2006), documentations for KDE, Ubuntu, GNOME and PHP from Opus collection (Tiedemann, 2009), collectively merged in a I T -domain, Ted Talks ( T A L K ) (Cettolo et al., 2012), and the Koran ( R E L ). Complementary experiments also use v12 of the News Commentary corpus ( N E W S ). Corpus statistics are in Table 1. En→De is a much larger task, for which we use corpora distributed for the News task of WMT205 including: European Central Bank corpus ( B A N K ), European Economic and Social Committee corpus ( E C O ), European"
2020.wmt-1.72,W18-5431,0,0.0197505,"ng. Likewise, it might be meaningful to explore ways to share subsets of adapters across domains. This, in turn, raises the issue of which layer(s) to adapt, a question that can be approached in the light of recent analyses of Transformers models, which conjecture that the higher layers encode 618 1 In the decoder, the stack of self-attention and cross encoder-decoder attention only counts as one attention layer and only corresponds to one residual adapter. global patterns with a more “semantic” interpretation, while the lower layers encode local patterns akin to morpho-syntactic information (Raganato and Tiedemann, 2018). A related question concerns the regularization of adapter layers to mitigate overfitting. Reducing the number of adapters, or their dimensions, is simple, but such choices are difficult to optimize numerically – an issue that becomes important as the number of domain grows. Less naive alternatives can also be entertained, such as applying weight decay or layer regularization to the adapter. Implementing these requires to modify the objective function in a way that still allows for a smooth optimization problem. For instance, weight decay applies a penalization on the weights of the adapters,"
2020.wmt-1.72,N18-2080,0,0.0270309,"ents. Full fine-tuning, that adapts all the parameters of a baseline model usually significantly improves the quality of the NMT for the chosen domain. However, it also yields large losses in translation quality for other domains, a phenomenon referred to as “catastrophic forgetting” in the neural network literature (McCloskey and Cohen, 1989). Therefore, a fully fine-tuned model is only useful to one target domain. As the number of domains to handle grows, training, and maintaining a separate model for each task can quickly become tedious and resource-expensive. Several recent studies (e.g. (Vilar, 2018; Wuebker et al., 2018; Michel and Neubig, 2018; Bapna and Firat, 2019)) have proposed more lightweight schemes to perform domain adaptation, while also preserving the value of pre-trained models. Our main inspiration is the latter work, whose proposal relies on small adapter components that are plugged in each hidden layer. These adapters are trained only with the in-domain data, keeping the pre-trained model frozen. Because these additional 617 Proceedings of the 5th Conference on Machine Translation (WMT), pages 617–628 c Online, November 19–20, 2020. 2020 Association for Computational Ling"
2020.wmt-1.72,P13-1082,0,0.0169293,"erogeneous sources is a common scenario in natural language processing (Dredze and Crammer, 2008; Finkel and Manning, 2009). It is thus no wonder that the design of multi-domain systems has been proposed for many tasks. In this short survey, we exclusively focus on machine translation; it is likely that similar methods (parameter sharing, instance selection/weighting, adversarial training, etc) have also been proposed for other tasks. Early approaches to multi-domain MT were proposed for statistical MT, either considering multiple data sources (eg. Banerjee et al. (2010); Clark et al. (2012); Sennrich et al. (2013); Huck et al. (2015)) or domains containing several topics (Eidelman et al., 2012; Hasler et al., 2014). Two main strategies emerge: feature-based methods, where domain labels are integrated through supplementary features; and instance-based methods, involving a measure of similarity between train and test domains. The former approach has also been adapted to NMT: Kobus et al. (2017); Tars and Fishel (2018) use an additional domain feature in an RNN model, in the form of an extra domain-token or of additional domain-features associated with each word. Chen et al. (2016) apply domain control on"
2020.wmt-1.72,D18-1041,0,0.0177965,"the work by Jiang et al. (2019), who consider a Transformer model containing both domain-specific and domain-agnostic heads. Britz et al. (2017) study three general techniques to take domain information into account in training: they rely on either domain classification or domain normalization on the source or target side. A contribution of this study is an adversarial training scheme to normalize representations across domains and make the combination of multiple data sources more effective. Similar techniques (parameter sharing, automatic domain classification/normalization) are at play in Zeng et al. (2018): in this work, the lower layers of the MT use auxiliary classification tasks to disentangle domain-specific from domain-agnostic representations. These representations are first processed separately, then merged to compute the final translation. Farajian et al. (2017); Li et al. (2018) are two recent representatives of the instance-based approach: for each test sentence, a small adaptation corpus is collected based on similarity measures and used to fine-tune a mix-domain model. As shown in the former work, also adapting the training regime on a per sentence basis is crucial to make these tec"
2020.wmt-1.72,D16-1163,0,0.0230272,"en perspective to also make adapted models more robust to label domain errors. 1 Introduction Owing to multiple improvements, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are av"
2021.tacl-1.2,W17-4712,0,0.0471642,"Missing"
2021.tacl-1.2,2012.eamt-1.60,0,0.0159778,"ing, hoping that the loss in performance with respect to the baseline (using correct domain tags) will remain small. 4.1 Data and Metrics We experiment with translation from English into French and use texts initially originating from six domains, corresponding to the following data sources: the UFAL Medical corpus V1.0 (MED);3 the European Central Bank corpus (BANK) (Tiedemann, 2012); The JRC-Acquis Communautaire corpus (LAW) (Steinberger et al., 2006), documentations for KDE, Ubuntu, GNOME, and PHP from Opus collection (Tiedemann, 2009), collectively merged in a IT-domain; TED Talks (TALK) (Cettolo et al., 2012); and the Koran (REL). Complementary experiments also use v12 of the News Commentary corpus (NEWS). Most corpora are available from the Opus Web site.4 These corpora were deduplicated and tokenized with inhouse tools; statistics are in Table 1. To reduce the number of types and build open-vocabulary systems, we use Byte-Pair Encoding (Sennrich et al., 2016b) with 30,000 merge operations on a corpus containing all sentences in both languages. We randomly select in each corpus a development and a test set of 1,000 lines and keep the rest for training.5 Validation sets are used to chose the best"
2021.tacl-1.2,2010.amta-papers.16,0,0.0653994,"Missing"
2021.tacl-1.2,2016.amta-researchers.10,0,0.0119147,"s (Zeng et al., 2018; Pham et al., 2019). It is here expected that the MDMT scenario should be more profitable when the domain mix includes domains that are closely related and can share more information. 2.1 Formalizing Multi-Domain Translation We conventionally define a domain d as a distribution Dd (x) over some feature space X that is shared across domains (Pan and Yang, 2010): In machine translation, X is the representation space for source sentences; each domain corresponds to a specific source of data, and differs from the other data sources in terms of textual genre, thematic content (Chen et al., 2016; Zhang et al., 2016), register (Sennrich et al., 2016a), style (Niu et al., 2018), an so forth. Translation in domain d is formalized by a translation function hd (y |x) pairing sentences in a source language with sentences in a target language y ∈ Y. hd is usually assumed to be deterministic (hence y = hd (x)), but can differ from one domain to the other. A typical learning scenario in MT is to have access to samples from nd domains, which means that the training distribution Ds is a mixture  = 1 . . . nd } Ds (x) = d λsd Dd (x), with {λsd , d the corresponding mixture weights ( d λsd = 1)"
2021.tacl-1.2,D19-1165,0,0.126219,"Missing"
2021.tacl-1.2,C18-1111,0,0.136754,"Missing"
2021.tacl-1.2,N16-1101,0,0.0795562,"Missing"
2021.tacl-1.2,W17-4713,0,0.538849,"rangroup.com Abstract in particular (e.g., Daum´e III and Marcu, 2006; Blitzer, 2007; Jiang and Zhai, 2007). Various techniques thus exist to handle both the situations where a (small) training sample drawn from Dt is available in training, or where only samples of source-side (or target-side) sentences are available (see Foster and Kuhn [2007]; Bertoldi and Federico [2009]; Axelrod et al. [2011]; for proposals from the statistical MT era, or Chu and Wang [2018] for a recent survey of DA for Neural MT). A seemingly related problem is multi-domain (MD) machine translation (Sajjad et al., 2017; Farajian et al., 2017b; Kobus et al., 2017; Zeng et al., 2018; Pham et al., 2019) where one single system is trained and tested with data from multiple domains. MD machine translation (MDMT) corresponds to a very common situation, where all available data, no matter its origin, is used to train a robust system that performs well for any kind of new input. If the intuitions behind MDMT are quite simple, the exact specifications of MDMT systems are rarely spelled out: For instance, should MDMT perform well when the test data is distributed like the training data, when it is equally distributed across domains or when"
2021.tacl-1.2,Q17-1024,0,0.0772896,"Missing"
2021.tacl-1.2,D12-1119,0,0.0748605,"Missing"
2021.tacl-1.2,D17-1156,0,0.0558605,"Missing"
2021.tacl-1.2,N16-1005,0,0.177442,"re expected that the MDMT scenario should be more profitable when the domain mix includes domains that are closely related and can share more information. 2.1 Formalizing Multi-Domain Translation We conventionally define a domain d as a distribution Dd (x) over some feature space X that is shared across domains (Pan and Yang, 2010): In machine translation, X is the representation space for source sentences; each domain corresponds to a specific source of data, and differs from the other data sources in terms of textual genre, thematic content (Chen et al., 2016; Zhang et al., 2016), register (Sennrich et al., 2016a), style (Niu et al., 2018), an so forth. Translation in domain d is formalized by a translation function hd (y |x) pairing sentences in a source language with sentences in a target language y ∈ Y. hd is usually assumed to be deterministic (hence y = hd (x)), but can differ from one domain to the other. A typical learning scenario in MT is to have access to samples from nd domains, which means that the training distribution Ds is a mixture  = 1 . . . nd } Ds (x) = d λsd Dd (x), with {λsd , d the corresponding mixture weights ( d λsd = 1). Multi-domain learning, as defined in Dredze and Cram"
2021.tacl-1.2,2020.ngt-1.5,0,0.0344781,"Missing"
2021.tacl-1.2,D18-1041,0,0.0888128,"aum´e III and Marcu, 2006; Blitzer, 2007; Jiang and Zhai, 2007). Various techniques thus exist to handle both the situations where a (small) training sample drawn from Dt is available in training, or where only samples of source-side (or target-side) sentences are available (see Foster and Kuhn [2007]; Bertoldi and Federico [2009]; Axelrod et al. [2011]; for proposals from the statistical MT era, or Chu and Wang [2018] for a recent survey of DA for Neural MT). A seemingly related problem is multi-domain (MD) machine translation (Sajjad et al., 2017; Farajian et al., 2017b; Kobus et al., 2017; Zeng et al., 2018; Pham et al., 2019) where one single system is trained and tested with data from multiple domains. MD machine translation (MDMT) corresponds to a very common situation, where all available data, no matter its origin, is used to train a robust system that performs well for any kind of new input. If the intuitions behind MDMT are quite simple, the exact specifications of MDMT systems are rarely spelled out: For instance, should MDMT perform well when the test data is distributed like the training data, when it is equally distributed across domains or when the test distribution is unknown? Shoul"
2021.tacl-1.2,C16-1170,0,0.0155586,"8; Pham et al., 2019). It is here expected that the MDMT scenario should be more profitable when the domain mix includes domains that are closely related and can share more information. 2.1 Formalizing Multi-Domain Translation We conventionally define a domain d as a distribution Dd (x) over some feature space X that is shared across domains (Pan and Yang, 2010): In machine translation, X is the representation space for source sentences; each domain corresponds to a specific source of data, and differs from the other data sources in terms of textual genre, thematic content (Chen et al., 2016; Zhang et al., 2016), register (Sennrich et al., 2016a), style (Niu et al., 2018), an so forth. Translation in domain d is formalized by a translation function hd (y |x) pairing sentences in a source language with sentences in a target language y ∈ Y. hd is usually assumed to be deterministic (hence y = hd (x)), but can differ from one domain to the other. A typical learning scenario in MT is to have access to samples from nd domains, which means that the training distribution Ds is a mixture  = 1 . . . nd } Ds (x) = d λsd Dd (x), with {λsd , d the corresponding mixture weights ( d λsd = 1). Multi-domain learni"
arranz-etal-2004-bilingual,ide-etal-2000-xces,0,\N,Missing
arranz-etal-2004-bilingual,W02-1012,0,\N,Missing
arranz-etal-2004-bilingual,N01-1026,0,\N,Missing
arranz-etal-2004-bilingual,W02-0808,0,\N,Missing
C10-1027,W09-0432,0,0.0225697,"ion system for 233 the pair A → B, for which the parallel data is sparse; assuming further that such parallel resources exist for pairs A → C and for C → B, it is then tempting to perform the translation indirectly through pivoting, by first translating from A to C, then from C to B. Direct implementations of this idea are discussed e.g. in (Utiyama and Isahara, 2007). Pivoting can also intervene earlier in the process, for instance as a means to automatically generate the missing parallel resource, an idea that has also been considered to adapt an existing translation systems to new domains (Bertoldi and Federico, 2009). Pivoting can finally be used to fix or improve the translation model: (Cohn and Lapata, 2007) augments the phrase table for a baseline bilingual system with supplementary phrases obtained by pivoting into a third language. Triangulation in translation Triangulation techniques are somewhat more general and only require the availabily of one auxiliary system (or one auxiliary parallel corpus). For instance, the authors of (Chen et al., 2008) propose to use the translation model of an auxiliary C → B system to filter-out the phrase-table of a primary A → B system. 2.2 Our framework As in other"
C10-1027,W08-0309,0,0.0145645,"ount all the available translations and scores. Various to lead to measurable improvements. 2 We plan to experiment next on using predictions at the document level. proposals have been made to efficiently perform such a combination, using auxiliary data structures such as n-best lists, word lattices or consensus networks (see for instance (Kumar and Byrne, 2004; Rosti et al., 2007; Matusov et al., 2008; Hildebrand and Vogel, 2008; Tromble et al., 2008)). Theses techniques have proven extremely effective and have allowed to deliver very significant gains in several recent evaluation campaigns (Callison-Burch et al., 2008). Multisource translation A related, yet more resourceful approach, consists in trying to combine several systems providing translations from different sources into the same target, provided such multilingual sources are available. (Och and Ney, 2001) propose to select the most promising translation amongst the hypotheses produced by several Foreign→English systems, where output selection is based on the translation scores. The intuition that if a system assigns a high figure of merits to the translation of a particular sentence, then this translation should be preferred, is implemented in the"
C10-1027,P96-1041,0,0.0252501,"am model, our SMT system uses six additional models which are linearly combined following a discriminative modeling framework: two lexicalized reordering (Tillmann, 2004) models,a target-language model, two lexicon models, a ’weak’ distancebased distortion model, a word bonus model and a translation unit bonus model. Coefficients in this linear combination are tuned over development data with the MERT optimization toolkit4 , slightly modified to use our decoder’s n-best lists. For this study, we used 3-gram bilingual and 3-gram target language models built using modified Kneser-Ney smoothing (Chen and Goodman, 1996); model estimation was performed with the SRI language modeling toolkit.5 Target language 4 http://www.statmt.org/moses http://wwww.speech.sri.com/projects/ srilm 235 5 models were trained on the target side of the bitext corpora. After preprocessing the corpora with standard tokenization tools, word-to-word alignments are performed in both directions, source-to-target and target-to-source. In our system implementation, the GIZA++ toolkit6 is used to compute the word alignments. Then, the grow-diag-final-and heuristic is used to obtain the final alignments from which translation units are extr"
C10-1027,chen-etal-2008-improving,0,0.146052,"n the translation scores. The intuition that if a system assigns a high figure of merits to the translation of a particular sentence, then this translation should be preferred, is implemented in the M AX combination heuristics, whose relative (lack of) success is discussed in (Schwartz, 2008). A similar idea is explored in (Nomoto, 2004), where the sole target language model score is used to rank competing outputs. (Schroeder et al., 2009) propose to combine the available sources prior to translation, under the form of a multilingual lattice, which is decoded with a multisource phrase table. (Chen et al., 2008) integrate the available auxiliary information in a different manner, and discuss how to improve the translation model of the primary system: the idea is to use the entries in the phrase table of the auxiliary system to filter out those accidental correspondences that pollute the main translation model. The most effective implementation of multisource translation to date however consists in using mono-source system combination techniques (Schroeder et al., 2009). Translation through pivoting The use of auxiliary systems has also been proposed in another common situation, as a possible remedy t"
C10-1027,P07-1092,0,0.149323,"parallel resources exist for pairs A → C and for C → B, it is then tempting to perform the translation indirectly through pivoting, by first translating from A to C, then from C to B. Direct implementations of this idea are discussed e.g. in (Utiyama and Isahara, 2007). Pivoting can also intervene earlier in the process, for instance as a means to automatically generate the missing parallel resource, an idea that has also been considered to adapt an existing translation systems to new domains (Bertoldi and Federico, 2009). Pivoting can finally be used to fix or improve the translation model: (Cohn and Lapata, 2007) augments the phrase table for a baseline bilingual system with supplementary phrases obtained by pivoting into a third language. Triangulation in translation Triangulation techniques are somewhat more general and only require the availabily of one auxiliary system (or one auxiliary parallel corpus). For instance, the authors of (Chen et al., 2008) propose to use the translation model of an auxiliary C → B system to filter-out the phrase-table of a primary A → B system. 2.2 Our framework As in other works, we propose to make use of several MT systems (of any type) to improve translation perfor"
C10-1027,P03-2017,1,0.625206,"used as auxiliary information for the decoding of the direct system. Configurations 4 and 5 are instances of multisource translation, where a paraphrase or a translation of the source text is available. Lastly, configuration 6 illustrates the case where a human translator, with knowledge of the target language and at least of one of the available source languages, could influence the decoding by providing desired3 words (e.g. only for source words or phrases that would be judged difficult to translate). This human supervision through a feedback text in real time is similar to the proposal of (Dymetman et al., 2003). Given this framework, several questions arise, 3 The proposal as it is limits the hypotheses produced by the system to those that are attainable given its training data. It is conceivable, however, to find ways of introducing new knowledge in this framework. 234 the most important underlying this work being whether the performance of SMT systems can be improved by using other SMT systems. Another point of interest is whether improvements made to auxiliary systems can yield improvement to the direct system, without the latter undergoing any modification. 2.3 Furthermore, we are interested in"
C10-1027,2008.amta-srw.3,0,0.0297833,"iew here. System combination An often used strategy consists in combining the output of several systems for a fixed language pair, and to rescore the resulting set of hypotheses taking into account all the available translations and scores. Various to lead to measurable improvements. 2 We plan to experiment next on using predictions at the document level. proposals have been made to efficiently perform such a combination, using auxiliary data structures such as n-best lists, word lattices or consensus networks (see for instance (Kumar and Byrne, 2004; Rosti et al., 2007; Matusov et al., 2008; Hildebrand and Vogel, 2008; Tromble et al., 2008)). Theses techniques have proven extremely effective and have allowed to deliver very significant gains in several recent evaluation campaigns (Callison-Burch et al., 2008). Multisource translation A related, yet more resourceful approach, consists in trying to combine several systems providing translations from different sources into the same target, provided such multilingual sources are available. (Och and Ney, 2001) propose to select the most promising translation amongst the hypotheses produced by several Foreign→English systems, where output selection is based on t"
C10-1027,N04-1022,0,0.043046,"has been implemented in many different ways which we briefly review here. System combination An often used strategy consists in combining the output of several systems for a fixed language pair, and to rescore the resulting set of hypotheses taking into account all the available translations and scores. Various to lead to measurable improvements. 2 We plan to experiment next on using predictions at the document level. proposals have been made to efficiently perform such a combination, using auxiliary data structures such as n-best lists, word lattices or consensus networks (see for instance (Kumar and Byrne, 2004; Rosti et al., 2007; Matusov et al., 2008; Hildebrand and Vogel, 2008; Tromble et al., 2008)). Theses techniques have proven extremely effective and have allowed to deliver very significant gains in several recent evaluation campaigns (Callison-Burch et al., 2008). Multisource translation A related, yet more resourceful approach, consists in trying to combine several systems providing translations from different sources into the same target, provided such multilingual sources are available. (Och and Ney, 2001) propose to select the most promising translation amongst the hypotheses produced by"
C10-1027,J06-4004,1,0.865906,"Missing"
C10-1027,max-etal-2010-contrastive,1,0.929341,"nerally produce a search space that differs from that of the direct translation systems. As such, they create a new translation system out of various systems for which diagnosis becomes more difficult. This paper instead focusses on improving a single system, which should be state-of-the-art as regards data and models. We propose a framework in which information coming from external sources is used to boost lexical choices and guide the decoder into making more informed choices.1 1 We performed initial experiments where the complementary information was exploited during n-best list reranking (Max et al., 2010), but except for the multisource condition the list of hypotheses contained too little useful variation 232 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 232–240, Beijing, August 2010 Complementary sources can be of different nature: they can involve other automatic systems (for the same or different language pairs) and/or human knowledge. Furthermore, complementary information is injected at the lexical level, thus making targeted fine-grained lexical predictions useful. Importantly, those predictions are exploited at the sentence level2 ,"
C10-1027,P04-1063,0,0.0322173,"slations from different sources into the same target, provided such multilingual sources are available. (Och and Ney, 2001) propose to select the most promising translation amongst the hypotheses produced by several Foreign→English systems, where output selection is based on the translation scores. The intuition that if a system assigns a high figure of merits to the translation of a particular sentence, then this translation should be preferred, is implemented in the M AX combination heuristics, whose relative (lack of) success is discussed in (Schwartz, 2008). A similar idea is explored in (Nomoto, 2004), where the sole target language model score is used to rank competing outputs. (Schroeder et al., 2009) propose to combine the available sources prior to translation, under the form of a multilingual lattice, which is decoded with a multisource phrase table. (Chen et al., 2008) integrate the available auxiliary information in a different manner, and discuss how to improve the translation model of the primary system: the idea is to use the entries in the phrase table of the auxiliary system to filter out those accidental correspondences that pollute the main translation model. The most effecti"
C10-1027,2001.mtsummit-papers.46,0,0.187357,"the former and syntactic models for the latter. Another promising approach consists in exploiting complementary sources of information in order to build better translations, as done by consensus-based system combination (e.g. (Matusov et al., 2008)). This, however, requires to François Yvon LIMSI-CNRS Univ. Paris Sud yvon@limsi.fr have several systems available for the same language pair. Considering that the same training data would be available to all systems, differences in translation modelling are expected to produce redundant and complementary hypotheses. Multisource translation (e.g. (Och and Ney, 2001; Schwartz, 2008)) is a variant, involving source texts available in several languages which can be translated by systems for different language pairs and whose outputs can be successfully combined into better translations (Schroeder et al., 2009). One theoretical expectation of multisource translation is that it can successfully reduce ambiguity of the original source text, but does so under the rare conditions of availability of existing (accurate) translations. In contrast, pivot-based system combination (e.g. (Utiyama and Isahara, 2007; Wu and Wang, 2007)) aims at compensating the lack of"
C10-1027,N07-1061,0,0.303217,"t and complementary hypotheses. Multisource translation (e.g. (Och and Ney, 2001; Schwartz, 2008)) is a variant, involving source texts available in several languages which can be translated by systems for different language pairs and whose outputs can be successfully combined into better translations (Schroeder et al., 2009). One theoretical expectation of multisource translation is that it can successfully reduce ambiguity of the original source text, but does so under the rare conditions of availability of existing (accurate) translations. In contrast, pivot-based system combination (e.g. (Utiyama and Isahara, 2007; Wu and Wang, 2007)) aims at compensating the lack of training data for a given language pair by producing translation hypotheses obtained by pivoting via an intermediary language for which better systems are available. These techniques generally produce a search space that differs from that of the direct translation systems. As such, they create a new translation system out of various systems for which diagnosis becomes more difficult. This paper instead focusses on improving a single system, which should be state-of-the-art as regards data and models. We propose a framework in which informa"
C10-1027,vilar-etal-2006-error,0,0.0375447,"Missing"
C10-1027,P07-1108,0,0.039341,"ses. Multisource translation (e.g. (Och and Ney, 2001; Schwartz, 2008)) is a variant, involving source texts available in several languages which can be translated by systems for different language pairs and whose outputs can be successfully combined into better translations (Schroeder et al., 2009). One theoretical expectation of multisource translation is that it can successfully reduce ambiguity of the original source text, but does so under the rare conditions of availability of existing (accurate) translations. In contrast, pivot-based system combination (e.g. (Utiyama and Isahara, 2007; Wu and Wang, 2007)) aims at compensating the lack of training data for a given language pair by producing translation hypotheses obtained by pivoting via an intermediary language for which better systems are available. These techniques generally produce a search space that differs from that of the direct translation systems. As such, they create a new translation system out of various systems for which diagnosis becomes more difficult. This paper instead focusses on improving a single system, which should be state-of-the-art as regards data and models. We propose a framework in which information coming from ext"
C10-1027,N07-1029,0,0.0453162,"Missing"
C10-1027,E09-1082,0,0.137318,"., 2008)). This, however, requires to François Yvon LIMSI-CNRS Univ. Paris Sud yvon@limsi.fr have several systems available for the same language pair. Considering that the same training data would be available to all systems, differences in translation modelling are expected to produce redundant and complementary hypotheses. Multisource translation (e.g. (Och and Ney, 2001; Schwartz, 2008)) is a variant, involving source texts available in several languages which can be translated by systems for different language pairs and whose outputs can be successfully combined into better translations (Schroeder et al., 2009). One theoretical expectation of multisource translation is that it can successfully reduce ambiguity of the original source text, but does so under the rare conditions of availability of existing (accurate) translations. In contrast, pivot-based system combination (e.g. (Utiyama and Isahara, 2007; Wu and Wang, 2007)) aims at compensating the lack of training data for a given language pair by producing translation hypotheses obtained by pivoting via an intermediary language for which better systems are available. These techniques generally produce a search space that differs from that of the d"
C10-1027,2008.amta-srw.6,0,0.0501316,"tactic models for the latter. Another promising approach consists in exploiting complementary sources of information in order to build better translations, as done by consensus-based system combination (e.g. (Matusov et al., 2008)). This, however, requires to François Yvon LIMSI-CNRS Univ. Paris Sud yvon@limsi.fr have several systems available for the same language pair. Considering that the same training data would be available to all systems, differences in translation modelling are expected to produce redundant and complementary hypotheses. Multisource translation (e.g. (Och and Ney, 2001; Schwartz, 2008)) is a variant, involving source texts available in several languages which can be translated by systems for different language pairs and whose outputs can be successfully combined into better translations (Schroeder et al., 2009). One theoretical expectation of multisource translation is that it can successfully reduce ambiguity of the original source text, but does so under the rare conditions of availability of existing (accurate) translations. In contrast, pivot-based system combination (e.g. (Utiyama and Isahara, 2007; Wu and Wang, 2007)) aims at compensating the lack of training data for"
C10-1027,2009.mtsummit-papers.14,0,0.0186901,"on hypotheses of a sentence in the target language, we can then boost the likeliness of the words and phrases occurring in these hypotheses by deriving an auxiliary language model for each test sentence. This allows us to integrate this auxiliary information during the search and thus provides a tighter integration with the direct system. This idea has successfully been used in speech recognition, using for instance close captions (Placeway and Lafferty, 1996) or an imperfect translation (Paulik et al., 2005) to provide auxiliary in-domain adaptation data for the recognizer’s language model. (Simard and Isabelle, 2009) proposed a similar approach in Machine Translation in which they use the target-side of an exact match in a translation memory to build language models on a per sentence basis used in their decoder. This strategy can be implemented in a straightforward manner, by simply training a language model using the n-best list as an adaptation corpus. Being automatically generated, hypotheses in the n-best list are not entirely reliable: in particular, they may contain very unlikely target sequences at the junction of two segments. It is however straightforward to filter these out using the available p"
C10-1027,N04-4026,0,0.0131259,"sh system for lexical boosting via triangulation through Spanish 3 Experiments and results 3.1 Translation engine In this study, we used our own machine translation engine, which implements the n-grambased approach to statistical machine translation (Mariño et al., 2006). The translation model is implemented as a stochastic finite-state transducer trained using a n-gram language model of (source,target) pairs. In addition to a bilingual n-gram model, our SMT system uses six additional models which are linearly combined following a discriminative modeling framework: two lexicalized reordering (Tillmann, 2004) models,a target-language model, two lexicon models, a ’weak’ distancebased distortion model, a word bonus model and a translation unit bonus model. Coefficients in this linear combination are tuned over development data with the MERT optimization toolkit4 , slightly modified to use our decoder’s n-best lists. For this study, we used 3-gram bilingual and 3-gram target language models built using modified Kneser-Ney smoothing (Chen and Goodman, 1996); model estimation was performed with the SRI language modeling toolkit.5 Target language 4 http://www.statmt.org/moses http://wwww.speech.sri.com/"
C10-1027,D08-1065,0,0.0285168,"An often used strategy consists in combining the output of several systems for a fixed language pair, and to rescore the resulting set of hypotheses taking into account all the available translations and scores. Various to lead to measurable improvements. 2 We plan to experiment next on using predictions at the document level. proposals have been made to efficiently perform such a combination, using auxiliary data structures such as n-best lists, word lattices or consensus networks (see for instance (Kumar and Byrne, 2004; Rosti et al., 2007; Matusov et al., 2008; Hildebrand and Vogel, 2008; Tromble et al., 2008)). Theses techniques have proven extremely effective and have allowed to deliver very significant gains in several recent evaluation campaigns (Callison-Burch et al., 2008). Multisource translation A related, yet more resourceful approach, consists in trying to combine several systems providing translations from different sources into the same target, provided such multilingual sources are available. (Och and Ney, 2001) propose to select the most promising translation amongst the hypotheses produced by several Foreign→English systems, where output selection is based on the translation scores."
C10-2023,C04-1073,0,0.0519156,"ngle words are reordered within a relatively small window distance. It consist of the easiest case as typically, the use of phrases (in the sense of translation units of the phrase-based approach to SMT) is believed to adequately perform such reorderings. Mid-range reorderings involve reorderings between two or more phrases (translation units) which are closely positioned, typically within a window of about 6 words. Many alternatives have been proposed to tackle midrange reorderings through the introduction of linguistic information in MT systems. To the best of our knowledge, the authors of (Xia and McCord, 2004) were the first to address this problem in the statistical MT paradigm. They automatically build a set of linguistically grounded rewrite rules, aimed at reordering the source sentence so as to match the word order of the target side. Similarly, (Collins, et al 2005) and (Popovic and Ney, 2006) reorder the source sentence using a small set of hand-crafted rules for GermanEnglish translation. (Crego and Mari˜no, 2007) show that the ordering problem can be more accurately solved by building a source-sentence word lattice containing the most promising reordering hypotheses, allowing the decoder t"
C10-2023,E09-1043,0,0.0859444,"llow the above idea of making the ordering of the source sentence similar to the target sentence before decoding (Niehues and Kolss, 2009), long-range reorderings are typically better addressed by syntax-based and hierarchical (Chiang, 2007) models. In (Zollmann et al., 2008), an interesting comparison between phrase-based, hierarchical and syntax-augmented models is carried out, concluding that hierarchical and syntaxbased models slightly outperform phrase-based models under large data conditions and for sufficiently non-monotonic language pairs. Encouraged by the work reported in (Hoang and Koehn, 2009), we tackle the mid-range reordering problem in SMT by introducing a ngram language model of bilingual units built from POS information. The rationale behind such a model is double: on the one hand we aim at introducing morpho-syntactic information into the reordering model, as we believe it plays an important role for predicting systematic word ordering differences between language pairs; at the same time that it drastically reduces the sparseness problem of standard translation units built from surface forms. On the other hand, n-gram language modeling is a robust approach, that enables to a"
C10-2023,P02-1040,0,0.0786557,"rtion model; and finally a word-bonus model and a tuple-bonus model which are used in order to compensate for the system preference for short translations. All language models used in this work are estimated using the SRI language modeling toolkit4 . According to our experience, KneserNey smoothing (Kneser and Ney, 1995) and interpolation of lower and higher n-grams options are used as they typically achieve the best performance. Optimization work is carried out by means of the widely used MERT toolkit5 which has been slightly modified to perform optimizations embedding our decoder. The BLEU (Papineni et al., 2002) score is used as objective function for MERT and to evaluate test performance. 4.3 Reordering in German-English and French-English Translation Two factors are found to greatly impact the overall translation performance: the morphological mismatch between languages, and their reordering needs. The vocabulary size is strongly influenced by the number of word forms for number, case, tense, mood, etc., while reordering needs refer to the difference in their syntactic structure. In this work, we are primarily interested on the reordering needs of each language pair. Figure 3 displays a quantitativ"
C10-2023,C08-1144,0,0.0514727,"Missing"
C10-2023,N04-4026,0,0.591269,"ing bi-text. (Zhang, et al 2007) introduce shallow parse (chunk) information to reorder the source sentence, aiming at extending the scope of their rewrite rules, encoding reordering hypotheses in the form of a confusion network that is then passed to the decoder. These studies tackle mid-range reorderings by predicting more or less accurate reordering hypotheses. However, none 197 Coling 2010: Poster Volume, pages 197–205, Beijing, August 2010 of them introduce a reordering model to be used in decoding time. Nowadays, most of SMT systems implement the well known lexicalized reordering model (Tillman, 2004). Basically, for each translation unit it estimates the probability of being translated monotone, swapped or placed discontiguous with respect to its previous translation unit. Integrated within the Moses (Koehn, et al 2007) decoder, the model achieves state-ofthe-art results for many translation tasks. One of the main reasons that explains the success of the model is that it considers information of the source- and target-side surface forms, while the above mentionned approaches attempt to hypothesize reorderings relying only on the information contained on the source-side words. Finally, lon"
C10-2023,popovic-ney-2006-pos,0,0.0407673,"ngs between two or more phrases (translation units) which are closely positioned, typically within a window of about 6 words. Many alternatives have been proposed to tackle midrange reorderings through the introduction of linguistic information in MT systems. To the best of our knowledge, the authors of (Xia and McCord, 2004) were the first to address this problem in the statistical MT paradigm. They automatically build a set of linguistically grounded rewrite rules, aimed at reordering the source sentence so as to match the word order of the target side. Similarly, (Collins, et al 2005) and (Popovic and Ney, 2006) reorder the source sentence using a small set of hand-crafted rules for GermanEnglish translation. (Crego and Mari˜no, 2007) show that the ordering problem can be more accurately solved by building a source-sentence word lattice containing the most promising reordering hypotheses, allowing the decoder to decide for the best word order hypothesis. Word lattices are built by means of rewrite rules operating on POS tags; such rules are automatically extracted from the training bi-text. (Zhang, et al 2007) introduce shallow parse (chunk) information to reorder the source sentence, aiming at exten"
C10-2023,P05-1066,0,0.150159,"Missing"
C10-2023,W09-0435,0,0.0118598,"e source- and target-side surface forms, while the above mentionned approaches attempt to hypothesize reorderings relying only on the information contained on the source-side words. Finally, long-range reorderings imply reorderings in the structure of the sentence. Such reorderings are necessary to model the translation for pairs like Arabic-English, as English typically follows the SVO order, while Arabic sentences have different structures. Even if several attempts exist which follow the above idea of making the ordering of the source sentence similar to the target sentence before decoding (Niehues and Kolss, 2009), long-range reorderings are typically better addressed by syntax-based and hierarchical (Chiang, 2007) models. In (Zollmann et al., 2008), an interesting comparison between phrase-based, hierarchical and syntax-augmented models is carried out, concluding that hierarchical and syntaxbased models slightly outperform phrase-based models under large data conditions and for sufficiently non-monotonic language pairs. Encouraged by the work reported in (Hoang and Koehn, 2009), we tackle the mid-range reordering problem in SMT by introducing a ngram language model of bilingual units built from POS in"
C10-2023,P07-2045,0,0.0078003,"network that is then passed to the decoder. These studies tackle mid-range reorderings by predicting more or less accurate reordering hypotheses. However, none 197 Coling 2010: Poster Volume, pages 197–205, Beijing, August 2010 of them introduce a reordering model to be used in decoding time. Nowadays, most of SMT systems implement the well known lexicalized reordering model (Tillman, 2004). Basically, for each translation unit it estimates the probability of being translated monotone, swapped or placed discontiguous with respect to its previous translation unit. Integrated within the Moses (Koehn, et al 2007) decoder, the model achieves state-ofthe-art results for many translation tasks. One of the main reasons that explains the success of the model is that it considers information of the source- and target-side surface forms, while the above mentionned approaches attempt to hypothesize reorderings relying only on the information contained on the source-side words. Finally, long-range reorderings imply reorderings in the structure of the sentence. Such reorderings are necessary to model the translation for pairs like Arabic-English, as English typically follows the SVO order, while Arabic sentence"
C10-2023,2007.iwslt-1.3,0,0.0187649,"so as to match the word order of the target side. Similarly, (Collins, et al 2005) and (Popovic and Ney, 2006) reorder the source sentence using a small set of hand-crafted rules for GermanEnglish translation. (Crego and Mari˜no, 2007) show that the ordering problem can be more accurately solved by building a source-sentence word lattice containing the most promising reordering hypotheses, allowing the decoder to decide for the best word order hypothesis. Word lattices are built by means of rewrite rules operating on POS tags; such rules are automatically extracted from the training bi-text. (Zhang, et al 2007) introduce shallow parse (chunk) information to reorder the source sentence, aiming at extending the scope of their rewrite rules, encoding reordering hypotheses in the form of a confusion network that is then passed to the decoder. These studies tackle mid-range reorderings by predicting more or less accurate reordering hypotheses. However, none 197 Coling 2010: Poster Volume, pages 197–205, Beijing, August 2010 of them introduce a reordering model to be used in decoding time. Nowadays, most of SMT systems implement the well known lexicalized reordering model (Tillman, 2004). Basically, for e"
C10-2023,C08-1098,0,0.0139348,"olution (a single POStagged version of each tuple). The training corpus composed of tagged units out of which our new model is estimated is accordingly modified to contain only those tagged units considered in decoding. Note that most of the ambiguity present in word tagging is resolved by the fact that translation units may contain multiple source and target side words. 4 French, German and English Part-of-speech tags are computed by means of the TreeTagger 1 toolkit. Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.). Lang. Sent. Train French 1.75 M English 1.75 M Tune French 2, 051 English 2, 051 Test French 2, 525 English 2, 525 Train German 1, 61 M English 1, 61 M Tune German 2, 051 English 2, 051 Test German 2, 525 English 2, 525 Evaluation Framework In this section, we perform evaluation experiments of our novel reordering model. First, we give details of the corpora and baseline system employed in our experiments and analyze the reordering needs of the translation tasks, FrenchEng"
C10-2023,P00-1056,0,0.259913,"Missing"
C10-2023,2005.iwslt-1.8,0,0.0403537,"− − − − 55.3 k 8, 957 1, 282 49.2 k 8, 359 1, 344 1 1 72.8 k 10, 832 1, 749 65.1 k 9, 568 1, 724 1 1 42.2 M 381 k 44.2 M 137 k − − − − 47, 8 k 10, 994 2, 153 49, 2 k 8, 359 1, 491 1 1 62, 8 k 12, 856 2, 704 65, 1 k 9, 568 1, 810 1 1 Table 1: Statistics for the training, tune and test data sets. 4.2 System Details After preprocessing the corpora with standard tokenization tools, word-to-word alignments are performed in both directions, source-to-target and target-to-source. In our system implementation, the GIZA++ toolkit3 is used to compute the word alignments. Then, the grow-diag-final-and (Koehn et al., 2005) heuristic is used to obtain the alignments from which tuples are extracted. In addition to the tuple n-gram translation model, our SMT system implements six additional feature functions which are linearly com201 1 www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger www.ims.uni-stuttgart.de/projekte/corplex/RFTagger 3 http://www.fjoch.com/GIZA++.html 2 bined following a discriminative modeling framework (Och and Ney, 2002): a target-language model which provides information about the target language structure and fluency; two lexicon models, which constitute complementary translation models c"
C10-2023,P02-1038,0,0.0378209,"directions, source-to-target and target-to-source. In our system implementation, the GIZA++ toolkit3 is used to compute the word alignments. Then, the grow-diag-final-and (Koehn et al., 2005) heuristic is used to obtain the alignments from which tuples are extracted. In addition to the tuple n-gram translation model, our SMT system implements six additional feature functions which are linearly com201 1 www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger www.ims.uni-stuttgart.de/projekte/corplex/RFTagger 3 http://www.fjoch.com/GIZA++.html 2 bined following a discriminative modeling framework (Och and Ney, 2002): a target-language model which provides information about the target language structure and fluency; two lexicon models, which constitute complementary translation models computed for each given tuple; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which are used in order to compensate for the system preference for short translations. All language models used in this work are estimated using the SRI language modeling toolkit4 . According to our experience, KneserNey smoothing (Kneser and Ney, 1995) and interpolation of lower and higher n-grams"
C10-2023,J06-4004,1,\N,Missing
C10-2023,J07-2003,0,\N,Missing
D18-1328,S16-1081,0,0.0271883,"elated Work Attempts to measure the impact of translation divergences in MT have focused on the introduction of noise in sentence alignments (Goutte et al., 2012), showing that statistical MT is highly robust to noise, and that performance only degrades seriously at very high noise levels. In contrast, neural MTs seem to be more sensitive to noise (Chen et al., 2016), as they tend to assign high probabilities to rare events (Hassan et al., 2018). Efforts devoted to characterising the degree of semantic equivalence between two snippets of texts in the same or different languages are presented (Agirre et al., 2016). In (Mueller and Thyagarajan, 2016), a monolingual sentence similarity network is proposed, making use of a simple LSTM layer to compute sentence representations. The authors show that a simple SVM classifier exploiting such sentence representations achieves state-of-the-art results in a textual entailment task. With the same objective, the system of He and Lin (2016) uses multiple convolutional layers and models pairwise word interactions. Our work is inspired by Carpuat et al. (2017), who train a SVM-based cross-lingual divergence detector using word alignments and sentence length features."
D18-1328,D18-1549,0,0.0254976,"performance on a parallel corpus. Therefore, the quality of MT engines is heavily dependent on the amount but also the quality of available parallel sentences.1 Parallel texts are unfortunately, scarce resources: There are relatively few language pairs for which parallel corpora of large sizes exist, and even for those pairs, available corpora only concern few restricted domains. To alleviate the lack of parallel data, several approaches have been developed over the years. They range from methods using non-parallel, or comparable data (Zhao and 1 Recent work on neural MT (Lample et al., 2018; Artetxe et al., 2018) completely dispenses with parallel data, using unsupervised methods to obtain performance improvements over word-by-word statistical MT. These systems however lag far behind supervised systems, as considered in this work. What do you feel, Spock? Que ressentez-vous? What do you feel? How much do you get paid? T’es pay´e combien de l’heure? How much do you get paid per hour? That seems a lot. 40 livres? 40 pounds? I brought you french fries! Je t’ai rapport´e des saucisses! I brought you sausage! Table 1: Examples of semantically divergent parallel sentences. English (en), French (fr) and glos"
D18-1328,W17-3209,0,0.0693794,"egree of semantic equivalence between two snippets of texts in the same or different languages are presented (Agirre et al., 2016). In (Mueller and Thyagarajan, 2016), a monolingual sentence similarity network is proposed, making use of a simple LSTM layer to compute sentence representations. The authors show that a simple SVM classifier exploiting such sentence representations achieves state-of-the-art results in a textual entailment task. With the same objective, the system of He and Lin (2016) uses multiple convolutional layers and models pairwise word interactions. Our work is inspired by Carpuat et al. (2017), who train a SVM-based cross-lingual divergence detector using word alignments and sentence length features. Their work shows that an NMT system trained only on non-divergent sentences yields slightly better translation scores, while requiring less training time. A follow-up study by the same authors (Vyas et al., 2018) achieves even better results, using the neural architecture of He and Lin (2016). Our work differs from theirs as we 2 https://github.com/jmcrego/similarity Figure 1: Illustration of the model. It computes the similarity of any source-target sentence pair (s, t), where s = (s1"
D18-1328,2016.amta-researchers.8,0,0.233467,"nglais, 2018; Grover and Mitra, 2017; Schwenk, 2018) to techniques that produce synthetic parallel data from monolingual corpora (Sennrich et al., 2016a; Chinea-Rios et al., 2017), using automated alignment/translation engines that are prone to the introduction of noise in the resulting parallel sentences. Mismatches in parallel sentences extracted from translated texts are also reported (Tiedemann, 2011; Xu and Yvon, 2016). This problem is mostly ignored in MT, where parallel sentences are considered to convey the exact same meaning; yet it seems particularly important for neural MT engines (Chen et al., 2016). Corpus-based approaches to machine translation rely on the availability of clean parallel corpora. Such resources are scarce, and because of the automatic processes involved in their preparation, they are often noisy. This paper describes an unsupervised method for detecting translation divergences in parallel sentences. We rely on a neural network that computes cross-lingual sentence similarity scores, which are then used to effectively filter out divergent translations. Furthermore, similarity scores predicted by the network are used to identify and fix some partial divergences, yielding a"
D18-1328,W17-4714,0,0.0456606,"Missing"
D18-1328,W04-3208,0,0.0366978,"Missing"
D18-1328,2012.amta-papers.7,0,0.04818,"with a different, arguably simpler, topology. We model sentence similarity by means of optimising a loss function based on word alignments. Furthermore, the network predicts word similarity scores that we further use to correct divergent sentences. 3 Neural Divergence Classifier The architecture of our network is inspired by the work on word alignment of Legrand et al. (2016), using however contextual, rather than fixed, word embeddings (see Figure 1). Related Work Attempts to measure the impact of translation divergences in MT have focused on the introduction of noise in sentence alignments (Goutte et al., 2012), showing that statistical MT is highly robust to noise, and that performance only degrades seriously at very high noise levels. In contrast, neural MTs seem to be more sensitive to noise (Chen et al., 2016), as they tend to assign high probabilities to rare events (Hassan et al., 2018). Efforts devoted to characterising the degree of semantic equivalence between two snippets of texts in the same or different languages are presented (Agirre et al., 2016). In (Mueller and Thyagarajan, 2016), a monolingual sentence similarity network is proposed, making use of a simple LSTM layer to compute sent"
D18-1328,C18-1122,0,0.0465598,"Missing"
D18-1328,P17-3003,0,0.0250778,"Missing"
D18-1328,N16-1108,0,0.0341746,"they tend to assign high probabilities to rare events (Hassan et al., 2018). Efforts devoted to characterising the degree of semantic equivalence between two snippets of texts in the same or different languages are presented (Agirre et al., 2016). In (Mueller and Thyagarajan, 2016), a monolingual sentence similarity network is proposed, making use of a simple LSTM layer to compute sentence representations. The authors show that a simple SVM classifier exploiting such sentence representations achieves state-of-the-art results in a textual entailment task. With the same objective, the system of He and Lin (2016) uses multiple convolutional layers and models pairwise word interactions. Our work is inspired by Carpuat et al. (2017), who train a SVM-based cross-lingual divergence detector using word alignments and sentence length features. Their work shows that an NMT system trained only on non-divergent sentences yields slightly better translation scores, while requiring less training time. A follow-up study by the same authors (Vyas et al., 2018) achieves even better results, using the neural architecture of He and Lin (2016). Our work differs from theirs as we 2 https://github.com/jmcrego/similarity"
D18-1328,W16-2207,0,0.213438,"Missing"
D18-1328,C14-1055,0,0.109632,"are in bold letters. Table 1 gives some examples of English-French parallel sentences that are not completely semantically equivalent, extracted from the OpenSubtitles corpus (Lison and Tiedemann, 2016). Multiples types of translation divergences are found in parallel corpora: Additional segments are included on either side of the parallel sentences (first and second rows) most likely due to errors in sentence segmentation; Some translations may be completely uncorrelated (third row); Inaccurate translations also exist (fourth row). Note that divergent translations can be due various reasons (Li et al., 2014), the study of which is beyond the 2967 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2967–2973 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics scope of this paper. In this work, we present an unsupervised method for building cross-lingual sentence embeddings based on modelling word similarity, relying on a neural architecture (see § 3) that is able to identify several types of common cross-lingual divergences. The resulting embeddings are then used to measure semantic equivalence between sentenc"
D18-1328,L16-1147,0,0.182552,"ed in this work. What do you feel, Spock? Que ressentez-vous? What do you feel? How much do you get paid? T’es pay´e combien de l’heure? How much do you get paid per hour? That seems a lot. 40 livres? 40 pounds? I brought you french fries! Je t’ai rapport´e des saucisses! I brought you sausage! Table 1: Examples of semantically divergent parallel sentences. English (en), French (fr) and gloss of French (gl). Divergences are in bold letters. Table 1 gives some examples of English-French parallel sentences that are not completely semantically equivalent, extracted from the OpenSubtitles corpus (Lison and Tiedemann, 2016). Multiples types of translation divergences are found in parallel corpora: Additional segments are included on either side of the parallel sentences (first and second rows) most likely due to errors in sentence segmentation; Some translations may be completely uncorrelated (third row); Inaccurate translations also exist (fourth row). Note that divergent translations can be due various reasons (Li et al., 2014), the study of which is beyond the 2967 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2967–2973 c Brussels, Belgium, October 31 - November"
D18-1328,J05-4003,0,0.208936,"Missing"
D18-1328,P18-2037,0,0.0386913,"Missing"
D18-1328,P16-1009,0,0.0359762,"processed with OpenNMT5 , performing minimal tokenisation. After tokenisation, each out-of-vocabulary word is mapped to a special UNK token, assuming a vocabulary containing the 50, 000 more frequent words. 4.2 Neural Divergence Word embeddings of Es = Et = 256 cells are initialised using fastText,6 further aligned by means of MUSE7 following the unsupervised 4 http://paracrawl.eu/ http://opennmt.net 6 https://github.com/facebookresearch/fastText 7 https://github.com/facebookresearch/MUSE 5 Neural Translation In addition to the basic tokenisation detailed above, we perform Byte-Pair Encoding (Sennrich et al., 2016b) with 30000 merge operations learned by joining both language sides. Neural systems are based on the open-source project OpenNMT; using a Transformer model similar to the model of Vaswani et al. (2017): both encoder and decoder have 6 layers; Multi-head attention is performed over 8 head; the hidden layer size is 512; and the inner layer of feed forward network is of size 2048. Word embeddings have 512 cells. We set the dropout probability to 0.1 and the batch size to 3072. The optimiser is Lazy Adam with β1 = 0.9, β2 = 0.98,  = 10−9 , warmup steps = 4000. Training stops after 30 epochs. 5"
D18-1328,P16-1162,0,0.0649685,"processed with OpenNMT5 , performing minimal tokenisation. After tokenisation, each out-of-vocabulary word is mapped to a special UNK token, assuming a vocabulary containing the 50, 000 more frequent words. 4.2 Neural Divergence Word embeddings of Es = Et = 256 cells are initialised using fastText,6 further aligned by means of MUSE7 following the unsupervised 4 http://paracrawl.eu/ http://opennmt.net 6 https://github.com/facebookresearch/fastText 7 https://github.com/facebookresearch/MUSE 5 Neural Translation In addition to the basic tokenisation detailed above, we perform Byte-Pair Encoding (Sennrich et al., 2016b) with 30000 merge operations learned by joining both language sides. Neural systems are based on the open-source project OpenNMT; using a Transformer model similar to the model of Vaswani et al. (2017): both encoder and decoder have 6 layers; Multi-head attention is performed over 8 head; the hidden layer size is 512; and the inner layer of feed forward network is of size 2048. Word embeddings have 512 cells. We set the dropout probability to 0.1 and the batch size to 3072. The optimiser is Lazy Adam with β1 = 0.9, β2 = 0.98,  = 10−9 , warmup steps = 4000. Training stops after 30 epochs. 5"
D18-1328,N18-1136,0,0.168216,"Missing"
D18-1328,L16-1099,1,0.844686,"ystrangroup.com ‡ LIMSI, CNRS, Universit´e Paris-Saclay 91405 Orsay, France firstname.lastname@limsi.fr Abstract Vogel, 2002; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Gr´egoire and Langlais, 2018; Grover and Mitra, 2017; Schwenk, 2018) to techniques that produce synthetic parallel data from monolingual corpora (Sennrich et al., 2016a; Chinea-Rios et al., 2017), using automated alignment/translation engines that are prone to the introduction of noise in the resulting parallel sentences. Mismatches in parallel sentences extracted from translated texts are also reported (Tiedemann, 2011; Xu and Yvon, 2016). This problem is mostly ignored in MT, where parallel sentences are considered to convey the exact same meaning; yet it seems particularly important for neural MT engines (Chen et al., 2016). Corpus-based approaches to machine translation rely on the availability of clean parallel corpora. Such resources are scarce, and because of the automatic processes involved in their preparation, they are often noisy. This paper describes an unsupervised method for detecting translation divergences in parallel sentences. We rely on a neural network that computes cross-lingual sentence similarity scores,"
D19-5225,kobus-etal-2017-domain,1,0.858772,"loy a set of grammatical constraints (tense, voice and person) which introduce syntactic/semantic variations in translations. Thus, our method aims at enhancing translation diversity, a major drawback highlighted in back-translated data (Edunov et al., 2018). Similar to our work, side constraints have already been used on neural models in a number of different scenarios. To the best of our knowledge, side constraints were first employed to control politeness in a NMT by (Sennrich et al., 2016a). Domain-adapted translations using a unique network enhanced with side constraints is presented in (Kobus et al., 2017). We consider 4 constraints regarding POS classes: noun, verb, adjective and adverb. For each constraint we build 3 clusters containing the set of words with H (high), M (medium) and L (low) frequency as computed over the training data. This is, the set of nouns occurring with highest frequency are arranged in the NH class, verbs with lower frequencies in VL, etc. We set the frequency thresholds to satisfy that the three clusters of a POS class have approximately the same number of occurrences in the training corpus. Note that when creating synthetic corpora, side constraint values are randoml"
D19-5225,P02-1040,0,0.107735,"ese sentences of our datasets (aligned to English). This is, we back-translate the Japanese side of the Japanese-English (JaRuNC) corpus to extend the data available for the Russian→Japanese translation direction. Equivalently, we backtranslate the Russian side of the Russian-English (JaRuNC and NC) corpora to increase the amount of data available for the Japanese→Russian direction. Thus, building new synthetic Japanese*-Russian and Russian*-Japanese corpora. 13 Since our model is multi-lingual, we don’t need additional networks 13 6 Evaluation All our results are computed following the BLEU (Papineni et al., 2002) score. Validation sets are used to select our best performing networks, while results shown in Table 5 are computed for the official test sets. As it can be seen, all our experiments to alleviate data scarcity boosted translation performance. A light decrease in accuracy is observed when using SC data for Russian→Japanese translation. The improvement is remarkable for the Japanese→Russian task for which the BLEU score is doubled from 7 to more than 14 points. We use * to denote synthetic data 192 System base +FT(JaRuNC,NC) +FT(JaRuNC,NC,BT,SYN) +FT(JaRuNC,NC,BT,SYN,SC) Ru-Ja 9.76 12.10 15.89"
D19-5225,N16-1005,0,0.376749,"air constitute very challenging conditions. A rather common situation in the translation industry, that motivated us to explore techniques that can help in the construction from scratch of efficient NMT engines. We present systems built using only the data provided by the organisers for both translation directions (Russian↔Japanese) and using the Transformer network introduced by (Vaswani et al., 2017). We enhance the baseline systems with several experiments that aim at alleviating the data scarcity problem. More precisely we run experiments following the back-translation method proposed by (Sennrich et al., 2016b) in which target Resources Datasets used for the evaluation can be found listed in the shared task web site1 . WAT organisers kindly provide a manually aligned, cleaned and filtered Japanese↔Russian, Japanese↔English and English↔Russian train, development and test corpora (JaRuNC)2 as well as a news domain Russian↔English corpus (NC)3 . In addition, use of the next out-of-domain data is encouraged: • Japanese↔English Wikipedia articles related to Kyoto (KFTT)4 . • Japanese↔English Subtitles (JESC)5 , • Japanese↔English asian scientific paper abstracts (ASPEC)6 , 1 lotus.kuee.kyoto-u.ac.jp/WA"
D19-5225,D18-1045,0,0.0162792,"т приезжает завтра Side Constraints Table 3: French-German sentence pair with frequency constraints. We propose a method to generate synthetic parallel data that uses a set of side constraints. Side constraints are used to guide the NMT model to produce distinct word translation alternatives based on their frequency in the training corpora. Furthermore, we employ a set of grammatical constraints (tense, voice and person) which introduce syntactic/semantic variations in translations. Thus, our method aims at enhancing translation diversity, a major drawback highlighted in back-translated data (Edunov et al., 2018). Similar to our work, side constraints have already been used on neural models in a number of different scenarios. To the best of our knowledge, side constraints were first employed to control politeness in a NMT by (Sennrich et al., 2016a). Domain-adapted translations using a unique network enhanced with side constraints is presented in (Kobus et al., 2017). We consider 4 constraints regarding POS classes: noun, verb, adjective and adverb. For each constraint we build 3 clusters containing the set of words with H (high), M (medium) and L (low) frequency as computed over the training data. Th"
D19-5225,P16-1009,0,0.511566,"air constitute very challenging conditions. A rather common situation in the translation industry, that motivated us to explore techniques that can help in the construction from scratch of efficient NMT engines. We present systems built using only the data provided by the organisers for both translation directions (Russian↔Japanese) and using the Transformer network introduced by (Vaswani et al., 2017). We enhance the baseline systems with several experiments that aim at alleviating the data scarcity problem. More precisely we run experiments following the back-translation method proposed by (Sennrich et al., 2016b) in which target Resources Datasets used for the evaluation can be found listed in the shared task web site1 . WAT organisers kindly provide a manually aligned, cleaned and filtered Japanese↔Russian, Japanese↔English and English↔Russian train, development and test corpora (JaRuNC)2 as well as a news domain Russian↔English corpus (NC)3 . In addition, use of the next out-of-domain data is encouraged: • Japanese↔English Wikipedia articles related to Kyoto (KFTT)4 . • Japanese↔English Subtitles (JESC)5 , • Japanese↔English asian scientific paper abstracts (ASPEC)6 , 1 lotus.kuee.kyoto-u.ac.jp/WA"
D19-5225,P16-1162,0,0.761133,"air constitute very challenging conditions. A rather common situation in the translation industry, that motivated us to explore techniques that can help in the construction from scratch of efficient NMT engines. We present systems built using only the data provided by the organisers for both translation directions (Russian↔Japanese) and using the Transformer network introduced by (Vaswani et al., 2017). We enhance the baseline systems with several experiments that aim at alleviating the data scarcity problem. More precisely we run experiments following the back-translation method proposed by (Sennrich et al., 2016b) in which target Resources Datasets used for the evaluation can be found listed in the shared task web site1 . WAT organisers kindly provide a manually aligned, cleaned and filtered Japanese↔Russian, Japanese↔English and English↔Russian train, development and test corpora (JaRuNC)2 as well as a news domain Russian↔English corpus (NC)3 . In addition, use of the next out-of-domain data is encouraged: • Japanese↔English Wikipedia articles related to Kyoto (KFTT)4 . • Japanese↔English Subtitles (JESC)5 , • Japanese↔English asian scientific paper abstracts (ASPEC)6 , 1 lotus.kuee.kyoto-u.ac.jp/WA"
D19-5615,P00-1037,0,0.526819,"Missing"
D19-5615,P01-1023,0,0.322072,"ent ordering. We made publicly available the transformer extension presented in this paper1 . 1 Introduction Data-to-text generation is an important task in natural language generation (NLG). It refers to the task of automatically producing a descriptive text from non-linguistic structured data (tables, database records, spreadsheets, etc.). Table 1 illustrates an example of data-to-text NLG, with statistics of a NBA basketball game (top) and the corresponding game summary (bottom). Traditional approaches perform the summary generation in two separate steps: content selection (“what to say”) (Duboue and McKeown, 2001, 2003) and surface realization (“how to say it”) (Stent et al., 2004; Reiter et al., 2005). After the emergence of sequence-to-sequence (S2S) 1. We adapt the Transformer (Vaswani et al., 2017) architecture by modifying the input table representation (record embedding) and introducing an additional objective function (content selection modelling). 2. We create synthetic data following two data augmentation techniques and investigate 1 https://github.com/gongliym/ data2text-transformer 148 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 148–156 c Hong Kon"
D19-5615,W03-1016,0,0.246106,"Missing"
D19-5615,P16-1154,0,0.0450671,"to choose a suitable template to render the content. More recently, work on this topic has focused on end-to-end generation models. Konstas and Lapata (2012) described an end-to-end generation model which jointly models content selection and surface realization. Mei et al. (2015) proposed a neural encoder-aligner-decoder model which first encodes the entire input record dataset then the aligner module performs the content selection for the decoder to generate output summary. Some other work extends the encoder-decoder model to be able to copy words directly from the input (Yang et al., 2016; Gu et al., 2016; Gulcehre et al., 2016). Wiseman et al. (2017) investigates different data-to-text generation approaches and introduces a new corpus (ROTOW IRE, see Table 1) 3 Data-to-Text Generation Model In this section, we first formulate the data-totext generation problem and introduce our datato-text generation baseline model. Next, we detail the extensions introduced to our baseline network, namely Record Embedding and Content Selection Modelling. Problem Statement The objective of data-to-text generation is to generate a descriptive summary given structured data. Input of the model consists of a table"
D19-5615,P98-2209,0,0.386396,"to-text generation task along with a series of automatic measures for the contentoriented evaluation. Based on (Wiseman et al., 2017), Puduppully et al. (2019) incorporates content selection and planing mechanisms into the encoder-decoder system and improves the stateof-the-art on the ROTOW IRE dataset. their impacts on different evaluation metrics. We show that our model outperforms current state-of-the-art systems on BLEU, content selection precision and content ordering metrics. 2 Related Work Automatic summary generation has been a topic of interest for a long time (Reiter and Dale, 1997; Tanaka-Ishii et al., 1998). It has interesting applications in many different domains, such as sport game summary generation (Barzilay and Lapata, 2005; Liang et al., 2009), weather-forecast generation (Reiter et al., 2005) and recipe generation (Yang et al., 2016). Traditional data-to-text generation approaches perform the summary generation in two separate steps: content selection and surface realization. For content selection, a number of approaches were proposed to automatically select the elements of content and extract ordering constraints from an aligned corpus of input data and output summaries (Duboue and McKe"
D19-5615,P16-1014,0,0.0250577,"able template to render the content. More recently, work on this topic has focused on end-to-end generation models. Konstas and Lapata (2012) described an end-to-end generation model which jointly models content selection and surface realization. Mei et al. (2015) proposed a neural encoder-aligner-decoder model which first encodes the entire input record dataset then the aligner module performs the content selection for the decoder to generate output summary. Some other work extends the encoder-decoder model to be able to copy words directly from the input (Yang et al., 2016; Gu et al., 2016; Gulcehre et al., 2016). Wiseman et al. (2017) investigates different data-to-text generation approaches and introduces a new corpus (ROTOW IRE, see Table 1) 3 Data-to-Text Generation Model In this section, we first formulate the data-totext generation problem and introduce our datato-text generation baseline model. Next, we detail the extensions introduced to our baseline network, namely Record Embedding and Content Selection Modelling. Problem Statement The objective of data-to-text generation is to generate a descriptive summary given structured data. Input of the model consists of a table of records (see Table 1"
D19-5615,N12-1093,0,0.0259739,"ual dependencies between input data items. For surface realization, Stent et al. (2004) proposed to transform the input data into an intermediary structure and then to generate natural language text from it; Reiter et al. (2005) presented a method to generate text using consistent data-to-word rules. Angeli et al. (2010) broke up the two steps into a sequence of local decisions where they used two classifiers to select content form database and another classifier to choose a suitable template to render the content. More recently, work on this topic has focused on end-to-end generation models. Konstas and Lapata (2012) described an end-to-end generation model which jointly models content selection and surface realization. Mei et al. (2015) proposed a neural encoder-aligner-decoder model which first encodes the entire input record dataset then the aligner module performs the content selection for the decoder to generate output summary. Some other work extends the encoder-decoder model to be able to copy words directly from the input (Yang et al., 2016; Gu et al., 2016; Gulcehre et al., 2016). Wiseman et al. (2017) investigates different data-to-text generation approaches and introduces a new corpus (ROTOW IR"
D19-5615,D17-1239,0,0.575701,"representation (record embedding) and introducing an additional objective function (content selection modelling). 2. We create synthetic data following two data augmentation techniques and investigate 1 https://github.com/gongliym/ data2text-transformer 148 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 148–156 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d for the data-to-text generation task along with a series of automatic measures for the contentoriented evaluation. Based on (Wiseman et al., 2017), Puduppully et al. (2019) incorporates content selection and planing mechanisms into the encoder-decoder system and improves the stateof-the-art on the ROTOW IRE dataset. their impacts on different evaluation metrics. We show that our model outperforms current state-of-the-art systems on BLEU, content selection precision and content ordering metrics. 2 Related Work Automatic summary generation has been a topic of interest for a long time (Reiter and Dale, 1997; Tanaka-Ishii et al., 1998). It has interesting applications in many different domains, such as sport game summary generation (Barzila"
D19-5615,D16-1128,0,0.050921,"Missing"
D19-5615,P09-1011,0,0.115289,"(2019) incorporates content selection and planing mechanisms into the encoder-decoder system and improves the stateof-the-art on the ROTOW IRE dataset. their impacts on different evaluation metrics. We show that our model outperforms current state-of-the-art systems on BLEU, content selection precision and content ordering metrics. 2 Related Work Automatic summary generation has been a topic of interest for a long time (Reiter and Dale, 1997; Tanaka-Ishii et al., 1998). It has interesting applications in many different domains, such as sport game summary generation (Barzilay and Lapata, 2005; Liang et al., 2009), weather-forecast generation (Reiter et al., 2005) and recipe generation (Yang et al., 2016). Traditional data-to-text generation approaches perform the summary generation in two separate steps: content selection and surface realization. For content selection, a number of approaches were proposed to automatically select the elements of content and extract ordering constraints from an aligned corpus of input data and output summaries (Duboue and McKeown, 2001, 2003). In (Barzilay and Lapata, 2005), the content selection is treated as a collective classification problem which allows the system"
D19-5615,P02-1040,0,0.105549,"Missing"
D19-5615,P04-1011,0,0.357397,"d in this paper1 . 1 Introduction Data-to-text generation is an important task in natural language generation (NLG). It refers to the task of automatically producing a descriptive text from non-linguistic structured data (tables, database records, spreadsheets, etc.). Table 1 illustrates an example of data-to-text NLG, with statistics of a NBA basketball game (top) and the corresponding game summary (bottom). Traditional approaches perform the summary generation in two separate steps: content selection (“what to say”) (Duboue and McKeown, 2001, 2003) and surface realization (“how to say it”) (Stent et al., 2004; Reiter et al., 2005). After the emergence of sequence-to-sequence (S2S) 1. We adapt the Transformer (Vaswani et al., 2017) architecture by modifying the input table representation (record embedding) and introducing an additional objective function (content selection modelling). 2. We create synthetic data following two data augmentation techniques and investigate 1 https://github.com/gongliym/ data2text-transformer 148 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 148–156 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Lingui"
I17-2046,D16-1139,0,0.0233101,"Missing"
I17-2046,W17-3204,0,0.054482,"Missing"
I17-2046,2016.amta-researchers.10,0,0.0551937,"Missing"
I17-2046,P02-1040,0,0.0975088,"Missing"
I17-2046,K16-1029,0,0.0557737,"Missing"
I17-2046,P15-1001,0,0.0623444,"Missing"
I17-2046,P16-1159,0,0.0457533,"Missing"
I17-2046,Q16-1027,0,0.0375008,"Missing"
J06-4004,W05-0823,1,0.838951,"Missing"
J06-4004,W00-0508,0,0.0142428,"y approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state perspective—more specifically, from"
J06-4004,J96-1002,0,0.02988,"Missing"
J06-4004,J90-2002,0,0.81424,"nslation was conceived as the problem of finding a sentence by decoding a given “encrypted” version of it (Weaver 1955). Although the idea seemed very feasible, enthusiasm faded shortly afterward because of the computational limitations of the time (Hutchins 1986). Finally, during the nineties, two factors made it possible for SMT to become an actual and practical technology: first, significant increment in both the computational power and storage capacity of computers, and second, the availability of large volumes of bilingual data. The first SMT systems were developed in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for adequacy of translation contents, times a target language probability p(T), which accounts for fluency of target constructions. For these first SMT systems, translation-model probabilities at the sentence level were approximated from word-based translation models that were trained by using bilingual corpora (Brown et al. 1993). In the case of target"
J06-4004,J93-2003,0,0.0556776,"d in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for adequacy of translation contents, times a target language probability p(T), which accounts for fluency of target constructions. For these first SMT systems, translation-model probabilities at the sentence level were approximated from word-based translation models that were trained by using bilingual corpora (Brown et al. 1993). In the case of target language probabilities, these were generally trained from monolingual data by using n-grams. Present SMT systems have evolved from the original ones in such a way that mainly differ from them in two respects: first, word-based translation models have been ∗ Department of Signal Theory and Communications, Campus Nord, Barcelona 08034, Spain. Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for publication: 5 July 2006 © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 4 replaced by phrase-b"
J06-4004,J04-2004,0,0.856987,"models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state perspective—more specifically, from the work of Casacuberta (2001) and Casacuberta and Vidal (2004). However, whereas in this earlier work the translation model is implemented by using a finite-state transducer, in the system presented here the translation model is implemented by using n-grams. In this way, the proposed translation system can take full advantage of the smoothing and consistency provided by standard back-off n-gram models. The translation model presented here actually constitutes a language model of a sort of “bilanguage” composed of bilin˜ 2002). An alternagual units, which will be referred to as tuples (de Gispert and Marino tive approach, which relies on bilingual-unit un"
J06-4004,N04-1033,0,0.0140655,"Missing"
J06-4004,2005.iwslt-1.23,1,0.883592,"Missing"
J06-4004,2005.mtsummit-papers.37,1,0.855856,"Missing"
J06-4004,2006.amta-papers.4,1,0.763242,"Missing"
J06-4004,P05-2012,1,0.804737,"Missing"
J06-4004,C86-1155,0,0.079146,"ament Plenary Sessions (EPPS). 1. Introduction The beginnings of statistical machine translation (SMT) can be traced back to the early fifties, closely related to the ideas from which information theory arose (Shannon and Weaver 1949) and inspired by works on cryptography (Shannon 1949, 1951) during World War II. According to this view, machine translation was conceived as the problem of finding a sentence by decoding a given “encrypted” version of it (Weaver 1955). Although the idea seemed very feasible, enthusiasm faded shortly afterward because of the computational limitations of the time (Hutchins 1986). Finally, during the nineties, two factors made it possible for SMT to become an actual and practical technology: first, significant increment in both the computational power and storage capacity of computers, and second, the availability of large volumes of bilingual data. The first SMT systems were developed in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for"
J06-4004,knight-al-onaizan-1998-translation,0,0.230855,"more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state persp"
J06-4004,N03-1017,0,0.0258625,"Missing"
J06-4004,2005.mtsummit-papers.36,1,0.177804,"Missing"
J06-4004,P00-1056,0,0.0440511,"tence pairs are removed from the training data to allow for a better performance of the alignment tool. Sentence pairs are removed according to the following two criteria: r r Fertility filtering: removes sentence pairs with a word ratio larger than a predefined threshold value. Length filtering: removes sentence pairs with at least one sentence of more than 100 words in length. This helps to maintain bounded alignment computational times. After preprocessing, word-to-word alignments are performed in both directions, source-to-target and target-to-source. In our system implementation, GIZA++ (Och and Ney 2000) is used for computing the alignments. A total of five iterations for models IBM-1 and HMM, and three iterations for models IBM-3 and IBM-4, are performed. Then, the obtained alignment sets are used for computing the intersection and the union of alignments from which tuples and embedded-word tuples are extracted, respectively. 4.2.2 Tuple Extraction and Pruning. A tuple set for each translation direction is extracted from the union set of alignments while avoiding source-nulled tuples by using the procedure described in Section 2.2.2. Then, the resulting tuple vocabularies are pruned accordin"
J06-4004,P02-1038,0,0.884384,"034, Spain. Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for publication: 5 July 2006 © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 4 replaced by phrase-based translation models (Zens, Och, and Ney 2002; Koehn, Och, and Marcu 2003) which are directly estimated from aligned bilingual corpora by considering relative frequencies, and second, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitesta"
J06-4004,J03-1002,0,0.00706932,"gram model alone, in the case of Table 2, and by using the tuple n-gram model along with the additional four feature functions described in Section 3.2, in the case of Table 3. Both translation directions, Spanish to English (ES → EN) and English to Spanish (EN → ES), are considered in each table. In the case of Table 2, model size and translation accuracy are evaluated against the type of alignment set used for extracting tuples. Three different alignment sets are considered: source-to-target, the union of source-to-target and target-to-source, and the “refined” alignment method described by Och and Ney (2003). For the results presented in Table 2, a pruning parameter value of N = 20 was used for the Spanish-to-English direction, while a value of N = 30 was used for the English-to-Spanish direction. As can be clearly seen in Table 2, the union alignment set happens to be the most favorable one for extracting tuples in both translation directions since it provides a significantly better translation accuracy, in terms of BLEU score, than the other two alignment sets considered. Notice also in Table 2 that the union set is the one providing the smallest model sizes according to the number of bigrams a"
J06-4004,P02-1040,0,0.105596,"alignment sets. Notice that BLEU measurements in this table correspond to translations computed by using the tuple n-gram model alone. Direction Alignment set Tuple voc. Bigrams Trigrams BLEU ES → EN Source-to-target union refined Source-to-target union refined 1.920 2.040 2.111 1.813 2.023 2.081 6.426 6.009 6.851 6.263 6.092 6.920 2.353 1.798 2.398 2.268 1.747 2.323 0.4424 0.4745 0.4594 0.4152 0.4276 0.4193 EN → ES when tuples are extracted from different alignment sets and when different pruning parameters are used, respectively. Translation accuracy is measured in terms of the BLEU score (Papineni et al. 2002), which is computed here for translations generated by using the tuple n-gram model alone, in the case of Table 2, and by using the tuple n-gram model along with the additional four feature functions described in Section 3.2, in the case of Table 3. Both translation directions, Spanish to English (ES → EN) and English to Spanish (EN → ES), are considered in each table. In the case of Table 2, model size and translation accuracy are evaluated against the type of alignment set used for extracting tuples. Three different alignment sets are considered: source-to-target, the union of source-to-targ"
J06-4004,N03-2036,0,0.00459439,"k the translation model is implemented by using a finite-state transducer, in the system presented here the translation model is implemented by using n-grams. In this way, the proposed translation system can take full advantage of the smoothing and consistency provided by standard back-off n-gram models. The translation model presented here actually constitutes a language model of a sort of “bilanguage” composed of bilin˜ 2002). An alternagual units, which will be referred to as tuples (de Gispert and Marino tive approach, which relies on bilingual-unit unigram probabilities, was developed by Tillmann and Xia (2003); in contrast, the approach presented here considers bilingualunit n-gram probabilities. In addition to the tuple n-gram translation model, the translation system presented here implements four specific feature functions that are log-linearly combined along with the translation model for performing the decoding ˜ et al. 2005). (Marino This article is intended to provide a detailed description of the n-gram-based translation system, as well as to demonstrate the system performance in a widedomain, large-vocabulary translation task. The article is structured as follows. First, Section 2 presents"
J06-4004,2002.tmi-tutorials.2,0,0.201063,"Missing"
J06-4004,2004.iwslt-evaluation.14,1,\N,Missing
J06-4004,N04-1021,0,\N,Missing
kobus-etal-2017-domain,P10-2041,0,\N,Missing
kobus-etal-2017-domain,W07-0733,0,\N,Missing
kobus-etal-2017-domain,2005.eamt-1.19,0,\N,Missing
kobus-etal-2017-domain,I08-2089,0,\N,Missing
kobus-etal-2017-domain,N16-1005,0,\N,Missing
kobus-etal-2017-domain,2016.amta-researchers.10,0,\N,Missing
kobus-etal-2017-domain,C16-1170,0,\N,Missing
kobus-etal-2017-domain,tiedemann-2012-parallel,0,\N,Missing
max-etal-2010-contrastive,E09-1010,0,\N,Missing
max-etal-2010-contrastive,W09-0441,0,\N,Missing
max-etal-2010-contrastive,P02-1040,0,\N,Missing
max-etal-2010-contrastive,W09-0401,0,\N,Missing
max-etal-2010-contrastive,P07-2045,0,\N,Missing
max-etal-2010-contrastive,J04-2004,0,\N,Missing
max-etal-2010-contrastive,W07-0734,0,\N,Missing
max-etal-2010-contrastive,J06-4004,1,\N,Missing
max-etal-2010-contrastive,E09-1082,0,\N,Missing
max-etal-2010-contrastive,2005.mtsummit-papers.11,0,\N,Missing
max-etal-2010-contrastive,vilar-etal-2006-error,0,\N,Missing
max-etal-2010-contrastive,2008.amta-srw.6,0,\N,Missing
max-etal-2010-contrastive,carpuat-wu-2008-evaluation,0,\N,Missing
N07-2022,P06-1002,0,0.0591237,"systems use a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Within this 1 Hereinafter, alignment will refer to word alignment, unless otherwise stated. A more general difficulty, however, is that of finding an alignment evaluation metric favoring alignments which benefit Machine Translation. The fact that the required alignment characteristics depend on each particular system makes it even more difficult. It seems that high precision alignments are better for phrase-based SMT (Chen and Federico, 2006; Ayan and Dorr, 2006), whereas high recall alignments are more suited to N-gram SMT (Mari˜no et al., 2006). In this context, alignment quality improvements does not necessarily imply translation quality improvements. This is in agreement with the observation of a poor correlation between word alignment error rate (AER (Och and Ney, 2000)) and automatic translation evaluation metrics (Ittycheriah and Roukos, 2005; Vilar et al., 2006). 85 Proceedings of NAACL HLT 2007, Companion Volume, pages 85–88, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics Recently some alignment evaluation metrics"
N07-2022,J93-2003,0,0.0320352,"Missing"
N07-2022,N06-1014,0,0.0312338,"ot tuned in function of the translation task, but only indirectly. In this paper, we propose a novel framework for discriminative training of alignment models with automated translation metrics as maximization criterion. In this approach, alignments are optimized for the translation task. In addition, no link labels at the word level are needed. This framework is evaluated in terms of automatic translation evaluation metrics, and an improvement of translation quality is observed. 1 Tuning alignment for an MT system is subject to practical difficulties. Unsupervised systems (Och and Ney, 2003; Liang et al., 2006) are based on generative models trained with the EM algorithm. They require large computational resources, and incorporating new features is difficult. In contrast, adding new features to some supervised systems (Liu et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) is easy, but the need of annotated data is a problem. Introduction In the first SMT systems (Brown et al., 1993), word alignment was introduced as a hidden variable of the translation model. When word-based translation models have been replaced by phrase-based models (Zens et al., 2002), alignment1 and translation model trai"
N07-2022,P05-1057,0,0.0715665,"roach, alignments are optimized for the translation task. In addition, no link labels at the word level are needed. This framework is evaluated in terms of automatic translation evaluation metrics, and an improvement of translation quality is observed. 1 Tuning alignment for an MT system is subject to practical difficulties. Unsupervised systems (Och and Ney, 2003; Liang et al., 2006) are based on generative models trained with the EM algorithm. They require large computational resources, and incorporating new features is difficult. In contrast, adding new features to some supervised systems (Liu et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) is easy, but the need of annotated data is a problem. Introduction In the first SMT systems (Brown et al., 1993), word alignment was introduced as a hidden variable of the translation model. When word-based translation models have been replaced by phrase-based models (Zens et al., 2002), alignment1 and translation model training have become two separated tasks. The system of Brown et al. was based on the noisy channel approach. Present SMT systems use a more general maximum entropy approach in which a log-linear combination of multiple feature funct"
N07-2022,J06-4004,1,0.894597,"Missing"
N07-2022,H05-1011,0,0.215348,"are optimized for the translation task. In addition, no link labels at the word level are needed. This framework is evaluated in terms of automatic translation evaluation metrics, and an improvement of translation quality is observed. 1 Tuning alignment for an MT system is subject to practical difficulties. Unsupervised systems (Och and Ney, 2003; Liang et al., 2006) are based on generative models trained with the EM algorithm. They require large computational resources, and incorporating new features is difficult. In contrast, adding new features to some supervised systems (Liu et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) is easy, but the need of annotated data is a problem. Introduction In the first SMT systems (Brown et al., 1993), word alignment was introduced as a hidden variable of the translation model. When word-based translation models have been replaced by phrase-based models (Zens et al., 2002), alignment1 and translation model training have become two separated tasks. The system of Brown et al. was based on the noisy channel approach. Present SMT systems use a more general maximum entropy approach in which a log-linear combination of multiple feature functions is imple"
N07-2022,C00-2163,0,0.0304844,"ion metric favoring alignments which benefit Machine Translation. The fact that the required alignment characteristics depend on each particular system makes it even more difficult. It seems that high precision alignments are better for phrase-based SMT (Chen and Federico, 2006; Ayan and Dorr, 2006), whereas high recall alignments are more suited to N-gram SMT (Mari˜no et al., 2006). In this context, alignment quality improvements does not necessarily imply translation quality improvements. This is in agreement with the observation of a poor correlation between word alignment error rate (AER (Och and Ney, 2000)) and automatic translation evaluation metrics (Ittycheriah and Roukos, 2005; Vilar et al., 2006). 85 Proceedings of NAACL HLT 2007, Companion Volume, pages 85–88, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics Recently some alignment evaluation metrics have been proposed which are more informative when the alignments are used to extract translation units (Fraser and Marcu, 2006; Ayan and Dorr, 2006). However, these metrics assess translation quality very indirectly. In this paper, we propose a novel framework for discriminative training of alignment models with au"
N07-2022,P02-1038,0,0.0396225,"eriah and Roukos, 2005) is easy, but the need of annotated data is a problem. Introduction In the first SMT systems (Brown et al., 1993), word alignment was introduced as a hidden variable of the translation model. When word-based translation models have been replaced by phrase-based models (Zens et al., 2002), alignment1 and translation model training have become two separated tasks. The system of Brown et al. was based on the noisy channel approach. Present SMT systems use a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Within this 1 Hereinafter, alignment will refer to word alignment, unless otherwise stated. A more general difficulty, however, is that of finding an alignment evaluation metric favoring alignments which benefit Machine Translation. The fact that the required alignment characteristics depend on each particular system makes it even more difficult. It seems that high precision alignments are better for phrase-based SMT (Chen and Federico, 2006; Ayan and Dorr, 2006), whereas high recall alignments are more suited to N-gram SMT (Mari˜no et al., 2006). In this context, alignment quality improveme"
N07-2022,J03-1002,0,0.0134264,"el parameters are not tuned in function of the translation task, but only indirectly. In this paper, we propose a novel framework for discriminative training of alignment models with automated translation metrics as maximization criterion. In this approach, alignments are optimized for the translation task. In addition, no link labels at the word level are needed. This framework is evaluated in terms of automatic translation evaluation metrics, and an improvement of translation quality is observed. 1 Tuning alignment for an MT system is subject to practical difficulties. Unsupervised systems (Och and Ney, 2003; Liang et al., 2006) are based on generative models trained with the EM algorithm. They require large computational resources, and incorporating new features is difficult. In contrast, adding new features to some supervised systems (Liu et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) is easy, but the need of annotated data is a problem. Introduction In the first SMT systems (Brown et al., 1993), word alignment was introduced as a hidden variable of the translation model. When word-based translation models have been replaced by phrase-based models (Zens et al., 2002), alignment1 and t"
N07-2022,P03-1021,0,0.0892164,"Missing"
N07-2022,P06-1097,0,0.0225194,"ignment quality improvements does not necessarily imply translation quality improvements. This is in agreement with the observation of a poor correlation between word alignment error rate (AER (Och and Ney, 2000)) and automatic translation evaluation metrics (Ittycheriah and Roukos, 2005; Vilar et al., 2006). 85 Proceedings of NAACL HLT 2007, Companion Volume, pages 85–88, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics Recently some alignment evaluation metrics have been proposed which are more informative when the alignments are used to extract translation units (Fraser and Marcu, 2006; Ayan and Dorr, 2006). However, these metrics assess translation quality very indirectly. In this paper, we propose a novel framework for discriminative training of alignment models with automated translation metrics as maximization criterion. Thus we just need a reference aligned at the sentence level instead of link labels at the word level. The paper is structured as follows. Section 2 explains the models used in our word aligner, focusing on the features designed to account for the specificities of the SMT system. In section 3, our minimum error training procedure is described and experim"
N07-2022,H91-1026,0,0.185519,"d because word positions s2 and s3 are embedded between links s1-t1 and s4-t1. Thus the link s4-t1 may introduces data sparseness in the translation model, although it may be a correct link. So we want to have a feature which counts the number of embedded word positions in an alignment. Figure 1: Word positions embedded in a tuple. In addition to the embedded word position feature, we used the same two distortion features as Moore to penalize reorderings in the alignment (one sums the number of crossing links, and the other one sums the amplitude of crossing links). We also used the φ2 score (Gale and Church, 1991) as a word association model, and as a POS-tags association model. 3 Experimental Work For these experiments we used the ChineseEnglish data provided for IWSLT’06 evaluation campaign (Paul, 2006). The training set contains 46000 sentences (of 6.7 and 7.0 average length). Parameters were tuned over the development set (dev4) provided, consisting of 489 sentences of 11.2 words in average, with 7 references. Our test set was a selection of 500 sentences (of 6 words in average, with 16 references) among dev1, dev2 and dev3 sets. 3.1 Optimization Procedure Once the alignment models were computed, a"
N07-2022,H05-1012,0,0.141103,"for the translation task. In addition, no link labels at the word level are needed. This framework is evaluated in terms of automatic translation evaluation metrics, and an improvement of translation quality is observed. 1 Tuning alignment for an MT system is subject to practical difficulties. Unsupervised systems (Och and Ney, 2003; Liang et al., 2006) are based on generative models trained with the EM algorithm. They require large computational resources, and incorporating new features is difficult. In contrast, adding new features to some supervised systems (Liu et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) is easy, but the need of annotated data is a problem. Introduction In the first SMT systems (Brown et al., 1993), word alignment was introduced as a hidden variable of the translation model. When word-based translation models have been replaced by phrase-based models (Zens et al., 2002), alignment1 and translation model training have become two separated tasks. The system of Brown et al. was based on the noisy channel approach. Present SMT systems use a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Wit"
N07-2022,2006.iwslt-papers.7,0,0.200467,"gnment characteristics depend on each particular system makes it even more difficult. It seems that high precision alignments are better for phrase-based SMT (Chen and Federico, 2006; Ayan and Dorr, 2006), whereas high recall alignments are more suited to N-gram SMT (Mari˜no et al., 2006). In this context, alignment quality improvements does not necessarily imply translation quality improvements. This is in agreement with the observation of a poor correlation between word alignment error rate (AER (Och and Ney, 2000)) and automatic translation evaluation metrics (Ittycheriah and Roukos, 2005; Vilar et al., 2006). 85 Proceedings of NAACL HLT 2007, Companion Volume, pages 85–88, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics Recently some alignment evaluation metrics have been proposed which are more informative when the alignments are used to extract translation units (Fraser and Marcu, 2006; Ayan and Dorr, 2006). However, these metrics assess translation quality very indirectly. In this paper, we propose a novel framework for discriminative training of alignment models with automated translation metrics as maximization criterion. Thus we just need a reference aligned at t"
N07-2022,2006.iwslt-papers.5,1,0.800234,"TM). A baseline SMT system, consisting of MARIE decoder and this translation model as unique feature2 , was used to produce a translation (OUT) of the development source set. Then, translation quality over the development set is maximized by iteratively varying the set of coefficients. The optimization procedure was performed by using the SPSA algorithm (Spall, 1992). SPSA is a stochastic implementation of the conjugate gradient method which requires only two evaluations of the objective function. It was observed to be more robust than the Downhill Simplex method when tuning SMT coefficients (Lambert and Banchs, 2006). Each function evaluation required to align the training corpus and build a new translation model. The algorithm converged after about 80 evaluations, lasting each 17 minutes with a 3 GHz processor. Alignment decoding was performed with a beam of 10 (it took 50 seconds and required 8 MB memory). Finally, the corpus was aligned with the optimum set of coefficients, and a full SMT system was build, with a target language model (trained on the provided training data), a word bonus model and two lexical models. SMT models weights were optimized with a standard Minimum Error Training (MET) strateg"
N07-2022,2002.tmi-tutorials.2,0,0.015191,"upervised systems (Och and Ney, 2003; Liang et al., 2006) are based on generative models trained with the EM algorithm. They require large computational resources, and incorporating new features is difficult. In contrast, adding new features to some supervised systems (Liu et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) is easy, but the need of annotated data is a problem. Introduction In the first SMT systems (Brown et al., 1993), word alignment was introduced as a hidden variable of the translation model. When word-based translation models have been replaced by phrase-based models (Zens et al., 2002), alignment1 and translation model training have become two separated tasks. The system of Brown et al. was based on the noisy channel approach. Present SMT systems use a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Within this 1 Hereinafter, alignment will refer to word alignment, unless otherwise stated. A more general difficulty, however, is that of finding an alignment evaluation metric favoring alignments which benefit Machine Translation. The fact that the required alignment characteristics depen"
N07-2022,2006.iwslt-evaluation.1,0,\N,Missing
N07-2035,2006.iwslt-evaluation.18,1,0.890672,"Missing"
N07-2035,2005.iwslt-1.23,1,0.905555,"Missing"
N07-2035,J06-4004,1,0.895476,"Missing"
N07-2035,E06-1005,1,0.799617,"ingual N -gram language model. In the phrase-based model, no monotonicity restriction is imposed on the segmentation and the probabilities are normally estimated simply by relative frequencies. This paper extends the analysis of both systems performed in (Crego et al., 2005a) by additionally performing a manual error analysis of both systems, which were the ones used by UPC and RWTH in the last Tc-Star evaluation. Furthermore, we will propose a way to combine both systems in order to improve the quality of translations. Experiments combining several kinds of MT systems have been presented in (Matusov et al., 2006), based only on the single best output of each system. Recently, a more straightforward approach of both systems has been performed in (Costa-juss` a et al., 2006) which simply selects, for each sentence, one of the provided hypotheses. This paper is organized as follows. In section 2, we briefly describe the phrase and the N -gram-based baseline systems. In the next section we present the evaluation framework. In Section 4 we report a structural comparison performed for both systems and, afterwards, in Section 5, we analyze the errors of both systems. Finally, in the last two sections we resc"
N07-2035,vilar-etal-2006-error,1,0.861806,"Missing"
N07-2035,N04-1033,1,0.812475,"n N -gram-based one. The exhaustive analysis includes a comparison of the translation models in terms of efficiency (number of translation units used in the search and computational time) and an examination of the errors in each system’s output. Additionally, we combine both systems, showing accuracy improvements. 1 Introduction Statistical machine translation (SMT) has evolved from the initial word-based translation models to more advanced models that take the context surrounding the words into account. The so-called phrase-based and N -gram-based models are two examples of these approaches (Zens and Ney, 2004; Mari˜ no et al., 2006). In current state-of-the-art SMT systems, the phrase-based or the N -gram-based models are usually the main features in a log-linear framework, reminiscent of the maximum entropy modeling approach. Two basic issues differentiate the N -gram-based system from the phrase-based one: the training data is sequentially segmented into bilingual units; and the probability of these units is estimated as a bilingual N -gram language model. In the phrase-based model, no monotonicity restriction is imposed on the segmentation and the probabilities are normally estimated simply by"
N07-2035,W06-3120,1,\N,Missing
P07-2054,2006.amta-papers.4,1,0.751245,"Missing"
P07-2054,2005.iwslt-1.23,1,0.868915,"Missing"
P07-2054,koen-2004-pharaoh,0,0.0425744,"and efficient algorithm (O(n), being n the search size) can be used in order to discard them, before rescoring work. Additionally, given that partial model costs are needed in rescoring work, our decoder allows to output the individual model costs computed for each translation unit (token t). Costs are encoded within the token s, as in the next example: (0 (1 &quot;o#or{1.5,0.9,0.6,0.2}&quot; 6)) where the token t is now composed of the translation unit ’o#or’, followed by (four) model costs. Multiple translation hypotheses can only be extracted if hypotheses recombinations are carefully saved. As in (Koehn, 2004), the decoder takes a record of any recombined hypothesis, allowing for a rigorous N -best generation. Model costs are referred to the current unit while the global score s is accumulated. Notice also that translation units (not words) are now used as tokens. 5 Experiments Experiments are carried out for a Spanish-to-English translation task using the EPPS data set, corresponding to session transcriptions of the European Parliament. Eff. base Beam size = 50 w/o cache 1, 820 w/ cache −50 Beam size = 100 w/o cache 2, 900 w/ cache −175 +tpos +reor +spos 2, 170 −110 2, 970 −190 3, 260 −210 4, 350"
P07-2054,J06-4004,1,0.914192,"Missing"
W05-0823,N04-1033,0,0.265878,"Missing"
W05-0823,2004.iwslt-evaluation.14,1,0.848754,"Missing"
W05-0823,N03-1017,0,0.0598703,"Missing"
W05-0823,P00-1056,0,0.0836903,"e pairs with a word ratio larger than 2.4. As a result of this preprocessing, the number of sentences in each training set was slightly reduced. However, no significant reduction was produced. In the case of French, a re-tokenizing procedure was performed in which all apostrophes appearing alone were attached to their corresponding words. For example, pairs of tokens such as l ’ and qu ’ were reduced to single tokens such as l’ and qu’. 134 Once the training data was preprocessed, a wordto-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000). As an approximation to the most probable alignment, the Viterbi alignment was considered. Then, the intersection and union of alignment sets in both directions were computed for each training set. 3.2 Feature Function Computation The considered translation system implements a total of five feature functions. The first of these models is the tuple 3-gram model, which was already described in section 2. Tuples for the translation model were extracted from the union set of alignments as shown in Figure 1. Once tuples had been extracted, the tuple vocabulary was pruned by using histogram pruning"
W05-0823,P02-1038,0,0.0900962,"were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model. 2 Bilingual N-gram Translation Model 1 Introduction During the last decade, statistical machine translation (SMT) systems have evolved from the original word-based approach (Brown et al., 1993) into phrase-based translation systems (Koehn et al., 2003). Similarly, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple models is implemented (Och and Ney, 2002). The SMT approach used in this work implements a log-linear combination of feature functions along with a translation model which is based on bilingual n-grams. This translation model was developed by de Gispert and Mari˜no (2002), and it differs from the well known phrase-based translation model in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. This model is described in section 2. Translation results from the four source languages made available for the shared task ("
W05-0823,P02-1040,0,0.0765659,"ts a beam-search strategy based on dynamic programming and takes into account all the five feature functions described above simultaneously. It also allows for three different pruning methods: threshold pruning, histogram pruning, and hypothesis recombination. For all the results presented in this work the decoder’s monotonic search modality was used. An optimization tool, which is based on a simplex method (Press et al., 2002), was developed and used for computing log-linear weights for each of the feature functions described above. This algorithm adjusts the log-linear weights so that BLEU (Papineni et al., 2002) is maximized over a given development set. One optimization for each language pair was performed by using the 2000-sentence development sets made available for the shared task. 4 Shared Task Results Table 2 presents the BLEU scores obtained for the shared task test data. Each test set consisted of 2000 sentences. The computed BLEU scores were case insensitive and used one translation reference. 135 sufficient and that , in the future , it is necessary to develop the Union better and a different structure... It is evident from these translation outputs that translation quality decreases when m"
W05-0823,J93-2003,0,\N,Missing
W06-3120,A00-1031,0,0.0229195,"ese figures in the optimization function. 3 Shared Task Results 3.1 Data The data provided for this shared task corresponds to a subset of the official transcriptions of the European Parliament Plenary Sessions, and it is available through the shared task website at: http://www.statmt.org/wmt06/shared-task/. The development set used to tune the system consists of a subset (500 first sentences) of the official development set made available for the Shared Task. We carried out a morphological analysis of the data. The English POS-tagging has been carried out using freely available T N T tagger (Brants, 2000). In the Spanish case, we have used the F reeling (Carreras et al., 2004) analysis tool which generates the POS-tagging for each input word. 3.2 Systems configurations The baseline system is the same for all tasks and includes the following features functions: cp, pp, lm, ibm1, ibm1−1 , wb, pb. The POStag target language model has been used in those tasks for which the tagger was available. Table 1 shows the reordering configuration used for each task. The Block Reordering (application 2) has been used when the source language belongs to the Romanic family. The length of the block is limited t"
W06-3120,W05-0827,1,0.879283,"Missing"
W06-3120,2005.iwslt-1.23,1,0.907982,"Missing"
W06-3120,W06-3125,1,0.884485,"Missing"
W06-3120,P05-2012,1,0.889176,"Missing"
W06-3120,W05-0831,0,0.0297232,"Full verb forms The morphology of the verbs usually differs in each language. Therefore, it is interesting to classify the verbs in order to address the rich variety of verbal forms. Each verb is reduced into its base form and reduced POS tag as explained in (de Gispert, 2005). This transformation is only done for the alignment, and its goal is to simplify the work of the word alignment improving its quality. Block reordering (br) The difference in word order between two languages is one of the most significant sources of error in SMT. Related works either deal with reordering in general as (Kanthak et al., 2005) or deal with local reordering as (Tillmann and Ney, 2003). We report a local reordering technique, which is implemented as a preprocessing stage, with two applications: (1) to improve only alignment quality, and (2) to improve alignment quality and to infer reordering in translation. Here, we present a short explanation of the algorithm, for further details see Costa-juss`a and Fonollosa (2006). 142 Proceedings of the Workshop on Statistical Machine Translation, pages 142–145, c New York City, June 2006. 2006 Association for Computational Linguistics of the bilingual phrase, and no word on ei"
W06-3120,W06-3114,0,0.0221223,"ts to observe its efficiency in all the pairs used in this evaluation. The rgraph has been applied in those cases where: we do not use br2 (there is no sense in applying them simultaneously); and we have the tagger for the source language model available. In the case of the pair GeEn, we have not experimented any reordering, we left the application of both reordering approaches as future work. 3.3 Discussion Table 2 presents the BLEU scores evaluated on the test set (using TRUECASE) for each configuration. The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006). For both, Es2En and Fr2En tasks, br helps slightly. The improvement of the approach depends on the quality of the alignment. The better alignments allow to extract higher quality Alignment Blocks (Costa-juss`a and Fonollosa, 2006). The En2Es task is improved when adding both br1 and rgraph. Similarly, the En2Fr task seems to perform fairly well when using the rgraph. In this case, the improvement of the approach depends on the quality of the alignment patterns (Crego et al., 2006). However, it has the advantage of delaying the final decision of reordering to the overall search, where all mod"
W06-3120,N03-1017,0,0.00728769,"o infer reordering in translation. Here, we present a short explanation of the algorithm, for further details see Costa-juss`a and Fonollosa (2006). 142 Proceedings of the Workshop on Statistical Machine Translation, pages 142–145, c New York City, June 2006. 2006 Association for Computational Linguistics of the bilingual phrase, and no word on either side of the phrase is aligned to a word out of the phrase. We limit the maximum size of any given phrase to 7. The huge increase in computational and storage cost of including longer phrases does not provide a significant improvement in quality (Koehn et al., 2003) as the probability of reappearance of larger phrases decreases. 2.3 Figure 1: Example of an Alignment Block, i.e. a pair of consecutive blocks whose target translation is swapped This reordering strategy is intended to infer the most probable reordering for sequences of words, which are referred to as blocks, in order to monotonize current data alignments and generalize reordering for unseen pairs of blocks. Given a word alignment, we identify those pairs of consecutive source blocks whose translation is swapped, i.e. those blocks which, if swapped, generate a correct monotone translation. Fi"
W06-3120,J04-4002,0,0.0268059,"created). Based on this information, the source side of the bilingual corpora are reordered. In case of applying the reordering technique for purpose (1), we modify only the source training corpora to realign and then we recover the original order of the training corpora. In case of using Block Reordering for purpose (2), we modify all the source corpora (both training and test), and we use the new training corpora to realign and build the final translation system. 2.2 Phrase Extraction Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in Och and Ney (2004). A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: words are consecutive along both sides 143 Feature functions Conditional and posterior probability (cp, pp) Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency in both directions. The target language model (lm) consists of an n-gram model, in which the probability of a translation hypothesis is approximated by the product of word n-gram probabilities. As default language model feature, we use a standard word-base"
W06-3120,carreras-etal-2004-freeling,0,\N,Missing
W06-3120,J03-1005,0,\N,Missing
W06-3120,N04-1033,0,\N,Missing
W06-3125,W05-0823,1,0.872672,"Missing"
W06-3125,A00-1031,0,0.117706,"d over the development set for each of the six translation directions considered. 163 This baseline system is actually very similar to the system used for last year’s shared task “Exploiting Parallel Texts for Statistical Machine Translation” of ACL’05 Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond (Banchs et al., 2005), whose results are available at: http://www.statmt.org/wpt05/ mt-shared-task/. A more detailed description of the system can be found in (2005). The tools used for POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English. All language models were estimated using the SRI language modeling toolkit. Word-to-word alignments were extracted with GIZA++. Improvements in word-toword alignments were achieved through verb group classification as described in (de Gispert, 2005). 3 Reordering Framework In this section we outline the reordering framework used for the experiments (Crego and Mari˜no, 2006). A highly constrained reordered search is performed by means of a set of reordering patterns (linguistically motivated rewrite patterns) which are used to extend the monotone search graph with additional arcs."
W06-3125,carreras-etal-2004-freeling,0,0.0548444,"Missing"
W06-3125,W06-3120,1,0.883993,"Missing"
W06-3125,N04-1033,0,0.0842803,"Missing"
W06-3125,P05-2012,1,0.901179,"Missing"
W06-3125,N03-1017,0,0.00542142,"luation with a tagged target language model (using Part-Of-Speech tags). For both Spanish-English translation directions and the English-to-French translation task, the baseline system allows for linguistically motivated sourceside reorderings. 2 Baseline N-gram-based SMT System 1 Introduction The statistical machine translation approach used in this work implements a log-linear combination of feature functions along with a translation model which is based on bilingual n-grams (de Gispert and Mari˜no, 2002). This translation model differs from the well known phrase-based translation approach (Koehn et al., 2003) in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. This translation approach is described in detail in (Mari˜no et al., 2005). For those translation tasks with Spanish or English as target language, an additional tagged (usAs already mentioned, the translation model used here is based on bilingual n-grams. It actually constitutes a language model of bilingual units, referred to as tuples, which approximates the joint probability between source and target languages by us"
W06-3125,2005.mtsummit-papers.36,1,0.909254,"Missing"
W06-3125,J93-2003,0,\N,Missing
W07-0720,W06-3125,1,0.849606,"Missing"
W07-0720,W06-1609,1,0.795127,"Missing"
W07-0720,W06-3114,0,0.0213063,"this system participation in the ACL 2007 SECOND WORK SHOP ON STATISTICAL MACHINE TRANSLA TION . Results on three pairs of languages are reported, namely from Spanish, French and German into English (and the other way round) for both the in-domain and out-of-domain tasks. 2 Baseline N-gram-based SMT System 1 Introduction Based on estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternatively to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in (Koehn and Monz, 2005; Koehn and Monz, 2006). Given the challenge of domain adaptation, efforts have been focused on improving strategies for Ngram-based SMT which could generalize better. Specifically, a novel reordering strategy is explored. It is based on extending the search by using precomputed statistical information. Results are promising while keeping computational expenses at a similar level as monotonic search. Additionally, a bonus for tuples from the out-of-domain corpus is The translation model is based on bilingual n-grams. It actually constitutes a language model of bilingual units, referred to as tuples, which approximat"
W07-0720,J06-4004,1,0.847357,"Missing"
W07-0720,E99-1010,0,0.731366,"smaller tuples which reduces the translation vocabulary sparseness. These new tuples are used to build the SMT system. 3 Baseline System Enhanced with a Weighted Reordering Input Graph This section briefly describes the statistical machine reordering (SMR) technique. Further details on the architecture of SMR system can be found on (Costa-juss`a and Fonollosa, 2006). 3.1 Concept The SMR system can be seen as a SMT system which translates from an original source language (S) to a reordered source language (S’), given a target language (T). The SMR technique works with statistical word classes (Och, 1999) instead of words themselves (particularly, we have used 200 classes in all experiments). Figure 1: SMR approach in the (A) training step (B) in the test step (the weight of each arch is in brackets). 3.2 Using SMR technique to improve SMT training The original source corpus S is translated into the reordered source corpus S’ with the SMR system. Figure 1 (A) shows the corresponding block diagram. The reordered training source corpus and the original training target corpus are used to build the SMT system. The main difference here is that the training is computed with the S’2T task instead of"
W07-0720,W05-0820,0,\N,Missing
W08-0307,W05-0909,0,0.0607395,"alls out of the projection of chunk c1 ([t4 , t4 ]). A further refinement can be done using the chunks of the target side. The same technique is applied by switching the role of source and target words/chunks in the algorithm described above and using the output of the basic source-based refinement (described above) as the high-recall alignment set, i.e., instead of Union. We use the standard four-reference NIST MTEval data sets for the years 2003, 2004 and 2005 (henceforth MT03, MT04 and MT05, respectively) for testing and the 2002 data set for tuning.6 BLEU4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and multiple-reference Word Error Rate scores are reported. SMT decoding is done using MARIE,7 a freely available N -gram-based decoder implementing a beam search strategy with distortion/reordering capabilities (Crego and Mari˜no, 2007a). Optimization is done with an in-house implementation of the SIMPLEX (Nelder and Mead, 1965) algorithm. 7 Evaluation 7.2 7.1 In this section we assess the accuracy results of the techniques introduced in this paper for alignment refinement and word reordering. Experimental Framework All of the training data used here is available from the Linguistic Data Con"
W08-0307,J93-2003,0,0.0122549,"h precision and high recall alignment sets, respectively. We will study the effect of various initial alignment sets (such as grow-diag-final instead of Union) in the future. The method is based on the fact that linguistic phrases (chunks), like raw words, have translation correspondences and can therefore be aligned. We use chunk information to reduce the number of allowed alignments for a given word. The simple idea that words in a source chunk are typically aligned to words in a single possible target chunk is used to discard alignments which link words from 2 We use IBM-1 to IBM-5 models (Brown et al., 1993) implemented with GIZA++ (Och and Ney, 2003). 57 Figure 4: Chunk projection: solid link are Intersection links and all links (solid and dashed) are Union links. We outline the algorithm next. The method can be decomposed in two steps. In the first step, using the Intersection set of alignments and source-side chunks, each chunk is projected into the target side. Figure 4 shows an example of word alignment refinement. The projection c′k of the chunk ck is composed of the sequence of consecutive target words [tlef t , tright ] which can be determined as follows: • All target words tj contained i"
W08-0307,P05-1066,0,0.215248,"ferent language pairs. Structural information offers a greater potential to learn generalizations about relationships between languages than flat-structure models. The need for these ‘mappings’ is specially relevant when handling language pairs with very different word order, such as Arabic-English or Chinese-English. Many alternatives have been proposed on using syntactic information in SMT systems. They range from those aiming at harmonizing (monotonizing) the word order of the considered language pairs by means of a set of linguistically-motivated reordering patterns (Xia and McCord, 2004; Collins et al., 2005) to others considering translation a synchronous parsing process where reorderings introduced in the overall search are syntactically motivated (Galley et al., 2004; Quirk et al., 2005). The work presented here follows the word order harmonization strategy. 53 Proceedings of the Third Workshop on Statistical Machine Translation, pages 53–61, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Collins et al. (2005) describe a technique for preprocessing German to look more like English syntactically. They used six transformations that are applied on German parsed te"
W08-0307,W06-1609,0,0.107022,"Missing"
W08-0307,P07-2054,1,0.891682,"Missing"
W08-0307,2007.mtsummit-papers.16,1,0.859517,"Missing"
W08-0307,2005.mtsummit-papers.37,1,0.84332,"Missing"
W08-0307,N04-4038,0,0.506991,"hologically complex containing clitics whose translations are represented separately in English and sometimes in a different order. For instance, possessive pronominal enclitics are attached to the noun they modify in Arabic but their translation precedes the English translation of the noun: kitAbu+hu1 ‘book+his → his book’. Other clitics include the definite article Al+ ‘the’, the conjunction w+ ‘and’ and the preposition l+ ‘of/for’, among others. We use the Penn Arabic Treebank tokenization scheme which splits three classes of clitics only. This scheme is compatible with the chunker we use (Diab et al., 2004). Secondly, Arabic verb subjects may be: prodropped (verb conjugated), pre-verbal (SVO), or post-verbal (VSO). The VSO order is quite challenging in the context of translation to English. For small noun phrases (NP), small phrase pairs in a phrase table and some degree of distortion can easily move the verb to follow the NP. But this becomes much less likely with very long NPs that exceed the size of phrases in a phrase table. Finally, Arabic adjectival modifiers typically follow their nouns (with a small exception of some superlative adjectives). For example, rajul Tawiyl (lit. man tall) tran"
W08-0307,N04-1035,0,0.0344264,"e need for these ‘mappings’ is specially relevant when handling language pairs with very different word order, such as Arabic-English or Chinese-English. Many alternatives have been proposed on using syntactic information in SMT systems. They range from those aiming at harmonizing (monotonizing) the word order of the considered language pairs by means of a set of linguistically-motivated reordering patterns (Xia and McCord, 2004; Collins et al., 2005) to others considering translation a synchronous parsing process where reorderings introduced in the overall search are syntactically motivated (Galley et al., 2004; Quirk et al., 2005). The work presented here follows the word order harmonization strategy. 53 Proceedings of the Third Workshop on Statistical Machine Translation, pages 53–61, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Collins et al. (2005) describe a technique for preprocessing German to look more like English syntactically. They used six transformations that are applied on German parsed text to reorder it before passing it on to a phrase-based system. They show a moderate statistically significant improvement. Our work differs from theirs crucially i"
W08-0307,P05-1071,1,0.806277,"sh newswire text from the English Gigaword corpus (LDC2003T05). Additionally, we use a 5-gram language model computed over the POS tagged English side of the training corpus. Language models are implemented using the SRILM toolkit (Stolcke, 2002). For Arabic tokenization, we use the Arabic TreeBank tokenization scheme: 4-way normalized segments into conjunction, particle, word and pronominal clitic. For POS tagging, we use the collapsed tagset for PATB (24 tags). Tokenization and POS tagging are done using the publicly available Morphological Analysis and Disambiguation of Arabic (MADA) tool (Habash and Rambow, 2005). For chunking Arabic, we use the AMIRA (ASVMT) toolkit (Diab et al., 2004). English preprocessing simply included down-casing, separating punctuation from words and splitting off “’s”. The English side is POS-tagged with TNT(Brants, 2000) and chunked with the freely available OpenNlp5 tools. 3 http://www.ldc.upenn.edu The parallel text includes Arabic News (LDC2004T17), eTIRR (LDC2004E72), English translation of Arabic Treebank (LDC2005E46), and Ummah (LDC2004T18). 5 http://opennlp.sourceforge.net/ 4 58 Results Alignment Refinement Experiment We contrast three systems built from different wor"
W08-0307,J06-4004,1,0.85668,"Missing"
W08-0307,P00-1056,0,0.0539878,"typically producing a large number of noisy (wrong) alignments. The N -gram-based SMT approach suffers highly from the presence of noisy alignments since translation units are extracted out of single alignment-based segmentations of training sentences. Noisy alignments lead to large translation units, which cause a loss of translation information and add to sparseness problems. We propose an alignment refinement method to reduce the number of wrong alignments. The method employs two initial alignment sets: one with high precision, the other with high recall. We use the Intersection and Union (Och and Ney, 2000) of both alignment directions2 as the high precision and high recall alignment sets, respectively. We will study the effect of various initial alignment sets (such as grow-diag-final instead of Union) in the future. The method is based on the fact that linguistic phrases (chunks), like raw words, have translation correspondences and can therefore be aligned. We use chunk information to reduce the number of allowed alignments for a given word. The simple idea that words in a source chunk are typically aligned to words in a single possible target chunk is used to discard alignments which link wo"
W08-0307,J03-1002,0,0.0367108,"espectively. We will study the effect of various initial alignment sets (such as grow-diag-final instead of Union) in the future. The method is based on the fact that linguistic phrases (chunks), like raw words, have translation correspondences and can therefore be aligned. We use chunk information to reduce the number of allowed alignments for a given word. The simple idea that words in a source chunk are typically aligned to words in a single possible target chunk is used to discard alignments which link words from 2 We use IBM-1 to IBM-5 models (Brown et al., 1993) implemented with GIZA++ (Och and Ney, 2003). 57 Figure 4: Chunk projection: solid link are Intersection links and all links (solid and dashed) are Union links. We outline the algorithm next. The method can be decomposed in two steps. In the first step, using the Intersection set of alignments and source-side chunks, each chunk is projected into the target side. Figure 4 shows an example of word alignment refinement. The projection c′k of the chunk ck is composed of the sequence of consecutive target words [tlef t , tright ] which can be determined as follows: • All target words tj contained in Intersection links (si , tj ) with source"
W08-0307,C04-1073,0,0.148179,"word order between different language pairs. Structural information offers a greater potential to learn generalizations about relationships between languages than flat-structure models. The need for these ‘mappings’ is specially relevant when handling language pairs with very different word order, such as Arabic-English or Chinese-English. Many alternatives have been proposed on using syntactic information in SMT systems. They range from those aiming at harmonizing (monotonizing) the word order of the considered language pairs by means of a set of linguistically-motivated reordering patterns (Xia and McCord, 2004; Collins et al., 2005) to others considering translation a synchronous parsing process where reorderings introduced in the overall search are syntactically motivated (Galley et al., 2004; Quirk et al., 2005). The work presented here follows the word order harmonization strategy. 53 Proceedings of the Third Workshop on Statistical Machine Translation, pages 53–61, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Collins et al. (2005) describe a technique for preprocessing German to look more like English syntactically. They used six transformations that are appl"
W08-0307,W07-0401,0,0.149171,"itional distortion-based reordering). The value of reordering is diminished if the decoder is run in a non-monotonic way. Recently, Crego and Mari˜no (2007b) employ POS tags to automatically learn reorderings in training. They allow all possible learned reorderings to be used to create a lattice that is input to the decoder, which further improves translation accuracy. Similarly, Costa-juss`a and Fonollosa (2006) use statistical word classes to generalize reorderings, which are learned/introduced in a translation process that transforms the source language into the target language word order. Zhang et al. (2007) describe a similar approach using unlexicalized context-free chunk tags (XPs) to learn reordering rules for Chinese-English SMT. Crego and Mari˜no (2007c) extend their previous work using syntax trees (dependency parsing) to learn reorderings on a Chinese-English task. Habash (2007) applies automatically-learned syntactic reordering rules (for Arabic-English SMT) to preprocess the input before passing it to a phrase-based SMT decoder. As in (Zhang et al., 2007), (Costa-juss`a and Fonollosa, 2006) and (Crego and Mari˜no, 2007b), we employ a word graph for a tight coupling between reordering an"
W08-0307,P02-1040,0,\N,Missing
W08-0307,A00-1031,0,\N,Missing
W08-0307,P05-1034,0,\N,Missing
W08-0315,W08-0315,1,0.0512755,"Missing"
W08-0315,J90-2002,0,0.809551,"Missing"
W08-0315,W07-0718,0,0.152941,"Missing"
W08-0315,carreras-etal-2004-freeling,0,0.138533,"Missing"
W08-0315,W06-3114,0,0.151633,"Missing"
W08-0315,P00-1056,0,0.073606,"Missing"
W08-0315,J04-4002,0,0.0735646,"Missing"
W08-0315,W05-0820,0,\N,Missing
W08-0315,A00-1031,0,\N,Missing
W08-0315,J06-4004,1,\N,Missing
W09-0417,2007.tmi-papers.28,0,0.0611781,"Missing"
W09-0417,2007.mtsummit-papers.11,0,0.0245406,"ights, since they dispense with the lexical reordering model; these weights were tuned on the same dataset, using an in-house implementation of the simplex algorithm. 3 3.1 of each entry of the phrase table; and by (ii) adding one or several contextual scores to the phrase table. Using standard MERT, the corresponding weights can be optimized on development data. A typical contextual score corresponds to p(e|f , C(f )), where C(f ) is some contextual information about the source phrase f . An external disambiguation system can be used to provide one global context score (Stroppa et al., 2007; Carpuat and Wu, 2007; Max et al., 2008)); alternatively, several scores based on single features can be estimated using relative frequencies (Gimpel and Smith, 2008): Extensions A context-aware system In phrase-based translation, source phrases are translated irrespective of their (source) context. This is often not perceived as a limitation as (i) typical text domains usually contain only few senses for polysemous words, thus limiting the use of word sense disambiguation (WSD); and (ii) using long-span target language models (4-grams and more) often capture sufficient context to select the more appropriate trans"
W09-0417,J04-2004,0,0.0544024,"mate such large LMs, a vocabulary was first defined for both languages by including all tokens in the WMT parallel data. This initial vocabulary of 130K words was then extended by adding the most frequent words observed in the additional training data. This procedure yielded a vocabulary of one million words in both languages. 2.5 A N-code baseline N-code implements the n-gram-based approach to Statistical Machine Translation (Mariño et al., 2006). In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training such a model requires to reorder source sentences so as to match the target word order. This is also performed via a stochastic finite-state reordering model, which uses part-of-speech information to generalise reordering patterns beyond lexical regularities. The reordering model is trained on a version of the parallel corpora where the source sentences have been reordered via the unfold heuristics (Crego and Mariño, 2007). A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models a"
W09-0417,P96-1041,0,0.135168,"es so as to match the target word order. This is also performed via a stochastic finite-state reordering model, which uses part-of-speech information to generalise reordering patterns beyond lexical regularities. The reordering model is trained on a version of the parallel corpora where the source sentences have been reordered via the unfold heuristics (Crego and Mariño, 2007). A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models and bilingual tuple models built using Kneser-Ney backoff (Chen and Goodman, 1996). The maximum tuple size was also set to 7. Language model training The training data were divided into several sets based on dates on genres (resp. 7 and 9 sets for English and French). On each set, a standard 4-gram LM was estimated from the 1M word vocabulary with in-house tools using absolute discounting interpolated with lower order models. The resulting LMs were then linearly interpolated using interpolation coefficients chosen so as to minimise perplexity of the development set (dev2009a). Due to memory limitations, the final LMs were pruned using perplexity as pruning criterion. 2.6 Tu"
W09-0417,W08-0310,1,0.911154,"Missing"
W09-0417,W08-0302,0,0.0772343,"f the simplex algorithm. 3 3.1 of each entry of the phrase table; and by (ii) adding one or several contextual scores to the phrase table. Using standard MERT, the corresponding weights can be optimized on development data. A typical contextual score corresponds to p(e|f , C(f )), where C(f ) is some contextual information about the source phrase f . An external disambiguation system can be used to provide one global context score (Stroppa et al., 2007; Carpuat and Wu, 2007; Max et al., 2008)); alternatively, several scores based on single features can be estimated using relative frequencies (Gimpel and Smith, 2008): Extensions A context-aware system In phrase-based translation, source phrases are translated irrespective of their (source) context. This is often not perceived as a limitation as (i) typical text domains usually contain only few senses for polysemous words, thus limiting the use of word sense disambiguation (WSD); and (ii) using long-span target language models (4-grams and more) often capture sufficient context to select the more appropriate translation for a source phrase based on the target context. In fact, attempts at using source contexts in phrase-based SMT have to date failed to sho"
W09-0417,P07-2045,0,0.0111904,"Missing"
W09-0417,2008.eamt-1.17,1,0.839871,"ense with the lexical reordering model; these weights were tuned on the same dataset, using an in-house implementation of the simplex algorithm. 3 3.1 of each entry of the phrase table; and by (ii) adding one or several contextual scores to the phrase table. Using standard MERT, the corresponding weights can be optimized on development data. A typical contextual score corresponds to p(e|f , C(f )), where C(f ) is some contextual information about the source phrase f . An external disambiguation system can be used to provide one global context score (Stroppa et al., 2007; Carpuat and Wu, 2007; Max et al., 2008)); alternatively, several scores based on single features can be estimated using relative frequencies (Gimpel and Smith, 2008): Extensions A context-aware system In phrase-based translation, source phrases are translated irrespective of their (source) context. This is often not perceived as a limitation as (i) typical text domains usually contain only few senses for polysemous words, thus limiting the use of word sense disambiguation (WSD); and (ii) using long-span target language models (4-grams and more) often capture sufficient context to select the more appropriate translation for a source"
W09-0417,J06-4004,1,0.915103,"Missing"
W09-0417,P03-1021,0,0.00849652,"ts based on dates on genres (resp. 7 and 9 sets for English and French). On each set, a standard 4-gram LM was estimated from the 1M word vocabulary with in-house tools using absolute discounting interpolated with lower order models. The resulting LMs were then linearly interpolated using interpolation coefficients chosen so as to minimise perplexity of the development set (dev2009a). Due to memory limitations, the final LMs were pruned using perplexity as pruning criterion. 2.6 Tuning procedure The Moses-based systems were tuned using the implementation of minimum error rate training (MERT) (Och, 2003) distributed with the Moses decoder, using the development corpus (dev2009a). For the context-less systems, tuning concerned the 14 usual weights; tuning the Out of vocabulary word and perplexity To evaluate our vocabulary and LMs, we used the official devtest and test sets. The out-of-vocabulary (OOV) rate was drastically reduced by increasing 101 22 weights of the context-aware systems (see 3.1) proved to be much more challenging, and the weights used in our submissions are probably far from optimal. The N-code systems only rely on 9 weights, since they dispense with the lexical reordering m"
W09-0417,E03-1076,0,\N,Missing
W09-0417,C08-1098,0,\N,Missing
W09-0417,E09-3008,0,\N,Missing
W09-0417,J04-2003,0,\N,Missing
W09-0417,H05-1085,0,\N,Missing
W10-1704,W09-0417,1,0.716697,", 2003) distributed with the Moses decoder, using the development corpus (news-test2008). The N -code systems were also tuned by the same implementation of MERT, which was slightly modified to match the requirements of our decoder. The BLEU score is used as objective function for MERT and to evaluate test performance. The interpolation experiment for FrenchEnglish was tuned on news-test2008a (first 1025 lines). Optimization was carried out over newstest2008b (last 1026 lines). Language Models The English and French language models (LMs) are the same as for the last year’s French-English task (Allauzen et al., 2009) and are heavily tuned to the newspaper/newswire genre, using the first part of the WMT09 official development data (dev2009a). We used all the authorized news corpora, including the French and English Gigaword corpora, for translating both into French (1.4 billion tokens) and English (3.7 billion tokens). To estimate such LMs, a vocabulary was defined for both languages by including all tokens in the WMT parallel data. This initial vocabulary of 130K words was then extended with the most frequent words observed in the training data, yielding a vocabulary of one million words in both languages"
W10-1704,J04-2004,0,0.140666,"lgorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder in the form of word lattices (Crego and no, 2006). French-English systems 4.1 Baseline N -coder systems For this language pair, we used our in-house N -code system, which implements the n-grambased approach to SMT. In a nutshell, the translation model is implemented as a stochastic finitestate transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, our system implements eight feature functions which are optimally combined using a discriminative training framework (Och, 2003): a target-language model; two lexicon models, which give complementary translation scores for each tuple; two lexicalized reordering models aiming at predicting the orien"
W10-1704,corston-oliver-gamon-2004-normalizing,0,0.0994184,"ies both at training and decoding time. When aligning parallel texts at the word level, German compound words typically tend to align with more than one English word; this, in turn, tends to increase the number of possible translation counterparts for each English type, and to make the corresponding alignment scores less reliable. In decoding, new compounds or unseen morphological variants of existing words artificially increase the number outof-vocabulary (OOV) forms, which severely hurts the overall translation quality. Several researchers have proposed normalization (Niessen and Ney, 2004; Corston-oliver and Gamon, 2004; Goldwater and McClosky, 2005) and compound splitting (Koehn and Knight, 2003; Stymne, 2008; Stymne, 2009) methods. Our approach here is similar, yet uses different implementations; we also studied the joint effect of combining both techniques. 2 3.1 Reducing the lexical redundancy 1 Introduction System architecture and resources In German, determiners, pronouns, nouns and adjectives carry inflection marks (typically suffixes) In this section, we describe the main characteristics of the phrase-based systems developed for this 54 Proceedings of the Joint 5th Workshop on Statistical Machine Tra"
W10-1704,W08-0310,1,0.8106,"Missing"
W10-1704,H05-1085,0,0.0320767,"g time. When aligning parallel texts at the word level, German compound words typically tend to align with more than one English word; this, in turn, tends to increase the number of possible translation counterparts for each English type, and to make the corresponding alignment scores less reliable. In decoding, new compounds or unseen morphological variants of existing words artificially increase the number outof-vocabulary (OOV) forms, which severely hurts the overall translation quality. Several researchers have proposed normalization (Niessen and Ney, 2004; Corston-oliver and Gamon, 2004; Goldwater and McClosky, 2005) and compound splitting (Koehn and Knight, 2003; Stymne, 2008; Stymne, 2009) methods. Our approach here is similar, yet uses different implementations; we also studied the joint effect of combining both techniques. 2 3.1 Reducing the lexical redundancy 1 Introduction System architecture and resources In German, determiners, pronouns, nouns and adjectives carry inflection marks (typically suffixes) In this section, we describe the main characteristics of the phrase-based systems developed for this 54 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages"
W10-1704,E09-3008,0,0.0133135,"to align with more than one English word; this, in turn, tends to increase the number of possible translation counterparts for each English type, and to make the corresponding alignment scores less reliable. In decoding, new compounds or unseen morphological variants of existing words artificially increase the number outof-vocabulary (OOV) forms, which severely hurts the overall translation quality. Several researchers have proposed normalization (Niessen and Ney, 2004; Corston-oliver and Gamon, 2004; Goldwater and McClosky, 2005) and compound splitting (Koehn and Knight, 2003; Stymne, 2008; Stymne, 2009) methods. Our approach here is similar, yet uses different implementations; we also studied the joint effect of combining both techniques. 2 3.1 Reducing the lexical redundancy 1 Introduction System architecture and resources In German, determiners, pronouns, nouns and adjectives carry inflection marks (typically suffixes) In this section, we describe the main characteristics of the phrase-based systems developed for this 54 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 54–59, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computation"
W10-1704,E03-1076,0,0.457998,", German compound words typically tend to align with more than one English word; this, in turn, tends to increase the number of possible translation counterparts for each English type, and to make the corresponding alignment scores less reliable. In decoding, new compounds or unseen morphological variants of existing words artificially increase the number outof-vocabulary (OOV) forms, which severely hurts the overall translation quality. Several researchers have proposed normalization (Niessen and Ney, 2004; Corston-oliver and Gamon, 2004; Goldwater and McClosky, 2005) and compound splitting (Koehn and Knight, 2003; Stymne, 2008; Stymne, 2009) methods. Our approach here is similar, yet uses different implementations; we also studied the joint effect of combining both techniques. 2 3.1 Reducing the lexical redundancy 1 Introduction System architecture and resources In German, determiners, pronouns, nouns and adjectives carry inflection marks (typically suffixes) In this section, we describe the main characteristics of the phrase-based systems developed for this 54 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 54–59, c Uppsala, Sweden, 15-16 July 2010. 201"
W10-1704,N04-4026,0,0.276697,"the translation model, our system implements eight feature functions which are optimally combined using a discriminative training framework (Och, 2003): a target-language model; two lexicon models, which give complementary translation scores for each tuple; two lexicalized reordering models aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. One novelty this year are the introduction of lexicalized reordering models (Tillmann, 2004). Such models require to estimate reordering probabilities for each phrase pairs, typically distinguishing three case, depending whether the current phrase is translated monotone, swapped or discontiguous with respect to the 4.2 A bilingual POS-based reordering model For this year evaluation, we also experimented with an additional reordering model, which is estimated as a standard n-gram language model, over generalized translation units. In the experiments reported below, we generalized tuples using POS tags, instead of raw word forms. Figure 1 displays the same sequence of tuples when built"
W10-1704,J06-4004,1,0.805664,"Missing"
W10-1704,J04-2003,0,0.0577646,"s a number of difficulties both at training and decoding time. When aligning parallel texts at the word level, German compound words typically tend to align with more than one English word; this, in turn, tends to increase the number of possible translation counterparts for each English type, and to make the corresponding alignment scores less reliable. In decoding, new compounds or unseen morphological variants of existing words artificially increase the number outof-vocabulary (OOV) forms, which severely hurts the overall translation quality. Several researchers have proposed normalization (Niessen and Ney, 2004; Corston-oliver and Gamon, 2004; Goldwater and McClosky, 2005) and compound splitting (Koehn and Knight, 2003; Stymne, 2008; Stymne, 2009) methods. Our approach here is similar, yet uses different implementations; we also studied the joint effect of combining both techniques. 2 3.1 Reducing the lexical redundancy 1 Introduction System architecture and resources In German, determiners, pronouns, nouns and adjectives carry inflection marks (typically suffixes) In this section, we describe the main characteristics of the phrase-based systems developed for this 54 Proceedings of the Joint 5th Wor"
W10-1704,P03-1021,0,0.100273,"o SMT. In a nutshell, the translation model is implemented as a stochastic finitestate transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, our system implements eight feature functions which are optimally combined using a discriminative training framework (Och, 2003): a target-language model; two lexicon models, which give complementary translation scores for each tuple; two lexicalized reordering models aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. One novelty this year are the introduction of lexicalized reordering models (Tillmann, 2004). Such models require to estimate reordering probabilities for each phrase pairs, typically distinguishing three case, depending whether"
W10-1704,C08-1098,0,0.0783579,"vocabulary and to improve the robustness of the alignment probabilities, we considered various normalization strategies for the different word classes. In a nutshell, normalizing amounts to collapsing several German forms of a given lemma into a unique representative, using manually written normalization patterns. A pattern typically specifies which forms of a given morphological paradigm should be considered equivalent when translating into English. These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags. Table 1 displays the analysis of an example sentence. 2 In most cases, normalization patterns replace a word form by its lemma; in order to partially preserve some inflection marks, we introduced two generic suffixes, +s and +en which respectively denote plural and genitive wherever needed. Typical normalization rules take the following form: • For articles, adjectives, and pronouns (Indefinite , possessive, demonstrative, relative and reflexive), if a token has; – Genitive case: replace with lemma+en (Ex. des, der, des, der → d+en) – Pl"
W10-1704,P07-2045,0,\N,Missing
W11-2135,W10-1704,1,0.806759,"the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores (Papineni et al., 2002). Thus all systems are built in “true-case.” As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which poses a number of difficulties both at training and decoding time. Thus, to translate from German to English, the German side was normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010)), which aims at reducing the lexical redundancy and splitting complex compounds. Using the same pre-processing scheme to translate from English to German would require to postprocess the output to undo the pre-processing. As in our last year’s experiments (Allauzen et al., 2010), this pre-processing step could be achieved with a two-step decoding. However, by stacking two decoding steps, we may stack errors as well. Thus, for this direction, we used the German tokenizer provided by the organizers. 3.2 contains large portions that are not useful for translating news text. The first filter aime"
W11-2135,J92-4003,0,0.317321,"Missing"
W11-2135,J04-2004,0,0.208795,"is estimated and tuned as described in Section 4.1. Moreover, we also introduce in Section 4.2 the use of the SOUL language model (LM) (Le et al., 2011) in SMT. Based on neural networks, the SOUL LM can handle an arbitrary large vocabulary and a high order markovian assumption (up to 10-gram in this work). Finally, experimental results are reported in Section 5 both in terms of BLEU scores and translation edit rates (TER) measured on the provided newstest2010 dataset. 2 System Overview Our in-house n-code SMT system implements the bilingual n-gram approach to Statistical Machine Translation (Casacuberta and Vidal, 2004). Given a 1 This kind of characters was used for Teletype up to the seventies or early eighties. 309 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 309–315, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics source sentence sJ1 , a translation hypothesis tˆ1I is defined as the sentence which maximizes a linear combination of feature functions: ( ) M tˆ1I = arg max t1I ∑ λm hm (sJ1 ,t1I ) (1) m=1 a word-aligned corpus (using MGIZA++2 with default settings) in such a way that a unique segmentation of the bilingual corpus is achi"
W11-2135,W08-0310,1,0.792506,"Missing"
W11-2135,D10-1044,0,0.0594607,"Missing"
W11-2135,D08-1076,0,0.0565273,"iew of these rather inconclusive experiments, we chose to stick to the classical MERT for the submitted results. Optimization Issues Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al., 2007), MERT is the most widely used algorithm for system optimization. However, standard MERT procedure is known to suffer from instability of results and very slow training cycle with approximate estimates of one decoding cycle for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a generalpurpose finite state automata framework (Sokolov and Yvon, 2011). The last two approaches, hereafter referred to as ZHN and BBN, replace the BLEU objective function, with the usual BLEU score on expected n-gram counts (Rosti et al., 2010) and with an expected BLEU score for normal n-gram counts (Zens et al., 2007), respectively. All expecta314 Conclusion In this paper, we described our submissions to WMT’11"
W11-2135,P03-1021,0,0.271147,"s (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase-based system: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT), see details in Section 5.4), using the provided newstest2009 data as development set. 2.1 Training Our translation model is estimated over a training corpus composed of tuple sequences using classical smoothing techniques. Tuples are extracted from 310 The resulting sequence of tuples (1) is further refined to avoid NULL words in the source side of the tuples (2). Once the whole bilingual training data is segmented into tuples, n-gram language model probabilities can be estimated. In this example, note that the English source words perfect and translations"
W11-2135,P02-1040,0,0.0853055,"n models. To train the target language models, we also used all provided data and monolingual corpora released by the LDC for French and English. Moreover, all parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994). For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008). 3.1 Tokenization 4 We took advantage of our in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores (Papineni et al., 2002). Thus all systems are built in “true-case.” As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which poses a number of difficulties both at training and decoding time. Thus, to translate from German to English, the German side was normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010)), which aims at reducing the lexical redundancy and splitting complex compounds. Using the same pre-processing scheme to translate from English to German would require t"
W11-2135,W10-1748,0,0.0323177,"for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a generalpurpose finite state automata framework (Sokolov and Yvon, 2011). The last two approaches, hereafter referred to as ZHN and BBN, replace the BLEU objective function, with the usual BLEU score on expected n-gram counts (Rosti et al., 2010) and with an expected BLEU score for normal n-gram counts (Zens et al., 2007), respectively. All expecta314 Conclusion In this paper, we described our submissions to WMT’11 in the French-English and GermanEnglish shared translation tasks, in both directions. For this year’s participation, we only used n-code, our open source Statistical Machine Translation system based on bilingual n-grams. Our contributions are threefold. First, we have shown that n-gram based systems can achieve state-of-the-art performance on large scale tasks in terms of automatic metrics such as BLEU. Then, as already sho"
W11-2135,C08-1098,0,0.0510396,"action and reordering rules. 3 Data Pre-processing and Selection We used all the available parallel data allowed in the constrained task to compute the word alignments, except for the French-English tasks where the United Nation corpus was not used to train our translation models. To train the target language models, we also used all provided data and monolingual corpora released by the LDC for French and English. Moreover, all parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994). For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008). 3.1 Tokenization 4 We took advantage of our in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores (Papineni et al., 2002). Thus all systems are built in “true-case.” As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which poses a number of difficulties both at training and decoding time. Thus, to translate from German to English, the G"
W11-2135,2011.eamt-1.33,1,0.771526,"system optimization. However, standard MERT procedure is known to suffer from instability of results and very slow training cycle with approximate estimates of one decoding cycle for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a generalpurpose finite state automata framework (Sokolov and Yvon, 2011). The last two approaches, hereafter referred to as ZHN and BBN, replace the BLEU objective function, with the usual BLEU score on expected n-gram counts (Rosti et al., 2010) and with an expected BLEU score for normal n-gram counts (Zens et al., 2007), respectively. All expecta314 Conclusion In this paper, we described our submissions to WMT’11 in the French-English and GermanEnglish shared translation tasks, in both directions. For this year’s participation, we only used n-code, our open source Statistical Machine Translation system based on bilingual n-grams. Our contributions are threefold."
W11-2135,N04-4026,0,0.357669,"estimated by using the n-gram assumption: K p(sJ1 ,t1I ) = ∏ p((s,t)k |(s,t)k−1 . . . (s,t)k−n+1 ) k=1 Figure 1: Tuple extraction from a sentence pair. where s refers to a source symbol (t for target) and (s,t)k to the kth tuple of the given bilingual sentence pair. It is worth noticing that, since both languages are linked up in tuples, the context information provided by this translation model is bilingual. In addition to the translation model, eleven feature functions are combined: a target-language model (see Section 4 for details); four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase-based system: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003) (Minimu"
W11-2135,D07-1080,0,0.0221974,"ever, showed any consistent and significant improvement for the majority of setups tried (with the exception of the BBN approach, that had almost always improved over n-best MERT, but for the sole French to English translation direction). Additional experiments with 9 complementary translation models as additional features were performed with lattice-MERT, but neither showed any substantial improvement. In the view of these rather inconclusive experiments, we chose to stick to the classical MERT for the submitted results. Optimization Issues Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al., 2007), MERT is the most widely used algorithm for system optimization. However, standard MERT procedure is known to suffer from instability of results and very slow training cycle with approximate estimates of one decoding cycle for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a gen"
W11-2135,D07-1055,0,0.0524218,"Missing"
W11-2142,J04-2004,0,0.0163665,"e a bilingual language model, an additional language model in the phrase-based system in which each token consist of a target word and all 360 source words it is aligned to. The bilingual tokens enter the translation process as an additional target factor. 2.3 LIMSI-CNRS Single System 2.3.1 System overview The LIMSI system is built with n-code2 , an open source statistical machine translation system based on bilingual n-grams. 2.3.2 n-code Overview In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a wordbonus model and a tuple-bonus model w"
W11-2142,J07-2003,0,0.0225833,"and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase"
W11-2142,W08-0310,1,0.899949,"Missing"
W11-2142,P07-1019,0,0.0206925,"or Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded ph"
W11-2142,P02-1040,0,0.102913,"Missing"
W11-2142,E03-1076,0,0.0231201,"l table includes two identical Jane systems which are optimized on different criteria. The one optimized on TER−BLEU yields a much lower TER. Final Systems We preprocess the training data prior to training the system, first by normalizing symbols such as quotes, dashes and apostrophes. Then smart-casing of the first words of each sentence is performed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogeneous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 For the German→English task, RWTH conducted experiments comparing the standard phrase extraction with the phrase training technique described in Section 2.1.3. Further experiments included the use of additional language model training data, reranking of n-best lists generated by the phrase-based system, and different optimization criteria. A considerable increase in translation quality can be achieved by application of German compound splitting (Koehn and Knight, 20"
W11-2142,W07-0732,1,0.818478,"a statistical post editing (SPE) component. The SYSTRAN system is traditionally classified as a rule-based system. However, over the decades, its development has always been driven by pragmatic considerations, progressively integrating many of the most efficient MT approaches and techniques. Nowadays, the baseline engine can be considered as a linguistic-oriented system making use of dependency analysis, general transfer rules as well as of large manually encoded dictionaries (100k − 800k entries per language pair). The basic setup of the SPE component is identical to the one described in (L. Dugast and Koehn, 2007). A statistical translation model is trained on the rule-based translation of the source and the target side of the parallel corpus. This is done separately for each parallel corpus. Language models are trained on each target half of the parallel corpora and also on additional in-domain corpora. Moreover, the following measures − limiting unwanted statistical effects − were applied: • Named entities are replaced by special tokens on both sides. This usually improves word alignment, since the vocabulary size is significantly reduced. In addition, entity translation is handled more reliably by t"
W11-2142,E06-1005,1,0.83205,"d 15M phrases from the news/europarl corpora, provided as training data for WMT 2011. Weights for these separate models were tuned by the MERT algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. A deeper description will be also given in the WMT11 system combination paper of RWTH Aachen University. For this task only the A2L framework has been used. 4 Experiments We tried different system combinations with different sets of single systems and different optimization criteria. As RWTH has two different translation systems, we pu"
W11-2142,W09-0435,1,0.836207,"ystem applies a bilingual language model to extend the context of source language words available for translation. The individual models are described briefly in the following. 2.2.3 POS-based Reordering Model We use a reordering model that is based on partsof-speech (POS) and learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. In addition to continuous reordering rules that model short-range reordering (Rottmann and Vogel, 2007), we apply noncontinuous rules to address long-range reorderings as typical for German-English translation (Niehues and Kolss, 2009). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. 2.2.4 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract also phrase pairs for originally discontinuous phrases and could apply them during translation of reordered test sentences. Therefore,"
W11-2142,J03-1002,1,0.00747756,"joint translation by combining the knowledge of the four project partners. Each group develop and maintain their own different machine translation system. These single systems differ not only in their general approach, but also in the preprocessing of training and test data. To take the advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. RWTH Aachen Single Systems For the WMT 2011 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned bilingual corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algor"
W11-2142,P03-1021,0,0.147772,"etups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tab"
W11-2142,P07-2045,0,0.0131162,"parallel corpus (whose target is identical to the source). This was added to the parallel text in order to improve word alignment. • Singleton phrase pairs are deleted from the phrase table to avoid overfitting. • Phrase pairs not containing the same number of entities on the source and the target side are also discarded. • Phrase pairs appearing less than 2 times were pruned. The SPE language model was trained 15M phrases from the news/europarl corpora, provided as training data for WMT 2011. Weights for these separate models were tuned by the MERT algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice accor"
W11-2142,2007.tmi-papers.21,0,0.0203552,"lattices. Part-of-speech tags are obtained using the TreeTag1 http://hunspell.sourceforge.net/ ger (Schmid, 1994). In addition, the system applies a bilingual language model to extend the context of source language words available for translation. The individual models are described briefly in the following. 2.2.3 POS-based Reordering Model We use a reordering model that is based on partsof-speech (POS) and learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. In addition to continuous reordering rules that model short-range reordering (Rottmann and Vogel, 2007), we apply noncontinuous rules to address long-range reorderings as typical for German-English translation (Niehues and Kolss, 2009). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. 2.2.4 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract als"
W11-2142,N04-4026,0,0.0225696,"ew In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a wordbonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003), using the news"
W11-2142,W05-0836,1,0.901096,"rd heuristic phrase extraction techniques, performing force alignment phrase training (FA) gives an improvement in BLEU on newstest2008 and newstest2009, but a degradation in TER. The addition of LDC Gigaword corpora (+GW) to the language model training data shows improvements in both BLEU and TER. Reranking was done on 1000-best lists generated by the the best available 359 Preprocessing System Overview The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation. Optimization with regard to the BLEU score is done using Minimum Error Rate Training as described by Venugopal et al. (2005). The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a GIZA++ Word Alignment. We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained on the Gigaword corpus. Reordering is performed based on continuous and non-continuous POS rules to cover short and long-range reorderings. The long-range reordering rules were also applied to the training corpus and phrase extraction was performed on the resulting reordering lattices. Part-of-speech tags are obtained using the TreeTag1 http://hunspell.sourceforge.net"
W11-2142,W10-1738,1,0.832324,"are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institut"
W11-2142,P10-1049,1,0.823271,"ons. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid and less memory consuming experiments with a better translation quality. 2.1.4 system (PBT (FA)+GW). Following models were applied: n-gram posteriors (Zens and Ney, 2006), sentence length model, a 6-gram LM and"
W11-2142,W06-3110,1,0.850765,"ase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid and less memory consuming experiments with a better translation quality. 2.1.4 system (PBT (FA)+GW). Following models were applied: n-gram posteriors (Zens and Ney, 2006), sentence length model, a 6-gram LM and IBM-1 lexicon models in both normal and inverse direction. These models are combined in a log-linear fashion and the scaling factors are tuned in the same manner as the baseline system (using TER−4BLEU on newstest2009). The final table includes two identical Jane systems which are optimized on different criteria. The one optimized on TER−BLEU yields a much lower TER. Final Systems We preprocess the training data prior to training the system, first by normalizing symbols such as quotes, dashes and apostrophes. Then smart-casing of the first words of each"
W11-2142,2008.iwslt-papers.8,1,0.820865,"preprocessing of training and test data. To take the advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. RWTH Aachen Single Systems For the WMT 2011 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned bilingual corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hie"
W11-2142,D08-1076,0,\N,Missing
W11-2168,P07-1020,0,0.013269,"a two step process, where a set of possible source reorderings, represented as a parse forest, are associated with possible target sentences, using, as we do, a finitestate translation model. This translation model is trained discriminatively by marginalizing out the (unobserved) reordering variables; inference can be performed effectively by intersecting the input parse forest with a transducer representing translation options. A third strategy is to consider a simpler class of derivation process, which only partly describe the mapping between f and e. This is, for instance, the approach of (Bangalore et al., 2007), where a simple bag-of-word representation of the target sentence is computed using a battery of boolean classifiers (one for each target word). In this approach, discriminative training is readily applicable, as the required supervision is overtly present in example source-target pairs (f , e); however, a complementary reshaping/reordering step is necessary to turn the bag-of-word into a full-fledged translation. This work was recently revisited in (Mauser et al., 2009), where a conditional model predicting the presence of each target phrase provides a supplementary score for the standard “l"
W11-2168,D08-1023,0,0.22415,"rmed using the Rprop algorithm4 (Riedmiller and Braun, 1993), which provides the memory efficiency needed to cope with the very large feature sets considered here. Training with a target language model One of the main strength of the phrase-based “log-linear” models is their ability to make use of powerful target side language models trained on very large amounts of monolingual texts. This ability is crucial to achieve good performance and has to be preserved no matter the difficulties that occur when one moves away from conventional phrase-based systems (Chiang, 2005; Huang and Chiang, 2007; Blunsom and Osborne, 2008; K¨aa¨ ri¨ainen, 2009). It thus seems appropriate to include a LM feature function in our model or alternatively to define: P (e e|˜f ) = 1 PLM (e e) exp(θT G(˜f , e e)), ˜ Z(f ; θ) where PLM Pis the target language model and e) exp(θT G(˜f , e e)). ImpleZ(˜f ; θ) = e e PLM (e menting this approach implies to deal with the lack of synchronization between the units of the translation models, which are variable-length (possibly empty) tuples, and the units of the language models, which are plain words. In practice, this extension is implemented by performing training and inference over a graph"
W11-2168,P08-1024,0,0.660098,"The model thus defines the probability of a segmented target e e = eeI1 given the segmented and reordered source sentence ˜f = fe1I . To complete the model, one just needs to define a distribution over source segmentations P (˜f |f ). Given the deterministic relationship between e and e e expressed by the “unsegmentation” function φ which maps e e with e = φ(e e), we then have: X P (e|f ) = P (e e, ˜f |f ) ˜ f ,e e|φ(e e)=e = X P (e e, |˜f , f )P (˜f |f ) ˜ f ,e e|φ(e e)=e = X P (e e, |˜f )P (˜f |f ) ˜ f ,e e|φ(e e)=e 2 Assuming first order dependencies. This is a significant difference with (Blunsom et al., 2008), as we do not need to introduce latent variables during training. 3 544 In practice, we will only consider a restricted number of possible segmentation/reorderings of the source, denoted L(f ), and compute the best translation e∗ as φ(e e∗ ), where: e e∗ = arg max P (e e|f ) e e e, |˜f , f )P (˜f |f ) ≈ arg max P (e (1) ˜ f ∈L(f ),e e Even with these simplifying assumptions, this approach raises several challenging computational problems. First, training a CRF is quadratic in the number of labels, of which we will have plenty (typically hundreds of thousands). A second issue is decoding: as w"
W11-2168,J90-2002,0,0.716657,". Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). It also enables to easily integrate linguistically informed (describing morphological or morpho-syntactical properties of phrases) and/or contextual features into the translation model. In return, in addition to having a better trained model, we also expect (i) to mak"
W11-2168,J04-2004,0,0.319641,"ity of the whole enterprise is achieved through an efficient implementation of the conditional random fields (CRFs) model using a weighted finite-state transducers library. This approach is experimentally contrasted with several conventional phrase-based systems. 1 To overcome the NP-hard problems that derive from the need to consider all possible permutations of the source sentence, we make here a radical simplification and consider training the translation model given a fixed segmentation and reordering. This idea is not new, and is one of the grounding principle of n-gram-based approaches (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) in SMT. The novelty here is that we will use this assumption to recast machine translation (MT) in the familiar terms of a sequence labeling task. Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and"
W11-2168,D08-1024,0,0.0537665,"Missing"
W11-2168,N09-1025,0,0.0413879,"use of a target language model in training and/or decoding. 5 Related work Discriminative learning approaches have proven successful for many NLP tasks, notably thanks to their ability to cope with flexible linguistic representations and to accommodate potentially redundant descriptions. This is especially appealing for machine translation, where the mapping between a source word or phrase and its target correlate(s) seems to involve an large array of factors, such as its morphology, its syntactic role, its meaning, its lexical context, etc. (see eg. (Och et al., 2004; Gimpel and Smith, 2008; Chiang et al., 2009), for inspiration regarding potentially useful features in SMT). Discriminative learning requires (i) a parameterized scoring function and (ii) a training objective. The scoring function is usually assumed to be linear and ranks candidate outputs y for input x according to θT G(x, y), where θ is the parameter vector. θ 549 and G deterministically imply the input/output mapping as x → arg maxy θT G(x, y). Given a set of training pairs {xi , y i , i = 1 . . . N }, parameters are learned by optimizing some regularized loss function of θ, so as to make the inferred input/output mapping faithfully"
W11-2168,P05-1033,0,0.130734,"extremely large. Optimization is performed using the Rprop algorithm4 (Riedmiller and Braun, 1993), which provides the memory efficiency needed to cope with the very large feature sets considered here. Training with a target language model One of the main strength of the phrase-based “log-linear” models is their ability to make use of powerful target side language models trained on very large amounts of monolingual texts. This ability is crucial to achieve good performance and has to be preserved no matter the difficulties that occur when one moves away from conventional phrase-based systems (Chiang, 2005; Huang and Chiang, 2007; Blunsom and Osborne, 2008; K¨aa¨ ri¨ainen, 2009). It thus seems appropriate to include a LM feature function in our model or alternatively to define: P (e e|˜f ) = 1 PLM (e e) exp(θT G(˜f , e e)), ˜ Z(f ; θ) where PLM Pis the target language model and e) exp(θT G(˜f , e e)). ImpleZ(˜f ; θ) = e e PLM (e menting this approach implies to deal with the lack of synchronization between the units of the translation models, which are variable-length (possibly empty) tuples, and the units of the language models, which are plain words. In practice, this extension is implemented"
W11-2168,P05-1066,0,0.0417081,"f tuples such as: (demanda, said ) or (de nouveau, again). p(ul |ui−1 . . . ui−n+1 ). i=1 The probability of a sentence pair (f , e) is then either recovered by marginalization, or approximated 543 la femme voil´ee e: the veiled dame ˜f : la voil´ee femme demanda de nouveau said again Figure 1: The tuple extraction process The original (top) and reordered (bottom) French sentence aligned with its translation. At test time, the source text is reordered so as to match the reordering implied by the disentanglement procedure. Various proposals has been made to perform such source side reordering (Collins et al., 2005; Xia and McCord, 2004), or even learning reordering rules based on syntactic or morphosyntactic information (Crego and Mari˜no, 2007). The latter approach amounts to accumulate reordering patterns during the training; test source sentences are then non-deterministically reordered in all possible ways yielding a word graph. This graph is then monotonously decoded, where the score of a translation hypothesis combines information from the translation models as well as from other information sources (lexicalized reordering model, target 1 Here, using the MGIZA++ package (Gao and Vogel, 2008). sid"
W11-2168,W02-1001,0,0.0556563,"word aligned sentences (f , e), but lack the explicit derivation h from f to e that is required to train the model in a fully supervised way. The approach of (Liang et al., 2006a) circumvents the issue by assuming that the hidden derivation h can be approximated through forced decoding. Assuming that h is in fact observed as the optimal (Viterbi) derivation h∗ from f to e given the current parameter value10 , it is straightforward to recast the training of a phrase-based system as a standard structured learning problem, thus amenable to training algorithms such as the averaged perceptron of (Collins, 2002). This approximation is however not genuine, and the choice of the most appropriate derivation seems to raises intriguing issues (Watanabe et al., 2007; Chiang et al., 2008). The authors of (Blunsom et al., 2008; Blunsom and Osborne, 2008) consider models for which it is computationally possible to marginalize out all possible derivations of a given translation. As demonstrated in these papers, this approach is tractable even when the derivation process is a based on synchronous context-free grammars, rather that finitestate devices. However, the computational cost as10 If one actually exists"
W11-2168,P07-1033,0,0.0599041,"Missing"
W11-2168,P08-2007,0,0.0160335,"is idea is not new, and is one of the grounding principle of n-gram-based approaches (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) in SMT. The novelty here is that we will use this assumption to recast machine translation (MT) in the familiar terms of a sequence labeling task. Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallu"
W11-2168,W06-3105,0,0.0286103,"sed approaches (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) in SMT. The novelty here is that we will use this assumption to recast machine translation (MT) in the familiar terms of a sequence labeling task. Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). It also enables to easily integrate linguistically inform"
W11-2168,N06-4004,0,0.0285908,"systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). It also enables to easily integrate linguistically informed (describing morphological or morpho-syntactical properties of phrases) and/or contextual features into the translation model. In return, in addition to having a better trained model, we also expect (i) to make estimation less sensible to data sparsity issues and ("
W11-2168,N10-1128,0,0.0385918,"o marginalize out all possible derivations of a given translation. As demonstrated in these papers, this approach is tractable even when the derivation process is a based on synchronous context-free grammars, rather that finitestate devices. However, the computational cost as10 If one actually exists in the model, thus raising the issue of reference reachability, see discussion in Section 3. sociated with training and inference remains very high, especially when using a target side language model, which seems to preclude the application to large-scale translation tasks11 . The recent work of (Dyer and Resnik, 2010) proceeds from a similar vein: translation is however modeled as a two step process, where a set of possible source reorderings, represented as a parse forest, are associated with possible target sentences, using, as we do, a finitestate translation model. This translation model is trained discriminatively by marginalizing out the (unobserved) reordering variables; inference can be performed effectively by intersecting the input parse forest with a transducer representing translation options. A third strategy is to consider a simpler class of derivation process, which only partly describe the"
W11-2168,P08-1112,0,0.059883,"Missing"
W11-2168,W08-0509,0,0.0149225,"ing (Collins et al., 2005; Xia and McCord, 2004), or even learning reordering rules based on syntactic or morphosyntactic information (Crego and Mari˜no, 2007). The latter approach amounts to accumulate reordering patterns during the training; test source sentences are then non-deterministically reordered in all possible ways yielding a word graph. This graph is then monotonously decoded, where the score of a translation hypothesis combines information from the translation models as well as from other information sources (lexicalized reordering model, target 1 Here, using the MGIZA++ package (Gao and Vogel, 2008). side language model (LM), word and phrase penalties, etc). 2.2 Translating with CRFs A discriminative version of the n-gram approach consists in modeling P (e|f ) instead of P (e, f ), which can be efficiently performed with CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). Assuming matched sequences of observations (x = L xL 1 ) and labels (y = y1 ), CRFs express the conditional probability of labels as: P (y1L |xL 1) = 1 L exp(θT G(xL 1 , y1 )), Z(xL 1 ; θ) where θ is a parameter vector and G denotes a vector of feature functions testing various properties of x and y. In the linear-"
W11-2168,W08-0302,0,0.0125396,"segmentations, and the use of a target language model in training and/or decoding. 5 Related work Discriminative learning approaches have proven successful for many NLP tasks, notably thanks to their ability to cope with flexible linguistic representations and to accommodate potentially redundant descriptions. This is especially appealing for machine translation, where the mapping between a source word or phrase and its target correlate(s) seems to involve an large array of factors, such as its morphology, its syntactic role, its meaning, its lexical context, etc. (see eg. (Och et al., 2004; Gimpel and Smith, 2008; Chiang et al., 2009), for inspiration regarding potentially useful features in SMT). Discriminative learning requires (i) a parameterized scoring function and (ii) a training objective. The scoring function is usually assumed to be linear and ranks candidate outputs y for input x according to θT G(x, y), where θ is the parameter vector. θ 549 and G deterministically imply the input/output mapping as x → arg maxy θT G(x, y). Given a set of training pairs {xi , y i , i = 1 . . . N }, parameters are learned by optimizing some regularized loss function of θ, so as to make the inferred input/outp"
W11-2168,P07-1019,0,0.034987,"e. Optimization is performed using the Rprop algorithm4 (Riedmiller and Braun, 1993), which provides the memory efficiency needed to cope with the very large feature sets considered here. Training with a target language model One of the main strength of the phrase-based “log-linear” models is their ability to make use of powerful target side language models trained on very large amounts of monolingual texts. This ability is crucial to achieve good performance and has to be preserved no matter the difficulties that occur when one moves away from conventional phrase-based systems (Chiang, 2005; Huang and Chiang, 2007; Blunsom and Osborne, 2008; K¨aa¨ ri¨ainen, 2009). It thus seems appropriate to include a LM feature function in our model or alternatively to define: P (e e|˜f ) = 1 PLM (e e) exp(θT G(˜f , e e)), ˜ Z(f ; θ) where PLM Pis the target language model and e) exp(θT G(˜f , e e)). ImpleZ(˜f ; θ) = e e PLM (e menting this approach implies to deal with the lack of synchronization between the units of the translation models, which are variable-length (possibly empty) tuples, and the units of the language models, which are plain words. In practice, this extension is implemented by performing training"
W11-2168,D09-1107,0,0.0313224,"Missing"
W11-2168,N03-1017,0,0.093172,"is that we will use this assumption to recast machine translation (MT) in the familiar terms of a sequence labeling task. Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). It also enables to easily integrate linguistically informed (describing morphological or morpho-syntactical properties of phrases) and/or contextual"
W11-2168,N04-1022,0,0.0760989,": it suffices to perform the search in π2 (S ◦R)◦ −log(D)◦T ◦F ◦L, where L represents a n-gram language model. When combining several models, notably a source segmentation model and/or a target language model for rescoring, we have made sure to rescale the (log)probabilities so as to balance the language model scores with the CRF scores, and to use a fixed word bonus to make hypotheses of different length more comparable. All these parameters are tuned as part of the decoder development process. It is finally noteworthy that, in our architecture, alternative decoding strategies, such as MBR (Kumar and Byrne, 2004) are also readily implemented. 4 Experiments 4.1 Corpora and metrics For these experiments, we have used a medium size training corpus, extracted from the datasets made available for WMT 20116 evaluation campaign, and have focused on one translation direction, from French to English7 . Translation model training uses the entire NewsCommentary subpart of the WMT’2011 training 6 7 statmt.org/wmt11 Results in the other direction suggest similar conclusions. le : the/θle,the ∗ : the/0 0 ∗ : the/θthe ∗ : the/0 0 1 0 ∗ : cat/θthe,cat 1 chat : cat/θchat,cat DET : the/θDET,the Figure 2: Feature matche"
W11-2168,P10-1052,1,0.823642,"th a very small number of different labels. A first simplification is thus to consider that the set of possible “labels” ee for a source sequence fe is limited to those that are seen in training: all the other associations (fe, ee) are deemed impossible, which amounts to setting the corresponding parameter value to −∞. A second speed-up is to enforce sparsity in the model, through the use of a `1 regularization term (Tibshirani, 1996): on the one hand, this greatly reduces the memory usage; furthermore, sparse models are also prone to various optimization of the forward-backward computations (Lavergne et al., 2010). As discussed in (Ng, 2004; Turian et al., 2007), this feature selection strategy is well suited to the task at hand, where the number of possible features is extremely large. Optimization is performed using the Rprop algorithm4 (Riedmiller and Braun, 1993), which provides the memory efficiency needed to cope with the very large feature sets considered here. Training with a target language model One of the main strength of the phrase-based “log-linear” models is their ability to make use of powerful target side language models trained on very large amounts of monolingual texts. This ability i"
W11-2168,P06-1096,0,0.128,"Missing"
W11-2168,N06-1014,0,0.677324,"ing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). It also enables to easily integrate linguistically informed (describing morphological or morpho-syntactical properties of phrases) and/or contextual features into the translation model. In return, in addition to having a better trained model, we also expect (i) to make estimation less sensible to dat"
W11-2168,W02-1018,0,0.0546135,"on and consider training the translation model given a fixed segmentation and reordering. This idea is not new, and is one of the grounding principle of n-gram-based approaches (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) in SMT. The novelty here is that we will use this assumption to recast machine translation (MT) in the familiar terms of a sequence labeling task. Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools t"
W11-2168,J06-4004,1,0.936184,"Missing"
W11-2168,D09-1022,0,0.0228507,"s of derivation process, which only partly describe the mapping between f and e. This is, for instance, the approach of (Bangalore et al., 2007), where a simple bag-of-word representation of the target sentence is computed using a battery of boolean classifiers (one for each target word). In this approach, discriminative training is readily applicable, as the required supervision is overtly present in example source-target pairs (f , e); however, a complementary reshaping/reordering step is necessary to turn the bag-of-word into a full-fledged translation. This work was recently revisited in (Mauser et al., 2009), where a conditional model predicting the presence of each target phrase provides a supplementary score for the standard “log-linear” model. This line of research has been continued notably in (K¨aa¨ ri¨ainen, 2009), which introduces an exponential model of bag of phrases (allowing some overlap), that enables to capture localized dependencies between target words, while preserving (to some extend) the efficiency of training and inference. Supervision is here indirectly provided by word alignment and correlated phrase extraction processes implemented in conventional phrase-based systems (Koehn"
W11-2168,P02-1040,0,0.0865962,"ng. Various statistics regarding these corpora are reproduced on Table 1. All the training corpora were aligned using MGIZA++ with standard parameters8 , and processed in the standard tuple extraction pipeline. The development and test corpora were also processed analogously. For the sake of comparison, we also trained a standard n-gram-based and a Moses system (Koehn et al., 2007) with default parameters and a 3-gram target LM trained using only the target side of our parallel corpus. The development set (test 2009) was used to tune these two systems. All performance are measured using BLEU (Papineni et al., 2002). 8 As part of a much larger batch of texts. 4.2 Features The baseline system is composed only of translation features [trs] and target bigram features [t2g]. The former correspond to functions of the form e, i) = I(fei = s ∧ eei = t), where s gus,t (˜f , e and t respectively denote source and target phrases and I() is the indicator function. These are also generalized to part-of-speech and also to any possible source phrase, giving rise to features such as e, i) = I(e ei = t). Target bigram features gu∗,t = (˜f , e e, i) = correspond to functions of the form gbt,t0 (˜f , e I(e ei−1 = t ∧ eei"
W11-2168,N04-4026,0,0.119029,"Missing"
W11-2168,D07-1080,0,0.0463638,"he approach of (Liang et al., 2006a) circumvents the issue by assuming that the hidden derivation h can be approximated through forced decoding. Assuming that h is in fact observed as the optimal (Viterbi) derivation h∗ from f to e given the current parameter value10 , it is straightforward to recast the training of a phrase-based system as a standard structured learning problem, thus amenable to training algorithms such as the averaged perceptron of (Collins, 2002). This approximation is however not genuine, and the choice of the most appropriate derivation seems to raises intriguing issues (Watanabe et al., 2007; Chiang et al., 2008). The authors of (Blunsom et al., 2008; Blunsom and Osborne, 2008) consider models for which it is computationally possible to marginalize out all possible derivations of a given translation. As demonstrated in these papers, this approach is tractable even when the derivation process is a based on synchronous context-free grammars, rather that finitestate devices. However, the computational cost as10 If one actually exists in the model, thus raising the issue of reference reachability, see discussion in Section 3. sociated with training and inference remains very high, es"
W11-2168,P10-1049,0,0.099124,"Missing"
W11-2168,C04-1073,0,0.0361538,"anda, said ) or (de nouveau, again). p(ul |ui−1 . . . ui−n+1 ). i=1 The probability of a sentence pair (f , e) is then either recovered by marginalization, or approximated 543 la femme voil´ee e: the veiled dame ˜f : la voil´ee femme demanda de nouveau said again Figure 1: The tuple extraction process The original (top) and reordered (bottom) French sentence aligned with its translation. At test time, the source text is reordered so as to match the reordering implied by the disentanglement procedure. Various proposals has been made to perform such source side reordering (Collins et al., 2005; Xia and McCord, 2004), or even learning reordering rules based on syntactic or morphosyntactic information (Crego and Mari˜no, 2007). The latter approach amounts to accumulate reordering patterns during the training; test source sentences are then non-deterministically reordered in all possible ways yielding a word graph. This graph is then monotonously decoded, where the score of a translation hypothesis combines information from the translation models as well as from other information sources (lexicalized reordering model, target 1 Here, using the MGIZA++ package (Gao and Vogel, 2008). side language model (LM),"
W11-2168,P07-2045,0,\N,Missing
W11-2168,2009.eamt-smart.4,0,\N,Missing
W11-2168,N04-1021,0,\N,Missing
W12-3140,J04-2004,0,0.0800912,"nslation model relies on a specific decomposition of the joint probability of a sentence pair P(s, t) using the n-gram assumption: a sentence pair is decomposed into a sequence of bilingual units called tuples, defining a joint segmentation of the source and target. In the approach of (Mari˜no et al., 2006), this segmentation is a by-product of source reordering which ultimately derives from initial word and phrase alignments. 2.3.1 An Overview of n-code The baseline translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a wordbonus model and a tuple-bonus model"
W12-3140,J07-2003,0,0.0283293,"322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting"
W12-3140,W08-0310,1,0.935329,"Missing"
W12-3140,2010.iwslt-papers.6,0,0.0806631,"as last year5 and thus the same target language model as detailed in (Allauzen et al., 2011). For English, we took advantage of our in-house text processing tools for tokenization and detokenization steps (D´echelotte et al., 2008) and our system was built in ”true-case”. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which is detrimental both at training and decoding time. Thus, the German side was normalized using a specific pre-processing scheme (Allauzen et al., 2010; Durgar El-Kahlout and Yvon, 2010), which notably aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. 2.4 SYSTRAN Software, Inc. Single System The data submitted by SYSTRAN were obtained by a system composed of the standard SYSTRAN MT engine in combination with a statistical post editing (SPE) component. 4 http://geek.kyloo.net/software 5 The fifth edition of the English Gigaword (LDC2011T07) was not used. 325 The SYSTRAN system is traditionally classified as a rule-based system. However, over the decades, its development has alwa"
W12-3140,P07-1019,0,0.0343944,"on criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The"
W12-3140,E03-1076,0,0.55884,"ranslation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The language model is trained on the parallel data as well as the provided News crawl, the 109 French-English, UN and LDC Gigaword Fourth Edition corpora. For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). 2.2 2.2.1 Karlsruhe Institute of Technology Single Syst"
W12-3140,W07-0732,1,0.793396,"rule-based system. However, over the decades, its development has always been driven by pragmatic considerations, progressively integrating many of the most efficient MT approaches and techniques. Nowadays, the baseline engine can be considered as a linguistic-oriented system making use of dependency analysis, general transfer rules as well as of large manually encoded dictionaries (100k 800k entries per language pair). The SYSTRAN phrase-based SPE component views the output of the rule-based system as the source language, and the (human) reference translation as the target language, see (L. Dugast and Koehn, 2007). It performs corrections and adaptions learned from the 5-gram language model trained on the parallel target-to-target corpus. Moreover, the following measures - limiting unwanted statistical effects - were applied: • Named entities, time and numeric expressions are replaced by special tokens on both sides. This usually improves word alignment, since the vocabulary size is significantly reduced. In addition, entity translation is handled more reliably by the rule-based engine. • The intersection of both vocabularies (i.e. vocabularies of the rule-based output and the reference translation) is"
W12-3140,N12-1005,1,0.858653,"bilingual pairs, which means that the underlying vocabulary can be quite large. Unfortunately, the parallel data available to train these models are typically smaller than the corresponding monolingual corpora used to train target language models. It is very likely then, that such models should face severe estimation problems. In such setting, using neural network language 3 Part-of-speech labels for English and German are computed using the TreeTagger (Schmid, 1995). 2 http://ncode.limsi.fr/ 324 model techniques seem all the more appropriate. For this study, we follow the recommendations of Le et al. (2012), who propose to factor the joint probability of a sentence pair by decomposing tuples in two (source and target) parts, and further each part in words. This yields a word factored translation model that can be estimated in a continuous space using the SOUL architecture (Le et al., 2011). The design and integration of a SOUL model for large SMT tasks is far from easy, given the computational cost of computing n-gram probabilities. The solution used here was to resort to a two pass approach: the first pass uses a conventional back-off n-gram model to produce a k-best list; in the second pass, t"
W12-3140,J06-4004,1,0.843277,"Missing"
W12-3140,E06-1005,1,0.849031,"glish LDC Gigaword corpus using KneserNey (Kneser and Ney, 1995) smoothing was added. Weights for these separate models were tuned by the Mert algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. 4 Experiments This year, we tried different sets of single systems for system combination. As RWTH has two different translation systems, we put the output of both systems into system combination. Although both systems have the same preprocessing and language model, their hypotheses differ because of their different decoding approach."
W12-3140,D09-1022,1,0.869178,"done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using t"
W12-3140,2011.iwslt-evaluation.9,1,0.871665,"ocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We us"
W12-3140,P10-2041,0,0.0181873,"ound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The language model is trained on the parallel data as well as the provided News crawl, the 109 French-English, UN and LDC Gigaword Fourth Edition corpora. For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). 2.2 2.2.1 Karlsruhe Institute of Technology Single System quotes, dashes and apostrophes. Then smart-casing of the first words of each sentence is performed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogenous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003b). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-bas"
W12-3140,W09-0435,1,0.860476,"Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Furthermore, we use a 5gram cluster-based language model trained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as i"
W12-3140,W08-0303,1,0.84967,"very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters o"
W12-3140,2011.iwslt-papers.6,1,0.847434,"he Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Further"
W12-3140,W11-2124,1,0.868397,"length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known"
W12-3140,J03-1002,1,0.00977827,"RO partner trained their systems on the parallel Europarl and News Commentary corpora. All single systems were tuned on the newstest2009 or newstest2010 development set. The newstest2011 dev set was used to train the system combination parameters. Finally, the newstest2008-newstest2010 dev sets were used to compare the results of the different system combination settings. In this Section all four different system engines are presented. 2.1 RWTH Aachen Single Systems For the WMT 2012 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram langu"
W12-3140,P03-1021,0,0.230828,"n (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out"
W12-3140,P07-2045,0,0.00885526,"to improve word alignment. • Singleton phrase pairs are deleted from the phrase table to avoid overfitting. • Phrase pairs not containing the same number of entities on the source and the target side are also discarded. The SPE language model was trained on 2M bilingual phrases from the news/Europarl corpora, provided as training data for WMT 2012. An additional language model built from 15M phrases of the English LDC Gigaword corpus using KneserNey (Kneser and Ney, 1995) smoothing was added. Weights for these separate models were tuned by the Mert algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice accor"
W12-3140,W08-1006,0,0.100843,"ained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. For the test sentences, the reordering based on parts-of-speech and trees allows us to change the word order in the source sentence so that the sentence can be translated more easily. In addition, we build reordering lattices for all trainin"
W12-3140,2007.tmi-papers.21,0,0.266844,"the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Furthermore, we use a 5gram cluster-based language model trained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are"
W12-3140,N04-4026,0,0.0510078,"of n-code The baseline translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a wordbonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003), using the n"
W12-3140,W05-0836,1,0.92366,"rmed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogenous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003b). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system a"
W12-3140,W10-1738,1,0.875756,"by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 P"
W12-3140,2008.iwslt-papers.8,1,0.841876,"parameters. Finally, the newstest2008-newstest2010 dev sets were used to compare the results of the different system combination settings. In this Section all four different system engines are presented. 2.1 RWTH Aachen Single Systems For the WMT 2012 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2."
W12-3140,W11-2135,1,\N,Missing
W12-3140,W10-1704,1,\N,Missing
W12-3140,D08-1076,0,\N,Missing
W17-4722,D15-1166,0,0.0436414,"rent hidden state hi and a context vector ci that aims at capturing relevant source-side information. Neural MT System Neural machine translation (NMT) is a new methodology for machine translation that has led to remarkable improvements, particularly in terms of human evaluation, compared to rule-based and statistical machine translation (SMT) systems (Crego et al., 2016; Wu et al., 2016). NMT has now become a widely-applied technique for machine translation, as well as an effective approach 1 Figure 2 illustrates the attention layer. It implements the ""general"" attentional architecture from (Luong et al., 2015). The idea of a global attentional model is to consider all the hidden states of the encoder when deriving the context vector ct . Hence, global alignment weights at are derived by http://opennmt.net 265 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 265–270 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics Figure 1: Schematic view of our MT network. comparing the current target hidden state ht with each source hidden state hs : at (s) = P be stacked. More details about our system can be found in (Crego e"
W17-4722,P10-2041,0,0.021134,"at training work described in Table 3 is built as continuation of the model at epoch 13 on Table 2. Table 3 shows also BLEU scores over newstest2017 for the best performing network. As for the experiments detailed in Table 2, once all splits of the synthetic corpus were used to train our models with learning rate always set to 1.0 (5 epochs for German→English and 8 epochs for English→German), we began a decay mode. In this case, we decided to reduce the amount of training examples from 9 to 5 millions due to time restrictions. To select the training data we employed the algorithm detailed in (Moore and Lewis, 2010). It aims at identifying sentences in a generic corpus that are closer to domainsuch replacement is called a merge, and the number of merges is a tuneable parameter. Encodings were computed over the union of both German and English training corpora after preprocessing, aiming at improving consistency between source and target segmentations. Finally, case information was considered by the network as an additional feature. It allowed us to work with a lowercased vocabulary and treat recasing as a separate problem (Crego et al., 2016). 3.2 Training Details All experiments employ the NMT system de"
W17-4722,P16-1009,0,0.116247,"ith standard tokenisation tools. German words were further preprocessed to split compounds, following a similar algorithm as the built-in for Moses. Additional monolingual data was also used for both German and English available for the shared task: News Crawl: articles from 2016. Basic statistics of the tokenised data are available in Table 1. We used a byte pair encoding technique2 (BPE) to segment word forms and achieve openvocabulary translation with a fixed vocabulary of 30, 000 source and target tokens. BPE was originally devised as a compression algorithm, adapted to word segmentation (Sennrich et al., 2016b). It recursively replaces frequent consecutive bytes with a symbol that does not occur elsewhere. Each Figure 2: Attention layer of the MT network. Note that for the sake of simplicity figure 1 illustrates a two-layers LSTM encoder/decoder while any arbitrary number of LSTM layers can 2 https://github.com/rsennrich/ subword-nmt 266 #sents Parallel En 4.6M De 4.6M Monolingual En 20,6M De 34,7M #words vocab. Lmean 103.7M 104.5M 627k 836k 22.6 22.8 463,6M 620,8M 1.18M 3.36M 22.5 17.8 using multi-bleu.perl3 . Training time per epoch is also shown in row Time measured in number of hours. As expec"
W17-4722,P16-1162,0,0.319344,"ith standard tokenisation tools. German words were further preprocessed to split compounds, following a similar algorithm as the built-in for Moses. Additional monolingual data was also used for both German and English available for the shared task: News Crawl: articles from 2016. Basic statistics of the tokenised data are available in Table 1. We used a byte pair encoding technique2 (BPE) to segment word forms and achieve openvocabulary translation with a fixed vocabulary of 30, 000 source and target tokens. BPE was originally devised as a compression algorithm, adapted to word segmentation (Sennrich et al., 2016b). It recursively replaces frequent consecutive bytes with a symbol that does not occur elsewhere. Each Figure 2: Attention layer of the MT network. Note that for the sake of simplicity figure 1 illustrates a two-layers LSTM encoder/decoder while any arbitrary number of LSTM layers can 2 https://github.com/rsennrich/ subword-nmt 266 #sents Parallel En 4.6M De 4.6M Monolingual En 20,6M De 34,7M #words vocab. Lmean 103.7M 104.5M 627k 836k 22.6 22.8 463,6M 620,8M 1.18M 3.36M 22.5 17.8 using multi-bleu.perl3 . Training time per epoch is also shown in row Time measured in number of hours. As expec"
W17-4722,P17-4012,1,\N,Missing
W18-2715,P17-2091,0,0.201752,"print, we applied basic optimization techniques to reduce the final size of our models. Our strategy for the shared task was to take advantage of four main optimization techniques: (a) sequence-level distillation, in particular cross-class distillation from a transformer model (Vaswani et al., 2017) to an RNN, (b) architecture search, changing the structure of the network by increasing the size of the most efficient modules, reducing the size of the most costly modules and replacing default gated units, (c) specialized precomputation such as reducing dynamically the runtime target vocabulary (Shi and Knight, 2017), and (d) quantization and faster matrix operations, based on the work of Devlin (2017) and gemmlowp2 . All of these methods are employed in a special-purpose C++-based decoder CTranslate3 . The complete training workflow including data preparation and distillation is described in Section 2. Inference techniques and quantization are described in Section 3. Our experiments compare the different approaches in terms of speed and accuracy. A meta Introduction As neural machine translation becomes more widely deployed in production environments, it becomes also increasingly important to serve trans"
W18-2715,W18-2701,0,0.0219747,"a preparation and distillation is described in Section 2. Inference techniques and quantization are described in Section 3. Our experiments compare the different approaches in terms of speed and accuracy. A meta Introduction As neural machine translation becomes more widely deployed in production environments, it becomes also increasingly important to serve translations models in a way as fast and as memory-efficient as possible, both on dedicated GPU and on standard CPU hardwares. The WNMT 2018 shared task1 focused on comparing different systems on both accuracy and computational efficiency (Birch et al., 2018). This paper describes the entry for the OpenNMT system to this competition. Our specific interest was to explore the different techniques for training and optimizing CPU models for very high throughput while preserving highest possible accuracy compared to state-of-the-art. While we did not put real focus on memory and docker size foot2 1 https://sites.google.com/site/wnmt18/ shared-task 3 https://github.com/google/gemmlowp https://github.com/OpenNMT/CTranslate 122 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 122–128 c Melbourne, Australia, July 20, 2018"
W18-2715,D17-1300,0,0.0327723,"ategy for the shared task was to take advantage of four main optimization techniques: (a) sequence-level distillation, in particular cross-class distillation from a transformer model (Vaswani et al., 2017) to an RNN, (b) architecture search, changing the structure of the network by increasing the size of the most efficient modules, reducing the size of the most costly modules and replacing default gated units, (c) specialized precomputation such as reducing dynamically the runtime target vocabulary (Shi and Knight, 2017), and (d) quantization and faster matrix operations, based on the work of Devlin (2017) and gemmlowp2 . All of these methods are employed in a special-purpose C++-based decoder CTranslate3 . The complete training workflow including data preparation and distillation is described in Section 2. Inference techniques and quantization are described in Section 3. Our experiments compare the different approaches in terms of speed and accuracy. A meta Introduction As neural machine translation becomes more widely deployed in production environments, it becomes also increasingly important to serve translations models in a way as fast and as memory-efficient as possible, both on dedicated"
W18-2715,D16-1139,1,0.873957,"uccessful for reducing the size of neural models. We considered the transformer network as our teacher network. We used OpenNMT-tf 6 to train two transformer based systems: base and large described in Table 2 with their evaluation in Table 3. For both, the learning rate is set to 2.0 and warmup steps 8000, we average the last 8 checkpoints to get the final model. Our baseline system outperforms the provided baseline Sockeye model by +0.37 BLEU on newstest2014. 2.2 Distillation to RNN To train our smaller student system, we follow the sequence-level knowledge distillation approach described by Kim and Rush (2016). First, we build the full transformer as above. Next, we use the teacher system to retranslate all the training source sentences to generate a set of simplified target sentences. Then, we use this simplified corpus (original source and newly generated target) to train a student system. The student system can be assigned with smaller network size, in our case a RNN-based sequence-to-sequence model similar to Bahdanau et al. (2014) Results from Crego and Senellart (2016) show that the distillation process not only improves the throughput of the student models and reduce their size, but can also"
W18-2715,P17-4012,1,0.866484,"Missing"
W18-2715,P07-2045,0,0.0250883,"Missing"
W18-2715,D15-1166,0,0.146064,"Missing"
W18-2715,N18-2074,0,0.0351052,"d Senellart (2016) who reported that student models could outperform their teacher for reference RNN-based model. (b) We compare quantitatively different quantizations, and (c) we give an improved algorithm to dynamically select target vocabulary for a given batch. Finally, we also report several complementary experiments that resulted in systems inside of the pareto convex border. For instance, we compare using 8-bit quantization to 16-bit quantization. 2 Teacher Model: Transformer Transformer networks (Vaswani et al., 2017) are the current state-of-the art in many machine translation tasks (Shaw et al., 2018). The network directly models the representations of each sentence with a self-attention mechanism. Hence much longer term dependencies than with standard sequence-to-sequence models can be learned, which is especially important for language pairs like English-German. In addition, transformer allows to easily parallelise the MLE training process across multiple GPUs. However, a large number of parameters are needed by the network to obtain its best performance. In order to reduce the model size, we applied knowledge distillation, a technique that has proven successful for reducing the size of"
W18-3914,W16-4819,0,0.165364,"combination of filters 3*128, 4*128, 5*128 (convention adopted throughout the whole article: filter size*number of filters) inspired from (Kim, 2014) which gave a macro F1 score of 0.433. They also reported in their paper a Long-Short Term Memory network (LSTM) with pre-compiled word embeddings giving as F1 score 0.423 after bug correction (Guggilla, 2016). The team mitsls used another character-level CNN architecture based on (Kim et al., 2016) using filters of increasing size 1*50, 2*50, 3*100, 4*100, 5*100, 6*100, 7*100 which gave a better F1 score 0.483 and ranked 2nd in the competition (Belinkov and Glass, 2016). Character-level CNNs present the advantage to be fast and able to learn local representations, which can be likened to char n-grams features successfully used by SVMs. One reason advanced by (Sadat et al., 2014) for the efficiency of character-level representations for Arabic Dialect Identification in speech transcription is that a great part of the variation between Arabic dialects is based on their affixes. However, as noticed by the organisers, Arabic speakers distinguish Arabic dialects not only according to words but also on the basis of speech cues absent from written transcripts. So f"
W18-3914,W16-4802,0,0.0233465,"s work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence details: 128 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 128–136 Santa Fe, New Mexico, USA, August 20, 2018. scores ranging from 0.495 to 0.513. Three teams reported experiments with neural network architectures that were finally not submitted as their models following other machine learning methods obtained higher accuracy scores, QCRI (Eldesouki et al., 2016), GW LT3 (Zirikly et al., 2016) and tufbasfs (C¸o¨ ltekin and Rama, 2016). Two teams submitted systems training neural networks: the team cgli competed using a character-level Convolutional Neural Network (CNN) with the combination of filters 3*128, 4*128, 5*128 (convention adopted throughout the whole article: filter size*number of filters) inspired from (Kim, 2014) which gave a macro F1 score of 0.433. They also reported in their paper a Long-Short Term Memory network (LSTM) with pre-compiled word embeddings giving as F1 score 0.423 after bug correction (Guggilla, 2016). The team mitsls used another character-level CNN architecture based on (Kim et al., 2016) usi"
W18-3914,W16-4828,0,0.0137054,"scribed in (Ali et al., 2014). The best performing systems obtained F1 This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence details: 128 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 128–136 Santa Fe, New Mexico, USA, August 20, 2018. scores ranging from 0.495 to 0.513. Three teams reported experiments with neural network architectures that were finally not submitted as their models following other machine learning methods obtained higher accuracy scores, QCRI (Eldesouki et al., 2016), GW LT3 (Zirikly et al., 2016) and tufbasfs (C¸o¨ ltekin and Rama, 2016). Two teams submitted systems training neural networks: the team cgli competed using a character-level Convolutional Neural Network (CNN) with the combination of filters 3*128, 4*128, 5*128 (convention adopted throughout the whole article: filter size*number of filters) inspired from (Kim, 2014) which gave a macro F1 score of 0.433. They also reported in their paper a Long-Short Term Memory network (LSTM) with pre-compiled word embeddings giving as F1 score 0.423 after bug correction (Guggilla, 2016). The team mitsls used"
W18-3914,W16-4824,0,0.0164015,"scores, QCRI (Eldesouki et al., 2016), GW LT3 (Zirikly et al., 2016) and tufbasfs (C¸o¨ ltekin and Rama, 2016). Two teams submitted systems training neural networks: the team cgli competed using a character-level Convolutional Neural Network (CNN) with the combination of filters 3*128, 4*128, 5*128 (convention adopted throughout the whole article: filter size*number of filters) inspired from (Kim, 2014) which gave a macro F1 score of 0.433. They also reported in their paper a Long-Short Term Memory network (LSTM) with pre-compiled word embeddings giving as F1 score 0.423 after bug correction (Guggilla, 2016). The team mitsls used another character-level CNN architecture based on (Kim et al., 2016) using filters of increasing size 1*50, 2*50, 3*100, 4*100, 5*100, 6*100, 7*100 which gave a better F1 score 0.483 and ranked 2nd in the competition (Belinkov and Glass, 2016). Character-level CNNs present the advantage to be fast and able to learn local representations, which can be likened to char n-grams features successfully used by SVMs. One reason advanced by (Sadat et al., 2014) for the efficiency of character-level representations for Arabic Dialect Identification in speech transcription is that"
W18-3914,D14-1181,0,0.00370822,"g from 0.495 to 0.513. Three teams reported experiments with neural network architectures that were finally not submitted as their models following other machine learning methods obtained higher accuracy scores, QCRI (Eldesouki et al., 2016), GW LT3 (Zirikly et al., 2016) and tufbasfs (C¸o¨ ltekin and Rama, 2016). Two teams submitted systems training neural networks: the team cgli competed using a character-level Convolutional Neural Network (CNN) with the combination of filters 3*128, 4*128, 5*128 (convention adopted throughout the whole article: filter size*number of filters) inspired from (Kim, 2014) which gave a macro F1 score of 0.433. They also reported in their paper a Long-Short Term Memory network (LSTM) with pre-compiled word embeddings giving as F1 score 0.423 after bug correction (Guggilla, 2016). The team mitsls used another character-level CNN architecture based on (Kim et al., 2016) using filters of increasing size 1*50, 2*50, 3*100, 4*100, 5*100, 6*100, 7*100 which gave a better F1 score 0.483 and ranked 2nd in the competition (Belinkov and Glass, 2016). Character-level CNNs present the advantage to be fast and able to learn local representations, which can be likened to char"
W18-3914,W16-4801,0,0.176055,"Missing"
W18-3914,W14-5904,0,0.0722905,"per a Long-Short Term Memory network (LSTM) with pre-compiled word embeddings giving as F1 score 0.423 after bug correction (Guggilla, 2016). The team mitsls used another character-level CNN architecture based on (Kim et al., 2016) using filters of increasing size 1*50, 2*50, 3*100, 4*100, 5*100, 6*100, 7*100 which gave a better F1 score 0.483 and ranked 2nd in the competition (Belinkov and Glass, 2016). Character-level CNNs present the advantage to be fast and able to learn local representations, which can be likened to char n-grams features successfully used by SVMs. One reason advanced by (Sadat et al., 2014) for the efficiency of character-level representations for Arabic Dialect Identification in speech transcription is that a great part of the variation between Arabic dialects is based on their affixes. However, as noticed by the organisers, Arabic speakers distinguish Arabic dialects not only according to words but also on the basis of speech cues absent from written transcripts. So for the DSL shared task 2017 (Zampieri et al., 2017), the ADI dataset contained twice more utterances than in 2016 and not only speech transcriptions but also acoustic features corresponding to sentences, namely i-"
W18-3914,J14-1006,0,0.0681215,"Missing"
W18-3914,W17-1201,0,0.167572,"Missing"
W18-3914,N16-1178,0,0.0236066,"challenge, but also end-to-end systems directly working on acoustic representation of speech data. 3.2.1 SVM In order to compare traditional machine learning and neural network approaches, we trained a multi-class Support Vector Machine (SVM) classifier using a radial basis function. We used the freely available LIBSVM1 software (Chang and Lin, 2011). 3.2.2 Multi-Input CNN (run 1) Our search for a simple and fast architecture to independently learn input embeddings of different type and combine them oriented us towards the Multi Group Convolutional Neural Network, also called MultiInput CNN (Zhang et al., 2016). Initially designed to join different word embeddings, these models allow the input embeddings to come from various sources and not to share the same dimensionality. Therefore, our first run is a Multi-Input CNN that we tailored to take as input the lexical, phonetic and acoustic data proposed for the challenge: it independently learns char embeddings and 4 phone embeddings by running convolutions with various filter sizes, respectively char-level convolutions on word transcripts and phone-level convolutions on phone transcripts. Then it concatenates the 5 resulting embeddings and the given a"
W18-3914,W16-4804,0,0.0143195,"e best performing systems obtained F1 This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence details: 128 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 128–136 Santa Fe, New Mexico, USA, August 20, 2018. scores ranging from 0.495 to 0.513. Three teams reported experiments with neural network architectures that were finally not submitted as their models following other machine learning methods obtained higher accuracy scores, QCRI (Eldesouki et al., 2016), GW LT3 (Zirikly et al., 2016) and tufbasfs (C¸o¨ ltekin and Rama, 2016). Two teams submitted systems training neural networks: the team cgli competed using a character-level Convolutional Neural Network (CNN) with the combination of filters 3*128, 4*128, 5*128 (convention adopted throughout the whole article: filter size*number of filters) inspired from (Kim, 2014) which gave a macro F1 score of 0.433. They also reported in their paper a Long-Short Term Memory network (LSTM) with pre-compiled word embeddings giving as F1 score 0.423 after bug correction (Guggilla, 2016). The team mitsls used another character-level CNN ar"
W18-6485,P18-4020,0,0.0697091,"Missing"
W18-6485,P07-2045,0,0.0111924,"esearch/MUSE 6 https://github.com/clab/fast align 7 https://github.com/TALP-UPC/FreeLing.git 5 http://opennmt.net 936 Figure 2: BLEU score of the best submission of each participant measured for the neural MT system trained with 100M tokens. Score is averaged over the six blind test sets. the sentence pairs on the 1 billion word GermanEnglish Paracrawl corpus. Scores do not have to be meaningful, except that higher scores indicate better quality. The performance of the submissions is evaluated by sub-sampling 10 million and 100 million word corpora based on these scores, training statistical (Koehn et al., 2007) and neural (Junczys-Dowmunt et al., 2018) MT systems with these corpora, and assessing translation quality on six blind test sets8 using the BLEU (Papineni et al., 2002) score. Figure 2 displays the score of the best submission of each individual participant corresponding to the 100 million tokens corpus using the neural MT system. BLEU score is averaged over the six blind test sets. As it can be seen, very similar results were obtained by most of the participants. Accuracy results fall within a margin of 3 points BLEU for the first 16 classified. 6 in our objective as we built a very simple"
W18-6485,W16-2207,0,0.0375384,"Missing"
W18-6485,P02-1040,0,0.109343,"ch participant measured for the neural MT system trained with 100M tokens. Score is averaged over the six blind test sets. the sentence pairs on the 1 billion word GermanEnglish Paracrawl corpus. Scores do not have to be meaningful, except that higher scores indicate better quality. The performance of the submissions is evaluated by sub-sampling 10 million and 100 million word corpora based on these scores, training statistical (Koehn et al., 2007) and neural (Junczys-Dowmunt et al., 2018) MT systems with these corpora, and assessing translation quality on six blind test sets8 using the BLEU (Papineni et al., 2002) score. Figure 2 displays the score of the best submission of each individual participant corresponding to the 100 million tokens corpus using the neural MT system. BLEU score is averaged over the six blind test sets. As it can be seen, very similar results were obtained by most of the participants. Accuracy results fall within a margin of 3 points BLEU for the first 16 classified. 6 in our objective as we built a very simple network that was able to filter out divergent sentence pairs. Only assisted by a very simple filtering technique using rules based on length and language identification."
W18-6485,D18-1328,1,0.826078,"(nets and nett ) that compute in context representations of source (si ) and target words (tj ). L(src, tgt) = I X The model is composed of 2 Bi-directional LSTM subnetworks, nets and nett , which respectively encode source and target sentences. Since both nets and nett take the same form we describe + i=1 J X j=1 935   src log 1 + eaggrs (i,S)∗Yi +   tgt log 1 + eaggrt (j,S)∗Yj (3) where Yisrc and Yjtgt are vectors with reference labels containing −1 when the word is present in the translated sentence, and +1 for divergent (unpaired) words. Further details on the network can be found in (Pham et al., 2018). 3.1 most frequent words of each language are used as vocabulary. Each out-of-vocabulary word is mapped to a special UNK token. Word embeddings (LTs and LTt ) are initialised using fastText4 , further aligned by means of MUSE5 following the unsupervised method detailed in (Lample et al., 2018). Size of embeddings is Es = Et = 256 cells. Both Bi-LSTM use 256-dimensional hidden representations (E = 512). We use r = 1.0. Optimisation of the parameters is done using the stochastic gradient descent method along with gradient clipping (rescaling gradients whose norm exceeds a threshold) to avoid th"
