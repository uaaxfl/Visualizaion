2021.cmcl-1.7,{CMCL} 2021 Shared Task on Eye-Tracking Prediction,2021,-1,-1,5,0,3242,nora hollenstein,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,0,"Eye-tracking data from reading represent an important resource for both linguistics and natural language processing. The ability to accurately model gaze features is crucial to advance our understanding of language processing. This paper describes the Shared Task on Eye-Tracking Data Prediction, jointly organized with the eleventh edition of the Work- shop on Cognitive Modeling and Computational Linguistics (CMCL 2021). The goal of the task is to predict 5 different token- level eye-tracking metrics of the Zurich Cognitive Language Processing Corpus (ZuCo). Eye-tracking data were recorded during natural reading of English sentences. In total, we received submissions from 13 registered teams, whose systems include boosting algorithms with handcrafted features, neural models leveraging transformer language models, or hybrid approaches. The winning system used a range of linguistic and psychometric features in a gradient boosting framework."
2020.sigdial-1.25,Filtering conversations through dialogue acts labels for improving corpus-based convergence studies,2020,-1,-1,3,0,14953,simone fuscone,Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Cognitive models of conversation and research on user-adaptation in dialogue systems involves a better understanding of speakers convergence in conversation. Convergence effects have been established on controlled data sets, for various acoustic and linguistic variables. Tracking interpersonal dynamics on generic corpora has provided positive but more contrasted outcomes. We propose here to enrich large conversational corpora with dialogue act (DA) information. We use DA-labels as filters in order to create data sub sets featuring homogeneous conversational activity. Those data sets allow a more precise comparison between speakers{'} speech variables. Our experiences consist of comparing convergence on low level variables (Energy, Pitch, Speech Rate) measured on raw data sets, with human and automatically DA-labelled data sets. We found that such filtering does help in observing convergence suggesting that studies on interpersonal dynamics should consider such high level dialogue activity types and their related NLP topics as important ingredients of their toolboxes."
2020.paclic-1.5,Exploiting weak-supervision for classifying Non-Sentential Utterances in {M}andarin Conversations,2020,-1,-1,2,0,15799,xinyi chen,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.lrec-1.69,"The {ISO} Standard for Dialogue Act Annotation, Second Edition",2020,-1,-1,7,0,16745,harry bunt,Proceedings of the 12th Language Resources and Evaluation Conference,0,"ISO standard 24617-2 for dialogue act annotation, established in 2012, has in the past few years been used both in corpus annotation and in the design of components for spoken and multimodal dialogue systems. This has brought some inaccuracies and undesirbale limitations of the standard to light, which are addressed in a proposed second edition. This second edition allows a more accurate annotation of dependence relations and rhetorical relations in dialogue. Following the ISO 24617-4 principles of semantic annotation, and borrowing ideas from EmotionML, a triple-layered plug-in mechanism is introduced which allows dialogue act descriptions to be enriched with information about their semantic content, about accompanying emotions, and other information, and allows the annotation scheme to be customised by adding application-specific dialogue act types."
2020.lrec-1.84,Multimodal Corpus of Bidirectional Conversation of Human-human and Human-robot Interaction during f{MRI} Scanning,2020,-1,-1,4,0,16775,birgit rauchbauer,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper we present investigation of real-life, bi-directional conversations. We introduce the multimodal corpus derived from these natural conversations alternating between human-human and human-robot interactions. The human-robot interactions were used as a control condition for the social nature of the human-human conversations. The experimental set up consisted of conversations between the participant in a functional magnetic resonance imaging (fMRI) scanner and a human confederate or conversational robot outside the scanner room, connected via bidirectional audio and unidirectional videoconferencing (from the outside to inside the scanner). A cover story provided a framework for natural, real-life conversations about images of an advertisement campaign. During the conversations we collected a multimodal corpus for a comprehensive characterization of bi-directional conversations. In this paper we introduce this multimodal corpus which includes neural data from functional magnetic resonance imaging (fMRI), physiological data (blood flow pulse and respiration), transcribed conversational data, as well as face and eye-tracking recordings. Thus, we present a unique corpus to study human conversations including neural, physiological and behavioral data."
2020.lrec-1.89,{B}rain{P}redict: a Tool for Predicting and Visualising Local Brain Activity,2020,-1,-1,2,0,16776,youssef hmamouche,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we present a tool allowing dynamic prediction and visualization of an individual{'}s local brain activity during a conversation. The prediction module of this tool is based on classifiers trained using a corpus of human-human and human-robot conversations including fMRI recordings. More precisely, the module takes as input behavioral features computed from raw data, mainly the participant and the interlocutor speech but also the participant{'}s visual input and eye movements. The visualisation module shows in real-time the dynamics of brain active areas synchronised with the behavioral raw data. In addition, it shows which integrated behavioral features are used to predict the activity in individual brain areas."
W18-4703,Downward Compatible Revision of Dialogue Annotation,2018,0,1,6,0,16745,harry bunt,Proceedings 14th Joint {ACL} - {ISO} Workshop on Interoperable Semantic Annotation,0,"This paper discusses some aspects of revising the ISO standard for dialogue act annotation (ISO 24617-2). The revision is aimed at making annotations using the ISO scheme more accurate and at providing more powerful tools for building natural language based dialogue systems, without invalidating the annotated resources that have been built, with the current version of the standard. In support of the revision of the standard, an analysis is provided of the downward compatibility of a revised annotation scheme with the original scheme at the levels of abstract syntax, concrete syntax, and semantics of annotations."
L16-1148,{L}ex{F}r: Adapting the {L}ex{I}t Framework to Build a Corpus-based {F}rench Subcategorization Lexicon,2016,27,1,3,0,927,giulia rambelli,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper introduces LexFr, a corpus-based French lexical resource built by adapting the framework LexIt, originally developed to describe the combinatorial potential of Italian predicates. As in the original framework, the behavior of a group of target predicates is characterized by a series of syntactic (i.e., subcategorization frames) and semantic (i.e., selectional preferences) statistical information (a.k.a. distributional profiles) whose extraction process is mostly unsupervised. The first release of LexFr includes information for 2,493 verbs, 7,939 nouns and 2,628 adjectives. In these pages we describe the adaptation process and evaluated the final resource by comparing the information collected for 20 test verbs against the information available in a gold standard dictionary. In the best performing setting, we obtained 0.74 precision, 0.66 recall and 0.70 F-measure."
L16-1245,4{C}ouv: A New Treebank for {F}rench,2016,0,0,3,0,929,philippe blache,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The question of the type of text used as primary data in treebanks is of certain importance. First, it has an influence at the discourse level: an article is not organized in the same way as a novel or a technical document. Moreover, it also has consequences in terms of semantic interpretation: some types of texts can be easier to interpret than others. We present in this paper a new type of treebank which presents the particularity to answer to specific needs of experimental linguistic. It is made of short texts (book backcovers) that presents a strong coherence in their organization and can be rapidly interpreted. This type of text is adapted to short reading sessions, making it easy to acquire physiological data (e.g. eye movement, electroencepholagraphy). Such a resource offers reliable data when looking for correlations between computational models and human language processing."
L16-1507,A {CUP} of {C}o{F}ee: A large Collection of feedback Utterances Provided with communicative function annotations,2016,0,0,1,1,11527,laurent prevot,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"There have been several attempts to annotate communicative functions to utterances of verbal feedback in English previously. Here, we suggest an annotation scheme for verbal and non-verbal feedback utterances in French including the categories base, attitude, previous and visual. The data comprises conversations, maptasks and negotiations from which we extracted ca. 13,000 candidate feedback utterances and gestures. 12 students were recruited for the annotation campaign of ca. 9,500 instances. Each instance was annotated by between 2 and 7 raters. The evaluation of the annotation agreement resulted in an average best-pair kappa of 0.6. While the base category with the values acknowledgement, evaluation, answer, elicit achieve good agreement, this is not the case for the other main categories. The data sets, which also include automatic extractions of lexical, positional and acoustic features, are freely available and will further be used for machine learning classification experiments to analyse the form-function relationship of feedback."
Y15-1034,Annotation and Classification of {F}rench Feedback Communicative Functions,2015,21,1,1,1,11527,laurent prevot,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"Feedback utterances are among the most fre- quent in dialogue. Feedback is also a crucial aspect of all linguistic theories that take social interaction involving language into account. However, determining communicative func- tions is a notoriously difficult task both for human interpreters and systems. It involves an interpretative process that integrates vari- ous sources of information. Existing work on communicative function classification comes from either dialogue act tagging where it is generally coarse grained concerning the feed- back phenomena or it is token-based and does not address the variety of forms that feed- back utterances can take. This paper intro- duces an annotation framework, the dataset and the related annotation campaign (involv- ing 7 raters to annotate nearly 6000 utter- ances). We present its evaluation not merely in terms of inter-rater agreement but also in terms of usability of the resulting reference dataset both from a linguistic research per- spective and from a more applicative view- point."
W15-4620,A {SIP} of {C}o{F}ee : A Sample of Interesting Productions of Conversational Feedback,2015,19,2,1,1,11527,laurent prevot,Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Feedback utterances are among the most frequent in dialogue. Feedback is also a crucial aspect of linguistic theories that take social interaction, involving lan- guage, into account. This paper introduces the corpora and datasets of a project scru- tinizing this kind of feedback utterances in French. We present the genesis of the cor- pora (for a total of about 16 hours of tran- scribed and phone force-aligned speech) involved in the project. We introduce the resulting datasets and discuss how they are being used in on-going work with focus on the form-function relationship of conver- sational feedback. All the corpora created and the datasets produced in the frame- work of this project will be made available for research purposes."
bigi-etal-2014-representing,Representing Multimodal Linguistic Annotated data,2014,12,3,3,0,16725,brigitte bigi,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The question of interoperability for linguistic annotated resources covers different aspects. First, it requires a representation framework making it possible to compare, and eventually merge, different annotation schema. In this paper, a general description level representing the multimodal linguistic annotations is proposed. It focuses on time representation and on the data content representation: This paper reconsiders and enhances the current and generalized representation of annotations. An XML schema of such annotations is proposed. A Python API is also proposed. This framework is implemented in a multi-platform software and distributed under the terms of the GNU Public License."
gorisch-etal-2014-aix,Aix Map Task corpus: The {F}rench multimodal corpus of task-oriented dialogue,2014,11,6,5,0,18179,jan gorisch,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper introduces the Aix Map Task corpus, a corpus of audio and video recordings of task-oriented dialogues. It was modelled after the original HCRC Map Task corpus. Lexical material was designed for the analysis of speech and prosody, as described in Ast{\'e}sano et al. (2007). The design of the lexical material, the protocol and some basic quantitative features of the existing corpus are presented. The corpus was collected under two communicative conditions, one audio-only condition and one face-to-face condition. The recordings took place in a studio and a sound attenuated booth respectively, with head-set microphones (and in the face-to-face condition with two video cameras). The recordings have been segmented into Inter-Pausal-Units and transcribed using transcription conventions containing actual productions and canonical forms of what was said. It is made publicly available online."
peshkov-prevot-2014-segmentation,"Segmentation evaluation metrics, a comparison grounded on prosodic and discourse units",2014,10,2,2,0,39939,klim peshkov,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Knowledge on evaluation metrics and best practices of using them have improved fast in the recent years Fort et al. (2012). However, the advances concern mostly evaluation of classification related tasks. Segmentation tasks have received less attention. Nevertheless, there are crucial in a large number of linguistic studies. A range of metrics is available (F-score on boundaries, F-score on units, WindowDiff ((WD), Boundary Similarity (BS) but it is still relatively difficult to interpret these metrics on various linguistic segmentation tasks, such as prosodic and discourse segmentation. In this paper, we consider real segmented datasets (introduced in Peshkov et al. (2012)) as references which we deteriorate in different ways (random addition of boundaries, random removal boundaries, near-miss errors introduction). This provide us with various measures on controlled datasets and with an interesting benchmark for various linguistic segmentation tasks."
Y13-1007,"A Quantitative Comparative Study of Prosodic and Discourse Units, the Case of {F}rench and {T}aiwan {M}andarin",2013,34,1,1,1,11527,laurent prevot,"Proceedings of the 27th Pacific Asia Conference on Language, Information, and Computation ({PACLIC} 27)",0,"Studies of spontaneous conversational speech grounded on large and richly annotated corpora are still rare due to the scarcity of such resources. Comparative studies based on such resources are even more rarely found because of the extra-need of comparability in terms of content, genre and speaking style. The present paper presents our efforts for establishing such a dataset for two typologically diverse languages: French and Taiwan Mandarin. To the primary data, we added morphosyntactic, chunking, prosodic and discourse annotation in order to be able to carry out quantitative comparative studies of the syntax-discourse-prosody interfaces. We introduced our work on the data creation itself as well as some preliminary results of the boundary alignment between prosodic and discourse units and how POS and chunks are distributed on these boundaries."
W13-4011,A quantitative view of feedback lexical markers in conversational {F}rench,2013,17,2,1,1,11527,laurent prevot,Proceedings of the {SIGDIAL} 2013 Conference,0,This paper presents a quantitative description of the lexical items used for linguistic feedback in the Corpus of Interactional Data (CID). The paper includes the raw figures for feedback lexical item as well as more detailed figures concerning interindividual variability. This effort is a first step before a broader analysis including more discourse situations and featuring communicative function annotation.
O13-1025,Observing Features of {PTT} Neologisms: A Corpus-driven Study with N-gram Model,2013,5,3,3,0,41569,tsunjui liu,Proceedings of the 25th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2013),0,"PTT (xe6x89xb9xe8xb8xa2xe8xb8xa2) is one of the largest web forums in Taiwan. In the last few years, its importance has been growing rapidly because it has been widely mentioned by most of the mainstream media. It is observed that its influence reflects not only on the society but also on the language novel use in Taiwan. In this research, a pipeline processing system in Python was developed to collect the data from PTT, and the n-gram model with proposed linguistic filter are adopted with the attempt to capture two-character neologisms emerged in PTT. Evaluation task with 25 subjects was conducted against the system's performance with the calculation of Fleissxe2x80x99 kappa measure. Linguistic discussion as well as the comparison with time series analysis of frequency data are provided. It is hoped that the detection of neologisms in PTT can be improved by observing the features, which may even facilitate the prediction of the neologisms in the future."
afantenos-etal-2012-empirical,An empirical resource for discovering cognitive principles of discourse organisation: the {ANNODIS} corpus,2012,35,36,10,0,17571,stergos afantenos,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes the ANNODIS resource, a discourse-level annotated corpus for French. The corpus combines two perspectives on discourse: a bottom-up approach and a top-down approach. The bottom-up view incrementally builds a structure from elementary discourse units, while the top-down view focuses on the selective annotation of multi-level discourse structures. The corpus is composed of texts that are diversified with respect to genre, length and type of discursive organisation. The methodology followed here involves an iterative design of annotation guidelines in order to reach satisfactory inter-annotator agreement levels. This allows us to raise a few issues relevant for the comparison of such complex objects as discourse structures. The corpus also serves as a source of empirical evidence for discourse theories. We present here two first analyses taking advantage of this new annotated corpus --one that tested hypotheses on constraints governing discourse structure, and another that studied the variations in composition and signalling of multi-level discourse structures."
2011.jeptalnrecital-court.27,Un calcul de termes typ{\\'e}s pour la pragmatique lexicale: chemins et voyageurs fictifs dans un corpus de r{\\'e}cits de voyage (A calculation of typed terms for lexical pragmatics: paths and fictional travellers in a travel stories corpus),2011,-1,-1,2,0,17941,richard moot,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Ce travail s{'}inscrit dans l{'}analyse automatique d{'}un corpus de r{\'e}cits de voyage. {\`A} cette fin, nous raffinons la s{\'e}mantique de Montague pour rendre compte des ph{\'e}nom{\`e}nes d{'}adaptation du sens des mots au contexte dans lequel ils apparaissent. Ici, nous mod{\'e}lisons les constructions de type {`}le chemin descend pendant une demi-heure{'} o{\`u} ledit chemin introduit un voyageur fictif qui le parcourt, en {\'e}tendant des id{\'e}es que le dernier auteur a d{\'e}velopp{\'e} avec Bassac et Mery. Cette introduction du voyageur utilise la mont{\'e}e de type afin que le quantificateur introduisant le voyageur porte sur toute la phrase et que les propri{\'e}t{\'e}s du chemin ne deviennent pas des propri{\'e}t{\'e}s du voyageur, f{\^u}t-il fictif. Cette analyse s{\'e}mantique (ou plut{\^o}t sa traduction en lambda-DRT) est d{'}ores et d{\'e}j{\`a} implant{\'e}e pour une partie du lexique de Grail."
Y10-1096,"Computational Modeling of Verb Acquisition, from a Monolingual to a Bilingual Study",2010,18,2,1,1,11527,laurent prevot,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"Going cross-linguistic is a an important but challenging track for validating a com- putational model of lexical organization. Our starting point is a computational model that has been established and validated on French language and we attempted to apply it on Man- darin language. The main ingredients of this model are computational lexical resources and a psycho-linguistic protocol involving extra-linguistic material (video-clips). At this stage, all the psycho-linguistic experiments have been ran, most of the resources have been built but some comparative analyses are not fully completed. Still the project is advanced enough to report on the issues we had to address while performing this cross-linguistic move concerning the resources, the analysis of the data and the data alignment across languages."
blache-etal-2010-otim,The {OTIM} Formal Annotation Model: A Preliminary Step before Annotation Scheme,2010,9,2,8,0,929,philippe blache,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Large annotation projects, typically those addressing the question of multimodal annotation in which many different kinds of information have to be encoded, have to elaborate precise and high level annotation schemes. Doing this requires first to define the structure of the information: the different objects and their organization. This stage has to be as much independent as possible from the coding language constraints. This is the reason why we propose a preliminary formal annotation model, represented with typed feature structures. This representation requires a precise definition of the different objects, their properties (or features) and their relations, represented in terms of type hierarchies. This approach has been used to specify the annotation scheme of a large multimodal annotation project (OTIM) and experimented in the annotation of a multimodal corpus (CID, Corpus of Interactional Data). This project aims at collecting, annotating and exploiting a dialogue video corpus in a multimodal perspective (including speech and gesture modalities). The corpus itself, is made of 8 hours of dialogues, fully transcribed and richly annotated (phonetics, syntax, pragmatics, gestures, etc.)."
C10-2008,A Formal Scheme for Multimodal Grammars,2010,19,1,2,0,929,philippe blache,Coling 2010: Posters,0,"We present in this paper a formal approach for the representation of multimodal information. This approach, thanks to the to use of typed feature structures and hypergraphs, generalizes existing ones (typically annotation graphs) in several ways. It first proposes an homogenous representation of different types of information (nodes and relations) coming from different domains (speech, gestures). Second, it makes it possible to specify constraints representing the interaction between the different modalities, in the perspective of developing multimodal grammars."
Y09-1036,Using Extra-Linguistic Material for {M}andarin-{F}rench Verbal Constructions Comparison,2009,15,2,2,0,15790,pierre magistry,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,"Systematic cross-linguistic studies of verbs syntactic-semantic behaviors for ty- pologically distant languages such as Mandarin Chinese and French are difficult to conduct. Such studies are nevertheless necessary due to the crucial role that verbal constructions play in the mental lexicon. This paper addresses the problem by combining psycho-linguistics and computational methods. Psycho-linguistics provides us with a bilingual corpus that fea- tures verbal construction associated with carefully built extra-linguistic material (short video clips). Computational approaches bring us distributional semantic models (DSM) to measure the distance between linguistic elements in the extra-linguistic space. These models allows for cross-linguistic measures that we evaluate against manually annotated data. In this pa- per, we discuss the results, potential shortcomings involving cultural variability and how to measure such bias."
W09-3303,{W}iktionary for Natural Language Processing: Methodology and Limitations,2009,0,2,4,0,38143,emmanuel navarro,Proceedings of the 2009 Workshop on The People{'}s Web Meets {NLP}: Collaboratively Constructed Semantic Resources (People{'}s Web),0,None
2009.jeptalnrecital-court.5,{ANNODIS}: une approche outill{\\'e}e de l{'}annotation de structures discursives,2009,-1,-1,12,0,43228,mariepaule perywoodley,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Le projet ANNODIS vise la construction d{'}un corpus de textes annot{\'e}s au niveau discursif ainsi que le d{\'e}veloppement d{'}outils pour l{'}annotation et l{'}exploitation de corpus. Les annotations adoptent deux points de vue compl{\'e}mentaires : une perspective ascendante part d{'}unit{\'e}s de discours minimales pour construire des structures complexes via un jeu de relations de discours ; une perspective descendante aborde le texte dans son entier et se base sur des indices pr{\'e}-identifi{\'e}s pour d{\'e}tecter des structures discursives de haut niveau. La construction du corpus est associ{\'e}e {\`a} la cr{\'e}ation de deux interfaces : la premi{\`e}re assiste l{'}annotation manuelle des relations et structures discursives en permettant une visualisation du marquage issu des pr{\'e}traitements ; une seconde sera destin{\'e}e {\`a} l{'}exploitation des annotations. Nous pr{\'e}sentons les mod{\`e}les et protocoles d{'}annotation {\'e}labor{\'e}s pour mettre en oeuvre, au travers de l{'}interface d{\'e}di{\'e}e, la campagne d{'}annotation."
W08-1912,"Toward a cognitive organization for electronic dictionaries, the case for semantic proxemy",2008,16,12,3,1,18789,bruno gaume,Coling 2008: Proceedings of the Workshop on Cognitive Aspects of the Lexicon ({COGALEX} 2008),0,"We compare a psycholinguistic approach of mental lexicon organization with a computational approach of implicit lexical organization as found in dictionaries. In this work, we associate dictionaries with 'small world' graphs. This multidisciplinary approach aims at showing that implicit structure of dictionaries, mathematically identified, fits the way young children categorize. These dictionary graphs might therefore be considered as 'cognitive artifacts'. This shows the importance of semantic proximity both in cognitive and computational organization of verbs lexicon."
chung-etal-2008-extracting,Extracting Concrete Senses of Lexicon through Measurement of Conceptual Similarity in Ontologies,2008,5,0,2,0,15600,siawfong chung,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The measurement of conceptual similarity in a hierarchical structure has been proposed by studies such as Wu and Palmer (1994) which have been summarized and evaluated in Budanisky and Hirst (2006). The present study applies the measurement of conceptual similarity to conceptual metaphor research by comparing concreteness of ontological resource nodes to several prototypical concrete nodes selected by human subjects. Here, the purpose of comparing conceptual similarity between nodes is to select a concrete sense for a word which is used metaphorically. Through using WordNet-SUMO interface such as SinicaBow (Huang, Chang and Lee, 2004), concrete senses of a lexicon will be selected once its SUMO nodes have been compared in terms of conceptual similarity with the prototypical concrete nodes. This study has strong implications for the interaction of psycholinguistic and computational linguistic fields in conceptual metaphor research."
P07-2018,"Rethinking {C}hinese Word Segmentation: Tokenization, Character Classification, or Wordbreak Identification",2007,6,27,4,0,1504,churen huang,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"This paper addresses two remaining challenges in Chinese word segmentation. The challenge in HLT is to find a robust segmentation method that requires no prior lexical knowledge and no extensive training to adapt to new types of data. The challenge in modelling human cognition and acquisition it to segment words efficiently without using knowledge of wordhood. We propose a radical method of word segmentation to meet both challenges. The most critical concept that we introduce is that Chinese word segmentation is the classification of a string of character-boundaries (CB's) into either word-boundaries (WB's) and non-word-boundaries. In Chinese, CB's are delimited and distributed in between two characters. Hence we can use the distributional properties of CB among the background character strings to predict which CB's are WB's."
Y06-1043,Using the {S}wadesh list for creating a simple common taxonomy,2006,18,1,1,1,11527,laurent prevot,"Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation",0,"One of the main goal of xe2x80x99Developing International Standards of Language Resources for Semantic Web Applicationsxe2x80x99 [Takenobu et al., 2006], an international project sponsored by Japanxe2x80x99s NEDO foundation, is to implement standards of language resources that can be very robust when applied to different languages of the world. In addition, the project concentrates on Asian language resources. Hence the project plans to construct lexica of Japanese, Mandarin, Thai and Italian; and integrate them as parts of a multilingual resource linked to the original Princeton WordNet (WN) [Fellbaum, 1998]. It is therefore comparable with projects such as EuroWordNet [Vossen, 1998], in which the coherence and homogeneity across different languages remained the main issue that hampered the true interoperability of the final resource. Since the project is exposed to these risks by its design, a priority for the project members is therefore to tackle this issue from the very beginning. For this purpose, one of the measures taken is to build jointly an upper-level ontology1 that will play the role of a structured interlingual index. The NEDO participants are currently exploring several ways for selecting a basic vocabulary that will serve as a starting point for designing this language-independent core of the resource. This paper describes some of the preliminary experiments we are currently conducing. More precisely we are (i) using the Swadesh list [Swadesh, 1952] as a basic core vocabulary and (ii) exploring the possibilities offered by this list for creating a simple common ontology. These practical objectives confront us however with a complex discussion of contemporary linguistics, namely the relativist/universalist debate [Gumperz and Levinson, 1996], and more precisely its consequences for the lexical organization. In the context of this project the existence and the nature of a common universal structure is a background question that we would like to contribute to answer."
P06-2106,Infrastructure for Standardization of {A}sian Language Resources,2006,10,18,10,0,301,takenobu tokunaga,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"As an area of great linguistic and cultural diversity, Asian language resources have received much less attention than their western counterparts. Creating a common standard for Asian language resources that is compatible with an international standard has at least three strong advantages: to increase the competitive edge of Asian countries, to bring Asian countries to closer to their western counterparts, and to bring more cohesion among Asian countries. To achieve this goal, we have launched a two year project to create a common standard for Asian language resources. The project is comprised of four research items, (1) building a description framework of lexical entries, (2) building sample lexicons, (3) building an upper-layer ontology and (4) evaluating the proposed framework through an application. This paper outlines the project in terms of its aim and approach."
I05-7013,Interfacing Ontologies and Lexical Resources,2005,-1,-1,1,1,11527,laurent prevot,Proceedings of {O}nto{L}ex 2005 - Ontologies and Lexical Resources,0,None
