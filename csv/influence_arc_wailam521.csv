2020.aacl-main.29,W05-0909,0,0.025349,"0.49 0.308 0.506 0.347 0.524 0.322 0.505 ROUGEL BLEU Extractor BLEU ROUGEL 0.335 0.557 0.655 0.708 0.746 0.757 0.71 0.764 0.489 0.618 0.8 0.6 Extractor Ratio 0.6 0.4 0.4 2 4 6 4.4 Experimental Results The performances of our KB-to-text generator and triple extractor are shown in the left and right of Table 2 respectively. Both generator and extractor of our model outperform all baseline models significantly and consistently. The comparison between our EGD model and the supervised SEG model indicates that our unsupervised EGD model 264 6 2 4 6 0.40 0.35 0.3 2 PDG 4 0.45 0.4 ton, 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004), and CIDEr (Vedantam et al., 2015). These metrics are calculated with the evaluation code provided in Novikova et al. (2017). Moreover, we also evaluate the performance of the extractor with precision, recall, and F1 scores (Manning et al., 2010). In PDG, we set α = 0.8, β1 = 0.2, β2 = 0.6. We firstly pre-train the extractor and the generator in the PDG model with the data generated by PDG until convergence. All other models are fine-tuned on the PDG model. For the DL model, we train the generator for 5 steps with the txt2txt process and train the extractor with the kb2kb"
2020.aacl-main.29,N19-1071,0,0.0245563,"s very expensive to prepare. Many methods have been proposed to tackle the dataset insufficiency problem in other tasks. Fu et al. (2020c) propose to directly train the model on partially-aligned data in which the data and the text are not necessarily exactly math, and it can be built automatically. He et al. (2016); Sennrich et al. (2016); Yi et al. (2017) propose dual learning frameworks. They pre-train a weak model with parallel data and refine the model with monolingual data. This strategy has been applied in many related tasks including semantic parsing (Cao et al., 2019), summarization (Baziotis et al., 2019) and 259 pre-train Text PDG KB Eθ PDG KB split G Text KB1 … KB2 Gφ … Text2 Text1 E Textn E Eθ KB KB1 LE kb2kb KB2 Text KBn KB Gφ Data from corpus Eθ KB E/G split Gφ … Text KBn … KB2 KB1 … Text2 ARLG <latexit sha1_base64=""Y8Oe2xpmNErGDbs8P67Jf4pRbrs="">AAAB+XicbVDLSsNAFJ3UV62vVFfSzWARXJWkRZuCi4IILly0YGuhCWEynbZDJw9mJoUS8iduXCji1v9w4U6/xknbhVYPDBzOuZd75ngRo0IaxqeWW1vf2NzKbxd2dvf2D/TiYVeEMcekg0MW8p6HBGE0IB1JJSO9iBPke4zce5OrzL+fEi5oGNzJWUQcH40COqQYSSW5um77SI4xYslt6ibXacHVy0alYRm1RhUaFcu6qNbPFTHmgOaSlJtFu/TVPn5vufqHPQhx7JNAYoaE6JtGJJ0EcUkxI2nBjgWJEJ6gEekrGiCfCCeZJ0/hqVIGcBhy9QIJ5+rPjQT5Qsx8T01mOcWql"
2020.aacl-main.29,P19-1007,0,0.274549,"ship enables the design of a closed-loop learning framework in which we link KB-to-text generation and its dual task of triple extraction so as to reconstruct the unaligned KB triples and texts. The non-differentiability issue of picking words from our neural model before reconstruction makes it hard to train the extractor or generator effectively using backpropagation. To solve this issue, we apply Reinforcement Learning (RL) based on policy gradients into our dual learning framework to optimize our extractor or generator according to the rewards. Some semi-supervised works (He et al., 2016; Cao et al., 2019) have been proposed to generate 258 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 258–268 c December 4 - 7, 2020. 2020 Association for Computational Linguistics 101 Helena is discovered by James Craig Watson who was born in Canada. E <101 Helena, discoverer, James Craig Watson> 1.0 1.0 <James Craig Watson, nationality, Canada>, 0.9 <James Craig Watson, profession, Writer>, 0.5 <James Craig Watson, deathPlace, Australia> 0.5 Traditional RL G 101 Hele"
2020.aacl-main.29,2020.emnlp-main.90,1,0.674992,"pre-train the weak model. Another line of research proposes to use some extra annotations instead of using aligned data. Lample et al. (2018a,b) propose to train an unsupervised NMT system based on few annotated word pairs (Conneau et al., 2018). Luo et al. (2019) propose to generate pseudo data with a rule-based template (Li et al., 2018). However, these models cannot be directly applied in our scenario since our dataset is too complicated to make these annotations. Fu et al. (2020b) propose to utilize topic information from a dynamic topic tracker to solve the dataset insufficiency problem. Cheng et al. (2020) propose to generate better text description for a few entities by exploring the knowledge from KB and distill the useful part. In the field of computer vision, Zhu et al. (2017) propose cycleGAN which uses a cycled training method that transforms the input into another data form and then transforms it back, minimizing the recover loss. The method works well in the image domain but has some problems in text generation considering the non-differentiable discrete layer. We follow the ideas of cycleGAN to train the whole model without supervised data and adopt the RL method proposed in dual learn"
2020.aacl-main.29,D18-1426,0,0.0174228,"ise, it is sampled form the next token in Ks . This process can be expressed mathematically as:  w ∼ p(w) ri > α    i−1 X T˜i = ,  Ks [1 + 1(T˜j ∈ Ks )] otherwise   j=1 in which 1(C) = 1 if condition C is true and 0 otherwise. T˜j ∈ Ks indicates whether the word T˜j is sampled from Ks . This pseudo text data is used to solve the cold start problem when training the extractor. Pseudo KB Generator generates pseudo KB triples for each text and form a pseudo supervised training data. This data is used to solve the cold start problem when pre-training the generator. Similar with the work of Freitag and Roy (2018), for an input sequence T we randomly remove words in the input text with a probability β1 and sample new words by sampling words from a distribution ˜ with a probability β2 . The generated sequence K is the pseudo KB sequence for each text. Similar to the Pseudo Text Generator, we randomly add some words by sampling from the distribution p(w). We do not use the probability calculated from Kt since it may sample some wrong relations or wrong entity names which undermines the performance. Mathematically, it can be expressed as: 3.5 Traditional reinforcement learning for sequence generation calc"
2020.aacl-main.29,2020.coling-main.215,1,0.904227,"paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 14204418). “101 Helena is discovered by James Craig Watson.”. Recently, many research works have been proposed for this task. For example, Gardent et al. (2017a,b) create the WebNLG dataset to generate description for triples sampled from DBPedia (Auer et al., 2007). Lebret et al.’s (2016) method generates people’s biographies from extracted Wikipedia infobox. Novikova et al. (2017) propose to generate restaurant reviews by some given attributes and Fu et al. (2020a) create the WikiEvent dataset to generate text based on an event chain. However, the works mentioned above usually map structured triples to text via a supervised seq-to-seq (Sutskever et al., 2014) model, in which large amounts of annotated data is necessary and the annotation is very expensive and time-consuming. We aim to tackle the problem of completely unsupervised KB-to-text generation which only requires a text corpus and a KB corpus and does not assume any alignment between them. We propose a dual learning framework based on the inverse relationship between the KB-to-text generation"
2020.aacl-main.29,2020.emnlp-main.738,1,0.939481,"paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 14204418). “101 Helena is discovered by James Craig Watson.”. Recently, many research works have been proposed for this task. For example, Gardent et al. (2017a,b) create the WebNLG dataset to generate description for triples sampled from DBPedia (Auer et al., 2007). Lebret et al.’s (2016) method generates people’s biographies from extracted Wikipedia infobox. Novikova et al. (2017) propose to generate restaurant reviews by some given attributes and Fu et al. (2020a) create the WikiEvent dataset to generate text based on an event chain. However, the works mentioned above usually map structured triples to text via a supervised seq-to-seq (Sutskever et al., 2014) model, in which large amounts of annotated data is necessary and the annotation is very expensive and time-consuming. We aim to tackle the problem of completely unsupervised KB-to-text generation which only requires a text corpus and a KB corpus and does not assume any alignment between them. We propose a dual learning framework based on the inverse relationship between the KB-to-text generation"
2020.aacl-main.29,P17-1017,0,0.641013,"ge Base (KB)-to-text task focuses on generating plain text descriptions from given knowledge bases (KB) triples which makes them accessible to users. For instance, given a KB triple <101 Helena, discoverer, James Craig Watson>, it is expected to generate a description sentence such as ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 14204418). “101 Helena is discovered by James Craig Watson.”. Recently, many research works have been proposed for this task. For example, Gardent et al. (2017a,b) create the WebNLG dataset to generate description for triples sampled from DBPedia (Auer et al., 2007). Lebret et al.’s (2016) method generates people’s biographies from extracted Wikipedia infobox. Novikova et al. (2017) propose to generate restaurant reviews by some given attributes and Fu et al. (2020a) create the WikiEvent dataset to generate text based on an event chain. However, the works mentioned above usually map structured triples to text via a supervised seq-to-seq (Sutskever et al., 2014) model, in which large amounts of annotated data is necessary and the annotation is very e"
2020.aacl-main.29,W17-3518,0,0.411061,"ge Base (KB)-to-text task focuses on generating plain text descriptions from given knowledge bases (KB) triples which makes them accessible to users. For instance, given a KB triple <101 Helena, discoverer, James Craig Watson>, it is expected to generate a description sentence such as ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 14204418). “101 Helena is discovered by James Craig Watson.”. Recently, many research works have been proposed for this task. For example, Gardent et al. (2017a,b) create the WebNLG dataset to generate description for triples sampled from DBPedia (Auer et al., 2007). Lebret et al.’s (2016) method generates people’s biographies from extracted Wikipedia infobox. Novikova et al. (2017) propose to generate restaurant reviews by some given attributes and Fu et al. (2020a) create the WikiEvent dataset to generate text based on an event chain. However, the works mentioned above usually map structured triples to text via a supervised seq-to-seq (Sutskever et al., 2014) model, in which large amounts of annotated data is necessary and the annotation is very e"
2020.aacl-main.29,W18-2703,0,0.128865,"9P7LemHgtAm8bkvOg6WlDOPNhVTnHYCQbHrcNp2pmex376mQjLfu1KzgPZdPPbYiBGstDQwMz0XqwnBPKrNB9HpZe18nhqYOWQVUQyILFQpo0JFk0KxXMyXoG0tHJSrpnvZ98bBS31gvvaGPgld6inCsZRdGwWqH2GhGOF0nuqFkgaYTPGYdjX1sEtlP1qEn8MjrQzhyBf6eQou1O8bEXalnLmOnoyjyt9eLP7ldUM1Kvcj5gWhoh5ZHhqFHCofxk3AIROUKD7TBBPBdFZIJlhgonRfcQlfP4X/k1besgtWvmHnqidgiSTIgkNwDGxQAlVwAeqgCQiYgVtwDx6MG+POeDSelqMJ43MnA37AeP4AU8SXgA==</latexit> Figure 2: The extractor-generator dual (EGD) framework. It contains three processes namely a pre-train process, a kb2kb process and a txt2txt process. information narration (Sun et al., 2018). However, as indicated in Hoang et al. (2018), the dual learning approach is not easy to train. Moreover, these methods still need some aligned data to pre-train the weak model. Another line of research proposes to use some extra annotations instead of using aligned data. Lample et al. (2018a,b) propose to train an unsupervised NMT system based on few annotated word pairs (Conneau et al., 2018). Luo et al. (2019) propose to generate pseudo data with a rule-based template (Li et al., 2018). However, these models cannot be directly applied in our scenario since our dataset is too complicated to make these annotations. Fu et al. (2020b) pro"
2020.aacl-main.29,J82-2005,0,0.552672,"Missing"
2020.aacl-main.29,D16-1128,0,0.14657,"Missing"
2020.aacl-main.29,D16-1127,0,0.0138708,"the head, relation and tail entity respectively. We denote the texts corpus as T = {Ti |∀i} (i) (i) (i) in which Ti = [t1 , t2 , · · · , tni ] is the ith sen(i) tence and tj is the jth word in the sentence. In our problem, we are only given a collection of KB triples Kt ⊂ K and a collection of text Tt ⊂ T without any alignment information between them. The ultimate goal is to train a model that generates the corresponding text in T describing the given triple list from K. Reinforcement Learning (RL) has been utilized to solve the infeasibility of backpropagation through discrete tokens layer. Li et al. (2016) propose to use RL to focus on the long term target and thus improve the performance. Yu et al. (2017) propose to use the RL in generative adversarial networks to solve the discrete tokens problem. He et al. (2016); Sun et al. (2018) propose to use RL in dual training. As far as we know, no studies of RL have been conducted for KB triples in which 260 3.2 Extractor-Generator Dual Framework Our proposed Extractor-Generator Dual (EGD) framework is composed of a generator G and an extractor E that translate data in one form to another. We denote all trainable parameters in E and G as θ and φ, res"
2020.aacl-main.29,N18-1169,0,0.0666997,"Missing"
2020.aacl-main.29,P09-1011,0,0.169491,"s proposed to describe a list of triples sampled from DBPedia (Auer et al., 2007). Except for the KB triples, many other types of data have also been investigated for how to generate text from them. For example, E2E (Novikova et al., 2017) aims at generating text from some restaurants’ attributes. Wikibio (Lebret et al., 2016) proposes to generate biographies for the Wikipedia infobox while WikiEvent (Fu et al., 2020a) proposes to generate text based on an event chain. Besides, Chen and Mooney (2008); Wiseman et al. (2017) propose to generate a summarization of a match based on the scores and Liang et al. (2009) propose to generate weather reports based on the records. All these tasks require an elaborately annotated dataset which is very expensive to prepare. Many methods have been proposed to tackle the dataset insufficiency problem in other tasks. Fu et al. (2020c) propose to directly train the model on partially-aligned data in which the data and the text are not necessarily exactly math, and it can be built automatically. He et al. (2016); Sennrich et al. (2016); Yi et al. (2017) propose dual learning frameworks. They pre-train a weak model with parallel data and refine the model with monolingua"
2020.aacl-main.29,W04-1013,0,0.0400811,"ess proposed in He et al. (2016); Zhu et al. (2017). It is fine-tuned on the PDG model and iterates alternatively between txt2txt and kb2kb processes. Here, we do not use any reinforcement learning component. DL-RL1 uses the dual learning process together with an RL component. It is similar to the dual learning method proposed in He et al. (2016); Zhu et al. (2017). We use the PDG’s data to train the weak model. It uses the log-likelihood of the recover process’s output sequence as the reward. DL-RL2 follows the settings of Sun et al. (2018). Different from DL-RL1, this model uses the ROUGEL (Lin, 2004) score of the recovered sequence instead of using the log-likelihood as the reward. SEG is a Supervised Extractor-Generator using the original setting of WebNLG for both generator and extractor. It utilizes all the alignment information between KB and text and thus provides an upper bound for our experiment. 4.3 Experimental settings We evaluate the performances of the generator and the extractor with several metrics including BLEU (Papineni et al., 2002), NIST (Dodding263 PDG DL DL-RL1 DL-RL2 EGD EGD w/o ARLE EGD w/o ARLG EGD w/o PDG SEG BLEU 0.322 0.352 0.356 0.356 0.369 0.351 0.353 0.010 0."
2020.aacl-main.29,W17-5525,0,0.200505,"Missing"
2020.aacl-main.29,P02-1040,0,0.107894,"he recover process’s output sequence as the reward. DL-RL2 follows the settings of Sun et al. (2018). Different from DL-RL1, this model uses the ROUGEL (Lin, 2004) score of the recovered sequence instead of using the log-likelihood as the reward. SEG is a Supervised Extractor-Generator using the original setting of WebNLG for both generator and extractor. It utilizes all the alignment information between KB and text and thus provides an upper bound for our experiment. 4.3 Experimental settings We evaluate the performances of the generator and the extractor with several metrics including BLEU (Papineni et al., 2002), NIST (Dodding263 PDG DL DL-RL1 DL-RL2 EGD EGD w/o ARLE EGD w/o ARLG EGD w/o PDG SEG BLEU 0.322 0.352 0.356 0.356 0.369 0.351 0.353 0.010 0.406 NIST 7.06 7.71 7.73 7.75 7.77 7.72 7.77 0.82 8.31 Generator METEOR ROUGEL 0.349 0.505 0.347 0.528 0.350 0.532 0.350 0.533 0.364 0.541 0.347 0.529 0.348 0.531 0.037 0.119 0.385 0.585 CIDEr 2.63 2.96 3.00 2.99 3.13 2.97 2.99 0.02 3.66 BLEU 0.489 0.735 0.760 0.757 0.775 0.770 0.729 0.020 0.848 NIST 6.01 10.4 10.8 10.7 11.1 10.9 10.4 0.42 11.8 METEOR 0.351 0.502 0.501 0.503 0.503 0.501 0.505 0.026 0.595 Extractor ROUGEL CIDEr 0.618 3.97 0.743 5.67 0.755 5"
2020.aacl-main.29,P16-1009,0,0.0381503,"event chain. Besides, Chen and Mooney (2008); Wiseman et al. (2017) propose to generate a summarization of a match based on the scores and Liang et al. (2009) propose to generate weather reports based on the records. All these tasks require an elaborately annotated dataset which is very expensive to prepare. Many methods have been proposed to tackle the dataset insufficiency problem in other tasks. Fu et al. (2020c) propose to directly train the model on partially-aligned data in which the data and the text are not necessarily exactly math, and it can be built automatically. He et al. (2016); Sennrich et al. (2016); Yi et al. (2017) propose dual learning frameworks. They pre-train a weak model with parallel data and refine the model with monolingual data. This strategy has been applied in many related tasks including semantic parsing (Cao et al., 2019), summarization (Baziotis et al., 2019) and 259 pre-train Text PDG KB Eθ PDG KB split G Text KB1 … KB2 Gφ … Text2 Text1 E Textn E Eθ KB KB1 LE kb2kb KB2 Text KBn KB Gφ Data from corpus Eθ KB E/G split Gφ … Text KBn … KB2 KB1 … Text2 ARLG <latexit sha1_base64=""Y8Oe2xpmNErGDbs8P67Jf4pRbrs="">AAAB+XicbVDLSsNAFJ3UV62vVFfSzWARXJWkRZuCi4IILly0YGuhCWEynbZDJw9mJoUS"
2020.aacl-main.29,D18-1236,0,0.339367,"6ZVUNEDgcM593JPjhNwJhVCb0ZiZXVtfSO5mdra3tndM9P7LemHgtAm8bkvOg6WlDOPNhVTnHYCQbHrcNp2pmex376mQjLfu1KzgPZdPPbYiBGstDQwMz0XqwnBPKrNB9HpZe18nhqYOWQVUQyILFQpo0JFk0KxXMyXoG0tHJSrpnvZ98bBS31gvvaGPgld6inCsZRdGwWqH2GhGOF0nuqFkgaYTPGYdjX1sEtlP1qEn8MjrQzhyBf6eQou1O8bEXalnLmOnoyjyt9eLP7ldUM1Kvcj5gWhoh5ZHhqFHCofxk3AIROUKD7TBBPBdFZIJlhgonRfcQlfP4X/k1besgtWvmHnqidgiSTIgkNwDGxQAlVwAeqgCQiYgVtwDx6MG+POeDSelqMJ43MnA37AeP4AU8SXgA==</latexit> Figure 2: The extractor-generator dual (EGD) framework. It contains three processes namely a pre-train process, a kb2kb process and a txt2txt process. information narration (Sun et al., 2018). However, as indicated in Hoang et al. (2018), the dual learning approach is not easy to train. Moreover, these methods still need some aligned data to pre-train the weak model. Another line of research proposes to use some extra annotations instead of using aligned data. Lample et al. (2018a,b) propose to train an unsupervised NMT system based on few annotated word pairs (Conneau et al., 2018). Luo et al. (2019) propose to generate pseudo data with a rule-based template (Li et al., 2018). However, these models cannot be directly applied in our scenario since our dataset is too complicated to"
2020.aacl-main.54,P17-1021,0,0.384375,"ioned tags to the POI database. Furthermore, based only on tag information, it is almost impossible for semantic parsing methods to make use of the distance correlation between questions and POIs. Another line of solution is adopting similarity matching models for calculating the similarity score between questions and POIs. Recent years have witnessed rapid growth in various kinds of semantic similarity based QA systems such as Convolutional Neural Network Architecture (Hu et al., 2014), LSTM Based Answer Selection (Tan et al., 2015, 2016), and Cross-Attention Based Question Answering System (Hao et al., 2017). Despite the success in common landscapes, most existing studies of this family cannot work well for POI oriented QA, since it is ineffective for them to handle the unique properties of POI elements such as tags and locations. As a result, a significant gap remains between academic proposals and the industry standard of implementing location based services. It is nontrivial to extend existing QA models to handle the challenges of POI oriented QA. In general, the unique challenges for this problem mainly 542 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for C"
2020.aacl-main.54,D07-1002,0,0.0963735,"questions. • We conduct comprehensive experiments on two real-world datasets enabling the evaluation of the results from different perspectives. Experimental results demonstrate significant improvements of PJI over all the state-of-theart baselines. 2 Related Work QA with Semantic Parser Semantic parsing shines at handling complex linguistic constructions and obtains reasonable performance on question answering problems. Traditionally, semantic parsers like AMR (Banarescu et al., 2012) and SQL (Androutsopoulos et al., 1995) map sentences to formal representations of their underlying meaning (Shen and Lapata, 2007; Yao et al., 2014; Hill et al., 2015; Talmor et al., 2017). By leveraging a knowledge base, semantic parsing is reduced to query graph generation and stage searching. Neural Approaches for QA With the recent development in deep learning, neural networks have achieved great success in question answer problems (Salakhutdinov and Hinton, 2009; Collobert et al., 2011; Socher et al., 2012; Hu et al., 2014; Tan et al., 2015, 2016). Most of these models use a deep neural network like GRU (Chung et al., 2014) and LSTM (Hochreiter and Schmidhuber, 1997) to handle the long texts required for QA. Furthe"
2020.aacl-main.54,D12-1110,0,0.0253067,"ance on question answering problems. Traditionally, semantic parsers like AMR (Banarescu et al., 2012) and SQL (Androutsopoulos et al., 1995) map sentences to formal representations of their underlying meaning (Shen and Lapata, 2007; Yao et al., 2014; Hill et al., 2015; Talmor et al., 2017). By leveraging a knowledge base, semantic parsing is reduced to query graph generation and stage searching. Neural Approaches for QA With the recent development in deep learning, neural networks have achieved great success in question answer problems (Salakhutdinov and Hinton, 2009; Collobert et al., 2011; Socher et al., 2012; Hu et al., 2014; Tan et al., 2015, 2016). Most of these models use a deep neural network like GRU (Chung et al., 2014) and LSTM (Hochreiter and Schmidhuber, 1997) to handle the long texts required for QA. Further improvements like attention mechanism are applied to focus on the most relevant facts (Hao et al., 2017; Zhao et al., 2019). The relevance score of each QA pair is the cosine similarity of the semantic vectors. The final answers to each question are then sorted by the similarity score. knowledge graph, POI has two important properties which are tags and location. Tags refer to a sho"
2020.aacl-main.54,S17-1020,0,0.0496576,"Missing"
2020.aacl-main.54,P16-1044,0,0.0734279,"Missing"
2020.aacl-main.54,D14-1167,0,0.0649314,"Missing"
2020.aacl-main.54,W14-2416,0,0.0753285,"Missing"
2020.aacl-main.70,P17-1036,0,0.0129105,"uding natural language sentences and attribute-value pairs, we transform them into unified representations. It can be observed that these two types of information are actually complementary to each other where the attribute term in an attribute-value pair can well indicate the major focus of such snippet, while a textual sentence can usually provide more detailed semantic information. To highlight the focus of a natural language sentence c¯ ∈ D ∪ R, we can extract m aspect terms: ca = {ca1 , ca2 , ..., cam } = AE(¯ c) (1) where AE(·) refers to a reasonable aspect extraction algorithm such as (He et al., 2017) used in our experiments. ca are the extracted m aspects. These extracted aspects are typically not exactly the same as the terms in the attribute set, but they play a similar role as characterizing the focus of the candidate snippet. For an attribute-value pair (ai , vi ) ∈ A, since the main focus of such a snippet is already highlighted by the attribute term ai , we directly treat ai as the aspect ca and construct a pesudo-sentence ct by concatenating the attribute and value terms. To this end, any raw snippet cˆ ∈ Cp , regardless of its original information type (i.e., whether it is an attr"
2020.aacl-main.70,P11-1055,0,0.0590727,"l approach is to utilize a large number of annotated sentence pairs (Chen et al., 2019a) to conduct the training. However, this manual solution is not effective in PQA settings due to the large volume of candidate snippets and the product-specific nature of questions and candidates. Fortunately, we can take advantage of the original user-posted answers to their corresponding questions via a weak supervision paradigm during the training phase which has been successfully applied to provide imperfect labels but with far more less human efforts in many NLP tasks such as knowledge-base completion (Hoffmann et al., 2011) and sentiment analysis (Severyn and Moschitti, 2015b) etc. Given a question q, we have its answer a during the training phase as auxiliary information to obtain the label y for the candidate snippet c. To make use of the information of the whole QA pair, the entire QA pair (q, a) is first fused to an integrated textual (17) where Pre-TE refers to a pre-trained text encoder. We adopt GloVe (Pennington et al., 2014), Elmo (Peters et al., 2018) and BERT (Devlin et al., 2019) in our experiments. cos(·) denotes the cosine similarity score between the two encoded sentence representations. We denote"
2020.aacl-main.70,D14-1181,0,0.00281095,"vely, where dh is the number of hidden units of the LSTM network. Besides the free text part, there are also m aspects for each candidate snippet c. They are useful when measuring the relevance between q and c since they can be regarded as the most salient part of the candidate snippet. Unlike a textual sentence, aspect terms are often quite short, so we directly employ the character-level embedding to transform each aspect term cai to a vector representation denoted as hai : hai = eccai = MaxPool(Conv(cai )) (5) where MaxPool(·) and Conv(·) denote the maxpooling and convolutional operations (Kim, 2014). (2) where ct is the textual sentence of cˆ. Such a unified representation facilitates effective processing of 698 2.2 Question-Snippet Relevance Matching Aspect-enhanced Representations To utilize the aspect information, we design a gated attention mechanism to highlight the relevant information in the question q. Specifically, for the k-th word in the context-aware question representation, denoted as Hkq , we measure the relative importance αk[i] of this word given the i-th aspect term:  exp (Hkq )T hai   αk[i] = Pl (6) q T a q exp (H ) h j=1 j i Loss L y Automatic Label Construction we"
2020.aacl-main.70,C18-1181,0,0.0199203,"cus in the question and snippets. This result shows the necessity of effectively exploring different types of information of the concerned product instead of considering a single information source as in previous works. 4 Question-2: Will this work with my unlocked fire phone i have straight talk i want to switch to the amazon fire phone. Related Work In recent years, many deep learning based methods have been proposed for the answer selection task in community question answering (CQA) platforms. These models can be generally categorized into two types according to their network architecture (Lai et al., 2018), namely Siamese networks (Tan et al., 2016; Mueller and Thyagarajan, 2016) and Compare-Aggregate networks (Wang et al., 2017; Rao et al., 2019; Deng et al., 2020a). Product-related Question Answering (PQA) problem has drawn a lot of attention recently, due to the increasing popularity of online shopping. Most of the existing works utilize reviews as their major information to provide responses for a given question. McAuley and Yang (2016) treat reviews as “experts” to handle the answer selection task. Later, product aspects are considered to further improve the performance (Yu and Lam, 2018)."
2020.aacl-main.70,N19-1423,0,0.103308,"ts into a unified representation composed of a free text sentence and a set of focused aspects. Then for measuring the question-snippet relevance, a gated fusion approach is designed to get aspect-enhanced representations. Also, a question intent analysis module is designed to better determine which information source is more suitable for providing responses. To handle the shortage of labeled data for model training, we develop a weak supervision paradigm making use of the original user-posted answers during training. Some external resources including pre-trained language models such as BERT (Devlin et al., 2019) are utilized to obtain weak supervision signals to facilitate the training process. Our main contributions are as follows: 697 • We explore to utilize heterogeneous information including attribute-value pairs and natural language sentences from both product details and user reviews to tackle the PQA task. • To handle the lack of labeled data, we design an effective weak supervision paradigm making use of available answers in training phase. • Experiments on real-world E-commerce dataset show that our proposed model achieves superior performance over state-of-the-art models. 2 The Proposed Fra"
2020.aacl-main.70,D19-1018,0,0.0227901,"ippets during the testing phase. Thus a simple classifier with a few amount of labeled data can learn to integrate these relevance scores for the construction of “gold” labels with the help of original answers. 700 1 40 questions with their candidate snippets are annotated for this purpose, a SVM classifier is used in our experiment. 3 3.1 Experiments Table 1: Response Selection Performance We perform experiments on real-world data to validate the model effectiveness. The question-answer pairs and reviews are drawn from the Amazon QA dataset (McAuley and Yang, 2016) and Amazon review dataset (Ni et al., 2019). Product details are crawled from the corresponding products’ pages and incorporated into our dataset. In this way, we construct a heterogeneous dataset, which includes in total 5,395 QA pairs of 3,840 products spanning three product categories, namely, “Cell Phones and Accessories”, “Sports and Outdoors” and “Tools and Home Improvement”. For each question, we first utilize the BM25 algorithm to conduct an initial filtering and collect the 50 top-ranked snippets from the corresponding product information as candidate snippets. After discarding empty or meaningless strings, we obtain 219,563 q"
2020.aacl-main.70,D14-1162,0,0.0840205,"igm during the training phase which has been successfully applied to provide imperfect labels but with far more less human efforts in many NLP tasks such as knowledge-base completion (Hoffmann et al., 2011) and sentiment analysis (Severyn and Moschitti, 2015b) etc. Given a question q, we have its answer a during the training phase as auxiliary information to obtain the label y for the candidate snippet c. To make use of the information of the whole QA pair, the entire QA pair (q, a) is first fused to an integrated textual (17) where Pre-TE refers to a pre-trained text encoder. We adopt GloVe (Pennington et al., 2014), Elmo (Peters et al., 2018) and BERT (Devlin et al., 2019) in our experiments. cos(·) denotes the cosine similarity score between the two encoded sentence representations. We denote the computed relevance scores with the aforementioned pre-trained models as s2 , s3 , s4 respectively. After obtaining these relevance signals, a small amount of human-annotated question-snippet pairs are used to train a simple classifier for learning to combine these signals into the single label y 1 . Note that it seems to be unnecessary to design any framework if a simple classifier with a few amount of labeled"
2020.aacl-main.70,N18-1202,0,0.0425005,"ich has been successfully applied to provide imperfect labels but with far more less human efforts in many NLP tasks such as knowledge-base completion (Hoffmann et al., 2011) and sentiment analysis (Severyn and Moschitti, 2015b) etc. Given a question q, we have its answer a during the training phase as auxiliary information to obtain the label y for the candidate snippet c. To make use of the information of the whole QA pair, the entire QA pair (q, a) is first fused to an integrated textual (17) where Pre-TE refers to a pre-trained text encoder. We adopt GloVe (Pennington et al., 2014), Elmo (Peters et al., 2018) and BERT (Devlin et al., 2019) in our experiments. cos(·) denotes the cosine similarity score between the two encoded sentence representations. We denote the computed relevance scores with the aforementioned pre-trained models as s2 , s3 , s4 respectively. After obtaining these relevance signals, a small amount of human-annotated question-snippet pairs are used to train a simple classifier for learning to combine these signals into the single label y 1 . Note that it seems to be unnecessary to design any framework if a simple classifier with a few amount of labeled data and some pre-trained m"
2020.aacl-main.70,D19-1540,0,0.289174,"igure 1: A sample E-commerce product associated with its product details, user reviews, and QA pairs ing the information from both product details and user reviews to obtain relevant snippets serving as responses for improving user satisfaction. This task presents some new research challenges: (i) The heterogeneity of candidate information needs to be appropriately handled. From the above example, we can see that there exists both attributevalue pairs and natural language texts as candidate responses, which implies that typical answer selection approaches (Tan et al., 2016; Wang et al., 2017; Rao et al., 2019) are incapable of handling the concerned task. (ii) Product details and user reviews contain different types of information, which are suitable for answering questions with different information needs. Returning to the example in Figure 1, considering a more subjective question asking about user experience “How is the key travel”, snippets from reviews such as “...good key travel and solid feel..” can provide more appropriate responses. Thus, we can observe that questions with different intents can be better answered by snippets from different sources, which should be exploited when measuring"
2020.aacl-main.70,D15-1044,0,0.022412,"(2019a) propose to tackle PQA task by directly retrieving review sentences as answers. However, it requires a large number of labeled questionreview pairs. Yu et al. (2018b) assume that relevant QA pairs are always available for a given question which can be utilized to provide the responses. Some other works formulate the PQA task as a reading comprehension problem (Xu et al., 2019), where the main focus is to extract a text span as the answer given a relevant review, which is unavailable in many cases. Given some successful applications of text generation models such as text summarization (Rush et al., 2015) and response generation (Tao et al., 2018), some models are proposed to generate an answer sentence (Gao et al., 2019; Chen et al., 2019b) given relevant product information, some later works specifically consider the user opinion information during such generation process (Deng et al., 2020b). Since most product-related questions are looking for diverse answers, we argue that information extracted from reliable sources is more effective and explainable 703 solution for the PQA task. More recently, some studies consider the answer helpfulness prediction task (Zhang et al., 2020b) and answer r"
2020.aacl-main.70,N19-1242,0,0.0420248,"Missing"
2020.aacl-main.70,C18-1186,1,0.740382,", and others can voluntarily answer them. Very often, it takes a long time for an asker to wait for an answer on such platforms. Therefore, automatically providing a proper response to a product-related question can greatly improve user online shopping experience and stimulate purchase decisions. ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). Several efforts have been made to tackle such product-related question answering (PQA) task (McAuley and Yang, 2016; Yu et al., 2018a; Gao et al., 2019; Chen et al., 2019b; Deng et al., 2020b). The existing methods can be generally categorized regarding the involved information source, i.e., from where the responses are obtained. A pioneer work by McAuley and Yang (McAuley and Yang, 2016) investigates answer selection via detecting clues from user reviews. From then on, the review set becomes a commonly used auxiliary information for predicting the answer types or distinguishing true answers from randomly sampled ones (Wan and McAuley, 2016; Yu and Lam, 2018). However, these methods are not feasible for newly-posted questi"
2020.acl-main.119,D15-1198,0,0.0422559,"o (b) obligate-01 ARG2 go-02 po The boy must not go lar ity ? Figure 1: AMR graph construction given the partially constructed graph: (a) one possible expansion resulting in the boy concept. (b) another possible expansion resulting in the - (negation) concept. rocal causation of relation prediction and concept prediction has not been closely-studied and wellutilized. There are also some exceptions staying beyond the above categorization. Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinatory categorical grammar. Groschwitz et al. (2018); Lindemann et al. (2019) view AMR graphs as the structure AM algebra. 3 Motivation Our approach is inspired by the deliberation process when a human expert is deducing a semantic graph from a sentence. The output graph starts from an empty graph and spans incrementally in a node-by-node manner. At any time step of this process, we are distilling the information for the next expansion. We call it expansion because the new node, as an abstract concept of some specific text fragments in the input sentence, is derived to complete som"
2020.acl-main.119,D17-1130,0,0.155698,"s by large margins. 2 Related Work & Background On a coarse-grained level, we can categorize existing AMR parsing approaches into two main classes: Two-stage parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) uses a pipeline design for concept identification and relation prediction, where the concept decisions precede all relation decisions; One-stage parsing constructs a parse graph incrementally. For more fine-grained analysis, those one-stage parsing methods can be further categorized into three types: Transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017; Naseem et al., 2019) processes a sentence from left-to-right and constructs the graph incrementally by alternately inserting a new node or building a new edge. Seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018) views parsing as sequence-to-sequence transduction by some linearization of the AMR graph. The concept and relation prediction are then treated equally with a shared vocabulary. The third class is graph-based parsing (Cai and Lam, 2019; Zhang et al"
2020.acl-main.119,W13-2322,0,0.507454,"nments as latent variables (Lyu and Titov, 2018), attention-based sequenceto-sequence transduction models (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017), and attention-based sequence-to-graph transduction models (Cai and Lam, 2019; Zhang et al., 2019b). Sequence-to-graph transduction models build a semantic graph incrementally via spanning one node at every step. This property is appealing in terms of both computational efficiency and cognitive modeling since it mimics what human experts usually do, i.e., first grasping the core ideas then digging into more details (Banarescu et al., 2013; Cai and Lam, 2019). Unfortunately, the parsing accuracy of existing works including recent state-of-the-arts (Zhang et al., 2019a,b) remain unsatisfactory compared to human-level performance,1 especially in cases where the sentences are rather long and informative, which indicates substantial room for improvement. One possible reason for the deficiency is the inherent defect of one-pass prediction process; that is, the lack of the modeling capability of the interactions between concept prediction and relation prediction, which is critical to achieving fullyinformed and unambiguous decisions."
2020.acl-main.119,S16-1176,0,0.146894,"tially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418) and the Direct Grant of the Faculty of Engineering, CUHK (Project Code: 4055093). Wai Lam The Chinese University of Hong Kong wlam@se.cuhk.edu.hk prediction but also brings a close tie for concept prediction and relation prediction. While most previous works rely on a pre-trained aligner to train a parser, some recent attempts include: modeling the alignments as latent variables (Lyu and Titov, 2018), attention-based sequenceto-sequence transduction models (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017), and attention-based sequence-to-graph transduction models (Cai and Lam, 2019; Zhang et al., 2019b). Sequence-to-graph transduction models build a semantic graph incrementally via spanning one node at every step. This property is appealing in terms of both computational efficiency and cognitive modeling since it mimics what human experts usually do, i.e., first grasping the core ideas then digging into more details (Banarescu et al., 2013; Cai and Lam, 2019). Unfortunately, the parsing accuracy of existing works including recent state-of-the-art"
2020.acl-main.119,D19-1393,1,0.511134,"418) and the Direct Grant of the Faculty of Engineering, CUHK (Project Code: 4055093). Wai Lam The Chinese University of Hong Kong wlam@se.cuhk.edu.hk prediction but also brings a close tie for concept prediction and relation prediction. While most previous works rely on a pre-trained aligner to train a parser, some recent attempts include: modeling the alignments as latent variables (Lyu and Titov, 2018), attention-based sequenceto-sequence transduction models (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017), and attention-based sequence-to-graph transduction models (Cai and Lam, 2019; Zhang et al., 2019b). Sequence-to-graph transduction models build a semantic graph incrementally via spanning one node at every step. This property is appealing in terms of both computational efficiency and cognitive modeling since it mimics what human experts usually do, i.e., first grasping the core ideas then digging into more details (Banarescu et al., 2013; Cai and Lam, 2019). Unfortunately, the parsing accuracy of existing works including recent state-of-the-arts (Zhang et al., 2019a,b) remain unsatisfactory compared to human-level performance,1 especially in cases where the sentences"
2020.acl-main.119,P13-2131,0,0.627,"Missing"
2020.acl-main.119,E17-1051,0,0.39638,"state-of-the-art models by large margins. 2 Related Work & Background On a coarse-grained level, we can categorize existing AMR parsing approaches into two main classes: Two-stage parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) uses a pipeline design for concept identification and relation prediction, where the concept decisions precede all relation decisions; One-stage parsing constructs a parse graph incrementally. For more fine-grained analysis, those one-stage parsing methods can be further categorized into three types: Transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017; Naseem et al., 2019) processes a sentence from left-to-right and constructs the graph incrementally by alternately inserting a new node or building a new edge. Seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018) views parsing as sequence-to-sequence transduction by some linearization of the AMR graph. The concept and relation prediction are then treated equally with a shared vocabulary. The third class is graph-based parsi"
2020.acl-main.119,N19-1423,0,0.272873,"h expansion. 4.2 Sequence Encoder As mentioned above, we employ a sequence encoder to convert the input sentence into vector representations. The sequence encoder follows the multi-layer Transformer architecture described in Vaswani et al. (2017). At the bottom layer, each token is firstly transformed into the concatenation of features learned by a character-level convolutional neural network (charCNN, Kim et al., 2016) and randomly initialized embeddings for its lemma, part-of-speech tag, and named entity tag. Additionally, we also include features learned by pre-trained language model BERT (Devlin et al., 2019).2 Formally, for an input sequence w1 , w2 , . . . , wn with length n, we insert a special token BOS at the beginning of the sequence. For clarity, we omit the detailed transformations (Vaswani et al., 2017) and denote the final output from our sequence encoder as {h0 , h1 , . . . , hn } ∈ Rd , where h0 corresponds the special token BOS and serves as an overall rep2 We obtain word-level representations from pre-trained BERT in the same way as Zhang et al. (2019a,b), where subtoken representations at the last layer are averaged. resentation while others are considered as contextualized word rep"
2020.acl-main.119,P81-1022,0,0.561222,"Missing"
2020.acl-main.119,S16-1186,0,0.0893696,"Missing"
2020.acl-main.119,P14-1134,0,0.654485,"Missing"
2020.acl-main.119,P18-1170,0,0.117667,"Missing"
2020.acl-main.119,P16-1154,0,0.0451178,"kens in the input sequence. We then compute the probability distribution of the new concept label through a hybrid of three channels. First, αt is fed through an MLP and softmax to obtain a probability distribution over a pre-defined vocabulary: MLP(αt ) = (W V h1:n )αt + yt P (vocab) = softmax(W (vocab) (1) (vocab) MLP(αt ) + b ), where W V ∈ Rd×d denotes the learnable linear projection that transforms the text memories into the value subspace, and the value vectors are averaged according to αt for concept label prediction. Second, the attention weights αt directly serve as a copy mechanism (Gu et al., 2016; See et al., 2017), i,e., the probabilities of copying a token lemma from the input text as a node label. Third, to address the attribute values such as person names or numerical strings, we also use αt for another copy mechanism that directly copies the original strings of input tokens. The above three channels are combined via a soft switch to control the production of the concept label from different sources: P (c) =p0 · P (vocab) (c) X X +p1 · ( αt [i]) + p2 · ( αt [i]), i∈L(c) i∈T (c) where [i] indexes the i-th element and L(c) and T (c) are index sets of lemmas and tokens respectively t"
2020.acl-main.119,D18-1198,0,0.286341,"coarse-grained level, we can categorize existing AMR parsing approaches into two main classes: Two-stage parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) uses a pipeline design for concept identification and relation prediction, where the concept decisions precede all relation decisions; One-stage parsing constructs a parse graph incrementally. For more fine-grained analysis, those one-stage parsing methods can be further categorized into three types: Transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017; Naseem et al., 2019) processes a sentence from left-to-right and constructs the graph incrementally by alternately inserting a new node or building a new edge. Seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018) views parsing as sequence-to-sequence transduction by some linearization of the AMR graph. The concept and relation prediction are then treated equally with a shared vocabulary. The third class is graph-based parsing (Cai and Lam, 2019; Zhang et al., 2019b), where at each time step, a"
2020.acl-main.119,P17-1014,0,0.493841,"from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418) and the Direct Grant of the Faculty of Engineering, CUHK (Project Code: 4055093). Wai Lam The Chinese University of Hong Kong wlam@se.cuhk.edu.hk prediction but also brings a close tie for concept prediction and relation prediction. While most previous works rely on a pre-trained aligner to train a parser, some recent attempts include: modeling the alignments as latent variables (Lyu and Titov, 2018), attention-based sequenceto-sequence transduction models (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017), and attention-based sequence-to-graph transduction models (Cai and Lam, 2019; Zhang et al., 2019b). Sequence-to-graph transduction models build a semantic graph incrementally via spanning one node at every step. This property is appealing in terms of both computational efficiency and cognitive modeling since it mimics what human experts usually do, i.e., first grasping the core ideas then digging into more details (Banarescu et al., 2013; Cai and Lam, 2019). Unfortunately, the parsing accuracy of existing works including recent state-of-the-arts (Zhang et al., 2019a"
2020.acl-main.119,P19-1450,0,0.0982572,"struction given the partially constructed graph: (a) one possible expansion resulting in the boy concept. (b) another possible expansion resulting in the - (negation) concept. rocal causation of relation prediction and concept prediction has not been closely-studied and wellutilized. There are also some exceptions staying beyond the above categorization. Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinatory categorical grammar. Groschwitz et al. (2018); Lindemann et al. (2019) view AMR graphs as the structure AM algebra. 3 Motivation Our approach is inspired by the deliberation process when a human expert is deducing a semantic graph from a sentence. The output graph starts from an empty graph and spans incrementally in a node-by-node manner. At any time step of this process, we are distilling the information for the next expansion. We call it expansion because the new node, as an abstract concept of some specific text fragments in the input sentence, is derived to complete some missing elements in the current semantic graph. Specifically, given the input sentence"
2020.acl-main.119,D18-1264,0,0.202336,"el, we can categorize existing AMR parsing approaches into two main classes: Two-stage parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) uses a pipeline design for concept identification and relation prediction, where the concept decisions precede all relation decisions; One-stage parsing constructs a parse graph incrementally. For more fine-grained analysis, those one-stage parsing methods can be further categorized into three types: Transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017; Naseem et al., 2019) processes a sentence from left-to-right and constructs the graph incrementally by alternately inserting a new node or building a new edge. Seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018) views parsing as sequence-to-sequence transduction by some linearization of the AMR graph. The concept and relation prediction are then treated equally with a shared vocabulary. The third class is graph-based parsing (Cai and Lam, 2019; Zhang et al., 2019b), where at each time step, a new node along wi"
2020.acl-main.119,P18-1037,0,0.782,"poses great difficulty in concept ∗ The work described in this paper is substantially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418) and the Direct Grant of the Faculty of Engineering, CUHK (Project Code: 4055093). Wai Lam The Chinese University of Hong Kong wlam@se.cuhk.edu.hk prediction but also brings a close tie for concept prediction and relation prediction. While most previous works rely on a pre-trained aligner to train a parser, some recent attempts include: modeling the alignments as latent variables (Lyu and Titov, 2018), attention-based sequenceto-sequence transduction models (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017), and attention-based sequence-to-graph transduction models (Cai and Lam, 2019; Zhang et al., 2019b). Sequence-to-graph transduction models build a semantic graph incrementally via spanning one node at every step. This property is appealing in terms of both computational efficiency and cognitive modeling since it mimics what human experts usually do, i.e., first grasping the core ideas then digging into more details (Banarescu et al., 2013; Cai and Lam, 2019). Unfo"
2020.acl-main.119,P14-5010,0,0.00324479,"s the individual effect of two model components but also helps facilitate fair comparisons with prior works. 6.1 Experiments Experimental Setup Datasets Our evaluation is conducted on two AMR public releases: AMR 2.0 (LDC0217T10) and AMR 1.0 (LDC2014T12). AMR 2.0 is the latest and largest AMR sembank that was extensively used in recent works. AMR 1.0 shares the same development and test set with AMR, while the size of its training set is only about one-third of AMR 2.0, making it a good testbed to evaluate our model’s sensitivity for data size.6 Implementation Details We use Stanford CoreNLP (Manning et al., 2014) for tokenization, lemmatization, part-of-speech, and named entity tagging. The hyper-parameters of our models are chosen on the development set of AMR 2.0. Without explicit specification, we perform N = 4 steps of iterative inference. Other hyper-parameter settings can be found in the Appendix. Our models are trained using ADAM (Kingma and Ba, 2014) for up to 60K steps (first 50K with the random sibling order and last 10K with deterministic order), with early stopping based on development set performance. We fix BERT parameters similar to Zhang et al. (2019a,b) due to the GPU memory limit. Du"
2020.acl-main.119,P19-1451,0,0.570823,"sing approaches into two main classes: Two-stage parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) uses a pipeline design for concept identification and relation prediction, where the concept decisions precede all relation decisions; One-stage parsing constructs a parse graph incrementally. For more fine-grained analysis, those one-stage parsing methods can be further categorized into three types: Transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017; Naseem et al., 2019) processes a sentence from left-to-right and constructs the graph incrementally by alternately inserting a new node or building a new edge. Seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018) views parsing as sequence-to-sequence transduction by some linearization of the AMR graph. The concept and relation prediction are then treated equally with a shared vocabulary. The third class is graph-based parsing (Cai and Lam, 2019; Zhang et al., 2019b), where at each time step, a new node along with its connections to existing nodes are j"
2020.acl-main.119,S16-1181,0,0.049088,"ssing the previous state-of-the-art models by large margins. 2 Related Work & Background On a coarse-grained level, we can categorize existing AMR parsing approaches into two main classes: Two-stage parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) uses a pipeline design for concept identification and relation prediction, where the concept decisions precede all relation decisions; One-stage parsing constructs a parse graph incrementally. For more fine-grained analysis, those one-stage parsing methods can be further categorized into three types: Transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017; Naseem et al., 2019) processes a sentence from left-to-right and constructs the graph incrementally by alternately inserting a new node or building a new edge. Seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018) views parsing as sequence-to-sequence transduction by some linearization of the AMR graph. The concept and relation prediction are then treated equally with a shared vocabulary. The third clas"
2020.acl-main.119,D17-1129,0,0.0793279,"ize existing AMR parsing approaches into two main classes: Two-stage parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) uses a pipeline design for concept identification and relation prediction, where the concept decisions precede all relation decisions; One-stage parsing constructs a parse graph incrementally. For more fine-grained analysis, those one-stage parsing methods can be further categorized into three types: Transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017; Naseem et al., 2019) processes a sentence from left-to-right and constructs the graph incrementally by alternately inserting a new node or building a new edge. Seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018) views parsing as sequence-to-sequence transduction by some linearization of the AMR graph. The concept and relation prediction are then treated equally with a shared vocabulary. The third class is graph-based parsing (Cai and Lam, 2019; Zhang et al., 2019b), where at each time step, a new node along with its connections t"
2020.acl-main.119,K15-1004,0,0.0487726,"- The current partial (solid) and full (solid + dashed) AMR graphs for the sentence “The boy must no go” (a) G0 ? AR obligate-01 ARG2 go-02 The boy must not go (b) obligate-01 ARG2 go-02 po The boy must not go lar ity ? Figure 1: AMR graph construction given the partially constructed graph: (a) one possible expansion resulting in the boy concept. (b) another possible expansion resulting in the - (negation) concept. rocal causation of relation prediction and concept prediction has not been closely-studied and wellutilized. There are also some exceptions staying beyond the above categorization. Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinatory categorical grammar. Groschwitz et al. (2018); Lindemann et al. (2019) view AMR graphs as the structure AM algebra. 3 Motivation Our approach is inspired by the deliberation process when a human expert is deducing a semantic graph from a sentence. The output graph starts from an empty graph and spans incrementally in a node-by-node manner. At any time step of this process, we are distilling the information for the next"
2020.acl-main.119,P18-1171,0,0.0176619,"; One-stage parsing constructs a parse graph incrementally. For more fine-grained analysis, those one-stage parsing methods can be further categorized into three types: Transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017; Naseem et al., 2019) processes a sentence from left-to-right and constructs the graph incrementally by alternately inserting a new node or building a new edge. Seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018) views parsing as sequence-to-sequence transduction by some linearization of the AMR graph. The concept and relation prediction are then treated equally with a shared vocabulary. The third class is graph-based parsing (Cai and Lam, 2019; Zhang et al., 2019b), where at each time step, a new node along with its connections to existing nodes are jointly decided, either in order (Cai and Lam, 2019) or in parallel (Zhang et al., 2019b). So far, the recipobligate-01 ARG2 AR go-02 G0 boy po lar ity The boy must not go - The current partial (solid) and full (solid + dashed) AMR graphs for the sentence"
2020.acl-main.119,P19-1009,0,0.702176,"Missing"
2020.acl-main.119,E17-1035,0,0.101707,"& Background On a coarse-grained level, we can categorize existing AMR parsing approaches into two main classes: Two-stage parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) uses a pipeline design for concept identification and relation prediction, where the concept decisions precede all relation decisions; One-stage parsing constructs a parse graph incrementally. For more fine-grained analysis, those one-stage parsing methods can be further categorized into three types: Transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017; Naseem et al., 2019) processes a sentence from left-to-right and constructs the graph incrementally by alternately inserting a new node or building a new edge. Seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018) views parsing as sequence-to-sequence transduction by some linearization of the AMR graph. The concept and relation prediction are then treated equally with a shared vocabulary. The third class is graph-based parsing (Cai and Lam, 2019; Zhang et al., 2019b), where at"
2020.acl-main.119,D19-1392,0,0.529317,"Missing"
2020.acl-main.119,D15-1136,0,0.0383567,"“The boy must no go” (a) G0 ? AR obligate-01 ARG2 go-02 The boy must not go (b) obligate-01 ARG2 go-02 po The boy must not go lar ity ? Figure 1: AMR graph construction given the partially constructed graph: (a) one possible expansion resulting in the boy concept. (b) another possible expansion resulting in the - (negation) concept. rocal causation of relation prediction and concept prediction has not been closely-studied and wellutilized. There are also some exceptions staying beyond the above categorization. Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinatory categorical grammar. Groschwitz et al. (2018); Lindemann et al. (2019) view AMR graphs as the structure AM algebra. 3 Motivation Our approach is inspired by the deliberation process when a human expert is deducing a semantic graph from a sentence. The output graph starts from an empty graph and spans incrementally in a node-by-node manner. At any time step of this process, we are distilling the information for the next expansion. We call it expansion because the new node, as an abstract concept of so"
2020.acl-main.119,P17-1099,0,0.0514321,"sequence. We then compute the probability distribution of the new concept label through a hybrid of three channels. First, αt is fed through an MLP and softmax to obtain a probability distribution over a pre-defined vocabulary: MLP(αt ) = (W V h1:n )αt + yt P (vocab) = softmax(W (vocab) (1) (vocab) MLP(αt ) + b ), where W V ∈ Rd×d denotes the learnable linear projection that transforms the text memories into the value subspace, and the value vectors are averaged according to αt for concept label prediction. Second, the attention weights αt directly serve as a copy mechanism (Gu et al., 2016; See et al., 2017), i,e., the probabilities of copying a token lemma from the input text as a node label. Third, to address the attribute values such as person names or numerical strings, we also use αt for another copy mechanism that directly copies the original strings of input tokens. The above three channels are combined via a soft switch to control the production of the concept label from different sources: P (c) =p0 · P (vocab) (c) X X +p1 · ( αt [i]) + p2 · ( αt [i]), i∈L(c) i∈T (c) where [i] indexes the i-th element and L(c) and T (c) are index sets of lemmas and tokens respectively that have the surfac"
2020.acl-main.26,W18-6518,0,0.0159447,"Duan et al., 2017; Yang et al., 2017; Golub et al., 2017), collaborating QA and QG model (Tang et al., 2018, 2017), and unified learning (Xiao et al., 2018). Although question generation has been applied on other datasets, e.g., Wikipedia (Du and Cardie, 2018), most of the existing QG works treat it as a dual task of reading comprehension (Yu et al., 2018; Cui et al., 2017), namely generating a question from a piece of text where a certain text span is marked as answer, in spite of several exceptions where only sentences without answer spans are used for generating questions (Du et al., 2017; Chali and Baghaee, 2018). Such generation setting is not suitable for reviews due to the lack of (question, review) pairs and improper assumption of text span answer as aforementioned. There are works training the question generation model with the user-written QA pairs in E-commerce sites (Hu et al., 2018; Chali and Baghaee, 2018), but the practicality is limited since the questions are only generated from answers instead of reviews. Transfer learning (Pan and Yang, 2009; Tan et al., 2017; Li et al., 2020) refers to a broad scope of methods that exploit knowledge across domains for handling tasks in the target domai"
2020.acl-main.26,D13-1172,0,0.0287804,"on. To generate to the point questions about the major aspects in reviews, related features extracted in an unsupervised manner are incorporated without the burden of aspect annotation. Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task. 1 Introduction The user-written reviews for products or service have become an important information source and there are a few research areas analyzing such data, including aspect extraction (Bing et al., 2016; Chen et al., 2013), product recommendation (Chelliah and Sarkar, 2017), and sentiment analysis (Li et al., 2018; Zhao et al., 2018a). Reviews reflect certain concerns or experiences of users on products or services, and such information is valuable for other ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). † The work was done when Qian Yu was an intern at Alibaba. potential consumers. However, there are few mechanisms assisting users for efficient review digestion. It is time-consu"
2020.acl-main.26,P17-1055,0,0.0299294,"pans for facilitating the learning (Wang et al., 2019). Question generation models can be combined with its dual task, i.e., reading comprehension or question answering with various motivations, such as improving auxiliary task performance (Duan et al., 2017; Yang et al., 2017; Golub et al., 2017), collaborating QA and QG model (Tang et al., 2018, 2017), and unified learning (Xiao et al., 2018). Although question generation has been applied on other datasets, e.g., Wikipedia (Du and Cardie, 2018), most of the existing QG works treat it as a dual task of reading comprehension (Yu et al., 2018; Cui et al., 2017), namely generating a question from a piece of text where a certain text span is marked as answer, in spite of several exceptions where only sentences without answer spans are used for generating questions (Du et al., 2017; Chali and Baghaee, 2018). Such generation setting is not suitable for reviews due to the lack of (question, review) pairs and improper assumption of text span answer as aforementioned. There are works training the question generation model with the user-written QA pairs in E-commerce sites (Hu et al., 2018; Chali and Baghaee, 2018), but the practicality is limited since the"
2020.acl-main.26,D17-1219,0,0.102854,"arning is categorized into four groups (Pan and Yang, 2009), namely instance transfer, feature representation transfer, parameter transfer, and relational knowledge transfer. Our learning framework can be regarded as a case of instance transfer with iterative instance adaptation and augmentation. Related Work Question generation (QG) is an emerging research topic due to its wide application scenarios such as education (Wang et al., 2018), goal-oriented dialogue (Lee et al., 2018), and question answering (Duan et al., 2017). The preliminary neural QG models (Du et al., 2017; Zhou et al., 2017; Du and Cardie, 2017) outperform the rule-based methods relying on hand-craft features, and thereafter various models have been proposed to further improve the performance via incorporating question type (Dong et al., 2018), answer position (Sun et al., 2018), long passage modeling (Zhao et al., 2018b), question difficulty (Gao et al., 2019), and to the point context (Li et al., 2019). Some works try to find the possible answer text spans for facilitating the learning (Wang et al., 2019). Question generation models can be combined with its dual task, i.e., reading comprehension or question answering with various m"
2020.acl-main.26,P18-1177,0,0.112982,"Missing"
2020.acl-main.26,P17-1123,0,0.468063,"s or experiences of users on products or services, and such information is valuable for other ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). † The work was done when Qian Yu was an intern at Alibaba. potential consumers. However, there are few mechanisms assisting users for efficient review digestion. It is time-consuming for users to locate critical review parts that they care about, particularly in long reviews. We propose to utilize question generation (QG) (Du et al., 2017) as a new means to overcome this problem. Specifically, given a review sentence, the generated question is expected to ask about the concerned aspect of this product, from the perspective of the review writer. Such question can be regarded as a reading anchor of the review sentence, and it is easier to view and conceive due to its concise form. As an example, the review for a battery case product in Table 1 is too long to find sentences that can answer a user question such as “How long will the battery last?”. Given the generated questions in the right column, it would be much easier to find o"
2020.acl-main.26,D19-1317,1,0.843323,"to its wide application scenarios such as education (Wang et al., 2018), goal-oriented dialogue (Lee et al., 2018), and question answering (Duan et al., 2017). The preliminary neural QG models (Du et al., 2017; Zhou et al., 2017; Du and Cardie, 2017) outperform the rule-based methods relying on hand-craft features, and thereafter various models have been proposed to further improve the performance via incorporating question type (Dong et al., 2018), answer position (Sun et al., 2018), long passage modeling (Zhao et al., 2018b), question difficulty (Gao et al., 2019), and to the point context (Li et al., 2019). Some works try to find the possible answer text spans for facilitating the learning (Wang et al., 2019). Question generation models can be combined with its dual task, i.e., reading comprehension or question answering with various motivations, such as improving auxiliary task performance (Duan et al., 2017; Yang et al., 2017; Golub et al., 2017), collaborating QA and QG model (Tang et al., 2018, 2017), and unified learning (Xiao et al., 2018). Although question generation has been applied on other datasets, e.g., Wikipedia (Du and Cardie, 2018), most of the existing QG works treat it as a du"
2020.acl-main.26,P18-1087,1,0.843326,"acted in an unsupervised manner are incorporated without the burden of aspect annotation. Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task. 1 Introduction The user-written reviews for products or service have become an important information source and there are a few research areas analyzing such data, including aspect extraction (Bing et al., 2016; Chen et al., 2013), product recommendation (Chelliah and Sarkar, 2017), and sentiment analysis (Li et al., 2018; Zhao et al., 2018a). Reviews reflect certain concerns or experiences of users on products or services, and such information is valuable for other ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). † The work was done when Qian Yu was an intern at Alibaba. potential consumers. However, there are few mechanisms assisting users for efficient review digestion. It is time-consuming for users to locate critical review parts that they care about, particularly in long rev"
2020.acl-main.26,D14-1162,0,0.0844023,"Missing"
2020.acl-main.26,D17-1090,0,0.079253,"r to view and conceive due to its concise form. As an example, the review for a battery case product in Table 1 is too long to find sentences that can answer a user question such as “How long will the battery last?”. Given the generated questions in the right column, it would be much easier to find out the helpful part of the review. Recently, as a topic attracting significant research attention, question generation is regarded as a dual task of reading comprehension in most works, namely generating a question from a sentence with a fixed text segment in the sentence designated as the answer (Duan et al., 2017; Sun et al., 2018). Two unique characteristics of our review-based question generation task differentiate it from the previous question generation works. First, there is no review-question pairs available for training, thus a simple Seq2Seq-based question generation model for learning the mapping from the input (i.e. review) to the output (i.e. question) cannot be applied. Even though we can easily obtain large volumes of user-posed review sets and question sets, they are just separate datasets and cannot provide any supervision of input-output mapping (i.e. reviewquestion pair). The second o"
2020.acl-main.26,D18-1427,0,0.230602,"ive due to its concise form. As an example, the review for a battery case product in Table 1 is too long to find sentences that can answer a user question such as “How long will the battery last?”. Given the generated questions in the right column, it would be much easier to find out the helpful part of the review. Recently, as a topic attracting significant research attention, question generation is regarded as a dual task of reading comprehension in most works, namely generating a question from a sentence with a fixed text segment in the sentence designated as the answer (Duan et al., 2017; Sun et al., 2018). Two unique characteristics of our review-based question generation task differentiate it from the previous question generation works. First, there is no review-question pairs available for training, thus a simple Seq2Seq-based question generation model for learning the mapping from the input (i.e. review) to the output (i.e. question) cannot be applied. Even though we can easily obtain large volumes of user-posed review sets and question sets, they are just separate datasets and cannot provide any supervision of input-output mapping (i.e. reviewquestion pair). The second one is that differen"
2020.acl-main.26,D17-1087,0,0.0166934,"have been proposed to further improve the performance via incorporating question type (Dong et al., 2018), answer position (Sun et al., 2018), long passage modeling (Zhao et al., 2018b), question difficulty (Gao et al., 2019), and to the point context (Li et al., 2019). Some works try to find the possible answer text spans for facilitating the learning (Wang et al., 2019). Question generation models can be combined with its dual task, i.e., reading comprehension or question answering with various motivations, such as improving auxiliary task performance (Duan et al., 2017; Yang et al., 2017; Golub et al., 2017), collaborating QA and QG model (Tang et al., 2018, 2017), and unified learning (Xiao et al., 2018). Although question generation has been applied on other datasets, e.g., Wikipedia (Du and Cardie, 2018), most of the existing QG works treat it as a dual task of reading comprehension (Yu et al., 2018; Cui et al., 2017), namely generating a question from a piece of text where a certain text span is marked as answer, in spite of several exceptions where only sentences without answer spans are used for generating questions (Du et al., 2017; Chali and Baghaee, 2018). Such generation setting is not"
2020.acl-main.26,P17-1036,0,0.153488,"spect Extraction. Product aspects usually play a major role in all of product questions, answers and reviews, since they are the discussion focus of such text content. Thus, such aspects can act as connections in modeling input pairs of qa and r via the partially shared structure. To help the semantic vector hα in Eqn 3 capture salient aspects of reviews, an autoencoder module is connected to the encoding layer for reconstructing hα . Together with the matrix M, the autoencoder can be used to extract salient aspects from reviews. Note that this combined structure is similar to the ABAE model (He et al., 2017), which has been shown effective for unsupervised aspect extraction. Compared with supervised aspect detection methods, such a unsupervised module avoid the burden of aspect annotation for different product categories, and our experiments demonstrate that regularization based on this module is effective. Specifically, hα is mapped to an aspect distribution pα and then reconstructed: pα = softmax(Wp · hα + bp ) α0 h α =p ·A We adapt the Seq2Seq model for the aspect-focused generation model, which is updated gradually via the transferred and augmented instances. With the help of aspect-based var"
2020.acl-main.26,N18-2072,0,0.0266661,"questions. The second challenge, namely the issue that some verbose answers contain irrelevant content especially for subjective questions. To handle this challenge, we propose a learning framework with adaptive instance transfer and augmentation. Firstly, a pre-trained generation model based on user-posed answer-question pairs is utilized as an initial question generator. A ranker is designed to work together with the generator to improve the training instance set by distilling it via removing unsuitable answer-question pairs to avoid “negative transfer” (Pan and Yang, 2009), and augmenting (Kobayashi, 2018) it by adding suitable reviewquestion pairs. For selecting suitable reviews for question generation, the ranker considers two factors: the major aspects in a review and the review’s suitability for question generation. The two factors are captured via a reconstruction objective and a reinforcement objective with reward given by the generator. Thus, the ranker and the generator are iteratively enhanced, and the adaptively transferred answer-question pairs and the augmented reviewquestion pairs gradually relieve the data lacking problem. In accordance with the second characteristic of our task,"
2020.acl-main.26,N18-1141,0,0.0350535,"Missing"
2020.acl-main.26,P15-1060,0,0.0440336,"Missing"
2020.acl-main.26,P17-1096,0,0.0171941,"fter various models have been proposed to further improve the performance via incorporating question type (Dong et al., 2018), answer position (Sun et al., 2018), long passage modeling (Zhao et al., 2018b), question difficulty (Gao et al., 2019), and to the point context (Li et al., 2019). Some works try to find the possible answer text spans for facilitating the learning (Wang et al., 2019). Question generation models can be combined with its dual task, i.e., reading comprehension or question answering with various motivations, such as improving auxiliary task performance (Duan et al., 2017; Yang et al., 2017; Golub et al., 2017), collaborating QA and QG model (Tang et al., 2018, 2017), and unified learning (Xiao et al., 2018). Although question generation has been applied on other datasets, e.g., Wikipedia (Du and Cardie, 2018), most of the existing QG works treat it as a dual task of reading comprehension (Yu et al., 2018; Cui et al., 2017), namely generating a question from a piece of text where a certain text span is marked as answer, in spite of several exceptions where only sentences without answer spans are used for generating questions (Du et al., 2017; Chali and Baghaee, 2018). Such gener"
2020.acl-main.26,D18-1424,0,0.396601,"ervised manner are incorporated without the burden of aspect annotation. Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task. 1 Introduction The user-written reviews for products or service have become an important information source and there are a few research areas analyzing such data, including aspect extraction (Bing et al., 2016; Chen et al., 2013), product recommendation (Chelliah and Sarkar, 2017), and sentiment analysis (Li et al., 2018; Zhao et al., 2018a). Reviews reflect certain concerns or experiences of users on products or services, and such information is valuable for other ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). † The work was done when Qian Yu was an intern at Alibaba. potential consumers. However, there are few mechanisms assisting users for efficient review digestion. It is time-consuming for users to locate critical review parts that they care about, particularly in long reviews. We propose to"
2020.coling-main.215,W05-0909,0,0.0939199,"Michigan&gt;, &lt;James Craig Watson, deathCause, Peritonitis&gt;} &lt;James Craig Watson, nationality, Canada&gt;, 0 &lt;James Craig Watson, deathPlace, Madison , Wisconsin&gt;} Figure 3: Topic distribution at each step. White stands for higher probability while black stands for lower. the number of topics is set to 100. In the T2S model, we set the number of topics to 200. In the DTT model, we set the number of topics to 500. We evaluate all the models with the same evaluation script 4 . Several metrics are evaluated, including BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST L (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and CIDEr (Vedantam et al., 2015). Since some metrics are sensitive to randomness, we run each model for 5 times and report the median score with the standard deviation. 4.4 Results The experimental results are shown in Table 2. We can observe that our DTT model outperforms all comparison models significantly and consistently. It illustrates that the DTT model can capture the dynamic topic information to mitigate the off-topic problem and thus improves the overall generation performance. Besides, our DTT model not only improves performance but also improves the stability of the performance. I"
2020.coling-main.215,2016.amta-researchers.10,0,0.298795,"here is no existing dataset containing the topic annotations. Therefore, it is difficult to adopt supervised learning approaches for detecting topics. Moreover, it is even more expensive to annotate the dynamic change of topic information in one sentence, as exemplified by the topic changes from “person” to “company” in Figure 1. Therefore, we investigate the task of automatically detecting the hidden topic information and incorporating such information for the generation of sentences. Many works have been proposed to utilize the static topic information to improve the generation performance. Chen et al. (2016) and Ou et al. (2018) propose to represent the topic for each sentence as a learnable vector. The topic is predicted by the input sentence and is used to enhance the generating phase. Xing et al. (2017) and Zhang et al. (2016) detect the topic representation by applying a pre-trained LDA model on the input sequence. Moreover, Choudhary et al. (2017) and Ou et al. (2018) predict the topic representation directly from the input sequence using Recurrent Neural Networks (RNN). All the above methods make an assumption that during generation the topic does not change so as to make the problem tracta"
2020.coling-main.215,2020.emnlp-main.90,1,0.65436,"GTR-LSTM model to encode not only the triple information, but also the structure information of the entity graph into hidden semantic space. Jain et al. (2018) exploit a mixed hierarchical attention based encoder-decoder model to leverage the structure and content information. Shimorina and Gardent (2018) propose to use delexicalization and copy mechanism to enhance the performance of the sequence-to-sequence framework. Konstas and Lapata (2013) and Wiseman et al. (2018) propose to use template based methods to generate the text by using the extracted template information in the training set. Cheng et al. (2020) propose to generate text description for entities by utilizing the knowledge distilled from the existing knowledge base. However, none of the above works consider the topic information in the KB-to-text generation process and thus not directly comparable to our work proposed in this paper. Some works in text generation (Gatt and Krahmer, 2018) have been proposed to incorporate the topic information to help generate the text. These ideas can be adopted in KB-to-text generation. Tars and Fishel (2018) and Johnson et al. (2017) add an extra topic tag into the source sentence for incorporating th"
2020.coling-main.215,E17-1060,0,0.117272,"den topic representation and it calculates the most suitable local topic representation for each local topic state. The topic memory is used to memorize the previous local topic representation and computes the dynamic topic state for each generation step to guide the target sentence generation procedure. 2 Related Work Recently, various data-to-text tasks have been proposed handling different kinds of data. Gardent et al. (2017a; Gardent et al. (2017b) construct the WebNLG dataset which aims at generating text descriptions based on DBpedia (Auer et al., 2007) triples. Lebret et al. (2016) and Chisholm et al. (2017) propose to generate a person’s biography based on Wikipedia’s infobox. Fu et al. (2020a) build the WikiEvent dataset aiming at generating text based on an event chain. Novikova et al. (2017) generate restaurant reviews based on the information of restaurant attributes. Wiseman et al. (2017) generate basketball match descriptions based on the game records. Moreover, Fu et al. (2020c) propose to directly train the model on partially-aligned data called WITA while Fu et al. (2020b) propose to train a model based on purely unaligned data unsupervised with a dual learning framework. All of the abo"
2020.coling-main.215,2020.aacl-main.29,1,0.83056,"ch local topic state. The topic memory is used to memorize the previous local topic representation and computes the dynamic topic state for each generation step to guide the target sentence generation procedure. 2 Related Work Recently, various data-to-text tasks have been proposed handling different kinds of data. Gardent et al. (2017a; Gardent et al. (2017b) construct the WebNLG dataset which aims at generating text descriptions based on DBpedia (Auer et al., 2007) triples. Lebret et al. (2016) and Chisholm et al. (2017) propose to generate a person’s biography based on Wikipedia’s infobox. Fu et al. (2020a) build the WikiEvent dataset aiming at generating text based on an event chain. Novikova et al. (2017) generate restaurant reviews based on the information of restaurant attributes. Wiseman et al. (2017) generate basketball match descriptions based on the game records. Moreover, Fu et al. (2020c) propose to directly train the model on partially-aligned data called WITA while Fu et al. (2020b) propose to train a model based on purely unaligned data unsupervised with a dual learning framework. All of the above problems aim at 2370 converting some formatted data into natural language texts faci"
2020.coling-main.215,2020.emnlp-main.738,1,0.786987,"ch local topic state. The topic memory is used to memorize the previous local topic representation and computes the dynamic topic state for each generation step to guide the target sentence generation procedure. 2 Related Work Recently, various data-to-text tasks have been proposed handling different kinds of data. Gardent et al. (2017a; Gardent et al. (2017b) construct the WebNLG dataset which aims at generating text descriptions based on DBpedia (Auer et al., 2007) triples. Lebret et al. (2016) and Chisholm et al. (2017) propose to generate a person’s biography based on Wikipedia’s infobox. Fu et al. (2020a) build the WikiEvent dataset aiming at generating text based on an event chain. Novikova et al. (2017) generate restaurant reviews based on the information of restaurant attributes. Wiseman et al. (2017) generate basketball match descriptions based on the game records. Moreover, Fu et al. (2020c) propose to directly train the model on partially-aligned data called WITA while Fu et al. (2020b) propose to train a model based on purely unaligned data unsupervised with a dual learning framework. All of the above problems aim at 2370 converting some formatted data into natural language texts faci"
2020.coling-main.215,P17-1017,0,0.527677,"luding question answering and recommendation systems have benefited from KBs (Wang et al., 2017) as external knowledge sources to improve the results. Though KBs have achieved great success in supporting and improving various text mining tasks, they are still incomprehensible to humans due to the over-rigid structured format. Reading a bunch of triples always annoys people since the form is not easily understandable especially to people who have never heard about KBs. In order to address this problem, recently some researchers have proposed the KB-to-text generation task (Lebret et al., 2016; Gardent et al., 2017a; Gardent et al., 2017b) to bridge the gap between KBs and natural language. This KB-to-text generation problem aims at directly converting a group of KBs triples into human-readable sentences. For example, given a triple group ( hBill Gates, BirthPlace, Seattle i, hBill Gates, FounderOf, Microsoft i), the goal is to generate a comprehensible sentence such as “Bill Gates, the founder of Microsoft, was born in Seattle.” Some works employ the techniques in the text generation area (Gatt and Krahmer, 2018) to tackle the KB-to-text problem. Though these models have achieved some success, there ar"
2020.coling-main.215,W17-3518,0,0.45507,"luding question answering and recommendation systems have benefited from KBs (Wang et al., 2017) as external knowledge sources to improve the results. Though KBs have achieved great success in supporting and improving various text mining tasks, they are still incomprehensible to humans due to the over-rigid structured format. Reading a bunch of triples always annoys people since the form is not easily understandable especially to people who have never heard about KBs. In order to address this problem, recently some researchers have proposed the KB-to-text generation task (Lebret et al., 2016; Gardent et al., 2017a; Gardent et al., 2017b) to bridge the gap between KBs and natural language. This KB-to-text generation problem aims at directly converting a group of KBs triples into human-readable sentences. For example, given a triple group ( hBill Gates, BirthPlace, Seattle i, hBill Gates, FounderOf, Microsoft i), the goal is to generate a comprehensible sentence such as “Bill Gates, the founder of Microsoft, was born in Seattle.” Some works employ the techniques in the text generation area (Gatt and Krahmer, 2018) to tackle the KB-to-text problem. Though these models have achieved some success, there ar"
2020.coling-main.215,N18-2098,0,0.013554,"ation of the KBs. Chisholm et al. (2017) propose to directly rank the triples by relation frequency and flatten the triples to pure text. The flattened text is used as the input for a sequence-to-sequence model to generate the output text. Vougiouklis et al. (2018) propose to use a triple encoder to encode each triple into a hidden vector. The decoder input is constructed by simply concatenating all of the hidden vectors. Trisedya et al. (2018) propose a GTR-LSTM model to encode not only the triple information, but also the structure information of the entity graph into hidden semantic space. Jain et al. (2018) exploit a mixed hierarchical attention based encoder-decoder model to leverage the structure and content information. Shimorina and Gardent (2018) propose to use delexicalization and copy mechanism to enhance the performance of the sequence-to-sequence framework. Konstas and Lapata (2013) and Wiseman et al. (2018) propose to use template based methods to generate the text by using the extracted template information in the training set. Cheng et al. (2020) propose to generate text description for entities by utilizing the knowledge distilled from the existing knowledge base. However, none of t"
2020.coling-main.215,P17-4012,0,0.115144,"the above works consider the topic information in the KB-to-text generation process and thus not directly comparable to our work proposed in this paper. Some works in text generation (Gatt and Krahmer, 2018) have been proposed to incorporate the topic information to help generate the text. These ideas can be adopted in KB-to-text generation. Tars and Fishel (2018) and Johnson et al. (2017) add an extra topic tag into the source sentence for incorporating the topic information into the model. The whole model is built based on the sequence-to-sequence (Sutskever et al., 2014; Cho et al., 2014; Klein et al., 2017) framework with standard attention (Bahdanau et al., 2014; Luong et al., 2015). Mikolov and Zweig (2012) as well as Liu et al. (2015) propose to use the topic information as extra features to enhance the performance of the language model and word embedding. Chen et al. (2016) and Ou et al. (2018) use the same idea to utilize the topic feature to enhance the generation of the text. However, all these methods assume that the topic information is known in advance. Some methods investigate the problem setting that the topic information is not given and needs to be detected. For example, the topic"
2020.coling-main.215,D16-1128,0,0.455919,"2014). Many tasks including question answering and recommendation systems have benefited from KBs (Wang et al., 2017) as external knowledge sources to improve the results. Though KBs have achieved great success in supporting and improving various text mining tasks, they are still incomprehensible to humans due to the over-rigid structured format. Reading a bunch of triples always annoys people since the form is not easily understandable especially to people who have never heard about KBs. In order to address this problem, recently some researchers have proposed the KB-to-text generation task (Lebret et al., 2016; Gardent et al., 2017a; Gardent et al., 2017b) to bridge the gap between KBs and natural language. This KB-to-text generation problem aims at directly converting a group of KBs triples into human-readable sentences. For example, given a triple group ( hBill Gates, BirthPlace, Seattle i, hBill Gates, FounderOf, Microsoft i), the goal is to generate a comprehensible sentence such as “Bill Gates, the founder of Microsoft, was born in Seattle.” Some works employ the techniques in the text generation area (Gatt and Krahmer, 2018) to tackle the KB-to-text problem. Though these models have achieved"
2020.coling-main.215,W04-1013,0,0.0675852,", &lt;James Craig Watson, almaMater, University of Michigan&gt;, &lt;James Craig Watson, deathCause, Peritonitis&gt;} &lt;James Craig Watson, nationality, Canada&gt;, 0 &lt;James Craig Watson, deathPlace, Madison , Wisconsin&gt;} Figure 3: Topic distribution at each step. White stands for higher probability while black stands for lower. the number of topics is set to 100. In the T2S model, we set the number of topics to 200. In the DTT model, we set the number of topics to 500. We evaluate all the models with the same evaluation script 4 . Several metrics are evaluated, including BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST L (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and CIDEr (Vedantam et al., 2015). Since some metrics are sensitive to randomness, we run each model for 5 times and report the median score with the standard deviation. 4.4 Results The experimental results are shown in Table 2. We can observe that our DTT model outperforms all comparison models significantly and consistently. It illustrates that the DTT model can capture the dynamic topic information to mitigate the off-topic problem and thus improves the overall generation performance. Besides, our DTT model not only improves per"
2020.coling-main.215,D15-1166,0,0.599629,"rocess and thus not directly comparable to our work proposed in this paper. Some works in text generation (Gatt and Krahmer, 2018) have been proposed to incorporate the topic information to help generate the text. These ideas can be adopted in KB-to-text generation. Tars and Fishel (2018) and Johnson et al. (2017) add an extra topic tag into the source sentence for incorporating the topic information into the model. The whole model is built based on the sequence-to-sequence (Sutskever et al., 2014; Cho et al., 2014; Klein et al., 2017) framework with standard attention (Bahdanau et al., 2014; Luong et al., 2015). Mikolov and Zweig (2012) as well as Liu et al. (2015) propose to use the topic information as extra features to enhance the performance of the language model and word embedding. Chen et al. (2016) and Ou et al. (2018) use the same idea to utilize the topic feature to enhance the generation of the text. However, all these methods assume that the topic information is known in advance. Some methods investigate the problem setting that the topic information is not given and needs to be detected. For example, the topic information can be detected from Latent Dirichlet Allocation (LDA). Zhang et a"
2020.coling-main.215,W17-5525,0,0.197912,"Missing"
2020.coling-main.215,P02-1040,0,0.107999,"aMater, University of Michigan&gt;, &lt;James Craig Watson, almaMater, University of Michigan&gt;, &lt;James Craig Watson, deathCause, Peritonitis&gt;} &lt;James Craig Watson, nationality, Canada&gt;, 0 &lt;James Craig Watson, deathPlace, Madison , Wisconsin&gt;} Figure 3: Topic distribution at each step. White stands for higher probability while black stands for lower. the number of topics is set to 100. In the T2S model, we set the number of topics to 200. In the DTT model, we set the number of topics to 500. We evaluate all the models with the same evaluation script 4 . Several metrics are evaluated, including BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST L (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and CIDEr (Vedantam et al., 2015). Since some metrics are sensitive to randomness, we run each model for 5 times and report the median score with the standard deviation. 4.4 Results The experimental results are shown in Table 2. We can observe that our DTT model outperforms all comparison models significantly and consistently. It illustrates that the DTT model can capture the dynamic topic information to mitigate the off-topic problem and thus improves the overall generation performance. Besides, our DTT model no"
2020.coling-main.215,W18-6543,0,0.563828,"The flattened text is used as the input for a sequence-to-sequence model to generate the output text. Vougiouklis et al. (2018) propose to use a triple encoder to encode each triple into a hidden vector. The decoder input is constructed by simply concatenating all of the hidden vectors. Trisedya et al. (2018) propose a GTR-LSTM model to encode not only the triple information, but also the structure information of the entity graph into hidden semantic space. Jain et al. (2018) exploit a mixed hierarchical attention based encoder-decoder model to leverage the structure and content information. Shimorina and Gardent (2018) propose to use delexicalization and copy mechanism to enhance the performance of the sequence-to-sequence framework. Konstas and Lapata (2013) and Wiseman et al. (2018) propose to use template based methods to generate the text by using the extracted template information in the training set. Cheng et al. (2020) propose to generate text description for entities by utilizing the knowledge distilled from the existing knowledge base. However, none of the above works consider the topic information in the KB-to-text generation process and thus not directly comparable to our work proposed in this pa"
2020.coling-main.215,P18-1151,0,0.0130424,"ing some formatted data into natural language texts facilitating more understandability. Some models have proposed to solve the KB-to-text problem by utilizing various information of the KBs. Chisholm et al. (2017) propose to directly rank the triples by relation frequency and flatten the triples to pure text. The flattened text is used as the input for a sequence-to-sequence model to generate the output text. Vougiouklis et al. (2018) propose to use a triple encoder to encode each triple into a hidden vector. The decoder input is constructed by simply concatenating all of the hidden vectors. Trisedya et al. (2018) propose a GTR-LSTM model to encode not only the triple information, but also the structure information of the entity graph into hidden semantic space. Jain et al. (2018) exploit a mixed hierarchical attention based encoder-decoder model to leverage the structure and content information. Shimorina and Gardent (2018) propose to use delexicalization and copy mechanism to enhance the performance of the sequence-to-sequence framework. Konstas and Lapata (2013) and Wiseman et al. (2018) propose to use template based methods to generate the text by using the extracted template information in the tra"
2020.coling-main.215,P19-1240,0,0.0284316,"well as Liu et al. (2015) propose to use the topic information as extra features to enhance the performance of the language model and word embedding. Chen et al. (2016) and Ou et al. (2018) use the same idea to utilize the topic feature to enhance the generation of the text. However, all these methods assume that the topic information is known in advance. Some methods investigate the problem setting that the topic information is not given and needs to be detected. For example, the topic information can be detected from Latent Dirichlet Allocation (LDA). Zhang et al. (2016; Dziri et al. (2018; Wang et al. (2019) detect the topic distribution of words via topic model to enhance the translation procedure. Xing et al. (2017) propose a TA-Seq2Seq framework which uses the word topic information from LDA to generate the responses in chatbot dialog systems. Moreover, some researchers propose to directly detect the topic vector from the input sentences in the sequence-to-sequence framework. For example, Choudhary et al. (2017) propose to train a classifier to predict the topic of the source sentence and use it to help generate the dialog response. Ou et al. (2018) also propose to predict the topic vector dir"
2020.coling-main.215,D17-1239,0,0.0179987,"n procedure. 2 Related Work Recently, various data-to-text tasks have been proposed handling different kinds of data. Gardent et al. (2017a; Gardent et al. (2017b) construct the WebNLG dataset which aims at generating text descriptions based on DBpedia (Auer et al., 2007) triples. Lebret et al. (2016) and Chisholm et al. (2017) propose to generate a person’s biography based on Wikipedia’s infobox. Fu et al. (2020a) build the WikiEvent dataset aiming at generating text based on an event chain. Novikova et al. (2017) generate restaurant reviews based on the information of restaurant attributes. Wiseman et al. (2017) generate basketball match descriptions based on the game records. Moreover, Fu et al. (2020c) propose to directly train the model on partially-aligned data called WITA while Fu et al. (2020b) propose to train a model based on purely unaligned data unsupervised with a dual learning framework. All of the above problems aim at 2370 converting some formatted data into natural language texts facilitating more understandability. Some models have proposed to solve the KB-to-text problem by utilizing various information of the KBs. Chisholm et al. (2017) propose to directly rank the triples by relati"
2020.coling-main.215,D18-1356,0,0.016437,"triple into a hidden vector. The decoder input is constructed by simply concatenating all of the hidden vectors. Trisedya et al. (2018) propose a GTR-LSTM model to encode not only the triple information, but also the structure information of the entity graph into hidden semantic space. Jain et al. (2018) exploit a mixed hierarchical attention based encoder-decoder model to leverage the structure and content information. Shimorina and Gardent (2018) propose to use delexicalization and copy mechanism to enhance the performance of the sequence-to-sequence framework. Konstas and Lapata (2013) and Wiseman et al. (2018) propose to use template based methods to generate the text by using the extracted template information in the training set. Cheng et al. (2020) propose to generate text description for entities by utilizing the knowledge distilled from the existing knowledge base. However, none of the above works consider the topic information in the KB-to-text generation process and thus not directly comparable to our work proposed in this paper. Some works in text generation (Gatt and Krahmer, 2018) have been proposed to incorporate the topic information to help generate the text. These ideas can be adopted"
2020.coling-main.215,C16-1170,0,0.366443,"c information in one sentence, as exemplified by the topic changes from “person” to “company” in Figure 1. Therefore, we investigate the task of automatically detecting the hidden topic information and incorporating such information for the generation of sentences. Many works have been proposed to utilize the static topic information to improve the generation performance. Chen et al. (2016) and Ou et al. (2018) propose to represent the topic for each sentence as a learnable vector. The topic is predicted by the input sentence and is used to enhance the generating phase. Xing et al. (2017) and Zhang et al. (2016) detect the topic representation by applying a pre-trained LDA model on the input sequence. Moreover, Choudhary et al. (2017) and Ou et al. (2018) predict the topic representation directly from the input sequence using Recurrent Neural Networks (RNN). All the above methods make an assumption that during generation the topic does not change so as to make the problem tractable, which scarifies the advantage of modeling the dynamic nature of topic information. We propose a novel Dynamic Topic Tracker (DTT) neural model to tackle the problem. Different from existing models, our proposed DTT model"
2020.coling-main.437,C18-1279,1,0.88611,"Missing"
2020.coling-main.437,W15-4640,0,0.503492,"e proposed method substantially and consistently outperforms existing state-of-the-art methods on three multi-turn response selection benchmark datasets. 1 Introduction Recent years have witnessed many successful real-world applications on chatbots and AI assistants, such as the XiaoIce (Shum et al., 2018) from Microsoft and the E-commerce assistant AliMe (Li et al., 2017) from Alibaba Group, which owe to the extensive researches on dialogue systems. Existing works on building conversational models mainly study generation-based (Wen et al., 2017; Xing et al., 2017) or retrieval-based methods (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). In this work, we focus on the problem of multi-turn response selection for retrieval-based dialogue systems, which aims at selecting appropriate responses from a set of candidates as the reply for the given multi-turn utterances. Measuring the matching degree between the utterance context and the candidate response is the core of multi-turn response selection task. Recent works develop a variety of interaction model to enhance the utterance-response interaction from a broader (Zhou et al., 2018b; Tao et al., 2019a) or deeper perspective (Tao et al., 2019"
2020.coling-main.437,D19-1413,0,0.131125,"text and response: (i) In order to capture the interaction information between a candidate response and multi-turn utterances, most of existing iterative architectures may require deeper or more complex network structure along with the growth of the turns of conversations, which fall short to efficiently learn the multi-turn utterance representations. (ii) Existing methods mainly focus on measuring the semantic relevancy between the response and the given utterance context. Nevertheless, researchers observe that some latent features in the conversations, such as user intent (Wen et al., 2017; Perkins and Yang, 2019; Yang et al., 2020) or conversation topic (Xing et ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 4981 Proceedings of the 28th International Conference on Computational Linguistics, pages 4981–4992 Barcelona, Spain (Online), December 8-13, 2020 al., 2017; Yoon et al., 2018; Yoon et al., 2019), also attach great"
2020.coling-main.437,D19-1011,0,0.200165,"t al., 2017) or retrieval-based methods (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). In this work, we focus on the problem of multi-turn response selection for retrieval-based dialogue systems, which aims at selecting appropriate responses from a set of candidates as the reply for the given multi-turn utterances. Measuring the matching degree between the utterance context and the candidate response is the core of multi-turn response selection task. Recent works develop a variety of interaction model to enhance the utterance-response interaction from a broader (Zhou et al., 2018b; Tao et al., 2019a) or deeper perspective (Tao et al., 2019b; Wang et al., 2019; Yuan et al., 2019). Empirical evidences show that iterative architectures achieve state-of-the-art performance on multi-turn response selection, such as interactionover-interaction (Tao et al., 2019b), iterated attentive matching (Wang et al., 2019), and multi-hop selector (Yuan et al., 2019). Despite the effectiveness of these methods, multi-turn response selection task still remains some challenges when modeling the interaction between the utterance context and response: (i) In order to capture the interaction information betwee"
2020.coling-main.437,P19-1001,0,0.152253,"t al., 2017) or retrieval-based methods (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). In this work, we focus on the problem of multi-turn response selection for retrieval-based dialogue systems, which aims at selecting appropriate responses from a set of candidates as the reply for the given multi-turn utterances. Measuring the matching degree between the utterance context and the candidate response is the core of multi-turn response selection task. Recent works develop a variety of interaction model to enhance the utterance-response interaction from a broader (Zhou et al., 2018b; Tao et al., 2019a) or deeper perspective (Tao et al., 2019b; Wang et al., 2019; Yuan et al., 2019). Empirical evidences show that iterative architectures achieve state-of-the-art performance on multi-turn response selection, such as interactionover-interaction (Tao et al., 2019b), iterated attentive matching (Wang et al., 2019), and multi-hop selector (Yuan et al., 2019). Despite the effectiveness of these methods, multi-turn response selection task still remains some challenges when modeling the interaction between the utterance context and response: (i) In order to capture the interaction information betwee"
2020.coling-main.437,P17-1046,0,0.340021,"ubstantially and consistently outperforms existing state-of-the-art methods on three multi-turn response selection benchmark datasets. 1 Introduction Recent years have witnessed many successful real-world applications on chatbots and AI assistants, such as the XiaoIce (Shum et al., 2018) from Microsoft and the E-commerce assistant AliMe (Li et al., 2017) from Alibaba Group, which owe to the extensive researches on dialogue systems. Existing works on building conversational models mainly study generation-based (Wen et al., 2017; Xing et al., 2017) or retrieval-based methods (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). In this work, we focus on the problem of multi-turn response selection for retrieval-based dialogue systems, which aims at selecting appropriate responses from a set of candidates as the reply for the given multi-turn utterances. Measuring the matching degree between the utterance context and the candidate response is the core of multi-turn response selection task. Recent works develop a variety of interaction model to enhance the utterance-response interaction from a broader (Zhou et al., 2018b; Tao et al., 2019a) or deeper perspective (Tao et al., 2019b; Wang et al., 2"
2020.coling-main.437,N18-1142,0,0.27646,"as user intent (Wen et al., 2017; Perkins and Yang, 2019; Yang et al., 2020) or conversation topic (Xing et ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 4981 Proceedings of the 28th International Conference on Computational Linguistics, pages 4981–4992 Barcelona, Spain (Online), December 8-13, 2020 al., 2017; Yoon et al., 2018; Yoon et al., 2019), also attach great importance in dialogue systems, which have received little attention in recent multi-turn response selection studies. In this work, we propose Intra-/Inter-Interaction Network (I3 ) with latent interaction modeling to tackle the aforementioned issues. In specific, we adopt hierarchical structure instead of iterative structure to model the multi-level interactions in the multi-turn conversation, including the intra-utterance interaction between the response and each individual utterance, and the inter-utterance interaction among the response and the overa"
2020.coling-main.437,C18-1317,0,0.37538,"consistently outperforms existing state-of-the-art methods on three multi-turn response selection benchmark datasets. 1 Introduction Recent years have witnessed many successful real-world applications on chatbots and AI assistants, such as the XiaoIce (Shum et al., 2018) from Microsoft and the E-commerce assistant AliMe (Li et al., 2017) from Alibaba Group, which owe to the extensive researches on dialogue systems. Existing works on building conversational models mainly study generation-based (Wen et al., 2017; Xing et al., 2017) or retrieval-based methods (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). In this work, we focus on the problem of multi-turn response selection for retrieval-based dialogue systems, which aims at selecting appropriate responses from a set of candidates as the reply for the given multi-turn utterances. Measuring the matching degree between the utterance context and the candidate response is the core of multi-turn response selection task. Recent works develop a variety of interaction model to enhance the utterance-response interaction from a broader (Zhou et al., 2018b; Tao et al., 2019a) or deeper perspective (Tao et al., 2019b; Wang et al., 2019; Yuan et al., 201"
2020.coling-main.437,D16-1036,0,0.0592311,"Missing"
2020.coling-main.437,P18-1103,0,0.0136261,"eep attention matching network (Zhou et al., 2018b), multi-representation fusion network (Tao et al., 2019a), interaction-over-interaction network (Tao et al., 2019b), and multi-hop selector network (Yuan et al., 2019). In this work, we facilitate the interaction modeling by considering both intra-/inter-utterance interaction with a hierarchical encoder. Apart from measuring the semantic and contextual relevancy, several efforts have been made on discovering some latent features in the conversations for modeling the intent or topic coherency between the utterance and the response. Yoon et al. (2018) and Yoon et al. (2019) incorporate latent clustering into context-based response/answer selection models to fetch latent topic information. Yang et al. (2018) 4982 Aggregation Origin Interaction Utterance-1 . . . Utterance-n Intra-utterance Interaction Self-attention Layer Hu1r Hu1 . . . Dual-attention Layer Hunr Hun Inter-utterance Interaction Latent Interaction Cu1 O* u1 Ou1r Ou1 Self-attention Layer Self-attention Layer . . . . . . . . . Ounr Oun Cun O* un Oru Or Cr C* r Dual-attention Layer Dual-attention Layer Hru1 Hr Response Self-attention Layer . . . Hrun Hr Intra-utterance Encoder Se"
2020.emnlp-main.188,D19-1475,0,0.287165,"act checking problem on PQA. 2.2 T RUE PART T RUE U NSURE PART FALSE FALSE Community Votes nup = ntotal ndown < nup < ntotal ndown = nup nup < ndown < ntotal ndown = ntotal Table 2: Veracity labels from community votes. nup , ndown , ntotal refers to the number of upvotes, downvotes and total votes of the answer respectively. stituting around 2.7 million QA pairs in total. Fact Checking Datasets & Methods Automatically predicting the veracity of claims has been extensively studied in recent years and various fact checking datasets have been released (Thorne et al., 2018a; Sharma et al., 2019; Augenstein et al., 2019). Typically, the data are collected from news checking websites such as Politifact and Snopes, where the evidence is either not given (Rashkin et al., 2017; P´erez-Rosas et al., 2018) or provided as an external URL link containing machine-unreadable format ranging from statistical tables to PDF reports (Wang, 2017). One recent trend is that evidence-based fact checking has gained more attention where datasets with wellformatted claims and evidence are adopted (Thorne et al., 2018a; Popat et al., 2018; Chen et al., 2020). Fact checking methods are mostly tailored to specific types of datasets."
2020.emnlp-main.188,D18-2029,0,0.050361,"Missing"
2020.emnlp-main.188,N19-1423,0,0.00971927,"ieve relevant product information as evidence for providing external information when predicting the answer veracity. In E-commerce scenario, product descriptions from the manufacture and user reviews from the former buyers contain rich product information, which can be treated as the candidate information pool for the retrieval process. Similar with Thorne et al. (2018a), we rank the evidence sentences by TF-IDF similarity to the question text. To further improve the accuracy of the retrieved evidence, we only use the TF-IDF similarity as an initial filtering step, then the pre-trained BERT (Devlin et al., 2019) is utilized as the sentence encoder to encode the filtered evidence sentences and question text. The k nearest evidence sentences using cosine similarity with the encoded question are kept as the evidence for veracity verification. The statistics of the entire dataset is reported in Table 3. 4 Answer Veracity Prediction Problem Definition. Given an answer a to its corresponding question q, our aim is to predict the answer veracity which falls into one of the predefined veracity type, with the help of k relevant evidence sentences s1 , s2 , . . . , sk . In this section, we describe our propose"
2020.emnlp-main.188,P19-1244,1,0.792007,"from the competitors (Carmel et al., 2018). Therefore, automatically verifying the answer veracity is becoming a demanding need, which can offer a more reliable online shopping environment, for example, by triggering a double-check on the detected doubtful answers. Fact checking aiming at verifying the truthfulness of a given claim (Thorne and Vlachos, 2018; Sharma et al., 2019) can be a promising direction to tackle the concerned problem. However, the claim on which existing fact checking methods mainly focus is usually a standalone text snippet such as news (Wang, 2017; Popat et al., 2018; Ma et al., 2019) or twitter posts (Derczynski et al., 2017; Wei et al., 2019). To predict the veracity of an answer in the QA settings, one can notice that it is insufficient to consider the answer alone since the question text 2407 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417, c November 16–20, 2020. 2020 Association for Computational Linguistics also carries important semantic information for the prediction. Thus, we need to appropriately leverage the question text into the verification process. In the context of CQA problems, most existing studies"
2020.emnlp-main.188,S19-2149,0,0.228774,"of a candidate answer to the given question (Tay et al., 2017; Yang et al., 2019b) or ranking available answers for a given question (Zhang et al., 2020a). However, the notion of veracity poses a more rigorous requirement of an answer where it needs to be factually correct. For example, the given answer in Table 1 will be labeled as positive from the perspective of the typical CQA task (Nakov et al., 2017) since it is topically relevant to the question. But its verdict is indeed false which can be verified from the product description. Recently, a new shared task, namely SemEval-2019 Task 8 (Mihaylova et al., 2019) investigates the fact checking problem in question answering scenario, requiring a system to classify the veracity of answers in a web forum. However, only QA pairs are given in this task, making it less practical since most of the predictions require extra knowledge from external sources. Moreover, with only hundreds of QA pairs provided, such limited number of samples precludes its use to develop powerful machine learning based fact checking models. To tackle the aforementioned issues, we introduce a large scale fact checking dataset called AnswerFact for investigating the answer veracity i"
2020.emnlp-main.188,P16-2022,0,0.0215935,"ces to cross-check their internal coherence: Rda (10) Rda ×dh where w4 ∈ and W5 ∈ are trainable parameters, γ ∈ Rk denotes the coherence weight for each evidence sentence. As discussed in Lin et al. (2017), such vector representation usually focuses on one specific aspect among the sentences. To capture multiple factual aspects involved in the verification process, we extend Equation 10 to a multi-view agreement matching as follows: Γ = W4 · tanh(W5 · v¯sT ) 0 γ = softmax(maxcol (Γ)) ∈ R After obtaining the combined evidence embedding v˜s , we utilize it to verify the answer claim. Following (Mou et al., 2016; Yang et al., 2019a) for strengthening the inference relations between the evidence and answer claim, we integrate the answer claim embedding va , evidence embedding v˜s , their absolute difference |va − v˜s |, and the element-wise product va ⊗ v˜s into a prediction vector. Moreover, since the question text also implicitly contains useful semantic information, we also concatenate the question embedding vq to the prediction vector. It is then fed to a MLP layer to make the prediction: yˆ = MLP ([vq , va , v˜s , |va − v˜s |; va ⊗ v˜s ]) (14) The entire model can then be trained end-to-end by co"
2020.emnlp-main.188,S17-2003,0,0.0640148,"Missing"
2020.emnlp-main.188,D14-1162,0,0.0863246,"Missing"
2020.emnlp-main.188,C18-1287,0,0.0316136,"Missing"
2020.emnlp-main.188,D18-1003,0,0.232781,"ly malicious attacks from the competitors (Carmel et al., 2018). Therefore, automatically verifying the answer veracity is becoming a demanding need, which can offer a more reliable online shopping environment, for example, by triggering a double-check on the detected doubtful answers. Fact checking aiming at verifying the truthfulness of a given claim (Thorne and Vlachos, 2018; Sharma et al., 2019) can be a promising direction to tackle the concerned problem. However, the claim on which existing fact checking methods mainly focus is usually a standalone text snippet such as news (Wang, 2017; Popat et al., 2018; Ma et al., 2019) or twitter posts (Derczynski et al., 2017; Wei et al., 2019). To predict the veracity of an answer in the QA settings, one can notice that it is insufficient to consider the answer alone since the question text 2407 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417, c November 16–20, 2020. 2020 Association for Computational Linguistics also carries important semantic information for the prediction. Thus, we need to appropriately leverage the question text into the verification process. In the context of CQA problems, most"
2020.emnlp-main.188,D17-1317,0,0.0720949,"down = ntotal Table 2: Veracity labels from community votes. nup , ndown , ntotal refers to the number of upvotes, downvotes and total votes of the answer respectively. stituting around 2.7 million QA pairs in total. Fact Checking Datasets & Methods Automatically predicting the veracity of claims has been extensively studied in recent years and various fact checking datasets have been released (Thorne et al., 2018a; Sharma et al., 2019; Augenstein et al., 2019). Typically, the data are collected from news checking websites such as Politifact and Snopes, where the evidence is either not given (Rashkin et al., 2017; P´erez-Rosas et al., 2018) or provided as an external URL link containing machine-unreadable format ranging from statistical tables to PDF reports (Wang, 2017). One recent trend is that evidence-based fact checking has gained more attention where datasets with wellformatted claims and evidence are adopted (Thorne et al., 2018a; Popat et al., 2018; Chen et al., 2020). Fact checking methods are mostly tailored to specific types of datasets. Methods involving small datasets often use hand-crafted features to represent the claim (Mihaylova et al., 2018). These features are then fed into a SVM or"
2020.emnlp-main.188,S19-2203,0,0.0671324,"sk in QA settings (Mihaylova et al., 2018), which was later adopted as the SemEval2019 Task 8 (Mihaylova et al., 2019). Its goal is to classify an answer in the Qatar forum1 into true, false or non-factual. However, only QA pairs are given in this shared task to predict the answer veracity, making it less practical due to the lack of evidence sources. Moreover, the small number of training data consisting only 495 QA pairs restricts the possibility of trying some powerful machine learning models such as deep neural networks. 2408 1 http://www.qatarliving.com As pointed out in Mihaylova et al. (2019), verifying the verdict of answers in CQA requires using rich world knowledge. However, gathering relevant information as evidence can be difficult due to the open-domain nature of those questions. Compared with general CQA forums, PQA provides productspecific forums, making the evidence collection process more realistic and controllable. Also, as will be described in Section 3, the high proportion of factual type QA pairs also makes it suitable for studying the fact checking problem on PQA. 2.2 T RUE PART T RUE U NSURE PART FALSE FALSE Community Votes nup = ntotal ndown < nup < ntotal ndown ="
2020.emnlp-main.188,C18-1283,0,0.0143356,"eir veracity due to the lack of systematic quality control (Mihaylova et al., 2018). Those untruthful answers may attribute to multiple factors such as misunderstandings of the question, improper expressions during writing, and even intentionally malicious attacks from the competitors (Carmel et al., 2018). Therefore, automatically verifying the answer veracity is becoming a demanding need, which can offer a more reliable online shopping environment, for example, by triggering a double-check on the detected doubtful answers. Fact checking aiming at verifying the truthfulness of a given claim (Thorne and Vlachos, 2018; Sharma et al., 2019) can be a promising direction to tackle the concerned problem. However, the claim on which existing fact checking methods mainly focus is usually a standalone text snippet such as news (Wang, 2017; Popat et al., 2018; Ma et al., 2019) or twitter posts (Derczynski et al., 2017; Wei et al., 2019). To predict the veracity of an answer in the QA settings, one can notice that it is insufficient to consider the answer alone since the question text 2407 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417, c November 16–20, 2020"
2020.emnlp-main.188,N18-1074,0,0.0393684,"answers with votes can provide precious labeled data for our investigation in PQA forums. To ensure the quality of labels, we first filter out answers with total votes (including upvotes and downvotes) less than 2. Then following typical settings in fact checking datasets (Vlachos and Riedel, 2014; Wang, 2017; Augenstein et al., 2019), we consider the problem as a multi-class classification task and divide answers into five types according to their community votes as shown in Table 2. The rationality is that fully objective truth is often elusive and ill-defined as pointed out in Popat et al. (2018). For example, an answer may contain partially true information for the question. Thus, such veracity label partition can also be interpreted as measuring the answer credibility or reliability in multiple scales. 3.3 Evidence Retrieval We then use the question text to retrieve relevant product information as evidence for providing external information when predicting the answer veracity. In E-commerce scenario, product descriptions from the manufacture and user reviews from the former buyers contain rich product information, which can be treated as the candidate information pool for the retrie"
2020.emnlp-main.188,W18-5501,0,0.176795,"Missing"
2020.emnlp-main.188,W14-2508,0,0.0318601,"receive upvotes and downvotes from the former buyers. For factual type QA pairs, such community votes reflect users’ stance towards the statement claimed in the answer, indicating the overall veracity judgement given by the entire community. It is not surprising that some answers may not have any vote in practice. But those answers with votes can provide precious labeled data for our investigation in PQA forums. To ensure the quality of labels, we first filter out answers with total votes (including upvotes and downvotes) less than 2. Then following typical settings in fact checking datasets (Vlachos and Riedel, 2014; Wang, 2017; Augenstein et al., 2019), we consider the problem as a multi-class classification task and divide answers into five types according to their community votes as shown in Table 2. The rationality is that fully objective truth is often elusive and ill-defined as pointed out in Popat et al. (2018). For example, an answer may contain partially true information for the question. Thus, such veracity label partition can also be interpreted as measuring the answer credibility or reliability in multiple scales. 3.3 Evidence Retrieval We then use the question text to retrieve relevant produ"
2020.emnlp-main.188,P17-2067,0,0.395256,"intentionally malicious attacks from the competitors (Carmel et al., 2018). Therefore, automatically verifying the answer veracity is becoming a demanding need, which can offer a more reliable online shopping environment, for example, by triggering a double-check on the detected doubtful answers. Fact checking aiming at verifying the truthfulness of a given claim (Thorne and Vlachos, 2018; Sharma et al., 2019) can be a promising direction to tackle the concerned problem. However, the claim on which existing fact checking methods mainly focus is usually a standalone text snippet such as news (Wang, 2017; Popat et al., 2018; Ma et al., 2019) or twitter posts (Derczynski et al., 2017; Wei et al., 2019). To predict the veracity of an answer in the QA settings, one can notice that it is insufficient to consider the answer alone since the question text 2407 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417, c November 16–20, 2020. 2020 Association for Computational Linguistics also carries important semantic information for the prediction. Thus, we need to appropriately leverage the question text into the verification process. In the context o"
2020.emnlp-main.188,D19-1485,0,0.02231,"matically verifying the answer veracity is becoming a demanding need, which can offer a more reliable online shopping environment, for example, by triggering a double-check on the detected doubtful answers. Fact checking aiming at verifying the truthfulness of a given claim (Thorne and Vlachos, 2018; Sharma et al., 2019) can be a promising direction to tackle the concerned problem. However, the claim on which existing fact checking methods mainly focus is usually a standalone text snippet such as news (Wang, 2017; Popat et al., 2018; Ma et al., 2019) or twitter posts (Derczynski et al., 2017; Wei et al., 2019). To predict the veracity of an answer in the QA settings, one can notice that it is insufficient to consider the answer alone since the question text 2407 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417, c November 16–20, 2020. 2020 Association for Computational Linguistics also carries important semantic information for the prediction. Thus, we need to appropriately leverage the question text into the verification process. In the context of CQA problems, most existing studies focus on measuring the semantic relevance of a candidate answ"
2020.emnlp-main.188,P19-1465,0,0.239266,"e QA settings, one can notice that it is insufficient to consider the answer alone since the question text 2407 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417, c November 16–20, 2020. 2020 Association for Computational Linguistics also carries important semantic information for the prediction. Thus, we need to appropriately leverage the question text into the verification process. In the context of CQA problems, most existing studies focus on measuring the semantic relevance of a candidate answer to the given question (Tay et al., 2017; Yang et al., 2019b) or ranking available answers for a given question (Zhang et al., 2020a). However, the notion of veracity poses a more rigorous requirement of an answer where it needs to be factually correct. For example, the given answer in Table 1 will be labeled as positive from the perspective of the typical CQA task (Nakov et al., 2017) since it is topically relevant to the question. But its verdict is indeed false which can be verified from the product description. Recently, a new shared task, namely SemEval-2019 Task 8 (Mihaylova et al., 2019) investigates the fact checking problem in question answer"
2020.emnlp-main.547,C16-1053,0,0.0291434,"gion, China (Project Codes: 14200719). detailed analysis to explain or justify the final answers, such as questions in community QA (Ishida et al., 2018; Deng et al., 2020a) or explainable QA (Fan et al., 2019; Nakatsuji and Okui, 2020). As the example from PubMedQA (Jin et al., 2019) presented in Figure 1, the answer can be regarded as the summary over the document driven by the reasoning process of the given question. Most of related studies focus on query-based summarization approaches for summarizing the query-related content from the source document (Shen and Li, 2011; Wang et al., 2013; Cao et al., 2016; Nema et al., 2017). However, these approaches fall short of tackling question-driven summarization problem in QA scenario, since the query-based summarization process is typically based on semantic relevance measurement without a careful reasoning or inference process, which is essential to question-driven summarization. Currently, question-driven summarization is mainly explored by traditional information retrieval methods to select sentences from the source document to construct the final answer (Wang et al., 2014; Song et al., 2017; Yulianti et al., 2018), which heavily rely on hand-craft"
2020.emnlp-main.547,P12-1061,0,0.0265353,"tive pointer generator network with a multi-view coverage mechanism is designed to integrate diverse information from different perspectives. Experimental results show that the proposed method consistently outperforms stateof-the-art methods on two non-factoid QA datasets, namely WikiHow and PubMedQA. 1 Introduction Recent years have witnessed several attempts on exploring question-driven summarization, which aims at summarizing the source document with respect to a specific question, to produce a concise but informative answer in non-factoid question answering (QA) (Tomasoni and Huang, 2010; Chan et al., 2012; Song et al., 2017). Unlike factoid QA (Rajpurkar et al., 2016), e.g., “Who is the author of Harry Potter?”, whose answer is generally a single phrase or a short sentence with limited information, the answers for non-factoid questions are supposed to be more informative, involving some ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 14200719). detailed analysis to explain or justify the final answers, such as questions in community QA (Ishida et al., 2018; Deng et al."
2020.emnlp-main.547,P19-1346,0,0.304838,"factoid QA (Rajpurkar et al., 2016), e.g., “Who is the author of Harry Potter?”, whose answer is generally a single phrase or a short sentence with limited information, the answers for non-factoid questions are supposed to be more informative, involving some ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 14200719). detailed analysis to explain or justify the final answers, such as questions in community QA (Ishida et al., 2018; Deng et al., 2020a) or explainable QA (Fan et al., 2019; Nakatsuji and Okui, 2020). As the example from PubMedQA (Jin et al., 2019) presented in Figure 1, the answer can be regarded as the summary over the document driven by the reasoning process of the given question. Most of related studies focus on query-based summarization approaches for summarizing the query-related content from the source document (Shen and Li, 2011; Wang et al., 2013; Cao et al., 2016; Nema et al., 2017). However, these approaches fall short of tackling question-driven summarization problem in QA scenario, since the query-based summarization process is typically based on se"
2020.emnlp-main.547,P19-1222,0,0.103276,"s a large proportion of non-factoid questions, such as “how” or “why” type questions (Koupaee and Wang, 2018; Ishida et al., 2018; Deng et al., 2020a). Besides, some studies aim at generating a conclusion for the concerned question (Jin et al., 2019; Nakatsuji and Okui, 2020). Fan et al. (2019) propose a multi-task Seq2Seq model with the concatenation of the question and support documents to generate long-form answers. Iida et al. (2019) and Nakatsuji and Okui (2020) incorporate some background knowledge into Seq2Seq model for why questions and conclusion-centric questions. Some latest works (Feldman and El-Yaniv, 2019; Yadav et al., 2019; Nishida et al., 2019a) attempt to provide evidence or justifications for humanunderstandable explanation of the multi-hop inference process in factoid QA, where the inferred evidences are only treated as the middle steps for finding the answer. However, in non-factoid QA, the intermediate output is also important to form a complete answer, which requires a bridge between the multi-hop inference and summarization. 3 Proposed Framework We propose a question-driven abstractive summarization model, namely Multi-hop Selective Generator (MSG). The overview of MSG is depicted in"
2020.emnlp-main.547,P18-1013,0,0.0223402,"of each hop. Besides, it also achieves better performance to apply the proposed MAR Unit as the multi-hop unit, instead of repeatedly using Attentive Unit, indicating that it is not enough to only consider the question-related information, while the interrelationship among different sentences also attaches great importance. The second part in Table 4 presents the ablation study in terms of discarding other model components in MSG. In general, all the components contribute to the final performance to a certain extent. In detail, there are several notable observations: (1) Some existing works (Hsu et al., 2018; Nishida et al., 2019b) apply softmax function to normalize the weights of different sentences in the decoding phase, which falls short of differentiating the importance degree of each sentence. The result shows that MSG achieves better performance by employing gated attention to distinguish salient justification sentences for generating the summaries. (2) Discarding the question pointer casts a noticeably greater decrease on PubMedQA than WikiHow. We conjecture that those questions from PubMedQA contain more words available to be copied for generating precise summaries, as the statistic of t"
2020.emnlp-main.547,D19-1259,0,0.301226,"ter?”, whose answer is generally a single phrase or a short sentence with limited information, the answers for non-factoid questions are supposed to be more informative, involving some ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 14200719). detailed analysis to explain or justify the final answers, such as questions in community QA (Ishida et al., 2018; Deng et al., 2020a) or explainable QA (Fan et al., 2019; Nakatsuji and Okui, 2020). As the example from PubMedQA (Jin et al., 2019) presented in Figure 1, the answer can be regarded as the summary over the document driven by the reasoning process of the given question. Most of related studies focus on query-based summarization approaches for summarizing the query-related content from the source document (Shen and Li, 2011; Wang et al., 2013; Cao et al., 2016; Nema et al., 2017). However, these approaches fall short of tackling question-driven summarization problem in QA scenario, since the query-based summarization process is typically based on semantic relevance measurement without a careful reasoning or inference proces"
2020.emnlp-main.547,Q18-1023,0,0.0609531,"Missing"
2020.emnlp-main.547,C14-1113,0,0.0283584,"repetition issue along with the multiview pointer network and generate informative answers; (3) Experimental results show that the proposed method achieves state-of-the-art performance on WikiHow and PubMedQA datasets, and it is able to provide justification sentences as the evidence for the answer. 2 Related Works Query-based Summarization. Early works on query-based summarization focus on extracting query-related sentences to construct the summary (Lin et al., 2010; Shen and Li, 2011), which are later improved by exploiting sentence compression on the extracted sentences (Wang et al., 2013; Li and Li, 2014). Recently, some data-driven neural abstractive models are proposed to generate natural form of summaries with respect to the given query (Nema et al., 2017; Hasselqvist et al., 2017). However, current studies on query-based abstractive summarization are restricted by the lack of large-scale datasets (Baumel et al., 2016; Nema et al., 2017). One the other hand, some researchers spark a new pave of question-driven summarization in non-factoid QA (Song et al., 2017; Yulianti 6735 et al., 2018; Deng et al., 2020b), which requires the ability of reasoning or inference for supporting summarization,"
2020.emnlp-main.547,N10-1041,0,0.0275438,"i-hop reasoning to infer the important content for facilitating answer generation; (2) We propose a multi-view coverage mechanism to address the repetition issue along with the multiview pointer network and generate informative answers; (3) Experimental results show that the proposed method achieves state-of-the-art performance on WikiHow and PubMedQA datasets, and it is able to provide justification sentences as the evidence for the answer. 2 Related Works Query-based Summarization. Early works on query-based summarization focus on extracting query-related sentences to construct the summary (Lin et al., 2010; Shen and Li, 2011), which are later improved by exploiting sentence compression on the extracted sentences (Wang et al., 2013; Li and Li, 2014). Recently, some data-driven neural abstractive models are proposed to generate natural form of summaries with respect to the given query (Nema et al., 2017; Hasselqvist et al., 2017). However, current studies on query-based abstractive summarization are restricted by the lack of large-scale datasets (Baumel et al., 2016; Nema et al., 2017). One the other hand, some researchers spark a new pave of question-driven summarization in non-factoid QA (Song"
2020.emnlp-main.547,S15-2047,0,0.0661208,"Missing"
2020.emnlp-main.547,P17-1098,0,0.22495,"ct Codes: 14200719). detailed analysis to explain or justify the final answers, such as questions in community QA (Ishida et al., 2018; Deng et al., 2020a) or explainable QA (Fan et al., 2019; Nakatsuji and Okui, 2020). As the example from PubMedQA (Jin et al., 2019) presented in Figure 1, the answer can be regarded as the summary over the document driven by the reasoning process of the given question. Most of related studies focus on query-based summarization approaches for summarizing the query-related content from the source document (Shen and Li, 2011; Wang et al., 2013; Cao et al., 2016; Nema et al., 2017). However, these approaches fall short of tackling question-driven summarization problem in QA scenario, since the query-based summarization process is typically based on semantic relevance measurement without a careful reasoning or inference process, which is essential to question-driven summarization. Currently, question-driven summarization is mainly explored by traditional information retrieval methods to select sentences from the source document to construct the final answer (Wang et al., 2014; Song et al., 2017; Yulianti et al., 2018), which heavily rely on hand-crafted features or tedio"
2020.emnlp-main.547,P19-1225,0,0.112885,"uch as “how” or “why” type questions (Koupaee and Wang, 2018; Ishida et al., 2018; Deng et al., 2020a). Besides, some studies aim at generating a conclusion for the concerned question (Jin et al., 2019; Nakatsuji and Okui, 2020). Fan et al. (2019) propose a multi-task Seq2Seq model with the concatenation of the question and support documents to generate long-form answers. Iida et al. (2019) and Nakatsuji and Okui (2020) incorporate some background knowledge into Seq2Seq model for why questions and conclusion-centric questions. Some latest works (Feldman and El-Yaniv, 2019; Yadav et al., 2019; Nishida et al., 2019a) attempt to provide evidence or justifications for humanunderstandable explanation of the multi-hop inference process in factoid QA, where the inferred evidences are only treated as the middle steps for finding the answer. However, in non-factoid QA, the intermediate output is also important to form a complete answer, which requires a bridge between the multi-hop inference and summarization. 3 Proposed Framework We propose a question-driven abstractive summarization model, namely Multi-hop Selective Generator (MSG). The overview of MSG is depicted in Figure 2, which consists of three main co"
2020.emnlp-main.547,P19-1220,0,0.0855975,"uch as “how” or “why” type questions (Koupaee and Wang, 2018; Ishida et al., 2018; Deng et al., 2020a). Besides, some studies aim at generating a conclusion for the concerned question (Jin et al., 2019; Nakatsuji and Okui, 2020). Fan et al. (2019) propose a multi-task Seq2Seq model with the concatenation of the question and support documents to generate long-form answers. Iida et al. (2019) and Nakatsuji and Okui (2020) incorporate some background knowledge into Seq2Seq model for why questions and conclusion-centric questions. Some latest works (Feldman and El-Yaniv, 2019; Yadav et al., 2019; Nishida et al., 2019a) attempt to provide evidence or justifications for humanunderstandable explanation of the multi-hop inference process in factoid QA, where the inferred evidences are only treated as the middle steps for finding the answer. However, in non-factoid QA, the intermediate output is also important to form a complete answer, which requires a bridge between the multi-hop inference and summarization. 3 Proposed Framework We propose a question-driven abstractive summarization model, namely Multi-hop Selective Generator (MSG). The overview of MSG is depicted in Figure 2, which consists of three main co"
2020.emnlp-main.547,P17-1099,0,0.474188,"hst + b2 ), (27) where W2 and b2 are parameters to be learned. The final probability distribution of yt is obtained from three views of word distributions: X P q (yt ) = αqi , (28) i:wi =w t X P d (yt ) = α ˆ tdi , (29) P ρ i:wi =w (yt ) = [P (yt ), P q (yt ), P d (yt )], = softmax(Wρ [st : cqt : cdt ] + bρ ), P (yt ) = ρ · P all (yt ), all v (30) (31) (32) where Wρ and bρ are parameters to be learned, ρ is the multi-view pointer scalar to determine the weight of each view of the probability distribution. 6738 3.4 End-to-end Training Multi-view Coverage Loss. The original coverage mechanism (See et al., 2017) could only prevent repeated attention from one certain source text. However, the repetition problem becomes more severe, as we leverage both the question and document as the source text. Besides, similar to multi-view pointer network, coverage losses of different sources are supposed to be weighted by their contribution. Therefore, we design a multi-view coverage mechanism to address this issue as well as balance the generating and copying processes. In each P decoder timestep t, the coverage vector ct = t−1 t0 =0 at0 is used to represent the degree of coverage so far. The coverage vector ct"
2020.emnlp-main.547,P10-1078,0,0.0396731,"ized answer. A gated selective pointer generator network with a multi-view coverage mechanism is designed to integrate diverse information from different perspectives. Experimental results show that the proposed method consistently outperforms stateof-the-art methods on two non-factoid QA datasets, namely WikiHow and PubMedQA. 1 Introduction Recent years have witnessed several attempts on exploring question-driven summarization, which aims at summarizing the source document with respect to a specific question, to produce a concise but informative answer in non-factoid question answering (QA) (Tomasoni and Huang, 2010; Chan et al., 2012; Song et al., 2017). Unlike factoid QA (Rajpurkar et al., 2016), e.g., “Who is the author of Harry Potter?”, whose answer is generally a single phrase or a short sentence with limited information, the answers for non-factoid questions are supposed to be more informative, involving some ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 14200719). detailed analysis to explain or justify the final answers, such as questions in community QA (Ishida et al."
2020.emnlp-main.547,C14-1157,0,0.0211315,"lated content from the source document (Shen and Li, 2011; Wang et al., 2013; Cao et al., 2016; Nema et al., 2017). However, these approaches fall short of tackling question-driven summarization problem in QA scenario, since the query-based summarization process is typically based on semantic relevance measurement without a careful reasoning or inference process, which is essential to question-driven summarization. Currently, question-driven summarization is mainly explored by traditional information retrieval methods to select sentences from the source document to construct the final answer (Wang et al., 2014; Song et al., 2017; Yulianti et al., 2018), which heavily rely on hand-crafted features or tedious multi-stage pipelines. Besides, compared to extractive summarization (Cao et al., 2016), abstractive methods (Nema et al., 2017) can produce more coherent and logical summaries to answer the given question. To this end, we study question-driven abstractive summarization to generate natural form of answers by summarizing the source document with respect to a specific question. To tackle question-driven abstractive summarization, the content selection process for summarization is not only determin"
2020.emnlp-main.547,P13-1136,0,0.190969,"l Administrative Region, China (Project Codes: 14200719). detailed analysis to explain or justify the final answers, such as questions in community QA (Ishida et al., 2018; Deng et al., 2020a) or explainable QA (Fan et al., 2019; Nakatsuji and Okui, 2020). As the example from PubMedQA (Jin et al., 2019) presented in Figure 1, the answer can be regarded as the summary over the document driven by the reasoning process of the given question. Most of related studies focus on query-based summarization approaches for summarizing the query-related content from the source document (Shen and Li, 2011; Wang et al., 2013; Cao et al., 2016; Nema et al., 2017). However, these approaches fall short of tackling question-driven summarization problem in QA scenario, since the query-based summarization process is typically based on semantic relevance measurement without a careful reasoning or inference process, which is essential to question-driven summarization. Currently, question-driven summarization is mainly explored by traditional information retrieval methods to select sentences from the source document to construct the final answer (Wang et al., 2014; Song et al., 2017; Yulianti et al., 2018), which heavily"
2020.emnlp-main.547,D19-1260,0,0.0671343,"factoid questions, such as “how” or “why” type questions (Koupaee and Wang, 2018; Ishida et al., 2018; Deng et al., 2020a). Besides, some studies aim at generating a conclusion for the concerned question (Jin et al., 2019; Nakatsuji and Okui, 2020). Fan et al. (2019) propose a multi-task Seq2Seq model with the concatenation of the question and support documents to generate long-form answers. Iida et al. (2019) and Nakatsuji and Okui (2020) incorporate some background knowledge into Seq2Seq model for why questions and conclusion-centric questions. Some latest works (Feldman and El-Yaniv, 2019; Yadav et al., 2019; Nishida et al., 2019a) attempt to provide evidence or justifications for humanunderstandable explanation of the multi-hop inference process in factoid QA, where the inferred evidences are only treated as the middle steps for finding the answer. However, in non-factoid QA, the intermediate output is also important to form a complete answer, which requires a bridge between the multi-hop inference and summarization. 3 Proposed Framework We propose a question-driven abstractive summarization model, namely Multi-hop Selective Generator (MSG). The overview of MSG is depicted in Figure 2, which con"
2020.emnlp-main.738,W05-0909,0,0.118945,"ength and KB number, the data are mean, median, min and max respectively. 4 4.1 Experiments Experimental Setup We split WITA into a training set, a development set, and a testing set of 50,000, 5,000, and 400 records respectively. For the purpose of evaluating the performance of the models, we ask human helpers to annotate the testing set sentences. The human helpers are asked to revise the input KB triples and the corresponding target sentences making them exactly consistent with each other. We use several evaluation metrics including BLEU (Papineni et al., 2002), ROUGEL (Lin, 2004), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002) and CIDEr (Vedantam et al., 2015) with the package provided by Novikova et al. (2017). We follow the default setting in ROUGEL where β is set to 1.2. We build our model based on the Transformer model (Vaswani et al., 2017; Ott et al., 2019) and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to build the subword dictionary. We use Fairseq (Ott et al., 2019) to build our model and keep all hyper-parameters for Transformer unchanged. We set κ = 0.75 from {0.1, 0.25, 0.5, 0.75, 0.9} by extracting samples and ask human helper to evaluate. We use grid search to tune h"
2020.emnlp-main.738,2020.emnlp-main.90,1,0.659943,"ully et al. (2019) propose to generate a match report according to the match briefing. All these datasets are restricted to a few domains where well-aligned data is happened to be available. No existing works are focusing on handling partiallyaligned data. To solve the dataset scarcity problem, Fu et al. (2020c) propose to use dual learning to train generation models based on unaligned text and knowledge triples. The model generates text based on input triples and then predict the input triples with a dual extraction model. The two models are trained alternatively with dual learning. Although Cheng et al. (2020) proposed the ENTDESC task aiming at generating better text description for a few entities by exploring the knowledge from KB, their focus is more on distilling the useful part from the input knowledge. Text aligning has been studied for many years. Dyer et al. (2013) propose the Fast Align model which is a log-linear reparameterization of IBM Model 2. Legrand et al. (2016) propose a new score aggregation method to improve the alignment result. Moreover, attention-based models (Bahdanau et al., 2014) can also be recognized as a kind of alignment. However, these models focus on aligning source"
2020.emnlp-main.738,D16-1128,0,0.259989,"Missing"
2020.emnlp-main.738,W16-2207,0,0.0553607,"Missing"
2020.emnlp-main.738,N13-1073,0,0.0370695,"dataset scarcity problem, Fu et al. (2020c) propose to use dual learning to train generation models based on unaligned text and knowledge triples. The model generates text based on input triples and then predict the input triples with a dual extraction model. The two models are trained alternatively with dual learning. Although Cheng et al. (2020) proposed the ENTDESC task aiming at generating better text description for a few entities by exploring the knowledge from KB, their focus is more on distilling the useful part from the input knowledge. Text aligning has been studied for many years. Dyer et al. (2013) propose the Fast Align model which is a log-linear reparameterization of IBM Model 2. Legrand et al. (2016) propose a new score aggregation method to improve the alignment result. Moreover, attention-based models (Bahdanau et al., 2014) can also be recognized as a kind of alignment. However, these models focus on aligning source words to target words, and no existing models have been proposed to directly calculate supportiveness for generation tasks. In generation systems, Fu et al. (2020b) propose to dynamically align the current generation state with topics to improve the generation perform"
2020.emnlp-main.738,D19-1052,0,0.0237787,"Missing"
2020.emnlp-main.738,2020.coling-main.215,1,0.900372,"y Frank Herberfor the Dune univer ... ” which is even not a human-readable sentence. 5 Related Works During the past few years, many tasks have been proposed to generate human-readable text from the structured data. WebNLG (Gardent et al., 2017a,b; Ferreira et al., 2019) is proposed to describe KB triples sampled from DBPedia (Auer et al., 2007). 9190 The E2E (Novikova et al., 2017; Duˇsek et al., 2020) task is proposed for generating restaurant reviews based on the given attributes. Lebret et al. (2016) propose the Wikibio task to generate people’s biography based on given Wikipedia infobox. Fu et al. (2020a) propose to generate text based on event chains. Moreover, Liang et al. (2009) propose to generate weather reports for weather records and Wiseman et al. (2017), Chen and Mooney (2008) and Puduppully et al. (2019) propose to generate a match report according to the match briefing. All these datasets are restricted to a few domains where well-aligned data is happened to be available. No existing works are focusing on handling partiallyaligned data. To solve the dataset scarcity problem, Fu et al. (2020c) propose to use dual learning to train generation models based on unaligned text and knowl"
2020.emnlp-main.738,2020.aacl-main.29,1,0.902915,"y Frank Herberfor the Dune univer ... ” which is even not a human-readable sentence. 5 Related Works During the past few years, many tasks have been proposed to generate human-readable text from the structured data. WebNLG (Gardent et al., 2017a,b; Ferreira et al., 2019) is proposed to describe KB triples sampled from DBPedia (Auer et al., 2007). 9190 The E2E (Novikova et al., 2017; Duˇsek et al., 2020) task is proposed for generating restaurant reviews based on the given attributes. Lebret et al. (2016) propose the Wikibio task to generate people’s biography based on given Wikipedia infobox. Fu et al. (2020a) propose to generate text based on event chains. Moreover, Liang et al. (2009) propose to generate weather reports for weather records and Wiseman et al. (2017), Chen and Mooney (2008) and Puduppully et al. (2019) propose to generate a match report according to the match briefing. All these datasets are restricted to a few domains where well-aligned data is happened to be available. No existing works are focusing on handling partiallyaligned data. To solve the dataset scarcity problem, Fu et al. (2020c) propose to use dual learning to train generation models based on unaligned text and knowl"
2020.emnlp-main.738,P17-1017,0,0.432533,"nding to the text “developed in Canada”. The model is likely to bind the text to existing triples incorrectly. As a result, during the testing or operational stage, the model is likely to overly generate this kind of excerpt for similar triples. given structured data. For example, given the input knowledge base (KB) triple hCompany of Heroes, developer, Relic Entertainmenti, the aim is to generate a text description such as “Company of Heroes is developed by Relic Entertainment.”. In recent years, many works have been proposed to give impetus to the Data-to-Text generation task. For instance, Gardent et al. (2017a; 2017b) proposed the WebNLG task aiming at generating description text of the given KB triples. Novikova et al. (2017) proposed the E2E task aiming at generating restaurant reviews according to the given restaurant attributes. Lebret et al. (2016) proposed the WikiBio task in which the biography of each person is generated according to the given Wikipedia infobox. Introduction The Data-to-Text generation task focuses on generating human-readable text corresponding to some 1 Train The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/ distant_supervision_nl"
2020.emnlp-main.738,W17-3518,0,0.628287,"nding to the text “developed in Canada”. The model is likely to bind the text to existing triples incorrectly. As a result, during the testing or operational stage, the model is likely to overly generate this kind of excerpt for similar triples. given structured data. For example, given the input knowledge base (KB) triple hCompany of Heroes, developer, Relic Entertainmenti, the aim is to generate a text description such as “Company of Heroes is developed by Relic Entertainment.”. In recent years, many works have been proposed to give impetus to the Data-to-Text generation task. For instance, Gardent et al. (2017a; 2017b) proposed the WebNLG task aiming at generating description text of the given KB triples. Novikova et al. (2017) proposed the E2E task aiming at generating restaurant reviews according to the given restaurant attributes. Lebret et al. (2016) proposed the WikiBio task in which the biography of each person is generated according to the given Wikipedia infobox. Introduction The Data-to-Text generation task focuses on generating human-readable text corresponding to some 1 Train The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/ distant_supervision_nl"
2020.emnlp-main.738,P16-1154,0,0.0932135,"Missing"
2020.emnlp-main.738,P09-1011,0,0.115782,"entence. 5 Related Works During the past few years, many tasks have been proposed to generate human-readable text from the structured data. WebNLG (Gardent et al., 2017a,b; Ferreira et al., 2019) is proposed to describe KB triples sampled from DBPedia (Auer et al., 2007). 9190 The E2E (Novikova et al., 2017; Duˇsek et al., 2020) task is proposed for generating restaurant reviews based on the given attributes. Lebret et al. (2016) propose the Wikibio task to generate people’s biography based on given Wikipedia infobox. Fu et al. (2020a) propose to generate text based on event chains. Moreover, Liang et al. (2009) propose to generate weather reports for weather records and Wiseman et al. (2017), Chen and Mooney (2008) and Puduppully et al. (2019) propose to generate a match report according to the match briefing. All these datasets are restricted to a few domains where well-aligned data is happened to be available. No existing works are focusing on handling partiallyaligned data. To solve the dataset scarcity problem, Fu et al. (2020c) propose to use dual learning to train generation models based on unaligned text and knowledge triples. The model generates text based on input triples and then predict t"
2020.emnlp-main.738,D15-1166,0,0.0442865,"Missing"
2020.emnlp-main.738,W17-5525,0,0.103702,"Missing"
2020.emnlp-main.738,N19-4009,0,0.113018,"ce of the models, we ask human helpers to annotate the testing set sentences. The human helpers are asked to revise the input KB triples and the corresponding target sentences making them exactly consistent with each other. We use several evaluation metrics including BLEU (Papineni et al., 2002), ROUGEL (Lin, 2004), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002) and CIDEr (Vedantam et al., 2015) with the package provided by Novikova et al. (2017). We follow the default setting in ROUGEL where β is set to 1.2. We build our model based on the Transformer model (Vaswani et al., 2017; Ott et al., 2019) and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to build the subword dictionary. We use Fairseq (Ott et al., 2019) to build our model and keep all hyper-parameters for Transformer unchanged. We set κ = 0.75 from {0.1, 0.25, 0.5, 0.75, 0.9} by extracting samples and ask human helper to evaluate. We use grid search to tune hyper-parameters on the development set and choose ωw = 0.05 from {0.02,0.05,0.1,0.2,0.5,1.0,2.0,5.0}, choose ωc = 1.0 from {0.02,0.05,0.1,0.2,0.5,1.0, 2.0,5.0} and choose α = 0.1 from {0.02,0.05,0.1,0.2,0.5,1.0}. The model has 49M parameters and it takes 2.4 hours t"
2020.emnlp-main.738,P02-1040,0,0.107014,"ble 1: Statistics of WITA and WebNLG. For the text length and KB number, the data are mean, median, min and max respectively. 4 4.1 Experiments Experimental Setup We split WITA into a training set, a development set, and a testing set of 50,000, 5,000, and 400 records respectively. For the purpose of evaluating the performance of the models, we ask human helpers to annotate the testing set sentences. The human helpers are asked to revise the input KB triples and the corresponding target sentences making them exactly consistent with each other. We use several evaluation metrics including BLEU (Papineni et al., 2002), ROUGEL (Lin, 2004), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002) and CIDEr (Vedantam et al., 2015) with the package provided by Novikova et al. (2017). We follow the default setting in ROUGEL where β is set to 1.2. We build our model based on the Transformer model (Vaswani et al., 2017; Ott et al., 2019) and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to build the subword dictionary. We use Fairseq (Ott et al., 2019) to build our model and keep all hyper-parameters for Transformer unchanged. We set κ = 0.75 from {0.1, 0.25, 0.5, 0.75, 0.9} by extracting samples and ask"
2020.emnlp-main.738,P19-1195,0,0.0381904,"Missing"
2020.emnlp-main.738,P16-1162,0,0.0103105,"he testing set sentences. The human helpers are asked to revise the input KB triples and the corresponding target sentences making them exactly consistent with each other. We use several evaluation metrics including BLEU (Papineni et al., 2002), ROUGEL (Lin, 2004), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002) and CIDEr (Vedantam et al., 2015) with the package provided by Novikova et al. (2017). We follow the default setting in ROUGEL where β is set to 1.2. We build our model based on the Transformer model (Vaswani et al., 2017; Ott et al., 2019) and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to build the subword dictionary. We use Fairseq (Ott et al., 2019) to build our model and keep all hyper-parameters for Transformer unchanged. We set κ = 0.75 from {0.1, 0.25, 0.5, 0.75, 0.9} by extracting samples and ask human helper to evaluate. We use grid search to tune hyper-parameters on the development set and choose ωw = 0.05 from {0.02,0.05,0.1,0.2,0.5,1.0,2.0,5.0}, choose ωc = 1.0 from {0.02,0.05,0.1,0.2,0.5,1.0, 2.0,5.0} and choose α = 0.1 from {0.02,0.05,0.1,0.2,0.5,1.0}. The model has 49M parameters and it takes 2.4 hours to train it on a NVIDIA TITAN RTX graphics card. 4.2 Compa"
2020.emnlp-main.738,W18-6543,0,0.0851559,"TAN RTX graphics card. 4.2 Comparison Models We compare our full DSG model with the following baselines, state-of-the-art models, and ablations. S2S utilizes the traditional S2S model (Sutskever et al., 2014; Cho et al., 2014) equipped with atten0.463 0.496 0.518 0.500 0.555 0.540 0.522 7.97 8.05 8.36 8.61 8.71 8.59 8.38 0.385 0.417 0.421 0.403 0.425 0.421 0.421 0.693 0.721 0.730 0.711 0.742 0.740 0.734 4.12 4.53 4.75 4.65 5.02 4.97 4.83 Table 2: Main results. tion (Bahdanau et al., 2014; Luong et al., 2015) and copy (Gu et al., 2016) mechanism. It is recognised as the state-of-the-art model (Shimorina and Gardent, 2018) in the WebNLG (Gardent et al., 2017b) task. S2ST utilizes the prevalent Transformer model (Vaswani et al., 2017; Ott et al., 2019) which outperforms the traditional S2S model in many generation tasks. DSG-A utilizes the attention adaptor which adapts attention as the supportiveness scores in the loss. DSG-H is almost the same as our DSG model. The only difference is that the supportiveness scores are adapted with hard adaptor while our DSG model uses the soft adaptor. DSG w/o RBS is an ablation model. It removes the Rebalanced Beam Search component from our DSG model. DSG w/o SA is an ablatio"
2021.acl-long.567,2020.acl-main.692,0,0.0209311,"s shown in Table 3, our method performs better than BT with 2/4 bilingual pairs but performs worse with 1/4 bilingual pairs. Interestingly, the combination of BT and our method yields significant further gains, which demonstrates that our method is not only orthogonal but also complementary to BT. Non-parametric Domain Adaptation Lastly, the “plug and play” property of TM further motivates us to domain adaptation, where we adapt a single general-domain model to a specific domain by using domain-specific monolingual TM. Data To simulate a diverse multi-domain setting, we use the data splits in Aharoni and Goldberg (2020) originally collected by Koehn and Knowles (2017). It includes German-English parallel data for train/dev/test sets in five domains: Medical, Law, IT, Koran and Subtitles. Similar to the experiments in §4.3, we only use one fourth of bilingual pairs for training. The target side of the remaining data is treated as additional monolingual data for building domain-specific TM, and the source side is discarded. The data statistics can be found in the upper block of Table 4. The dev and test sets for each domain contains 2K instances. Models We first train a Transformer Base baseline (model #1) on"
2021.acl-long.567,P19-1175,0,0.185069,"dditional cues. Recent work has confirmed that TM can help Neural Machine Translation (NMT) models as well. In a similar spirit to prior work, TM-augmented NMT models do not discard the training corpus after training but keep exploiting it in the test time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), embedding-based matches (Cao and Xiong, 2018), or a hybrid (Bulte and Tezcan, 2019; Xu et al., 2020); In the generation stage, the retrieved translations are injected into a standard NMT model by attending over them with sophisticated memory networks (Gu et al., 2018; Cao and Xiong, 2018; Xia et al., 2019; He et al., 2021) or directly concatenating them to the source input (Bulte and Tezcan, 2019; Xu et al., 2020), or biasing the word distribution during decoding (Zhang et al., 2018). Most recently, Khandelwal et al. (2020) propose a token-level nearest neighbor search using complete translation context, i.e., both the source-side input and target-side prefix. Despite their"
2021.acl-long.567,N19-1124,1,0.849786,"Missing"
2021.acl-long.567,D19-1195,1,0.896459,"Missing"
2021.acl-long.567,1981.tc-1.7,0,0.702124,"Missing"
2021.acl-long.567,D18-1340,0,0.0657347,"statistical translation models as additional cues. Recent work has confirmed that TM can help Neural Machine Translation (NMT) models as well. In a similar spirit to prior work, TM-augmented NMT models do not discard the training corpus after training but keep exploiting it in the test time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), embedding-based matches (Cao and Xiong, 2018), or a hybrid (Bulte and Tezcan, 2019; Xu et al., 2020); In the generation stage, the retrieved translations are injected into a standard NMT model by attending over them with sophisticated memory networks (Gu et al., 2018; Cao and Xiong, 2018; Xia et al., 2019; He et al., 2021) or directly concatenating them to the source input (Bulte and Tezcan, 2019; Xu et al., 2020), or biasing the word distribution during decoding (Zhang et al., 2018). Most recently, Khandelwal et al. (2020) propose a token-level nearest neighbor search using complete translation context, i.e., both the source-side input"
2021.acl-long.567,P17-1171,0,0.024959,"den states for later nearest neighbor search at each decoding step, which is very compute-intensive. The distinctions between our work and prior work are obvious: (1) The TM in our framework is a collection of monolingual sentences rather than bilingual sentence pairs; (2) We use learnable task-specific retrieval rather than generic retrieval mechanisms. Retrieval for Text Generation Discrete retrieval as an intermediate step has been shown beneficial to a variety of natural language processing tasks. One typical use is to retrieve supporting evidence for open-domain question answering (e.g., Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020). Recently, retrieval-guided generation has gained increasing interest in a wide range of text generation tasks such as language modeling (Guu et al., 2018; Khandelwal et al., 2019; Guu et al., 2020), dialogue response generation (Weston et al., 2018; Wu et al., 2019; Cai et al., 7308 Retrieval Model Input x source sentence encoder ?""#$ (?) target sentence encoder ?()( (?) Translation Memory ? Translation Model ?1 ?5 … Source Encoder ?(?, ?1 ) ?(?, ?5 ) … Memory Encoder relevant TM MIPS … Decoder relevance scores ?-,/ Output y 23 /01 bias attention de"
2021.acl-long.567,C18-1111,0,0.0275205,"Missing"
2021.acl-long.567,N19-1423,0,0.0187413,"optimized for the ultimate translation goal. Experiments show that the proposed method obtains substantial improvements. Remarkably, it even outperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the ability to leverage monolingual data, our model also demonstrates effectiveness in low-resource and domain adaptation scenarios. 1 Introduction Augmenting parametric neural network models with non-parametric memory (Khandelwal et al., 2019; Guu et al., 2020; Lewis et al., 2020a,b) has recently emerged as a promising direction to relieve the demand for ever-larger model size (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020). For the task of Machine Translation (MT), inspired by the Computer-Aided Translation (CAT) tools by professional human translators for increasing productivity for decades (Yamada, 2011), the usefulness of Translation Memory (TM) has long been recognized (Huang et al., 2021). In general, TM is a database that stores pairs of source text and its corresponding translations. Like for human ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Co"
2021.acl-long.567,D18-1045,0,0.0241265,"s-lingual setting. NMT using Monolingual Data To our knowledge, the integration of monolingual data for NMT was first investigated by Gulcehre et al. (2015), who separately trained target-side language models using monolingual data, and then integrated them during decoding either through re-scoring the beam, or by feeding the hidden state of the language model to the NMT model. Jean et al. (2015) also explored re-ranking the NMT output with a n-gram language model. Another successful method for leveraging monolingual data in NMT is back-translation (Sennrich et al., 2016; Fadaee et al., 2017; Edunov et al., 2018; He et al., 2016), where a reverse translation model is used to translate monolingual sentences from the target language to the source language to generate synthetic parallel sentences. Recent studies (Jiao et al., 2021; He et al., 2019) showed that self-training, where the synthetic parallel sentences are created by translating monolingual sentences in the source language, is also helpful. Our method is orthogonal to previous work and bears a unique feature: it can use more monolingual data without re-training (see §4.3). 3 Proposed Approach We start by formalizing the translation task as a"
2021.acl-long.567,P17-2090,0,0.0274589,"nging due to the cross-lingual setting. NMT using Monolingual Data To our knowledge, the integration of monolingual data for NMT was first investigated by Gulcehre et al. (2015), who separately trained target-side language models using monolingual data, and then integrated them during decoding either through re-scoring the beam, or by feeding the hidden state of the language model to the NMT model. Jean et al. (2015) also explored re-ranking the NMT output with a n-gram language model. Another successful method for leveraging monolingual data in NMT is back-translation (Sennrich et al., 2016; Fadaee et al., 2017; Edunov et al., 2018; He et al., 2016), where a reverse translation model is used to translate monolingual sentences from the target language to the source language to generate synthetic parallel sentences. Recent studies (Jiao et al., 2021; He et al., 2019) showed that self-training, where the synthetic parallel sentences are created by translating monolingual sentences in the source language, is also helpful. Our method is orthogonal to previous work and bears a unique feature: it can use more monolingual data without re-training (see §4.3). 3 Proposed Approach We start by formalizing the t"
2021.acl-long.567,D17-1146,0,0.0127575,"even outperforming strong TMaugmented baselines. This is remarkable given that previous TM-augmented models rely on bilingual TM while our model only exploits the target side. (2) Our model can substantially boost translation quality in low-resource scenarios by utilizing extra monolingual TM that is not present in training pairs. (3) Our model gains a strong cross-domain transferability by hot-swapping domain-specific monolingual memory. 2 Related Work TM-augmented NMT This work contributes primarily to the research line of Translation Memory (TM) augmented Neural Machine Translation (NMT). Feng et al. (2017) augmented NMT with a bilingual dictionary to tackle infrequent word translation. Gu et al. (2018) proposed a model that retrieves examples similar to the test source sentence and encodes retrieved source-target pairs with keyvalue memory networks. Cao and Xiong (2018); Cao et al. (2019) used a gating mechanism to balance the impact of the translation memory. Zhang et al. (2018) proposed guiding models by retrieving n-grams and up-weighting the probabilities of retrieved n-grams. Bulte and Tezcan (2019) and Xu et al. (2020) used fuzzy-matching with translation memories and augment source seque"
2021.acl-long.567,P16-1154,0,0.0122898,"re Li is the length of the token sequence zi . We compute a cross attention over all TM sentences: exp(ht T Wm zi,j )) αij = PM PL T i i=1 k=1 exp(ht Wm zi,k ) ct = Wc Li M X X (1) αij zi,j i=1 j=1 where αij is the attention score of the j-th token in zi , ct is a weighted combination of memory embeddings, and Wm and Wc are trainable matrices. The cross attention is used twice during decoding. First, the decoder’s hidden state ht is updated by a weighted sum of memory embeddings, i.e., ht = ht + ct . Second, we consider each attention score as a probability of copying the corresponding token (Gu et al., 2016; See et al., 2017). Formally, the next-token probabilities are computed as: p(yt |·) = (1 − λt )Pv (yt ) + λt Li M X X αij 1zij =yt i=1 j=1 where 1 is the indicator function and λt is a gating variable computed by another feed-forward network λt = g(ht , ct ). Inspired by Lewis et al. (2020a), to enable the gradient flow from the translation output to the retrieval model, we bias the attention scores with the relevance scores, rewriting Eq. (1) as: exp(ht T Wm zi,j + βf (x, zi )) αij = PM PL T i i=1 k=1 exp(ht Wm zi,k + βf (x, zi )) (2) where β is a trainable scalar that controls the weight o"
2021.acl-long.567,Q18-1031,0,0.0281054,"lection of monolingual sentences rather than bilingual sentence pairs; (2) We use learnable task-specific retrieval rather than generic retrieval mechanisms. Retrieval for Text Generation Discrete retrieval as an intermediate step has been shown beneficial to a variety of natural language processing tasks. One typical use is to retrieve supporting evidence for open-domain question answering (e.g., Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020). Recently, retrieval-guided generation has gained increasing interest in a wide range of text generation tasks such as language modeling (Guu et al., 2018; Khandelwal et al., 2019; Guu et al., 2020), dialogue response generation (Weston et al., 2018; Wu et al., 2019; Cai et al., 7308 Retrieval Model Input x source sentence encoder ?""#$ (?) target sentence encoder ?()( (?) Translation Memory ? Translation Model ?1 ?5 … Source Encoder ?(?, ?1 ) ?(?, ?5 ) … Memory Encoder relevant TM MIPS … Decoder relevance scores ?-,/ Output y 23 /01 bias attention dense index Figure 1: Overall framework. For an input sentence x in the source language, the retrieval model uses Maximum Inner Product Search (MIPS) to find the top-M TM sentences {zi }M i=1 in the t"
2021.acl-long.567,2021.acl-long.246,1,0.808584,"est time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), embedding-based matches (Cao and Xiong, 2018), or a hybrid (Bulte and Tezcan, 2019; Xu et al., 2020); In the generation stage, the retrieved translations are injected into a standard NMT model by attending over them with sophisticated memory networks (Gu et al., 2018; Cao and Xiong, 2018; Xia et al., 2019; He et al., 2021) or directly concatenating them to the source input (Bulte and Tezcan, 2019; Xu et al., 2020), or biasing the word distribution during decoding (Zhang et al., 2018). Most recently, Khandelwal et al. (2020) propose a token-level nearest neighbor search using complete translation context, i.e., both the source-side input and target-side prefix. Despite their differences, we identify two major limitations in previous research. First, the translation memory has to be a bilingual corpus consisting of aligned source-target pairs. This requirement limits the memory bank to bilingual pairs and preclud"
2021.acl-long.567,P10-1064,0,0.0531179,"Missing"
2021.acl-long.567,W15-3014,0,0.025228,"on from this line of research. However, retrieval-guided generation has so far been mainly investigated for knowledge retrieval in the same language. The memory retrieval in this work is more challenging due to the cross-lingual setting. NMT using Monolingual Data To our knowledge, the integration of monolingual data for NMT was first investigated by Gulcehre et al. (2015), who separately trained target-side language models using monolingual data, and then integrated them during decoding either through re-scoring the beam, or by feeding the hidden state of the language model to the NMT model. Jean et al. (2015) also explored re-ranking the NMT output with a n-gram language model. Another successful method for leveraging monolingual data in NMT is back-translation (Sennrich et al., 2016; Fadaee et al., 2017; Edunov et al., 2018; He et al., 2016), where a reverse translation model is used to translate monolingual sentences from the target language to the source language to generate synthetic parallel sentences. Recent studies (Jiao et al., 2021; He et al., 2019) showed that self-training, where the synthetic parallel sentences are created by translating monolingual sentences in the source language, is"
2021.acl-long.567,2021.acl-long.221,0,0.0323651,"gual data, and then integrated them during decoding either through re-scoring the beam, or by feeding the hidden state of the language model to the NMT model. Jean et al. (2015) also explored re-ranking the NMT output with a n-gram language model. Another successful method for leveraging monolingual data in NMT is back-translation (Sennrich et al., 2016; Fadaee et al., 2017; Edunov et al., 2018; He et al., 2016), where a reverse translation model is used to translate monolingual sentences from the target language to the source language to generate synthetic parallel sentences. Recent studies (Jiao et al., 2021; He et al., 2019) showed that self-training, where the synthetic parallel sentences are created by translating monolingual sentences in the source language, is also helpful. Our method is orthogonal to previous work and bears a unique feature: it can use more monolingual data without re-training (see §4.3). 3 Proposed Approach We start by formalizing the translation task as a retrieve-then-generate process in §3.1. Then in §3.2, we describe the model design for the crosslingual memory retrieval model. In §3.3, we describe the model design for the memory-augmented translation model. Lastly, we"
2021.acl-long.567,P10-2041,0,0.268089,"Missing"
2021.acl-long.567,2020.emnlp-main.550,0,0.0301666,"search at each decoding step, which is very compute-intensive. The distinctions between our work and prior work are obvious: (1) The TM in our framework is a collection of monolingual sentences rather than bilingual sentence pairs; (2) We use learnable task-specific retrieval rather than generic retrieval mechanisms. Retrieval for Text Generation Discrete retrieval as an intermediate step has been shown beneficial to a variety of natural language processing tasks. One typical use is to retrieve supporting evidence for open-domain question answering (e.g., Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020). Recently, retrieval-guided generation has gained increasing interest in a wide range of text generation tasks such as language modeling (Guu et al., 2018; Khandelwal et al., 2019; Guu et al., 2020), dialogue response generation (Weston et al., 2018; Wu et al., 2019; Cai et al., 7308 Retrieval Model Input x source sentence encoder ?""#$ (?) target sentence encoder ?()( (?) Translation Memory ? Translation Model ?1 ?5 … Source Encoder ?(?, ?1 ) ?(?, ?5 ) … Memory Encoder relevant TM MIPS … Decoder relevance scores ?-,/ Output y 23 /01 bias attention dense index Figure 1: Overall framework. For"
2021.acl-long.567,W04-3250,0,0.189957,"al (fixed) 66.68 66.24 63.06 62.73 cross-lingual (fixed Etgt )† 67.66 67.16 63.73 63.22 cross-lingual† 67.73 67.42 64.18 63.86 Retriever De⇒En Dev Test En⇒De Dev Test 60.10 61.85 60.26 61.72 55.54 57.43 55.14 56.88 59.82 63.62 63.25 64.39 64.48 60.76 63.85 63.06 64.01 64.62 55.01 57.88 57.61 58.12 58.77 54.90 57.53 56.97 57.92 58.42 Table 2: Experimental results (BLEU scores) on four translation tasks. ∗ Results are from Xia et al. (2019). †The two variants of our method (model #4 and model #5) are significantly better than other baselines with p-value < 0.01, tested by bootstrap re-sampling (Koehn, 2004). 2. TM-augmented NMT using source similarity search. To isolate the effect of architectural changes in NMT models, we replace our cross-lingual memory retriever with traditional source-side similarity search. Specifically, we use the fuzzy match system used in Xia et al. (2019) and many others, which is based on BM25 and edit distance. steps throughout all experiments. When trained with asynchronous index refresh, the re-indexing interval is 3K training steps.1 4.2 Conventional Experiments Following prior work in TM-augmented NMT, we first conduct experiments in a setting where the bilingual"
2021.acl-long.567,W17-3204,0,0.0269431,"BT with 2/4 bilingual pairs but performs worse with 1/4 bilingual pairs. Interestingly, the combination of BT and our method yields significant further gains, which demonstrates that our method is not only orthogonal but also complementary to BT. Non-parametric Domain Adaptation Lastly, the “plug and play” property of TM further motivates us to domain adaptation, where we adapt a single general-domain model to a specific domain by using domain-specific monolingual TM. Data To simulate a diverse multi-domain setting, we use the data splits in Aharoni and Goldberg (2020) originally collected by Koehn and Knowles (2017). It includes German-English parallel data for train/dev/test sets in five domains: Medical, Law, IT, Koran and Subtitles. Similar to the experiments in §4.3, we only use one fourth of bilingual pairs for training. The target side of the remaining data is treated as additional monolingual data for building domain-specific TM, and the source side is discarded. The data statistics can be found in the upper block of Table 4. The dev and test sets for each domain contains 2K instances. Models We first train a Transformer Base baseline (model #1) on the concatenation of bilingual pairs in all domai"
2021.acl-long.567,2010.jec-1.4,0,0.0246038,"Translation (MT), inspired by the Computer-Aided Translation (CAT) tools by professional human translators for increasing productivity for decades (Yamada, 2011), the usefulness of Translation Memory (TM) has long been recognized (Huang et al., 2021). In general, TM is a database that stores pairs of source text and its corresponding translations. Like for human ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). Correspondence to Yan Wang. translation, early work (Koehn and Senellart, 2010; He et al., 2010; Utiyama et al., 2011; Wang et al., 2013, inter alia) presents translations for similar source input to statistical translation models as additional cues. Recent work has confirmed that TM can help Neural Machine Translation (NMT) models as well. In a similar spirit to prior work, TM-augmented NMT models do not discard the training corpus after training but keep exploiting it in the test time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-s"
2021.acl-long.567,P19-1612,0,0.0249513,"r nearest neighbor search at each decoding step, which is very compute-intensive. The distinctions between our work and prior work are obvious: (1) The TM in our framework is a collection of monolingual sentences rather than bilingual sentence pairs; (2) We use learnable task-specific retrieval rather than generic retrieval mechanisms. Retrieval for Text Generation Discrete retrieval as an intermediate step has been shown beneficial to a variety of natural language processing tasks. One typical use is to retrieve supporting evidence for open-domain question answering (e.g., Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020). Recently, retrieval-guided generation has gained increasing interest in a wide range of text generation tasks such as language modeling (Guu et al., 2018; Khandelwal et al., 2019; Guu et al., 2020), dialogue response generation (Weston et al., 2018; Wu et al., 2019; Cai et al., 7308 Retrieval Model Input x source sentence encoder ?""#$ (?) target sentence encoder ?()( (?) Translation Memory ? Translation Model ?1 ?5 … Source Encoder ?(?, ?1 ) ?(?, ?5 ) … Memory Encoder relevant TM MIPS … Decoder relevance scores ?-,/ Output y 23 /01 bias attention dense index Figure 1"
2021.acl-long.567,P02-1040,0,0.111161,"in gradient estimate. We explore both options in our experiments. 4 Experiments We experiment with the proposed approach in three settings: (1) the conventional setting where the available TM is limited to the bilingual training corpus, (2) the low-resource setting where bilingual training pairs are scarce but extra monolingual data is exploited as additional TM, and (3) nonparametric domain adaptation using monolingual TM. Note that existing TM-augmented NMT models are only applicable to the first setting, the last two settings only become possible with our proposed model. We use BLEU score (Papineni et al., 2002) as the evaluation metric. 4.1 The second task is token-level cross-alignment, which aims to predict the tokens in the target language given the source sentence representation and vice versa. Formally, we use bag-of-words losses: X X (i) Ltok = − log p(wy |Xi ) + log p(wx |Yi ) wy ∈Yi Dataset En⇔Es En⇔De Implementation Details We build our model using Transformer blocks with the same configuration as Transformer Base (Vaswani et al., 2017) (8 attention heads, 512 dimensional hidden state, and 2048 dimensional feed-forward state). The number of Transformer blocks is 3 for the retrieval model, 4"
2021.acl-long.567,P17-1099,0,0.0281594,"th of the token sequence zi . We compute a cross attention over all TM sentences: exp(ht T Wm zi,j )) αij = PM PL T i i=1 k=1 exp(ht Wm zi,k ) ct = Wc Li M X X (1) αij zi,j i=1 j=1 where αij is the attention score of the j-th token in zi , ct is a weighted combination of memory embeddings, and Wm and Wc are trainable matrices. The cross attention is used twice during decoding. First, the decoder’s hidden state ht is updated by a weighted sum of memory embeddings, i.e., ht = ht + ct . Second, we consider each attention score as a probability of copying the corresponding token (Gu et al., 2016; See et al., 2017). Formally, the next-token probabilities are computed as: p(yt |·) = (1 − λt )Pv (yt ) + λt Li M X X αij 1zij =yt i=1 j=1 where 1 is the indicator function and λt is a gating variable computed by another feed-forward network λt = g(ht , ct ). Inspired by Lewis et al. (2020a), to enable the gradient flow from the translation output to the retrieval model, we bias the attention scores with the relevance scores, rewriting Eq. (1) as: exp(ht T Wm zi,j + βf (x, zi )) αij = PM PL T i i=1 k=1 exp(ht Wm zi,k + βf (x, zi )) (2) where β is a trainable scalar that controls the weight of the relevance sco"
2021.acl-long.567,P16-1009,0,0.219679,"his work is more challenging due to the cross-lingual setting. NMT using Monolingual Data To our knowledge, the integration of monolingual data for NMT was first investigated by Gulcehre et al. (2015), who separately trained target-side language models using monolingual data, and then integrated them during decoding either through re-scoring the beam, or by feeding the hidden state of the language model to the NMT model. Jean et al. (2015) also explored re-ranking the NMT output with a n-gram language model. Another successful method for leveraging monolingual data in NMT is back-translation (Sennrich et al., 2016; Fadaee et al., 2017; Edunov et al., 2018; He et al., 2016), where a reverse translation model is used to translate monolingual sentences from the target language to the source language to generate synthetic parallel sentences. Recent studies (Jiao et al., 2021; He et al., 2019) showed that self-training, where the synthetic parallel sentences are created by translating monolingual sentences in the source language, is also helpful. Our method is orthogonal to previous work and bears a unique feature: it can use more monolingual data without re-training (see §4.3). 3 Proposed Approach We start"
2021.acl-long.567,2011.mtsummit-papers.37,0,0.623043,"Aided Translation (CAT) tools by professional human translators for increasing productivity for decades (Yamada, 2011), the usefulness of Translation Memory (TM) has long been recognized (Huang et al., 2021). In general, TM is a database that stores pairs of source text and its corresponding translations. Like for human ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). Correspondence to Yan Wang. translation, early work (Koehn and Senellart, 2010; He et al., 2010; Utiyama et al., 2011; Wang et al., 2013, inter alia) presents translations for similar source input to statistical translation models as additional cues. Recent work has confirmed that TM can help Neural Machine Translation (NMT) models as well. In a similar spirit to prior work, TM-augmented NMT models do not discard the training corpus after training but keep exploiting it in the test time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps"
2021.acl-long.567,P13-1002,0,0.17433,") tools by professional human translators for increasing productivity for decades (Yamada, 2011), the usefulness of Translation Memory (TM) has long been recognized (Huang et al., 2021). In general, TM is a database that stores pairs of source text and its corresponding translations. Like for human ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). Correspondence to Yan Wang. translation, early work (Koehn and Senellart, 2010; He et al., 2010; Utiyama et al., 2011; Wang et al., 2013, inter alia) presents translations for similar source input to statistical translation models as additional cues. Recent work has confirmed that TM can help Neural Machine Translation (NMT) models as well. In a similar spirit to prior work, TM-augmented NMT models do not discard the training corpus after training but keep exploiting it in the test time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps (Gu et al., 2018;"
2021.acl-long.567,W18-5713,0,0.0240343,"task-specific retrieval rather than generic retrieval mechanisms. Retrieval for Text Generation Discrete retrieval as an intermediate step has been shown beneficial to a variety of natural language processing tasks. One typical use is to retrieve supporting evidence for open-domain question answering (e.g., Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020). Recently, retrieval-guided generation has gained increasing interest in a wide range of text generation tasks such as language modeling (Guu et al., 2018; Khandelwal et al., 2019; Guu et al., 2020), dialogue response generation (Weston et al., 2018; Wu et al., 2019; Cai et al., 7308 Retrieval Model Input x source sentence encoder ?""#$ (?) target sentence encoder ?()( (?) Translation Memory ? Translation Model ?1 ?5 … Source Encoder ?(?, ?1 ) ?(?, ?5 ) … Memory Encoder relevant TM MIPS … Decoder relevance scores ?-,/ Output y 23 /01 bias attention dense index Figure 1: Overall framework. For an input sentence x in the source language, the retrieval model uses Maximum Inner Product Search (MIPS) to find the top-M TM sentences {zi }M i=1 in the target language. The translation M model takes {zi }M and corresponding relevance scores {f (x,"
2021.acl-long.567,2020.acl-main.144,0,0.573379,"ork has confirmed that TM can help Neural Machine Translation (NMT) models as well. In a similar spirit to prior work, TM-augmented NMT models do not discard the training corpus after training but keep exploiting it in the test time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), embedding-based matches (Cao and Xiong, 2018), or a hybrid (Bulte and Tezcan, 2019; Xu et al., 2020); In the generation stage, the retrieved translations are injected into a standard NMT model by attending over them with sophisticated memory networks (Gu et al., 2018; Cao and Xiong, 2018; Xia et al., 2019; He et al., 2021) or directly concatenating them to the source input (Bulte and Tezcan, 2019; Xu et al., 2020), or biasing the word distribution during decoding (Zhang et al., 2018). Most recently, Khandelwal et al. (2020) propose a token-level nearest neighbor search using complete translation context, i.e., both the source-side input and target-side prefix. Despite their differences, we i"
2021.acl-long.567,N18-1120,0,0.470857,", inter alia) presents translations for similar source input to statistical translation models as additional cues. Recent work has confirmed that TM can help Neural Machine Translation (NMT) models as well. In a similar spirit to prior work, TM-augmented NMT models do not discard the training corpus after training but keep exploiting it in the test time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), embedding-based matches (Cao and Xiong, 2018), or a hybrid (Bulte and Tezcan, 2019; Xu et al., 2020); In the generation stage, the retrieved translations are injected into a standard NMT model by attending over them with sophisticated memory networks (Gu et al., 2018; Cao and Xiong, 2018; Xia et al., 2019; He et al., 2021) or directly concatenating them to the source input (Bulte and Tezcan, 2019; Xu et al., 2020), or biasing the word distribution during decoding (Zhang et al., 2018). Most recently, Khandelwal et al. (2020) propose a token-level nearest neighbor search usi"
2021.acl-short.64,W18-6217,0,0.458705,"Missing"
2021.acl-short.64,D17-1047,1,0.84049,"ectively. ∗ Work done when Wenxuan Zhang was an intern at Alibaba. The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). 1 The data and code can be found at https://github. com/IsakZhang/Generative-ABSA The main research line of ABSA focuses on the identification of those sentiment elements such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al., 2019b). By designin"
2021.acl-short.64,2020.acl-main.582,0,0.817037,"a pre-defined category set. For example, 2.2 Generation Model R14 R15 R16 JERE-MHS† SpanMlt (Zhao et al., 2020) SDRN (Chen et al., 2020) 53.41 52.34 68.66 66.18 62.39 66.02 75.60 73.30 58.12 59.64 64.68 65.75 63.84 67.65 71.78 73.67 G AS -A NNOTATION -R G AS -E XTRACTION -R G AS -A NNOTATION G AS -E XTRACTION 68.74 67.58 69.55 68.08 72.66 73.22 75.15 74.12 65.03 65.83 67.93 67.19 73.75 74.12 75.42 74.54 Table 1: Main results of the AOPE task. The best results are in bold, second best results are underlined. Results are the average F1 scores over 5 runs. † denotes results are from Zhao et al. (2020). Input: A big disappointment, all around. The pizza was cold and the cheese wasn’t even fully melted. Target (Annotation-style): A big disappointment, all around. The [pizza |food quality |negative] was cold and the [cheese | food quality |negative] wasn’t even fully melted [null |restaurant general |negative]. Target (Extraction-style): (pizza, food quality, negative); (cheese, food quality, negative); (null, restaurant general, negative); Similarly, we pack each aspect term, the aspect category it belongs to, and its sentiment polarity into a bracket to build the target sentence for the ann"
2021.acl-short.64,2020.acl-main.340,0,0.697434,"pinion terms, we append the associated opinion modifier to each aspect term in the form of [aspect |opinion] for constructing the target sentence, as shown in the above example. The prediction of the coupled aspect and opinion term is thus achieved by including them in the same bracket. For the extraction-style paradigm, we treat the desired pairs as the target, which resembles direct extraction of the expected sentiment elements but in a generative manner. Unified ABSA (UABSA) is the task of extracting aspect terms and predicting their sentiment polarities at the same time (Li et al., 2019a; Chen and Qian, 2020). We also formulate it as an (aspect, sentiment polarity) pair extraction problem. For the same example given above, we aim to extract two pairs: (Salads, positive) and (server, positive). Similarly, we replace each aspect term as [aspect | sentiment polarity] under the annotation-style formulation and treat the desired pairs as the target output in the extraction-style paradigm to reformulate the UABSA task as a text generation problem. Aspect Sentiment Triplet Extraction (ASTE) aims to discover more complicated (aspect, opinion, sentiment polarity) triplets (Peng et al., 2020): Input: The Un"
2021.acl-short.64,N19-1259,0,0.0145666,"size of 16 and accumulate gradients every two batches. The learning rate is set to be 3e-4. The model is trained up to 20 epochs for the AOPE, UABSA, and ASTE task and 30 epochs for the TASD task. 3 3.1 Experiments Experimental Setup Datasets We evaluate the proposed G AS framework on four popular benchmark datasets including Laptop14, Rest14, Rest15, and Rest16, originally provided by the SemEval shared challenges (Pontiki et al., 2014, 2015, 2016). For each ABSA task, we use the public datasets derived from them with more sentiment annotations. Specifically, we adopt the dataset provided by Fan et al. (2019), Li et al. (2019a), Xu et al. (2020), Wan et al. (2020) for the AOPE, UABSA, ASTE, TASD task respectively. For a fair comparison, we use the same data split as previous works. Evaluation Metrics We adopt F1 scores as the main evaluation metrics for all tasks. A prediction is correct if and only if all its predicted sentiment elements in the pair or triplet are correct. Experiment Details We adopt the T5 base model from huggingface Transformer library2 for 2 https://github.com/huggingface/ transformers 3.2 Main Results The main results for the AOPE, UABSA, ASTE, TASD task are reported in Table"
2021.acl-short.64,P19-1048,0,0.400794,"l., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al., 2019b). By designing taskspecific classification networks, the prediction is made in a discriminative manner, using the class index as labels for training (Huang and Carley, 2018; Wan et al., 2020). However, these methods ignore the label semantics, i.e., the meaning of the natural language labels, during the training process. Intuitively, knowing the meaning of “food quality” and “restaurant ambiance”, it can be much easier to identify that the former one is more likely to"
2021.acl-short.64,P19-1051,0,0.436993,"Missing"
2021.acl-short.64,D18-1136,0,0.1029,"e elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al., 2019b). By designing taskspecific classification networks, the prediction is made in a discriminative manner, using the class index as labels for training (Huang and Carley, 2018; Wan et al., 2020). However, these methods ignore the label semantics, i.e., the meaning of the natural language labels, during the training process. Intuitively, knowing the meaning of “food quality” and “restaurant ambiance”, it can be much easier to identify that the former one is more likely to be the correct aspect category for the concerned aspect “pizza”. Such semantics of the label can be more helpful for the joint extraction of multiple sentiment elements, due to the complicated interactions of those involved elements. For example, understanding “delicious” is an adjective for descri"
2021.acl-short.64,D19-1654,0,0.0169393,"ne when Wenxuan Zhang was an intern at Alibaba. The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). 1 The data and code can be found at https://github. com/IsakZhang/Generative-ABSA The main research line of ABSA focuses on the identification of those sentiment elements such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al., 2019b). By designing taskspecific class"
2021.acl-short.64,D19-5505,1,0.787255,"g Special Administrative Region, China (Project Code: 14200719). 1 The data and code can be found at https://github. com/IsakZhang/Generative-ABSA The main research line of ABSA focuses on the identification of those sentiment elements such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al., 2019b). By designing taskspecific classification networks, the prediction is made in a discriminative manner, using the class index as labels for training (Huang and Carley, 2018; Wan et al., 2020)."
2021.acl-short.64,D15-1168,0,0.186739,"Missing"
2021.acl-short.64,P19-1056,0,0.180058,"ect term (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al., 2019b). By designing taskspecific classification networks, the prediction is made in a discriminative manner, using the class index as labels for training (Huang and Carley, 2018; Wan et al., 2020). However, these methods ignore the label semantics, i.e., the meaning of the natural language labels, during the training process. Intuitively, knowing the meaning of “food quality” and “restaurant ambiance”, it can be much easier to identify that the former one"
2021.acl-short.64,P19-1344,0,0.0615644,"us.”, the corresponding elements are “pizza”, “delicious”, “food quality” and “positive”, respectively. ∗ Work done when Wenxuan Zhang was an intern at Alibaba. The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). 1 The data and code can be found at https://github. com/IsakZhang/Generative-ABSA The main research line of ABSA focuses on the identification of those sentiment elements such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated"
2021.acl-short.64,S15-2082,0,0.281002,"Missing"
2021.acl-short.64,S14-2004,0,0.51721,"Missing"
2021.acl-short.64,D16-1058,0,0.0673404,"nd “positive”, respectively. ∗ Work done when Wenxuan Zhang was an intern at Alibaba. The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). 1 The data and code can be found at https://github. com/IsakZhang/Generative-ABSA The main research line of ABSA focuses on the identification of those sentiment elements such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al.,"
2021.acl-short.64,2020.emnlp-main.183,1,0.947028,"a pre-defined category set. For example, 2.2 Generation Model R14 R15 R16 JERE-MHS† SpanMlt (Zhao et al., 2020) SDRN (Chen et al., 2020) 53.41 52.34 68.66 66.18 62.39 66.02 75.60 73.30 58.12 59.64 64.68 65.75 63.84 67.65 71.78 73.67 G AS -A NNOTATION -R G AS -E XTRACTION -R G AS -A NNOTATION G AS -E XTRACTION 68.74 67.58 69.55 68.08 72.66 73.22 75.15 74.12 65.03 65.83 67.93 67.19 73.75 74.12 75.42 74.54 Table 1: Main results of the AOPE task. The best results are in bold, second best results are underlined. Results are the average F1 scores over 5 runs. † denotes results are from Zhao et al. (2020). Input: A big disappointment, all around. The pizza was cold and the cheese wasn’t even fully melted. Target (Annotation-style): A big disappointment, all around. The [pizza |food quality |negative] was cold and the [cheese | food quality |negative] wasn’t even fully melted [null |restaurant general |negative]. Target (Extraction-style): (pizza, food quality, negative); (cheese, food quality, negative); (null, restaurant general, negative); Similarly, we pack each aspect term, the aspect category it belongs to, and its sentiment polarity into a bracket to build the target sentence for the ann"
2021.acl-short.64,2020.emnlp-main.286,0,0.345888,"g was an intern at Alibaba. The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). 1 The data and code can be found at https://github. com/IsakZhang/Generative-ABSA The main research line of ABSA focuses on the identification of those sentiment elements such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al., 2019b). By designing taskspecific classification networks, the"
2021.acl-short.64,2020.acl-main.296,0,0.637006,"4200719). 1 The data and code can be found at https://github. com/IsakZhang/Generative-ABSA The main research line of ABSA focuses on the identification of those sentiment elements such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al., 2019b). By designing taskspecific classification networks, the prediction is made in a discriminative manner, using the class index as labels for training (Huang and Carley, 2018; Wan et al., 2020). However, these methods ignore the label semantics, i.e.,"
2021.emnlp-main.726,2020.emnlp-main.27,0,0.0354735,"s, which are usually formulated as either token-level or sequence-level classification problems, underutilize the rich semantic information of the label (i.e., the meaning of sentiment elements to be predicted) since they treat the labels as number indices during training. Intuitively, the aspect term “pasta” is unlikely to be coupled with the aspect category “service general” due to the large semantic gap between them. But such information cannot be suitably utilized in those classification-type methods. Inspired by recent success in formulating various NLP tasks as text generation problems (Athiwaratkun et al., 2020; Paolini et al., 2021; Liu et al., 2021), we propose to tackle ASQP in a sequenceto-sequence (S2S) manner in this paper. On one hand, the sentiment quads can be predicted in an end-to-end manner, alleviating the potential error propagation in the pipeline solutions. On the other hand, the rich label semantic information could be fully exploited by learning to generate the sentiment elements in the natural language form. of natural sentences, rather than directly treating the desired sentiment quad text sequence as the generation target (Zhang et al., 2021). We summarize our contributions as f"
2021.emnlp-main.726,W18-6217,0,0.0124173,"al., 2017), Li-unified-R (Li et al., 2019a), Peng-pipeline (Peng et al., 2020) which firstly extract aspect and opinion terms separately, then conduct the pairing; Two-stage (Huang et al., 2021) which proposes a two-stage method to enhance the correlation between aspects and opinions; and 2) end-to-end models including GTS (Wu et al., 2020) and Jet (Xu et al., 2020), both designing unified tagging schemes in order to solve the task in an end-to-end fashion. For the TASD task, we adopt the dataset prepared by Wan et al. (2020). We compare with a pipeline-type baseline method Baseline-1-f_lex (Brun and Nikoulina, 2018), two BERT based models including TAS-CRF and TAS-TO (Wan et al., 2020), and a recent model MEJD (Wu et al., 2021) 3 which utilizes a graph structure to model the depenThe mapping relation between the category and their indexes is pre-defined based on the entire dataset. dency among the sentiment elements. 9215 Different from previous classification-type methods for tackling ABSA problem, our PARAPHRASE modeling can take advantage of the semantics of the sentiment elements by generating the natural language labels. We conduct ablation studies to further investigate the impact of the label sema"
2021.emnlp-main.726,2021.naacl-main.167,0,0.0203398,"els in all cases. 4) The experiment also suggests that our PARAPHRASE method naturally facilitates the knowledge transfer across related tasks with the unified framework, which can be especially beneficial in the low-resource setting.1 2 Related Work ABSA has been extensively studied in recent years where the main research line is the extraction of the sentiment elements. Early studies focus on the prediction of a single element such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Xu et al., 2018; Ma et al., 2019), detecting the mentioned aspect category (Zhou et al., 2015; Bu et al., 2021), and predicting the sentiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo et al., 2019; Chen and Qian, 2020); and the co-extraction of aspect category an"
2021.emnlp-main.726,2020.coling-main.72,0,0.122704,"entiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo et al., 2019; Chen and Qian, 2020); and the co-extraction of aspect category and sentiment polarity (Cai et al., 2020). More recently, triplet prediction tasks are proposed in ABSA, aiming to predict the sentiment elements in triplet format. Peng et al. (2020) propose the aspect sentiment triplet extraction (ASTE) task, which has received lots of attention (Xu et al., 2020; Huang et al., 2021; Mao et al., 2021; Chen Exploiting generation modeling for the ASQP task mainly faces two challenges: (i) how to linearize the desired sentiment information so as to facilitate the S2S learning? (ii) how can we utilize the pretrained models for tackling the task, which is a common practice now for solving various ABSA ta"
2021.emnlp-main.726,2020.acl-main.582,0,0.394942,"5), aspect opinionated sentence, which can reveal a more sentiment classification based on either an aspect comprehensive and complete aspect-level sencategory (Ruder et al., 2016; Hu et al., 2019a) or timent structure. We further propose a novel PARAPHRASE modeling paradigm to cast the an aspect term (Huang and Carley, 2018). More ASQP task to a paraphrase generation process. recent works propose to extract multiple associated On one hand, the generation formulation alsentiment elements at the same time (Zhang et al., lows solving ASQP in an end-to-end manner, 2021). For example, Chen et al. (2020) consider the alleviating the potential error propagation in aspect and opinion term pairwise extraction; Peng the pipeline solution. On the other hand, the seet al. (2020) propose the aspect sentiment triplet mantics of the sentiment elements can be fully extraction (ASTE) task to detect the (aspect term, exploited by learning to generate them in the natural language form. Extensive experiments opinion term, sentiment polarity) triplets; Wan et al. on benchmark datasets show the superiority of (2020) handle the target aspect sentiment detection our proposed method and the capacity of cross(TA"
2021.emnlp-main.726,P19-1048,0,0.0189274,"2016; Xu et al., 2018; Ma et al., 2019), detecting the mentioned aspect category (Zhou et al., 2015; Bu et al., 2021), and predicting the sentiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo et al., 2019; Chen and Qian, 2020); and the co-extraction of aspect category and sentiment polarity (Cai et al., 2020). More recently, triplet prediction tasks are proposed in ABSA, aiming to predict the sentiment elements in triplet format. Peng et al. (2020) propose the aspect sentiment triplet extraction (ASTE) task, which has received lots of attention (Xu et al., 2020; Huang et al., 2021; Mao et al., 2021; Chen Exploiting generation modeling for the ASQP task mainly faces two challenges: (i) how to linearize the desired sentiment information so as to facilitate the"
2021.emnlp-main.726,D19-1467,0,0.0798106,"extract those sentiment elements (Pontiki et al., ing the four elements in one shot. In this work, 2014, 2015, 2016). Early studies focus on the prewe introduce the Aspect Sentiment Quad Prediction of a single element such as aspect term diction (ASQP) task, aiming to jointly detect extraction (Liu et al., 2015; Xu et al., 2018), asall sentiment elements in quads for a given pect category detection (Zhou et al., 2015), aspect opinionated sentence, which can reveal a more sentiment classification based on either an aspect comprehensive and complete aspect-level sencategory (Ruder et al., 2016; Hu et al., 2019a) or timent structure. We further propose a novel PARAPHRASE modeling paradigm to cast the an aspect term (Huang and Carley, 2018). More ASQP task to a paraphrase generation process. recent works propose to extract multiple associated On one hand, the generation formulation alsentiment elements at the same time (Zhang et al., lows solving ASQP in an end-to-end manner, 2021). For example, Chen et al. (2020) consider the alleviating the potential error propagation in aspect and opinion term pairwise extraction; Peng the pipeline solution. On the other hand, the seet al. (2020) propose the aspec"
2021.emnlp-main.726,P19-1051,0,0.0718185,"extract those sentiment elements (Pontiki et al., ing the four elements in one shot. In this work, 2014, 2015, 2016). Early studies focus on the prewe introduce the Aspect Sentiment Quad Prediction of a single element such as aspect term diction (ASQP) task, aiming to jointly detect extraction (Liu et al., 2015; Xu et al., 2018), asall sentiment elements in quads for a given pect category detection (Zhou et al., 2015), aspect opinionated sentence, which can reveal a more sentiment classification based on either an aspect comprehensive and complete aspect-level sencategory (Ruder et al., 2016; Hu et al., 2019a) or timent structure. We further propose a novel PARAPHRASE modeling paradigm to cast the an aspect term (Huang and Carley, 2018). More ASQP task to a paraphrase generation process. recent works propose to extract multiple associated On one hand, the generation formulation alsentiment elements at the same time (Zhang et al., lows solving ASQP in an end-to-end manner, 2021). For example, Chen et al. (2020) consider the alleviating the potential error propagation in aspect and opinion term pairwise extraction; Peng the pipeline solution. On the other hand, the seet al. (2020) propose the aspec"
2021.emnlp-main.726,D18-1136,0,0.105194,"studies focus on the prewe introduce the Aspect Sentiment Quad Prediction of a single element such as aspect term diction (ASQP) task, aiming to jointly detect extraction (Liu et al., 2015; Xu et al., 2018), asall sentiment elements in quads for a given pect category detection (Zhou et al., 2015), aspect opinionated sentence, which can reveal a more sentiment classification based on either an aspect comprehensive and complete aspect-level sencategory (Ruder et al., 2016; Hu et al., 2019a) or timent structure. We further propose a novel PARAPHRASE modeling paradigm to cast the an aspect term (Huang and Carley, 2018). More ASQP task to a paraphrase generation process. recent works propose to extract multiple associated On one hand, the generation formulation alsentiment elements at the same time (Zhang et al., lows solving ASQP in an end-to-end manner, 2021). For example, Chen et al. (2020) consider the alleviating the potential error propagation in aspect and opinion term pairwise extraction; Peng the pipeline solution. On the other hand, the seet al. (2020) propose the aspect sentiment triplet mantics of the sentiment elements can be fully extraction (ASTE) task to detect the (aspect term, exploited by"
2021.emnlp-main.726,D19-5505,1,0.895374,"2015; Yin et al., 2016; Xu et al., 2018; Ma et al., 2019), detecting the mentioned aspect category (Zhou et al., 2015; Bu et al., 2021), and predicting the sentiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo et al., 2019; Chen and Qian, 2020); and the co-extraction of aspect category and sentiment polarity (Cai et al., 2020). More recently, triplet prediction tasks are proposed in ABSA, aiming to predict the sentiment elements in triplet format. Peng et al. (2020) propose the aspect sentiment triplet extraction (ASTE) task, which has received lots of attention (Xu et al., 2020; Huang et al., 2021; Mao et al., 2021; Chen Exploiting generation modeling for the ASQP task mainly faces two challenges: (i) how to linearize the desired sentiment information so as"
2021.emnlp-main.726,D15-1168,0,0.0668368,"Missing"
2021.emnlp-main.726,P19-1056,0,0.0180852,"2019), detecting the mentioned aspect category (Zhou et al., 2015; Bu et al., 2021), and predicting the sentiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo et al., 2019; Chen and Qian, 2020); and the co-extraction of aspect category and sentiment polarity (Cai et al., 2020). More recently, triplet prediction tasks are proposed in ABSA, aiming to predict the sentiment elements in triplet format. Peng et al. (2020) propose the aspect sentiment triplet extraction (ASTE) task, which has received lots of attention (Xu et al., 2020; Huang et al., 2021; Mao et al., 2021; Chen Exploiting generation modeling for the ASQP task mainly faces two challenges: (i) how to linearize the desired sentiment information so as to facilitate the S2S learning? (ii) how can we utili"
2021.emnlp-main.726,P19-1344,0,0.0160416,"P as well as other ABSA tasks, outperforming the previous state-of-the-art models in all cases. 4) The experiment also suggests that our PARAPHRASE method naturally facilitates the knowledge transfer across related tasks with the unified framework, which can be especially beneficial in the low-resource setting.1 2 Related Work ABSA has been extensively studied in recent years where the main research line is the extraction of the sentiment elements. Early studies focus on the prediction of a single element such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Xu et al., 2018; Ma et al., 2019), detecting the mentioned aspect category (Zhou et al., 2015; Bu et al., 2021), and predicting the sentiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo e"
2021.emnlp-main.726,S15-2082,0,0.0730391,"Missing"
2021.emnlp-main.726,S14-2004,0,0.108259,"Missing"
2021.emnlp-main.726,D16-1103,0,0.0925659,"nstead of predictor extract those sentiment elements (Pontiki et al., ing the four elements in one shot. In this work, 2014, 2015, 2016). Early studies focus on the prewe introduce the Aspect Sentiment Quad Prediction of a single element such as aspect term diction (ASQP) task, aiming to jointly detect extraction (Liu et al., 2015; Xu et al., 2018), asall sentiment elements in quads for a given pect category detection (Zhou et al., 2015), aspect opinionated sentence, which can reveal a more sentiment classification based on either an aspect comprehensive and complete aspect-level sencategory (Ruder et al., 2016; Hu et al., 2019a) or timent structure. We further propose a novel PARAPHRASE modeling paradigm to cast the an aspect term (Huang and Carley, 2018). More ASQP task to a paraphrase generation process. recent works propose to extract multiple associated On one hand, the generation formulation alsentiment elements at the same time (Zhang et al., lows solving ASQP in an end-to-end manner, 2021). For example, Chen et al. (2020) consider the alleviating the potential error propagation in aspect and opinion term pairwise extraction; Peng the pipeline solution. On the other hand, the seet al. (2020)"
2021.emnlp-main.726,N19-1035,0,0.0187814,"eously, which can handle the case where the aspect term is implicit expressed in the given text (treated as “null”) (Wu et al., 2021). Built on top of those tasks, we introduce the aspect sentiment quad prediction problem, aiming to predict the four sentiment elements in one shot, which can provide a more detailed and comprehensive sentiment structure for a given text. Adopting pretrained transformer-based models such as BERT (Devlin et al., 2019) has become a common practice for tackling the ABSA problem. Especially, many ABSA tasks benefit from appropriately utilizing the pretrained models. Sun et al. (2019) transform the aspect sentiment classification task as a language inference problem by constructing an auxiliary sentence. Chen et al. (2021) and Mao et al. (2021) formulate multiple ABSA tasks as a reading comprehension task to fully utilize the knowledge of the pre-trained model. Very recently, there are some attempts on tackling ABSA problem in a S2S manner, either treating the class index (Yan et al., 2021) or the desired sentiment element sequence (Zhang et al., 2021) as the target of the generation model. In this work, we propose a PARAPHRASE modeling that can better utilize the knowledg"
2021.emnlp-main.726,D16-1058,0,0.0227744,"facilitates the knowledge transfer across related tasks with the unified framework, which can be especially beneficial in the low-resource setting.1 2 Related Work ABSA has been extensively studied in recent years where the main research line is the extraction of the sentiment elements. Early studies focus on the prediction of a single element such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Xu et al., 2018; Ma et al., 2019), detecting the mentioned aspect category (Zhou et al., 2015; Bu et al., 2021), and predicting the sentiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo et al., 2019; Chen and Qian, 2020); and the co-extraction of aspect category and sentiment polarity (Cai et al., 2020). More recently, triplet prediction tasks are pr"
2021.emnlp-main.726,2020.emnlp-main.286,0,0.0164906,"related tasks with the unified framework, which can be especially beneficial in the low-resource setting.1 2 Related Work ABSA has been extensively studied in recent years where the main research line is the extraction of the sentiment elements. Early studies focus on the prediction of a single element such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Xu et al., 2018; Ma et al., 2019), detecting the mentioned aspect category (Zhou et al., 2015; Bu et al., 2021), and predicting the sentiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo et al., 2019; Chen and Qian, 2020); and the co-extraction of aspect category and sentiment polarity (Cai et al., 2020). More recently, triplet prediction tasks are proposed in ABSA, aiming to predict the sentiment"
2021.emnlp-main.726,2021.acl-short.64,1,0.533825,"s as text generation problems (Athiwaratkun et al., 2020; Paolini et al., 2021; Liu et al., 2021), we propose to tackle ASQP in a sequenceto-sequence (S2S) manner in this paper. On one hand, the sentiment quads can be predicted in an end-to-end manner, alleviating the potential error propagation in the pipeline solutions. On the other hand, the rich label semantic information could be fully exploited by learning to generate the sentiment elements in the natural language form. of natural sentences, rather than directly treating the desired sentiment quad text sequence as the generation target (Zhang et al., 2021). We summarize our contributions as follows: 1) We study a new task, namely aspect sentiment quad prediction (ASQP) in this work and introduce two datasets with sentiment quad annotations for each sample, aiming to analyze more comprehensive aspect-level sentiment information. 2) We propose to tackle ASQP as a paraphrase generation problem, which can predict the sentiment quads in one shot and fully utilize the semantics information of natural language labels. 3) Extensive experiments show that the proposed PARAPHRASE modeling is effective to tackle ASQP as well as other ABSA tasks, outperform"
2021.emnlp-main.726,2020.acl-main.296,0,0.0362008,"focus on the prediction of a single element such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Xu et al., 2018; Ma et al., 2019), detecting the mentioned aspect category (Zhou et al., 2015; Bu et al., 2021), and predicting the sentiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo et al., 2019; Chen and Qian, 2020); and the co-extraction of aspect category and sentiment polarity (Cai et al., 2020). More recently, triplet prediction tasks are proposed in ABSA, aiming to predict the sentiment elements in triplet format. Peng et al. (2020) propose the aspect sentiment triplet extraction (ASTE) task, which has received lots of attention (Xu et al., 2020; Huang et al., 2021; Mao et al., 2021; Chen Exploiting generation modeling for the ASQP task"
2021.emnlp-main.726,2020.findings-emnlp.234,0,0.0283253,"ablations on the sentiment polarity and aspect category, the model suffers more when the aspect category is projected to an indexed symbol. The possible reason is that there are only three types of sentiment polarities, which is much less than the number of types for the aspect category. Therefore, it can be easier for the model to learn the mapping between the special symbols and the polarity type during the training. 5.2 5.3 Effect of Label Semantics L14 R14 R15 R16 CMLA+ (Wang et al., 2017) Li-unified-R (Li et al., 2019a) P-pipeline (Peng et al., 2020) Jet+BERT (Xu et al., 2020) GTS+BERT (Wu et al., 2020) Two-Stage (Huang et al., 2021) 33.16 42.34 42.87 51.04 55.21 58.58 42.79 51.00 51.46 62.40 64.81 68.16 37.01 47.82 52.32 57.53 54.88 58.59 41.72 44.31 54.21 63.83 66.08 67.52 GAS (Zhang et al., 2021) PARAPHRASE 58.19 61.13 70.52 72.03 60.23 62.56 69.05 71.70 Table 3: Results of the ASTE task compared with previous state-of-the-art models. F1 scores are reported. Results on ASTE and TASD Tasks As described in Sec 3.4, the proposed PARA PHRASE modeling provides a unified framework to tackle the ABSA problem, we thus test it on the ASTE and TASD tasks, and compare with the previous state-of-the-"
2021.emnlp-main.726,P18-2094,0,0.136292,"ements, including the aspect category, aspect Due to its broad application scenarios, many reterm, opinion term, and sentiment polarity. Existing studies usually consider the detection of search efforts have been made on ABSA to predict partial sentiment elements, instead of predictor extract those sentiment elements (Pontiki et al., ing the four elements in one shot. In this work, 2014, 2015, 2016). Early studies focus on the prewe introduce the Aspect Sentiment Quad Prediction of a single element such as aspect term diction (ASQP) task, aiming to jointly detect extraction (Liu et al., 2015; Xu et al., 2018), asall sentiment elements in quads for a given pect category detection (Zhou et al., 2015), aspect opinionated sentence, which can reveal a more sentiment classification based on either an aspect comprehensive and complete aspect-level sencategory (Ruder et al., 2016; Hu et al., 2019a) or timent structure. We further propose a novel PARAPHRASE modeling paradigm to cast the an aspect term (Huang and Carley, 2018). More ASQP task to a paraphrase generation process. recent works propose to extract multiple associated On one hand, the generation formulation alsentiment elements at the same time ("
2021.emnlp-main.726,2020.emnlp-main.183,1,0.871168,"5), aspect opinionated sentence, which can reveal a more sentiment classification based on either an aspect comprehensive and complete aspect-level sencategory (Ruder et al., 2016; Hu et al., 2019a) or timent structure. We further propose a novel PARAPHRASE modeling paradigm to cast the an aspect term (Huang and Carley, 2018). More ASQP task to a paraphrase generation process. recent works propose to extract multiple associated On one hand, the generation formulation alsentiment elements at the same time (Zhang et al., lows solving ASQP in an end-to-end manner, 2021). For example, Chen et al. (2020) consider the alleviating the potential error propagation in aspect and opinion term pairwise extraction; Peng the pipeline solution. On the other hand, the seet al. (2020) propose the aspect sentiment triplet mantics of the sentiment elements can be fully extraction (ASTE) task to detect the (aspect term, exploited by learning to generate them in the natural language form. Extensive experiments opinion term, sentiment polarity) triplets; Wan et al. on benchmark datasets show the superiority of (2020) handle the target aspect sentiment detection our proposed method and the capacity of cross(TA"
2021.emnlp-main.726,2021.acl-long.188,0,0.0380688,"ls such as BERT (Devlin et al., 2019) has become a common practice for tackling the ABSA problem. Especially, many ABSA tasks benefit from appropriately utilizing the pretrained models. Sun et al. (2019) transform the aspect sentiment classification task as a language inference problem by constructing an auxiliary sentence. Chen et al. (2021) and Mao et al. (2021) formulate multiple ABSA tasks as a reading comprehension task to fully utilize the knowledge of the pre-trained model. Very recently, there are some attempts on tackling ABSA problem in a S2S manner, either treating the class index (Yan et al., 2021) or the desired sentiment element sequence (Zhang et al., 2021) as the target of the generation model. In this work, we propose a PARAPHRASE modeling that can better utilize the knowledge of the pre-trained model via casting the original task to a paraphrase generation process. ASQP The wine list yesterday … ! The wine list yesterday was excellent, but the place is too tiny for me! ASQP Figure 1: Overview of the paraphrase generation framework. The underlined task identifier in the input is only used under the cross-task transfer setting. tion problem and solve it in a sequence-to-sequence man"
2021.emnlp-main.727,L18-1344,0,0.0167474,"fic knowledge for the target languages, previous works usually first translate the source sentence with an off-the-shelf translation system, then word alignment tools such as fastAlign (Dyer et al., 2013) are used to map the token-level label from the source sentence to the translated sentence (Mayhew et al., 2017; Fei et al., 2020). Some heuristics are proposed for alleviating the alignment error, for example, by conducting a phrase-to-phrase mapping to refine the aspect boundary (Klinger and Cimiano, 2015; Li et al., 2020). However, the word or phrase alignment itself is a challenging task (Akbik and Vollgraf, 2018). The sentences of the ABSA task are usually user-generated (e.g., product reviews and tweets) and informal, which further hinders the translatethen-align method to produce satisfactory pseudolabeled target data (Lohar et al., 2019). The inaccurate pseudo labels inevitably limit the task-specific knowledge and lead to poor model performance. We propose an alignment-free label projection method for obtaining the pseudo-labeled data in the target language3 . As depicted in the upper-left portion of Figure 1, we first mark each aspect term in the sentence with a special symbol (e.g., different br"
2021.emnlp-main.727,N18-1053,0,0.12141,"imiano, 2015). Later methods make use of the task of extracting mentioned aspects from a given cross-lingual word embeddings (Ruder et al., 2019) sentence and predicting their corresponding senti- trained on large parallel corpus to allow the model ment polarities1 (Liu, 2012; Pontiki et al., 2014). to be used in a language-independent manner, by Consider the following example, “The food is great, simply switching the word embedding layer while but the service is kinda disappointing”, we can de- keeping the model unchanged (Barnes et al., 2016; tect two mentioned aspect terms “food” and “ser- Akhtar et al., 2018; Jebbara and Cimiano, 2019) vice”, and judge their corresponding sentiments when adopted for different languages. as positive and negative, respectively. Given its Recently, employing multilingual pre-trained ∗ models such as the multilingual BERT (Devlin Work done when Wenxuan Zhang was an intern at Alibaba. This work was supported by Alibaba Group through et al., 2019) and XLM-Roberta (Conneau et al., Alibaba Research Intern Program, and a grant from the Re2020) has become the de-facto approach to tackle search Grant Council of the Hong Kong Special Administrathe cross-lingual transfer for"
2021.emnlp-main.727,C16-1152,0,0.0537568,"Missing"
2021.emnlp-main.727,2020.acl-main.340,0,0.0676377,"Missing"
2021.emnlp-main.727,2020.acl-main.747,0,0.11228,"learned in the pre-training stage (Wu and Dredze, 2019; K et al., 2020). There are some challenges for adopting such a paradigm to solve the cross-lingual ABSA task. The language-specific knowledge plays an essential role in tackling the ABSA problem, since the concerned texts are often written by ordinary users with all kinds of abbreviations or slang. The aspect terms and the opinion expressions may also be language-dependent. However, the languagespecific knowledge of the zero-shot method purely comes from the pre-training process where the lowresource languages might be under-represented (Conneau et al., 2020; Pfeiffer et al., 2020). Utilizing the translated target language data with projected labels is a plausible idea to compensate the language-specific knowledge (Li et al., 2020). But the performance of such translation-based methods largely depends on the quality of the translation and label projection. The task-specific knowledge in the translated data would also be limited if the projected label quality is unsatisfactory. ACS method to the multilingual setting, assuming multiple translation engines are available. In this case, the target languages can benefit from the task-specific knowledge"
2021.emnlp-main.727,N19-1423,0,0.00855939,"od [seafood]POS . Labeled French Sentence with [Mermaid Inn]POS est un bon restaurant dans l’ ensemble avec de très translate-then-align Method bons [fruits]POS de mer . Labeled French Sentence with [Mermaid Inn]POS est un bon restaurant dans l’ ensemble avec de très alignment-free Method bons [fruits de mer]POS . Table 6: Example of different label projection methods with French as the target language. We use the bracket to highlight the aspect term, the corresponding sentiment polarities are shown as the subscript for each aspect. on large multilingual corpus, such as the multilingual BERT (Devlin et al., 2019) and XLM-Roberta (Conneau et al., 2020), have shown significant improvements for various cross-lingual NLP tasks. Thanks to the language knowledge learned in the pre-training process, fine-tuning the model on the labeled source language data and directly conducting the inference on the target data can achieve impressive cross-lingual adaptation performance (Wu and Dredze, 2019; Pires et al., 2019; K et al., 2020). Some studies further utilize the translation system together with the pre-trained models, for example, by direct data transfer (Fei et al., 2020; Hu et al., 2020), data augmentation"
2021.emnlp-main.727,N13-1073,0,0.197432,"ur moi. The [nourriture] is very fresh and delicious, but this {endroit} is too small for me. []: nourriture, positive {}: endroit, negative Figure 1: Example of the alignment-free label projection method (upper part) and the aspect code-switching strategy (lower part). Here we use English and French as the source and target language respectively. 2.2 Alignment-free Label Projection To obtain the language-specific knowledge for the target languages, previous works usually first translate the source sentence with an off-the-shelf translation system, then word alignment tools such as fastAlign (Dyer et al., 2013) are used to map the token-level label from the source sentence to the translated sentence (Mayhew et al., 2017; Fei et al., 2020). Some heuristics are proposed for alleviating the alignment error, for example, by conducting a phrase-to-phrase mapping to refine the aspect boundary (Klinger and Cimiano, 2015; Li et al., 2020). However, the word or phrase alignment itself is a challenging task (Akbik and Vollgraf, 2018). The sentences of the ABSA task are usually user-generated (e.g., product reviews and tweets) and informal, which further hinders the translatethen-align method to produce satisf"
2021.emnlp-main.727,2020.acl-main.627,0,0.0489553,"Missing"
2021.emnlp-main.727,P19-1048,1,0.844521,"-align paradigm, our method does not rely on any word alignment tool for projecting the labels from the source to the translated target sentence, which avoids the mis-alignment issue brought in by this step. The high-quality la2 Methodology beled target data thus preserves more task-specific knowledge, helping establish a strong baseline by 2.1 Problem Formulation purely training on such pseudo-labeled data. PreviWe formulate the ABSA task as a sequence labeling ous findings suggest that training on the bilingual corpus (i.e., labeled source data and translated tar- problem (Li et al., 2019b; He et al., 2019). Given L get data) often leads to better performance in cross- a sentence x = {xi }i=1 with L tokens, the model predicts a label sequence y = {yi }L lingual transfer tasks (Hu et al., 2020). Inspired i=1 where by this finding and to further enhance the interac- yi ∈ Y = {B, I, E, S}-{POS, NEU, NEG} ∪ {O} denotes the aspect boundary and its sentiment polartions between the two languages, we propose an ity for the corresponding token xi . For example, aspect code-switching (ACS) mechanism, which yi = B-POS means xi is the beginning of a positive switches the aspect terms between the source and"
2021.emnlp-main.727,P19-1051,0,0.0540098,"Missing"
2021.emnlp-main.727,N19-1257,0,0.025294,"methods make use of the task of extracting mentioned aspects from a given cross-lingual word embeddings (Ruder et al., 2019) sentence and predicting their corresponding senti- trained on large parallel corpus to allow the model ment polarities1 (Liu, 2012; Pontiki et al., 2014). to be used in a language-independent manner, by Consider the following example, “The food is great, simply switching the word embedding layer while but the service is kinda disappointing”, we can de- keeping the model unchanged (Barnes et al., 2016; tect two mentioned aspect terms “food” and “ser- Akhtar et al., 2018; Jebbara and Cimiano, 2019) vice”, and judge their corresponding sentiments when adopted for different languages. as positive and negative, respectively. Given its Recently, employing multilingual pre-trained ∗ models such as the multilingual BERT (Devlin Work done when Wenxuan Zhang was an intern at Alibaba. This work was supported by Alibaba Group through et al., 2019) and XLM-Roberta (Conneau et al., Alibaba Research Intern Program, and a grant from the Re2020) has become the de-facto approach to tackle search Grant Council of the Hong Kong Special Administrathe cross-lingual transfer for many NLP tasks (Hu tive Regi"
2021.emnlp-main.727,2020.emnlp-main.369,0,0.0230427,"020; Liang et al., 2020; Mao et al., While most existing studies focus on English 2021; Zhang et al., 2021). texts, handling ABSA in resource-poor lanThe majority of existing ABSA studies are conguages remains a challenging problem. In this paper, we consider the unsupervised crossducted on English texts. However, in real-world lingual transfer for the ABSA task, where only scenarios such as the E-commerce website, users’ labeled data in the source language is available opinions are usually expressed in different lanand we aim at transferring its knowledge to the guages (Pontiki et al., 2016; Keung et al., 2020). target language having no labeled data. To this Manually annotating a large quantity of ABSA data end, we propose an alignment-free label profor every language can be extremely costly. In this jection method to obtain high-quality pseudowork, we investigate the unsupervised cross-lingual labeled data of the target language with the transfer for the ABSA task, where we only have help of the translation system, which could preserve more accurate task-specific knowledge labeled data in the source language and aim to in the target language. For better utilizing the transfer the knowledge to targ"
2021.emnlp-main.727,K15-1016,0,0.0273519,"as the source and target language respectively. 2.2 Alignment-free Label Projection To obtain the language-specific knowledge for the target languages, previous works usually first translate the source sentence with an off-the-shelf translation system, then word alignment tools such as fastAlign (Dyer et al., 2013) are used to map the token-level label from the source sentence to the translated sentence (Mayhew et al., 2017; Fei et al., 2020). Some heuristics are proposed for alleviating the alignment error, for example, by conducting a phrase-to-phrase mapping to refine the aspect boundary (Klinger and Cimiano, 2015; Li et al., 2020). However, the word or phrase alignment itself is a challenging task (Akbik and Vollgraf, 2018). The sentences of the ABSA task are usually user-generated (e.g., product reviews and tweets) and informal, which further hinders the translatethen-align method to produce satisfactory pseudolabeled target data (Lohar et al., 2019). The inaccurate pseudo labels inevitably limit the task-specific knowledge and lead to poor model performance. We propose an alignment-free label projection method for obtaining the pseudo-labeled data in the target language3 . As depicted in the upper-l"
2021.emnlp-main.727,D17-1269,0,0.0627898,"Missing"
2021.emnlp-main.727,2020.emnlp-main.617,0,0.0157667,"ining stage (Wu and Dredze, 2019; K et al., 2020). There are some challenges for adopting such a paradigm to solve the cross-lingual ABSA task. The language-specific knowledge plays an essential role in tackling the ABSA problem, since the concerned texts are often written by ordinary users with all kinds of abbreviations or slang. The aspect terms and the opinion expressions may also be language-dependent. However, the languagespecific knowledge of the zero-shot method purely comes from the pre-training process where the lowresource languages might be under-represented (Conneau et al., 2020; Pfeiffer et al., 2020). Utilizing the translated target language data with projected labels is a plausible idea to compensate the language-specific knowledge (Li et al., 2020). But the performance of such translation-based methods largely depends on the quality of the translation and label projection. The task-specific knowledge in the translated data would also be limited if the projected label quality is unsatisfactory. ACS method to the multilingual setting, assuming multiple translation engines are available. In this case, the target languages can benefit from the task-specific knowledge contained in different"
2021.emnlp-main.727,P19-1493,0,0.0329588,"Missing"
2021.emnlp-main.727,D19-5505,1,0.930201,"ous translate-then-align paradigm, our method does not rely on any word alignment tool for projecting the labels from the source to the translated target sentence, which avoids the mis-alignment issue brought in by this step. The high-quality la2 Methodology beled target data thus preserves more task-specific knowledge, helping establish a strong baseline by 2.1 Problem Formulation purely training on such pseudo-labeled data. PreviWe formulate the ABSA task as a sequence labeling ous findings suggest that training on the bilingual corpus (i.e., labeled source data and translated tar- problem (Li et al., 2019b; He et al., 2019). Given L get data) often leads to better performance in cross- a sentence x = {xi }i=1 with L tokens, the model predicts a label sequence y = {yi }L lingual transfer tasks (Hu et al., 2020). Inspired i=1 where by this finding and to further enhance the interac- yi ∈ Y = {B, I, E, S}-{POS, NEU, NEG} ∪ {O} denotes the aspect boundary and its sentiment polartions between the two languages, we propose an ity for the corresponding token xi . For example, aspect code-switching (ACS) mechanism, which yi = B-POS means xi is the beginning of a positive switches the aspect terms betw"
2021.emnlp-main.727,S14-2004,0,0.124041,"Missing"
2021.emnlp-main.727,P15-2128,0,0.0645053,"Missing"
2021.emnlp-main.727,D19-1077,0,0.0557,"Kong Special Administrathe cross-lingual transfer for many NLP tasks (Hu tive Region, China (Project Codes: 14204418). 1 Also called End-to-End ABSA or Unified ABSA et al., 2020). Typically, the model is first fine-tuned 9220 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9220–9230 c November 7–11, 2021. 2021 Association for Computational Linguistics on labeled source language data and then can be directly used for inference on the target language data (i.e., zero-shot approach), thanks to the language knowledge learned in the pre-training stage (Wu and Dredze, 2019; K et al., 2020). There are some challenges for adopting such a paradigm to solve the cross-lingual ABSA task. The language-specific knowledge plays an essential role in tackling the ABSA problem, since the concerned texts are often written by ordinary users with all kinds of abbreviations or slang. The aspect terms and the opinion expressions may also be language-dependent. However, the languagespecific knowledge of the zero-shot method purely comes from the pre-training process where the lowresource languages might be under-represented (Conneau et al., 2020; Pfeiffer et al., 2020). Utilizin"
2021.emnlp-main.727,2021.acl-short.64,1,0.757218,"ith Aspect Term Code-Switching ∗ Wenxuan Zhang1 , Ruidan He2 , Haiyun Peng2 , Lidong Bing2 and Wai Lam1 1 The Chinese University of Hong Kong 2 DAMO Academy, Alibaba Group {wxzhang,wlam}@se.cuhk.edu.hk {ruidan.he,haiyun.p,l.bing}@alibaba-inc.com Abstract wide application scenarios, it has attracted lots of attention in the NLP community in recent years Many efforts have been made in solving the (Li et al., 2019a; He et al., 2019; Hu et al., 2019; Aspect-based sentiment analysis (ABSA) task. Chen and Qian, 2020; Liang et al., 2020; Mao et al., While most existing studies focus on English 2021; Zhang et al., 2021). texts, handling ABSA in resource-poor lanThe majority of existing ABSA studies are conguages remains a challenging problem. In this paper, we consider the unsupervised crossducted on English texts. However, in real-world lingual transfer for the ABSA task, where only scenarios such as the E-commerce website, users’ labeled data in the source language is available opinions are usually expressed in different lanand we aim at transferring its knowledge to the guages (Pontiki et al., 2016; Keung et al., 2020). target language having no labeled data. To this Manually annotating a large quantity o"
2021.findings-acl.90,W13-2322,0,0.0935512,"on for Computational Linguistics 2020; Saxena et al., 2020; Xu et al., 2020). But it is known to suffer from sparsity, where complex question clues are unlikely to be covered by the closed-form relations in KG (Zhao et al., 2020; Zhang et al., 2020b). Another issue is that KG requires large human labor and is easy to become outdated if not maintained timely. To take advantages of both rich textual corpora and explicit graph structure and make it compatible to all textual knowledge, we explore the usefulness of Abstract Meaning Representation (AMR) as a graph annotation to a textual fact. AMR (Banarescu et al., 2013) is a semantic formalism that represents the meaning of a sentence into a rooted, directed graph. Figure 1 shows some examples of AMR graphs, where nodes represent concepts and edges represent the relations. Unlike other semantic role labeling that only considers the relations between predicates and their arguments (Song et al., 2019), the aim of AMR is to capture every meaningful content in high-level abstraction while removing away inflections and function words in a sentence. As a result, AMR allows us to explore textual facts and simultaneously attributes them with explicit graph structure"
2021.findings-acl.90,P19-1615,0,0.0619266,"ous approaches that use additional KGs. It obtains 81.6 accuracy on OpenBookQA (Mihaylov et al., 2018), and pushes the state-of-the-art result on ARC-Challenge (Clark et al., 2018) to 68.94 in a computationally practicable setting. 2 Related Work Multi-hop QA with External Resource. Despite the success of pretrained model in most Natural Language Processing (NLP) tasks, it performs poorly in multi-hop QA, where some information is missing to answer questions (Zhu et al., 2021b). Textual corpora contain rich and diverse knowledge, which is likely to cover the clues to answer complex questions. Banerjee et al. (2019) demonstrate some carefully designed queries can effectively retrieve relevant facts. Yadav et al. (2019); Deng et al. (2020) extract groups of evidence facts considering the relevance, overlap and coverage, but such method requires exponential computation in the retrieval step. Feldman and El-Yaniv (2019); Yadav et al. (2020) construct a fact chain by iteratively reformulating the query to focus on the missing information. However, the fact chain often grows obliquely as a result of the failure of first fact retrieval, making the QA model brittle. As some recent QA datasets (Yang et al., 2018"
2021.findings-acl.90,P19-1470,0,0.015999,"osed model. The black dash lines in AMR-SG indicate that we cut the connection between question nodes and choice nodes. The pink arrows indicate two paths that can be spotted in AMR-SG. Facts with red background are active facts detected. The dashed node Active Fact-level Connection Graph indicates fact4 is not considered as a valid node as it is not an active fact. missing information. Other works avoid the sparsity of KGs by constructing KGs directly from textual knowledge. OpenIE (Saha and Mausam, 2018) is widely used in knowledge base question answering to extract entity-relation triples (Bosselut et al., 2019; Zhao et al., 2020; Deng et al., 2019). However, OpenIE favors precision over recall, which is not necessarily effective to form connections among diverse evidence facts for multi-hop QA. Wikipedia contains internal hyperlinks, which are effective to build graph connections from unstructured articles (Asai et al., 2020; Liu et al., 2020). However, such hyperlinks are not available in most textual corpora. vided with J answer choices Cij , j ∈ {1, 2, ..., J}. As shown in Figure 2, our framework consists of three components: (1) a Fact Retrieval component to retrieve evidence facts Fˆ = {Fˆ 1 ,"
2021.findings-acl.90,2020.acl-main.119,1,0.838853,"h to extract active facts and construct an Active Fact-level Connection Graph to capture their relations with the question and the answer choice. 3.2.1 AMR-SG Construction As the nodes of AMR are high-level abstraction of concepts conveyed in the corresponding textual fact, two AMRs sharing the same node indicate that they concern about the same concept, which shows their correlation. This motivates us to construct AMR-SG, shown in Figure 2, to represent the relations of the corresponding hypothesis and evidence facts for each question-choice pair. We leverage the state-of-the-art AMR parser (Cai and Lam, 2020) to generate AMR G = {GH , G1 , ..., Gm } for a hypothesis and all facts in the corresponding fact pool, where GH , Gi are the AMR of the hypothesis and the ith fact respectively. AMR is also a directed and edge-labeled graph, which implies information specified in the edge is propagated in one predefined direction. However, such inner-AMR (edge labels and directions) information does not contribute to inter-AMR relations. Therefore, we only care about if there exists an edge between two nodes but ignore the edge labels and directions. During construction, we regard GH as the start point of AM"
2021.findings-acl.90,2020.emnlp-main.547,1,0.739169,"t on ARC-Challenge (Clark et al., 2018) to 68.94 in a computationally practicable setting. 2 Related Work Multi-hop QA with External Resource. Despite the success of pretrained model in most Natural Language Processing (NLP) tasks, it performs poorly in multi-hop QA, where some information is missing to answer questions (Zhu et al., 2021b). Textual corpora contain rich and diverse knowledge, which is likely to cover the clues to answer complex questions. Banerjee et al. (2019) demonstrate some carefully designed queries can effectively retrieve relevant facts. Yadav et al. (2019); Deng et al. (2020) extract groups of evidence facts considering the relevance, overlap and coverage, but such method requires exponential computation in the retrieval step. Feldman and El-Yaniv (2019); Yadav et al. (2020) construct a fact chain by iteratively reformulating the query to focus on the missing information. However, the fact chain often grows obliquely as a result of the failure of first fact retrieval, making the QA model brittle. As some recent QA datasets (Yang et al., 2018; Mihaylov et al., 2018; Khot et al., 2020) annotate a gold evidence fact for each question, it enables training supervised c"
2021.findings-acl.90,N19-1423,0,0.0114744,"external knowledge resources and the aggregation of retrieved facts to answer complex natural language questions (Yang et al., 2018). ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). Currently, external knowledge is mostly stored in two forms – textual and graph structure (e.g. Knowledge Graph (KG)). Textual corpora contain rich and diverse evidence facts, which are ideal knowledge resources for multi-hop QA. Especially with the success of pretrained models (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019), we can get powerful representations for such textual facts. However, retrieving relevant and useful facts to fill the knowledge gap for inferring the answer is still a challenging problem. In addition, the reasoning process over the facts is hidden by the unexplainable neural network, which hinders the deployment of real-life applications. On the other hand, KG is able to provide structural clues about relevant entities for explainable predictions (Feng et al., 1044 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1044–105"
2021.findings-acl.90,P19-1222,0,0.0171889,"s of pretrained model in most Natural Language Processing (NLP) tasks, it performs poorly in multi-hop QA, where some information is missing to answer questions (Zhu et al., 2021b). Textual corpora contain rich and diverse knowledge, which is likely to cover the clues to answer complex questions. Banerjee et al. (2019) demonstrate some carefully designed queries can effectively retrieve relevant facts. Yadav et al. (2019); Deng et al. (2020) extract groups of evidence facts considering the relevance, overlap and coverage, but such method requires exponential computation in the retrieval step. Feldman and El-Yaniv (2019); Yadav et al. (2020) construct a fact chain by iteratively reformulating the query to focus on the missing information. However, the fact chain often grows obliquely as a result of the failure of first fact retrieval, making the QA model brittle. As some recent QA datasets (Yang et al., 2018; Mihaylov et al., 2018; Khot et al., 2020) annotate a gold evidence fact for each question, it enables training supervised classifier to identify the correct fact driven by a query (Nie et al., 2019; Qiu et al., 2019; Tu et al., 2020; Banerjee and Baral, 2020). Min et al. (2018) take a further step to joi"
2021.findings-acl.90,2020.emnlp-main.99,0,0.403528,"to get informative facts. The introduced AMR serves as a bridge that enables an explicit reasoning process over a graph structure among questions, answers and relevant facts. As exemplified in Figure 1, a hypothesis is first derived from a question and an answer choice. We then parse the hypothesis and a large number of facts to corresponding AMRs. After that, we dynamically construct AMR-SG for each questionchoice pair by merging the AMRs of its hypothesis and relevant facts. Unlike previous works on multi-hop QA that rely on existing KGs to find relations among entities (Wang et al., 2020; Feng et al., 2020), our proposed AMR-SG is dynamically constructed, which reveals intrinsic relations of facts and can naturally form any-hop connections. After construction, we analyze all connected paths starting from the question to the answer on AMR-SG. We focus the consideration of facts on those paths because they together connect the question with the answer, indicating their active roles in filling the knowledge gap. The connections of facts on AMR-SG can be further used as the supervision for downstream reasoning. Therefore, we adopt GCN (Kipf and Welling, 2017) to model the factlevel information passi"
2021.findings-acl.90,2020.findings-emnlp.171,0,0.0514208,"n et al., 2019; Radford et al., 2019), where the latter is more fair to compare with us. 5 5.1 Results Main Results OpenBookQA. The test set accuracy is shown in Table 2. AMR-SG-Full is our full model based on AristoRoBERTa. Results show that AMR-SG-Full can surpass models leveraging additional KG. It demonstrates that the fundamental improvement of AMR-SG-Full comes from the knowledge mining of the textual corpus. However, such knowledge resource has not been fully investigated by existing methods and contains richer and more diverse evidence facts than KGs. We do not compare with UnifiedQA (Khashabi et al., 2020) and T5 3B (Raffel et al., 2020) as they rely on extremely large pretrained models (at least 3B parameters), which are not fair for comparison. ARC-Challenge. We also implement AMR-SGFull on another difficult multi-hop QA dataset: ARC-Challenge. It consists of the questions only answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm (Clark et al., 2018), which theoretically is not friendly to our approach. As shown in Table 3, we can still obtain 2.47 accuracy improvement comparing to AristoRoBERTaV7 and achieve a new state-of-theart performance in a comput"
2021.findings-acl.90,D17-1082,0,0.0204768,"imilar to AllenAI (2019) using 20 facts as the context. 1048 facts, which are highly related to the questions in this dataset. Therefore, for OpenBookQA, we retrieve 10 facts from the open-book and another 90 facts from ARC Corpus, forming the 100 facts in the fact pool. We then select up to 15 active facts using our approach as the context. PG PG AlBERT + KB MHGRN KF-SIR 4.2 AristoRoBERTaV7 + AMR-SG-Full Implementation We implement our approach on two pretrained models: RoBERTa (Liu et al., 2019) and AristoRoBERTa (AllenAI, 2019). AristoRoBERTa employs the RoBERTa architecture but uses RACE (Lai et al., 2017) to first fine-tune the RoBERTa model. We prepare active facts as the context to further fine-tune the model with the target dataset. For OpenBookQA, we continue to fine-tune the QA model following the same procedure as AllenAI (2019), where the initial learning rate is 2e-5, the batch size is 12 and the max sequence length is 256. For ARC-Challenge, the initial learning rate, the batch size and the max sequence length are 1e-5, 6, and 416 respectively. We use grid search to find optimal hyper-parameters, where the learning rate is chosen from {5e-6, 1e5, 2e-5}, the batch size is chosen from {"
2021.findings-acl.90,W15-4502,0,0.188892,"ˆ m }1 for each question-choice pair from a large textual corpus; (2) a Semantic Graph Construction & Analytics component that dynamically constructs a semantic graph, named AMR-SG, to select active facts F = {F 1 , ..., F n } from Fˆ and capture their relations A; and (3) a Hypothesis Assessment component that classifies whether the question-choice is correct, given the active facts and their relations in (2). AMR. Recent success in AMR research makes it possible to benefit downstream tasks, such as summarization (Takase et al., 2016; Dohare et al., 2017; Liao et al., 2018), event detection (Li et al., 2015) and machine translation (Song et al., 2019). In the domain of QA, AMR has been used to form logic queries and conduct symbolic reasoning (Mitra and Baral, 2016; Kapanipathi et al., 2020). Comparing to name entity (Zhong et al., 2020) or other crosssentence annotations (Lei et al., 2018; Zhang et al., 2020a), we use AMR to build our semantic graph because it is align-free and can be easily adapted to powerful pretrained models. 3.1 3 Framework Description Fact Retrieval Hypothesis Generation. As shown in Figure 2, we first generate a hypothesis Hij for the ith question and the j th choice. A h"
2021.findings-acl.90,C18-1101,0,0.247237,"ve evidence facts Fˆ = {Fˆ 1 , ..., Fˆ m }1 for each question-choice pair from a large textual corpus; (2) a Semantic Graph Construction & Analytics component that dynamically constructs a semantic graph, named AMR-SG, to select active facts F = {F 1 , ..., F n } from Fˆ and capture their relations A; and (3) a Hypothesis Assessment component that classifies whether the question-choice is correct, given the active facts and their relations in (2). AMR. Recent success in AMR research makes it possible to benefit downstream tasks, such as summarization (Takase et al., 2016; Dohare et al., 2017; Liao et al., 2018), event detection (Li et al., 2015) and machine translation (Song et al., 2019). In the domain of QA, AMR has been used to form logic queries and conduct symbolic reasoning (Mitra and Baral, 2016; Kapanipathi et al., 2020). Comparing to name entity (Zhong et al., 2020) or other crosssentence annotations (Lei et al., 2018; Zhang et al., 2020a), we use AMR to build our semantic graph because it is align-free and can be easily adapted to powerful pretrained models. 3.1 3 Framework Description Fact Retrieval Hypothesis Generation. As shown in Figure 2, we first generate a hypothesis Hij for the it"
2021.findings-acl.90,D19-1282,0,0.0114275,"(Nie et al., 2019; Qiu et al., 2019; Tu et al., 2020; Banerjee and Baral, 2020). Min et al. (2018) take a further step to jointly predict the answer span and select evidence facts in a unified model. Though these supervised retrievers have achieved impressive improvement, they heavily rely on the annotated gold facts, which are not always available in real-world applications. In addition, previous works also explore the effectiveness of structured knowledge by either encoding the nodes (Yang and Mitchell, 2017; Wang et al., 2019), triples (Mihaylov and Frank, 2018; Wang et al., 2020), paths (Lin et al., 2019; Lei et al., 2020) or tabular (Zhu et al., 2021a) to capture the 1045 CLS Textual Corpus Fact Extraction (?????? ) � AMR-SG (??) SEP ?????? Question nodes ??1 … ?? ?? (?????? , ??) Max Pooling fact1 ???? fact2 Choice nodes fact3 fact4 Hypothesis Generation Active Fact-level Connection Graph (???? , ?????? ) (??) ??1 ??2 ??3 GCN … K Layers GCN ?? ???? Question Answer Choice Fact Retrieval RoBERTa hypothesis ?? ???????? Classification Semantic Graph Construction & Analytics Hypothesis Assessment Figure 2: Overall architecture of our proposed model. The black dash lines in AMR-SG indicate that w"
2021.findings-acl.90,2020.acl-main.604,0,0.0123553,"t is not an active fact. missing information. Other works avoid the sparsity of KGs by constructing KGs directly from textual knowledge. OpenIE (Saha and Mausam, 2018) is widely used in knowledge base question answering to extract entity-relation triples (Bosselut et al., 2019; Zhao et al., 2020; Deng et al., 2019). However, OpenIE favors precision over recall, which is not necessarily effective to form connections among diverse evidence facts for multi-hop QA. Wikipedia contains internal hyperlinks, which are effective to build graph connections from unstructured articles (Asai et al., 2020; Liu et al., 2020). However, such hyperlinks are not available in most textual corpora. vided with J answer choices Cij , j ∈ {1, 2, ..., J}. As shown in Figure 2, our framework consists of three components: (1) a Fact Retrieval component to retrieve evidence facts Fˆ = {Fˆ 1 , ..., Fˆ m }1 for each question-choice pair from a large textual corpus; (2) a Semantic Graph Construction & Analytics component that dynamically constructs a semantic graph, named AMR-SG, to select active facts F = {F 1 , ..., F n } from Fˆ and capture their relations A; and (3) a Hypothesis Assessment component that classifies whether t"
2021.findings-acl.90,N15-1114,0,0.0263445,"SG; Error-Free-Adapted is that we use the error-free AMRs annotated to fine-tune the AMR parser and use the tuned parser to generate AMR for all the remaining facts (including hypotheses and common facts, about 900k in total). The test set accuracy are 81.6, 80.2, 80.4 for Fully-Automatic, Mixed and Error-Free-Adapted respectively. It is interesting to note that using Fully-Automatic AMRs results in higher QA accuracy than Mixed and Error-Free-Adapted, where the latter two contain a mix of AMRs with different levels of quality. This phenomenon has also been observed in other AMR applications (Liu et al., 2015; Hardy and Vlachos, 2018), where automatic parses perform well than manual parses. We conjecture that this can be attributed to the discrepancy between the error-free AMRs and the automatically parsed AMRs in the choices of AMR concepts with similar meaning. This small difference in concept choices may omit potential connections, results in some important facts failing to be detected. In conTable 6: A case study showing how our framework selects useful facts to completely fill the knowledge gap. trast, automatically parsed AMRs contain errors, but they are consistent in their concept choices,"
2021.findings-acl.90,2021.ccl-1.108,0,0.0206196,"Missing"
2021.findings-acl.90,D18-1260,0,0.0915639,"ion, we analyze all connected paths starting from the question to the answer on AMR-SG. We focus the consideration of facts on those paths because they together connect the question with the answer, indicating their active roles in filling the knowledge gap. The connections of facts on AMR-SG can be further used as the supervision for downstream reasoning. Therefore, we adopt GCN (Kipf and Welling, 2017) to model the factlevel information passing. Experimental results demonstrate that our approach outperforms previous approaches that use additional KGs. It obtains 81.6 accuracy on OpenBookQA (Mihaylov et al., 2018), and pushes the state-of-the-art result on ARC-Challenge (Clark et al., 2018) to 68.94 in a computationally practicable setting. 2 Related Work Multi-hop QA with External Resource. Despite the success of pretrained model in most Natural Language Processing (NLP) tasks, it performs poorly in multi-hop QA, where some information is missing to answer questions (Zhu et al., 2021b). Textual corpora contain rich and diverse knowledge, which is likely to cover the clues to answer complex questions. Banerjee et al. (2019) demonstrate some carefully designed queries can effectively retrieve relevant f"
2021.findings-acl.90,P18-1076,0,0.0125162,"sifier to identify the correct fact driven by a query (Nie et al., 2019; Qiu et al., 2019; Tu et al., 2020; Banerjee and Baral, 2020). Min et al. (2018) take a further step to jointly predict the answer span and select evidence facts in a unified model. Though these supervised retrievers have achieved impressive improvement, they heavily rely on the annotated gold facts, which are not always available in real-world applications. In addition, previous works also explore the effectiveness of structured knowledge by either encoding the nodes (Yang and Mitchell, 2017; Wang et al., 2019), triples (Mihaylov and Frank, 2018; Wang et al., 2020), paths (Lin et al., 2019; Lei et al., 2020) or tabular (Zhu et al., 2021a) to capture the 1045 CLS Textual Corpus Fact Extraction (?????? ) � AMR-SG (??) SEP ?????? Question nodes ??1 … ?? ?? (?????? , ??) Max Pooling fact1 ???? fact2 Choice nodes fact3 fact4 Hypothesis Generation Active Fact-level Connection Graph (???? , ?????? ) (??) ??1 ??2 ??3 GCN … K Layers GCN ?? ???? Question Answer Choice Fact Retrieval RoBERTa hypothesis ?? ???????? Classification Semantic Graph Construction & Analytics Hypothesis Assessment Figure 2: Overall architecture of our proposed model. T"
2021.findings-acl.90,P18-1160,0,0.012648,"retrieval step. Feldman and El-Yaniv (2019); Yadav et al. (2020) construct a fact chain by iteratively reformulating the query to focus on the missing information. However, the fact chain often grows obliquely as a result of the failure of first fact retrieval, making the QA model brittle. As some recent QA datasets (Yang et al., 2018; Mihaylov et al., 2018; Khot et al., 2020) annotate a gold evidence fact for each question, it enables training supervised classifier to identify the correct fact driven by a query (Nie et al., 2019; Qiu et al., 2019; Tu et al., 2020; Banerjee and Baral, 2020). Min et al. (2018) take a further step to jointly predict the answer span and select evidence facts in a unified model. Though these supervised retrievers have achieved impressive improvement, they heavily rely on the annotated gold facts, which are not always available in real-world applications. In addition, previous works also explore the effectiveness of structured knowledge by either encoding the nodes (Yang and Mitchell, 2017; Wang et al., 2019), triples (Mihaylov and Frank, 2018; Wang et al., 2020), paths (Lin et al., 2019; Lei et al., 2020) or tabular (Zhu et al., 2021a) to capture the 1045 CLS Textual"
2021.findings-acl.90,D19-1258,0,0.129327,"Missing"
2021.findings-acl.90,P19-1617,0,0.0811461,"erage, but such method requires exponential computation in the retrieval step. Feldman and El-Yaniv (2019); Yadav et al. (2020) construct a fact chain by iteratively reformulating the query to focus on the missing information. However, the fact chain often grows obliquely as a result of the failure of first fact retrieval, making the QA model brittle. As some recent QA datasets (Yang et al., 2018; Mihaylov et al., 2018; Khot et al., 2020) annotate a gold evidence fact for each question, it enables training supervised classifier to identify the correct fact driven by a query (Nie et al., 2019; Qiu et al., 2019; Tu et al., 2020; Banerjee and Baral, 2020). Min et al. (2018) take a further step to jointly predict the answer span and select evidence facts in a unified model. Though these supervised retrievers have achieved impressive improvement, they heavily rely on the annotated gold facts, which are not always available in real-world applications. In addition, previous works also explore the effectiveness of structured knowledge by either encoding the nodes (Yang and Mitchell, 2017; Wang et al., 2019), triples (Mihaylov and Frank, 2018; Wang et al., 2020), paths (Lin et al., 2019; Lei et al., 2020)"
2021.findings-acl.90,C18-1194,0,0.0159032,"cation Semantic Graph Construction & Analytics Hypothesis Assessment Figure 2: Overall architecture of our proposed model. The black dash lines in AMR-SG indicate that we cut the connection between question nodes and choice nodes. The pink arrows indicate two paths that can be spotted in AMR-SG. Facts with red background are active facts detected. The dashed node Active Fact-level Connection Graph indicates fact4 is not considered as a valid node as it is not an active fact. missing information. Other works avoid the sparsity of KGs by constructing KGs directly from textual knowledge. OpenIE (Saha and Mausam, 2018) is widely used in knowledge base question answering to extract entity-relation triples (Bosselut et al., 2019; Zhao et al., 2020; Deng et al., 2019). However, OpenIE favors precision over recall, which is not necessarily effective to form connections among diverse evidence facts for multi-hop QA. Wikipedia contains internal hyperlinks, which are effective to build graph connections from unstructured articles (Asai et al., 2020; Liu et al., 2020). However, such hyperlinks are not available in most textual corpora. vided with J answer choices Cij , j ∈ {1, 2, ..., J}. As shown in Figure 2, our"
2021.findings-acl.90,2020.acl-main.412,0,0.139218,"for such textual facts. However, retrieving relevant and useful facts to fill the knowledge gap for inferring the answer is still a challenging problem. In addition, the reasoning process over the facts is hidden by the unexplainable neural network, which hinders the deployment of real-life applications. On the other hand, KG is able to provide structural clues about relevant entities for explainable predictions (Feng et al., 1044 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1044–1056 August 1–6, 2021. ©2021 Association for Computational Linguistics 2020; Saxena et al., 2020; Xu et al., 2020). But it is known to suffer from sparsity, where complex question clues are unlikely to be covered by the closed-form relations in KG (Zhao et al., 2020; Zhang et al., 2020b). Another issue is that KG requires large human labor and is easy to become outdated if not maintained timely. To take advantages of both rich textual corpora and explicit graph structure and make it compatible to all textual knowledge, we explore the usefulness of Abstract Meaning Representation (AMR) as a graph annotation to a textual fact. AMR (Banarescu et al., 2013) is a semantic formalism that repre"
2021.findings-acl.90,Q19-1002,0,0.42866,"maintained timely. To take advantages of both rich textual corpora and explicit graph structure and make it compatible to all textual knowledge, we explore the usefulness of Abstract Meaning Representation (AMR) as a graph annotation to a textual fact. AMR (Banarescu et al., 2013) is a semantic formalism that represents the meaning of a sentence into a rooted, directed graph. Figure 1 shows some examples of AMR graphs, where nodes represent concepts and edges represent the relations. Unlike other semantic role labeling that only considers the relations between predicates and their arguments (Song et al., 2019), the aim of AMR is to capture every meaningful content in high-level abstraction while removing away inflections and function words in a sentence. As a result, AMR allows us to explore textual facts and simultaneously attributes them with explicit graph structure for explainable fact quality assessment and reasoning. In this paper, we propose a novel framework that incorporates AMR to make explainable knowledge retrieval and reasoning for multi-hop QA. Our framework works on textual knowledge, which is easy to obtain and allows us to get informative facts. The introduced AMR serves as a bridg"
2021.findings-acl.90,D16-1112,0,0.0269584,": (1) a Fact Retrieval component to retrieve evidence facts Fˆ = {Fˆ 1 , ..., Fˆ m }1 for each question-choice pair from a large textual corpus; (2) a Semantic Graph Construction & Analytics component that dynamically constructs a semantic graph, named AMR-SG, to select active facts F = {F 1 , ..., F n } from Fˆ and capture their relations A; and (3) a Hypothesis Assessment component that classifies whether the question-choice is correct, given the active facts and their relations in (2). AMR. Recent success in AMR research makes it possible to benefit downstream tasks, such as summarization (Takase et al., 2016; Dohare et al., 2017; Liao et al., 2018), event detection (Li et al., 2015) and machine translation (Song et al., 2019). In the domain of QA, AMR has been used to form logic queries and conduct symbolic reasoning (Mitra and Baral, 2016; Kapanipathi et al., 2020). Comparing to name entity (Zhong et al., 2020) or other crosssentence annotations (Lei et al., 2018; Zhang et al., 2020a), we use AMR to build our semantic graph because it is align-free and can be easily adapted to powerful pretrained models. 3.1 3 Framework Description Fact Retrieval Hypothesis Generation. As shown in Figure 2, we f"
2021.findings-acl.90,2020.findings-emnlp.369,0,0.517321,"btain and allows us to get informative facts. The introduced AMR serves as a bridge that enables an explicit reasoning process over a graph structure among questions, answers and relevant facts. As exemplified in Figure 1, a hypothesis is first derived from a question and an answer choice. We then parse the hypothesis and a large number of facts to corresponding AMRs. After that, we dynamically construct AMR-SG for each questionchoice pair by merging the AMRs of its hypothesis and relevant facts. Unlike previous works on multi-hop QA that rely on existing KGs to find relations among entities (Wang et al., 2020; Feng et al., 2020), our proposed AMR-SG is dynamically constructed, which reveals intrinsic relations of facts and can naturally form any-hop connections. After construction, we analyze all connected paths starting from the question to the answer on AMR-SG. We focus the consideration of facts on those paths because they together connect the question with the answer, indicating their active roles in filling the knowledge gap. The connections of facts on AMR-SG can be further used as the supervision for downstream reasoning. Therefore, we adopt GCN (Kipf and Welling, 2017) to model the factlev"
2021.findings-acl.90,D19-1260,0,0.0966849,"pushes the state-of-the-art result on ARC-Challenge (Clark et al., 2018) to 68.94 in a computationally practicable setting. 2 Related Work Multi-hop QA with External Resource. Despite the success of pretrained model in most Natural Language Processing (NLP) tasks, it performs poorly in multi-hop QA, where some information is missing to answer questions (Zhu et al., 2021b). Textual corpora contain rich and diverse knowledge, which is likely to cover the clues to answer complex questions. Banerjee et al. (2019) demonstrate some carefully designed queries can effectively retrieve relevant facts. Yadav et al. (2019); Deng et al. (2020) extract groups of evidence facts considering the relevance, overlap and coverage, but such method requires exponential computation in the retrieval step. Feldman and El-Yaniv (2019); Yadav et al. (2020) construct a fact chain by iteratively reformulating the query to focus on the missing information. However, the fact chain often grows obliquely as a result of the failure of first fact retrieval, making the QA model brittle. As some recent QA datasets (Yang et al., 2018; Mihaylov et al., 2018; Khot et al., 2020) annotate a gold evidence fact for each question, it enables t"
2021.findings-acl.90,2020.acl-main.414,0,0.185241,"Natural Language Processing (NLP) tasks, it performs poorly in multi-hop QA, where some information is missing to answer questions (Zhu et al., 2021b). Textual corpora contain rich and diverse knowledge, which is likely to cover the clues to answer complex questions. Banerjee et al. (2019) demonstrate some carefully designed queries can effectively retrieve relevant facts. Yadav et al. (2019); Deng et al. (2020) extract groups of evidence facts considering the relevance, overlap and coverage, but such method requires exponential computation in the retrieval step. Feldman and El-Yaniv (2019); Yadav et al. (2020) construct a fact chain by iteratively reformulating the query to focus on the missing information. However, the fact chain often grows obliquely as a result of the failure of first fact retrieval, making the QA model brittle. As some recent QA datasets (Yang et al., 2018; Mihaylov et al., 2018; Khot et al., 2020) annotate a gold evidence fact for each question, it enables training supervised classifier to identify the correct fact driven by a query (Nie et al., 2019; Qiu et al., 2019; Tu et al., 2020; Banerjee and Baral, 2020). Min et al. (2018) take a further step to jointly predict the answ"
2021.findings-acl.90,P17-1132,0,0.0141766,"for each question, it enables training supervised classifier to identify the correct fact driven by a query (Nie et al., 2019; Qiu et al., 2019; Tu et al., 2020; Banerjee and Baral, 2020). Min et al. (2018) take a further step to jointly predict the answer span and select evidence facts in a unified model. Though these supervised retrievers have achieved impressive improvement, they heavily rely on the annotated gold facts, which are not always available in real-world applications. In addition, previous works also explore the effectiveness of structured knowledge by either encoding the nodes (Yang and Mitchell, 2017; Wang et al., 2019), triples (Mihaylov and Frank, 2018; Wang et al., 2020), paths (Lin et al., 2019; Lei et al., 2020) or tabular (Zhu et al., 2021a) to capture the 1045 CLS Textual Corpus Fact Extraction (?????? ) � AMR-SG (??) SEP ?????? Question nodes ??1 … ?? ?? (?????? , ??) Max Pooling fact1 ???? fact2 Choice nodes fact3 fact4 Hypothesis Generation Active Fact-level Connection Graph (???? , ?????? ) (??) ??1 ??2 ??3 GCN … K Layers GCN ?? ???? Question Answer Choice Fact Retrieval RoBERTa hypothesis ?? ???????? Classification Semantic Graph Construction & Analytics Hypothesis Assessment"
2021.findings-acl.90,2020.acl-main.549,0,0.0380209,"Fˆ and capture their relations A; and (3) a Hypothesis Assessment component that classifies whether the question-choice is correct, given the active facts and their relations in (2). AMR. Recent success in AMR research makes it possible to benefit downstream tasks, such as summarization (Takase et al., 2016; Dohare et al., 2017; Liao et al., 2018), event detection (Li et al., 2015) and machine translation (Song et al., 2019). In the domain of QA, AMR has been used to form logic queries and conduct symbolic reasoning (Mitra and Baral, 2016; Kapanipathi et al., 2020). Comparing to name entity (Zhong et al., 2020) or other crosssentence annotations (Lei et al., 2018; Zhang et al., 2020a), we use AMR to build our semantic graph because it is align-free and can be easily adapted to powerful pretrained models. 3.1 3 Framework Description Fact Retrieval Hypothesis Generation. As shown in Figure 2, we first generate a hypothesis Hij for the ith question and the j th choice. A hypothesis is a completed statement derived from each question-choice pair. Comparing to simply concatenating the question and the choice, a hypothesis contains less meaningless words and maintain a good grammatical structure, which ca"
2021.findings-emnlp.237,L18-1157,0,0.0344834,"Missing"
2021.findings-emnlp.237,Q19-1038,0,0.0237917,", and ZH), we construct silver training data following Blloshmi et al. (2020). Specifically, we use OPUS-MT (Tiedemann and Thottingal, 2020)4 , 4 https://huggingface.co/transformers/ model_doc/marian.html 2782 an off-the-shelf translation tool, to translate English sentences in AMR2.0 to other foreign languages. To ensure the quality of silver data, we filter out data with less accurate translations via back-translation consistency check. That is, the translation quality is measured by the cosine similarity between the original English sentence and its back-translated counterpart using LASER (Artetxe and Schwenk, 2019). We refer readers to Blloshmi et al. (2020) for an exhaustive description of the data filtering process. Detailed statistics of our training, dev, and test sets are shown in Table 1. Language English(EN) German(DE) Spanish(ES) Italian(IT) Chinese(ZH) Test 1,371∗ 1,371∗ 1,371∗ 1,371∗ 1,371∗ • w/o KD-FT. To show the benefits from KD, we conduct an ablation experiment where the KD-FT stage (F3) is skipped. The training process becomes P1→P2→F4. • w/o Gold-FT. To validate the necessity of the fine-tuning with gold AMR graph, we also report the model results without the final GoldFT (F4) stage. Th"
2021.findings-emnlp.237,W13-2322,0,0.0493841,"arse all different languages including English. We identify that noisy input and precise output are the key to successful distillation. Together with extensive pre-training, we obtain an AMR parser whose performances surpass all previously published results on four different foreign languages, including German, Spanish, Italian, and Chinese, by large margins (up to 18.8 S MATCH points on Chinese and on average 11.3 S MATCH points). Our parser also achieves comparable performance on English to the latest state-of-the-art Englishonly parser. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a broad-coverage semantic formalism that encodes the meaning of a sentence as a rooted, directed, and labeled graph, where nodes represent concepts and edges represent relations among concepts. AMR parsing is the task of translating natural language sentences into their corresponding AMR graphs, which encompasses a set of natural language understanding tasks, such as named entity recognition, semantic role labeling, and coreference resolution. AMR has proved to be beneficial to a wide range of applications such as text summarization (Liao et al., 2018), machine translation (Song et al., 20"
2021.findings-emnlp.237,S16-1176,0,0.0117734,"imple English AMR parser. Blloshmi et al. (2020) find random noise. that translating the source side of existing English Effect of Data Sizes for Knowledge Distillation AMR dataset into other target languages produces Lastly, we study the relation between model per- better silver training data. Sheth et al. (2021) foformance and the size of monolingual data used cus on improving cross-lingual word-to-node alignfor KD. Figure 3 shows that the S MATCH scores ment for training cross-lingual AMR parsers that 2785 rely on explicit alignment. Our work follows the alignment-free seq2seq formulation (Barzdins and Gosko, 2016; Konstas et al., 2017; Van Noord and Bos, 2017; Peng et al., 2017; Zhang et al., 2019a; Ge et al., 2019; Bevilacqua et al., 2021) and we alternatively study this problem from the perspective of knowledge distillation, which provides a new way to enable multilingual AMR parsing. Knowledge Distillation for Sequence Generation Knowledge distillation (KD) is a classic technique originally proposed for model compression (Buciluˇa et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015). KD suggests training a (smaller) student model to mimic a (larger) teacher model, by minimizing the loss (typica"
2021.findings-emnlp.237,2020.emnlp-main.195,0,0.0582609,"d Lam, 2019, 2020) rely on the textual overlap between English words and AMR node values (i.e., concepts). include the original English test set in our evaluation. On four zero-resource languages, our single universal parser consistently outperforms the previous best results by large margins (+11.3 S MATCH points on average and up to +18.8 S MATCH points). Meanwhile, our parser achieves competitive results on English even compared with the latest state-ofthe-art English AMR parser in the literature. To sum up, our contributions are listed below: Some initial attempts (Damonte and Cohen, 2018; Blloshmi et al., 2020; Sheth et al., 2021) towards multilingual AMR parsing mainly investigated the construction of pseudo parallel data via annotation projection. In this paper, we study multilingual AMR parsing from the perspective of knowledge distillation (Buciluˇa et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015; Kim and Rush, 2016), where our primary goal is to improve a multilingual AMR parser by using an existing English parser as its teacher. We focus on a strict multilingual setting for developing one AMR parser that can parse all different languages. In contrast to the language-specific (one pars"
2021.findings-emnlp.237,D19-1393,1,0.891737,"Missing"
2021.findings-emnlp.237,2020.acl-main.119,1,0.854087,"Missing"
2021.findings-emnlp.237,P13-2131,0,0.066217,"Missing"
2021.findings-emnlp.237,W14-5808,0,0.0350731,"Missing"
2021.findings-emnlp.237,2020.acl-main.747,0,0.0326833,"gued that the method is not informative in terms of the cross-lingual properties of AMR (Damonte and Cohen, 2018; Blloshmi et al., 2020). To tackle cross-lingual AMR parsing, most previous work relies on pre-trained multilingual language models and silver training data (i.e., pseudo parallel data). Pre-trained Multilingual Language Model Previous work proves that language-independent features provided by pre-trained multilingual language models can boost cross-lingual parsing performance. For example, Blloshmi et al. (2020) use mBERT (Devlin et al., 2019) and Sheth et al. (2021) employ XLM-R (Conneau et al., 2020). We present experiments on the benchmark dataset created by Damonte and Cohen (2018), covering four different languages with no training data, Silver Training Data There are two typical including German, Spanish, Italian, and Chinese. methods for creating silver training examples: (I) To cover as many languages as possible, we also Parsing English to AMR (Damonte and Cohen, 2779 2018). This approach creates silver training examples for the foreign language X through an external X-EN parallel corpus and an existing English AMR parser. The English sentences of the parallel corpus are parsed usi"
2021.findings-emnlp.237,P84-1044,0,0.399229,"Missing"
2021.findings-emnlp.237,D18-1232,0,0.0469771,"Missing"
2021.findings-emnlp.237,N18-1104,0,0.112821,"glaubt. El chico quiere que la chica le crea. Il ragazzo vuole che la ragazza gli creda. Figure 1: An example of AMR. Sentences written in English and other languages share the same meaning and therefore correspond to the same AMR graph. English sentences with the same meaning correspond to the same AMR graph. Furthermore, there are no explicit alignments between elements (nodes or edges) in the graph and words in the text. While this property leads to a distinct difficulty in AMR parsing, it also suggests the potential of AMR to work as an interlingua (Xue et al., 2014; Hajiˇc et al., 2014; Damonte and Cohen, 2018), which could be useful to multilingual applications of natural language understanding (Liang et al., 2020; Hu et al., 2020). An example is given in Figure 1, we represent the semantics of semantically-equivalent sentences in other languages using the same AMR graph. This defines the multilingual AMR parsing problem we seek to address in this paper. Multilingual AMR parsing is an extremely challenging task due to several reasons. First, AMR was initially designed for and heavily biased towards English, thus the parsing has to overcome ∗ some structural linguistic divergences among lanThis work"
2021.findings-emnlp.237,N19-1423,0,0.164842,"nte and Cohen, 2018; Uhrig et al., 2021). However, it is argued that the method is not informative in terms of the cross-lingual properties of AMR (Damonte and Cohen, 2018; Blloshmi et al., 2020). To tackle cross-lingual AMR parsing, most previous work relies on pre-trained multilingual language models and silver training data (i.e., pseudo parallel data). Pre-trained Multilingual Language Model Previous work proves that language-independent features provided by pre-trained multilingual language models can boost cross-lingual parsing performance. For example, Blloshmi et al. (2020) use mBERT (Devlin et al., 2019) and Sheth et al. (2021) employ XLM-R (Conneau et al., 2020). We present experiments on the benchmark dataset created by Damonte and Cohen (2018), covering four different languages with no training data, Silver Training Data There are two typical including German, Spanish, Italian, and Chinese. methods for creating silver training examples: (I) To cover as many languages as possible, we also Parsing English to AMR (Damonte and Cohen, 2779 2018). This approach creates silver training examples for the foreign language X through an external X-EN parallel corpus and an existing English AMR parser."
2021.findings-emnlp.237,P14-1134,0,0.0242669,"ative Research (AIR) Program. guages (Damonte and Cohen, 2018; Zhu et al., 2778 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2778–2789 November 7–11, 2021. ©2021 Association for Computational Linguistics 2019). Second, the human-annotated resources for training are only available in English and none is present in other languages. Moreover, since the AMR graph involves rich semantic labels, the AMR annotation for other languages can be laborintensive and unaffordable. Third, current modeling techniques focus mostly on English. For example, existing AMR aligners (Flanigan et al., 2014; Pourdamghani et al., 2014; Liu et al., 2018) and widely-used pointer-generator mechanisms (Zhang et al., 2019b; Cai and Lam, 2019, 2020) rely on the textual overlap between English words and AMR node values (i.e., concepts). include the original English test set in our evaluation. On four zero-resource languages, our single universal parser consistently outperforms the previous best results by large margins (+11.3 S MATCH points on average and up to +18.8 S MATCH points). Meanwhile, our parser achieves competitive results on English even compared with the latest state-ofthe-art English AMR p"
2021.findings-emnlp.237,P16-5005,0,0.0243887,"Missing"
2021.findings-emnlp.237,D16-1139,0,0.343809,"nd up to +18.8 S MATCH points). Meanwhile, our parser achieves competitive results on English even compared with the latest state-ofthe-art English AMR parser in the literature. To sum up, our contributions are listed below: Some initial attempts (Damonte and Cohen, 2018; Blloshmi et al., 2020; Sheth et al., 2021) towards multilingual AMR parsing mainly investigated the construction of pseudo parallel data via annotation projection. In this paper, we study multilingual AMR parsing from the perspective of knowledge distillation (Buciluˇa et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015; Kim and Rush, 2016), where our primary goal is to improve a multilingual AMR parser by using an existing English parser as its teacher. We focus on a strict multilingual setting for developing one AMR parser that can parse all different languages. In contrast to the language-specific (one parser one language) setting, our setting is more challenging yet more appealing in practice. Intuitively, knowledge distillation is effective because the teacher’s output provides a rich training signal for the student parser. We develop both the teacher parser and the student parser with language-agnostic seq2seq design and e"
2021.findings-emnlp.237,2005.mtsummit-papers.11,0,0.0630837,"rser is trained. • Multilingual. One single parser is trained to parse all target languages. While this paper focuses on the multilingual setting, we also report the results of the languagespecific parsers in previous work (Damonte and Cohen, 2018; Blloshmi et al., 2020; Sheth et al., 2021) for comparative reference. 4.3 Dev 1,368∗ 1,319 1,325 1,322 1,311 Table 1: The number of instances per language and for each data split. ∗ marks gold quality and otherwise silver quality. Knowledge Distillation Data For the knowledge distillation stage, we use 320K English sentences in the Europarl corpus (Koehn, 2005), which contains parallel sentence pairs of En⇔DE, En⇔ES, and En⇔IT. Unless otherwise specified, we use sequence-level KD with noisy input from OPUS-MT. Note that essentially our noisy KD only requires monolingual English data. Nevertheless, we choose Europarl following Damonte and Cohen (2018); Blloshmi et al. (2020) and use the gold translations as noise-free input to demonstrate the impact of our noisy KD comparatively (§5.2). 4.2 Train 36,521∗ 34,415 34,552 34,521 33,221 Models Model Variants Our full training pipeline consists of multiple pre-training and fine-tuning stages. To study the"
2021.findings-emnlp.237,P17-1014,0,0.117256,"o vuole che la ragazza gli creda. The boy’s desire is for the girl to believe him. El chico quiere que la chica le crea. parse ( &lt;V0> want-01 :ARG0 ( &lt;V1> boy ) :ARG1 ( &lt;V2> believe-01 :ARG0 ( &lt;V3> girl ) :ARG1 &lt;V1> ) ) match Linearized AMR ( &lt;V0> want-01 :ARG0 ( &lt;V1> boy ) :ARG1 ( &lt;V2> believe-01 :ARG0 ( &lt;V3> girl ) :ARG1 &lt;V1> ) ) parallel Figure 2: Illustration of different training stages. Stage P1 is omitted for space limit. is linearized through a depth-first traversal starting from the root. For edge ordering, we use the default order in the release files of AMR datasets as suggested by Konstas et al. (2017). The bottom right of Figure 2 illustrates the linearization result of the AMR graph in Figure 1. The output sequence of our seq2seq model may produce an invalid graph. For example, the parenthesis parity may be broken, resulting in an incomplete graph. To ensure the validity of the graph produced in parsing, post-processing steps such as parenthesis parity restoration and invalid segment removal are introduced. We use the pre- and postprocessing scripts provided by Bevilacqua et al. (2021).1 3.3 Training Stages We now clarify the four different training stages. The whole training process is r"
2021.findings-emnlp.237,D16-1180,0,0.0272877,"ation, which provides a new way to enable multilingual AMR parsing. Knowledge Distillation for Sequence Generation Knowledge distillation (KD) is a classic technique originally proposed for model compression (Buciluˇa et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015). KD suggests training a (smaller) student model to mimic a (larger) teacher model, by minimizing the loss (typically crossentropy) between the teacher/student predictions (Romero et al., 2015; Yim et al., 2017; Zagoruyko and Komodakis, 2017). KD has been successfully applied to various natural language understanding tasks (Kuncoro et al., 2016; Hu et al., 2018; Sanh et al., 2019). For sequence generation tasks, Kim and Rush (2016) first introduce sequence-level KD, which aims to mimic the teacher’s actions at the sequence-level. KD has been proved useful in a range of sequence generation tasks such as machine translation (Freitag et al., 2017; Tan et al., 2019), non-autoregressive text generation (Gu et al., 2017; Zhou et al., 2019), and text summarization (Liu et al., 2020a). To the best of our knowledge, our paper is the first work to investigate the potential of knowledge distillation in the context of crosslingual AMR parsing."
2021.findings-emnlp.237,2020.acl-main.703,0,0.0171654,"in different languages. Damonte and Cohen (2018) show that it is possible to use the original AMR tant ingredient for superior performance, we also conduct experiments where the reference transla- annotations devised for English as representation for equivalent sentences in other languages and retions in Europarl are used as noise-free input to the lease a cross-lingual AMR evaluation benchmark student. Also, to show that the noise from MT is (Damonte and Cohen, 2020) very recently. Crossnon-trivial, we further employ BART-style random lingual AMR parsing suffers severely from the data noise (Lewis et al., 2020) for comparison. BARTscarcity issue; there is no gold annotated training style noise masks text spans in the input and we tune the rate of word deletion. The results are pre- data for languages other than English. Damonte and Cohen (2018) propose to build silver trainsented in Table 4. We show that MT noise is indeed ing data based on external bitext resources and helpful and its role cannot be replaced by simple English AMR parser. Blloshmi et al. (2020) find random noise. that translating the source side of existing English Effect of Data Sizes for Knowledge Distillation AMR dataset into oth"
2021.findings-emnlp.237,2021.eacl-main.30,0,0.0682866,"on the textual overlap between English words and AMR node values (i.e., concepts). include the original English test set in our evaluation. On four zero-resource languages, our single universal parser consistently outperforms the previous best results by large margins (+11.3 S MATCH points on average and up to +18.8 S MATCH points). Meanwhile, our parser achieves competitive results on English even compared with the latest state-ofthe-art English AMR parser in the literature. To sum up, our contributions are listed below: Some initial attempts (Damonte and Cohen, 2018; Blloshmi et al., 2020; Sheth et al., 2021) towards multilingual AMR parsing mainly investigated the construction of pseudo parallel data via annotation projection. In this paper, we study multilingual AMR parsing from the perspective of knowledge distillation (Buciluˇa et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015; Kim and Rush, 2016), where our primary goal is to improve a multilingual AMR parser by using an existing English parser as its teacher. We focus on a strict multilingual setting for developing one AMR parser that can parse all different languages. In contrast to the language-specific (one parser one language) sett"
2021.findings-emnlp.237,C18-1101,0,0.0117174,"ct Meaning Representation (AMR) (Banarescu et al., 2013) is a broad-coverage semantic formalism that encodes the meaning of a sentence as a rooted, directed, and labeled graph, where nodes represent concepts and edges represent relations among concepts. AMR parsing is the task of translating natural language sentences into their corresponding AMR graphs, which encompasses a set of natural language understanding tasks, such as named entity recognition, semantic role labeling, and coreference resolution. AMR has proved to be beneficial to a wide range of applications such as text summarization (Liao et al., 2018), machine translation (Song et al., 2019), and question answering (Kapanipathi et al., 2020; Xu et al., 2021). One most critical feature of the AMR formalism is that it abstracts away from syntactic realization and surface forms. As shown in Figure 1, different English The boy wants the girl to believe him. The boy’s desire is for the girl to believe him. The boy wants to be believed by the girl. AMR want-01 :ARG0 boy :ARG1 :ARG1 believe-01 :ARG0 girl German, Spanish, Italian, and Chinese Der Junge möchte, dass das Mädchen ihm glaubt. El chico quiere que la chica le crea. Il ragazzo vuole che"
2021.findings-emnlp.237,2021.naacl-main.56,0,0.0476053,"Missing"
2021.findings-emnlp.237,D18-1264,0,0.0141859,"Cohen, 2018; Zhu et al., 2778 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2778–2789 November 7–11, 2021. ©2021 Association for Computational Linguistics 2019). Second, the human-annotated resources for training are only available in English and none is present in other languages. Moreover, since the AMR graph involves rich semantic labels, the AMR annotation for other languages can be laborintensive and unaffordable. Third, current modeling techniques focus mostly on English. For example, existing AMR aligners (Flanigan et al., 2014; Pourdamghani et al., 2014; Liu et al., 2018) and widely-used pointer-generator mechanisms (Zhang et al., 2019b; Cai and Lam, 2019, 2020) rely on the textual overlap between English words and AMR node values (i.e., concepts). include the original English test set in our evaluation. On four zero-resource languages, our single universal parser consistently outperforms the previous best results by large margins (+11.3 S MATCH points on average and up to +18.8 S MATCH points). Meanwhile, our parser achieves competitive results on English even compared with the latest state-ofthe-art English AMR parser in the literature. To sum up, our contri"
2021.findings-emnlp.237,2020.tacl-1.47,0,0.261224,"cross-lingual language understanding tasks. For cross-lingual AMR parsing, in particular, Blloshmi et al. (2020) used mBERT2 (Devlin et al., 2019) while Sheth et al. (2021) employed XLM-R3 (Conneau et al., 1 https://github.com/SapienzaNLP/spring 2 bert-base-multilingual-cased 3 xlm-roberta-large 2020) to provide language-independent features. Unlike previous work, we argue that such encoderonly pre-trained models are not the most suitable choice for our seq2seq parser. Instead, we adopt mBART, an encoder-decoder denoising language model pre-trained with monolingual corpora in many languages (Liu et al., 2020b), to initialize both the encoder and decoder of our seq2seq parser. P2: Multilingual Machine Translation Pretraining (MMT-PT) The task of multilingual machine translation (MMT) is to learn one single model to translate between various language pairs. Essentially, natural languages can be considered as informal meaning representations compared to formal meaning representation such as AMR. On the other hand, AMR can be regarded as a special language. The above observations connect the dots between MMT and multilingual AMR parsing, both of which model the process of digesting the semantics in o"
2021.findings-emnlp.237,E17-1035,0,0.0132477,"translating the source side of existing English Effect of Data Sizes for Knowledge Distillation AMR dataset into other target languages produces Lastly, we study the relation between model per- better silver training data. Sheth et al. (2021) foformance and the size of monolingual data used cus on improving cross-lingual word-to-node alignfor KD. Figure 3 shows that the S MATCH scores ment for training cross-lingual AMR parsers that 2785 rely on explicit alignment. Our work follows the alignment-free seq2seq formulation (Barzdins and Gosko, 2016; Konstas et al., 2017; Van Noord and Bos, 2017; Peng et al., 2017; Zhang et al., 2019a; Ge et al., 2019; Bevilacqua et al., 2021) and we alternatively study this problem from the perspective of knowledge distillation, which provides a new way to enable multilingual AMR parsing. Knowledge Distillation for Sequence Generation Knowledge distillation (KD) is a classic technique originally proposed for model compression (Buciluˇa et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015). KD suggests training a (smaller) student model to mimic a (larger) teacher model, by minimizing the loss (typically crossentropy) between the teacher/student predictions (Romero"
2021.findings-emnlp.237,D14-1048,0,0.0229144,"ogram. guages (Damonte and Cohen, 2018; Zhu et al., 2778 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2778–2789 November 7–11, 2021. ©2021 Association for Computational Linguistics 2019). Second, the human-annotated resources for training are only available in English and none is present in other languages. Moreover, since the AMR graph involves rich semantic labels, the AMR annotation for other languages can be laborintensive and unaffordable. Third, current modeling techniques focus mostly on English. For example, existing AMR aligners (Flanigan et al., 2014; Pourdamghani et al., 2014; Liu et al., 2018) and widely-used pointer-generator mechanisms (Zhang et al., 2019b; Cai and Lam, 2019, 2020) rely on the textual overlap between English words and AMR node values (i.e., concepts). include the original English test set in our evaluation. On four zero-resource languages, our single universal parser consistently outperforms the previous best results by large margins (+11.3 S MATCH points on average and up to +18.8 S MATCH points). Meanwhile, our parser achieves competitive results on English even compared with the latest state-ofthe-art English AMR parser in the literature. To"
2021.findings-emnlp.237,Q19-1002,0,0.0182946,"u et al., 2013) is a broad-coverage semantic formalism that encodes the meaning of a sentence as a rooted, directed, and labeled graph, where nodes represent concepts and edges represent relations among concepts. AMR parsing is the task of translating natural language sentences into their corresponding AMR graphs, which encompasses a set of natural language understanding tasks, such as named entity recognition, semantic role labeling, and coreference resolution. AMR has proved to be beneficial to a wide range of applications such as text summarization (Liao et al., 2018), machine translation (Song et al., 2019), and question answering (Kapanipathi et al., 2020; Xu et al., 2021). One most critical feature of the AMR formalism is that it abstracts away from syntactic realization and surface forms. As shown in Figure 1, different English The boy wants the girl to believe him. The boy’s desire is for the girl to believe him. The boy wants to be believed by the girl. AMR want-01 :ARG0 boy :ARG1 :ARG1 believe-01 :ARG0 girl German, Spanish, Italian, and Chinese Der Junge möchte, dass das Mädchen ihm glaubt. El chico quiere que la chica le crea. Il ragazzo vuole che la ragazza gli creda. Figure 1: An exampl"
2021.findings-emnlp.237,2020.eamt-1.61,0,0.0248388,"Missing"
2021.findings-emnlp.237,2021.iwpt-1.6,0,0.0221391,"all different languages including English. • We obtain a performant multilingual AMR parser, establishing new state-of-the-art results on multiple languages. We hope our parser can facilitate the multilingual applications of AMR. 2 2.1 Background Prior Work Cross-lingual AMR parsing is the task of mapping a sentence in any language X to the AMR graph of its English translation. To date, there is no human-annotated X-AMR parallel dataset for training. Therefore, one straightforward solution is to translate the sentences from X into English then apply an English parser (Damonte and Cohen, 2018; Uhrig et al., 2021). However, it is argued that the method is not informative in terms of the cross-lingual properties of AMR (Damonte and Cohen, 2018; Blloshmi et al., 2020). To tackle cross-lingual AMR parsing, most previous work relies on pre-trained multilingual language models and silver training data (i.e., pseudo parallel data). Pre-trained Multilingual Language Model Previous work proves that language-independent features provided by pre-trained multilingual language models can boost cross-lingual parsing performance. For example, Blloshmi et al. (2020) use mBERT (Devlin et al., 2019) and Sheth et al. (2"
2021.findings-emnlp.237,2020.emnlp-main.196,0,0.0220711,"guage pairs. Essentially, natural languages can be considered as informal meaning representations compared to formal meaning representation such as AMR. On the other hand, AMR can be regarded as a special language. The above observations connect the dots between MMT and multilingual AMR parsing, both of which model the process of digesting the semantics in one form and and conveying the same semantics in another form. Therefore, we argue that pre-training our parser using the MMT task should be helpful. In fact, the usefulness of MT pre-training has also been validated in English AMR parsing (Xu et al., 2020). In practice, we directly use the mBARTmmt checkpoint (Tang et al., 2020), an MMT model covering 50 languages that are trained from mBART. F3: Knowledge Distillation Fine-tuning (KDFT) Motivated by the fact that the parsing accuracy on English is significantly better than those 2781 on other languages, we propose to reduce the performance gap via knowledge distillation (Kim and Rush, 2016). Specifically, we first pre-train a highperformance AMR parser for English and treat it as the teacher model. By considering our multilingual AMR parser as the student model, the goal is to transfer the kno"
2021.findings-emnlp.237,2021.findings-acl.90,1,0.720625,"e meaning of a sentence as a rooted, directed, and labeled graph, where nodes represent concepts and edges represent relations among concepts. AMR parsing is the task of translating natural language sentences into their corresponding AMR graphs, which encompasses a set of natural language understanding tasks, such as named entity recognition, semantic role labeling, and coreference resolution. AMR has proved to be beneficial to a wide range of applications such as text summarization (Liao et al., 2018), machine translation (Song et al., 2019), and question answering (Kapanipathi et al., 2020; Xu et al., 2021). One most critical feature of the AMR formalism is that it abstracts away from syntactic realization and surface forms. As shown in Figure 1, different English The boy wants the girl to believe him. The boy’s desire is for the girl to believe him. The boy wants to be believed by the girl. AMR want-01 :ARG0 boy :ARG1 :ARG1 believe-01 :ARG0 girl German, Spanish, Italian, and Chinese Der Junge möchte, dass das Mädchen ihm glaubt. El chico quiere que la chica le crea. Il ragazzo vuole che la ragazza gli creda. Figure 1: An example of AMR. Sentences written in English and other languages share the"
2021.findings-emnlp.237,xue-etal-2014-interlingua,0,0.0627008,"Missing"
2021.findings-emnlp.237,P19-1009,0,0.0336454,"Missing"
2021.findings-emnlp.237,D19-1392,0,0.0214797,"Missing"
2021.findings-emnlp.237,W19-3320,0,0.0259951,"Missing"
2021.findings-emnlp.390,Q17-1010,0,0.00733991,"urs, we concatenate the question and answer sequences as their inputs for them to utilize the answer information. For our proposed model, we report the results for the following variants: Base Model, which only uses LU to train the model; Base+ATE, where the base model is augmented with the ATE task using L as the loss function; Base+QA, where the base model is augmented with the pre-training of QA pair matching; Full Model, our full model involving both auxiliary tasks.7 3.3 Experimental Settings For baseline models using pre-trained word vectors, we use cc.zh.300.vec8 trained with fastText (Bojanowski et al., 2017) for fair comparison. For BERT-based models including ours, we use the same pre-trained BERT-Base,Chinese9 in all experiments, which includes 12 transformer layers and the hidden dimension dh is 768. For our proposed model, the parameters of BERT is further fine-tuned during the training process. Regarding the network architectures, the hidden dimension of the answer encoding module da is 300, the dimension of the encoded answer vector de is 64. For the local context capturing layer, the 7 The code is publicly available at https://github. com/IsakZhang/ABSA-QA. 8 https://github.com/facebookres"
2021.findings-emnlp.390,P17-1152,0,0.251728,"ation from the obtain the related opinion information, which is answer to help extract the discussed aspect. the “value” part. Similarly, we can also obtain 4584 the question-attended answer representation as ¯ a = ATTN(H a , H q ). H The matched information from the answer can well indicate the mentioned aspects. For example, it may rephrase or simply repeat the aspect term asked in the question and then present their senti¯q ment. To combine the attended representations H and the original representations H q , a multi-layer perceptron is typically involved in solving the text matching task (Chen et al., 2017; Yang et al., 2019). However, since we are tackling a token-level prediction problem, such a fusion method would obscure the fine-grained feature representations. We propose a gated fusion approach to absorb the aspect information from the answer while also maintain the most salient information in each question token. Concretely, for the i-th word, we have: ¯ q + bg ) g = σ(W r hqi + W a h i q q ˜ ¯q h = g h + (1 − g) h i i i (2) (3) where W r and W a are trainable parameters, σ and denote the sigmoid function and the element˜q wise multiplication respectively. The resulting h t represents th"
2021.findings-emnlp.390,2020.acl-main.582,0,0.152581,"ictions are marked with 3/7 respectively. 3.4.4 Case Analysis task aims to detect the mentioned aspect (He et al., 2017; Xu et al., 2019; Tulkens and van CranenWe present some sample cases including input burgh, 2020; Li et al., 2020; Wei et al., 2020). The QA pairs and predictions given by the baseline second aspect sentiment classification (ASC) task Span-Pipeline model, our proposed base model then predicts the sentiment polarity, assuming an and the full model in Table 4. We can see that aspect is given (Sun et al., 2019; Tang et al., 2020; Span-Pipeline fails when the alignment is needed Chen et al., 2020b; Zheng et al., 2020). between the question and answer sentences. For Since separately handling these two tasks igexample, the second answer A2 only comments on nores the relations between them and leads to unthe “last long” aspect, thus Span-Pipeline just ransatisfactory performance, recent works attempt to domly assigns a sentiment polarity for the “mask solve it in a unified framework. These studies either blemishes”. Regarding the third question Q3 , its adopt a unified tagging scheme (Li et al., 2019b,a; answer expresses “okay” to both aspects mentioned Hu et al., 2019) or solving them i"
2021.findings-emnlp.390,2020.acl-main.338,0,0.158998,"ictions are marked with 3/7 respectively. 3.4.4 Case Analysis task aims to detect the mentioned aspect (He et al., 2017; Xu et al., 2019; Tulkens and van CranenWe present some sample cases including input burgh, 2020; Li et al., 2020; Wei et al., 2020). The QA pairs and predictions given by the baseline second aspect sentiment classification (ASC) task Span-Pipeline model, our proposed base model then predicts the sentiment polarity, assuming an and the full model in Table 4. We can see that aspect is given (Sun et al., 2019; Tang et al., 2020; Span-Pipeline fails when the alignment is needed Chen et al., 2020b; Zheng et al., 2020). between the question and answer sentences. For Since separately handling these two tasks igexample, the second answer A2 only comments on nores the relations between them and leads to unthe “last long” aspect, thus Span-Pipeline just ransatisfactory performance, recent works attempt to domly assigns a sentiment polarity for the “mask solve it in a unified framework. These studies either blemishes”. Regarding the third question Q3 , its adopt a unified tagging scheme (Li et al., 2019b,a; answer expresses “okay” to both aspects mentioned Hu et al., 2019) or solving them i"
2021.findings-emnlp.390,2020.acl-main.340,0,0.473455,"evaluated on three real-world datasets and the results show that our model outperforms several strong baselines adopted from related state-of-the-art models. Q: How about the screen? Is this phone’s battery life durable? Thanks in advance! A: Not as large as I thought. But the battery is quite good, I like it. TASK INPUT OUTPUT ATE-QA QA pair [screen]; [battery life] QA pair + [screen] NEG ASC-QA QA pair + [battery life] POS [screen] NEG ABSA-QA QA pair [battery life] POS Figure 1: Demonstrations of ABSA-QA task and its two sub-tasks including ATE-QA and ASC-QA. prediction (Li et al., 2019a; Chen and Qian, 2020a; Mao et al., 2021; Zhang et al., 2021) have received increasing attention in recent years. Most existing ABSA studies focus on a single opinionated sentence such as the customer review (Pontiki et al., 2014, 2015). Besides product reviews, another kind of opinion sharing platform, namely question answering (QA) forum, has been provided on many E-commerce websites, due to the rising demand for users and sellers to communicate with the former buyers to obtain their opinions towards various aspects of the concerned 1 Introduction product (Zhang et al., 2020b). Thus, investigatAspect-based senti"
2021.findings-emnlp.390,P17-1036,0,0.123915,"e sentence “The feel of the restaurant was Several attempts have been made on analyzing crowded but the food is great.”, ATE is to detect the sentiment information in QA forums. However, the mentioned aspects “feel” and “food”, whereas they either predict an overall sentiment polarity tosupposing aspects are given, ASC predicts their wards the entire QA pair (Shen et al., 2018; Hu sentiment polarities as negative and positive re- et al., 2020) or only consider partial ABSA-QA spectively. Given the broad application scenarios, problems. For example, Wang et al. (2019) tackle the two sub-tasks (He et al., 2017; Sun et al., 2019; the ASC-QA task under the assumption that the Tulkens and van Cranenburgh, 2020) and their joint targeted aspects are given. As illustrated in Figure ∗ 1, they perform aspect-level sentiment classificaThe work described in this paper is substantially supported by a grant from the Research Grant Council of the tion according to both of the QA pair and the input Hong Kong Special Administrative Region, China (Project aspect. However, obtaining the discussed aspects Code: 14200719). Work partially done when Wenxuan Zhang was an intern at Alibaba. is not a trivial task, which i"
2021.findings-emnlp.390,P19-1048,0,0.261327,"l of the tion according to both of the QA pair and the input Hong Kong Special Administrative Region, China (Project aspect. However, obtaining the discussed aspects Code: 14200719). Work partially done when Wenxuan Zhang was an intern at Alibaba. is not a trivial task, which is especially difficult 4582 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4582–4591 November 7–11, 2021. ©2021 Association for Computational Linguistics for those QA pairs involving multiple aspects. Inspired by previous success on jointly solving the two sub-tasks in review-oriented ABSA (He et al., 2019; Luo et al., 2019; Chen and Qian, 2020a), we aim to handle the ABSA-QA task in a similar unified setting in this work1 . As shown in Figure 1, given a question-answer pair, our goal is to jointly detect the discussed aspect(s) and predict their corresponding sentiment polarities. To tackle the ABSA-QA task, an intuitive idea would be concatenating the question and answer sentence, then employing the existing ABSA models to solve it. However, the question and answer sentence are two parallel sequences, therefore, simply concatenating them cannot produce a semanticfluent expression. In such a c"
2021.findings-emnlp.390,P19-1051,0,0.295664,"entiment polarities. To tackle the ABSA-QA task, an intuitive idea would be concatenating the question and answer sentence, then employing the existing ABSA models to solve it. However, the question and answer sentence are two parallel sequences, therefore, simply concatenating them cannot produce a semanticfluent expression. In such a concatenation, the aspect terms and their corresponding opinion words do not appear next or near to each other, making the position clue utilized by many ABSA models, i.e., the aspect modifier is closer to the corresponding aspect term in the sentence, invalid (Hu et al., 2019; He et al., 2019). To make matters worse, it will result in wrong proximity relation, for instance, compared with “quite good”, “not as large as” is nearer to “battery life” in the example. Meanwhile, because the opinions are expressed in an interactive manner, i.e., the question asks about one or multiple aspects and the answer expresses the opinions towards them, the aspect terms are likely to be omitted or rephrased in the answer sentence. Returning to the example in Figure 1, the aspect “battery life” is shortened to “battery” while the explicit mention of the aspect “screen” is directly"
2021.findings-emnlp.390,2020.acl-main.631,0,0.0123657,"[质量]POS 3 [quality]POS [质量]POS 7 [质量]POS 7 [质量]NEG 3 [quality]POS [quality]POS [quality]NEG None 7 Table 4: Case analysis. The “Examples” column contains sample QA pairs with gold labels where words in brackets are annotated aspect terms, the subscripts denotes their sentiment polarities. “None” in predictions denotes that no aspect terms are extracted. The correct/incorrect predictions are marked with 3/7 respectively. 3.4.4 Case Analysis task aims to detect the mentioned aspect (He et al., 2017; Xu et al., 2019; Tulkens and van CranenWe present some sample cases including input burgh, 2020; Li et al., 2020; Wei et al., 2020). The QA pairs and predictions given by the baseline second aspect sentiment classification (ASC) task Span-Pipeline model, our proposed base model then predicts the sentiment polarity, assuming an and the full model in Table 4. We can see that aspect is given (Sun et al., 2019; Tang et al., 2020; Span-Pipeline fails when the alignment is needed Chen et al., 2020b; Zheng et al., 2020). between the question and answer sentences. For Since separately handling these two tasks igexample, the second answer A2 only comments on nores the relations between them and leads to unthe “l"
2021.findings-emnlp.390,D19-5505,1,0.565803,"proposed method is evaluated on three real-world datasets and the results show that our model outperforms several strong baselines adopted from related state-of-the-art models. Q: How about the screen? Is this phone’s battery life durable? Thanks in advance! A: Not as large as I thought. But the battery is quite good, I like it. TASK INPUT OUTPUT ATE-QA QA pair [screen]; [battery life] QA pair + [screen] NEG ASC-QA QA pair + [battery life] POS [screen] NEG ABSA-QA QA pair [battery life] POS Figure 1: Demonstrations of ABSA-QA task and its two sub-tasks including ATE-QA and ASC-QA. prediction (Li et al., 2019a; Chen and Qian, 2020a; Mao et al., 2021; Zhang et al., 2021) have received increasing attention in recent years. Most existing ABSA studies focus on a single opinionated sentence such as the customer review (Pontiki et al., 2014, 2015). Besides product reviews, another kind of opinion sharing platform, namely question answering (QA) forum, has been provided on many E-commerce websites, due to the rising demand for users and sellers to communicate with the former buyers to obtain their opinions towards various aspects of the concerned 1 Introduction product (Zhang et al., 2020b). Thus, invest"
2021.findings-emnlp.390,P19-1056,0,0.0909167,"ording to both of the QA pair and the input Hong Kong Special Administrative Region, China (Project aspect. However, obtaining the discussed aspects Code: 14200719). Work partially done when Wenxuan Zhang was an intern at Alibaba. is not a trivial task, which is especially difficult 4582 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4582–4591 November 7–11, 2021. ©2021 Association for Computational Linguistics for those QA pairs involving multiple aspects. Inspired by previous success on jointly solving the two sub-tasks in review-oriented ABSA (He et al., 2019; Luo et al., 2019; Chen and Qian, 2020a), we aim to handle the ABSA-QA task in a similar unified setting in this work1 . As shown in Figure 1, given a question-answer pair, our goal is to jointly detect the discussed aspect(s) and predict their corresponding sentiment polarities. To tackle the ABSA-QA task, an intuitive idea would be concatenating the question and answer sentence, then employing the existing ABSA models to solve it. However, the question and answer sentence are two parallel sequences, therefore, simply concatenating them cannot produce a semanticfluent expression. In such a concatenation, the"
2021.findings-emnlp.390,P19-1465,0,0.127581,"in the related opinion information, which is answer to help extract the discussed aspect. the “value” part. Similarly, we can also obtain 4584 the question-attended answer representation as ¯ a = ATTN(H a , H q ). H The matched information from the answer can well indicate the mentioned aspects. For example, it may rephrase or simply repeat the aspect term asked in the question and then present their senti¯q ment. To combine the attended representations H and the original representations H q , a multi-layer perceptron is typically involved in solving the text matching task (Chen et al., 2017; Yang et al., 2019). However, since we are tackling a token-level prediction problem, such a fusion method would obscure the fine-grained feature representations. We propose a gated fusion approach to absorb the aspect information from the answer while also maintain the most salient information in each question token. Concretely, for the i-th word, we have: ¯ q + bg ) g = σ(W r hqi + W a h i q q ˜ ¯q h = g h + (1 − g) h i i i (2) (3) where W r and W a are trainable parameters, σ and denote the sigmoid function and the element˜q wise multiplication respectively. The resulting h t represents the fused representati"
2021.findings-emnlp.390,D13-1171,0,0.0871582,"Missing"
2021.findings-emnlp.390,S15-2082,0,0.0727637,"Missing"
2021.findings-emnlp.390,S14-2004,0,0.124535,"Missing"
2021.findings-emnlp.390,N19-1035,0,0.235306,"eel of the restaurant was Several attempts have been made on analyzing crowded but the food is great.”, ATE is to detect the sentiment information in QA forums. However, the mentioned aspects “feel” and “food”, whereas they either predict an overall sentiment polarity tosupposing aspects are given, ASC predicts their wards the entire QA pair (Shen et al., 2018; Hu sentiment polarities as negative and positive re- et al., 2020) or only consider partial ABSA-QA spectively. Given the broad application scenarios, problems. For example, Wang et al. (2019) tackle the two sub-tasks (He et al., 2017; Sun et al., 2019; the ASC-QA task under the assumption that the Tulkens and van Cranenburgh, 2020) and their joint targeted aspects are given. As illustrated in Figure ∗ 1, they perform aspect-level sentiment classificaThe work described in this paper is substantially supported by a grant from the Research Grant Council of the tion according to both of the QA pair and the input Hong Kong Special Administrative Region, China (Project aspect. However, obtaining the discussed aspects Code: 14200719). Work partially done when Wenxuan Zhang was an intern at Alibaba. is not a trivial task, which is especially diffi"
2021.findings-emnlp.390,2021.acl-short.64,1,0.703495,"and the results show that our model outperforms several strong baselines adopted from related state-of-the-art models. Q: How about the screen? Is this phone’s battery life durable? Thanks in advance! A: Not as large as I thought. But the battery is quite good, I like it. TASK INPUT OUTPUT ATE-QA QA pair [screen]; [battery life] QA pair + [screen] NEG ASC-QA QA pair + [battery life] POS [screen] NEG ABSA-QA QA pair [battery life] POS Figure 1: Demonstrations of ABSA-QA task and its two sub-tasks including ATE-QA and ASC-QA. prediction (Li et al., 2019a; Chen and Qian, 2020a; Mao et al., 2021; Zhang et al., 2021) have received increasing attention in recent years. Most existing ABSA studies focus on a single opinionated sentence such as the customer review (Pontiki et al., 2014, 2015). Besides product reviews, another kind of opinion sharing platform, namely question answering (QA) forum, has been provided on many E-commerce websites, due to the rising demand for users and sellers to communicate with the former buyers to obtain their opinions towards various aspects of the concerned 1 Introduction product (Zhang et al., 2020b). Thus, investigatAspect-based sentiment analysis (ABSA) usually ing the ABS"
2021.findings-emnlp.390,2020.acl-main.296,0,0.0360922,"re representaSpan-Pipeline. Our proposed model, both the base and full model successfully handle these two cases, tions (He et al., 2019; Luo et al., 2019). Recently, showing the necessity to model the interactions be- there are also some attempts of combining another tween the given QA pairs. For the last example Q4 , related task, namely opinion term extraction (OTE), with the ATE and/or ASC tasks to provide a more the answer does not provide any direct comment on the asked aspects, for instance, it does not men- complete understanding of the aspect-level user sentiment (Chen et al., 2020a; Zhao et al., 2020; tion aspect “quality” or any related opinion term Chen and Qian, 2020b; Liang et al., 2020; Zhang such as “bad” at all, making it difficult to predict et al., 2021). the sentiment polarity. Our proposed full model However, most existing studies target at cusequipped with the QA matching pre-training gives tomer reviews (Pontiki et al., 2014, 2015) or twitter correct predictions on them, which attributes to the posts (Mitchell et al., 2013). Thus the proposed pre-training that brings in some prior knowledge methods are often tailored for observations made in for identifying that the answer is"
2021.findings-emnlp.390,2020.acl-main.588,0,0.0349641,"notes that no aspect terms are extracted. The correct/incorrect predictions are marked with 3/7 respectively. 3.4.4 Case Analysis task aims to detect the mentioned aspect (He et al., 2017; Xu et al., 2019; Tulkens and van CranenWe present some sample cases including input burgh, 2020; Li et al., 2020; Wei et al., 2020). The QA pairs and predictions given by the baseline second aspect sentiment classification (ASC) task Span-Pipeline model, our proposed base model then predicts the sentiment polarity, assuming an and the full model in Table 4. We can see that aspect is given (Sun et al., 2019; Tang et al., 2020; Span-Pipeline fails when the alignment is needed Chen et al., 2020b; Zheng et al., 2020). between the question and answer sentences. For Since separately handling these two tasks igexample, the second answer A2 only comments on nores the relations between them and leads to unthe “last long” aspect, thus Span-Pipeline just ransatisfactory performance, recent works attempt to domly assigns a sentiment polarity for the “mask solve it in a unified framework. These studies either blemishes”. Regarding the third question Q3 , its adopt a unified tagging scheme (Li et al., 2019b,a; answer expresses"
2021.findings-emnlp.390,2020.acl-main.290,0,0.0312401,"Missing"
2021.findings-emnlp.390,P19-1345,0,0.071119,"ontiki et al., 2014). For an QA pairs. example sentence “The feel of the restaurant was Several attempts have been made on analyzing crowded but the food is great.”, ATE is to detect the sentiment information in QA forums. However, the mentioned aspects “feel” and “food”, whereas they either predict an overall sentiment polarity tosupposing aspects are given, ASC predicts their wards the entire QA pair (Shen et al., 2018; Hu sentiment polarities as negative and positive re- et al., 2020) or only consider partial ABSA-QA spectively. Given the broad application scenarios, problems. For example, Wang et al. (2019) tackle the two sub-tasks (He et al., 2017; Sun et al., 2019; the ASC-QA task under the assumption that the Tulkens and van Cranenburgh, 2020) and their joint targeted aspects are given. As illustrated in Figure ∗ 1, they perform aspect-level sentiment classificaThe work described in this paper is substantially supported by a grant from the Research Grant Council of the tion according to both of the QA pair and the input Hong Kong Special Administrative Region, China (Project aspect. However, obtaining the discussed aspects Code: 14200719). Work partially done when Wenxuan Zhang was an intern"
2021.findings-emnlp.390,2020.acl-main.339,0,0.0231202,"y]POS [质量]POS 7 [质量]POS 7 [质量]NEG 3 [quality]POS [quality]POS [quality]NEG None 7 Table 4: Case analysis. The “Examples” column contains sample QA pairs with gold labels where words in brackets are annotated aspect terms, the subscripts denotes their sentiment polarities. “None” in predictions denotes that no aspect terms are extracted. The correct/incorrect predictions are marked with 3/7 respectively. 3.4.4 Case Analysis task aims to detect the mentioned aspect (He et al., 2017; Xu et al., 2019; Tulkens and van CranenWe present some sample cases including input burgh, 2020; Li et al., 2020; Wei et al., 2020). The QA pairs and predictions given by the baseline second aspect sentiment classification (ASC) task Span-Pipeline model, our proposed base model then predicts the sentiment polarity, assuming an and the full model in Table 4. We can see that aspect is given (Sun et al., 2019; Tang et al., 2020; Span-Pipeline fails when the alignment is needed Chen et al., 2020b; Zheng et al., 2020). between the question and answer sentences. For Since separately handling these two tasks igexample, the second answer A2 only comments on nores the relations between them and leads to unthe “last long” aspect, t"
2021.findings-emnlp.390,N19-1242,0,0.0414769,"Missing"
2021.findings-emnlp.409,K16-1002,0,0.0470319,"stigate the holistic quality of the generated sentences. For each dataset, we firstly choose two source sentences and then randomly select 25 exemplars for each source sentence to generate a total of 50 sentences. Table 5 shows the results of human assessment. It can be seen that our model obtains a higher score than SGCP and CGEN, which is consistent with the automatic evaluation results. These results are expected because SCPN and SGCP use a parse tree as the style which is lack of the lexical information and very unstable. Moreover, CEGN is VAE-based which is intrinsically harder to train (Bowman et al., 2016). QQP-Pos Model BLEU R-1 R-2 SCPN 15.6 40.6 20.5 36.7 66.9 45.0 SGCP CGEN 34.9 62.6 42.7 Ours 45.8 71.0 52.8 Ours-w/o-CCL 43.1 70.0 50.6 42.7 69.7 49.9 Ours-w/o-SCL Ours-w/o-both 40.8 68.4 48.4 ParaNMT SCPN 6.4 30.3 11.2 15.3 46.6 21.8 SGCP CGEN 13.6 44.8 21.0 Ours 16.2 50.6 25.3 Ours-w/o-CCL 15.3 50.8 25.2 Ours-w/o-SCL 15.2 50.2 24.4 15.3 50.2 24.9 Ours-w/o-both R-L 44.6 69.6 65.4 73.3 72.3 71.8 70.8 METEOR 19.6 39.8 37.4 45.8 43.5 43.6 41.6 34.6 49.7 48.3 52.1 52.4 51.5 51.6 14.6 25.9 24.8 28.4 28.0 28.0 27.7 Table 2: Automatic Evaluation Results. Model Ours ED-E ED-R 2.49 2.64 ED-E ED-R 4.3"
2021.findings-emnlp.409,N19-1423,0,0.173166,"onParaphrase generation (Gupta et al., 2017; Li et al., trastive learning originates from computer vision 2019), aiming to generate a sentence with the area (Chen et al., 2020a; Khosla et al., 2020) and same semantic meaning of the source sentence, now, it also shows its powerfulness in natural lanhas achieved a great success in recent years. To guage processing area. For instance, Iter et al. obtain a paraphrase sentence with a particular (2020) employ contrastive learning to improve the style, Exemplar-Guided Paraphrase Generation quality of discourse-level sentence representations. (EGPG) (Chen et al., 2019) has attracted considIn our proposed model, besides the basic encodererable attention. Different from other controllable decoder generation task, a content contrastive loss text generation tasks whose constraints are taken is designed to force the content encoder to distinfrom a finite set, e.g., binary sentiment or politguish features of the same content from features of ical slant (Yang et al., 2018; Prabhumoye et al., different content. Similarly, a style contrastive loss 2018), multiple personas (Kang et al., 2019), over is also employed to obtain a similar distinguishing ∗ The work descri"
2021.findings-emnlp.409,D19-1198,0,0.0171734,"enerate target word is obtained as follows paraphrase which is a component rearrangement of pt = sof tmax(W ht ) (3) the original input through manipulating the parse ht = GRU (ht−1 , e(yt−1 )) (4) tree. Contrastive Learning In the past few years, where h0 is initialized as [cXi , sZi ] and W is a parameter matrix. yt−1 is the word in the previous many unsupervised feature extraction algorithms have emerged, for instance, variational autoen- step t−1 and y0 is the special symbol [SOS] which coder (Kingma and Welling, 2014; Xu et al., 2020; represents the start of the sentence. e(yt−1 ) is the Gao et al., 2019b,a), generalised language mod- embedding of the word yt−1 . els (Brown et al., 2020; Devlin et al., 2019). All the Negative log-likelihood loss (NLL) is employed 4755 style contrastive loss as the basic optimization objective |Yi | 1 X I(yt )T log pt (5) Lnll = − i |Yi | t=1 where I(yt ) represents the one-hot encoding of the word yt in the vocabulary. Content Contrastive Learning (CCL) Considering that Xi and Yi share the same content, their content features should be close with each other in the content feature space. Contrastive Learning which is designed to minimize the distance between p"
2021.findings-emnlp.409,2020.acl-main.22,0,0.0133029,"ependent low-dimensional style features sZi : spaces (Cheng et al., 2020). Iyyer et al. (2018) and cXi = Ec (Xi ) (1) Kumar et al. (2020) directly utilise the parse tree sZi = Es (Zi ) (2) information of the exemplar as the style informaThen c and s are concatenated and inputted Xi Zi tion without separating style from sentences. Chen into the decoder as the initial hidden state to generet al. (2019) propose a model which can directly exate a sequence of probabilities over vocabulary. At tract style features from a modified target sentence. the step t, the predicted probability pt of the t-th Goyal and Durrett (2020) provide a way to generate target word is obtained as follows paraphrase which is a component rearrangement of pt = sof tmax(W ht ) (3) the original input through manipulating the parse ht = GRU (ht−1 , e(yt−1 )) (4) tree. Contrastive Learning In the past few years, where h0 is initialized as [cXi , sZi ] and W is a parameter matrix. yt−1 is the word in the previous many unsupervised feature extraction algorithms have emerged, for instance, variational autoen- step t−1 and y0 is the special symbol [SOS] which coder (Kingma and Welling, 2014; Xu et al., 2020; represents the start of the sentenc"
2021.findings-emnlp.409,2020.acl-main.439,0,0.0228643,"re of input by reconstructing the original input or predicting masked words and so on which do not take the relationships between the inputs into consideration. Therefore, contrastive learning, whose loss is designed to narrow down the distance between features of similar inputs and to enlarge the distance of dissimilar inputs, has been proposed and achieved a great success in both unsupervised (Chen et al., 2020a) and supervised (Khosla et al., 2020) image feature extraction. There are also some works trying to apply contrastive learning into natural language processing domain. For instance, Iter et al. (2020) propose a pretraining method for sentence representation which employs contrastive learning to improve the quality of discourse-level representations. Giorgi et al. (2020) utilise it to pretrain the transformer and btains state-of-the-art results on SentEval (Conneau and Kiela, 2018). All of the above successes spur us to test whether contrastive learning is helpful on EGPG. Paraphrase Generation Researches on paraphrase generation has been for a long time. Traditional methods solve this problem mainly through statistical machine translation (Quirk et al., 2004) or rule-based word substitutio"
2021.findings-emnlp.409,N18-1170,0,0.0941084,"es the tion tasks whose constraints are sentiment (Yang output. Our model is trained by optimizing three et al., 2018; Xu et al., 2021), gender (Prabhumoye losses simultaneously: (1) generation loss; (2) conet al., 2018), topics (Wang et al., 2021). These tent contrastive loss; (3) style contrastive loss. tasks are highy related to Disentangled RepresentaGeneration Task For Xi and Zi , we firstly obtion Learning (DRL) which maps different aspects tain their corresponding content features cXi and of the input data to independent low-dimensional style features sZi : spaces (Cheng et al., 2020). Iyyer et al. (2018) and cXi = Ec (Xi ) (1) Kumar et al. (2020) directly utilise the parse tree sZi = Es (Zi ) (2) information of the exemplar as the style informaThen c and s are concatenated and inputted Xi Zi tion without separating style from sentences. Chen into the decoder as the initial hidden state to generet al. (2019) propose a model which can directly exate a sequence of probabilities over vocabulary. At tract style features from a modified target sentence. the step t, the predicted probability pt of the t-th Goyal and Durrett (2020) provide a way to generate target word is obtained as follows paraphra"
2021.findings-emnlp.409,D19-1179,0,0.0139015,"ase Generation quality of discourse-level sentence representations. (EGPG) (Chen et al., 2019) has attracted considIn our proposed model, besides the basic encodererable attention. Different from other controllable decoder generation task, a content contrastive loss text generation tasks whose constraints are taken is designed to force the content encoder to distinfrom a finite set, e.g., binary sentiment or politguish features of the same content from features of ical slant (Yang et al., 2018; Prabhumoye et al., different content. Similarly, a style contrastive loss 2018), multiple personas (Kang et al., 2019), over is also employed to obtain a similar distinguishing ∗ The work described in this paper is substantially supeffect for the style features. Experimental results ported by a grant from the Research Grant Council of the on two benchmark datasets, namely QQP-Pos and Hong Kong Special Administrative Region, China (Project Code: 14200620). ParaNMT, show that superior performance can be 4754 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4754–4761 November 7–11, 2021. ©2021 Association for Computational Linguistics achieved with the help of the contrastive losses."
2021.findings-emnlp.409,2020.tacl-1.22,0,0.0545203,"timent (Yang output. Our model is trained by optimizing three et al., 2018; Xu et al., 2021), gender (Prabhumoye losses simultaneously: (1) generation loss; (2) conet al., 2018), topics (Wang et al., 2021). These tent contrastive loss; (3) style contrastive loss. tasks are highy related to Disentangled RepresentaGeneration Task For Xi and Zi , we firstly obtion Learning (DRL) which maps different aspects tain their corresponding content features cXi and of the input data to independent low-dimensional style features sZi : spaces (Cheng et al., 2020). Iyyer et al. (2018) and cXi = Ec (Xi ) (1) Kumar et al. (2020) directly utilise the parse tree sZi = Es (Zi ) (2) information of the exemplar as the style informaThen c and s are concatenated and inputted Xi Zi tion without separating style from sentences. Chen into the decoder as the initial hidden state to generet al. (2019) propose a model which can directly exate a sequence of probabilities over vocabulary. At tract style features from a modified target sentence. the step t, the predicted probability pt of the t-th Goyal and Durrett (2020) provide a way to generate target word is obtained as follows paraphrase which is a component rearrangement of pt"
2021.findings-emnlp.409,W07-0734,0,0.0999351,"emplar Zi for each source-target pair (Xi , Yi ) based on the POS tag sequence 1 similarity (refer to Appendix A). 4.2 Baselines & Metrics We compare our model with (1) SCPN (Iyyer et al., 2018) which employs a parse generator to output the full linearized parse tree as the style by inputting a parse template; (2) SGCP (Kumar et al., 2020) which extracts the style information directly from the parse tree of the exemplar sentence; (3) CGEN (Chen et al., 2019), an approach based on variational inference (Kingma and Welling, 2014). The evaluation metrics are BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and ROUGE (R) (Lin, 2004). We also conduct human evaluation to investigate the quality of the generated sentences. Moreover, we propose Content (10) 4756 1 We use NLTK for POS tagging. Matching Accuracy (CMA) to gauge the quality of the generated embeddings for the content. CMA will be introduced in Section 4.5. 4.3 Implementation Details Each sentence is trimmed with a maximum length 15. The word embedding is initialized with 300-d pretrained GloVe (Pennington et al., 2014). We use a BERT-based (Devlin et al., 2019) architecture for the style encoder Es and the dimension of style features is"
2021.findings-emnlp.409,W10-4223,0,0.0495158,"Missing"
2021.findings-emnlp.409,N18-1169,0,0.055609,"Missing"
2021.findings-emnlp.409,P19-1332,0,0.0350808,"Missing"
2021.findings-emnlp.409,W04-1013,0,0.0373008,"i , Yi ) based on the POS tag sequence 1 similarity (refer to Appendix A). 4.2 Baselines & Metrics We compare our model with (1) SCPN (Iyyer et al., 2018) which employs a parse generator to output the full linearized parse tree as the style by inputting a parse template; (2) SGCP (Kumar et al., 2020) which extracts the style information directly from the parse tree of the exemplar sentence; (3) CGEN (Chen et al., 2019), an approach based on variational inference (Kingma and Welling, 2014). The evaluation metrics are BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and ROUGE (R) (Lin, 2004). We also conduct human evaluation to investigate the quality of the generated sentences. Moreover, we propose Content (10) 4756 1 We use NLTK for POS tagging. Matching Accuracy (CMA) to gauge the quality of the generated embeddings for the content. CMA will be introduced in Section 4.5. 4.3 Implementation Details Each sentence is trimmed with a maximum length 15. The word embedding is initialized with 300-d pretrained GloVe (Pennington et al., 2014). We use a BERT-based (Devlin et al., 2019) architecture for the style encoder Es and the dimension of style features is 768. For content encoder"
2021.findings-emnlp.409,P02-1040,0,0.111117,"ar et al. (2020) to search an exemplar Zi for each source-target pair (Xi , Yi ) based on the POS tag sequence 1 similarity (refer to Appendix A). 4.2 Baselines & Metrics We compare our model with (1) SCPN (Iyyer et al., 2018) which employs a parse generator to output the full linearized parse tree as the style by inputting a parse template; (2) SGCP (Kumar et al., 2020) which extracts the style information directly from the parse tree of the exemplar sentence; (3) CGEN (Chen et al., 2019), an approach based on variational inference (Kingma and Welling, 2014). The evaluation metrics are BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and ROUGE (R) (Lin, 2004). We also conduct human evaluation to investigate the quality of the generated sentences. Moreover, we propose Content (10) 4756 1 We use NLTK for POS tagging. Matching Accuracy (CMA) to gauge the quality of the generated embeddings for the content. CMA will be introduced in Section 4.5. 4.3 Implementation Details Each sentence is trimmed with a maximum length 15. The word embedding is initialized with 300-d pretrained GloVe (Pennington et al., 2014). We use a BERT-based (Devlin et al., 2019) architecture for the style encoder Es and"
2021.findings-emnlp.409,D14-1162,0,0.0932443,"d on variational inference (Kingma and Welling, 2014). The evaluation metrics are BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and ROUGE (R) (Lin, 2004). We also conduct human evaluation to investigate the quality of the generated sentences. Moreover, we propose Content (10) 4756 1 We use NLTK for POS tagging. Matching Accuracy (CMA) to gauge the quality of the generated embeddings for the content. CMA will be introduced in Section 4.5. 4.3 Implementation Details Each sentence is trimmed with a maximum length 15. The word embedding is initialized with 300-d pretrained GloVe (Pennington et al., 2014). We use a BERT-based (Devlin et al., 2019) architecture for the style encoder Es and the dimension of style features is 768. For content encoder Ec , we use GRU (Chung et al., 2014) with hidden state size 512. During training, the teacher forcing technique is applied with the rate 1.0. The balancing parameters λ1 and λ2 are both set to 0.1. The temperature parameter τ is set to 0.5. We train our model using Adam optimizer with the learning rate 1e-4 and the training epochs are set to 30 and 45 for ParaNMT and QQP-Pos respectively. 4.4 Results As summarized in Table 2, our model outperforms SC"
2021.findings-emnlp.409,P18-1080,0,0.0406778,"Missing"
2021.findings-emnlp.409,C16-1275,0,0.0178786,"representations. Giorgi et al. (2020) utilise it to pretrain the transformer and btains state-of-the-art results on SentEval (Conneau and Kiela, 2018). All of the above successes spur us to test whether contrastive learning is helpful on EGPG. Paraphrase Generation Researches on paraphrase generation has been for a long time. Traditional methods solve this problem mainly through statistical machine translation (Quirk et al., 2004) or rule-based word substitution (Wubben et al., 2010). In the era of deep learning, approaches based on the encoder-decoder framework have emerged in large numbers (Prakash et al., 2016; Chen et al., 2020b). In addition to basic seq2seq model, Li et al. (2018) add a pair-wise discriminator to judge whether the input sentence and generated sentence are paraphrases of each other, with the help of reinforcement learning. To generate diverse paraphrases, i.e., one to many mapping, Gupta et al. (2017) combine the power of RNN-based sequence-to-sequence model and the variational autoencoder. At decoding time, a noise sampled from the Gaussian distribution are appended to input to generate a diverse output. Qian 3 Proposed Model et al. (2019) propose a approach which use multiple g"
2021.findings-emnlp.409,D19-1313,0,0.0232998,"Missing"
2021.findings-emnlp.409,W04-3219,0,0.0273268,"Missing"
2021.findings-emnlp.99,P17-1171,0,0.432469,"er to capture the evidence-evidence interac1 Introduction tions by reformulating the query with newly reQuestion Answering (QA) with external knowltrieved evidence fact. However, the retriever would edge has gained increasing attention in recent years inevitably retrieve partially related facts. Such as it mimics human behavior to first filter out relnoise is continuously amplified during the iteraevant knowledge from massive information. Prior tive retrieval process, raising concerns about the works usually employ a retriever-reader architecquality of the reasoning chain. Prior works adture (Chen et al., 2017), where the retriever redress this issue by training the retriever against the trieves top-ranked evidence facts from a large corground-truth reasoning chain (Yang et al., 2018; pus and the reader conducts reasoning with these Geva et al., 2021). However, such method is less facts. This architecture works well in single-hop effective when the ground-truth reasoning chain is QA, where the answer can be easily inferred with partially provided (Mihaylov et al., 2018; Ferguson only one evidence fact. However, it is hard to reet al., 2020) or not applicable when the groundtrieve all necessary evide"
2021.findings-emnlp.99,N19-1423,0,0.0192629,"Missing"
2021.findings-emnlp.99,2020.emnlp-main.710,0,0.0248726,"identify the correct evidence facts (Nie et al., 2019; Tu et al., 2020). It is a good step to control the quality of reasoning 5.3 Case Study on Reasoning chains, but still remains an issue when the groundWe show how our generated reasoning chain can truth reasoning chain is not available (Clark et al., support explainable reasoning in Table 6. Iterative 2018). Other works explore the effectiveness of retrieval can retrieve the first evidence fact as it KG by either automatically constructing the graph 1150 using named entity or semantic role labeling (Qiu et al., 2019; Bosselut et al., 2019; Fang et al., 2020; Chen et al., 2019) or resorting to existing KG (Saxena et al., 2020; Zhang et al., 2020; Yasunaga et al., 2021). Despite the high precision of those KGs, they are known to suffer from sparsity in existing KG (Zhao et al., 2020), where complex reasoning chains are unlikely to be covered by the closedform relations in KG (Lei et al., 2020). Dense-Vector Retriever: In contrast to termbased retriever implemented with TF-IDF or BM25 (Chen et al., 2017; Wang et al., 2018), densevector retriever has received increasing attention as it captures the semantic matching beyond simple word overlap and ca"
2021.findings-emnlp.99,2020.emnlp-main.99,0,0.0491976,"Missing"
2021.findings-emnlp.99,2020.emnlp-main.86,0,0.0624882,"Missing"
2021.findings-emnlp.99,2020.emnlp-main.550,0,0.181222,"ant ARG0 ARG0 have-03 ARG1 have-03 wall mod cell (b) Semantic Graph Construction (a) Overall Framework Figure 2: Overall architecture of CGR. In the reasoning chain generator, the black dash lines indicate that we discard the edges between question and answer nodes. The pink arrows indicate the generated reasoning chains. t=1 ??11 t=2 ℯ11 ℯ12 ?? ??21 ⊕ ??11 ??22 t=3 ℯ21 ℯ22 ℯ23 ℯ24 ??31 ⊕ ??21 ⊕ ??22 ??32 ℯ31 … ℯ33 … ℯ32 ℯ34 Figure 3: Iterative retrieval process with beam size 2. query is updated in each iteration, conditioned on the information at hand. We adopt a similar dualencoder of DPR (Karpukhin et al., 2020). The retrieval process is accomplished by maximum innerproduct search (MIPS) between the query representation and the evidence representation: evidence facts retrieved in all iterations to form a large evidence pool E, which is used to build the semantic graph as presented in Sec. 2.2. 2.2 Reasoning Chain Generator As shown in Figure 2, our reasoning chain generator is a non-parameterized component that generates reasoning chains using the evidence pool. It first relies on AMR to dynamically construct a semantic graph to show the relationship among the facts in the evidence pool, and then gen"
2021.findings-emnlp.99,2020.findings-emnlp.171,0,0.0196441,"67.15 67.06 AristoRoBERTa + CGR × × × X 66.47 69.20 Table 3: Test accuracy on ARC-Challenge. ♠ are unpublished methods. the reasoning chains as an additional output that reflect the step-by-step reasoning process to infer the answer, making the QA process explainable. When compared to recently published methods, we find that CGR can also surpass methods leveraging on additional KG. It suggests that textual knowledge resource is still under-investigated, where the gap between the query and indirect fact is one of the issues that restricts the retriever performance for multi-hop QA. UnifiedQA (Khashabi et al., 2020) and T5 3B (Raffel et al., 2020) are two extremely large models (with 30x more parameters than other models), which are not fair for comparison. ARC-Challenge: We implement CGR on another task: ARC-Challenge, where the groundtruth reasoning chain is not available. As shown in Table 3, our CGR significantly improves the baseline AristoRoBERTa with 2.73 accuracy score, which demonstrates the effectiveness of CGR in generating and modeling the reasoning chain in a more general manner. Notably, CGR achieves a new state-of-the-art performance in this challenging task in a computationally practicabl"
2021.findings-emnlp.99,W15-4502,0,0.0302241,"et al., 2017; Wang et al., 2018), densevector retriever has received increasing attention as it captures the semantic matching beyond simple word overlap and can be trained along with the reader (Zhu et al., 2021). It has been reported to outperform term-based methods in many open-domain QA tasks (Das et al., 2019; Karpukhin et al., 2020; Min et al., 2021), including those on multi-hop QA (Asai et al., 2020; Xiong et al., 2021). AMR: AMR has been successfully coupled with many natural language processing tasks in explicit reasoning, such as summarization (Liao et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019), and symbolic QA (Kapanipathi et al., 2020). Comparing to named entity (Shao et al., 2020), we use AMR as our graph annotation because it is not specifically designed for a particular domain and can be adapted to a wide range of textual data. 7 for question answering. In International Conference on Learning Representations. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for sembanking. In Proceedings of the"
2021.findings-emnlp.99,C18-1101,0,0.0229828,"implemented with TF-IDF or BM25 (Chen et al., 2017; Wang et al., 2018), densevector retriever has received increasing attention as it captures the semantic matching beyond simple word overlap and can be trained along with the reader (Zhu et al., 2021). It has been reported to outperform term-based methods in many open-domain QA tasks (Das et al., 2019; Karpukhin et al., 2020; Min et al., 2021), including those on multi-hop QA (Asai et al., 2020; Xiong et al., 2021). AMR: AMR has been successfully coupled with many natural language processing tasks in explicit reasoning, such as summarization (Liao et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019), and symbolic QA (Kapanipathi et al., 2020). Comparing to named entity (Shao et al., 2020), we use AMR as our graph annotation because it is not specifically designed for a particular domain and can be adapted to a wide range of textual data. 7 for question answering. In International Conference on Learning Representations. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation fo"
2021.findings-emnlp.99,D18-1260,0,0.353704,"process, raising concerns about the works usually employ a retriever-reader architecquality of the reasoning chain. Prior works adture (Chen et al., 2017), where the retriever redress this issue by training the retriever against the trieves top-ranked evidence facts from a large corground-truth reasoning chain (Yang et al., 2018; pus and the reader conducts reasoning with these Geva et al., 2021). However, such method is less facts. This architecture works well in single-hop effective when the ground-truth reasoning chain is QA, where the answer can be easily inferred with partially provided (Mihaylov et al., 2018; Ferguson only one evidence fact. However, it is hard to reet al., 2020) or not applicable when the groundtrieve all necessary evidence facts to confidently truth chain is unavailable (Clark et al., 2018). On answer a complex question requiring multi-hop reathe other hand, KG maintains a good growing disoning (Shao et al., 2021). As shown in Figure 1, rection for the reasoning chain. But building a KG ∗ The work described in this paper is substantially supusually involves corpus-specific annotations, such ported by a grant from the Research Grant Council of the as document-level hyperlinks or"
2021.findings-emnlp.99,D19-1258,0,0.0308408,"Missing"
2021.findings-emnlp.99,P19-1617,0,0.0201842,"enable training supervised classifier to identify the correct evidence facts (Nie et al., 2019; Tu et al., 2020). It is a good step to control the quality of reasoning 5.3 Case Study on Reasoning chains, but still remains an issue when the groundWe show how our generated reasoning chain can truth reasoning chain is not available (Clark et al., support explainable reasoning in Table 6. Iterative 2018). Other works explore the effectiveness of retrieval can retrieve the first evidence fact as it KG by either automatically constructing the graph 1150 using named entity or semantic role labeling (Qiu et al., 2019; Bosselut et al., 2019; Fang et al., 2020; Chen et al., 2019) or resorting to existing KG (Saxena et al., 2020; Zhang et al., 2020; Yasunaga et al., 2021). Despite the high precision of those KGs, they are known to suffer from sparsity in existing KG (Zhao et al., 2020), where complex reasoning chains are unlikely to be covered by the closedform relations in KG (Lei et al., 2020). Dense-Vector Retriever: In contrast to termbased retriever implemented with TF-IDF or BM25 (Chen et al., 2017; Wang et al., 2018), densevector retriever has received increasing attention as it captures the semantic"
2021.findings-emnlp.99,D19-1410,0,0.0124875,". Moreover, OpenBookQA provides an accompanying open-book with 1326 science facts, which are highly related to the questions in this dataset. Therefore, we performed an extra retrieval in the open-book in the first iteration. Methods Additional KG Output Chains Test Acc. PG AMR-SG DPR MHGRN KF-SIR X × × X × X × × X × 81.8 81.6 80.8 80.6 80.0 AristoRoBERTa + CGR × × × X 77.8 82.4 Table 2: Test accuracy on OpenBookQA. Methods that use additional KG or can output reasoning chains are ticked respectively. Methods Implementation: We use AristoRoBERTa (AllenAI, 2019) as our reader and SentenceBERT (Reimers and Gurevych, 2019) as our retriever. As indicated by Lewis et al. (2020), training the evidence encoder Ee that requires periodic updates of the evidence index is costly and does little improvement to the performance. Therefore, we fix the evidence encoder and cache all evidence representations offline for efficiency purposes. We set the beam size K to 10 and the total iteration step T to 2, resulting in an average size of evidence pool to be 78 and 53 for OpenBookQA and ARCChallenge respectively. We then select facts for the reader with maximum evidence size of 15 and 20 respectively. The number of sampled cha"
2021.findings-emnlp.99,2020.acl-main.412,0,0.0394249,"020). It is a good step to control the quality of reasoning 5.3 Case Study on Reasoning chains, but still remains an issue when the groundWe show how our generated reasoning chain can truth reasoning chain is not available (Clark et al., support explainable reasoning in Table 6. Iterative 2018). Other works explore the effectiveness of retrieval can retrieve the first evidence fact as it KG by either automatically constructing the graph 1150 using named entity or semantic role labeling (Qiu et al., 2019; Bosselut et al., 2019; Fang et al., 2020; Chen et al., 2019) or resorting to existing KG (Saxena et al., 2020; Zhang et al., 2020; Yasunaga et al., 2021). Despite the high precision of those KGs, they are known to suffer from sparsity in existing KG (Zhao et al., 2020), where complex reasoning chains are unlikely to be covered by the closedform relations in KG (Lei et al., 2020). Dense-Vector Retriever: In contrast to termbased retriever implemented with TF-IDF or BM25 (Chen et al., 2017; Wang et al., 2018), densevector retriever has received increasing attention as it captures the semantic matching beyond simple word overlap and can be trained along with the reader (Zhu et al., 2021). It has been re"
2021.findings-emnlp.99,2020.emnlp-main.583,0,0.0182278,"yond simple word overlap and can be trained along with the reader (Zhu et al., 2021). It has been reported to outperform term-based methods in many open-domain QA tasks (Das et al., 2019; Karpukhin et al., 2020; Min et al., 2021), including those on multi-hop QA (Asai et al., 2020; Xiong et al., 2021). AMR: AMR has been successfully coupled with many natural language processing tasks in explicit reasoning, such as summarization (Liao et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019), and symbolic QA (Kapanipathi et al., 2020). Comparing to named entity (Shao et al., 2020), we use AMR as our graph annotation because it is not specifically designed for a particular domain and can be adapted to a wide range of textual data. 7 for question answering. In International Conference on Learning Representations. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178–186, Sofia, Bulgaria. Association for Computation"
2021.findings-emnlp.99,Q19-1002,0,0.0137316,"evector retriever has received increasing attention as it captures the semantic matching beyond simple word overlap and can be trained along with the reader (Zhu et al., 2021). It has been reported to outperform term-based methods in many open-domain QA tasks (Das et al., 2019; Karpukhin et al., 2020; Min et al., 2021), including those on multi-hop QA (Asai et al., 2020; Xiong et al., 2021). AMR: AMR has been successfully coupled with many natural language processing tasks in explicit reasoning, such as summarization (Liao et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019), and symbolic QA (Kapanipathi et al., 2020). Comparing to named entity (Shao et al., 2020), we use AMR as our graph annotation because it is not specifically designed for a particular domain and can be adapted to a wide range of textual data. 7 for question answering. In International Conference on Learning Representations. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and I"
2021.findings-emnlp.99,2020.findings-emnlp.369,0,0.0373166,"Missing"
2021.findings-emnlp.99,2021.findings-acl.90,1,0.675212,"age size of evidence pool to be 78 and 53 for OpenBookQA and ARCChallenge respectively. We then select facts for the reader with maximum evidence size of 15 and 20 respectively. The number of sampled chains N is set to 1. 2 More details and analysis can be found in Appendix A.1 and A.2. Comparison Methods: For fair comparison, we compare CGR with recently published methods that use similar power of pretrained models, including five textual knowledge based methods: AristoRoBERTa (AllenAI, 2019), KF-SIR (Banerjee and Baral, 2020), FreeLB (Zhu et al., 2020), DPR (Karpukhin et al., 2020), AMR-SG (Xu et al., 2021) and another two methods leveraging on an additional knowledge graph (Speer et al., 2017): PG (Wang et al., 2020), and MHGRN (Feng et al., 2020). 4 4.1 Results QA Performance OpenBookQA: Table 2 shows the comparison results of OpenBookQA. Our CGR significantly improves over the baseline AristoRoBERTa with 4.6 accuracy score. Meanwhile, CGR can also provide Additional KG Output Chains Test Acc. AMR-SG FreeLB arcRoberta ♠ xlnet+Roberta ♠ × × × × × × × × 68.94 67.75 67.15 67.06 AristoRoBERTa + CGR × × × X 66.47 69.20 Table 3: Test accuracy on ARC-Challenge. ♠ are unpublished methods. the reasonin"
2021.findings-emnlp.99,D19-1260,0,0.017865,"evidence facts that fail to be retrieved by an iterative retriever and form a reasoning chain with AMR concepts as anchors. As the incorrect answers are not likely to form reasoning chains, the evidence facts on the reasoning chains are highly discriminative and can effectively support the reader to select the correct answer. More cases can be found in Appendix A.3. 6 Related Work Multi-hop QA: Multi-hop QA is a challenging task as it requires gathering multiple evidence facts, especially indirect facts, to form a reasoning chain. Early attempts mostly rely on iterative retrieval. For example,Yadav et al. (2019) extract evidence facts in consideration of their relevance, overlap and coverage. Banerjee et al. (2019); Yadav et al. (2020) reformulate their queries with unused words in the last iteration. However, these methods may retrieve irrelevant facts as the query grow biased to unimportant words. As some recent QA datasets annotate the ground-truth reasoning chain (Yang et al., 2018; Mihaylov et al., 2018), they enable training supervised classifier to identify the correct evidence facts (Nie et al., 2019; Tu et al., 2020). It is a good step to control the quality of reasoning 5.3 Case Study on Re"
2021.findings-emnlp.99,2020.acl-main.414,0,0.0164117,"the incorrect answers are not likely to form reasoning chains, the evidence facts on the reasoning chains are highly discriminative and can effectively support the reader to select the correct answer. More cases can be found in Appendix A.3. 6 Related Work Multi-hop QA: Multi-hop QA is a challenging task as it requires gathering multiple evidence facts, especially indirect facts, to form a reasoning chain. Early attempts mostly rely on iterative retrieval. For example,Yadav et al. (2019) extract evidence facts in consideration of their relevance, overlap and coverage. Banerjee et al. (2019); Yadav et al. (2020) reformulate their queries with unused words in the last iteration. However, these methods may retrieve irrelevant facts as the query grow biased to unimportant words. As some recent QA datasets annotate the ground-truth reasoning chain (Yang et al., 2018; Mihaylov et al., 2018), they enable training supervised classifier to identify the correct evidence facts (Nie et al., 2019; Tu et al., 2020). It is a good step to control the quality of reasoning 5.3 Case Study on Reasoning chains, but still remains an issue when the groundWe show how our generated reasoning chain can truth reasoning chain"
2021.findings-emnlp.99,D18-1259,0,0.082479,", the retriever would edge has gained increasing attention in recent years inevitably retrieve partially related facts. Such as it mimics human behavior to first filter out relnoise is continuously amplified during the iteraevant knowledge from massive information. Prior tive retrieval process, raising concerns about the works usually employ a retriever-reader architecquality of the reasoning chain. Prior works adture (Chen et al., 2017), where the retriever redress this issue by training the retriever against the trieves top-ranked evidence facts from a large corground-truth reasoning chain (Yang et al., 2018; pus and the reader conducts reasoning with these Geva et al., 2021). However, such method is less facts. This architecture works well in single-hop effective when the ground-truth reasoning chain is QA, where the answer can be easily inferred with partially provided (Mihaylov et al., 2018; Ferguson only one evidence fact. However, it is hard to reet al., 2020) or not applicable when the groundtrieve all necessary evidence facts to confidently truth chain is unavailable (Clark et al., 2018). On answer a complex question requiring multi-hop reathe other hand, KG maintains a good growing disoni"
2021.findings-emnlp.99,2021.naacl-main.45,0,0.156995,"p QA with direct (D) and indirect (I) facts to form a reasoning chain. multi-hop QA usually involves a sequential nature of evidence facts to form a reasoning chain, including (1) direct facts sharing a semantic relationship with the question or the answer; (2) indirect facts sharing little lexical or semantic overlap but serving an irreplaceable role to infer the answer. A common practice for forming such reasoning chains for multi-hop QA is to expand the chain with iterative retrieval (Xiong et al., 2021) or sample from an existing or pre-constructed Knowledge Graph (KG) (Asai et al., 2020; Yasunaga et al., 2021). On one hand, iterative retrieval allows the retriever to capture the evidence-evidence interac1 Introduction tions by reformulating the query with newly reQuestion Answering (QA) with external knowltrieved evidence fact. However, the retriever would edge has gained increasing attention in recent years inevitably retrieve partially related facts. Such as it mimics human behavior to first filter out relnoise is continuously amplified during the iteraevant knowledge from massive information. Prior tive retrieval process, raising concerns about the works usually employ a retriever-reader archite"
2021.wnut-1.20,C18-1279,1,0.903,"Missing"
2021.wnut-1.20,N19-1423,0,0.484477,"ain knowledge in the original datasets, and and answer pair, including Siamese Structure (Tay even a waste of valuable labeled data. et al., 2018; Chen et al., 2020), Attention-based Structure (Chen et al., 2017; Shen et al., 2018; Some early works (Min et al., 2017; Deng et al., Deng et al., 2021), and Compare-Aggregate Struc- 2018) leverage transfer learning (TL) based apture (Yoon et al., 2019). Recently, models with proaches to transfer knowledge from large-scale contextualized representations, e.g., ELMo (Pe- QA datasets, e.g., SQuAD (Rajpurkar et al., 2016), ters et al., 2018) and BERT (Devlin et al., 2019), for improving resource-poor answer selection tasks. contribute to major improvement on answer selec- With similar motivations, Yoon et al. (2019) comtion (Yoon et al., 2019; Garg et al., 2020). bine ELMo with transfer learning from a largescale relevant-sentence-selection dataset to en∗ The work described in this paper is substantially suphance the performance of the compare-aggregate ported by a grant from the Asian Institute of Supply Chains and Logistics, the Chinese University of Hong Kong. model. Garg et al. (2020) adopt BERT-based mod175 Proceedings of the 2021 EMNLP Workshop W-NUT: Th"
2021.wnut-1.20,D19-1604,0,0.0127819,"mputational Linguistics els with a Transfer-and-Adapt (TANDA) approach by using a large and high-quality QA dataset. However, such TL-based methods heavily rely on the existence of large-scale high-quality datasets with similar problem settings, which is also unrealistic in real-world applications. Besides, compared with utilizing cross-domain knowledge via TL, it would be more efficient to take advantage of the in-domain knowledge from the original dataset. Another way to handle the data scarceness and label imbalance issues is to employ hard negative sampling strategies (Zhang et al., 2017; Kumar et al., 2019) or curriculum training approaches (MacAvaney et al., 2020) for taking advantages of those randomly sampled negative answers. The essential motivation behind these methods is that the hard negative samples or the “difficult"" samples play a more important role in training a better ranking model. However, those “less"" important samples are likely to be overlooked in such cases, leading to a potential waste of data. In order to maximize the utility of the existing labeled data, we aim at investigating data augmentation strategy to construct weak supervision signals for enhancing answer selection"
2021.wnut-1.20,C18-1181,0,0.0158057,"ental results on three benchmark datasets including TREC-QA, WikiQA, and ANTIQUE show the effectiveness and the applicability of our method, which significantly improves the original ranking models across all datasets. 2 2.1 Method Preliminaries Let an ad-hoc neural ranking model be denoted as Rθ (q, a), which measures the relevance degree of a given question answer pair (q, a) parameterized by the model parameters θ. Given a set of training samples, there are three mainstream approaches to optimize the ranking performance, namely, pointwise training, pairwise training, and listwise training (Lai et al., 2018; Zhang et al., 2020). Here we focus on pointwise and pairwise training, since listwise training is less practical in applications when there is a large set of candidate answers. The pointwise training aims to optimize the predicted probability distribution of each QA pair to be consistent with the ground-truth label by minimizing the cross-entropy loss function: In this work, we propose an effective data augmentation strategy, namely Bilateral Generation (BiG), with a contrastive training objective for improving the performance of ranking question anpi = softmax(Rθ (qi , ai )), (1) swer pairs"
2021.wnut-1.20,2020.acl-main.703,0,0.0306965,"higher than the irrelevant answer a− , by imAlthough such pseudo-positive QA pairs may not posing a margin M between their relevance scores be perfectly matched, e.g., (q∗3 , a− with the given question q. 3 ), they are supposed to be more interrelated than the original neg2.2 BiG Data Augmentation ative QA pairs, which can be served as pairwise samples for contrastive estimation. For instance, To maximize the utility of the valuable labeled data (q∗3 , a− as well as alleviate the label imbalance issue, pre3 ) can be used to establish a contrastive sam176 trained language models, such as BART (Lewis et al., 2020), preserve promising advantages of synthesizing high-quality pseudo QA pairs through conditional generation, which can serve as weak supervision signals for contrastive learning with original data. Due the nature of question answering, there are two intuitive ways to generate augmentations, i.e., question generation and answer generation, namely Bilateral Generation (BiG). With the pre-trained conditional generation model, a certain number of training data is required to fine-tune a Question Generator (QG) and an Answer Generator (AG). We adopt the positive samples, i.e., the relevant (q, a+ )"
2021.wnut-1.20,P17-2081,0,0.017875,"answers for selecting the relevant or correct evant answers (Zhang et al., 2017; Kumar et al., answers to the given question. Many efforts have 2019). Therefore, the data scarceness and label been made on developing various neural models to imbalance issues lead to the underutilization of the measure the relevance degree between the question in-domain knowledge in the original datasets, and and answer pair, including Siamese Structure (Tay even a waste of valuable labeled data. et al., 2018; Chen et al., 2020), Attention-based Structure (Chen et al., 2017; Shen et al., 2018; Some early works (Min et al., 2017; Deng et al., Deng et al., 2021), and Compare-Aggregate Struc- 2018) leverage transfer learning (TL) based apture (Yoon et al., 2019). Recently, models with proaches to transfer knowledge from large-scale contextualized representations, e.g., ELMo (Pe- QA datasets, e.g., SQuAD (Rajpurkar et al., 2016), ters et al., 2018) and BERT (Devlin et al., 2019), for improving resource-poor answer selection tasks. contribute to major improvement on answer selec- With similar motivations, Yoon et al. (2019) comtion (Yoon et al., 2019; Garg et al., 2020). bine ELMo with transfer learning from a largescale"
2021.wnut-1.20,N18-1202,0,0.0535617,"Missing"
2021.wnut-1.20,D07-1003,0,0.0447747,"enhance the pairwise training in Eq.7: + − pr + − Lctrst set (q, A , A ) = Lset (q, A , A ) X  pr  (8) 1 + L (q, a∗ , a− ) + Lpr (q∗ , q, a− ) , − |A |− − a ∈A where the second term implements the contrastive training between the synthesized pseudo-positive QA pairs, (q, a∗ ) and (q∗ , a− ), and the original negative QA pairs, (q, a− ). 3 Experiment 3.1 3.1.1 Experimental Setup Datasets and Evaluation Metrics We evaluate our method on three widely-used benchmark datasets for answer selection: TRECQA, WikiQA, and ANTIQUE. The statistics of these datasets are described in Table 2. • TREC-QA (Wang et al., 2007), collected from TREC QA track 8-13 data, is a widely-adopted benchmark for factoid question answering. • WikiQA (Yang et al., 2015) is an open-domain factoid answer selection benchmark, in which the answers are collected from the Wikipedia. Following previous studies (Garg et al., 2020), we adopt the RAW version of training data and evaluate on the cleaned test data for both TREC-QA and WikiQA. The mean average precision (MAP) and mean reciprocal rank (MRR) are adopted as the evaluation metrics. • As mentioned in Section 2.1, conventional supervised ranking problem can only optimize the ranki"
2021.wnut-1.20,D15-1237,0,0.0800156,"Missing"
C02-1073,J93-1004,0,\N,Missing
C02-1073,A00-2024,0,\N,Missing
C02-1073,saggion-etal-2002-developing,1,\N,Missing
C02-1073,A00-2035,0,\N,Missing
C02-1073,W00-0401,1,\N,Missing
C02-1073,W00-0408,0,\N,Missing
C02-1073,E99-1011,0,\N,Missing
C02-1073,W97-0704,0,\N,Missing
C02-1073,W00-0403,1,\N,Missing
C02-1073,grover-etal-2000-lt,0,\N,Missing
C08-1134,H05-1091,0,0.0403212,"rds consisting of 4 words to the left and right of the target entity. Part-of-Speech: Part-of-speech tags are obtained using the Stanford POS Tagger3 , which used rich knowledge sources and features in a log-linear model. POS tags with a window size of 4 around the target entity are used. Morphological features: Such as whether the entity is capitalized or contains digits or punctuation, whether the entity ends in some suffixes such as -eer and -ician, etc. Syntactic features: Syntactic information can lead to significant improvements in extraction accuracy (e.g., Culotta and Sorensen (2004), Bunescu and Mooney (2005)). The POS-tagged corpus is submitted to the Stanford Lexicalized Dependency Parser4 which generates a dependency parse tree for each sentence and assigns word positions to each word. This parser can also output grammatical relations (typed dependency). The grammatical relations are of the form relation(reli , wi , wj ), where reli is one of the fixed set of relations assigned by the parser, and wi and wj are two words. The dependency paths, which contain the relevant terms describing the relations between the entity pairs, can be easily extracted. We design a set of first-order formulae that"
C08-1134,P04-1054,0,0.0197171,"ontextual features: Bag-of-words consisting of 4 words to the left and right of the target entity. Part-of-Speech: Part-of-speech tags are obtained using the Stanford POS Tagger3 , which used rich knowledge sources and features in a log-linear model. POS tags with a window size of 4 around the target entity are used. Morphological features: Such as whether the entity is capitalized or contains digits or punctuation, whether the entity ends in some suffixes such as -eer and -ician, etc. Syntactic features: Syntactic information can lead to significant improvements in extraction accuracy (e.g., Culotta and Sorensen (2004), Bunescu and Mooney (2005)). The POS-tagged corpus is submitted to the Stanford Lexicalized Dependency Parser4 which generates a dependency parse tree for each sentence and assigns word positions to each word. This parser can also output grammatical relations (typed dependency). The grammatical relations are of the form relation(reli , wi , wj ), where reli is one of the fixed set of relations assigned by the parser, and wi and wj are two words. The dependency paths, which contain the relevant terms describing the relations between the entity pairs, can be easily extracted. We design a set of"
C08-1134,N06-1038,0,0.0134963,"he collaborative environment) of this online resource. Currently Wikipedia has approximately 9.25 million articles in more than 200 languages. We investigate the task of discovering semantic relations between entity pairs from Wikipedia’s English encyclopedic articles. The basic entry in Wikipedia is an article, which mainly defines and describes an entity (also known as principal entity) or an event, and consists of a hypertext document with hyperlinks to other pages within or outside Wikipedia. This document mentions some other entities as secondary entities related to the principal entity (Culotta et al., 2006). All the entities are hyper-linked within the text, and the topic of an article usually defines the principal entity. Moreover, Wikipedia has the category hierarchy structure which is used to classify articles according to their content. All these characteristics make Wikipedia an appropriate resource for the task of relation extraction. In this paper, we predict only relations between the principal entity and each mentioned secondary entity. An illustrative example of Wikipedia article is shown in Figure 2, where the principal entity Albert Einstein is boxed and in italic font, and sec1 http"
C08-1134,A00-2030,0,0.0757123,"Missing"
C08-1134,N07-2032,0,0.0251089,"Missing"
C08-1134,P07-1076,0,0.023688,"for relations. This approach mainly relies on syntactic structures to extract relations. Syntactic structures are important for relation extraction, but insufficient to extract relations accurately. The obtained F-measure was only 37.76, which shows that there is a large room for improving. To the best of our knowledge, our approach is the first attempt at using MLNs for relation extraction from Wikipedia which achieves state-of-the-art performance. We mention some other related work. Bunescu and Mooney (2007) presented an approach to extract relations from the Web using minimal supervision. Rosenfeld and Feldman (2007) presented a method for improving semi-supervised relation extraction from the Web using corpus statistics on entities. Our work is different from these research work. We investigate supervised relation extraction from Wikipedia based on probabilistic and logic integrated graphical models. 10 Conclusion We summarize the contribution of this paper. First, we propose a new integrated model based on MLNs, which provide a natural and systematic way by modeling entity relations in a coherent undirected graph collectively and integrating implicit relation extraction easily, to extract relations in e"
C08-1134,P07-2040,0,0.0438791,"Missing"
C08-1134,P05-1053,0,0.0498906,"Missing"
C08-1134,P07-1073,0,\N,Missing
C10-2160,N06-1038,0,0.00930847,"algorithms. When probability distributions are very complex or even unknown, the GS algorithm cannot be applied. ICA iteratively infers the states of variables given the current predicted labeling assignments of neighboring variables as observed information. Prediction errors on labels may then propagate during the iterations and the algorithm will then have difficulties to generalize correctly. We mention some recently published results related to Wikipedia datasets (Note that it is difficult to compare with them strictly, since these results can be based on different experimental settings). Culotta et al. (2006) used a data set with a 70/30 split for training/testing and Nguyen et al. (2007) used 5930 articles for training and 45 for testing, to perform relatione extraction from Wikipedia. And the obtained F-measures were 67.91 and 37.76, respectively. Yu et al. (2009) proposed an integrated approach incorporating probabilistic graphical models with first-order logic to perform relation extraction from encyclopedia articles, with a F-measure of 65.66. All these systems assume that the golden-standard entities are already known and they only perform relation extraction. However, such assumption is not"
C10-2160,D09-1047,0,0.0220082,"inear programming formulation to seek an optimal global assignment to these variables. Zhang and Clark (2008) employed the generalized perceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding. Toutanova et al. (2008) presented a model capturing the linguistic intuition that a semantic argument frame is a joint structure, with strong dependencies among the arguments. Finkel and Manning (2009) proposed a discriminative featurebased constituency parser for joint named entity recognition and parsing. And Dahlmeier et al. (2009) proposed a joint model for word sense disambiguation of prepositions and semantic role labeling of prepositional phrases. However, most of the mentioned approaches are task-specific (e.g., (Toutanova et al., 2008) for semantic role labeling, and (Finkel and Manning, 2009) for parsing and NER), and they can hardly be applicable to other NLP tasks. Since we capture rich and complex dependencies between subtasks via potential functions in probabilistic graphical models, our approach is general and can be easily applied to a variety of NLP and IE tasks. 8 Conclusion and Future Work In this paper,"
C10-2160,N09-1037,0,0.0280928,"d Yih (2007) considered multiple constraints between variables from tasks such as named entities and relations, and developed a integer linear programming formulation to seek an optimal global assignment to these variables. Zhang and Clark (2008) employed the generalized perceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding. Toutanova et al. (2008) presented a model capturing the linguistic intuition that a semantic argument frame is a joint structure, with strong dependencies among the arguments. Finkel and Manning (2009) proposed a discriminative featurebased constituency parser for joint named entity recognition and parsing. And Dahlmeier et al. (2009) proposed a joint model for word sense disambiguation of prepositions and semantic role labeling of prepositional phrases. However, most of the mentioned approaches are task-specific (e.g., (Toutanova et al., 2008) for semantic role labeling, and (Finkel and Manning, 2009) for parsing and NER), and they can hardly be applicable to other NLP tasks. Since we capture rich and complex dependencies between subtasks via potential functions in probabilistic graphical"
C10-2160,W06-1673,0,0.0209245,"Missing"
C10-2160,P07-1120,0,0.0226221,"independent stages. Such kind of design is widely adopted in NLP. ∗ The work described in this paper is substantially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project No: CUHK4128/07) and the Direct Grant of the Faculty of Engineering, CUHK (Project Codes: 2050442 and 2050476). This work is also affiliated with the MicrosoftCUHK Joint Laboratory for Human-centric Computing and Interface Technologies. Usually, one can pass N-best lists between different stages in pipeline architectures, and this often gives useful improvements (Hollingshead and Roark, 2007). However, effectively making use of N-best lists often requires lots of engineering and human effort (Toutanova, 2005). On the other hand, one can record the complete distribution at each stage in a pipeline, to compute or approximate the complete distribution at the next stage. Doing this is generally infeasible, and this solution is rarely adopted in practice. One promising way to tackle the problem of error propagation is to explore joint learning which integrates evidences from multiple sources and captures mutual benefits across multiple components of a pipeline for all relevant subtasks"
C10-2160,P05-1073,0,0.0157394,"aking use of N-best lists often requires lots of engineering and human effort (Toutanova, 2005). On the other hand, one can record the complete distribution at each stage in a pipeline, to compute or approximate the complete distribution at the next stage. Doing this is generally infeasible, and this solution is rarely adopted in practice. One promising way to tackle the problem of error propagation is to explore joint learning which integrates evidences from multiple sources and captures mutual benefits across multiple components of a pipeline for all relevant subtasks simultaneously (e.g., (Toutanova et al., 2005), (Poon and Domingos, 2007), (Singh et al., 2009)). Joint learning aims to handle multiple hypotheses and uncertainty information and predict many variables at once such that subtasks can aid each other to boost the performance, and thus usually leads to complex model structure. However, it is typically intractable to run a joint model and they sometimes can hurt the performance, since they 1399 Coling 2010: Poster Volume, pages 1399–1407, Beijing, August 2010 increase the number of paths to propagate errors. Due to these difficulties, research on building joint approaches is still in the begi"
C10-2160,I08-1044,1,0.759444,"aphical Model Approach∗ Xiaofeng Y U Wai L AM Information Systems Laboratory Department of Systems Engineering & Engineering Management The Chinese University of Hong Kong {xfyu,wlam}@se.cuhk.edu.hk Abstract The most common and simplest approach to performing compound NLP tasks is the 1-best pipeline architecture, which only takes the 1-best hypothesis of each stage and pass it to the next one. Although it is comparatively easy to build and efficient to run, this pipeline approach is highly ineffective and suffers from serious problems such as error propagation (Finkel et al., 2006; Yu, 2007; Yu et al., 2008). It is not surprising that, the end-to-end performance will be restricted and upper-bounded. In this paper, we investigate the problem of entity identification and relation extraction from encyclopedia articles, and we propose a joint discriminative probabilistic model with arbitrary graphical structure to optimize all relevant subtasks simultaneously. This modeling offers a natural formalism for exploiting rich dependencies and interactions between relevant subtasks to capture mutual benefits, as well as a great flexibility to incorporate a large collection of arbitrary, overlapping and noni"
C10-2160,N07-2032,0,0.0131688,"S algorithm cannot be applied. ICA iteratively infers the states of variables given the current predicted labeling assignments of neighboring variables as observed information. Prediction errors on labels may then propagate during the iterations and the algorithm will then have difficulties to generalize correctly. We mention some recently published results related to Wikipedia datasets (Note that it is difficult to compare with them strictly, since these results can be based on different experimental settings). Culotta et al. (2006) used a data set with a 70/30 split for training/testing and Nguyen et al. (2007) used 5930 articles for training and 45 for testing, to perform relatione extraction from Wikipedia. And the obtained F-measures were 67.91 and 37.76, respectively. Yu et al. (2009) proposed an integrated approach incorporating probabilistic graphical models with first-order logic to perform relation extraction from encyclopedia articles, with a F-measure of 65.66. All these systems assume that the golden-standard entities are already known and they only perform relation extraction. However, such assumption is not valid in practice. Notably, our approach deals with a fairly more challenging pr"
C10-2160,N07-2050,1,0.81284,"t via A Graphical Model Approach∗ Xiaofeng Y U Wai L AM Information Systems Laboratory Department of Systems Engineering & Engineering Management The Chinese University of Hong Kong {xfyu,wlam}@se.cuhk.edu.hk Abstract The most common and simplest approach to performing compound NLP tasks is the 1-best pipeline architecture, which only takes the 1-best hypothesis of each stage and pass it to the next one. Although it is comparatively easy to build and efficient to run, this pipeline approach is highly ineffective and suffers from serious problems such as error propagation (Finkel et al., 2006; Yu, 2007; Yu et al., 2008). It is not surprising that, the end-to-end performance will be restricted and upper-bounded. In this paper, we investigate the problem of entity identification and relation extraction from encyclopedia articles, and we propose a joint discriminative probabilistic model with arbitrary graphical structure to optimize all relevant subtasks simultaneously. This modeling offers a natural formalism for exploiting rich dependencies and interactions between relevant subtasks to capture mutual benefits, as well as a great flexibility to incorporate a large collection of arbitrary, ov"
C10-2160,P08-1101,0,0.0300199,"tably, our approach deals with a fairly more challenging problem involving both entity identification and relation extraction, and it is more applicable to real-world IE tasks. 7 Related Work A number of previous researchers have taken steps toward joint models in NLP and information extraction, and we mention some recently proposed, closely related approaches here. Roth and Yih (2007) considered multiple constraints between variables from tasks such as named entities and relations, and developed a integer linear programming formulation to seek an optimal global assignment to these variables. Zhang and Clark (2008) employed the generalized perceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding. Toutanova et al. (2008) presented a model capturing the linguistic intuition that a semantic argument frame is a joint structure, with strong dependencies among the arguments. Finkel and Manning (2009) proposed a discriminative featurebased constituency parser for joint named entity recognition and parsing. And Dahlmeier et al. (2009) proposed a joint model for word sense disambiguation of prepositions and semantic rol"
C10-2160,J08-2002,0,\N,Missing
C10-2161,J95-4004,0,0.472675,"Missing"
C10-2161,W02-1001,0,0.27932,"Missing"
C10-2161,H05-1099,0,0.022351,"tly, our model can be easily applied to other sequence labeling tasks. 1 Introduction The problem of annotating or labeling observation sequences arises in many applications across a variety of scientific disciplines, most prominently in natural language processing, speech recognition, information extraction, and bioinformatics. Recently, the predominant formalism for modeling and predicting label sequences has been based on discriminative graphical models and variants. Among such models, maximum margin Markov networks (M 3 N ) and variants ( Taskar et al. (2003); Taskar (2004); Taskar et al. (2005)) have recently gained popularity in the machine learning community. While the M 3 N framework makes extensive use of many theoretical results ∗ The work described in this paper is substantially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project No: CUHK4128/07) and the Direct Grant of the Faculty of Engineering, CUHK (Project Codes: 2050442 and 2050476). This work is also affiliated with the MicrosoftCUHK Joint Laboratory for Human-centric Computing and Interface Technologies. Traditionally, M 3 N can be trained using the structu"
C10-2161,W00-0730,0,0.126884,"Missing"
C10-2161,N01-1025,0,0.132241,"Missing"
C10-2161,H05-1124,0,0.0178925,"tly, our model can be easily applied to other sequence labeling tasks. 1 Introduction The problem of annotating or labeling observation sequences arises in many applications across a variety of scientific disciplines, most prominently in natural language processing, speech recognition, information extraction, and bioinformatics. Recently, the predominant formalism for modeling and predicting label sequences has been based on discriminative graphical models and variants. Among such models, maximum margin Markov networks (M 3 N ) and variants ( Taskar et al. (2003); Taskar (2004); Taskar et al. (2005)) have recently gained popularity in the machine learning community. While the M 3 N framework makes extensive use of many theoretical results ∗ The work described in this paper is substantially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project No: CUHK4128/07) and the Direct Grant of the Faculty of Engineering, CUHK (Project Codes: 2050442 and 2050476). This work is also affiliated with the MicrosoftCUHK Joint Laboratory for Human-centric Computing and Interface Technologies. Traditionally, M 3 N can be trained using the structu"
C10-2161,W00-0726,0,0.0748289,"m dependencies of parameters. v can be computed by the simple iterative update: vt+1 = λvt − ηt · (Gt + λHt vt ), (18) where the factor 0 ≤ λ ≤ 1 governs the time scale over which long-term dependencies are taken into account, and Ht vt can be calculated efficiently alongside the gradient by forward-mode algorithmic differentiation via Equation 14. This Hessianvector product is computed implicitly and it is the key to SMD’s efficiency. The pseudo-code for the SGD algorithm is shown in Figure 1. 4 4.1 Experiments: A Case Study of NP Chunking Data Our data comes from the CoNLL 2000 shared task (Sang and Buchholz, 2000). The dataset is divided into a standard training set of 8,936 sentences and a testing set of 2,012 sentences. This data consists of the same partitions of the Wall Street Journal corpus (WSJ) as the widely used data for NP chunking: sections 15-18 as training data (211,727 tokens) and section 20 as test data (47,377 tokens). And the annotation of the data has been derived from the WSJ corpus. Table 1: Input feature template qk (x, t) for NP chunking. In this table wt is the token (word) at position t, pt is the POS tag at position t, w ranges over all words in the training data, and p ranges"
C10-2161,N03-1028,0,0.233286,"Missing"
C10-2161,W04-3201,0,0.0810083,"Missing"
C10-2161,C08-1106,0,\N,Missing
C12-1080,W12-2207,0,0.0438477,"Missing"
C12-1080,W08-0909,0,0.170812,"Missing"
C12-1080,C10-1062,0,0.0256717,"Missing"
C12-1080,C02-1142,0,0.112802,"Missing"
C12-1080,D08-1020,0,0.255386,"Missing"
C12-1080,P05-1065,0,0.357388,"Missing"
C12-1080,J10-2002,0,0.029014,"Missing"
C12-1080,W11-1415,0,0.0449463,"Missing"
C12-1080,J01-1001,0,0.125674,"Missing"
C12-1080,J91-1002,0,\N,Missing
C18-1186,D14-1162,0,0.0868794,"ect modeling. The learned hidden representation from RBM contains aspect information and can be obtained efficiently. Similar to the aspect-based similarity in the first feature, we can also compute a similarity score based on the RBM as the second feature. • Features based on Word Embedding Word embedding techniques map each term to a distributed representation capturing semantics of text. Three features are designed according to different types of word embedding. For the first two features, we adopt a set of pre-trained word embedding, known as Global Vectors for word representation (GloVe)(Pennington et al., 2014). Given a question q = (tq1 , tq2 , ...), each term tqi in q can be represented as a word embedding denoted as v(tqi ). Then the question q can be represented as the average of each term representation. The review sentence is also represented in the same way. Then the first feature is obtained by computing the inner product of these two representations reflecting the semantic similarity between the question q and the review r. Another representation of question is to use the largest value in each dimension among all the term vectors as the value of the corresponding dimension in the question r"
C18-1186,P08-1082,0,0.0698613,"ct questions is that they can only make use of the typically small QA collection of the corresponding product, and similar questions associated with other products are not helpful due to different specifications. QA models (Shen et al., 2017; Yang et al., 2016) try to capture the relation between questions and answers. QA-LSTM (Tan et al., 2016) is developed for question answer matching via bidirectional LSTM with max pooling. The QA model proposed in (Wang and Jiang, 2016) also utilizes LSTM. Learning-torank model has also been adopted in some method for question-answer matching, such as in (Surdeanu et al., 2008). Generally existing QA models cannot handle the heterogeneous nature of answer collections and review collections in the problem setting investigated in this paper. McAuley and Yang (2016) propose to exploit product reviews for answer prediction via a Mixture of Expert (MoE) model. This MoE model makes use of a review relevance function and an answer prediction function. One restriction of this model is that it can only be used for answer selection given a candidate answer set. Although the QA collections and review collections are involved in the learning procedures, one assumption is that a"
C18-1186,P16-1044,0,0.423581,"ting question-answer pair. Community Question Answering (CQA) approaches can be employed, but they can only make use of the QA collection of the corresponding product, which typically contains just a small amount of QA pairs. Furthermore, similar questions associated with other products are not helpful due to different product specifications. Another limitation of CQA methods is that they cannot make use of the review collection which is another important information source for generating responses. Another possible approach is to employ a question answering learning approach such as QA-LSTM (Tan et al., 2016). But these QA models typically cannot effectively exploit reviews due to the heterogeneous nature of answer collections and review collections. Recently a chatbot for E-commerce sites known as SuperAgent has been developed (Cui et al., 2017). SuperAgent considers both QA collections and reviews when answering questions. However, it employs separated modules for each of the information sources without mutual coordination. Moreover, it requires external knowledge and a large volume of annotated data. Some models based on Mixture of Expert (MoE) (McAuley and Yang, 2016; Wan and McAuley, 2016) ar"
C18-1186,N16-1170,0,0.0152718,"te the similarities evaluation via the recurrent neural network. One shortcoming of these CQA methods for tackling the task of E-commerce product questions is that they can only make use of the typically small QA collection of the corresponding product, and similar questions associated with other products are not helpful due to different specifications. QA models (Shen et al., 2017; Yang et al., 2016) try to capture the relation between questions and answers. QA-LSTM (Tan et al., 2016) is developed for question answer matching via bidirectional LSTM with max pooling. The QA model proposed in (Wang and Jiang, 2016) also utilizes LSTM. Learning-torank model has also been adopted in some method for question-answer matching, such as in (Surdeanu et al., 2008). Generally existing QA models cannot handle the heterogeneous nature of answer collections and review collections in the problem setting investigated in this paper. McAuley and Yang (2016) propose to exploit product reviews for answer prediction via a Mixture of Expert (MoE) model. This MoE model makes use of a review relevance function and an answer prediction function. One restriction of this model is that it can only be used for answer selection gi"
C18-1186,P15-1060,0,0.0506236,"Missing"
C18-1186,D12-1036,0,0.0170043,"ing the response to the given question. Learning for response review ranking in this problem setting is challenging since no labeled review sample is available. The existing QA pairs in the corresponding QA collection contains knowledge for response ranking. These QA pairs and the corresponding retrieved review lists can be exploited as distant supervision for question-based response ranking. The relationship between such available QA pairs and reviews can be modeled for facilitating the learning of question-based review ranking. 2 Related Work Product question answering is an emerging topic. Yu et al. (2012) extract hierarchical structure from the product review collection, and then select sentences from reviews based on the structure. Their model only focuses on review collections. Community Question Answering (CQA) (Zhou et al., 2011) approaches can be adopted to tackle this problem. For example, Zhou et al. (2015) propose to learn continuous word embeddings based on the QA corpus incorporating metadata such as category information, and the learned word embedding can be applied for question retrieval in CQA platform. Chen et al. (2018) encode the question text together with users’ social intera"
C18-1186,D10-1006,0,0.0485605,"). The likelihood is smoothed by Jelinek-Mercer method: pλ (w|r) = (1 − λ)p(w|r) + λp(w|Cr ) (8) where p(w|Cr ) is the probability of the term w in the review collection. Similarly, we can formulate the second feature about the likelihood of the reviews given the question q. • Features based on Aspect-based Similarity. For text data from E-commerce platforms, aspects are an important concept which captures features or attributes about products. Two features related to aspects are designed. The first feature is derived from the aspect discovery model based on Latent Dirichlet Allocation (LDA) (Zhao et al., 2010). Trained with the text collection containing reviews, questions and answers, this model can transform each given text into an aspect representation. Let the representations for a question q and a review r be denoted as v(q) and v(r). We employ the cosine similarity score of these two representations to model the aspect-based similarity between the question and the review. The second feature is based on the Restricted Boltzmann Machine (RBM) (Wang et al., 2015) trained as an aspect modeling. The learned hidden representation from RBM contains aspect information and can be obtained efficiently."
C18-1186,P11-1066,0,0.0372477,"ledge for response ranking. These QA pairs and the corresponding retrieved review lists can be exploited as distant supervision for question-based response ranking. The relationship between such available QA pairs and reviews can be modeled for facilitating the learning of question-based review ranking. 2 Related Work Product question answering is an emerging topic. Yu et al. (2012) extract hierarchical structure from the product review collection, and then select sentences from reviews based on the structure. Their model only focuses on review collections. Community Question Answering (CQA) (Zhou et al., 2011) approaches can be adopted to tackle this problem. For example, Zhou et al. (2015) propose to learn continuous word embeddings based on the QA corpus incorporating metadata such as category information, and the learned word embedding can be applied for question retrieval in CQA platform. Chen et al. (2018) encode the question text together with users’ social interactions for handling the lexical gap among questions. A random walk based learning method is designed to facilitate the similarities evaluation via the recurrent neural network. One shortcoming of these CQA methods for tackling the ta"
C18-1186,P15-1025,0,0.0197138,"lists can be exploited as distant supervision for question-based response ranking. The relationship between such available QA pairs and reviews can be modeled for facilitating the learning of question-based review ranking. 2 Related Work Product question answering is an emerging topic. Yu et al. (2012) extract hierarchical structure from the product review collection, and then select sentences from reviews based on the structure. Their model only focuses on review collections. Community Question Answering (CQA) (Zhou et al., 2011) approaches can be adopted to tackle this problem. For example, Zhou et al. (2015) propose to learn continuous word embeddings based on the QA corpus incorporating metadata such as category information, and the learned word embedding can be applied for question retrieval in CQA platform. Chen et al. (2018) encode the question text together with users’ social interactions for handling the lexical gap among questions. A random walk based learning method is designed to facilitate the similarities evaluation via the recurrent neural network. One shortcoming of these CQA methods for tackling the task of E-commerce product questions is that they can only make use of the typically"
D17-1221,P15-1153,1,0.892052,"he word level to the clause level, which include news headers such as “BEIJING, Nov. 24 (Xinhua) –”, intra-sentential attribution such as “, police said Thursday”, “, he said”, etc. The information filtered by the rules will be processed according to the word salience score. Information with smaller salience score (&lt; ) will be removed. 2084 2.3.2 Phrase-based Optimization for Summary Construction After coarse-grained compression on each single sentence as described above, we design a unified optimization method for summary generation. We refine the phrase-based summary construction model in (Bing et al., 2015) by adjusting the goal as compressive summarization. We consider the salience information obtained by our neural attention model and the compressed sentences in the coarse-grained compression component. Based on the parsed constituency tree for each input sentence as described in Section 2.3.1, we extract the noun-phrases (NPs) and verb-phrases (VPs). The salience Si of a phrase Pi is defined as: X X Si = { tf (t)/ tf (t)} × ai (15) t∈Pi t∈T opic where ai is the salience of the sentence containing Pi . tf (t) is the frequency of the concept t (unigram/bigram) in the whole topic. Thus, Si inher"
D17-1221,P16-1046,0,0.0226281,"mpression component. Finally, the attention weights are integrated into a phrase-based optimization framework for compressive summary generation. In fact, the notion of “attention” has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015). However, very few previous works employ attention mechanism to tackle MDS. Rush et al. (2015) and Nallapati et al. (2016) employed attention-based sequenceto-sequence (seq2seq) framework only for sentence summarization. Gu et al. (2016), Cheng and Lapata (2016), and Nallapati et al. (2016) also utilized seq2seq based framework with attention modeling for short text or single document summarization. Different from their works, our framework aims at conducting multi-document summarization in an unsupervised manner. Our contributions are as follows: (1) We propose a cascaded attention model that captures salient information in different semantic representations. (2) The attention weights are learned automatically by an unsupervised data reconstruction framework which can capture the sentence salience. By adding sparsity constraints on the number of out"
D17-1221,W04-3247,0,0.565526,"nts on the number of output vectors, we can generate condensed information which can be treated as word salience. Fine-grained and coarse-grained sentence compression strategies are incorporated to produce compressive summaries. Experiments on some benchmark data sets show that our framework achieves better results than the state-of-the-art methods. 1 Introduction The goal of Multi-Document Summarization (MDS) is to automatically produce a succinct summary, preserving the most important information of a set of documents describing a topic1 (Luhn, 1958; Edmundson, 1969; Goldstein et al., 2000; Erkan and Radev, 2004b; Wan et al., 2007; Nenkova and McKeown, 2012). Considering the procedure of summary writing by humans, when people read, they will remember and forget part ∗ The work described in this paper is supported by grants from the Research and Development Grant of Huawei Technologies Co. Ltd (YB2015100076/TH1510257) and the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). 1 A topic represents a real event, e.g., “AlphaGo versus Lee Sedol”. of the content. Information which is more important may make a deep impression easily. When people recall and digest"
D17-1221,W00-0405,0,0.241849,"adding sparsity constraints on the number of output vectors, we can generate condensed information which can be treated as word salience. Fine-grained and coarse-grained sentence compression strategies are incorporated to produce compressive summaries. Experiments on some benchmark data sets show that our framework achieves better results than the state-of-the-art methods. 1 Introduction The goal of Multi-Document Summarization (MDS) is to automatically produce a succinct summary, preserving the most important information of a set of documents describing a topic1 (Luhn, 1958; Edmundson, 1969; Goldstein et al., 2000; Erkan and Radev, 2004b; Wan et al., 2007; Nenkova and McKeown, 2012). Considering the procedure of summary writing by humans, when people read, they will remember and forget part ∗ The work described in this paper is supported by grants from the Research and Development Grant of Huawei Technologies Co. Ltd (YB2015100076/TH1510257) and the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). 1 A topic represents a real event, e.g., “AlphaGo versus Lee Sedol”. of the content. Information which is more important may make a deep impression easily. When pe"
D17-1221,P16-1154,1,0.918562,"rained sentence compression component. Finally, the attention weights are integrated into a phrase-based optimization framework for compressive summary generation. In fact, the notion of “attention” has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015). However, very few previous works employ attention mechanism to tackle MDS. Rush et al. (2015) and Nallapati et al. (2016) employed attention-based sequenceto-sequence (seq2seq) framework only for sentence summarization. Gu et al. (2016), Cheng and Lapata (2016), and Nallapati et al. (2016) also utilized seq2seq based framework with attention modeling for short text or single document summarization. Different from their works, our framework aims at conducting multi-document summarization in an unsupervised manner. Our contributions are as follows: (1) We propose a cascaded attention model that captures salient information in different semantic representations. (2) The attention weights are learned automatically by an unsupervised data reconstruction framework which can capture the sentence salience. By adding sparsity constra"
D17-1221,D13-1047,0,0.0188099,"hat aoi = Aoi,: ∈ Rm is the attention weight vector for si . According to Equation 9, a large value in aoi conveys a meaning that the corresponding sentence should contribute more when generating si . We also use the magnitude of the columns in Ao to represent the salience of sentences. 2.3 2.3.1 Compressive Summary Generation Phase Coarse-grained Sentence Compression Using the information distillation result from the cascaded neural attention model, we conduct coarse-grained compression for each individual sentence. Such strategy has been adopted in some multi-document summarization methods (Li et al., 2013; Wang et al., 2013; Yao et al., 2015). Our coarse-grained sentence compression jointly considers word salience obtained from the neural attention model and linguistically-motivated rules. The linguistically-motivated rules are designed based on the observed obvious evidence for uncritical information from the word level to the clause level, which include news headers such as “BEIJING, Nov. 24 (Xinhua) –”, intra-sentential attribution such as “, police said Thursday”, “, he said”, etc. The information filtered by the rules will be processed according to the word salience score. Information wit"
D17-1221,W04-1013,0,0.0249676,"Missing"
D17-1221,P98-2222,0,0.0532758,"ESU4 (R-SU4) are reported. 4 R-1 0.280 0.308 0.360 0.373 0.340 0.377 0.391 0.392 0.393* R-1 0.302 0.312 0.378 0.403 0.353 0.398 0.408 0.419 0.423* R-2 0.046 0.058 0.075 0.083 0.055 0.087 0.097 0.103 0.107* R-SU4 0.088 0.102 0.130 0.144 0.112 0.137 0.150 0.156 0.161* tence compression (Section 2.3.1) show that the compression can indeed improve the sumamrization performance. 4.2 Main Results of Compressive MDS We compare our system C-Attention with several unsupervised summarization baselines and state-of-the-art models. Random baseline selects sentences randomly for each topic. Lead baseline (Wasson, 1998) ranks the news chronologically and extracts the leading sentences one by one. TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004a) estimate sentence salience by applying the PageRank algorithm to the sentence graph. PKUTM (Li et al., 2011) employs manifold-ranking for sentence scoring and selection; ABS-Phrase (Bing et al., 2015) generates abstractive summaries using phrase-based optimization framework. Three other unsupervised methods based on sparse coding are also compared, namely, DSDR (He et al., 2012), MDS-Sparse (Liu et al., 2015), and RAMDS (Li et al., 2015). As sh"
D17-1221,D12-1022,0,0.028573,"der to obtain coherent summaries with good readability, we add some constraints into the ILP framework such as sentence generation constraint: Let βk denote the selection indicator of the sentence xk . If any phrase from xk is selected, βk = 1. Otherwise, βk = 0. For generating a compressed summary sentence, it is required that if βk = 1, at least one NP and at lease one VP of the sentence should be selected. It is expressed as: ∀Pi ∈ xk , αi ≤ βk ∧ X i αi ≥ βk , Other constraints include sentence number, summary length, phrase co-occurrence, etc. For details, please refer to McDonald (2007), Woodsend and Lapata (2012), and Bing et al. (2015). The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as the simplex algorithm (Dantzig and Thapa, 2006). In the implementation, we use a package called lp solve3 . In the post-processing, the phrases and sentences in a summary are ordered according to their natural order if they come from the same document. Otherwise, they are ordered according to the timestamps of the corresponding documents. 3 Experimental Setup 3.1 Datasets DUC: Both DUC 2006 and DUC 2007 are used in our evaluation. DUC 2006 and DU"
D17-1221,D15-1166,0,0.250292,"is preserved. Precisely, the recaller outputs fewer vectors s than that of the input UHFRQVWUXFW ܺ ܿଶ ܪ ܿଶ Dec ݏଶ ܿଵ ݏଵ ܿଵ UHFDOOHU ܵ ܣ ܣ ܿ Enc UHDGHU supervised manner. Thereafter, the word salience is fed into a coarse-grained sentence compression component. Finally, the attention weights are integrated into a phrase-based optimization framework for compressive summary generation. In fact, the notion of “attention” has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015). However, very few previous works employ attention mechanism to tackle MDS. Rush et al. (2015) and Nallapati et al. (2016) employed attention-based sequenceto-sequence (seq2seq) framework only for sentence summarization. Gu et al. (2016), Cheng and Lapata (2016), and Nallapati et al. (2016) also utilized seq2seq based framework with attention modeling for short text or single document summarization. Different from their works, our framework aims at conducting multi-document summarization in an unsupervised manner. Our contributions are as follows: (1) We propose a cascaded attention model tha"
D17-1221,C12-1128,0,0.047742,"Missing"
D17-1221,K16-1028,0,0.165303,"ଶ ܿଵ ݏଵ ܿଵ UHFDOOHU ܵ ܣ ܣ ܿ Enc UHDGHU supervised manner. Thereafter, the word salience is fed into a coarse-grained sentence compression component. Finally, the attention weights are integrated into a phrase-based optimization framework for compressive summary generation. In fact, the notion of “attention” has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015). However, very few previous works employ attention mechanism to tackle MDS. Rush et al. (2015) and Nallapati et al. (2016) employed attention-based sequenceto-sequence (seq2seq) framework only for sentence summarization. Gu et al. (2016), Cheng and Lapata (2016), and Nallapati et al. (2016) also utilized seq2seq based framework with attention modeling for short text or single document summarization. Different from their works, our framework aims at conducting multi-document summarization in an unsupervised manner. Our contributions are as follows: (1) We propose a cascaded attention model that captures salient information in different semantic representations. (2) The attention weights are learned automatically b"
D17-1221,D15-1044,0,0.299409,"W ܺ ܿଶ ܪ ܿଶ Dec ݏଶ ܿଵ ݏଵ ܿଵ UHFDOOHU ܵ ܣ ܣ ܿ Enc UHDGHU supervised manner. Thereafter, the word salience is fed into a coarse-grained sentence compression component. Finally, the attention weights are integrated into a phrase-based optimization framework for compressive summary generation. In fact, the notion of “attention” has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015). However, very few previous works employ attention mechanism to tackle MDS. Rush et al. (2015) and Nallapati et al. (2016) employed attention-based sequenceto-sequence (seq2seq) framework only for sentence summarization. Gu et al. (2016), Cheng and Lapata (2016), and Nallapati et al. (2016) also utilized seq2seq based framework with attention modeling for short text or single document summarization. Different from their works, our framework aims at conducting multi-document summarization in an unsupervised manner. Our contributions are as follows: (1) We propose a cascaded attention model that captures salient information in different semantic representations. (2) The attention weights"
D17-1221,P13-1136,0,0.0304866,"Rm is the attention weight vector for si . According to Equation 9, a large value in aoi conveys a meaning that the corresponding sentence should contribute more when generating si . We also use the magnitude of the columns in Ao to represent the salience of sentences. 2.3 2.3.1 Compressive Summary Generation Phase Coarse-grained Sentence Compression Using the information distillation result from the cascaded neural attention model, we conduct coarse-grained compression for each individual sentence. Such strategy has been adopted in some multi-document summarization methods (Li et al., 2013; Wang et al., 2013; Yao et al., 2015). Our coarse-grained sentence compression jointly considers word salience obtained from the neural attention model and linguistically-motivated rules. The linguistically-motivated rules are designed based on the observed obvious evidence for uncritical information from the word level to the clause level, which include news headers such as “BEIJING, Nov. 24 (Xinhua) –”, intra-sentential attribution such as “, police said Thursday”, “, he said”, etc. The information filtered by the rules will be processed according to the word salience score. Information with smaller salience"
D17-1221,W04-3252,0,\N,Missing
D17-1221,C98-2217,0,\N,Missing
D17-1222,C16-1053,0,0.00399758,"maries. (3) Experimental results on some benchmark datasets in different languages show that our framework achieves better performance than the state-of-the-art models. 2 Related Works Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document (Nenkova and McKeown, 2012). Traditionally, the summarization methods can be classified into three categories: extraction-based methods (Erkan and Radev, 2004; Goldstein et al., 2000; Wan et al., 2007; Min et al., 2012; Nallapati et al., 2017; Cheng and Lapata, 2016; Cao et al., 2016; Song et al., 2017), compression-based methods (Li et al., 2013; Wang et al., 2013; Li et al., 2015, 2017), and abstraction-based methods. In fact, previous investigations show that human-written summaries are more abstractive (Barzilay and McKeown, 2005; Bing et al., 2015). Abstraction-based approaches can generate new sentences based on the facts from different source sentences. Barzilay and McKeown (2005) employed sentence fusion to generate a new sentence. Bing et al. (2015) proposed a more fine-grained fusion framework, where new sentences are generated by selecting and merging salient p"
D17-1222,P10-1084,0,0.014277,"Missing"
D17-1222,P16-1046,0,0.0456727,"s of the abstractive summaries. (3) Experimental results on some benchmark datasets in different languages show that our framework achieves better performance than the state-of-the-art models. 2 Related Works Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document (Nenkova and McKeown, 2012). Traditionally, the summarization methods can be classified into three categories: extraction-based methods (Erkan and Radev, 2004; Goldstein et al., 2000; Wan et al., 2007; Min et al., 2012; Nallapati et al., 2017; Cheng and Lapata, 2016; Cao et al., 2016; Song et al., 2017), compression-based methods (Li et al., 2013; Wang et al., 2013; Li et al., 2015, 2017), and abstraction-based methods. In fact, previous investigations show that human-written summaries are more abstractive (Barzilay and McKeown, 2005; Bing et al., 2015). Abstraction-based approaches can generate new sentences based on the facts from different source sentences. Barzilay and McKeown (2005) employed sentence fusion to generate a new sentence. Bing et al. (2015) proposed a more fine-grained fusion framework, where new sentences are generated by selecting and"
D17-1222,N16-1012,0,0.147341,"50. We also set the dimension of hidden states and latent variables to 500. The maximum length of documents and summaries is 120 and 25 respectively, and the batch size is also 256. The beam size of the decoder was set to be 10. Adadelta (Schmidhuber, 2015) with hyperparameter ρ = 0.95 and  = 1e − 6 is used for gradient based optimization. Our neural network based framework is implemented using Theano (Theano Development Team, 2016). 5 5.1 • RNN-distract (Chen et al., 2016) uses a new attention mechanism by distracting the historical attention in the decoding steps. • RAS-LSTM and RAS-Elman (Chopra et al., 2016) both consider words and word positions as input and use convolutional encoders to handle the source information. For the attention based sequence decoding process, RAS-Elman selects Elman RNN (Elman, 1990) as decoder, and RAS-LSTM selects Long Short-Term Memory architecture (Hochreiter and Schmidhuber, 1997). • LenEmb (Kikuchi et al., 2016) uses a mechanism to control the summary length by considering the length embedding vector as the input. Experimental Settings Results and Discussions ROUGE Evaluation Table 1: ROUGE-F1 on validation sets Dataset GIGA LCSTS System StanD DRGD StanD DRGD R-1"
D17-1222,W00-0405,0,0.167306,"scriminative deterministic variables are jointly considered in the generation process of the abstractive summaries. (3) Experimental results on some benchmark datasets in different languages show that our framework achieves better performance than the state-of-the-art models. 2 Related Works Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document (Nenkova and McKeown, 2012). Traditionally, the summarization methods can be classified into three categories: extraction-based methods (Erkan and Radev, 2004; Goldstein et al., 2000; Wan et al., 2007; Min et al., 2012; Nallapati et al., 2017; Cheng and Lapata, 2016; Cao et al., 2016; Song et al., 2017), compression-based methods (Li et al., 2013; Wang et al., 2013; Li et al., 2015, 2017), and abstraction-based methods. In fact, previous investigations show that human-written summaries are more abstractive (Barzilay and McKeown, 2005; Bing et al., 2015). Abstraction-based approaches can generate new sentences based on the facts from different source sentences. Barzilay and McKeown (2005) employed sentence fusion to generate a new sentence. Bing et al. (2015) proposed a mo"
D17-1222,J05-3002,0,0.170352,"rating a summary that retains the most important content of the original text document (Nenkova and McKeown, 2012). Traditionally, the summarization methods can be classified into three categories: extraction-based methods (Erkan and Radev, 2004; Goldstein et al., 2000; Wan et al., 2007; Min et al., 2012; Nallapati et al., 2017; Cheng and Lapata, 2016; Cao et al., 2016; Song et al., 2017), compression-based methods (Li et al., 2013; Wang et al., 2013; Li et al., 2015, 2017), and abstraction-based methods. In fact, previous investigations show that human-written summaries are more abstractive (Barzilay and McKeown, 2005; Bing et al., 2015). Abstraction-based approaches can generate new sentences based on the facts from different source sentences. Barzilay and McKeown (2005) employed sentence fusion to generate a new sentence. Bing et al. (2015) proposed a more fine-grained fusion framework, where new sentences are generated by selecting and merging salient phrases. These methods can be regarded as a kind of indirect abstractive summarization, and complicated constraints are used to guarantee the linguistic quality. Recently, some researchers employ neural network based framework to tackle the abstractive sum"
D17-1222,P15-1153,1,0.900844,"ntroduction Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document (Edmundson, 1969; Luhn, 1958; Nenkova and McKeown, 2012). Different from the common extraction-based and compression-based methods, abstraction-based methods aim at constructing new sentences as summaries, thus they require a deeper understanding of the text and the capability of generating new sentences, which provide an obvious advantage in improving the focus of a summary, reducing the redundancy, and keeping a good compression rate (Bing et al., 2015; Rush et al., 2015; Nallapati et al., 2016). ∗ The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). Some previous research works show that human-written summaries are more abstractive (Jing and McKeown, 2000). Moreover, our investigation reveals that people may naturally follow some inherent structures when they write the abstractive summaries. To illustrate this observation, we show some examples in Figure 1, which are some top story summaries or headlines from the channel “Technology”"
D17-1222,P16-1154,0,0.0798899,"e-grained fusion framework, where new sentences are generated by selecting and merging salient phrases. These methods can be regarded as a kind of indirect abstractive summarization, and complicated constraints are used to guarantee the linguistic quality. Recently, some researchers employ neural network based framework to tackle the abstractive summarization problem. Rush et al. (2015) proposed a neural network based model with local attention modeling, which is trained on the Gigaword corpus, but combined with an additional loglinear extractive summarization model with handcrafted features. Gu et al. (2016) integrated a copying mechanism into a seq2seq framework to improve the quality of the generated summaries. Chen et al. (2016) proposed a new attention mechanism that not only considers the important source 2092 segments, but also distracts them in the decoding step in order to better grasp the overall meaning of input documents. Nallapati et al. (2016) utilized a trick to control the vocabulary size to improve the training efficiency. The calculations in these methods are all deterministic and the representation ability is limited. Miao and Blunsom (2016) extended the seq2seq framework and pr"
D17-1222,D15-1229,0,0.257327,"acting the first sentence from articles with the headline to form a sourcesummary pair. We directly download the prepared dataset used in (Rush et al., 2015). It roughly contains 3.8M training pairs, 190K validation pairs, and 2,000 test pairs. DUC-20042 is another English dataset only used for testing in our experiments. It contains 500 documents. Each document contains 4 model summaries written by experts. The length of the summary is limited to 75 bytes. LCSTS is a large-scale Chinese short text summarization dataset, consisting of pairs of (short text, summary) collected from Sina Weibo3 (Hu et al., 2015). We take Part-I as the training set, Part-II as the development set, and Part-III as the test set. There is a score in range 1 ∼ 5 labeled by human to indicate how relevant an article and its summary is. We only reserve those pairs with scores no less than 3. The size of the three sets are 2.4M, 8.7k, and 725 respectively. In our experiments, we only take Chinese character sequence as input, without performing word segmentation. 4.2 Evaluation Metrics We use ROUGE score (Lin, 2004) as our evaluation metric with standard options. The basic idea of ROUGE is to count the number of overlapping un"
D17-1222,A00-2024,0,0.0855086,"ethods aim at constructing new sentences as summaries, thus they require a deeper understanding of the text and the capability of generating new sentences, which provide an obvious advantage in improving the focus of a summary, reducing the redundancy, and keeping a good compression rate (Bing et al., 2015; Rush et al., 2015; Nallapati et al., 2016). ∗ The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). Some previous research works show that human-written summaries are more abstractive (Jing and McKeown, 2000). Moreover, our investigation reveals that people may naturally follow some inherent structures when they write the abstractive summaries. To illustrate this observation, we show some examples in Figure 1, which are some top story summaries or headlines from the channel “Technology” of CNN. After analyzing the summaries carefully, we can find some common structures from them, such as “What”, “What-Happened” , “Who Action What”, etc. For example, the summary “Apple sues Qualcomm for nearly $1 billion” can be structuralized as “Who (Apple) Action (sues) What (Qualcomm)”. Similarly, the summaries"
D17-1222,D16-1140,0,0.0415869,"Our neural network based framework is implemented using Theano (Theano Development Team, 2016). 5 5.1 • RNN-distract (Chen et al., 2016) uses a new attention mechanism by distracting the historical attention in the decoding steps. • RAS-LSTM and RAS-Elman (Chopra et al., 2016) both consider words and word positions as input and use convolutional encoders to handle the source information. For the attention based sequence decoding process, RAS-Elman selects Elman RNN (Elman, 1990) as decoder, and RAS-LSTM selects Long Short-Term Memory architecture (Hochreiter and Schmidhuber, 1997). • LenEmb (Kikuchi et al., 2016) uses a mechanism to control the summary length by considering the length embedding vector as the input. Experimental Settings Results and Discussions ROUGE Evaluation Table 1: ROUGE-F1 on validation sets Dataset GIGA LCSTS System StanD DRGD StanD DRGD R-1 32.69 36.25 33.88 36.71 R-2 15.29 17.61 21.49 24.00 R-L 30.60 33.55 31.05 34.10 We first depict the performance of our model DRGD by comparing to the standard decoders (StanD) of our own implementation. The comparison results on the validation datasets of Gigawords and LCSTS are shown in Table 1. From the results we can see that our proposed"
D17-1222,koen-2004-pharaoh,0,0.0931554,"ss. To generate summaries precisely, we first integrate the recurrent generative decoding component with the discriminative deterministic decoding component, and map the latent structure variable zt and the deterministic decoding hidden state hdt 2 to a new hidden variable: d d d dz d2 ht + bhy ) ht y = tanh(Wzhy zt + Whh (12) d Given the combined decoding state ht y at the time t, the probability of generating any target word yt is given as follows: d d ht y + bdhy ) yt = ς(Why (13) d ∈ Rky ×kh and bd ∈ Rky . ς(·) is the where Why hy softmax function. Finally, we use a beam search algorithm (Koehn, 2004) for decoding and generating the best summary. 3.4 Learning Although the proposed model contains a recurrent generative decoder, the whole framework is fully differentiable. As shown in Section 3.3, both the recurrent deterministic decoder and the recurrent generative decoder are designed based on neural networks. Therefore, all the parameters in our model can be optimized in an end-to-end paradigm using back-propagation. We use {X}N and {Y }N to denote the training source and target sequence. Generally, the objective of our framework consists of two terms. One term is the negative loglikeliho"
D17-1222,K16-1028,0,0.418007,"the process of automatically generating a summary that retains the most important content of the original text document (Edmundson, 1969; Luhn, 1958; Nenkova and McKeown, 2012). Different from the common extraction-based and compression-based methods, abstraction-based methods aim at constructing new sentences as summaries, thus they require a deeper understanding of the text and the capability of generating new sentences, which provide an obvious advantage in improving the focus of a summary, reducing the redundancy, and keeping a good compression rate (Bing et al., 2015; Rush et al., 2015; Nallapati et al., 2016). ∗ The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). Some previous research works show that human-written summaries are more abstractive (Jing and McKeown, 2000). Moreover, our investigation reveals that people may naturally follow some inherent structures when they write the abstractive summaries. To illustrate this observation, we show some examples in Figure 1, which are some top story summaries or headlines from the channel “Technology” of CNN. After analyzing the summaries carefu"
D17-1222,D15-1044,0,0.768032,"ic summarization is the process of automatically generating a summary that retains the most important content of the original text document (Edmundson, 1969; Luhn, 1958; Nenkova and McKeown, 2012). Different from the common extraction-based and compression-based methods, abstraction-based methods aim at constructing new sentences as summaries, thus they require a deeper understanding of the text and the capability of generating new sentences, which provide an obvious advantage in improving the focus of a summary, reducing the redundancy, and keeping a good compression rate (Bing et al., 2015; Rush et al., 2015; Nallapati et al., 2016). ∗ The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). Some previous research works show that human-written summaries are more abstractive (Jing and McKeown, 2000). Moreover, our investigation reveals that people may naturally follow some inherent structures when they write the abstractive summaries. To illustrate this observation, we show some examples in Figure 1, which are some top story summaries or headlines from the channel “Technology” of CNN. After analy"
D17-1222,D13-1047,0,0.0606521,"fferent languages show that our framework achieves better performance than the state-of-the-art models. 2 Related Works Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document (Nenkova and McKeown, 2012). Traditionally, the summarization methods can be classified into three categories: extraction-based methods (Erkan and Radev, 2004; Goldstein et al., 2000; Wan et al., 2007; Min et al., 2012; Nallapati et al., 2017; Cheng and Lapata, 2016; Cao et al., 2016; Song et al., 2017), compression-based methods (Li et al., 2013; Wang et al., 2013; Li et al., 2015, 2017), and abstraction-based methods. In fact, previous investigations show that human-written summaries are more abstractive (Barzilay and McKeown, 2005; Bing et al., 2015). Abstraction-based approaches can generate new sentences based on the facts from different source sentences. Barzilay and McKeown (2005) employed sentence fusion to generate a new sentence. Bing et al. (2015) proposed a more fine-grained fusion framework, where new sentences are generated by selecting and merging salient phrases. These methods can be regarded as a kind of indirect abst"
D17-1222,W04-1013,0,0.015671,"short text summarization dataset, consisting of pairs of (short text, summary) collected from Sina Weibo3 (Hu et al., 2015). We take Part-I as the training set, Part-II as the development set, and Part-III as the test set. There is a score in range 1 ∼ 5 labeled by human to indicate how relevant an article and its summary is. We only reserve those pairs with scores no less than 3. The size of the three sets are 2.4M, 8.7k, and 725 respectively. In our experiments, we only take Chinese character sequence as input, without performing word segmentation. 4.2 Evaluation Metrics We use ROUGE score (Lin, 2004) as our evaluation metric with standard options. The basic idea of ROUGE is to count the number of overlapping units between generated summaries and the reference summaries, such as overlapped n-grams, word sequences, and word pairs. F-measures of ROUGE-1 (R-1), ROUGE-2 (R-2), ROUGE-L (RL) and ROUGE-SU4 (R-SU4) are reported. 4.3 Comparative Methods We compare our model with some baselines and state-of-the-art methods. Because the datasets are 2096 1 https://catalog.ldc.upenn.edu/ldc2012t21 http://duc.nist.gov/duc2004 3 http://www.weibo.com 2 • ASC+FSC1 (Miao and Blunsom, 2016) uses a generativ"
D17-1222,P09-2075,0,0.0173551,"all meaning of input documents. Nallapati et al. (2016) utilized a trick to control the vocabulary size to improve the training efficiency. The calculations in these methods are all deterministic and the representation ability is limited. Miao and Blunsom (2016) extended the seq2seq framework and proposed a generative model to capture the latent summary information, but they do not consider the recurrent dependencies in their generative model leading to limited representation ability. Some research works employ topic models to capture the latent information from source documents or sentences. Wang et al. (2009) proposed a new Bayesian sentence-based topic model by making use of both the term-document and termsentence associations to improve the performance of sentence selection. Celikyilmaz and HakkaniTur (2010) estimated scores for sentences based on their latent characteristics using a hierarchical topic model, and trained a regression model to extract sentences. However, they only use the latent topic information to conduct the sentence salience estimation for extractive summarization. In contrast, our purpose is to model and learn the latent structure information from the target summaries and us"
D17-1222,P13-1136,0,0.0123536,"show that our framework achieves better performance than the state-of-the-art models. 2 Related Works Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document (Nenkova and McKeown, 2012). Traditionally, the summarization methods can be classified into three categories: extraction-based methods (Erkan and Radev, 2004; Goldstein et al., 2000; Wan et al., 2007; Min et al., 2012; Nallapati et al., 2017; Cheng and Lapata, 2016; Cao et al., 2016; Song et al., 2017), compression-based methods (Li et al., 2013; Wang et al., 2013; Li et al., 2015, 2017), and abstraction-based methods. In fact, previous investigations show that human-written summaries are more abstractive (Barzilay and McKeown, 2005; Bing et al., 2015). Abstraction-based approaches can generate new sentences based on the facts from different source sentences. Barzilay and McKeown (2005) employed sentence fusion to generate a new sentence. Bing et al. (2015) proposed a more fine-grained fusion framework, where new sentences are generated by selecting and merging salient phrases. These methods can be regarded as a kind of indirect abstractive summarizati"
D17-1222,D16-1031,0,0.0846778,"of the generated summaries. However, very few existing works specifically consider the latent structure information of summaries in their summarization models. Although a very popular neural network based sequence-to-sequence (seq2seq) framework has been proposed to tackle the abstractive summarization problem (Lopyrev, 2015; Rush et al., 2015; Nallapati et al., 2016), the calculation of the internal decoding states is entirely deterministic. The deterministic transformations in these discriminative models lead to limitations on the representation ability of the latent structure information. Miao and Blunsom (2016) extended the seq2seq framework and proposed a generative model to capture the latent summary information, but they did not consider the recurrent dependencies in their generative model leading to limited representation ability. To tackle the above mentioned problems, we design a new framework based on sequenceto-sequence oriented encoder-decoder model equipped with a latent structure modeling component. We employ Variational Auto-Encoders (VAEs) (Kingma and Welling, 2013; Rezende et al., 2014) as the base model for our generative framework which can handle the inference problem associated wit"
D17-1222,C12-1128,0,0.0296115,"Missing"
D17-1222,K16-1002,0,\N,Missing
D17-1310,S14-2051,0,0.56419,"aspects, but the ground truth of the corresponding opinion words is not commonly provided. Some works tackling the ATE task ignore the consideration of opinion words and just focus on aspect term modeling and learning (Jin ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). We thank Lidong Bing and Piji Li for their helpful comments on this draft and the anonymous reviewers for their valuable feedback. et al., 2009; Jakob and Gurevych, 2010; Toh and Wang, 2014; Chernyshevich, 2014; Manek et al., 2017; San Vicente et al., 2015; Liu et al., 2015; Poria et al., 2016; Toh and Su, 2016; Yin et al., 2016). They fail to leverage opinion information which is supposed to be useful clues. Some works tackling the ATE task consider opinion information (Hu and Liu, 2004a,b; Popescu and Etzioni, 2005; Zhuang et al., 2006; Qiu et al., 2011; Liu et al., 2012b, 2013a,b, 2014) in an unsupervised or partially supervised manner. Qiu et al. (2011) proposed Double Propagation (DP) to collectively extract aspect terms and opinion words based on information propagation over a dependency graph"
D17-1310,S14-2004,0,0.54819,"ate the proposed MIN framework, we perform comparison with the following two groups of methods: (1) CRF based methods: • CRF: Conditional Random Fields with basic feature templates2 and word embeddings. • Semi-CRF: First-order semi-Markov conditional random fields (Sarawagi et al., 2004) and the feature template in Cuong et al. (2014) is adopted. (6) T log[P (Yi,t |Xi,t )] For S-LSTM, sentence-level cross entropy error are employed to calculate the corresponding loss: Loss(S) = − • IHS RD (Chernyshevich, 2014), NLANGP (Toh and Su, 2016): Best systems in ATE subtask in SemEval ABSA challenges (Pontiki et al., 2014, 2016). N 1 X P (YiS,g |Xi ) log[P (YiS |Xi )] N i=1 • DLIREC (Toh and Wang, 2014), AUEB (Xenos et al., 2016): Top-ranked CRF-based systems in ATE subtask in SemEval ABSA challenges (Pontiki et al., 2014, 2016). (7) Then, losses from different LSTMs are combined to form the training objective of the MIN framework: J(θ) = Loss(A) + Loss(O) + Loss(S) (8) • WDEmb (Yin et al., 2016): Enhanced CRF with word embeddings, linear context embeddings and dependency path embeddings. . D1 D2 #TRAIN/#TEST Sentences 3045/800 2000/676 #TRAIN/#TEST Aspects 2358/654 1743/622 (2) Neural Network based methods •"
D17-1310,H05-1043,0,0.0960063,"Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). We thank Lidong Bing and Piji Li for their helpful comments on this draft and the anonymous reviewers for their valuable feedback. et al., 2009; Jakob and Gurevych, 2010; Toh and Wang, 2014; Chernyshevich, 2014; Manek et al., 2017; San Vicente et al., 2015; Liu et al., 2015; Poria et al., 2016; Toh and Su, 2016; Yin et al., 2016). They fail to leverage opinion information which is supposed to be useful clues. Some works tackling the ATE task consider opinion information (Hu and Liu, 2004a,b; Popescu and Etzioni, 2005; Zhuang et al., 2006; Qiu et al., 2011; Liu et al., 2012b, 2013a,b, 2014) in an unsupervised or partially supervised manner. Qiu et al. (2011) proposed Double Propagation (DP) to collectively extract aspect terms and opinion words based on information propagation over a dependency graph. One drawback is that it heavily relies on the dependency parser, which is prone to generate mistakes when applying on informal online reviews. Liu et al. (2014) modeled relation between aspects and opinions by constructing a bipartite heterogenous graph. It cannot perform well without a high-quality phrase ch"
D17-1310,D12-1123,0,0.49615,"ion, China (Project Code: 14203414). We thank Lidong Bing and Piji Li for their helpful comments on this draft and the anonymous reviewers for their valuable feedback. et al., 2009; Jakob and Gurevych, 2010; Toh and Wang, 2014; Chernyshevich, 2014; Manek et al., 2017; San Vicente et al., 2015; Liu et al., 2015; Poria et al., 2016; Toh and Su, 2016; Yin et al., 2016). They fail to leverage opinion information which is supposed to be useful clues. Some works tackling the ATE task consider opinion information (Hu and Liu, 2004a,b; Popescu and Etzioni, 2005; Zhuang et al., 2006; Qiu et al., 2011; Liu et al., 2012b, 2013a,b, 2014) in an unsupervised or partially supervised manner. Qiu et al. (2011) proposed Double Propagation (DP) to collectively extract aspect terms and opinion words based on information propagation over a dependency graph. One drawback is that it heavily relies on the dependency parser, which is prone to generate mistakes when applying on informal online reviews. Liu et al. (2014) modeled relation between aspects and opinions by constructing a bipartite heterogenous graph. It cannot perform well without a high-quality phrase chunker and POS tagger reducing its flexibility. As unsuper"
D17-1310,P13-1172,0,0.0309136,"Missing"
D17-1310,J11-1002,0,0.689591,"Administrative Region, China (Project Code: 14203414). We thank Lidong Bing and Piji Li for their helpful comments on this draft and the anonymous reviewers for their valuable feedback. et al., 2009; Jakob and Gurevych, 2010; Toh and Wang, 2014; Chernyshevich, 2014; Manek et al., 2017; San Vicente et al., 2015; Liu et al., 2015; Poria et al., 2016; Toh and Su, 2016; Yin et al., 2016). They fail to leverage opinion information which is supposed to be useful clues. Some works tackling the ATE task consider opinion information (Hu and Liu, 2004a,b; Popescu and Etzioni, 2005; Zhuang et al., 2006; Qiu et al., 2011; Liu et al., 2012b, 2013a,b, 2014) in an unsupervised or partially supervised manner. Qiu et al. (2011) proposed Double Propagation (DP) to collectively extract aspect terms and opinion words based on information propagation over a dependency graph. One drawback is that it heavily relies on the dependency parser, which is prone to generate mistakes when applying on informal online reviews. Liu et al. (2014) modeled relation between aspects and opinions by constructing a bipartite heterogenous graph. It cannot perform well without a high-quality phrase chunker and POS tagger reducing its flexi"
D17-1310,S15-2127,0,0.146804,"onding opinion words is not commonly provided. Some works tackling the ATE task ignore the consideration of opinion words and just focus on aspect term modeling and learning (Jin ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). We thank Lidong Bing and Piji Li for their helpful comments on this draft and the anonymous reviewers for their valuable feedback. et al., 2009; Jakob and Gurevych, 2010; Toh and Wang, 2014; Chernyshevich, 2014; Manek et al., 2017; San Vicente et al., 2015; Liu et al., 2015; Poria et al., 2016; Toh and Su, 2016; Yin et al., 2016). They fail to leverage opinion information which is supposed to be useful clues. Some works tackling the ATE task consider opinion information (Hu and Liu, 2004a,b; Popescu and Etzioni, 2005; Zhuang et al., 2006; Qiu et al., 2011; Liu et al., 2012b, 2013a,b, 2014) in an unsupervised or partially supervised manner. Qiu et al. (2011) proposed Double Propagation (DP) to collectively extract aspect terms and opinion words based on information propagation over a dependency graph. One drawback is that it heavily relies on th"
D17-1310,P14-1030,0,0.0609027,"opinion information which is supposed to be useful clues. Some works tackling the ATE task consider opinion information (Hu and Liu, 2004a,b; Popescu and Etzioni, 2005; Zhuang et al., 2006; Qiu et al., 2011; Liu et al., 2012b, 2013a,b, 2014) in an unsupervised or partially supervised manner. Qiu et al. (2011) proposed Double Propagation (DP) to collectively extract aspect terms and opinion words based on information propagation over a dependency graph. One drawback is that it heavily relies on the dependency parser, which is prone to generate mistakes when applying on informal online reviews. Liu et al. (2014) modeled relation between aspects and opinions by constructing a bipartite heterogenous graph. It cannot perform well without a high-quality phrase chunker and POS tagger reducing its flexibility. As unsupervised or partially supervised frameworks cannot take the full advantages of aspect annotations commonly found in the training data, the above methods lead to deficiency in leveraging the data. Recently, Wang et al. (2016) considered relation between opinion words and aspect words in a supervised model named RNCRF. However, RNCRF tends to suffer from parsing errors since the structure of the"
D17-1310,S16-1045,0,0.402018,"ckling the ATE task ignore the consideration of opinion words and just focus on aspect term modeling and learning (Jin ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). We thank Lidong Bing and Piji Li for their helpful comments on this draft and the anonymous reviewers for their valuable feedback. et al., 2009; Jakob and Gurevych, 2010; Toh and Wang, 2014; Chernyshevich, 2014; Manek et al., 2017; San Vicente et al., 2015; Liu et al., 2015; Poria et al., 2016; Toh and Su, 2016; Yin et al., 2016). They fail to leverage opinion information which is supposed to be useful clues. Some works tackling the ATE task consider opinion information (Hu and Liu, 2004a,b; Popescu and Etzioni, 2005; Zhuang et al., 2006; Qiu et al., 2011; Liu et al., 2012b, 2013a,b, 2014) in an unsupervised or partially supervised manner. Qiu et al. (2011) proposed Double Propagation (DP) to collectively extract aspect terms and opinion words based on information propagation over a dependency graph. One drawback is that it heavily relies on the dependency parser, which is prone to generate mistakes"
D17-1310,D15-1168,0,0.375302,"s not commonly provided. Some works tackling the ATE task ignore the consideration of opinion words and just focus on aspect term modeling and learning (Jin ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). We thank Lidong Bing and Piji Li for their helpful comments on this draft and the anonymous reviewers for their valuable feedback. et al., 2009; Jakob and Gurevych, 2010; Toh and Wang, 2014; Chernyshevich, 2014; Manek et al., 2017; San Vicente et al., 2015; Liu et al., 2015; Poria et al., 2016; Toh and Su, 2016; Yin et al., 2016). They fail to leverage opinion information which is supposed to be useful clues. Some works tackling the ATE task consider opinion information (Hu and Liu, 2004a,b; Popescu and Etzioni, 2005; Zhuang et al., 2006; Qiu et al., 2011; Liu et al., 2012b, 2013a,b, 2014) in an unsupervised or partially supervised manner. Qiu et al. (2011) proposed Double Propagation (DP) to collectively extract aspect terms and opinion words based on information propagation over a dependency graph. One drawback is that it heavily relies on the dependency parse"
D17-1310,S14-2038,0,0.370812,"ions for opinionated aspects, but the ground truth of the corresponding opinion words is not commonly provided. Some works tackling the ATE task ignore the consideration of opinion words and just focus on aspect term modeling and learning (Jin ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). We thank Lidong Bing and Piji Li for their helpful comments on this draft and the anonymous reviewers for their valuable feedback. et al., 2009; Jakob and Gurevych, 2010; Toh and Wang, 2014; Chernyshevich, 2014; Manek et al., 2017; San Vicente et al., 2015; Liu et al., 2015; Poria et al., 2016; Toh and Su, 2016; Yin et al., 2016). They fail to leverage opinion information which is supposed to be useful clues. Some works tackling the ATE task consider opinion information (Hu and Liu, 2004a,b; Popescu and Etzioni, 2005; Zhuang et al., 2006; Qiu et al., 2011; Liu et al., 2012b, 2013a,b, 2014) in an unsupervised or partially supervised manner. Qiu et al. (2011) proposed Double Propagation (DP) to collectively extract aspect terms and opinion words based on information propagation ov"
D17-1310,D16-1059,0,0.437124,"tion over a dependency graph. One drawback is that it heavily relies on the dependency parser, which is prone to generate mistakes when applying on informal online reviews. Liu et al. (2014) modeled relation between aspects and opinions by constructing a bipartite heterogenous graph. It cannot perform well without a high-quality phrase chunker and POS tagger reducing its flexibility. As unsupervised or partially supervised frameworks cannot take the full advantages of aspect annotations commonly found in the training data, the above methods lead to deficiency in leveraging the data. Recently, Wang et al. (2016) considered relation between opinion words and aspect words in a supervised model named RNCRF. However, RNCRF tends to suffer from parsing errors since the structure of the recursive network hinges on the dependency parse tree. CMLA (Wang et al., 2017a) used a multilayer neural model where each layer consists of aspect attention and opinion attention. However CMLA merely employs standard GRU without extended memories. We propose MIN (Memory Interaction Network), a novel LSTM-based deep multi-task learning framework for the ATE task. Two LSTMs with extended memory are designed for handling 2886"
D17-1310,D10-1101,0,\N,Missing
D17-1310,H05-2017,0,\N,Missing
D18-1420,K16-1002,0,0.108391,"Missing"
D18-1420,Q18-1031,0,0.0250233,"ms, producing monotonous language, and generating short common sentences (Li et al., 2017). To solve these problems, some researchers branch out into the way of post-editing (could be under some guidance, say sentiment polarity) a given message to generate text of better quality. For example, skeleton-based text generation first outlines a skeleton in the form of phrases/words, and then starts from the skeleton to generate text (Wang et al., 2017; Xiao et al., 2016). Another line of works conduct editing on an existing sentence and expect that the output will serve particular purposes better (Guu et al., 2018). Similarly in conversation, some systems post-edit the retrieval results to generate new sentences as the response (Song et al., 2016). The third type is to perform editing on the input under the guidance of specific style. For example, Shen et al. (2017) take a sentence with negative sentiment as input, and edit it to transfer its sentiment polarity into positive. In this paper, we generalize the third type of post-editing into a more general scenario, named Quantifiable Sequence Editing (QuaSE). Specifically, in the training stage, each input sentence is associated with a numeric outcome. F"
D18-1420,W17-4902,0,0.0179707,"lies in the content factors z and z 0 . Given that z and z 0 are not enforced to resemble each other when Lsim is excluded from this tuning step, Lrec and Ld−rec cannot be minimized simultaneously. Moreover, when we minimize Lsim in the second step with the weights of Lrec and Lmse fixed, we observe that Ld−rec also decreases, which complies with the above analysis. 5 Related Works Inspired by the task of image style transfer (Gatys et al., 2016; Liu and Tuzel, 2016), researchers proposed the task of text style transfer and obtained some encouraging results (Fu et al., 2018; Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Zhang et al., 2018; Li et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018). Existing studies on text style transfer mainly aim at transferring text from an original style into a target style, e.g., from negative to positive, from male to female, from rude/normal to polite; from modern text to Shakespeare style, etc. In contrast, our proposed task QuaSE assumes each sentence is associated with an outcome pertaining to continues values, and the editing is under the guidance of a specific target. To transfer the style of a sentence, the paradigm of most works (Shen"
D18-1420,D17-1230,0,0.030564,"lts are reported and discussed to elaborate the peculiarities of our framework. 1 ∗ The work described in this paper was done when Yi Liao was an intern at Tencent AI Lab. The work is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: CUHK413510) 1 Our code and data are available at https:// bitbucket.org/leoeaton/quase/src/master/ Introduction Typical neural text generation is observed suffering from the problems of repetitions in word ngrams, producing monotonous language, and generating short common sentences (Li et al., 2017). To solve these problems, some researchers branch out into the way of post-editing (could be under some guidance, say sentiment polarity) a given message to generate text of better quality. For example, skeleton-based text generation first outlines a skeleton in the form of phrases/words, and then starts from the skeleton to generate text (Wang et al., 2017; Xiao et al., 2016). Another line of works conduct editing on an existing sentence and expect that the output will serve particular purposes better (Guu et al., 2018). Similarly in conversation, some systems post-edit the retrieval results"
D18-1420,N18-1169,0,0.0943885,"Missing"
D18-1420,P14-5010,0,0.00247152,"ry of the primary data. The vocabulary size of the dataset is 9,625. In total, our dataset contains 599K sentences, and we randomly hold 50K for test, 10K for validation, and the remaining for training. For training, we need each input sentence being associated with a rating value, and for test, we need to measure the rating of a generated sentence to check if the generated sentence satisfies the specified outcome target. Therefore, an automatic method is needed for measuring the rating values of training sentences and generated sentences. We employ the sentiment analyzer in Stanford CoreNLP (Manning et al., 2014) to do so. Specifically, we first invoke CoreNLP to output the probability of each rating in {1, 2, 3, 4, 5} for a sentence, then we take the sum of the probabilitymultiplied ratings as the sentence rating. Some statistics of the data is given in Table 2. Hereafter, we use “rating” and “outcome” interchangeably. 3859 2 https://www.yelp.com/dataset/challenge Rating interval Sentence# [1, 2) 34273 [2, 3) 231740 [3, 4) 165159 [4, 5] 167803 4.2 Table 2: Numbers of sentences in each rating interval. One may think that would it be possible to use the original rating given by Yelp users as outcome fo"
D18-1420,Q18-1027,0,0.0238259,"Lsim is excluded from this tuning step, Lrec and Ld−rec cannot be minimized simultaneously. Moreover, when we minimize Lsim in the second step with the weights of Lrec and Lmse fixed, we observe that Ld−rec also decreases, which complies with the above analysis. 5 Related Works Inspired by the task of image style transfer (Gatys et al., 2016; Liu and Tuzel, 2016), researchers proposed the task of text style transfer and obtained some encouraging results (Fu et al., 2018; Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Zhang et al., 2018; Li et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018). Existing studies on text style transfer mainly aim at transferring text from an original style into a target style, e.g., from negative to positive, from male to female, from rude/normal to polite; from modern text to Shakespeare style, etc. In contrast, our proposed task QuaSE assumes each sentence is associated with an outcome pertaining to continues values, and the editing is under the guidance of a specific target. To transfer the style of a sentence, the paradigm of most works (Shen et al., 2017; Mueller et al., 2017; Prabhumoye et al., 2018) first learns the latent representation of th"
D18-1420,P18-1080,0,0.0520265,"resemble each other when Lsim is excluded from this tuning step, Lrec and Ld−rec cannot be minimized simultaneously. Moreover, when we minimize Lsim in the second step with the weights of Lrec and Lmse fixed, we observe that Ld−rec also decreases, which complies with the above analysis. 5 Related Works Inspired by the task of image style transfer (Gatys et al., 2016; Liu and Tuzel, 2016), researchers proposed the task of text style transfer and obtained some encouraging results (Fu et al., 2018; Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Zhang et al., 2018; Li et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018). Existing studies on text style transfer mainly aim at transferring text from an original style into a target style, e.g., from negative to positive, from male to female, from rude/normal to polite; from modern text to Shakespeare style, etc. In contrast, our proposed task QuaSE assumes each sentence is associated with an outcome pertaining to continues values, and the editing is under the guidance of a specific target. To transfer the style of a sentence, the paradigm of most works (Shen et al., 2017; Mueller et al., 2017; Prabhumoye et al., 2018) first learns the late"
D18-1420,D17-1298,0,0.0298791,"of the style. A transferred sentence is generated from a modified latent representation. Different from the aforementioned works based on latent representations, Li et al. (2018) propose a simpler method that achieves attribute transfer by changing a few attribute marker words or phrases in the sentence that are indicative of a particular attribute, while leaving the rest of the sentence largely unchanged. The simple method is able to generate better-quality sentences than the aforementioned works. Besides style transfer, sentence editing models can be developed for other tasks. For example, Schmaltz et al. (2017) propose neural sequence-labelling models for correcting the grammatical errors of sentences. 6 Conclusions We proposed a new task namely Quantifiable Sequence Editing (QuaSE), where a model needs to edit an input sentences towards the direction of a numerical outcome target. To tackle this task, we proposed a novel framework that simultaneously exploits the single sentences and pseudo-parallel sentence pairs. For evaluation, we prepared a dataset with Yelp sentences and their ratings. Experimental results show that our framework outperforms the compared methods under the measures of sentiment"
D18-1420,N18-1138,0,0.0434783,"en that z and z 0 are not enforced to resemble each other when Lsim is excluded from this tuning step, Lrec and Ld−rec cannot be minimized simultaneously. Moreover, when we minimize Lsim in the second step with the weights of Lrec and Lmse fixed, we observe that Ld−rec also decreases, which complies with the above analysis. 5 Related Works Inspired by the task of image style transfer (Gatys et al., 2016; Liu and Tuzel, 2016), researchers proposed the task of text style transfer and obtained some encouraging results (Fu et al., 2018; Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Zhang et al., 2018; Li et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018). Existing studies on text style transfer mainly aim at transferring text from an original style into a target style, e.g., from negative to positive, from male to female, from rude/normal to polite; from modern text to Shakespeare style, etc. In contrast, our proposed task QuaSE assumes each sentence is associated with an outcome pertaining to continues values, and the editing is under the guidance of a specific target. To transfer the style of a sentence, the paradigm of most works (Shen et al., 2017; Mueller et al., 2017; Prab"
D19-1024,D15-1060,1,0.888782,"Missing"
D19-1024,E17-1104,0,0.0173039,"l., 2013), where L1 -norm is used. Finally, during the meta-training phase or the training stage of the meta-testing phase, a loss function L related to C is computed, and the metalearner is adopted to optimize L so that the framework can be easily adapted to new relations and entities. Otherwise, during the testing stage of the meta-testing phase, we collect scores of the correct triplet and other candidates, and then we compute metrics based on the rank of correct triplet within all scores for evaluation. 4.1 The core of the encoding process is a N -layer convolutional neural network (CNN) (Conneau et al., 2017), which is shown to have excellent ability of extracting information. In our CNN, the basic convolutional block consists of three consecutive operations: two 1-d convolutions, an instance normalization (Ulyanov et al., 2016), and a non-linear mapping. For the pooling strategy, max pooling with a proper stride is used to distill the key information in the previous N − 1 layers, and Description Encoder In KG, if an entity is involved in multiple relations, it is natural that different relations are more 253 mean pooling is used to gather the information in the last layer. Moreover, in Step 3, we"
D19-1024,N18-2053,0,0.0284368,"y a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). 1 The implementation of our framework can be found in https://github.com/ZihaoWang/ Few-shot-KGC. 250 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 250–260, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 0.10 0.05 0.00 Relation Frequency et al., 2013; Socher et al., 2013; Wang et al., 2014; Trouillon et al., 2016; Nguyen et al., 2018) that have been proposed to learn good embeddings for entities and relations. However, embedding of uncommon relation or entities can not learn a good representation due to the data insufficiency. Some research has proposed that additional information can be introduced to enhance the learning performance. Among different types of information, textual descriptions is commonly considered by previous works (Zhong et al., 2015; Toutanova et al., 2015; Xie et al., 2016; Shi and Weninger, 2018). Recently, meta-learning is also proposed by (Xiong et al., 2018) to learn infrequent long-tailed relation"
D19-1024,D18-1223,0,0.229806,"encent AI Lab, Shenzhen, China 3 R&D Center Singapore, Machine Intelligence Technology, Alibaba DAMO Academy zihaowangbupt@gmail.com {kplai, wlam}@se.cuhk.edu.hk pijili@tencent.com l.bing@alibaba-inc.com Abstract KGs still have the issue of incomplete facts. To deal with the problem, Knowledge Graph Completion (KGC) task is introduced to automatically deduce and fill the missing facts. There exist many previous works focusing on this task and embedding-based methods (Bordes et al., 2013; Wang et al., 2014; Trouillon et al., 2016) achieves the best performance among them. Recent works such as (Xiong et al., 2018) have pointed out that relations in KGs follow a long-tailed distribution. To be more precise, a large proportion of relations have only a few facts in KGs. However, previous works of KGC usually focused on small proportions of frequent relations and ignored the remaining ones. One observation is that they often conducted experiments on small datasets such as FB15k and WN18 (Bordes et al., 2013) where a relation typically possesses thousands of facts. Moreover, after analyzing real-world KGs, we find that the more infrequently a relation appears, the entities within its facts are also more unc"
D19-1024,P15-1128,0,0.0414025,"ng stage. Experiments are conducted on two datasets from real-world KGs, and the results show that our framework 1 outperforms previous methods when dealing with infrequent relations and their accompanying uncommon entities. 1 Introduction Modern knowledge graphs (KGs)(Bollacker et al., 2008; Lehmann et al., 2015; Vrandeˇci´c and Kr¨otzsch, 2014) consist of a large number of facts, where each fact is represented as a triplet consisting of two entities and a binary relation between them. KGs provide rich information and it has been widely adopted in different tasks, such as question answering (Yih et al., 2015), information extraction (Bing et al., 2017, 2015, 2016) and image classification (Marino et al., 2017). However, Previous works such as (Xiong et al., 2018) only focused on those infrequent relations and ignored the accompanying problem of uncommon entities. When handling uncommon entities, relying only on the structural information of KGs would lead to inferior performance due to data insufficiency, ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). 1 The impl"
D19-1024,D15-1174,0,0.299412,"sociation for Computational Linguistics 0.10 0.05 0.00 Relation Frequency et al., 2013; Socher et al., 2013; Wang et al., 2014; Trouillon et al., 2016; Nguyen et al., 2018) that have been proposed to learn good embeddings for entities and relations. However, embedding of uncommon relation or entities can not learn a good representation due to the data insufficiency. Some research has proposed that additional information can be introduced to enhance the learning performance. Among different types of information, textual descriptions is commonly considered by previous works (Zhong et al., 2015; Toutanova et al., 2015; Xie et al., 2016; Shi and Weninger, 2018). Recently, meta-learning is also proposed by (Xiong et al., 2018) to learn infrequent long-tailed relations in KG. • We propose a novel model to extract relationspecific information from entity description for entities with multiple relations. 2.2 Meta-Learning Meta-learning (Lemke et al., 2015) aims at learning common experiences across different tasks and easy adapting the existing model to new tasks. One interesting application of meta-learning is few-shot learning problem where each task has only a few training data available. Some research focus"
D19-1024,D15-1031,0,0.188219,"r 3–7, 2019. 2019 Association for Computational Linguistics 0.10 0.05 0.00 Relation Frequency et al., 2013; Socher et al., 2013; Wang et al., 2014; Trouillon et al., 2016; Nguyen et al., 2018) that have been proposed to learn good embeddings for entities and relations. However, embedding of uncommon relation or entities can not learn a good representation due to the data insufficiency. Some research has proposed that additional information can be introduced to enhance the learning performance. Among different types of information, textual descriptions is commonly considered by previous works (Zhong et al., 2015; Toutanova et al., 2015; Xie et al., 2016; Shi and Weninger, 2018). Recently, meta-learning is also proposed by (Xiong et al., 2018) to learn infrequent long-tailed relations in KG. • We propose a novel model to extract relationspecific information from entity description for entities with multiple relations. 2.2 Meta-Learning Meta-learning (Lemke et al., 2015) aims at learning common experiences across different tasks and easy adapting the existing model to new tasks. One interesting application of meta-learning is few-shot learning problem where each task has only a few training data availa"
D19-1393,D15-1198,0,0.0397459,"eq2seq models (Bahdanau et al., 2014; Luong et al., 2015) can be readily utilized. Despite its simplicity, the performance of the current seq2seq models lag behind when the training data is limited. The first reason is that seq2seq models are often not as effective on smaller datasets. The second reason is that the linearized AMRs add the challenges of making use of the graph structure information. There are also some notable exceptions. Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinatory categorical grammar. Groschwitz et al. (2018); Lindemann et al. (2019) view AMR graphs as the structure AM algebra. Most AMR parsers require an explicit alignment between tokens in the sentences and nodes in the AMR graph during training. Since such information is not annotated, a pre-trained aligner (Flanigan et al., 2014; Pourdamghani et al., 2014; Liu et al., 2018) is often required. More recently, Lyu and Titov (2018) demonstrate that the alignments can be treated as latent variables in a joint probabilistic model. 3 3.1 Background and Overview Background of Multi-head A"
D19-1393,D17-1130,0,0.422449,"the following characteristics: (1) The nodes in AMR have no explicit alignment to text tokens; (2) The graph structure is more complicated because of frequent reentrancies and non-projective arcs; (3) There is a large and sparse vocabulary of possible node types (concepts). Many methods for AMR parsing have been developed in the past years, which can be categorized into three main classes: Graph-based parsing (Flanigan et al., 2014; Lyu and Titov, 2018) uses a pipeline design for concept identification and relation prediction. Transition-based parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017) processes a sentence from left-to-right and constructs the graph incrementally. The third class is seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017), which views parsing as sequence-to-sequence transduction by a linearization (depth-first traversal) of the AMR graph. 3799 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3799–3809, c Hong Kong, China, November 3–7, 2019. 2019"
D19-1393,W13-2322,0,0.368195,"of great interest. We evaluate our model on the latest AMR sembank and achieve the state-of-the-art performance in the sense that no heuristic graph re-categorization is adopted. More importantly, the experiments show that our parser is especially good at obtaining the core semantics. 1 earthquake mod time op1 big prosper-01 manner sudden op2 happiness mod such Figure 1: AMR for the sentence “During a time of prosperity and happiness, such a big earthquake suddenly struck.”, where the subgraphs close to the root represent the core semantics. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted labeled directed graph. As illustrated by an example in Figure 1, AMR abstracts away from the surface forms in text, where the root serves as a rudimentary representation of the overall focus while the details are elaborated as the depth of the graph increases. AMR has been proved useful for many downstream NLP tasks, including text summarization (Liu et al., 2015; Hardy and Vlachos, 2018) and question answering (Mitra and Baral, 2016). The task of AMR parsing is to map natural language strings to AMR semantic graphs a"
D19-1393,S16-1176,0,0.448254,"large and sparse vocabulary of possible node types (concepts). Many methods for AMR parsing have been developed in the past years, which can be categorized into three main classes: Graph-based parsing (Flanigan et al., 2014; Lyu and Titov, 2018) uses a pipeline design for concept identification and relation prediction. Transition-based parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017) processes a sentence from left-to-right and constructs the graph incrementally. The third class is seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017), which views parsing as sequence-to-sequence transduction by a linearization (depth-first traversal) of the AMR graph. 3799 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3799–3809, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics While existing graph-based models cannot sufficiently model the interactions between individual decisions, the autoregressive nature of transitionbased and seq2s"
D19-1393,S17-2157,0,0.0406881,"Missing"
D19-1393,P13-2131,0,0.540558,"Missing"
D19-1393,E17-1051,0,0.65646,"re challenging due to the following characteristics: (1) The nodes in AMR have no explicit alignment to text tokens; (2) The graph structure is more complicated because of frequent reentrancies and non-projective arcs; (3) There is a large and sparse vocabulary of possible node types (concepts). Many methods for AMR parsing have been developed in the past years, which can be categorized into three main classes: Graph-based parsing (Flanigan et al., 2014; Lyu and Titov, 2018) uses a pipeline design for concept identification and relation prediction. Transition-based parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017) processes a sentence from left-to-right and constructs the graph incrementally. The third class is seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017), which views parsing as sequence-to-sequence transduction by a linearization (depth-first traversal) of the AMR graph. 3799 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3799–3809, c Hong Kon"
D19-1393,P81-1022,0,0.722009,"Missing"
D19-1393,S16-1186,0,0.059568,"Missing"
D19-1393,P14-1134,0,0.6729,"or is grateful for the discussions with Zhisong Zhang and Zhijiang Guo. and Clark, 2009) and dependency parsing (K¨ubler et al., 2009), AMR parsing is considered more challenging due to the following characteristics: (1) The nodes in AMR have no explicit alignment to text tokens; (2) The graph structure is more complicated because of frequent reentrancies and non-projective arcs; (3) There is a large and sparse vocabulary of possible node types (concepts). Many methods for AMR parsing have been developed in the past years, which can be categorized into three main classes: Graph-based parsing (Flanigan et al., 2014; Lyu and Titov, 2018) uses a pipeline design for concept identification and relation prediction. Transition-based parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017) processes a sentence from left-to-right and constructs the graph incrementally. The third class is seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017), which views parsing as sequence-to-sequence transduction by a linearization (depth-first traversal) of the AMR graph. 3799 Proceedings of the 2019 Co"
D19-1393,P17-1043,0,0.064935,"acle design for handling the complexity of AMR graphs. Notably, most existing methods including the state-the-of-art parsers often rely on heavy graph re-categorization for reducing the complexity of the original AMR graphs. For graph recategorization, specific subgraphs of AMR are grouped together and assigned to a single node with a new compound category (Werling et al., 1 Depth-first traversal in seq2seq models does not produce a strictly top-down order due to the reentrancies in AMR. 2 Since the size of AMR graph is approximately linear in the length of sentence. 2015; Wang and Xue, 2017; Foland and Martin, 2017; Lyu and Titov, 2018; Groschwitz et al., 2018; Guo and Lu, 2018). The hand-crafted rules for re-categorization are often non-trivial, requiring exhaustive screening and expert-level manual efforts. For instance, in the re-categorization system of Lyu and Titov (2018), the graph :ARG3−of fragment “temporal-quantity −→ :quant :unit rate-entity-91 −→ year −→ 1” will be replaced by one single nested node “rate-entity-3(annual-01)”. There are hundreds of such manual heuristic rules. This kind of re-categorization has been shown to have considerable effects on the performance (Wang and Xue, 2017; G"
D19-1393,W17-6810,0,0.0329288,"Lu (2018) is a transitionbased parser with refined search space for AMR. Certain concepts and relations (e.g., reentrancies) are removed to reduce the burdens during training. Lyu and Titov (2018) is a graph-based method that achieves the best-reported result evaluated by the ordinary Smatch metric. Their parser uses different LSTMs for concept prediction, relation identification, and root identification sequentially. Also, the relation identification stage has the time complexity of O(m2 log m) where m is the number of concepts. Groschwitz et al. (2018) views AMR as terms of the AM algebra (Groschwitz et al., 2017), which allows standard tree-based parsing techniques to be applicable. The complexity of their projective decoder is O(m5 ). Last but not least, all these models except for that of van Noord and Bos (2017) require hand-crafted heuristics for graph re-categorization. We consider the Smatch-weighted metric as the most suitable metric for measuring the parser’s quality on capturing core semantics. The comparison shows that our method significantly outperforms all other methods. The Smatch-core metric also demonstrates the advantage of our 3805 Model Buys and Blunsom (2017) van Noord and Bos (201"
D19-1393,P18-1170,0,0.229177,"Missing"
D19-1393,P19-1450,0,0.0624406,"spite its simplicity, the performance of the current seq2seq models lag behind when the training data is limited. The first reason is that seq2seq models are often not as effective on smaller datasets. The second reason is that the linearized AMRs add the challenges of making use of the graph structure information. There are also some notable exceptions. Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinatory categorical grammar. Groschwitz et al. (2018); Lindemann et al. (2019) view AMR graphs as the structure AM algebra. Most AMR parsers require an explicit alignment between tokens in the sentences and nodes in the AMR graph during training. Since such information is not annotated, a pre-trained aligner (Flanigan et al., 2014; Pourdamghani et al., 2014; Liu et al., 2018) is often required. More recently, Lyu and Titov (2018) demonstrate that the alignments can be treated as latent variables in a joint probabilistic model. 3 3.1 Background and Overview Background of Multi-head Attention The multi-head attention mechanism introduced by Vaswani et al. (2017) is used a"
D19-1393,N15-1114,0,0.0342572,"earthquake suddenly struck.”, where the subgraphs close to the root represent the core semantics. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted labeled directed graph. As illustrated by an example in Figure 1, AMR abstracts away from the surface forms in text, where the root serves as a rudimentary representation of the overall focus while the details are elaborated as the depth of the graph increases. AMR has been proved useful for many downstream NLP tasks, including text summarization (Liu et al., 2015; Hardy and Vlachos, 2018) and question answering (Mitra and Baral, 2016). The task of AMR parsing is to map natural language strings to AMR semantic graphs automatically. Compared to constituent parsing (Zhang ∗ time ARG2 The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). The first author is grateful for the discussions with Zhisong Zhang and Zhijiang Guo. and Clark, 2009) and dependency parsing (K¨ubler et al., 2009), AMR parsing is considered more challenging d"
D19-1393,D18-1264,0,0.512596,"have no explicit alignment to text tokens; (2) The graph structure is more complicated because of frequent reentrancies and non-projective arcs; (3) There is a large and sparse vocabulary of possible node types (concepts). Many methods for AMR parsing have been developed in the past years, which can be categorized into three main classes: Graph-based parsing (Flanigan et al., 2014; Lyu and Titov, 2018) uses a pipeline design for concept identification and relation prediction. Transition-based parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017) processes a sentence from left-to-right and constructs the graph incrementally. The third class is seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017), which views parsing as sequence-to-sequence transduction by a linearization (depth-first traversal) of the AMR graph. 3799 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3799–3809, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Lingu"
D19-1393,D15-1166,0,0.00801688,"2017; Ballesteros and Al-Onaizan, 2017; Liu et al., 2018; Peng et al., 2018; Guo and Lu, 2018; Naseem et al., 2019) borrow techniques from shift-reduce dependency parsing. Yet the non-trivial nature of AMR graphs (e.g., reentrancies and non-projective arcs) makes the transition system even more complicated and difficult to train (Guo and Lu, 2018). (3) Seq2seq-based methods (Barzdins and Gosko, 2016; Peng et al., 2017; Konstas et al., 2017; van Noord and Bos, 2017) treat AMR parsing as sequence-to-sequence problem by linearizing AMR graphs, thus existing seq2seq models (Bahdanau et al., 2014; Luong et al., 2015) can be readily utilized. Despite its simplicity, the performance of the current seq2seq models lag behind when the training data is limited. The first reason is that seq2seq models are often not as effective on smaller datasets. The second reason is that the linearized AMRs add the challenges of making use of the graph structure information. There are also some notable exceptions. Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinatory categorical grammar"
D19-1393,P16-1154,0,0.0240877,"d on the parser state ht and the sentence representation s1:n , where ast,i denotes the attention weight of the word wi in the current time step. This component then updates the parser state with the alignment information via the following equation: ht = LN (ht + W conc n X ast,i si ) i=1 The probability of generating a specific concept c from the concept vocabulary P V is calculated as gen(c|ht ) = exp(xc T ht )/ c0 ∈V exp(xc0 T ht ), where xc (for c ∈ V) denotes the model parameters. To address the data sparsity issue in concept prediction, we introduce a copy mechanism in similar spirit to Gu et al. (2016). Besides generation, our model can either directly copy an input token wi (e.g, for entity names) or map wi to one concept m(wi ) according to the alignment statistics4 in the training data (e.g., for “went”, it would propose go). Formally, the prediction probability of a concept c is given by: P (c|ht ) =P (copy|ht ) n X ast,i [[wi = c]] i=1 +P (map|ht ) n X ast,i [[m(wi ) = c]] i=1 +P (gen|ht )gen(c|ht ) where [[. . .]] is the indicator function. P (copy|ht ), P (map|ht ) and P (gen|ht ) are the probabilities of three prediction modes respectively, computed by a single layer neural network"
D19-1393,P18-1037,0,0.159118,"discussions with Zhisong Zhang and Zhijiang Guo. and Clark, 2009) and dependency parsing (K¨ubler et al., 2009), AMR parsing is considered more challenging due to the following characteristics: (1) The nodes in AMR have no explicit alignment to text tokens; (2) The graph structure is more complicated because of frequent reentrancies and non-projective arcs; (3) There is a large and sparse vocabulary of possible node types (concepts). Many methods for AMR parsing have been developed in the past years, which can be categorized into three main classes: Graph-based parsing (Flanigan et al., 2014; Lyu and Titov, 2018) uses a pipeline design for concept identification and relation prediction. Transition-based parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017) processes a sentence from left-to-right and constructs the graph incrementally. The third class is seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017), which views parsing as sequence-to-sequence transduction by a linearization (depth-first traversal) of the AMR graph. 3799 Proceedings of the 2019 Conference on Empirical"
D19-1393,D18-1198,0,0.523928,"The nodes in AMR have no explicit alignment to text tokens; (2) The graph structure is more complicated because of frequent reentrancies and non-projective arcs; (3) There is a large and sparse vocabulary of possible node types (concepts). Many methods for AMR parsing have been developed in the past years, which can be categorized into three main classes: Graph-based parsing (Flanigan et al., 2014; Lyu and Titov, 2018) uses a pipeline design for concept identification and relation prediction. Transition-based parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017) processes a sentence from left-to-right and constructs the graph incrementally. The third class is seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017), which views parsing as sequence-to-sequence transduction by a linearization (depth-first traversal) of the AMR graph. 3799 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3799–3809, c Hong Kong, China, November 3–7, 2019. 2019 Association for C"
D19-1393,P14-5010,0,0.00391692,"adopt a beam search to approximate the best graph. Specifically, for each partially constructed graph, we only consider the top-K concepts obtaining the best single-step probability (a product of the corresponding concept, arc, and relation label probability), where K is the beam size. Only the best K graphs at each time step are kept for the next expansion. 5 5.1 Experiments Setup We focus on the most recent LDC2017T10 dataset, as it is the largest AMR corpus. It consists of 36521, 1368, and 1371 sentences in the training, development, and testing sets respectively. We use Stanford CoreNLP (Manning et al., 2014) for text preprocessing, including tokenization, lemmatization, part-of-speech, and namedentity tagging. The input for sentence encoder 3804 • Smatch-weighted: This metric weights different triples by their importance of composing the core ideas. The root distance d of a triple is defined as the minimum root distance of its involving nodes, the weight of the triple is then computed as: w = min(−d + dthr , 1) In other words, the weight has a linear decay in root distance until dthr . If two triples are matched, the minimum importance score of them is obtained. In our experiments, dthr is set to"
D19-1393,D18-1086,0,0.0319956,"ly struck.”, where the subgraphs close to the root represent the core semantics. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted labeled directed graph. As illustrated by an example in Figure 1, AMR abstracts away from the surface forms in text, where the root serves as a rudimentary representation of the overall focus while the details are elaborated as the depth of the graph increases. AMR has been proved useful for many downstream NLP tasks, including text summarization (Liu et al., 2015; Hardy and Vlachos, 2018) and question answering (Mitra and Baral, 2016). The task of AMR parsing is to map natural language strings to AMR semantic graphs automatically. Compared to constituent parsing (Zhang ∗ time ARG2 The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). The first author is grateful for the discussions with Zhisong Zhang and Zhijiang Guo. and Clark, 2009) and dependency parsing (K¨ubler et al., 2009), AMR parsing is considered more challenging due to the following charac"
D19-1393,K15-1004,0,0.103354,"Peng et al., 2017; Konstas et al., 2017; van Noord and Bos, 2017) treat AMR parsing as sequence-to-sequence problem by linearizing AMR graphs, thus existing seq2seq models (Bahdanau et al., 2014; Luong et al., 2015) can be readily utilized. Despite its simplicity, the performance of the current seq2seq models lag behind when the training data is limited. The first reason is that seq2seq models are often not as effective on smaller datasets. The second reason is that the linearized AMRs add the challenges of making use of the graph structure information. There are also some notable exceptions. Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinatory categorical grammar. Groschwitz et al. (2018); Lindemann et al. (2019) view AMR graphs as the structure AM algebra. Most AMR parsers require an explicit alignment between tokens in the sentences and nodes in the AMR graph during training. Since such information is not annotated, a pre-trained aligner (Flanigan et al., 2014; Pourdamghani et al., 2014; Liu et al., 2018) is often required. More recently, Lyu and Titov (20"
D19-1393,E17-1035,0,0.348517,"In addition, for computational efficacy, usually only first-order information is considered for edge scoring. (2) Transition-based methods (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Liu et al., 2018; Peng et al., 2018; Guo and Lu, 2018; Naseem et al., 2019) borrow techniques from shift-reduce dependency parsing. Yet the non-trivial nature of AMR graphs (e.g., reentrancies and non-projective arcs) makes the transition system even more complicated and difficult to train (Guo and Lu, 2018). (3) Seq2seq-based methods (Barzdins and Gosko, 2016; Peng et al., 2017; Konstas et al., 2017; van Noord and Bos, 2017) treat AMR parsing as sequence-to-sequence problem by linearizing AMR graphs, thus existing seq2seq models (Bahdanau et al., 2014; Luong et al., 2015) can be readily utilized. Despite its simplicity, the performance of the current seq2seq models lag behind when the training data is limited. The first reason is that seq2seq models are often not as effective on smaller datasets. The second reason is that the linearized AMRs add the challenges of making use of the graph structure information. There are also some notable exceptions. Peng et al. (2015"
D19-1393,D14-1048,0,0.0293378,"e of the graph structure information. There are also some notable exceptions. Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinatory categorical grammar. Groschwitz et al. (2018); Lindemann et al. (2019) view AMR graphs as the structure AM algebra. Most AMR parsers require an explicit alignment between tokens in the sentences and nodes in the AMR graph during training. Since such information is not annotated, a pre-trained aligner (Flanigan et al., 2014; Pourdamghani et al., 2014; Liu et al., 2018) is often required. More recently, Lyu and Titov (2018) demonstrate that the alignments can be treated as latent variables in a joint probabilistic model. 3 3.1 Background and Overview Background of Multi-head Attention The multi-head attention mechanism introduced by Vaswani et al. (2017) is used as a basic building block in our framework. The multi-head attention consists of H attention heads, and each of which learns a distinct attention function. Given a query vector x and a set of vectors {y1 , y2 , . . . , ym } or in short y1:m , for each attention head, we project x a"
D19-1393,P15-1095,0,0.198489,"Missing"
D19-1393,P19-1009,0,0.612399,"Missing"
D19-1393,W09-3825,0,0.106235,"Missing"
D19-1393,D16-1065,0,0.0836781,"rsers can be categorized into three classes: (1) Graph-based methods (Flanigan et al., 2014, 2016; Werling et al., 2015; Foland and Martin, 2017; Lyu and Titov, 2018; Zhang et al., 2019) adopt a pipeline approach for graph construction. It first maps continuous text spans into AMR concepts, then calculates the scores of possible edges and uses a max3800 imum spanning connected subgraph algorithm to select the final graph. The major deficiency is that the concept identification and relation prediction are strictly performed in order, yet the interactions between them should benefit both sides (Zhou et al., 2016). In addition, for computational efficacy, usually only first-order information is considered for edge scoring. (2) Transition-based methods (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Liu et al., 2018; Peng et al., 2018; Guo and Lu, 2018; Naseem et al., 2019) borrow techniques from shift-reduce dependency parsing. Yet the non-trivial nature of AMR graphs (e.g., reentrancies and non-projective arcs) makes the transition system even more complicated and difficult to train (Guo and Lu, 2018). (3) Seq2seq-based methods (Barzdins and Gosko, 2016;"
D19-1393,D15-1136,0,0.113026,"as sequence-to-sequence problem by linearizing AMR graphs, thus existing seq2seq models (Bahdanau et al., 2014; Luong et al., 2015) can be readily utilized. Despite its simplicity, the performance of the current seq2seq models lag behind when the training data is limited. The first reason is that seq2seq models are often not as effective on smaller datasets. The second reason is that the linearized AMRs add the challenges of making use of the graph structure information. There are also some notable exceptions. Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinatory categorical grammar. Groschwitz et al. (2018); Lindemann et al. (2019) view AMR graphs as the structure AM algebra. Most AMR parsers require an explicit alignment between tokens in the sentences and nodes in the AMR graph during training. Since such information is not annotated, a pre-trained aligner (Flanigan et al., 2014; Pourdamghani et al., 2014; Liu et al., 2018) is often required. More recently, Lyu and Titov (2018) demonstrate that the alignments can be treated as latent variables in a joint p"
D19-1393,D18-1548,0,0.0595626,"Missing"
D19-1393,S16-1181,0,0.730208,"ng is considered more challenging due to the following characteristics: (1) The nodes in AMR have no explicit alignment to text tokens; (2) The graph structure is more complicated because of frequent reentrancies and non-projective arcs; (3) There is a large and sparse vocabulary of possible node types (concepts). Many methods for AMR parsing have been developed in the past years, which can be categorized into three main classes: Graph-based parsing (Flanigan et al., 2014; Lyu and Titov, 2018) uses a pipeline design for concept identification and relation prediction. Transition-based parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017) processes a sentence from left-to-right and constructs the graph incrementally. The third class is seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017), which views parsing as sequence-to-sequence transduction by a linearization (depth-first traversal) of the AMR graph. 3799 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages"
D19-1393,D17-1129,0,0.63197,"lignment to text tokens; (2) The graph structure is more complicated because of frequent reentrancies and non-projective arcs; (3) There is a large and sparse vocabulary of possible node types (concepts). Many methods for AMR parsing have been developed in the past years, which can be categorized into three main classes: Graph-based parsing (Flanigan et al., 2014; Lyu and Titov, 2018) uses a pipeline design for concept identification and relation prediction. Transition-based parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017) processes a sentence from left-to-right and constructs the graph incrementally. The third class is seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017), which views parsing as sequence-to-sequence transduction by a linearization (depth-first traversal) of the AMR graph. 3799 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3799–3809, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics While existing"
D19-5505,N19-1423,0,0.205326,"agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient for capturing the complex semantic dependencies in the sentence. Meanwhile, the size of existing datasets is too small to train sophisticated task-specific architectures. Thus, introducing a context-aware word embedding3 layer pre-trained on large-scale datasets with deep LSTM (McCann et al., 2017; Peters et al., 2018; Howard and Ruder, 2018) or Transformer (Radford et al., 2018, 2019; Devlin et al., 2019; Lample ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). 1 Our code is open-source and available at: https:// github.com/lixin4ever/BERT-E2E-ABSA 2 Due to the limited space, we can not list all of the existing works here, please refer to the survey (Zhou et al., 2019) for more related papers. 3 In this paper, we generalize the concept of “word embedding” as a mapping between the word and the lowdimensional word representations. In this paper, we investigate t"
D19-5505,N19-1259,0,0.0378825,"al., 2016), from user-generated natural language texts (Liu, 2012). The most popular ABSA benchmark datasets are from SemEval ABSA challenges (Pontiki et al., 2014, 2015, 2016) where a few thousand review sentences with gold standard aspect sentiment annotations are provided. Table 1 summarizes three existing research problems related to ABSA. The first one is the original ABSA, aiming at predicting the sentiment polarity of the sentence towards the given aspect. Compared to this classification problem, the second one and the third one, namely, Aspectoriented Opinion Words Extraction (AOWE) (Fan et al., 2019) and End-to-End Aspect-based Sentiment Analysis (E2E-ABSA) (Ma et al., 2018a; Schmitt et al., 2018; Li et al., 2019a; Li and Lu, 2017, 2019), are related to a sequence tagging problem. Precisely, the goal of AOWE is to extract the aspect-specific opinion words from the sentence given the aspect. The goal of E2E-ABSA is to jointly detect aspect terms/categories and the corresponding aspect sentiments. Many neural models composed of a taskagnostic pre-trained word embedding layer and task-specific neural architecture have been proposed for the original ABSA task (i.e. the aspectlevel sentiment c"
D19-5505,P19-1048,0,0.453897,"RF layer on top of the BERT embedding layer. Different from the above mentioned neural models maximizing the token-level likelihood p(yt |xt ), the CRF-based model aims to find the globally most   rt T = σ(L N(Wx hL t ) + L N (Wh ht−1 )) zt T nt = tanh(L N(Wxn hL t ) + rt ∗ L N (Whn ht−1 )) (4) (3) hTt = (1 − zt ) ∗ nt + zt ∗ hTt−1 where σ is the sigmoid activation function and rt , zt , nt respectively denote the reset gate, update gate and new gate. Wx , Wh ∈ R2dimh ×dimh , Wxn , Whn ∈ Rdimh ×dimh are the parameters of 36 Model 2019a) LSTM-CRF BERT Models (Li et al., (Luo et al., 2019) (He et al., 2019) (Lample et al., 2016)] (Ma and Hovy, 2016)] (Liu et al., 2018)] BERT+Linear BERT+GRU BERT+SAN BERT+TFM BERT+CRF Table 2: Main results. The symbol retrieved from Li et al. (2019a). Dataset # sent LAPTOP # aspect # sent REST # aspect Train 2741 2041 3490 3893  Dev 304 256 387 413 LAPTOP R 54.89 50.47 51.26 59.40 58.90 60.47 58.71 58.64 59.49 F1 57.90 60.35 58.37 54.24 54.71 56.19 60.43 61.12 60.49 60.80 60.78 P 68.64 66.10 61.56 68.46 71.42 70.61 72.92 72.39 71.88 REST R 71.01 66.30 67.26 64.43 75.25 76.20 76.72 76.64 76.48 F1 69.80 72.78 66.20 64.29 66.38 73.22 73.24 74.72 74.41 74.06 denote"
D19-5505,P18-1031,0,0.0318527,"y or F1 score has reached a bottleneck. One reason is that the task-agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient for capturing the complex semantic dependencies in the sentence. Meanwhile, the size of existing datasets is too small to train sophisticated task-specific architectures. Thus, introducing a context-aware word embedding3 layer pre-trained on large-scale datasets with deep LSTM (McCann et al., 2017; Peters et al., 2018; Howard and Ruder, 2018) or Transformer (Radford et al., 2018, 2019; Devlin et al., 2019; Lample ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). 1 Our code is open-source and available at: https:// github.com/lixin4ever/BERT-E2E-ABSA 2 Due to the limited space, we can not list all of the existing works here, please refer to the survey (Zhou et al., 2019) for more related papers. 3 In this paper, we generalize the concept of “word embedding” as a mapping between the word and the lowd"
D19-5505,P19-1051,0,0.40859,"itional Word2Vec- or GloVebased embedding layer which only provides a single context-independent representation for each token, the BERT embedding layer takes the sentence as input and calculates the token-level representations using the information from the entire sentence. First of all, we pack the input features as H 0 = {e1 , · · · , eT }, where et (t ∈ [1, T ]) is 4 Both of ABSA and AOWE assume that the aspects in a sentence are given. Such setting makes them less practical in real-world scenarios since manual annotation of the finegrained aspect mentions/categories is quite expensive. 5 Hu et al. (2019) introduce BERT to handle the E2EABSA problem but their focus is to design a task-specific architecture rather than exploring the potential of BERT. 35 the combination of the token embedding, position embedding and segment embedding corresponding to the input token xt . Then L transformer layers are introduced to refine the token-level features layer by layer. Specifically, the representations H l = {hl1 , · · · , hlT } at the l-th (l ∈ [1, L]) layer are calculated below: GRU. Since directly applying RNN on the output of transformer, namely, the BERT representation hL t , may lead to unstable"
D19-5505,D18-1136,0,0.0504621,"aspect-specific opinion words from the sentence given the aspect. The goal of E2E-ABSA is to jointly detect aspect terms/categories and the corresponding aspect sentiments. Many neural models composed of a taskagnostic pre-trained word embedding layer and task-specific neural architecture have been proposed for the original ABSA task (i.e. the aspectlevel sentiment classification) (Tang et al., 2016; Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Ma et al., 2017, 2018b; Majumder et al., 2018; Li et al., 2018; He et al., 2018; Xue and Li, 2018; Wang et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Lei et al., 2019; Li et al., 2019b)2 , but the improvement of these models measured by the accuracy or F1 score has reached a bottleneck. One reason is that the task-agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient for capturing the complex semantic dependencies in the sentence. Meanwhile, the size of existing datasets is too small to train sophisticated task-specific architectures. Thus, introducing a context-aware word embedding3"
D19-5505,E17-2091,0,0.0203864,"18a; Schmitt et al., 2018; Li et al., 2019a; Li and Lu, 2017, 2019), are related to a sequence tagging problem. Precisely, the goal of AOWE is to extract the aspect-specific opinion words from the sentence given the aspect. The goal of E2E-ABSA is to jointly detect aspect terms/categories and the corresponding aspect sentiments. Many neural models composed of a taskagnostic pre-trained word embedding layer and task-specific neural architecture have been proposed for the original ABSA task (i.e. the aspectlevel sentiment classification) (Tang et al., 2016; Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Ma et al., 2017, 2018b; Majumder et al., 2018; Li et al., 2018; He et al., 2018; Xue and Li, 2018; Wang et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Lei et al., 2019; Li et al., 2019b)2 , but the improvement of these models measured by the accuracy or F1 score has reached a bottleneck. One reason is that the task-agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient for capturing the complex semantic dependencies in the senten"
D19-5505,D19-1549,0,0.0374129,"Missing"
D19-5505,P19-1056,0,0.685331,"uce a linear-chain CRF layer on top of the BERT embedding layer. Different from the above mentioned neural models maximizing the token-level likelihood p(yt |xt ), the CRF-based model aims to find the globally most   rt T = σ(L N(Wx hL t ) + L N (Wh ht−1 )) zt T nt = tanh(L N(Wxn hL t ) + rt ∗ L N (Whn ht−1 )) (4) (3) hTt = (1 − zt ) ∗ nt + zt ∗ hTt−1 where σ is the sigmoid activation function and rt , zt , nt respectively denote the reset gate, update gate and new gate. Wx , Wh ∈ R2dimh ×dimh , Wxn , Whn ∈ Rdimh ×dimh are the parameters of 36 Model 2019a) LSTM-CRF BERT Models (Li et al., (Luo et al., 2019) (He et al., 2019) (Lample et al., 2016)] (Ma and Hovy, 2016)] (Liu et al., 2018)] BERT+Linear BERT+GRU BERT+SAN BERT+TFM BERT+CRF Table 2: Main results. The symbol retrieved from Li et al. (2019a). Dataset # sent LAPTOP # aspect # sent REST # aspect Train 2741 2041 3490 3893  Dev 304 256 387 413 LAPTOP R 54.89 50.47 51.26 59.40 58.90 60.47 58.71 58.64 59.49 F1 57.90 60.35 58.37 54.24 54.71 56.19 60.43 61.12 60.49 60.80 60.78 P 68.64 66.10 61.56 68.46 71.42 70.61 72.92 72.39 71.88 REST R 71.01 66.30 67.26 64.43 75.25 76.20 76.72 76.64 76.48 F1 69.80 72.78 66.20 64.29 66.38 73.22 73.24 74.72"
D19-5505,D18-1504,0,0.120861,"popular ABSA benchmark datasets are from SemEval ABSA challenges (Pontiki et al., 2014, 2015, 2016) where a few thousand review sentences with gold standard aspect sentiment annotations are provided. Table 1 summarizes three existing research problems related to ABSA. The first one is the original ABSA, aiming at predicting the sentiment polarity of the sentence towards the given aspect. Compared to this classification problem, the second one and the third one, namely, Aspectoriented Opinion Words Extraction (AOWE) (Fan et al., 2019) and End-to-End Aspect-based Sentiment Analysis (E2E-ABSA) (Ma et al., 2018a; Schmitt et al., 2018; Li et al., 2019a; Li and Lu, 2017, 2019), are related to a sequence tagging problem. Precisely, the goal of AOWE is to extract the aspect-specific opinion words from the sentence given the aspect. The goal of E2E-ABSA is to jointly detect aspect terms/categories and the corresponding aspect sentiments. Many neural models composed of a taskagnostic pre-trained word embedding layer and task-specific neural architecture have been proposed for the original ABSA task (i.e. the aspectlevel sentiment classification) (Tang et al., 2016; Wang et al., 2016; Chen et al., 2017; Li"
D19-5505,N16-1030,0,0.485322,"fowicz et al. (2015). The computational formula of the task-specific hidden representation hTt ∈ Rdimh at the t-th time step is shown below: (6) where F FN refers to the point-wise feed-forward networks (Vaswani et al., 2017). Again, a linear layer with softmax activation is stacked on the designed SAN/TFM layer to output the predictions (same with that in Eq(4)). Conditional Random Fields Conditional Random Fields (CRF) (Lafferty et al., 2001) is effective in sequence modeling and has been widely adopted for solving the sequence labeling tasks together with neural models (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). In this paper, we introduce a linear-chain CRF layer on top of the BERT embedding layer. Different from the above mentioned neural models maximizing the token-level likelihood p(yt |xt ), the CRF-based model aims to find the globally most   rt T = σ(L N(Wx hL t ) + L N (Wh ht−1 )) zt T nt = tanh(L N(Wxn hL t ) + rt ∗ L N (Whn ht−1 )) (4) (3) hTt = (1 − zt ) ∗ nt + zt ∗ hTt−1 where σ is the sigmoid activation function and rt , zt , nt respectively denote the reset gate, update gate and new gate. Wx , Wh ∈ R2dimh ×dimh , Wxn , Whn ∈ Rdimh ×dimh are the parameters of 36 Mo"
D19-5505,P16-1101,0,0.225002,"The computational formula of the task-specific hidden representation hTt ∈ Rdimh at the t-th time step is shown below: (6) where F FN refers to the point-wise feed-forward networks (Vaswani et al., 2017). Again, a linear layer with softmax activation is stacked on the designed SAN/TFM layer to output the predictions (same with that in Eq(4)). Conditional Random Fields Conditional Random Fields (CRF) (Lafferty et al., 2001) is effective in sequence modeling and has been widely adopted for solving the sequence labeling tasks together with neural models (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). In this paper, we introduce a linear-chain CRF layer on top of the BERT embedding layer. Different from the above mentioned neural models maximizing the token-level likelihood p(yt |xt ), the CRF-based model aims to find the globally most   rt T = σ(L N(Wx hL t ) + L N (Wh ht−1 )) zt T nt = tanh(L N(Wxn hL t ) + rt ∗ L N (Whn ht−1 )) (4) (3) hTt = (1 − zt ) ∗ nt + zt ∗ hTt−1 where σ is the sigmoid activation function and rt , zt , nt respectively denote the reset gate, update gate and new gate. Wx , Wh ∈ R2dimh ×dimh , Wxn , Whn ∈ Rdimh ×dimh are the parameters of 36 Model 2019a) LSTM-CRF"
D19-5505,D18-1139,0,0.0318697,"hmark datasets are from SemEval ABSA challenges (Pontiki et al., 2014, 2015, 2016) where a few thousand review sentences with gold standard aspect sentiment annotations are provided. Table 1 summarizes three existing research problems related to ABSA. The first one is the original ABSA, aiming at predicting the sentiment polarity of the sentence towards the given aspect. Compared to this classification problem, the second one and the third one, namely, Aspectoriented Opinion Words Extraction (AOWE) (Fan et al., 2019) and End-to-End Aspect-based Sentiment Analysis (E2E-ABSA) (Ma et al., 2018a; Schmitt et al., 2018; Li et al., 2019a; Li and Lu, 2017, 2019), are related to a sequence tagging problem. Precisely, the goal of AOWE is to extract the aspect-specific opinion words from the sentence given the aspect. The goal of E2E-ABSA is to jointly detect aspect terms/categories and the corresponding aspect sentiments. Many neural models composed of a taskagnostic pre-trained word embedding layer and task-specific neural architecture have been proposed for the original ABSA task (i.e. the aspectlevel sentiment classification) (Tang et al., 2016; Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Ma e"
D19-5505,D13-1171,0,0.300659,"Missing"
D19-5505,N19-1035,0,0.0404263,"L L h h 5 6 L h 7 L h 8 L h 9 L h 10 BERT L-th Transformer Layer Table 1: Different problem settings in ABSA. Gold standard aspects and opinions are wrapped in [] and &lt;> respectively. The subscripts N and P refer to aspect sentiment. Underline :* or * indicates the association between the aspect and the opinion. ⋯ 1-st Transformer Layer Segment Embedding Position Embedding Token Embedding and Conneau, 2019; Yang et al., 2019; Dong et al., 2019) for fine-tuning a lightweight task-specific network using the labeled data has good potential for further enhancing the performance. Xu et al. (2019); Sun et al. (2019); Song et al. (2019); Yu and Jiang (2019); Rietzler et al. (2019); Huang and Carley (2019) have conducted some initial attempts to couple the deep contextualized word embedding layer with downstream neural models for the original ABSA task and establish the new state-of-the-art results. It encourages us to explore the potential of using such contextualized embeddings to the more difficult but practical task, i.e. E2E-ABSA (the third setting in Table 1).4 Note that we are not aiming at developing a task-specific architecture, instead, our focus is to examine the potential of contextualized embe"
D19-5505,D14-1162,0,0.0900216,"en proposed for the original ABSA task (i.e. the aspectlevel sentiment classification) (Tang et al., 2016; Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Ma et al., 2017, 2018b; Majumder et al., 2018; Li et al., 2018; He et al., 2018; Xue and Li, 2018; Wang et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Lei et al., 2019; Li et al., 2019b)2 , but the improvement of these models measured by the accuracy or F1 score has reached a bottleneck. One reason is that the task-agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient for capturing the complex semantic dependencies in the sentence. Meanwhile, the size of existing datasets is too small to train sophisticated task-specific architectures. Thus, introducing a context-aware word embedding3 layer pre-trained on large-scale datasets with deep LSTM (McCann et al., 2017; Peters et al., 2018; Howard and Ruder, 2018) or Transformer (Radford et al., 2018, 2019; Devlin et al., 2019; Lample ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council"
D19-5505,D16-1021,0,0.03627,"Aspect-based Sentiment Analysis (E2E-ABSA) (Ma et al., 2018a; Schmitt et al., 2018; Li et al., 2019a; Li and Lu, 2017, 2019), are related to a sequence tagging problem. Precisely, the goal of AOWE is to extract the aspect-specific opinion words from the sentence given the aspect. The goal of E2E-ABSA is to jointly detect aspect terms/categories and the corresponding aspect sentiments. Many neural models composed of a taskagnostic pre-trained word embedding layer and task-specific neural architecture have been proposed for the original ABSA task (i.e. the aspectlevel sentiment classification) (Tang et al., 2016; Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Ma et al., 2017, 2018b; Majumder et al., 2018; Li et al., 2018; He et al., 2018; Xue and Li, 2018; Wang et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Lei et al., 2019; Li et al., 2019b)2 , but the improvement of these models measured by the accuracy or F1 score has reached a bottleneck. One reason is that the task-agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient fo"
D19-5505,N18-1202,0,0.0426857,"asured by the accuracy or F1 score has reached a bottleneck. One reason is that the task-agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient for capturing the complex semantic dependencies in the sentence. Meanwhile, the size of existing datasets is too small to train sophisticated task-specific architectures. Thus, introducing a context-aware word embedding3 layer pre-trained on large-scale datasets with deep LSTM (McCann et al., 2017; Peters et al., 2018; Howard and Ruder, 2018) or Transformer (Radford et al., 2018, 2019; Devlin et al., 2019; Lample ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). 1 Our code is open-source and available at: https:// github.com/lixin4ever/BERT-E2E-ABSA 2 Due to the limited space, we can not list all of the existing works here, please refer to the survey (Zhou et al., 2019) for more related papers. 3 In this paper, we generalize the concept of “word embedding” as a mapping betw"
D19-5505,P18-1088,0,0.036111,"y, the goal of AOWE is to extract the aspect-specific opinion words from the sentence given the aspect. The goal of E2E-ABSA is to jointly detect aspect terms/categories and the corresponding aspect sentiments. Many neural models composed of a taskagnostic pre-trained word embedding layer and task-specific neural architecture have been proposed for the original ABSA task (i.e. the aspectlevel sentiment classification) (Tang et al., 2016; Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Ma et al., 2017, 2018b; Majumder et al., 2018; Li et al., 2018; He et al., 2018; Xue and Li, 2018; Wang et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Lei et al., 2019; Li et al., 2019b)2 , but the improvement of these models measured by the accuracy or F1 score has reached a bottleneck. One reason is that the task-agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient for capturing the complex semantic dependencies in the sentence. Meanwhile, the size of existing datasets is too small to train sophisticated task-specific architectures. Thus, in"
D19-5505,S15-2082,0,0.474457,"Missing"
D19-5505,D16-1058,0,0.340573,"BERT for End-to-End Aspect-based Sentiment Analysis∗ Xin Li1 , Lidong Bing2 , Wenxuan Zhang1 and Wai Lam1 1 Department of Systems Engineering and Engineering Management The Chinese University of Hong Kong, Hong Kong 2 R&D Center Singapore, Machine Intelligence Technology, Alibaba DAMO Academy {lixin,wxzhang,wlam}@se.cuhk.edu.hk l.bing@alibaba-inc.com Abstract Aspect-based sentiment analysis (ABSA) is to discover the users’ sentiment or opinion towards an aspect, usually in the form of explicitly mentioned aspect terms (Mitchell et al., 2013; Zhang et al., 2015) or implicit aspect categories (Wang et al., 2016), from user-generated natural language texts (Liu, 2012). The most popular ABSA benchmark datasets are from SemEval ABSA challenges (Pontiki et al., 2014, 2015, 2016) where a few thousand review sentences with gold standard aspect sentiment annotations are provided. Table 1 summarizes three existing research problems related to ABSA. The first one is the original ABSA, aiming at predicting the sentiment polarity of the sentence towards the given aspect. Compared to this classification problem, the second one and the third one, namely, Aspectoriented Opinion Words Extraction (AOWE) (Fan et al.,"
D19-5505,S14-2004,0,0.704498,"neering Management The Chinese University of Hong Kong, Hong Kong 2 R&D Center Singapore, Machine Intelligence Technology, Alibaba DAMO Academy {lixin,wxzhang,wlam}@se.cuhk.edu.hk l.bing@alibaba-inc.com Abstract Aspect-based sentiment analysis (ABSA) is to discover the users’ sentiment or opinion towards an aspect, usually in the form of explicitly mentioned aspect terms (Mitchell et al., 2013; Zhang et al., 2015) or implicit aspect categories (Wang et al., 2016), from user-generated natural language texts (Liu, 2012). The most popular ABSA benchmark datasets are from SemEval ABSA challenges (Pontiki et al., 2014, 2015, 2016) where a few thousand review sentences with gold standard aspect sentiment annotations are provided. Table 1 summarizes three existing research problems related to ABSA. The first one is the original ABSA, aiming at predicting the sentiment polarity of the sentence towards the given aspect. Compared to this classification problem, the second one and the third one, namely, Aspectoriented Opinion Words Extraction (AOWE) (Fan et al., 2019) and End-to-End Aspect-based Sentiment Analysis (E2E-ABSA) (Ma et al., 2018a; Schmitt et al., 2018; Li et al., 2019a; Li and Lu, 2017, 2019), are r"
D19-5505,N19-1242,0,0.0347135,"L h 2 L h 3 L h 4 L L h h 5 6 L h 7 L h 8 L h 9 L h 10 BERT L-th Transformer Layer Table 1: Different problem settings in ABSA. Gold standard aspects and opinions are wrapped in [] and &lt;> respectively. The subscripts N and P refer to aspect sentiment. Underline :* or * indicates the association between the aspect and the opinion. ⋯ 1-st Transformer Layer Segment Embedding Position Embedding Token Embedding and Conneau, 2019; Yang et al., 2019; Dong et al., 2019) for fine-tuning a lightweight task-specific network using the labeled data has good potential for further enhancing the performance. Xu et al. (2019); Sun et al. (2019); Song et al. (2019); Yu and Jiang (2019); Rietzler et al. (2019); Huang and Carley (2019) have conducted some initial attempts to couple the deep contextualized word embedding layer with downstream neural models for the original ABSA task and establish the new state-of-the-art results. It encourages us to explore the potential of using such contextualized embeddings to the more difficult but practical task, i.e. E2E-ABSA (the third setting in Table 1).4 Note that we are not aiming at developing a task-specific architecture, instead, our focus is to examine the potential of"
D19-5505,P18-1234,0,0.0213883,"problem. Precisely, the goal of AOWE is to extract the aspect-specific opinion words from the sentence given the aspect. The goal of E2E-ABSA is to jointly detect aspect terms/categories and the corresponding aspect sentiments. Many neural models composed of a taskagnostic pre-trained word embedding layer and task-specific neural architecture have been proposed for the original ABSA task (i.e. the aspectlevel sentiment classification) (Tang et al., 2016; Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Ma et al., 2017, 2018b; Majumder et al., 2018; Li et al., 2018; He et al., 2018; Xue and Li, 2018; Wang et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Lei et al., 2019; Li et al., 2019b)2 , but the improvement of these models measured by the accuracy or F1 score has reached a bottleneck. One reason is that the task-agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient for capturing the complex semantic dependencies in the sentence. Meanwhile, the size of existing datasets is too small to train sophisticated task-specific arch"
D19-5505,D15-1073,0,0.215456,"Missing"
D19-5505,D14-1179,0,\N,Missing
D19-5505,D16-1053,0,\N,Missing
D19-5505,D17-1047,1,\N,Missing
D19-5505,D18-1380,0,\N,Missing
D19-5505,D18-1377,0,\N,Missing
D19-5505,P18-1008,0,\N,Missing
D19-5505,D19-1550,0,\N,Missing
D19-5505,K19-1091,0,\N,Missing
I08-1044,J95-4004,0,0.24471,"Missing"
I08-1044,W06-0130,0,0.106022,"f real-world tasks, including NP chunking (Sha and Pereira, 2003), Chinese word segmentation (Peng et al., 2004), information extraction (Pinto et al., 2003; Peng and McCallum, 2004), named entity identification (McCallum and Li, 2003; Settles, 2004), and many others. 1 In this paper we only focus on PERs, LOCs and ORGs. Since temporal, numerical and monetary phrases can be well identified with rule-based approaches. 336 Recently, CRFs have been shown to perform exceptionally well on Chinese NER shared task on the third SIGHAN Chinese language processing bakeoff (SIGHAN06) (Zhou et al., 2006; Chen et al., 2006b,a). We follow the state-of-the-art CRF models using features that have been shown to be very effective in Chinese NER, namely the current character and its part-of-speech (POS) tag, several characters surrounding (both before and after) the current character and their POS tags, current word and several words surrounding the current word. We also observe some important issues that significantly influence the performance as follows: Window size: The primitive window size we use is 5 ( 2 characters preceding the current character and 2 following the current character). We extend the window size"
I08-1044,W06-0116,0,0.0772478,"f real-world tasks, including NP chunking (Sha and Pereira, 2003), Chinese word segmentation (Peng et al., 2004), information extraction (Pinto et al., 2003; Peng and McCallum, 2004), named entity identification (McCallum and Li, 2003; Settles, 2004), and many others. 1 In this paper we only focus on PERs, LOCs and ORGs. Since temporal, numerical and monetary phrases can be well identified with rule-based approaches. 336 Recently, CRFs have been shown to perform exceptionally well on Chinese NER shared task on the third SIGHAN Chinese language processing bakeoff (SIGHAN06) (Zhou et al., 2006; Chen et al., 2006b,a). We follow the state-of-the-art CRF models using features that have been shown to be very effective in Chinese NER, namely the current character and its part-of-speech (POS) tag, several characters surrounding (both before and after) the current character and their POS tags, current word and several words surrounding the current word. We also observe some important issues that significantly influence the performance as follows: Window size: The primitive window size we use is 5 ( 2 characters preceding the current character and 2 following the current character). We extend the window size"
I08-1044,C02-1054,0,0.0605491,"Missing"
I08-1044,W03-0430,0,0.0174935,"y of the desired outputs given the corresponding inputs. CRFs have the great flexibility to encode a wide variety of arbitrary, non-independent features and to straightforwardly combine rich domain knowledge. Furthermore, they are discriminatively trained, and are often more accurate than generative models, even with the same features. CRFs have been successfully applied to a number of real-world tasks, including NP chunking (Sha and Pereira, 2003), Chinese word segmentation (Peng et al., 2004), information extraction (Pinto et al., 2003; Peng and McCallum, 2004), named entity identification (McCallum and Li, 2003; Settles, 2004), and many others. 1 In this paper we only focus on PERs, LOCs and ORGs. Since temporal, numerical and monetary phrases can be well identified with rule-based approaches. 336 Recently, CRFs have been shown to perform exceptionally well on Chinese NER shared task on the third SIGHAN Chinese language processing bakeoff (SIGHAN06) (Zhou et al., 2006; Chen et al., 2006b,a). We follow the state-of-the-art CRF models using features that have been shown to be very effective in Chinese NER, namely the current character and its part-of-speech (POS) tag, several characters surrounding (b"
I08-1044,N04-1042,0,0.0149693,"l models trained to maximize the conditional probability of the desired outputs given the corresponding inputs. CRFs have the great flexibility to encode a wide variety of arbitrary, non-independent features and to straightforwardly combine rich domain knowledge. Furthermore, they are discriminatively trained, and are often more accurate than generative models, even with the same features. CRFs have been successfully applied to a number of real-world tasks, including NP chunking (Sha and Pereira, 2003), Chinese word segmentation (Peng et al., 2004), information extraction (Pinto et al., 2003; Peng and McCallum, 2004), named entity identification (McCallum and Li, 2003; Settles, 2004), and many others. 1 In this paper we only focus on PERs, LOCs and ORGs. Since temporal, numerical and monetary phrases can be well identified with rule-based approaches. 336 Recently, CRFs have been shown to perform exceptionally well on Chinese NER shared task on the third SIGHAN Chinese language processing bakeoff (SIGHAN06) (Zhou et al., 2006; Chen et al., 2006b,a). We follow the state-of-the-art CRF models using features that have been shown to be very effective in Chinese NER, namely the current character and its part-of"
I08-1044,C04-1081,0,0.0150328,"om Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models trained to maximize the conditional probability of the desired outputs given the corresponding inputs. CRFs have the great flexibility to encode a wide variety of arbitrary, non-independent features and to straightforwardly combine rich domain knowledge. Furthermore, they are discriminatively trained, and are often more accurate than generative models, even with the same features. CRFs have been successfully applied to a number of real-world tasks, including NP chunking (Sha and Pereira, 2003), Chinese word segmentation (Peng et al., 2004), information extraction (Pinto et al., 2003; Peng and McCallum, 2004), named entity identification (McCallum and Li, 2003; Settles, 2004), and many others. 1 In this paper we only focus on PERs, LOCs and ORGs. Since temporal, numerical and monetary phrases can be well identified with rule-based approaches. 336 Recently, CRFs have been shown to perform exceptionally well on Chinese NER shared task on the third SIGHAN Chinese language processing bakeoff (SIGHAN06) (Zhou et al., 2006; Chen et al., 2006b,a). We follow the state-of-the-art CRF models using features that have been shown to be very"
I08-1044,W04-1221,0,0.015105,"s given the corresponding inputs. CRFs have the great flexibility to encode a wide variety of arbitrary, non-independent features and to straightforwardly combine rich domain knowledge. Furthermore, they are discriminatively trained, and are often more accurate than generative models, even with the same features. CRFs have been successfully applied to a number of real-world tasks, including NP chunking (Sha and Pereira, 2003), Chinese word segmentation (Peng et al., 2004), information extraction (Pinto et al., 2003; Peng and McCallum, 2004), named entity identification (McCallum and Li, 2003; Settles, 2004), and many others. 1 In this paper we only focus on PERs, LOCs and ORGs. Since temporal, numerical and monetary phrases can be well identified with rule-based approaches. 336 Recently, CRFs have been shown to perform exceptionally well on Chinese NER shared task on the third SIGHAN Chinese language processing bakeoff (SIGHAN06) (Zhou et al., 2006; Chen et al., 2006b,a). We follow the state-of-the-art CRF models using features that have been shown to be very effective in Chinese NER, namely the current character and its part-of-speech (POS) tag, several characters surrounding (both before and a"
I08-1044,N03-1028,0,0.0298891,"Art 2.1 CRF Model for Chinese NER Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models trained to maximize the conditional probability of the desired outputs given the corresponding inputs. CRFs have the great flexibility to encode a wide variety of arbitrary, non-independent features and to straightforwardly combine rich domain knowledge. Furthermore, they are discriminatively trained, and are often more accurate than generative models, even with the same features. CRFs have been successfully applied to a number of real-world tasks, including NP chunking (Sha and Pereira, 2003), Chinese word segmentation (Peng et al., 2004), information extraction (Pinto et al., 2003; Peng and McCallum, 2004), named entity identification (McCallum and Li, 2003; Settles, 2004), and many others. 1 In this paper we only focus on PERs, LOCs and ORGs. Since temporal, numerical and monetary phrases can be well identified with rule-based approaches. 336 Recently, CRFs have been shown to perform exceptionally well on Chinese NER shared task on the third SIGHAN Chinese language processing bakeoff (SIGHAN06) (Zhou et al., 2006; Chen et al., 2006b,a). We follow the state-of-the-art CRF models"
I08-1044,H05-1054,0,0.0273055,"cant as p &lt; 0.001. and NER as a single tagging task on a sequence of known words. Although lexicalized HMMs was shown to be superior to standard HMMs, this approach has some disadvantages: it is a purely statistical model and it suffers from the problem of data sparseness. And the model fails to tag some complicated NEs (e.g., nested ORGs) correctly due to lack of domain adaptive techniques. The F-measures of LOCs and ORGs are only 87.13 and 83.60, which show that there is still a room for improving. A method of incorporating heuristic human knowledge into a statistical model was proposed in (Wu et al., 2005). Here Chinese NER was regarded as a probabilistic tagging problem and the heuristic human knowledge was used to reduce the searching space. However, this method assumes that POS tags are golden-standard in the training data and heuristic human knowledge is often ad hoc. These drawbacks make the method unstable and highly sensitive to POS errors; and when golden-standard POS tags are not available (this is often the case), it may degrade the performance. Cohen and Sarawagi (2004) proposed a semi-Markov model which combines a Markovian, HMM-like extraction process and a dictionary component. Th"
I08-1044,W06-0124,1,0.894915,"Missing"
I08-1044,W03-1709,0,0.0683713,"Missing"
I08-1044,P02-1060,0,0.110082,"Missing"
I08-1044,W06-0140,0,0.127535,"plied to a number of real-world tasks, including NP chunking (Sha and Pereira, 2003), Chinese word segmentation (Peng et al., 2004), information extraction (Pinto et al., 2003; Peng and McCallum, 2004), named entity identification (McCallum and Li, 2003; Settles, 2004), and many others. 1 In this paper we only focus on PERs, LOCs and ORGs. Since temporal, numerical and monetary phrases can be well identified with rule-based approaches. 336 Recently, CRFs have been shown to perform exceptionally well on Chinese NER shared task on the third SIGHAN Chinese language processing bakeoff (SIGHAN06) (Zhou et al., 2006; Chen et al., 2006b,a). We follow the state-of-the-art CRF models using features that have been shown to be very effective in Chinese NER, namely the current character and its part-of-speech (POS) tag, several characters surrounding (both before and after) the current character and their POS tags, current word and several words surrounding the current word. We also observe some important issues that significantly influence the performance as follows: Window size: The primitive window size we use is 5 ( 2 characters preceding the current character and 2 following the current character). We ext"
I08-1044,W03-0423,0,\N,Missing
I08-2078,W04-1217,0,0.0836951,"Missing"
I08-2078,W04-1213,0,0.0197117,"he two tasks is O(max(Ns2 , Nc2 )), which is much smaller than the single-phase approach in which the total number of features is O((Ns Nc )2 ). Another potential advantage of dividing the NER task into two tasks is that it allows greater flexibility in choosing an appropriate set of features for each task. In fact, adding more features may not necessarily increase performance. (Settles, 2004) reported that a system using a subset of features outperformed one using a full set of features. 5 Experiments We conducted our experiments on the GENIA corpus (Kim et al., 2003) provided in the JNLPBA (Kim et al., 2004) shared task1 . There are 2,000 MEDLINE abstracts in the GENIA corpus with named entities tagged in the IOB2 format. There are 18,546 sentences and 492,551 words in the training set, and 3,856 sentences and 101,039 words in the evaluation set. The line indicating the MEDLINE abstract ID boundary information is not used in our experiments. Each word is tagged with “B-X”, “I-X”, or “O” to indicate that the word is at the “beginning” (B) or “inside” (I) of a named entity of type X, or 1 http://research.nii.ac.jp/∼collier/ workshops/JNLPBA04st.htm F1 72.55 72.16 71.48 71.19 70.06 69.80 “outside” ("
I08-2078,I05-1057,0,0.0303612,"Missing"
I08-2078,N03-1028,0,0.142423,"Missing"
I08-2078,W04-1219,0,0.0785844,"Missing"
I08-2078,W02-2018,0,0.0986473,"Missing"
I08-2078,P05-1012,0,0.0851523,"Missing"
I08-2078,P06-1059,0,0.0481511,"Missing"
I08-2078,W04-1221,0,0.158051,"rder independence assumption made in a linear-chained CRF (Lafferty et al., 2001) model and calculate the best score using the Viterbi algorithm. 4 Cascaded Framework We divide the NER task into a segmentation task and a classification task. In the segmentation task, a sentence x is segmented, and possible segments of biomedical named entities are identified. In the classification task, the identified segments are classified into one of the possible named entity types or rejected. 596 System (Zhou and Su, 2004) Online Cascaded (Okanohara et al., 2006) (Kim et al., 2005) (Finkel et al., 2004) (Settles, 2004) In other words, in the segmentation task, the sentence x are segmented by y ˆs = argmax ws · Fs (x, y′ ) (11) y′ where Fs (·) is the set of segment features, and ws is the parameter for segmentation. In the classification task, the segments (which can be identified by ys ) in a sentence x are classified by y ˆc = argmax wc · Fc (x, ys , y′ ) Table 1: Comparisons with other systems on overall performance (in percentage). (12) y′ where Fc (·) is the set of classification features, and wc is the parameter for classification. In this cascaded framework, the number of possible labels in the segmen"
I08-2078,W02-1001,0,\N,Missing
I08-2078,P02-1062,0,\N,Missing
I08-4016,W06-0130,0,0.0731395,"CUHK 4179/03E, CUHK4193/04E, and CUHK4128/07) and the Direct Grant of the Faculty of Engineering, CUHK (Project Codes: 2050363 and 2050391). This work is also affiliated with the Microsoft-CUHK Joint Laboratory for Human-centric Computing and Interface Technologies. 102 Conditional Random Fields as Base Model Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models trained to maximize the conditional probability of the desired outputs given the corresponding inputs. CRFs have been shown to perform well on Chinese NER shared task on SIGHAN-4 (Zhou et al. (2006), Chen et al. (2006a), Chen et al. (2006b)). We employ CRFs as the base model in our framework. In this base model, we design features similar to the state-of-theart CRF models for Chinese NER. We use character features, word segmentation features, part-of-speech (POS) features, and dictionary features, as described below. Character features: These features are the current character, 2 characters preceding the current character and 2 following the current character. We extend the window size to 7 but find that it slightly hurts. The reason is that CRFs can deal with non-independent features. A larger window size"
I08-4016,W06-0116,0,0.0232016,"CUHK 4179/03E, CUHK4193/04E, and CUHK4128/07) and the Direct Grant of the Faculty of Engineering, CUHK (Project Codes: 2050363 and 2050391). This work is also affiliated with the Microsoft-CUHK Joint Laboratory for Human-centric Computing and Interface Technologies. 102 Conditional Random Fields as Base Model Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models trained to maximize the conditional probability of the desired outputs given the corresponding inputs. CRFs have been shown to perform well on Chinese NER shared task on SIGHAN-4 (Zhou et al. (2006), Chen et al. (2006a), Chen et al. (2006b)). We employ CRFs as the base model in our framework. In this base model, we design features similar to the state-of-theart CRF models for Chinese NER. We use character features, word segmentation features, part-of-speech (POS) features, and dictionary features, as described below. Character features: These features are the current character, 2 characters preceding the current character and 2 following the current character. We extend the window size to 7 but find that it slightly hurts. The reason is that CRFs can deal with non-independent features. A larger window size"
I08-4016,I08-1044,1,0.827482,"on the MSRA open track respectively, which show both the attractiveness and effectiveness of our proposed model. 1 2 Introduction We participated in the Chinese named entity recognition (NER) task for the fourth SIGHAN Chinese language processing bakeoff (SIGHAN-6). We submitted results for the open track of the NER task. Our official results achieved consistently high performance, including the first place on the CityU open track and fourth place on the MSRA open track. This paper presents an overview of our system due to space limit. A more detailed description of our model is presented in (Yu et al., 2008). Our Chinese NER system combines the strength of two graphical discriminative models, Conditional Random ∗ The work described in this paper is substantially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Nos: CUHK 4179/03E, CUHK4193/04E, and CUHK4128/07) and the Direct Grant of the Faculty of Engineering, CUHK (Project Codes: 2050363 and 2050391). This work is also affiliated with the Microsoft-CUHK Joint Laboratory for Human-centric Computing and Interface Technologies. 102 Conditional Random Fields as Base Model Conditional"
I08-4016,W06-0140,0,0.0615345,"China (Project Nos: CUHK 4179/03E, CUHK4193/04E, and CUHK4128/07) and the Direct Grant of the Faculty of Engineering, CUHK (Project Codes: 2050363 and 2050391). This work is also affiliated with the Microsoft-CUHK Joint Laboratory for Human-centric Computing and Interface Technologies. 102 Conditional Random Fields as Base Model Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models trained to maximize the conditional probability of the desired outputs given the corresponding inputs. CRFs have been shown to perform well on Chinese NER shared task on SIGHAN-4 (Zhou et al. (2006), Chen et al. (2006a), Chen et al. (2006b)). We employ CRFs as the base model in our framework. In this base model, we design features similar to the state-of-theart CRF models for Chinese NER. We use character features, word segmentation features, part-of-speech (POS) features, and dictionary features, as described below. Character features: These features are the current character, 2 characters preceding the current character and 2 following the current character. We extend the window size to 7 but find that it slightly hurts. The reason is that CRFs can deal with non-independent features. A"
N19-1124,E17-2029,0,0.0279951,"l techniques. Recently, end-to-end neural approaches (Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Sordoni et al., 2015) have attracted increasing interest. For those generative models, a notorious problem is the “safe response” problem: the generated responses are dull and generic, which may attribute to the lack of sufficient input information. The query alone cannot specify an informative response. To mitigate the issue, many research efforts have been paid to introducing other information source, such as unsupervised latent variable (Serban et al., 2017; Zhao et al., 2018; Cao and Clark, 2017; Shen et al., 2017), discourse-level variations (Zhao et al., 2017), topic information (Xing et al., 2017), speaker personality (Li et al., 2016b) and knowl2 Note the classifier could be fine-tuned with the training of our generators, which falls into the adversarial learning setting (Goodfellow et al., 2014). edge base (Ghazvininejad et al., 2018; Zhou et al., 2018). Our work follows the similar motivation and uses the output of IR systems as the additional knowledge source. Combination of IR and Generative models To combine IR and generative models, early work (Qiu et al., 2017) tried to re"
N19-1124,P18-1063,0,0.0252571,"latter took advantages of both sides. In a closed domain conversation setting, Pandey et al. (2018) further proposed to weight different training instances by context similarity. Our model differs from them in that we take an extra intermediate step for skeleton generation to filter the retrieval information before use, which shows the effectiveness in avoiding the erroneous copy in our experiments. Multi-step Language Generation Our work is also inspired by the recent success of decomposing an end-to-end language generation task into several sequential sub-tasks. For document summarization, Chen and Bansal (2018) first select salient sentences and then rewrite them in parallel. For sentiment-to-sentiment translation, Xu et al. (2018) first use a neutralization module to remove emotional words and then add sentiment to the neutralized content. Not only does their decomposition improve the overall performance, but also makes the whole generation process more interpretable. Our skeleton-to-response framework also sheds some light on the use of retrieval memories. 5 Experiments 5.1 Data We use the preprocessed data in (Wu et al., 2019) as our test bed. The total dataset consists of about 20 million single"
N19-1124,N16-1014,0,0.719468,"he IR-based models (Ji et al., 2014; Hu et al., 2014) directly copy an existing response from a training corpus when receiving a response request. Since the training corpus is usually collected from real-world conversations and possibly post-edited ∗ † Work done while DC was interning at Tencent AI Lab. Corresponding author. by a human, the retrieved responses are informative and grammatical. However, the performance of such systems drops when a given dialogue history is substantially different from those in the training corpus. The generative models (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016a), on the other hand, generate a new utterance from scratch. While those generative models have better generalization capacity in rare dialogue contexts, the generated responses tend to be universal and noninformative (e.g., “I don’t know”, “I think so” etc.) (Li et al., 2016a). It is partly due to the diversity of possible responses to a single query (i.e., the one-to-many problem). The dialogue query alone cannot decide a meaningful and specific response. Thus a well-trained model tends to generate the most frequent (safe but boring) responses instead. To summarize, IR-based models may give"
N19-1124,P18-1123,0,0.554103,"eters. The weight βw2 is obtained in a similar way with another set of parameters vD and WD . After acquiring the edit vector, we transform the prototype response r0 to a skeleton t by the follow( &lt; blank &gt; if m ˆ i = 0, φ(ri0 , hi , z) = , 0 ri else (3) where m ˆ i is the indicator and equals 0 if ri0 is replaced with a placeholder “&lt;blank&gt;” and 1 otherwise. The probability of m ˆ i = 1 is computed by P (m ˆ i = 1) = sigmoid(Wm [hi ⊕ z] + bm ). (4) 2.2 Response Generator The response generator can be implemented using most existing IR-augmented models (Song et al., 2016; Weston et al., 2018; Pandey et al., 2018), just by replacing the retrieved response input with the corresponding skeleton. We discuss our choices below. Encoders Two separate bidirectional LSTM (biLSTM) networks are used to obtain the distributed representations of the query memories and the skeleton memories, respectively. For biLSTM, 1221 the concatenation of the forward and the backward hidden states at each token position is considered a memory slot, producing two memory pools: Mq = {h1 , h2 , . . . , h|q |} for the input query, and Mt = {h01 , h02 , . . . , h0|t |} for the skeleton.1 Decoder During the generation process, our de"
N19-1124,P17-2079,0,0.0121808,"l., 2018; Cao and Clark, 2017; Shen et al., 2017), discourse-level variations (Zhao et al., 2017), topic information (Xing et al., 2017), speaker personality (Li et al., 2016b) and knowl2 Note the classifier could be fine-tuned with the training of our generators, which falls into the adversarial learning setting (Goodfellow et al., 2014). edge base (Ghazvininejad et al., 2018; Zhou et al., 2018). Our work follows the similar motivation and uses the output of IR systems as the additional knowledge source. Combination of IR and Generative models To combine IR and generative models, early work (Qiu et al., 2017) tried to re-rank the output from both models. However, the performance of such models is limited by the capacity of individual methods. Most related to our work, Song et al. (2016); Weston et al. (2018) and Wu et al. (2019) encoded the retrieved result into distributed representation and used it as the additional conditionals along with the standard query representation. While the former two only used the target side of the retrieved pairs, the latter took advantages of both sides. In a closed domain conversation setting, Pandey et al. (2018) further proposed to weight different training inst"
N19-1124,P15-1152,0,0.176413,"Missing"
N19-1124,N15-1020,0,0.0309983,"following objective is maximized: log D(r|q, rˆ, r, r) = log exp(hr T MD hq ) , P exp(hx T MD hq ) x∈{ˆ r ,r,r} (8) where hx is a vector representation of x, produced by a bidirectional LSTM (the last hidden state), and MD is a trainable matrix.2 4 Related Work Multi-source Dialogue Generation Chit-chat style dialogue system dates back to ELIZA (Weizenbaum, 1966). Early work uses handcrafted rules, while modern systems usually use data-driven approaches, e.g., information retrieval techniques. Recently, end-to-end neural approaches (Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Sordoni et al., 2015) have attracted increasing interest. For those generative models, a notorious problem is the “safe response” problem: the generated responses are dull and generic, which may attribute to the lack of sufficient input information. The query alone cannot specify an informative response. To mitigate the issue, many research efforts have been paid to introducing other information source, such as unsupervised latent variable (Serban et al., 2017; Zhao et al., 2018; Cao and Clark, 2017; Shen et al., 2017), discourse-level variations (Zhao et al., 2017), topic information (Xing et al., 2017), speaker"
N19-1124,P16-1094,0,0.0661705,"Missing"
N19-1124,D15-1166,0,0.129849,"cts a response skeleton. Lower: The response generator generates a response based on both the skeleton and the query. word embeddings to get the dense representations of I and D. The edit vector is computed as: X X z= αw1 Φ(w1 ) ⊕ βw2 Φ(w2 ), (1) w1 ∈I ing equations: 0 t = (φ(r10 , h1 , z), φ(r20 , h2 , z), · · · , φ(r|r 0 |, h|r 0 |, z)), w2 ∈D where ⊕ is the concatenation operation. Φ maps a word to its corresponding embedding vector, αw1 and βw2 are the weights of an insertion word w1 and a deletion word w2 respectively. The weights of different words are derived by an attention mechanism (Luong et al., 2015). Formally, the 0 ) is proretrieved response r0 = (r10 , r20 . . . , r|r 0| cessed by a bidirectional GRU network (biGRU). We denote the states of the biGRU (i.e. concatenation of forward and backward GRU states) as (h1 , h2 , . . . , h|r0 |). The weight αw1 is calculated by: exp(sw1 ) αw1 = P , w∈I exp(sw ) sw1 = vI&gt; tanh(WI [Φ(w1 ) ⊕ h|r0 |]), (2) where vI and WI are learnable parameters. The weight βw2 is obtained in a similar way with another set of parameters vD and WD . After acquiring the edit vector, we transform the prototype response r0 to a skeleton t by the follow( &lt; blank &gt; if m ˆ"
N19-1124,W18-5713,0,0.391089,"uery (i.e., the one-to-many problem). The dialogue query alone cannot decide a meaningful and specific response. Thus a well-trained model tends to generate the most frequent (safe but boring) responses instead. To summarize, IR-based models may give informative but inappropriate responses while generative models often do the opposite. It is desirable to combine both merits. Song et al. (2016) used an extra encoder for the retrieved response. The resulted dense representation, together with the original query, is used to feed the decoder in a standard S EQ 2S EQ model (Bahdanau et al., 2014). Weston et al. (2018) used a single encoder that takes the concatenation of the original query and the retrieved as input. Wu et al. (2019) noted that the retrieved information should be used in awareness of the context difference, and further proposed to construct an edit vector by explicitly encoding the lexical differences between the input query and the retrieved query. However, in our preliminary experiments, we found that the IR-guided models are inclined to degenerate into a copy mechanism, in which the generative models simply repeat the retrieved response without necessary modifications. Sharp performance"
N19-1124,P18-1101,0,0.0275646,"nformation retrieval techniques. Recently, end-to-end neural approaches (Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Sordoni et al., 2015) have attracted increasing interest. For those generative models, a notorious problem is the “safe response” problem: the generated responses are dull and generic, which may attribute to the lack of sufficient input information. The query alone cannot specify an informative response. To mitigate the issue, many research efforts have been paid to introducing other information source, such as unsupervised latent variable (Serban et al., 2017; Zhao et al., 2018; Cao and Clark, 2017; Shen et al., 2017), discourse-level variations (Zhao et al., 2017), topic information (Xing et al., 2017), speaker personality (Li et al., 2016b) and knowl2 Note the classifier could be fine-tuned with the training of our generators, which falls into the adversarial learning setting (Goodfellow et al., 2014). edge base (Ghazvininejad et al., 2018; Zhou et al., 2018). Our work follows the similar motivation and uses the output of IR systems as the additional knowledge source. Combination of IR and Generative models To combine IR and generative models, early work (Qiu et a"
N19-1124,P17-1061,0,0.0360971,"2015; Serban et al., 2016; Li et al., 2016a; Sordoni et al., 2015) have attracted increasing interest. For those generative models, a notorious problem is the “safe response” problem: the generated responses are dull and generic, which may attribute to the lack of sufficient input information. The query alone cannot specify an informative response. To mitigate the issue, many research efforts have been paid to introducing other information source, such as unsupervised latent variable (Serban et al., 2017; Zhao et al., 2018; Cao and Clark, 2017; Shen et al., 2017), discourse-level variations (Zhao et al., 2017), topic information (Xing et al., 2017), speaker personality (Li et al., 2016b) and knowl2 Note the classifier could be fine-tuned with the training of our generators, which falls into the adversarial learning setting (Goodfellow et al., 2014). edge base (Ghazvininejad et al., 2018; Zhou et al., 2018). Our work follows the similar motivation and uses the output of IR systems as the additional knowledge source. Combination of IR and Generative models To combine IR and generative models, early work (Qiu et al., 2017) tried to re-rank the output from both models. However, the performance of such"
N19-1297,D13-1160,0,0.0192398,"many large-scale knowledge bases (KBs) such as Freebase (Bollacker et al., 2008), DBpedia (Lehmann et al., 2015), YAGO (Suchanek et al., 2007) and Wikidata (Vrandeˇci´c author: Female English Best Female Tennis Player Award Introduction ∗ Corresponding uzy@tsinghua.edu.cn). Saginaw (liand Kr¨otzsch, 2014) to store facts of the real world. Most KBs typically organize the complex structured information about facts in the form of triples (head entity, relation, tail entity), e.g., (Bill Gates, CEOof, Microsoft Inc.). These KBs have been widely used in many AI and NLP tasks such as text analysis (Berant et al., 2013), question answering (Bordes et al., 2014a), and information retrieval (Hoffmann et al., 2011). The construction of these KBs is always an ongoing process due to the endless growth of realworld facts. Hence, many tasks such as knowledge base completion (KBC) and relation prediction (RP) are proposed to enrich KBs. The KBC task usually assumes that one entity and the relation r are given, and another entity is missing and required to be predicted. In general, we wish to predict the missing entity in (h, r, ?) or (?, r, t), where h and t denote a head and tail entity respectively. Similarly, the"
N19-1297,D14-1067,0,0.103026,"h as Freebase (Bollacker et al., 2008), DBpedia (Lehmann et al., 2015), YAGO (Suchanek et al., 2007) and Wikidata (Vrandeˇci´c author: Female English Best Female Tennis Player Award Introduction ∗ Corresponding uzy@tsinghua.edu.cn). Saginaw (liand Kr¨otzsch, 2014) to store facts of the real world. Most KBs typically organize the complex structured information about facts in the form of triples (head entity, relation, tail entity), e.g., (Bill Gates, CEOof, Microsoft Inc.). These KBs have been widely used in many AI and NLP tasks such as text analysis (Berant et al., 2013), question answering (Bordes et al., 2014a), and information retrieval (Hoffmann et al., 2011). The construction of these KBs is always an ongoing process due to the endless growth of realworld facts. Hence, many tasks such as knowledge base completion (KBC) and relation prediction (RP) are proposed to enrich KBs. The KBC task usually assumes that one entity and the relation r are given, and another entity is missing and required to be predicted. In general, we wish to predict the missing entity in (h, r, ?) or (?, r, t), where h and t denote a head and tail entity respectively. Similarly, the RP task predicts the missing relation gi"
N19-1297,P11-1055,0,0.0408868,"ehmann et al., 2015), YAGO (Suchanek et al., 2007) and Wikidata (Vrandeˇci´c author: Female English Best Female Tennis Player Award Introduction ∗ Corresponding uzy@tsinghua.edu.cn). Saginaw (liand Kr¨otzsch, 2014) to store facts of the real world. Most KBs typically organize the complex structured information about facts in the form of triples (head entity, relation, tail entity), e.g., (Bill Gates, CEOof, Microsoft Inc.). These KBs have been widely used in many AI and NLP tasks such as text analysis (Berant et al., 2013), question answering (Bordes et al., 2014a), and information retrieval (Hoffmann et al., 2011). The construction of these KBs is always an ongoing process due to the endless growth of realworld facts. Hence, many tasks such as knowledge base completion (KBC) and relation prediction (RP) are proposed to enrich KBs. The KBC task usually assumes that one entity and the relation r are given, and another entity is missing and required to be predicted. In general, we wish to predict the missing entity in (h, r, ?) or (?, r, t), where h and t denote a head and tail entity respectively. Similarly, the RP task predicts the missing relation given the head and tail entities and their evidence sen"
N19-1297,P15-1067,0,0.271261,"KBR models typically embed the semantics of both entities and relations into low-dimensional semantic space, i.e., embeddings. For example, TransE (Bordes et al., 2013) learns low-dimensional and real-valued embeddings for both entities and relations by regarding the relation of each triple fact as a translation from its head entity to the tail entity. TransE can thus compute the valid score for each triple by measuring how well the relation can play a translation between the head and tail entities. Many methods have been proposed to extend TransE to deal with various characteristics of KBs (Ji et al., 2015, 2016; He et al., 2015; Lin et al., 2015a). To solve the FDKB task using KBR, one feasible way is to exhaustively calculate the scores of all (r, t) combinations for the given head entity h. Afterwards, the highly-scored facts are returned as results. However, this idea has some drawbacks: (1) It takes all relations to calculate ranking scores for each head entity, ignoring the nature of the head entity. The combination of all possible relations and tail entities will lead to huge amount of computations. (2) A large set of candidate triples immerses the correct triples into a lot of noisy tri"
N19-1297,D15-1082,1,0.887079,"Missing"
N19-1297,P09-1113,0,0.0284715,"nsive analysis of the framework in discovering different kinds of facts. The contributions of this paper can be summarized as follows: (1) We introduce a new task of fact discovery from knowledge base, which is more practical. (2) We propose a new framework based on the facet decomposition which achieves promising results. 2 Related Work In recent years, many tasks (Wang et al., 2017) have been proposed to help represent and enrich KBs. Tasks such as knowledge base completion (KBC) (Bordes et al., 2013; Wang et al., 2014; Ji et al., 2015, 2016; Wang et al., 2017) and relation prediction (RP) (Mintz et al., 2009; Lin et al., 2015a; Xie et al., 2016) are widely studied and many models are proposed to improve the performance on these tasks. However, the intention of these tasks is to test the performance of models in representing KBs and thus they cannot be used directly to discover new facts of KBs. Moreover, our FDKB task is not a simple combination of the KBC and RP task since both of these two tasks require to know two of the triples while we assume we only know the head entity. A common approach to solving these tasks is to build a knowledge base representation (KBR) model with different kinds of"
P03-1048,J93-1004,0,0.0343641,"Missing"
P03-1048,W00-0403,1,0.724039,". The Kappa coefficient controls agreement P (A) by taking into account agreement by chance P (E) : K= P (A) − P (E) 1 − P (E) No matter how many items or annotators, or how the categories are distributed, K = 0 when there is no agreement other than what would be expected by chance, and K = 1 when agreement is perfect. If two annotators agree less than expected by chance, Kappa can also be negative. We report Kappa between three annotators in the case of human agreement, and between three humans and a system (i.e. four judges) in the next section. 3.1.3 Relative Utility Relative Utility (RU) (Radev et al., 2000) is tested on a large corpus for the first time in this project. RU takes into account chance agreement as a lower bound and interjudge agreement as an upper bound of performance. RU allows judges and summarizers to pick different sentences with similar content in their summaries without penalizing them for doing so. Each judge is asked to indicate the importance of each sentence in a cluster on a scale from 0 to 10. Judges also specify which sentences subsume or paraphrase each other. In relative utility, the score of an automatic summary increases with the importance of the sentences that it"
P03-1048,W97-0710,1,0.831436,"Missing"
P03-1048,E99-1011,0,\N,Missing
P03-1048,W97-0704,0,\N,Missing
P03-1048,J96-2004,0,\N,Missing
P03-1048,I05-2047,0,\N,Missing
P15-1050,H93-1012,0,0.196158,"Missing"
P15-1050,D07-1086,0,0.0364249,"ize these models for open domain entity retrieval. Entity models are also used in other fields besides entity retrieval. For example, entity topic models are used to perform entity prediction, classification of entity pairs, construction of entityentity network (Newman et al., 2006), as well as entity linking (Han and Sun, 2012). These models are not suitable for our retrieval framework. The decomposing of entity queries into factoid queries is related to query segmentation. Query segmentation has been used by search engines to support inverse lookup of words and phrases (Risvik et al., 2003; Bergsma and Wang, 2007). Our use of query decomposition is quite different compared to query segmentation. Be6 Conclusions We propose that an entity query is generated in a two-step process: users first select the facts that can distinguish target entities from the others; then choose words to express those facts. Following this motivation, we propose a retrieval framework by decomposing the original query into factoid queries. We also propose to construct an entity 521 factoid hierarchy as the entity model for the purpose of entity retrieval. Our entity factoid hierarchy can integrate information of different granu"
P15-1050,D12-1010,0,0.0453578,"Missing"
P15-1050,P03-1054,0,0.00447262,"umes that users generate queries by first selecting facts and then choosing query words for each fact. Based on the query generation process, we first decompose the query q into m subqueries qi (discussed in Section 3.2). Then the probability p(q|e) can be computed as: p(q|e) = = ' m Y p(qi |e) i=1 m X n Y p(qi |fk )p(fk |e) i=1 k=1 m Y max p(qi |fk ). i=1 k 3.2 Query analysis As mentioned earlier, we decompose the original query q into multiple factoid subqueries qi . For long queries issued in a verbose sentence, such as “which presidents were born in 1945”, dependency parsing is performed (Klein and Manning, 2003) and the resulting dependency tree is used to split the original query. For short queries issued in keywords, such as “vietnam war movies”, we decompose it based on possible key concepts expressed in the query. Usually a short query only contains a single entity, which is used to segment the original query into subqueries. Furthermore, stop structures in verbose queries is removed, following the method proposed in (Huston and Croft, 2010). Here a stop structure is defined as a phrase which provides no information regarding the information needs, such as “tell me the”. We also inject target ent"
P15-1050,P11-1080,0,0.0814553,"Missing"
P15-1050,P12-1040,0,0.0234402,"decreases due to the with a bag-of-words vector to represent the faclonger entity pseudo-document. This result is toid description. Factoid nodes in different depth not desirable for entity retrieval, since adding encode information in different granularities. more descriptions about other facts should An example of an entity factoid hierarchy, renot affect the confidence of existing facts. garding two factoids (birth date and birth place) Our factoid hierarchy avoids this problem by about Barack Obama, is given in Figure 1. The preserving all the entity descriptions in a hi515 et al., 2011; Wick et al., 2012). Here we design a factor graph model corresponding to the entity factoid hierarchy, together with new factor types and inference mechanism. Generally speaking, a factor graph is composed of two parts: a set of random variables and a set of factors that model the dependencies between random variables. An example of the factor graph construction corresponding to the factoid hierarchy involved in Figure 1 is given in Figure 2. In our factor graph approach, each factoid is represented as a random variable fi , corresponding to a rounded square node in Figure 2. The pairwise binary decision variab"
P15-1153,P13-1020,0,0.0682145,"s. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) o"
P15-1153,J05-3002,0,0.423913,"compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather information across sentence boundary, and hence have the potential to cover more content in a more concise manner. In this paper, we propose an abstractive MDS framework that can construct new sentences by 1587 Proceedings of"
P15-1153,P14-1086,0,0.0209135,"mmary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks. 6 Conclusions and Future Work We propose an abstractive MDS framework that constructs new sentences by exploring more finegrained syntactic units, namely,"
P15-1153,P11-1049,0,0.225126,"nces may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest a"
P15-1153,P13-1121,0,0.0381358,"n in the summary. To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based a"
P15-1153,P14-1085,0,0.00660705,"011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks. 6 Conclusions and Future Work We propose an abstractive MDS framework that constructs new sentences by exploring more finegrained syntactic units, namely, noun phrases and verb phrases. The designed optimization framework operates on the summary level so that more compl"
P15-1153,C04-1057,0,0.0172214,"011; Goldstein et al., 2000; Wan et al., 2007). Each sentence in the documents is firstly assigned a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the senten"
P15-1153,D08-1019,0,0.237574,"els cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather information across sentence boundary, and hence have the potential to cover more content in a more concise manner. In this paper, we propose an abstractive MDS framework that can construct new sentences by 1587 Proceedings of the 53rd Annual Meeting of the Association for Computational Lingu"
P15-1153,C10-1037,0,0.00615543,"different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather information across sentence boundary, and hence have the potential to cover more content in a more concise manner. In this paper, we propose an abstractive MDS framework that can construct new sentences by 1587 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th"
P15-1153,C10-1039,0,0.0485824,"on to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to su"
P15-1153,W11-1608,0,0.254617,"; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also expl"
P15-1153,P12-2069,0,0.0121625,"l., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, suc"
P15-1153,W09-1802,0,0.203135,"eously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based appro"
P15-1153,W00-0405,0,0.425057,"ch as “sent the boys outside”, “authorities said”, etc. In addition, the VP “killing five girls” of the original sentence with ID 64 is also excluded since it has significant redundancy with the summary sentence with ID 1. 5 Related Work Existing multi-document summarization (MDS) works can be classified into three categories: 1594 extraction-based approaches, compression-based approaches, and abstraction-based approaches. Extraction-based approaches are the most studied of the three. Early studies mainly followed a greedy strategy in sentence selection (C ¸ elikyilmaz and Hakkani-T¨ur, 2011; Goldstein et al., 2000; Wan et al., 2007). Each sentence in the documents is firstly assigned a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassilogl"
P15-1153,P12-1091,1,0.836203,"rkshops such as TAC, the pyramid is used as the major metric. Since manual pyramid evaluation is timeconsuming, and the exact evaluation scores are not reproducible especially when the assessors for our results are different from those of TAC, we employ the automated version of pyramid proposed in (Passonneau et al., 2013). The automated pyramid scoring procedure relies on distributional semantics to assign SCUs to a target summary. Specifically, all n-grams within sentence bounds are extracted, and converted into 100 dimension latent topical vectors via a weighted matrix factorization model (Guo and Diab, 2012). Similarly, the contributors and the label of an SCU are transformed into 100 dimensional vector representations. An SCU is assigned to a summary if there exists an n-gram such that the similarity score between the SCU low dimensional vector and the n-gram low dimensional vector exceeds a threshold. Passonneau et al. (2013) showed that the distributional similarity based method produces automated scores that correlate well with manual pyramid scores, yielding more accurate pyramid scores than string matching based automated methods (Harnly et al., 2005). In this paper, we adopt the same setti"
P15-1153,A00-2024,0,0.188299,"is end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather informat"
P15-1153,W14-1504,0,0.00804706,"Missing"
P15-1153,P03-1054,0,0.0033611,"en we formulate the sentence generation task as an optimization problem, and design constraints. In the end, we perform several post-processing steps to improve the order and the readability of the generated sentences. 2.1 Phrase Salience Calculation The first component decomposes the sentences in documents into a set of noun phrases (NPs) derived from the subject parts of a constituency tree and a set of verb-object phrases (VPs), representing potential key concepts and key facts, respectively. These phrases will serve as the basic elements for sentence generation. We employ Stanford parser (Klein and Manning, 2003) to obtain a constituency tree for each input sentence. After that, we extract NPs and VPs from the tree as follows: (1) The NPs and VPs that are the direct children of the sentence node (repre1588 sented by the S node) are extracted. (2) VPs (NPs) in a path on which all the nodes are VPs (NPs) are also recursively extracted and regarded as having the same parent node S. Recursive operation in the second step will only be carried out in two levels since the phrases in the lower levels may not be able to convey a complete fact. Take the tree in Figure 1 as an example, the corresponding sentence"
P15-1153,J13-4004,0,0.00850382,"ts are jointly considered. 2.2.1 Compatibility Relation Compatibility relation is designed to indicate whether an NP and a VP can be used to form a new sentence. For example, the NP “Police” from another sentence should not be the subject of the VP “sent the boys outside” extracted from Figure 1. We use some heuristics to find compatibility, and then expand the compatibility relation to more phrases by extracting coreference. To find coreference NPs (different mentions for the same entity), we first conduct coreference resolution for each document with Stanford coreference resolution package (Lee et al., 2013). We adopt those resolution rules that are able to achieve high quality and address our need for summarization. In particular, Sieve 1, 2, 3, 4, 5, 9, and 10 in the package are used. A set of clusters are obtained and each cluster contains the mentions that refer to the same entity in a document. The clusters from different documents in the same topic are merged by matching the named entities. After merging, the mentions that are not NPs extracted in the phrase extraction step are removed in each cluster. Two NPs in the same cluster are determined as alternative of each other. To find alternat"
P15-1153,N10-1134,0,0.119631,"s is firstly assigned a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant"
P15-1153,N03-1020,0,0.168226,"as to recognize alternate realizations of the same meaning. Different weights are assigned to SCUs based on their frequency in model summaries. A weighted inventory of SCUs named a pyramid is created, which constitutes a resource for investigating alternate realizations of the same meaning. Such property makes pyramid method more suitable to evaluSystem Our 22 43 17 Auto-pyr (Th: .6) 0.905 0.878 0.875 0.860 Auto-pyr (Th: .65) 0.793 0.775 0.756 0.741 Rank in TAC 2011 NA 1 2 3 Table 2: Comparison with the top 3 systems in TAC 2011. ate summaries. Another widely used evaluation metric is ROUGE (Lin and Hovy, 2003) and it evaluates summaries from word overlapping perspective. Because of the strict string matching, it ignores the semantic content units and performs better when larger sets of model summaries are available. In contrast to ROUGE, pyramid scoring is robust with as few as four model summaries (Nenkova and Passonneau, 2004). Therefore, in recent summarization evaluation workshops such as TAC, the pyramid is used as the major metric. Since manual pyramid evaluation is timeconsuming, and the exact evaluation scores are not reproducible especially when the assessors for our results are different"
P15-1153,W03-1101,0,0.416943,"the Hong Kong Special Administrative Region, China (Project Codes: 413510 and 14203414). The work was done when Weiwei Guo was in Columbia University summarization systems adopt the extractionbased approach which selects some original sentences from the source documents to create a short summary (Erkan and Radev, 2004; Wan et al., 2007). However, the restriction that the whole sentence should be selected potentially yields some overlapping information in the summary. To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whos"
P15-1153,W09-1801,0,0.0671521,"he redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that e"
P15-1153,P14-1115,0,0.0357106,"Missing"
P15-1153,N04-1019,1,0.536063,"f concepts and facts represented by NPs and VPs from the input documents. A salience score is computed for each phrase by exploiting redundancy of the document content in a global manner. The second component constructs new sentences by selecting and merging phrases based on their salience scores, and ensures the validity of new sentences using a integer linear optimization model. The contribution of this paper is two folds. (1) We extract NPs/VPs from constituency trees to represent key concepts/facts, and merge them to construct new sentences, which allows more summary content units (SCUs) (Nenkova and Passonneau, 2004) to be included in a sentence by breaking the original sentence boundaries. (2) The designed optimization framework for addressing the problem is unique and effective. Our optimization algorithm simultaneously selects and merges a set of phrases that maximize the number of covered SCUs in a summary. Meanwhile, since the basic unit is phrases, we design compatibility relations among NPs and VPs, as well as other optimization constraints, to ensure that the generated sentences contain correct facts. Compared with the sentence fusion approaches that compute salience scores of sentence clusters, o"
P15-1153,P14-1084,0,0.113882,"ntly, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks. 6 Conclusions and Future Work We propose an abstractive MDS framework that constructs new sentences by exploring more finegrained syntactic units, namely, noun phrases and verb phrases. The designed optimization framework operates on the summary level so that more complementary semantic content units can be incorporated. The phrase selection and merging is done simultaneously to achieve global optimal. Meanwhile, the cons"
P15-1153,J11-4007,0,0.0159621,"the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Chri"
P15-1153,E12-1023,0,0.123518,", sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integra"
P15-1153,P10-1058,0,0.0225702,"ularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items ("
P15-1153,D12-1022,0,0.286119,"n”, “walked into an Amish school, sent the boys outside and tied up and shot the girls, killing three of them”, “walked into an Amish school”, “sent the boys outside”, and “tied up and shot the girls, killing three of them”. 1 Because of the recursive operation, the extracted phrases may have overlaps. Later, we will show how to avoid such overlapping in phrase selection. A salience score is calculated for each phrase to indicate its importance. Different types of salience can be incorporated in our framework, such as position-based method (Yih et al., 2007), statistical feature based method (Woodsend and Lapata, 2012), concept-based method (Li et al., 2011), etc. One key characteristic of our approach is that the considered basic units are phrases instead of sentences. Such finer granularity leaves more room for better global salience score by potentially covering more distinct facts. In our implementation, we adopt a concept-based weight incorporating the position information. The concept set is designated to be the union set of unigrams, bigrams, and named entities in the documents. We remove stopwords and perform lemmatization before extracting unigrams and bigrams. The position-based term frequency is"
P15-1153,I08-1016,0,0.0590528,"sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a l"
P15-1153,P14-1087,0,0.0220839,"tion based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al"
P15-1153,P13-2026,1,\N,Missing
P16-1064,D10-1005,0,0.0262438,"ics at the vocabulary level, which are different from that of Zhang et al. (2010) and ours. The model in Zhang et al. (2010) adds the constraints of word translation pairs into PLSA. These constraints cannot handle the original word co-occurrences well. In contrast, we consider the language gap by incorporating word distributions from the other language, capturing the common semantics on the topic level. Moreover, we use a fully Bayesian paradigm with a prior distribution. Some existing topic methods conduct crosslingual sentiment analysis (Lu et al., 2011; Guo et al., 2010; Lin et al., 2014; Boyd-Graber and Resnik, 2010). These models are not suitable for our CTD task because they mainly detect common elements related to product aspects. Moreover some works focus more on detecting sentiments. 3 3.1 θde zne α θdc znc wne ϕe ηe e Ndw Nde wnc β ϕc ηc c Ndw Ndc K Figure 1: Our proposed graphical model butions ηe , with dimension Λe , and ηc , with dimension Λc , to help the generation of ϕek and ϕck . Precisely, we generate ηe and ηc from the Dirichlet prior distributions Dir(β ·1|Λe |) and Dir(β ·1|Λc |) respectively, where 1D denotes a D-dimensional vector whose components are 1. Then we draw ϕek from the mixtu"
P16-1064,P11-1033,0,0.0155455,"um´e (2010) focus on mining the correspondence of topics at the vocabulary level, which are different from that of Zhang et al. (2010) and ours. The model in Zhang et al. (2010) adds the constraints of word translation pairs into PLSA. These constraints cannot handle the original word co-occurrences well. In contrast, we consider the language gap by incorporating word distributions from the other language, capturing the common semantics on the topic level. Moreover, we use a fully Bayesian paradigm with a prior distribution. Some existing topic methods conduct crosslingual sentiment analysis (Lu et al., 2011; Guo et al., 2010; Lin et al., 2014; Boyd-Graber and Resnik, 2010). These models are not suitable for our CTD task because they mainly detect common elements related to product aspects. Moreover some works focus more on detecting sentiments. 3 3.1 θde zne α θdc znc wne ϕe ηe e Ndw Nde wnc β ϕc ηc c Ndw Ndc K Figure 1: Our proposed graphical model butions ηe , with dimension Λe , and ηc , with dimension Λc , to help the generation of ϕek and ϕck . Precisely, we generate ηe and ηc from the Dirichlet prior distributions Dir(β ·1|Λe |) and Dir(β ·1|Λc |) respectively, where 1D denotes a D-dimensi"
P16-1064,P10-1115,0,0.379116,"s “plane:飞 机 ocean:海洋 . . . ”. The assumption of one-toone mapping of words has some drawbacks. One drawback is that the correspondence of identified common topics is restricted to the vocabulary level. Another drawback is that the one-toone mapping of words cannot fit the original word occurrences well. For example, the English term “plane” appears in the English documents frequently while the Chinese translation “飞机” appears less. It is not reasonable that “plane” and “飞 机” share the same probability mass in common topics. Another closely related existing work is the PCLSA model proposed by Zhang et al. (2010). PCLSA employs a mixture of English words and Chinese words to represent common topics. It incorporates bilingual constraints into the Probabilistic Latent Semantic Analysis (PLSA) model (Hofmann, 2001) and assumes that word pairs in the dictionary share similar probability in a common topic. However, similar to one-to-one mapping of words, such bilingual constraints cannot handle well the original word co-occurrence in each language resulting in a degradation of the co2 Related Work Prasojo et al. (2015) and Biyani et al. (2015) organized news reader comments via identified entities or aspec"
P16-1064,D09-1092,0,0.446979,"Missing"
P16-1064,N10-1012,0,0.0209176,"The word-to-word bilingual constraints in PCLSA are not as effective. On the other hand, our MTCA model incorporates the bilingual translations using auxiliary distributions which incorporate word distributions from the other language on the topic level and can capture common semantics of multilingual reader comments. λ = 0.2 λ = 0.5 λ = 0.8 50 Topic Coherence Evaluation P M I(wi , wj ) = log 5 0 MCTA 0.138 0.158 0.120 0.169 0.152 0.152 0.111 0.124 0.096 0.154 0.137 We also evaluate the coherence of topics generated by PCLSA and MCTA, which indicates the interpretability of topics. Following Newman et al. (2010), we use a pointwise mutual information (PMI) score to measure the topic coherence. We compute the average PMI score of top 20 topic word pairs using Eq. 15. Newman et al. (2010) observed that it is important to use an external data set to evaluate PMI. Therefore, we use a 20word sliding window in Wikipedia (Shaoul, 2010) to identify the co-occurrence of word pairs. Determining Number of Topics 1840 1820 1800 1780 1760 1740 1720 1700 1680 PCLSA 0.117 0.126 0.117 0.138 0.109 0.138 0.108 0.099 0.085 0.133 0.117 Table 4: Topic coherence evaluation tified by MCTA have better topic commonality beca"
P16-1064,D09-1146,0,0.18643,"aints cannot handle well the original word co-occurrence in each language resulting in a degradation of the co2 Related Work Prasojo et al. (2015) and Biyani et al. (2015) organized news reader comments via identified entities or aspects. Such kind of organization via entities or aspects cannot capture common topics discussed by readers. Digesting merely based on entities fails to work in multilingual settings due to the fact that the common entities have distinct mentions in different languages. Zhai et al. (2004) discovered common topics from comparable texts via a PLSA based mixture model. Paul and Girju (2009) proposed a MixedCollection Topic Model for finding common topics from different collections. Despite the fact that the above models can find a kind of common topic, they only deal with a single language setting without considering the language gap. Some works discover common latent topics from multilingual corpora. For aligned corpora, they assume that the topic distribution in each document is the same (Vuli´c et al., 2011; Vuli´c and Moens, 2014; Erosheva et al., 2004; Fukumasu et al., 2012; Mimno et al., 2009; Ni et al., 2009; Zhang et al., 2013; Peng et al., 2014). However, aligned corpor"
P16-1064,P14-2110,0,0.0240109,"based mixture model. Paul and Girju (2009) proposed a MixedCollection Topic Model for finding common topics from different collections. Despite the fact that the above models can find a kind of common topic, they only deal with a single language setting without considering the language gap. Some works discover common latent topics from multilingual corpora. For aligned corpora, they assume that the topic distribution in each document is the same (Vuli´c et al., 2011; Vuli´c and Moens, 2014; Erosheva et al., 2004; Fukumasu et al., 2012; Mimno et al., 2009; Ni et al., 2009; Zhang et al., 2013; Peng et al., 2014). However, aligned corpora are often unavailable for most domains. For unaligned corpora, cross-lingual topic models use some language resources, such 677 as a bilingual dictionary or a bilingual knowledge base to bridge the language gap (Boyd-Graber and Blei, 2009; Zhang et al., 2010; Jagarlamudi and Daum´e III, 2010). As mentioned above, the goals of Boyd-Graber and Blei (2009) as well as Jagarlamud and Daum´e (2010) focus on mining the correspondence of topics at the vocabulary level, which are different from that of Zhang et al. (2010) and ours. The model in Zhang et al. (2010) adds the co"
P16-1064,D14-1040,0,0.0348108,"Missing"
P16-1064,P11-2084,0,0.0513094,"Missing"
P18-1087,E17-2091,0,0.366632,"neering and Engineering Management The Chinese University of Hong Kong, Hong Kong 2 Tencent AI Lab, Shenzhen, China {lixin,wlam,bshi}@se.cuhk.edu.hk lyndonbing@tencent.com Abstract “log on” and “better life”, and expresses positive sentiments over them. The task is usually formulated as predicting a sentiment category for a (target, sentence) pair. Recurrent Neural Networks (RNNs) with attention mechanism, firstly proposed in machine translation (Bahdanau et al., 2014), is the most commonly-used technique for this task. For example, Wang et al. (2016); Tang et al. (2016b); Yang et al. (2017); Liu and Zhang (2017); Ma et al. (2017) and Chen et al. (2017) employ attention to measure the semantic relatedness between each context word and the target, and then use the induced attention scores to aggregate contextual features for prediction. In these works, the attention weight based combination of word-level features for classification may introduce noise and downgrade the prediction accuracy. For example, in “This dish is my favorite and I always get it and never get tired of it.”, these approaches tend to involve irrelevant words such as “never” and “tired” when they highlight the opinion modifier “favor"
P18-1087,D15-1166,0,0.075596,"word and the target, and then use the induced attention scores to aggregate contextual features for prediction. In these works, the attention weight based combination of word-level features for classification may introduce noise and downgrade the prediction accuracy. For example, in “This dish is my favorite and I always get it and never get tired of it.”, these approaches tend to involve irrelevant words such as “never” and “tired” when they highlight the opinion modifier “favorite”. To some extent, this drawback is rooted in the attention mechanism, as also observed in machine translation (Luong et al., 2015) and image captioning (Xu et al., 2015). Another observation is that the sentiment of a target is usually determined by key phrases such as “is my favorite”. By this token, Convolutional Neural Networks (CNNs)—whose capability for extracting the informative n-gram features (also called “active local features”) as sentence representations has been verified in (Kim, 2014; Johnson and Zhang, 2015)— should be a suitable model for this classification problem. However, CNN likely fails in cases where a sentence expresses different sentiments over multiple targets, such as “great food but the service"
P18-1087,D17-1047,1,0.853645,"ese University of Hong Kong, Hong Kong 2 Tencent AI Lab, Shenzhen, China {lixin,wlam,bshi}@se.cuhk.edu.hk lyndonbing@tencent.com Abstract “log on” and “better life”, and expresses positive sentiments over them. The task is usually formulated as predicting a sentiment category for a (target, sentence) pair. Recurrent Neural Networks (RNNs) with attention mechanism, firstly proposed in machine translation (Bahdanau et al., 2014), is the most commonly-used technique for this task. For example, Wang et al. (2016); Tang et al. (2016b); Yang et al. (2017); Liu and Zhang (2017); Ma et al. (2017) and Chen et al. (2017) employ attention to measure the semantic relatedness between each context word and the target, and then use the induced attention scores to aggregate contextual features for prediction. In these works, the attention weight based combination of word-level features for classification may introduce noise and downgrade the prediction accuracy. For example, in “This dish is my favorite and I always get it and never get tired of it.”, these approaches tend to involve irrelevant words such as “never” and “tired” when they highlight the opinion modifier “favorite”. To some extent, this drawback is ro"
P18-1087,P14-2009,0,0.370583,"ˆ (l) = h(l) ∗ vi , i ∈ [1, n], l ∈ [1, L]. h i i • SVM (Kiritchenko et al., 2014): It is a traditional support vector machine based model with extensive feature engineering; (11) Based on Eq. 10 and Eq. 11, the words close to the target will be highlighted and those far away will be downgraded. v is also applied on the intermediate output to introduce the position information into each CPT layer. Then we feed the weighted h(L) to the convolutional layer, i.e., the top-most layer in Fig. 1, to generate the feature map c ∈ Rn−s+1 as follows: (L) ci = ReLU(w&gt; conv hi:i+s−1 + bconv ), • AdaRNN (Dong et al., 2014): It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree; • AE-LSTM, and ATAE-LSTM (Wang et al., 2016): AE-LSTM is a simple LSTM model incorporating the target embedding as input, while ATAE-LSTM extends AE-LSTM with attention; (12) (L) where hi:i+s−1 ∈ Rs·dimh is the concatenated vecˆ (L) , · · · , h ˆ (L) , and s is the kernel size. tor of h i i+s−1 wconv ∈ Rs·dimh and bconv ∈ R are learnable weights of the convolutional kernel. To capture the most informative features, we apply max pooling (Kim, 2014) and obtain the sentenc"
P18-1087,D14-1162,0,0.0839652,"g two attention-based LSTMs and introduces gates to measure the importance of left context, right context, and the entire sentence for the prediction; • RAM (Chen et al., 2017): RAM is a multilayer architecture where each layer consists of attention-based aggregation of word features and a GRU cell to learn the sentence representation. 3.2 We run the released codes of TD-LSTM and BILSTM-ATT-G to generate results, since their papers only reported results on TWITTER. We also rerun MemNet on our datasets and evaluate it with both accuracy and Macro-Averaged F1.5 We use pre-trained GloVe vectors (Pennington et al., 2014) to initialize the word embeddings and the dimension is 300 (i.e., dimw = 300). For out-of-vocabulary words, we randomly sample their embeddings from the uniform distribution U(−0.25, 0.25), as done in (Kim, 2014). We only use one convolutional kernel size because it was observed that CNN with single optimal kernel size is comparable with CNN having multiple kernel sizes on small datasets (Zhang and Wallace, 2017). To alleviate overfitting, we apply dropout on the input word embeddings of the LSTM and the ultimate sentence representation z. All weight matrices are initialized with the uniform"
P18-1087,S14-2004,0,0.451917,"t phrase interactively; • CNN-ASP: It is a CNN-based model implemented by us which directly concatenates target representation to each word embedding; (13) • TD-LSTM (Tang et al., 2016a): It employs two LSTMs to model the left and right contexts of the target separately, then performs predictions based on concatenated context representations; Finally, we pass z to a fully connected layer for sentiment prediction: p(y|wτ , w) = Softmax(Wf z + bf ). Experimental Setup As shown in Table 1, we evaluate the proposed TNet on three benchmark datasets: LAPTOP and REST are from SemEval ABSA challenge (Pontiki et al., 2014), containing user reviews in laptop domain and restaurant domain respectively. We also remove a few examples having the “conflict label” as done in (Chen et al., 2017); TWITTER is built by Dong et al. (2014), containing twitter posts. All tokens are lowercased without removal of stop words, symbols or digits, and sentences are zero-padded to the length of the longest sentence in the dataset. Evaluation metrics are Accuracy and Macro-Averaged F1 where the latter is more appropriate for datasets with unbalanced classes. We also conduct pairwise t-test on both Accuracy and Macro-Averaged F1 to ve"
P18-1087,P11-1016,0,0.728867,"he performance of TNet consistently dominates previous state-of-the-art methods on different types of data. The ablation studies show the efficacy of its different modules, and thus verify the rationality of TNet’s architecture. Apart from sentence level sentiment classification (Kim, 2014; Shi et al., 2018), aspect/target level sentiment classification is also an important research topic in the field of sentiment analysis. The early methods mostly adopted supervised learning approach with extensive hand-coded features (Blair-Goldensohn et al., 2008; Titov and McDonald, 2008; Yu et al., 2011; Jiang et al., 2011; Kiritchenko et al., 2014; Wagner et al., 2014; Vo and Zhang, 2015), and they fail to model the semantic relatedness between a target and its context which is critical for target sentiment analysis. Dong et al. (2014) incorporate the target information into the feature learning using dependency trees. As observed in previous works, the performance heavily relies on the quality of dependency parsing. Tang et al. (2016a) propose to split the context into two parts and associate target with contextual features separately. Similar to (Tang et al., 2016a), Zhang et al. (2016) develop a three-way g"
P18-1087,P18-1232,1,0.835326,"ific transformation component to better integrate target information into the word representation. Moreover, we employ CNN as the feature extractor for this classification problem, and rely on the contextpreserving and position relevance mechanisms to maintain the advantages of previous LSTM-based models. The performance of TNet consistently dominates previous state-of-the-art methods on different types of data. The ablation studies show the efficacy of its different modules, and thus verify the rationality of TNet’s architecture. Apart from sentence level sentiment classification (Kim, 2014; Shi et al., 2018), aspect/target level sentiment classification is also an important research topic in the field of sentiment analysis. The early methods mostly adopted supervised learning approach with extensive hand-coded features (Blair-Goldensohn et al., 2008; Titov and McDonald, 2008; Yu et al., 2011; Jiang et al., 2011; Kiritchenko et al., 2014; Wagner et al., 2014; Vo and Zhang, 2015), and they fail to model the semantic relatedness between a target and its context which is critical for target sentiment analysis. Dong et al. (2014) incorporate the target information into the feature learning using depen"
P18-1087,C16-1311,0,0.412731,"and Bei Shi1 1 Department of Systems Engineering and Engineering Management The Chinese University of Hong Kong, Hong Kong 2 Tencent AI Lab, Shenzhen, China {lixin,wlam,bshi}@se.cuhk.edu.hk lyndonbing@tencent.com Abstract “log on” and “better life”, and expresses positive sentiments over them. The task is usually formulated as predicting a sentiment category for a (target, sentence) pair. Recurrent Neural Networks (RNNs) with attention mechanism, firstly proposed in machine translation (Bahdanau et al., 2014), is the most commonly-used technique for this task. For example, Wang et al. (2016); Tang et al. (2016b); Yang et al. (2017); Liu and Zhang (2017); Ma et al. (2017) and Chen et al. (2017) employ attention to measure the semantic relatedness between each context word and the target, and then use the induced attention scores to aggregate contextual features for prediction. In these works, the attention weight based combination of word-level features for classification may introduce noise and downgrade the prediction accuracy. For example, in “This dish is my favorite and I always get it and never get tired of it.”, these approaches tend to involve irrelevant words such as “never” and “tired” whe"
P18-1087,D14-1181,0,0.274961,"to involve irrelevant words such as “never” and “tired” when they highlight the opinion modifier “favorite”. To some extent, this drawback is rooted in the attention mechanism, as also observed in machine translation (Luong et al., 2015) and image captioning (Xu et al., 2015). Another observation is that the sentiment of a target is usually determined by key phrases such as “is my favorite”. By this token, Convolutional Neural Networks (CNNs)—whose capability for extracting the informative n-gram features (also called “active local features”) as sentence representations has been verified in (Kim, 2014; Johnson and Zhang, 2015)— should be a suitable model for this classification problem. However, CNN likely fails in cases where a sentence expresses different sentiments over multiple targets, such as “great food but the service was dreadful!”. One reason is that CNN cannot fully explore the target information as done by RNN-based methTarget-oriented sentiment classification aims at classifying sentiment polarities over individual opinion targets in a sentence. RNN with attention seems a good fit for the characteristics of this task, and indeed it achieves the state-of-the-art performance. Af"
P18-1087,D16-1021,0,0.416934,"and Bei Shi1 1 Department of Systems Engineering and Engineering Management The Chinese University of Hong Kong, Hong Kong 2 Tencent AI Lab, Shenzhen, China {lixin,wlam,bshi}@se.cuhk.edu.hk lyndonbing@tencent.com Abstract “log on” and “better life”, and expresses positive sentiments over them. The task is usually formulated as predicting a sentiment category for a (target, sentence) pair. Recurrent Neural Networks (RNNs) with attention mechanism, firstly proposed in machine translation (Bahdanau et al., 2014), is the most commonly-used technique for this task. For example, Wang et al. (2016); Tang et al. (2016b); Yang et al. (2017); Liu and Zhang (2017); Ma et al. (2017) and Chen et al. (2017) employ attention to measure the semantic relatedness between each context word and the target, and then use the induced attention scores to aggregate contextual features for prediction. In these works, the attention weight based combination of word-level features for classification may introduce noise and downgrade the prediction accuracy. For example, in “This dish is my favorite and I always get it and never get tired of it.”, these approaches tend to involve irrelevant words such as “never” and “tired” whe"
P18-1087,S14-2076,0,0.69114,"anged during the feature combination. 949 Train Test Train Test Train Test LAPTOP REST TWITTER # Positive 980 340 2159 730 1567 174 # Negative 858 128 800 195 1563 174 3 # Neutral 454 171 632 196 3127 346 3.1 Specifically, we first calculate the position relevance vi between the i-th word and the target4 : (k+m−i) C i−k C i&lt;k+m k+m≤i≤n i&gt;n (10) where k is the index of the first target word, C is a pre-specified constant, and m is the length of the target wτ . Then, we use v to help CNN locate the correct opinion w.r.t. the given target: ˆ (l) = h(l) ∗ vi , i ∈ [1, n], l ∈ [1, L]. h i i • SVM (Kiritchenko et al., 2014): It is a traditional support vector machine based model with extensive feature engineering; (11) Based on Eq. 10 and Eq. 11, the words close to the target will be highlighted and those far away will be downgraded. v is also applied on the intermediate output to introduce the position information into each CPT layer. Then we feed the weighted h(L) to the convolutional layer, i.e., the top-most layer in Fig. 1, to generate the feature map c ∈ Rn−s+1 as follows: (L) ci = ReLU(w&gt; conv hi:i+s−1 + bconv ), • AdaRNN (Dong et al., 2014): It learns the sentence representation toward target for sentime"
P18-1087,D16-1076,0,0.053622,"Missing"
P18-1087,S14-2036,0,0.169243,"Missing"
P18-1087,D16-1058,0,0.480203,"ng Bing2 , Wai Lam1 and Bei Shi1 1 Department of Systems Engineering and Engineering Management The Chinese University of Hong Kong, Hong Kong 2 Tencent AI Lab, Shenzhen, China {lixin,wlam,bshi}@se.cuhk.edu.hk lyndonbing@tencent.com Abstract “log on” and “better life”, and expresses positive sentiments over them. The task is usually formulated as predicting a sentiment category for a (target, sentence) pair. Recurrent Neural Networks (RNNs) with attention mechanism, firstly proposed in machine translation (Bahdanau et al., 2014), is the most commonly-used technique for this task. For example, Wang et al. (2016); Tang et al. (2016b); Yang et al. (2017); Liu and Zhang (2017); Ma et al. (2017) and Chen et al. (2017) employ attention to measure the semantic relatedness between each context word and the target, and then use the induced attention scores to aggregate contextual features for prediction. In these works, the attention weight based combination of word-level features for classification may introduce noise and downgrade the prediction accuracy. For example, in “This dish is my favorite and I always get it and never get tired of it.”, these approaches tend to involve irrelevant words such as “nev"
P18-1087,P11-1150,0,0.0499508,"M-based models. The performance of TNet consistently dominates previous state-of-the-art methods on different types of data. The ablation studies show the efficacy of its different modules, and thus verify the rationality of TNet’s architecture. Apart from sentence level sentiment classification (Kim, 2014; Shi et al., 2018), aspect/target level sentiment classification is also an important research topic in the field of sentiment analysis. The early methods mostly adopted supervised learning approach with extensive hand-coded features (Blair-Goldensohn et al., 2008; Titov and McDonald, 2008; Yu et al., 2011; Jiang et al., 2011; Kiritchenko et al., 2014; Wagner et al., 2014; Vo and Zhang, 2015), and they fail to model the semantic relatedness between a target and its context which is critical for target sentiment analysis. Dong et al. (2014) incorporate the target information into the feature learning using dependency trees. As observed in previous works, the performance heavily relies on the quality of dependency parsing. Tang et al. (2016a) propose to split the context into two parts and associate target with contextual features separately. Similar to (Tang et al., 2016a), Zhang et al. (2016) d"
P18-1087,I17-1026,0,0.0291095,"since their papers only reported results on TWITTER. We also rerun MemNet on our datasets and evaluate it with both accuracy and Macro-Averaged F1.5 We use pre-trained GloVe vectors (Pennington et al., 2014) to initialize the word embeddings and the dimension is 300 (i.e., dimw = 300). For out-of-vocabulary words, we randomly sample their embeddings from the uniform distribution U(−0.25, 0.25), as done in (Kim, 2014). We only use one convolutional kernel size because it was observed that CNN with single optimal kernel size is comparable with CNN having multiple kernel sizes on small datasets (Zhang and Wallace, 2017). To alleviate overfitting, we apply dropout on the input word embeddings of the LSTM and the ultimate sentence representation z. All weight matrices are initialized with the uniform distribution U(−0.01, 0.01) and the biases are initialized Main Results As shown in Table 3, both TNet-LF and TNet-AS consistently achieve the best performance on all datasets, which verifies the efficacy of our whole TNet model. Moreover, TNet can perform well for different kinds of user generated content, such as product reviews with relatively formal sentences in LAPTOP and REST, and tweets with more ungrammati"
P18-1087,I17-1006,0,0.0135571,"preferred. The input targets are wrapped in brackets with the true labels given as subscripts. 7 indicates incorrect prediction. teraction between the target and its surrounding contexts. Despite the advantages of jointly modeling target and context, they are not capable of capturing long-range information when some critical context information is far from the target. To overcome this limitation, researchers bring in the attention mechanism to model target-context association (Tang et al., 2016a,b; Wang et al., 2016; Yang et al., 2017; Liu and Zhang, 2017; Ma et al., 2017; Chen et al., 2017; Zhang et al., 2017; Tay et al., 2017). Compared with these methods, our TNet avoids using attention for feature extraction so as to alleviate the attended noise. “long” in the fifth sentence is negative for “startup time”, while it could be positive for other targets such as “battery life” in the sixth sentence. The sentiment of target-specific opinion word is conditioned on the given target. Our TNet variants, armed with the word-level feature transformation w.r.t. the target, is capable of handling such case. We also find that all these models cannot give correct prediction for the last sentence, a commonly u"
P18-1087,D17-1310,1,\N,Missing
P18-1232,P07-1056,0,0.805243,"to predict the target word. The skip-gram model uses pairwise training examples which are much easier to integrate with sentiment information. Note that our model can be easily extended to more than two domains. Similarly, we use a domain-specific vector for each word in each domain and each word is also associated with a domain-common vector. We just need to extend the probability distribution of zw from Bernoulli distribution to Multinomial distribution according to the number of domains. 4 4.1 Experiment Experimental Setup We conducted experiments on the Amazon product reviews collected by Blitzer et al. (2007). We use four product categories: books (B), DVDs (D), electronic items (E), and kitchen appliances (K). A category corresponds to a domain. For each domain, there are 17,457 unlabeled reviews on average associated with rating scores from 1.0 to 5.0 for each domain. We use unlabeled reviews with rating score higher than 3.0 as positive reviews and unlabeled reviews with rating score lower than 3.0 Yang’s Work Yang et al. (2017) have proposed a method2 to learn domain-sensitive word embeddings. They choose pivot words and add a regularization item into the original skipgram objective function e"
P18-1232,P15-1071,0,0.24436,"ht” usually connotes a positive sentiment in the electronics domain since a lightweight device is easier to carry. In contrast, in the movie 2494 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2494–2504 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics domain, the word “lightweight” usually connotes a negative opinion describing movies that do not invoke deep thoughts among the audience. This observation motivates the study of learning domainsensitive word representations (Yang et al., 2017; Bollegala et al., 2015, 2014). They basically learn separate embeddings of the same word for different domains. To bridge the semantics of individual embedding spaces, they select a subset of words that are likely to be domain-insensitive and align the dimensions of their embeddings. However, the sentiment information is not exploited in these methods although they intend to tackle the task of sentiment classification. In this paper, we aim at learning word embeddings that are both domain-sensitive and sentiment-aware. Our proposed method can jointly model the sentiment semantics and domain specificity of words, ex"
P18-1232,P14-1058,0,0.0621552,"Missing"
P18-1232,D17-1047,1,0.936057,"ve Region, China (Project Code: 14203414). Introduction Sentiment classification aims to predict the sentiment polarity, such as “positive” or “negative”, over a piece of review. It has been a long-standing research topic because of its importance for many applications such as social media analysis, ecommerce, and marketing (Liu, 2012; Pang et al., 2008). Deep learning has brought in progress in various NLP tasks, including sentiment classification. Some researchers focus on designing RNN or CNN based models for predicting sentence level (Kim, 2014) or aspect level sentiment (Li et al., 2018; Chen et al., 2017; Wang et al., 2016). These works directly take the word embeddings pre-trained for general purpose as initial word representations and may conduct fine tuning in the training process. Some other researchers look into the problem of learning taskspecific word embeddings for sentiment classification aiming at solving some limitations of applying general pre-trained word embeddings. For example, Tang et al. (2014b) develop a neural network model to convey sentiment information in the word embeddings. As a result, the learned embeddings are sentiment-aware and able to distinguish words with simil"
P18-1232,W16-1620,0,0.0245813,"s for generating such representations have been investigated. For example, Bengio et al. propose a neural network architecture for this purpose (Bengio et al., 2003; Bengio, 2009). Later, Mikolov et al. (2013) propose two methods that are considerably more efficient, namely skip-gram and CBOW. This work has made it possible to learn word embeddings from large data sets, which has led to the current popularity of word embed2495 dings. Word embedding models have been applied to many tasks, such as named entity recognition (Turian et al., 2010), word sense disambiguation (Collobert et al., 2011; Iacobacci et al., 2016; Zhang and Hasan, 2017; Dave et al., 2018), parsing (Roth and Lapata, 2016), and document classification (Tang et al., 2014a,b; Shi et al., 2017). Sentiment classification has been a longstanding research topic (Liu, 2012; Pang et al., 2008; Chen et al., 2017; Moraes et al., 2013). Given a review, the task aims at predicting the sentiment polarity on the sentence level (Kim, 2014) or the aspect level (Li et al., 2018; Chen et al., 2017). Supervised learning algorithms have been widely used in sentiment classification (Pang et al., 2002). People usually use different expressions of sentiment s"
P18-1232,P18-1087,1,0.921412,"cial Administrative Region, China (Project Code: 14203414). Introduction Sentiment classification aims to predict the sentiment polarity, such as “positive” or “negative”, over a piece of review. It has been a long-standing research topic because of its importance for many applications such as social media analysis, ecommerce, and marketing (Liu, 2012; Pang et al., 2008). Deep learning has brought in progress in various NLP tasks, including sentiment classification. Some researchers focus on designing RNN or CNN based models for predicting sentence level (Kim, 2014) or aspect level sentiment (Li et al., 2018; Chen et al., 2017; Wang et al., 2016). These works directly take the word embeddings pre-trained for general purpose as initial word representations and may conduct fine tuning in the training process. Some other researchers look into the problem of learning taskspecific word embeddings for sentiment classification aiming at solving some limitations of applying general pre-trained word embeddings. For example, Tang et al. (2014b) develop a neural network model to convey sentiment information in the word embeddings. As a result, the learned embeddings are sentiment-aware and able to distingui"
P18-1232,W02-1011,0,0.021239,", word sense disambiguation (Collobert et al., 2011; Iacobacci et al., 2016; Zhang and Hasan, 2017; Dave et al., 2018), parsing (Roth and Lapata, 2016), and document classification (Tang et al., 2014a,b; Shi et al., 2017). Sentiment classification has been a longstanding research topic (Liu, 2012; Pang et al., 2008; Chen et al., 2017; Moraes et al., 2013). Given a review, the task aims at predicting the sentiment polarity on the sentence level (Kim, 2014) or the aspect level (Li et al., 2018; Chen et al., 2017). Supervised learning algorithms have been widely used in sentiment classification (Pang et al., 2002). People usually use different expressions of sentiment semantics in different domains. Due to the mismatch between domainspecific words, a sentiment classifier trained in one domain may not work well when it is directly applied to other domains. Thus cross-domain sentiment classification algorithms have been explored (Pan et al., 2010; Li et al., 2009; Glorot et al., 2011). These works usually find common feature spaces across domains and then share learned parameters from the source domain to the target domain. For example, Pan et al. (2010) propose a spectral feature alignment algorithm to"
P18-1232,P16-1113,0,0.0326032,"engio et al. propose a neural network architecture for this purpose (Bengio et al., 2003; Bengio, 2009). Later, Mikolov et al. (2013) propose two methods that are considerably more efficient, namely skip-gram and CBOW. This work has made it possible to learn word embeddings from large data sets, which has led to the current popularity of word embed2495 dings. Word embedding models have been applied to many tasks, such as named entity recognition (Turian et al., 2010), word sense disambiguation (Collobert et al., 2011; Iacobacci et al., 2016; Zhang and Hasan, 2017; Dave et al., 2018), parsing (Roth and Lapata, 2016), and document classification (Tang et al., 2014a,b; Shi et al., 2017). Sentiment classification has been a longstanding research topic (Liu, 2012; Pang et al., 2008; Chen et al., 2017; Moraes et al., 2013). Given a review, the task aims at predicting the sentiment polarity on the sentence level (Kim, 2014) or the aspect level (Li et al., 2018; Chen et al., 2017). Supervised learning algorithms have been widely used in sentiment classification (Pang et al., 2002). People usually use different expressions of sentiment semantics in different domains. Due to the mismatch between domainspecific wo"
P18-1232,C14-1018,0,0.363619,"ks, including sentiment classification. Some researchers focus on designing RNN or CNN based models for predicting sentence level (Kim, 2014) or aspect level sentiment (Li et al., 2018; Chen et al., 2017; Wang et al., 2016). These works directly take the word embeddings pre-trained for general purpose as initial word representations and may conduct fine tuning in the training process. Some other researchers look into the problem of learning taskspecific word embeddings for sentiment classification aiming at solving some limitations of applying general pre-trained word embeddings. For example, Tang et al. (2014b) develop a neural network model to convey sentiment information in the word embeddings. As a result, the learned embeddings are sentiment-aware and able to distinguish words with similar syntactic context but opposite sentiment polarity, such as the words “good” and “bad”. In fact, sentiment information can be easily obtained or derived in large scale from some data sources (e.g., the ratings provided by users), which allows reliable learning of such sentiment-aware embeddings. Apart from these words (e.g. “good” and “bad”) with consistent sentiment polarity in different contexts, the polari"
P18-1232,P14-1146,0,0.53574,"ks, including sentiment classification. Some researchers focus on designing RNN or CNN based models for predicting sentence level (Kim, 2014) or aspect level sentiment (Li et al., 2018; Chen et al., 2017; Wang et al., 2016). These works directly take the word embeddings pre-trained for general purpose as initial word representations and may conduct fine tuning in the training process. Some other researchers look into the problem of learning taskspecific word embeddings for sentiment classification aiming at solving some limitations of applying general pre-trained word embeddings. For example, Tang et al. (2014b) develop a neural network model to convey sentiment information in the word embeddings. As a result, the learned embeddings are sentiment-aware and able to distinguish words with similar syntactic context but opposite sentiment polarity, such as the words “good” and “bad”. In fact, sentiment information can be easily obtained or derived in large scale from some data sources (e.g., the ratings provided by users), which allows reliable learning of such sentiment-aware embeddings. Apart from these words (e.g. “good” and “bad”) with consistent sentiment polarity in different contexts, the polari"
P18-1232,P10-1040,0,0.0756154,"ed word representation instead, called word embeddings. Several techniques for generating such representations have been investigated. For example, Bengio et al. propose a neural network architecture for this purpose (Bengio et al., 2003; Bengio, 2009). Later, Mikolov et al. (2013) propose two methods that are considerably more efficient, namely skip-gram and CBOW. This work has made it possible to learn word embeddings from large data sets, which has led to the current popularity of word embed2495 dings. Word embedding models have been applied to many tasks, such as named entity recognition (Turian et al., 2010), word sense disambiguation (Collobert et al., 2011; Iacobacci et al., 2016; Zhang and Hasan, 2017; Dave et al., 2018), parsing (Roth and Lapata, 2016), and document classification (Tang et al., 2014a,b; Shi et al., 2017). Sentiment classification has been a longstanding research topic (Liu, 2012; Pang et al., 2008; Chen et al., 2017; Moraes et al., 2013). Given a review, the task aims at predicting the sentiment polarity on the sentence level (Kim, 2014) or the aspect level (Li et al., 2018; Chen et al., 2017). Supervised learning algorithms have been widely used in sentiment classification ("
P18-1232,D16-1058,0,0.10183,"Missing"
P18-1232,H05-1044,0,0.0490706,"ve because both of them consider the sentiment information in the embeddings. Our DSE model outperforms other methods which do not consider sentiments such as Yang, EmbeddingCat and EmbeddingAll. Note that the advantage of domain-sensitive embeddings would be insufficient for this task because the sentiment lexicons are not domain-specific. Lexicon Term Sentiment Classification To further evaluate the quality of the sentiment semantics of the learned word embeddings, we also conduct lexicon term sentiment classification on two popular sentiment lexicons, namely HL (Hu and Liu, 2004) and MPQA (Wilson et al., 2005). The words with neutral sentiment and phrases are removed. The statistics of HL and MPQA are shown in Table 3. We conduct multiple trials by selecting every possible two domains from books (B), DVDs (D), electronics items (E) and kitchen appliances (K). 5 Case Study Table 4 shows the probabilities of “lightweight”, “die”, “mysterious”, and “great” to be domaincommon for different domain combinations. For “lightweight”, its domain-common probability for the books domain and the DVDs domain (“B & D”) is quite high, i.e. p(z = 1) = 0.999, and the review examples in the last column show that the"
P18-1232,D17-1312,0,0.34591,"the word “lightweight” usually connotes a positive sentiment in the electronics domain since a lightweight device is easier to carry. In contrast, in the movie 2494 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2494–2504 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics domain, the word “lightweight” usually connotes a negative opinion describing movies that do not invoke deep thoughts among the audience. This observation motivates the study of learning domainsensitive word representations (Yang et al., 2017; Bollegala et al., 2015, 2014). They basically learn separate embeddings of the same word for different domains. To bridge the semantics of individual embedding spaces, they select a subset of words that are likely to be domain-insensitive and align the dimensions of their embeddings. However, the sentiment information is not exploited in these methods although they intend to tackle the task of sentiment classification. In this paper, we aim at learning word embeddings that are both domain-sensitive and sentiment-aware. Our proposed method can jointly model the sentiment semantics and domain"
P18-1232,P16-1085,0,\N,Missing
radev-etal-2004-mead,W02-0404,1,\N,Missing
radev-etal-2004-mead,H01-1056,1,\N,Missing
radev-etal-2004-mead,W00-1009,1,\N,Missing
radev-etal-2004-mead,P02-1040,0,\N,Missing
saggion-etal-2002-developing,J93-1004,0,\N,Missing
saggion-etal-2002-developing,W97-0703,0,\N,Missing
saggion-etal-2002-developing,A00-2035,0,\N,Missing
saggion-etal-2002-developing,W00-0408,0,\N,Missing
saggion-etal-2002-developing,E99-1011,0,\N,Missing
saggion-etal-2002-developing,W97-0704,0,\N,Missing
saggion-etal-2002-developing,W00-0403,1,\N,Missing
saggion-etal-2002-developing,grover-etal-2000-lt,0,\N,Missing
saggion-etal-2002-developing,J96-2004,0,\N,Missing
saggion-etal-2002-developing,I05-2047,0,\N,Missing
saggion-etal-2002-developing,W01-0100,0,\N,Missing
W17-4512,P15-1153,1,0.93793,"the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in inforIntroduction The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Bing et al., 2015; Li et al., 2017). In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents. With the development of social media and mobile equipments, more and more user generated ∗ The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). 1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ 2 91 https://goo.gl/DdU0vL Proceedings of the W"
W17-4512,D15-1166,0,0.0126449,"u(Wzh sz + bzh ) sx = sigmoid(Whx sh + bhx ) where x denotes a general sentence, and it can be a news sentence xd or a comment sentnece xc . By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) (3) VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state sih , we align it with each news encoder hidden state hjd 93 by an alignment vector ad ∈ Rnd . We also align it with each comments encoder hidden state hjc by an alignment vector ac ∈ Rnc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments: the news content, then it cont"
W17-4512,C12-1128,0,0.0727713,"Missing"
W17-4512,W00-0405,0,0.181996,"he summaries. One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in inforIntroduction The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Bing et al., 2015; Li et al., 2017). In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents. With the development of social media and mobile equipments, more and more user generated ∗ The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414"
W17-4512,P98-2222,0,0.140521,"n dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGESU4 are reported. 4.2 Experimental Settings Comparative Methods To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods: • RA-Sparse (Li et al., 2015): It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments. 5 5.1 • Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit. Results and Discussions Results on Our Dataset The results of our framework as well as the baseline methods are depicted in Table 1. It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the"
W17-4512,D12-1022,0,0.0241417,"rall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: X X max{ αi Si − αij (Si + Sj )Rij }, (12) i i<j where αi is the selection indicator for the phrase Pi , Si is the salience scores of Pi , αij and Rij is co-occurrence indicator and the similarity a pair of phrases (Pi , Pj ) respectively. The similarity is 94 calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), Bing et al. (2015), and Li et al. (2015). The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006). In the implementation, we use a package called lp solve5 . 3 “about an hour into its flight from Kuala Lumpur”, etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document. 3.2 The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find"
