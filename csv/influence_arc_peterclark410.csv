2020.emnlp-main.10,D15-1075,0,0.0251136,". More recently, 138 multihop datasets where the decomposition is not evident have appeared, e.g., WikiHop (Welbl et al., 2018), OBQA (Mihaylov et al., 2018), and QASC (Khot et al., 2020), posing more a realistic explanation challenge. However, explanation annotations are sparse (only QASC contains a single gold explanation per question), limiting their support for both training and evaluation of explanation systems, hence motivating this work. Finally there are a few human-authored explanation datasets. e-SNLI (Camburu et al., 2018) adds crowdsourced explanations to SNLI entailment problems (Bowman et al., 2015), and CoS-E (Rajani et al., 2019) adds explanations for CommonsenseQA questions (Talmor et al., 2019). This work differs from ours in two ways. First, the authored explanations are single-hop, directly linking a question to an answer. Second, the datasets were primarily designed for (explanation) language generation, while our goal is to assemble explanations from an authoritative corpus so that they have credible provenance. Our work is quite different from prior work focusing on textual entailment. Our goal is not to decide if a sentence is entailed, but to identify a valid explanation for w"
2020.emnlp-main.10,P18-1078,0,0.0514651,"ed and applied. We also find that using a variabilized version of the chains improves the models’ robustness. limits their utility for other tasks such as education and tutoring, where explanation plays a key role. This need has become particularly important with multihop question-answering, where multiple facts are needed to derive an answer. In this context, seeing a chain of reasoning leading to an answer, can help a user assess an answer’s validity. Our research here contributes to this goal. Introduction While neural systems have become remarkably adept at question answering (QA), e.g., (Clark and Gardner, 2018), their ability to explain those answers remains limited. This creates a barrier for deploying QA systems in practical settings, and 1 Code and datasets can be found at https://allenai. org/data/eqasc We are interested in questions where the decomposition into subquestions - hence the explanation structure - is not evident from the question, but has to be found. For example, “Does a suit of armor conduct electricity?” might be answered (hence explained) by first identifying what material armor is made of, even though the question itself does not mention materials. This contrasts with earlier m"
2020.emnlp-main.10,N19-1423,0,0.0399946,"ing determiners and adjectives into detected entities). e.g. ‘the blue whale is a mammal’ and ‘the blue whale breathes..’ leads to detection of ’the blue whale’). (3) Then, we assign a special token to each of the candidates, using a predefined set of unused tokens, which can be viewed as a set of variables. Some examples of GRCs are shown in Figure 3 and later in Figure 4, using X,Y,Z as the special token set (As our models are BERT-based, we use unusedi i ∈ {1, 2, ...} to denote these tokens). 4.2 Model Training To distinguish valid from invalid chains, we finetune a pre-trained BERT model (Devlin et al., 2019) for scoring the possible explanation chains. We encode a chain f 1 AND f 2 → H as: [CLS] f 1 [SEP] f 2 [SEP] H where [SEP] is a sentence boundary marker. Thereafter, we pass the chain through the BERT model (BERT-base-uncased). We employ a two layer feedforward neural network with ReLU non-linearity, as a binary classifier on the pooled [CLS] representation to predict valid vs invalid reasoning chains. Model parameters are trained to minimize the binary cross entropy loss. 141 5 Experiments For training, we use the annotated chains in the train split of eQASC alongwith the ‘gold’ chains provi"
2020.emnlp-main.10,2020.acl-main.408,0,0.0422312,"ablish baseline scores. Finally, we explore a delexicalized chain representation in which repeated noun phrases are replaced by variables, thus turning them into generalized reasoning chains, as illustrated in Figure 1. We find that generalized chains maintain performance while also being more robust to perturbations, suggesting a promising avenue for further research. 2 Related Work In the context of QA, there are multiple notions of explanation/justification, including showing an authoritative, answer-bearing sentence (Perez et al., 2019), a collection of text snippets supporting an answer (DeYoung et al., 2020), an attention map over a passage (Seo et al., 2016), a synthesized phrase connecting question and answer (Rajani et al., 2019), or the syntactic pattern used to locate the answer (Ye et al., 2020; Hancock et al., 2018). These methods are primarily designed for answers to “lookup” questions, to explain where and how an answer was found in a corpus. For questions requiring inference, the focus of this paper, an explanation is often taken as the chain of steps (typically sentences) leading to an answer. HotpotQA’s support task goes partway towards this by asking for answer-supporting sentences ("
2020.emnlp-main.10,W18-2501,0,0.0226009,"Missing"
2020.emnlp-main.10,P18-1175,0,0.0627888,"Missing"
2020.emnlp-main.10,D18-1260,1,0.928109,"Empirical Methods in Natural Language Processing, pages 137–150, c November 16–20, 2020. 2020 Association for Computational Linguistics Figure 2: QASC contains multiple-choice questions, plus one gold (valid) reasoning chain for the correct answer. To find valid reasoning chains, we first generate candidates for each answer option using a 2-step retrieval process (Section 3.2). We then collect annotations for the correct answer option chains to train and evaluate models to detect valid reasoning chains. (Above, chains A1 and A3 are valid, while A2, B1, and B2 are invalid). include OpenBookQA (Mihaylov et al., 2018) and more recently QASC (Khot et al., 2020). However, although providing QA pairs, these datasets provide limited explanation information. OpenBookQA does not come with any explanation data, and QASC only provides a single gold explanation for each answer, while in practice there may be multiple valid explanations. To alleviate this lack of data, we contribute three new datasets: The first (and largest) is eQASC, containing annotations on over 98K candidate explanations for the QASC dataset, including on multiple (typically 10) possible explanations for each answer, including both valid and in"
2020.emnlp-main.10,D19-1244,0,0.258831,"e these datasets to build models for detecting valid explanations, to establish baseline scores. Finally, we explore a delexicalized chain representation in which repeated noun phrases are replaced by variables, thus turning them into generalized reasoning chains, as illustrated in Figure 1. We find that generalized chains maintain performance while also being more robust to perturbations, suggesting a promising avenue for further research. 2 Related Work In the context of QA, there are multiple notions of explanation/justification, including showing an authoritative, answer-bearing sentence (Perez et al., 2019), a collection of text snippets supporting an answer (DeYoung et al., 2020), an attention map over a passage (Seo et al., 2016), a synthesized phrase connecting question and answer (Rajani et al., 2019), or the syntactic pattern used to locate the answer (Ye et al., 2020; Hancock et al., 2018). These methods are primarily designed for answers to “lookup” questions, to explain where and how an answer was found in a corpus. For questions requiring inference, the focus of this paper, an explanation is often taken as the chain of steps (typically sentences) leading to an answer. HotpotQA’s support"
2020.emnlp-main.10,P19-1487,0,0.0697178,"variables, thus turning them into generalized reasoning chains, as illustrated in Figure 1. We find that generalized chains maintain performance while also being more robust to perturbations, suggesting a promising avenue for further research. 2 Related Work In the context of QA, there are multiple notions of explanation/justification, including showing an authoritative, answer-bearing sentence (Perez et al., 2019), a collection of text snippets supporting an answer (DeYoung et al., 2020), an attention map over a passage (Seo et al., 2016), a synthesized phrase connecting question and answer (Rajani et al., 2019), or the syntactic pattern used to locate the answer (Ye et al., 2020; Hancock et al., 2018). These methods are primarily designed for answers to “lookup” questions, to explain where and how an answer was found in a corpus. For questions requiring inference, the focus of this paper, an explanation is often taken as the chain of steps (typically sentences) leading to an answer. HotpotQA’s support task goes partway towards this by asking for answer-supporting sentences (but not how they combine) (Yang et al., 2018). The R4C dataset takes this further, annotating how (and which) HotpotQA supporti"
2020.emnlp-main.10,D19-1340,0,0.0256422,"ic reasoning chains are often instantiations of more general patterns. For example, in Figure 1, the specific explanation can be seen as an instantiation of the more general pattern “X can cause Y” AND “Y can start Z” IMPLIES “X can cause Z”. We refer to such patterns as Generalized Reasoning Chains (GRCs). To encourage our model to recognize valid and invalid chains at the pattern level, we explore the following strategy: First, we transform candidate chains into generalized chains (GRCs) by replacing repeated noun phrases with variables (special tokens), a process known as delexicalization (Suntwal et al., 2019). We then train and test the model using the GRC representation. We hypothesize that distinguishing a valid justification chain from an invalid one should not need typing information in most cases. To identify the phrases to variabilize, (1) we first perform part-of-speech tagging on the sentences, and (2) extract candidate entities by identifying repeating nouns i.e. those which occur in at least two of the sentences in the chain (We stem the words before matching, and include any matching preceding determiners and adjectives into detected entities). e.g. ‘the blue whale is a mammal’ and ‘the"
2020.emnlp-main.10,N19-1421,0,0.0190074,"kiHop (Welbl et al., 2018), OBQA (Mihaylov et al., 2018), and QASC (Khot et al., 2020), posing more a realistic explanation challenge. However, explanation annotations are sparse (only QASC contains a single gold explanation per question), limiting their support for both training and evaluation of explanation systems, hence motivating this work. Finally there are a few human-authored explanation datasets. e-SNLI (Camburu et al., 2018) adds crowdsourced explanations to SNLI entailment problems (Bowman et al., 2015), and CoS-E (Rajani et al., 2019) adds explanations for CommonsenseQA questions (Talmor et al., 2019). This work differs from ours in two ways. First, the authored explanations are single-hop, directly linking a question to an answer. Second, the datasets were primarily designed for (explanation) language generation, while our goal is to assemble explanations from an authoritative corpus so that they have credible provenance. Our work is quite different from prior work focusing on textual entailment. Our goal is not to decide if a sentence is entailed, but to identify a valid explanation for why. For example, a SciTail (Khot et al., 2018) model may predict that “metals conduct heat” entails “"
2020.emnlp-main.10,W18-5501,0,0.0171097,"ions from an authoritative corpus so that they have credible provenance. Our work is quite different from prior work focusing on textual entailment. Our goal is not to decide if a sentence is entailed, but to identify a valid explanation for why. For example, a SciTail (Khot et al., 2018) model may predict that “metals conduct heat” entails “a spoon transmits energy”, but not offer an explanation as to why. Our work fills this gap by providing an explanation (e.g., “spoon is made of metal”, “heat is energy” from a larger retrieved context). Similarly, another entailmentbased dataset is FEVER (Thorne et al., 2018), testing where a larger context entails a claim. However, the FEVER task requires finding a context sentence that simply paraphrases the claim, rather than a reasoned-based explanation from more general statements - the aim of this work. 3 Explanation Datasets We now present our new datasets, first describing how we construct candidate chains for each QA pair, and then how they were annotated. 3.1 Task Definition We consider the task where the input is a question Q, (correct) answer A, and a corpus of sentences T . The (desired) output is a valid reasoning chain, constructed from sentences in"
2020.emnlp-main.10,I11-1101,0,0.0640044,"Missing"
2020.emnlp-main.10,Q18-1021,0,0.0229958,"s the chain of steps (typically sentences) leading to an answer. HotpotQA’s support task goes partway towards this by asking for answer-supporting sentences (but not how they combine) (Yang et al., 2018). The R4C dataset takes this further, annotating how (and which) HotpotQA supporting sentences chain together (Inoue et al., 2019). However, in HotpotQA and R4C, the decomposition (hence structure of the explanation) is evident in the question (Mihaylov et al., 2018), simplifying the task. More recently, 138 multihop datasets where the decomposition is not evident have appeared, e.g., WikiHop (Welbl et al., 2018), OBQA (Mihaylov et al., 2018), and QASC (Khot et al., 2020), posing more a realistic explanation challenge. However, explanation annotations are sparse (only QASC contains a single gold explanation per question), limiting their support for both training and evaluation of explanation systems, hence motivating this work. Finally there are a few human-authored explanation datasets. e-SNLI (Camburu et al., 2018) adds crowdsourced explanations to SNLI entailment problems (Bowman et al., 2015), and CoS-E (Rajani et al., 2019) adds explanations for CommonsenseQA questions (Talmor et al., 2019). This"
2020.emnlp-main.10,D18-1259,0,0.0687886,"ers remains limited. This creates a barrier for deploying QA systems in practical settings, and 1 Code and datasets can be found at https://allenai. org/data/eqasc We are interested in questions where the decomposition into subquestions - hence the explanation structure - is not evident from the question, but has to be found. For example, “Does a suit of armor conduct electricity?” might be answered (hence explained) by first identifying what material armor is made of, even though the question itself does not mention materials. This contrasts with earlier multihop QA datasets, e.g., HotpotQA (Yang et al., 2018), where the explanation structure is evident in the question itself. For example, “What nationality was James Miller’s wife?” implies a chain of reasoning to first finds Miller’s wife, then her nationality. Such cases are easier but less representative of natural questions. Multihop datasets of the kind where explanation structure is not evident 137 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 137–150, c November 16–20, 2020. 2020 Association for Computational Linguistics Figure 2: QASC contains multiple-choice questions, plus one gold (valid) r"
2020.emnlp-main.10,2020.findings-emnlp.145,0,0.0270728,"Missing"
2020.emnlp-main.520,W13-2322,0,0.0200382,"f our dataset to existing datasets edge, (ii) zero shot learning: during inference on a previously unseen domain, there are previously unseen attributes, entities, and state change types. This makes the problem very challenging and places this task in a novel setting (see §3.1) 3 Related Work Tracking state changes: Procedural text understanding addresses the task of tracking entity states throughout the text (Bosselut et al., 2018; Henaff et al., 2017). This ability is an important part of 6410 text understanding. While syntactic parsing methods such as AMR (abstract meaning representation) (Banarescu et al., 2013) represent “who did what to whom” by uncovering stated facts, tracking entity states uncovers unstated facts such as how ingredients change during a recipe. Datasets with closed state changes: The bAbI dataset (Weston et al., 2015) includes questions about objects moved throughout a paragraph, using machine-generated language over a deterministic domain with a small lexicon. The SCoNE dataset (Long et al., 2016) contains paragraphs describing a changing world state in three synthetic, deterministic domains. However, approaches developed using synthetic data often fail to handle the inherent co"
2020.emnlp-main.520,P16-1138,0,0.0281795,"ut et al., 2018; Henaff et al., 2017). This ability is an important part of 6410 text understanding. While syntactic parsing methods such as AMR (abstract meaning representation) (Banarescu et al., 2013) represent “who did what to whom” by uncovering stated facts, tracking entity states uncovers unstated facts such as how ingredients change during a recipe. Datasets with closed state changes: The bAbI dataset (Weston et al., 2015) includes questions about objects moved throughout a paragraph, using machine-generated language over a deterministic domain with a small lexicon. The SCoNE dataset (Long et al., 2016) contains paragraphs describing a changing world state in three synthetic, deterministic domains. However, approaches developed using synthetic data often fail to handle the inherent complexity in language when applied to organic, real-world data (Hermann et al., 2015; Winograd, 1972). The ProPara dataset (Dalvi et al., 2018) contains three state changes (create, destroy, move) for natural text describing scientific procedures. Other domain specific datasets include recipe domain (Bosselut et al., 2018), and biology experiments (Mysore et al., 2019). These datasets contain a small, closed set"
2020.emnlp-main.520,W19-4007,0,0.173076,"ns fill that gap easily with their commonsense but machines need to model these effects in the form of state changes. For example, when a potato is rubbed on a car window (to defog it), then the unstated effects of this action are the following state changes: windows becomes sticky, opaque, and the potato becomes dirty, etc. These changes can be tracked across the paragraph. An exemplary use case of text with actions is procedural 1 text (recipes, how-to guides, etc.) where modeling such state changes helps in various reasoning-based end tasks, e.g. automatic execution of biology experiments (Mysore et al., 2019), cooking recipes (Bollini et al., 2012) and everyday activities (Yang and Nyberg, 2015). While there has been great progress in tracking entity states in scientific processes (Dalvi et al., 2018), tracking ingredients in cooking recipes (Bosselut et al., 2018), and tracking the emotional reactions and motivations of characters in simple stories (Rashkin et al., 2018), prior tasks are restricted to a fixed, small set of state change types thus covering only a small fraction of the entire world state. Figure 1 illustrates this for a real-world procedure “How to Keep Car Windows Fog Free Using a"
2020.emnlp-main.520,P18-1213,0,0.231387,"ragraph. An exemplary use case of text with actions is procedural 1 text (recipes, how-to guides, etc.) where modeling such state changes helps in various reasoning-based end tasks, e.g. automatic execution of biology experiments (Mysore et al., 2019), cooking recipes (Bollini et al., 2012) and everyday activities (Yang and Nyberg, 2015). While there has been great progress in tracking entity states in scientific processes (Dalvi et al., 2018), tracking ingredients in cooking recipes (Bosselut et al., 2018), and tracking the emotional reactions and motivations of characters in simple stories (Rashkin et al., 2018), prior tasks are restricted to a fixed, small set of state change types thus covering only a small fraction of the entire world state. Figure 1 illustrates this for a real-world procedure “How to Keep Car Windows Fog Free Using a Potato”. Existing datasets such as ProPara (Dalvi et al., 2018) only model the existence and location attributes, limiting the fidelity with which they model the world. Specifically: • Attributes from domain-specific datasets such Download O PEN PI at https://allenai.org/data/openpi 6408 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Proc"
2020.emnlp-main.520,D18-1006,1,0.80527,"3 Manually inspecting the 45 predictions made by COMET Person needs to be arrested ) Person is arrested, gets dirty. 3.1 Positioning O PEN PI Figure 2.1 projects existing tasks and models along two different dimensions (open vocabulary, and variable-size low-specificity). We find that models bottom-left quadrant represents majority of the existing work on state changes such as ProPara (Dalvi et al., 2018) and bAbI (Weston et al., 2016)) in NLP community, and ALFRED (Shridhar et al., 2019) and VirtualHome (Puig et al., 2018) in Computer Vision. Correspondingly many models exist in that space ((Tandon et al., 2018), (Bosselut et al., 2018), (Henaff et al., 2017)). Very few models exist that can predict either open vocab (Rashkin et al., 2018), or variable size output (Bosselut et al., 2018). However, no existing task has both open vocabulary and variable-size low specificity– placing O PEN PI in a novel space. 4 4.1 Dataset Data Collection We set up a crowdsourcing task on Amazon Mechanical Turk where the annotators author the y= {yi } for every sentence of a wikihow.com article, filling in a sentence template for each yi as a guide. WikiHow contains a wide variety of goals (e.g., how to wash dishes) br"
2020.emnlp-main.520,H89-1033,0,0.573348,"uncovers unstated facts such as how ingredients change during a recipe. Datasets with closed state changes: The bAbI dataset (Weston et al., 2015) includes questions about objects moved throughout a paragraph, using machine-generated language over a deterministic domain with a small lexicon. The SCoNE dataset (Long et al., 2016) contains paragraphs describing a changing world state in three synthetic, deterministic domains. However, approaches developed using synthetic data often fail to handle the inherent complexity in language when applied to organic, real-world data (Hermann et al., 2015; Winograd, 1972). The ProPara dataset (Dalvi et al., 2018) contains three state changes (create, destroy, move) for natural text describing scientific procedures. Other domain specific datasets include recipe domain (Bosselut et al., 2018), and biology experiments (Mysore et al., 2019). These datasets contain a small, closed set of state change types that are relevant to a specific domain. Our dataset is general domain, and to accommodate this generality we have an open vocabulary of state changes. Datasets with open state changes: (Isola et al., 2015) propose manually defined antonymous adjective pairs (big,"
2020.findings-emnlp.171,N19-1300,0,0.202745,"n answering is a common tool for assessing how well can computers understand language and reason with it. To this end, the NLP community has introduced several distinct datasets, with four popular QA formats illustrated in Fig. 1. For instance, some datasets expect the answer to be “yes” or “no”, or a unique answer span in the associated paragraph (as opposed to multiple or no spans). These differences have motivated their study in silos, often encoding QA format into the model architecture itself. Efforts to exploit multiple datasets remain largely restricted to a single format. For example, Clark et al. (2019c) limit consideration to 1 https://github.com/allenai/unifiedqa multiple-choice datasets, while Talmor and Berant (2019) focus their generalization study on extractive span prediction models. To the best of our knowledge, no single QA system targets, not to mention excels at, all of these formats. This raises the question: Can QA models learn linguistic reasoning abilities that generalize across formats? Our intuition is simple: while question format and relevant knowledge may vary across QA datasets, the underlying linguistic understanding and reasoning abilities are largely common. A multip"
2020.findings-emnlp.171,P19-1595,0,0.0581037,"Missing"
2020.findings-emnlp.171,D19-1606,0,0.0603237,"Missing"
2020.findings-emnlp.171,N19-1423,0,0.0418664,"an any single-format expert. For example, while the system is trained on multiple-choice questions with 4 candidate answers, it works quite well on datasets with more than 4 candidate answers (QASC and CommonsenseQA have has 8 and 5 candidate answers per question, respectively). (3) Single-format experts are better at generalization only when the source and target datasets are very similar (for instance SQuAD and Quoref). 6.3 State-of-the-Art via Simple Fine-tuning Fine-tuning of pre-trained language models has become the standard paradigm for building datasetspecific stat-of-the-art systems (Devlin et al., 2019; Liu et al., 2019). The question we address here is: when it comes to QA, is there a value in using U NIFIED QA as a starting point for fine-tuning, as opposed to a vanilla language model that has not seen other QA datasets before? To address this question, we fine-tune each of U NIFIED QA, T5, and BART on several datasets by selecting the best check point on the dev set, and evaluating on the test set. Table 5 summarizes the results of the experiments. The table shows two variants: U NIFIED QAT5 and U NIFIED QABART . All results are based on the 11B version of T5. The columns indicate the ev"
2020.findings-emnlp.171,D19-5820,0,0.0458318,"asets (§6.3), establishing it as a powerful starting point for QA research. Our findings demonstrate that crossing QA format boundaries is not only qualitatively desirable but also quantitatively beneficial. 2 Related Work Several QA efforts have studied generalization across datasets of a single format. For instance, in MultiQA, Talmor and Berant (2019) study generalization and transfer, but only across extractive span selection datasets. Further, while they show strong leave-one-out style results, they find a single system performs substantially worse than one tuned to each dataset. In ORB, Dua et al. (2019a) propose a multi-dataset evaluation benchmark spanning extractive and abstractive formats. However, that study is limited to an evaluation of systems, falling short of addressing how to build such generalized models. The MRQA shared task (Fisch et al., 2019) focuses on span-prediction datasets. Unlike all these efforts, our goal is to investigate transfer and generalization across different QA formats, as well as to build a single system that does this well. Exploiting commonality across machine learning tasks has a rich history studied under transfer learning (Caruana, 1997; Clark et al., 2"
2020.findings-emnlp.171,N19-1246,0,0.120445,"asets (§6.3), establishing it as a powerful starting point for QA research. Our findings demonstrate that crossing QA format boundaries is not only qualitatively desirable but also quantitatively beneficial. 2 Related Work Several QA efforts have studied generalization across datasets of a single format. For instance, in MultiQA, Talmor and Berant (2019) study generalization and transfer, but only across extractive span selection datasets. Further, while they show strong leave-one-out style results, they find a single system performs substantially worse than one tuned to each dataset. In ORB, Dua et al. (2019a) propose a multi-dataset evaluation benchmark spanning extractive and abstractive formats. However, that study is limited to an evaluation of systems, falling short of addressing how to build such generalized models. The MRQA shared task (Fisch et al., 2019) focuses on span-prediction datasets. Unlike all these efforts, our goal is to investigate transfer and generalization across different QA formats, as well as to build a single system that does this well. Exploiting commonality across machine learning tasks has a rich history studied under transfer learning (Caruana, 1997; Clark et al., 2"
2020.findings-emnlp.171,D19-5801,0,0.0264481,"neralization across datasets of a single format. For instance, in MultiQA, Talmor and Berant (2019) study generalization and transfer, but only across extractive span selection datasets. Further, while they show strong leave-one-out style results, they find a single system performs substantially worse than one tuned to each dataset. In ORB, Dua et al. (2019a) propose a multi-dataset evaluation benchmark spanning extractive and abstractive formats. However, that study is limited to an evaluation of systems, falling short of addressing how to build such generalized models. The MRQA shared task (Fisch et al., 2019) focuses on span-prediction datasets. Unlike all these efforts, our goal is to investigate transfer and generalization across different QA formats, as well as to build a single system that does this well. Exploiting commonality across machine learning tasks has a rich history studied under transfer learning (Caruana, 1997; Clark et al., 2019b). McCann et al. (2018) and Keskar et al. (2019) study transfer among various NLP tasks by casting them into a single QA format—an elegant transfer learning approach but orthogonal to the goal of this work. As noted earlier, Raffel et al. (2020) investigat"
2020.findings-emnlp.171,2020.emnlp-main.550,1,0.900352,"Missing"
2020.findings-emnlp.171,N18-1023,1,0.910292,"Missing"
2020.findings-emnlp.171,2020.emnlp-main.12,1,0.824253,"Missing"
2020.findings-emnlp.171,Q18-1023,0,0.0434413,"e system U NIFIED QA. We also report results of training our system with BARTlarge , referred to as U NI FIED QABART (see §6.3). Details on the parameters of the models used are deferred to Appendix A.2. 3 Future references to ‘seed dataset’ point to the QA datasets used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakaguchi et al., 2020). Several of the MC datasets do not come with accompanying paragraphs (such as ARC, QASC, OBQA). For most of this the work, we keep the questi"
2020.findings-emnlp.171,Q19-1026,0,0.0482833,"ARTlarge , referred to as U NI FIED QABART (see §6.3). Details on the parameters of the models used are deferred to Appendix A.2. 3 Future references to ‘seed dataset’ point to the QA datasets used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakaguchi et al., 2020). Several of the MC datasets do not come with accompanying paragraphs (such as ARC, QASC, OBQA). For most of this the work, we keep the questions as is with no additional retrieval (unless otherwise mentioned). One other"
2020.findings-emnlp.171,D17-1082,0,0.0548349,"ed dataset’ point to the QA datasets used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakaguchi et al., 2020). Several of the MC datasets do not come with accompanying paragraphs (such as ARC, QASC, OBQA). For most of this the work, we keep the questions as is with no additional retrieval (unless otherwise mentioned). One other variability among these datasets is their number of candidate answers. While many datasets have four candidates (see Fig. 2), others have more. Lat"
2020.findings-emnlp.171,2020.acl-main.703,0,0.297564,"✓ Figure 2: Properties of various QA datasets included in this study: 5 extractive (EX), 3 abstractive (AB), 9 multiplechoice (MC), and 3 yes/no (YN). ‘idk’ denotes ‘I don’t know’ or unanswerable questions. BoolQ represents both the original dataset and its contrast-sets extension BoolQ-CS; similarly for ROPES, Quoref, and DROP. commonsense QA datasets listed in Fig. 2. In this work, we advocate for a unifying view of QA formats by building a format-agnostic QA system. Our work leverages recent progress in text-to-text pre-trained neural models, specifically T5 (Raffel et al., 2020) and BART (Lewis et al., 2020), but with a strong focus on differing QA formats. This paradigm allows unifying many NLP models, which formerly had task-specific designs, into a single text-to-text framework. Previous work uses textual prefixes to explicitly define the task associated with each input instance (Raffel et al., 2020; Radford et al., 2019b); often such attempts to build a single model for multiple NLP tasks underperform the standard pre-training plus finetuning setup (a model per task) (Raffel et al., 2020). Our work narrows down the scope to tasks that stay within the boundaries of QA, demonstrating that a uni"
2020.findings-emnlp.171,N06-1059,0,0.0398904,", and the binary (yes/no) subset of MultiRC (Khashabi et al., 2018). Contrast-sets. Additionally, we use contrastsets (Gardner et al., 2020) for several of our datasets (denoted with “CS”): BoolQ-CS, ROPESCS, Quoref-CS, DROP-CS. These evaluation sets are expert-generated perturbations that deviate from the patterns common in the original dataset. 4.2 Evaluation Metrics for Textual Output We evaluate each dataset using the metric used most often for it in prior work. For the EX format, it’s the F1 score of the extracted span relative to the gold label. For the AB format, we use ROUGE-L metric (Lin et al., 2006; Min et al., 2019; Nishida et al., 2019). For NatQA we use the exact-match metric, following Min et al. (2020). For the MC format, we match the generated text with the closest answer candidate based token overlap and compute the accuracy. For the YN format, we follow Clark et al. (2019a) to measure if the generated output matches the correct ‘yes’ or ‘no’ label. In rare cases where the output is longer than one word (e.g., ‘yes it is’), we check if it contains the correct label but Pilot Study: Can Out-of-Format Training Help? We first answer the question: Is the broad idea of benefiting from"
2020.findings-emnlp.171,D19-5808,1,0.901882,"argest available T5 model (11B parameters) as the starting point for training our model and call the system U NIFIED QA. We also report results of training our system with BARTlarge , referred to as U NI FIED QABART (see §6.3). Details on the parameters of the models used are deferred to Appendix A.2. 3 Future references to ‘seed dataset’ point to the QA datasets used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakaguchi et al., 2020). Several of the MC datasets do not com"
2020.findings-emnlp.171,2021.ccl-1.108,0,0.333636,"Missing"
2020.findings-emnlp.171,D18-1260,1,0.840127,"used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakaguchi et al., 2020). Several of the MC datasets do not come with accompanying paragraphs (such as ARC, QASC, OBQA). For most of this the work, we keep the questions as is with no additional retrieval (unless otherwise mentioned). One other variability among these datasets is their number of candidate answers. While many datasets have four candidates (see Fig. 2), others have more. Later (in §6.2) we will see that our approac"
2020.findings-emnlp.171,D19-1284,1,0.85112,"yes/no) subset of MultiRC (Khashabi et al., 2018). Contrast-sets. Additionally, we use contrastsets (Gardner et al., 2020) for several of our datasets (denoted with “CS”): BoolQ-CS, ROPESCS, Quoref-CS, DROP-CS. These evaluation sets are expert-generated perturbations that deviate from the patterns common in the original dataset. 4.2 Evaluation Metrics for Textual Output We evaluate each dataset using the metric used most often for it in prior work. For the EX format, it’s the F1 score of the extracted span relative to the gold label. For the AB format, we use ROUGE-L metric (Lin et al., 2006; Min et al., 2019; Nishida et al., 2019). For NatQA we use the exact-match metric, following Min et al. (2020). For the MC format, we match the generated text with the closest answer candidate based token overlap and compute the accuracy. For the YN format, we follow Clark et al. (2019a) to measure if the generated output matches the correct ‘yes’ or ‘no’ label. In rare cases where the output is longer than one word (e.g., ‘yes it is’), we check if it contains the correct label but Pilot Study: Can Out-of-Format Training Help? We first answer the question: Is the broad idea of benefiting from out-of-format tra"
2020.findings-emnlp.171,2020.emnlp-main.466,1,0.911295,"astsets (Gardner et al., 2020) for several of our datasets (denoted with “CS”): BoolQ-CS, ROPESCS, Quoref-CS, DROP-CS. These evaluation sets are expert-generated perturbations that deviate from the patterns common in the original dataset. 4.2 Evaluation Metrics for Textual Output We evaluate each dataset using the metric used most often for it in prior work. For the EX format, it’s the F1 score of the extracted span relative to the gold label. For the AB format, we use ROUGE-L metric (Lin et al., 2006; Min et al., 2019; Nishida et al., 2019). For NatQA we use the exact-match metric, following Min et al. (2020). For the MC format, we match the generated text with the closest answer candidate based token overlap and compute the accuracy. For the YN format, we follow Clark et al. (2019a) to measure if the generated output matches the correct ‘yes’ or ‘no’ label. In rare cases where the output is longer than one word (e.g., ‘yes it is’), we check if it contains the correct label but Pilot Study: Can Out-of-Format Training Help? We first answer the question: Is the broad idea of benefiting from out-of-format training even viable? For instance, is our intuition correct that an MC dataset can, in practice"
2020.findings-emnlp.171,P19-1225,0,0.0223133,"MultiRC (Khashabi et al., 2018). Contrast-sets. Additionally, we use contrastsets (Gardner et al., 2020) for several of our datasets (denoted with “CS”): BoolQ-CS, ROPESCS, Quoref-CS, DROP-CS. These evaluation sets are expert-generated perturbations that deviate from the patterns common in the original dataset. 4.2 Evaluation Metrics for Textual Output We evaluate each dataset using the metric used most often for it in prior work. For the EX format, it’s the F1 score of the extracted span relative to the gold label. For the AB format, we use ROUGE-L metric (Lin et al., 2006; Min et al., 2019; Nishida et al., 2019). For NatQA we use the exact-match metric, following Min et al. (2020). For the MC format, we match the generated text with the closest answer candidate based token overlap and compute the accuracy. For the YN format, we follow Clark et al. (2019a) to measure if the generated output matches the correct ‘yes’ or ‘no’ label. In rare cases where the output is longer than one word (e.g., ‘yes it is’), we check if it contains the correct label but Pilot Study: Can Out-of-Format Training Help? We first answer the question: Is the broad idea of benefiting from out-of-format training even viable? For"
2020.findings-emnlp.171,N15-1082,1,0.895224,"Missing"
2020.findings-emnlp.171,P18-2124,0,0.0304563,"e datasets become available or new formats are introduced. Unless otherwise noted, we use the largest available T5 model (11B parameters) as the starting point for training our model and call the system U NIFIED QA. We also report results of training our system with BARTlarge , referred to as U NI FIED QABART (see §6.3). Details on the parameters of the models used are deferred to Appendix A.2. 3 Future references to ‘seed dataset’ point to the QA datasets used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et a"
2020.findings-emnlp.171,D16-1264,0,0.108989,"ED QA model, or extend it as future datasets become available or new formats are introduced. Unless otherwise noted, we use the largest available T5 model (11B parameters) as the starting point for training our model and call the system U NIFIED QA. We also report results of training our system with BARTlarge , referred to as U NI FIED QABART (see §6.3). Details on the parameters of the models used are deferred to Appendix A.2. 3 Future references to ‘seed dataset’ point to the QA datasets used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA ("
2020.findings-emnlp.171,D13-1020,0,0.0572589,"A.2. 3 Future references to ‘seed dataset’ point to the QA datasets used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakaguchi et al., 2020). Several of the MC datasets do not come with accompanying paragraphs (such as ARC, QASC, OBQA). For most of this the work, we keep the questions as is with no additional retrieval (unless otherwise mentioned). One other variability among these datasets is their number of candidate answers. While many datasets have four candidates (see Fig."
2020.findings-emnlp.171,2020.emnlp-main.437,0,0.0611834,"ethodology, U NIFIED QA achieves performance on par with 8 dataset-specific expert models (§6.1), while also generalizing well to many unseen datasets of seen formats (§6.2). At the same time, we demonstrated that U NIFIED QA is a strong starting point for building QA systems: it can achieve state-of-the-art performance by simply fine-tuning on target datasets (6.3). We hope this effort will inspire a future line of work in the QA and NLP communities, moving towards more general and broader system designs. We leave extensions of U NIFIED QA to other formats such as to direct-answer questions (Roberts et al., 2020) as a promising avenue for future work. Acknowledgments The authors would like to thank Collin Raffel, Adam Roberts, and Nicholas Lourie for their help with the T5 framework and for providing feedback on an earlier version of this work. The authors would like to acknowledge grants by ONR N00014-18-1-2826 and DARPA N66001-19-2-403, and gifts from the Sloan Foundation and the Allen Institute for AI. Moreover, the authors would like to thank members of the Allen Institute for AI, UW-NLP, and the H2Lab at the University of Wash1904 ington for their valuable feedback and comments. TPU machines for"
2020.findings-emnlp.171,P16-1043,0,0.0228867,"ta and confirms our intuition that there are strong ties across QA formats in terms of the underlying reasoning abilities.2 Our unified question-answering system is based on the recent text-to-text frameworks, particularly, T5 (Raffel et al., 2020) and BART (Lewis et al., 2020). We first define a unifying encoding of the instances across various formats (§3.1). We then introduce U NIFIED QA (§3.2) that is a QA system trained on datasets in multiple formats, indicating new state-of-the-art results on 10 datasets and generalization to unseen datasets. 2 A more sophisticated teaching curriculum (Sachan and Xing, 2016) or approaches such as model distillation and teacher annealing (Clark et al., 2019b) are likely to further improve the performance of the resulting unified model, bolstering the strength of our advocacy for a unified view of all QA formats. We leave their exploration to future work. 3.1 Text-to-Text Encoding We convert each of our target datasets into a textin/text-out format (Raffel et al., 2020; Lewis et al., 2020; Radford et al., 2019b). The question always comes first, followed by some additional information (context paragraph or candidate answers, or both). We use “
” separators between"
2020.findings-emnlp.171,D19-1454,0,0.102815,"Missing"
2020.findings-emnlp.171,P19-1485,0,0.101486,"nd, the NLP community has introduced several distinct datasets, with four popular QA formats illustrated in Fig. 1. For instance, some datasets expect the answer to be “yes” or “no”, or a unique answer span in the associated paragraph (as opposed to multiple or no spans). These differences have motivated their study in silos, often encoding QA format into the model architecture itself. Efforts to exploit multiple datasets remain largely restricted to a single format. For example, Clark et al. (2019c) limit consideration to 1 https://github.com/allenai/unifiedqa multiple-choice datasets, while Talmor and Berant (2019) focus their generalization study on extractive span prediction models. To the best of our knowledge, no single QA system targets, not to mention excels at, all of these formats. This raises the question: Can QA models learn linguistic reasoning abilities that generalize across formats? Our intuition is simple: while question format and relevant knowledge may vary across QA datasets, the underlying linguistic understanding and reasoning abilities are largely common. A multiple-choice model may, therefore, benefit from training on an extractive answers dataset. Building upon this intuition, we"
2020.findings-emnlp.171,N19-1421,0,0.0331123,"1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakaguchi et al., 2020). Several of the MC datasets do not come with accompanying paragraphs (such as ARC, QASC, OBQA). For most of this the work, we keep the questions as is with no additional retrieval (unless otherwise mentioned). One other variability among these datasets is their number of candidate answers. While many datasets have four candidates (see Fig. 2), others have more. Later (in §6.2) we will see that our approach generalizes to datasets with different numbers of candidates, even if such questions have not bee"
2020.findings-emnlp.171,W17-2623,0,0.0547264,"w formats are introduced. Unless otherwise noted, we use the largest available T5 model (11B parameters) as the starting point for training our model and call the system U NIFIED QA. We also report results of training our system with BARTlarge , referred to as U NI FIED QABART (see §6.3). Details on the parameters of the models used are deferred to Appendix A.2. 3 Future references to ‘seed dataset’ point to the QA datasets used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakagu"
2020.findings-emnlp.300,2020.acl-main.499,0,0.0339123,"Missing"
2020.findings-emnlp.300,W19-4828,0,0.127822,"across different tasks. We apply the widely used parameter sharing approach, where a single representation layer is followed by task specific output layers (Baxter, 1997). This reduces the risk of overfitting to a single task and allows decisions on i, j, di , de to influence each other in the hidden layers of the network. We first describe our encoder and then the other layers on top, see Figure 2 for the model architecture. Encoder: To encode x1 . . . xK and question q we use the BERT architecture (Devlin et al., 2018) that has achieved state-of-the-art performance across several NLP tasks (Clark et al., 2019), 2 Note that this does not assume all sentences have the same directionality of influence. For example, a paragraph could include both positive and negative influences: “Predators arrive. Thus the rabbit population falls...”. Rather, the dj = di assumption is one of narrative coherence: the more predators arrive, the more the rabbit population falls. That is, within a paragraph, we assume enhancing one step will have enhanced effects (both positive or negative effects) on future steps - a property of a coherently authored paragraph. where the question q = qp ⊕ qe (⊕ stands for concatenation)."
2020.findings-emnlp.300,N18-1144,1,0.824342,"ral text understanding: Machine reading has seen tremendous progress. With machines reaching human performance in standard QA benchmarks (Devlin et al., 2018; Rajpurkar et al., 2016), more challenging datasets have been proposed (Dua et al., 2019) that require background knowledge, commonsense reasoning (Talmor et al., 2019) and visual reasoning (Antol et al., 2015; 1 All the code will be publicly shared upon acceptance Zellers et al., 2018). In the context of procedural text understanding which has gained considerable amount of attention recently, (Bosselut et al., 2018; Henaff et al., 2017; Dalvi et al., 2018) address the task of tracking entity states throughout the text. Recently, (Tandon et al., 2019) introduced the WIQA task to predict the effect of perturbations. Understanding the effects of perturbations, specifically, qualitative change, has been studied using formal frameworks in the qualitative reasoning community (Forbus, 1984; Weld and De Kleer, 2013) and counterfactual reasoning in the logic community (Lewis, 2013). The WIQA dataset situates this task in terms of natural language rather than formal reasoning, by treating the task as a mixture of reading comprehension and commonsense rea"
2020.findings-emnlp.300,N19-1246,0,0.0446553,"Missing"
2020.findings-emnlp.300,N18-2017,0,0.156846,"mal frameworks in the qualitative reasoning community (Forbus, 1984; Weld and De Kleer, 2013) and counterfactual reasoning in the logic community (Lewis, 2013). The WIQA dataset situates this task in terms of natural language rather than formal reasoning, by treating the task as a mixture of reading comprehension and commonsense reasoning. However, existing models do not explain the effects of perturbations. Explanations: Despite large-scale QA benchmarks, high scores do not necessarily reflect understanding (Min et al., 2019). Current models may not be robust or exploit annotation artifacts (Gururangan et al., 2018). This makes explanations desirable for interpretation (Selvaraju et al., 2017). Attention based explanation has been successfully used in vision tasks such as object detection (Petsiuk et al., 2018) because pixel information is explainable to humans. These and other token level attention models used in NLP tasks (Wiegreffe and Pinter, 2019) do not provide full-sentence explanations of a model’s decisions. Recently, several datasets with natural language explanations have been introduced, e.g., in natural language inference (Camburu et al., 2018), visual question answering (Park et al., 2018),"
2020.findings-emnlp.300,L18-1433,0,0.0627706,"hungry → (MORE/+) eagle swoops down → (MORE/+) eagle catches mouse → (MORE/+) eagle gets more food Table 1: Examples of our model’s predictions on the dev. set in the format: “qp → di xi → dj xj → de qe ”. Supporting sentences xi , xj are compressed e.g., “the person has his ears less protected” → “ears less protected” procedural while the current input is procedural in nature with a specific chronological structure. Another line of work provides more structure and organization to explanations, e.g., using scene graphs in computer vision (Ghosh et al., 2019). For elementary science questions, Jansen et al. (2018) uses a science knowledge graph. These approaches rely on a knowledge structure or graph but knowledge graphs are incomplete and costly to construct for every domain (Weikum and Theobald, 2010). There are trade-offs between unstructured and structured explanations. Unstructured explanations are available abundantly while structured explanations need to be constructed and hence are less scalable (Camburu et al., 2018). Generating free-form (unstructured) explanations is difficult to evaluate (Cui et al., 2018; Zhang et al., 2019), and adding qualitative structure over them is nontrivial. Taking"
2020.findings-emnlp.300,P19-1416,0,0.018969,"ffects of perturbations, specifically, qualitative change, has been studied using formal frameworks in the qualitative reasoning community (Forbus, 1984; Weld and De Kleer, 2013) and counterfactual reasoning in the logic community (Lewis, 2013). The WIQA dataset situates this task in terms of natural language rather than formal reasoning, by treating the task as a mixture of reading comprehension and commonsense reasoning. However, existing models do not explain the effects of perturbations. Explanations: Despite large-scale QA benchmarks, high scores do not necessarily reflect understanding (Min et al., 2019). Current models may not be robust or exploit annotation artifacts (Gururangan et al., 2018). This makes explanations desirable for interpretation (Selvaraju et al., 2017). Attention based explanation has been successfully used in vision tasks such as object detection (Petsiuk et al., 2018) because pixel information is explainable to humans. These and other token level attention models used in NLP tasks (Wiegreffe and Pinter, 2019) do not provide full-sentence explanations of a model’s decisions. Recently, several datasets with natural language explanations have been introduced, e.g., in natur"
2020.findings-emnlp.300,W19-4007,0,0.0509627,"Missing"
2020.findings-emnlp.300,P19-1487,0,0.0436311,"ee-form and knowledge graphs based explanations, we infer a qualitative structure over the sentences in the paragraph. This retains the rich interpretability and simpler evaluation of structured explanations as well as leverages the large-scale availability of sentences required for these explanation. It is an open research problem whether requiring explanation helps or hurts the original task being explained. On the natural language inference task (e-SNLI), Camburu et al. (2018) observed that models generate correct explanations at the expense of good performance. On the Cos-E task, recently Rajani et al. (2019) showed that explanations help the end-task. Our work extends along this line in a new task setting that involves perturbations and enriches natural language explanations with qualitative structure. 3 Problem definition We adopt the problem definition described in Tandon et al. (2019), and summarize it here. Input: 1. Procedural text with steps x1 . . . xK . Here, xk denotes step k (i.e., a sentence) in a procedural text comprising K steps. 2. A perturbation qp to the procedural text and its likely candidate effect qe . Output: An explanation structure that explains the effect of the perturbat"
2020.findings-emnlp.300,N19-1421,0,0.0475798,"Missing"
2020.findings-emnlp.300,D18-1006,1,0.73696,"(Yang and Nyberg, 2015). However, the goal of procedural text understanding in these settings remains a major challenge and requires two key abilities, (i) understanding the dynamics of the world inside a procedure by tracking entities and what events happen as the narrative unfolds. (ii) understanding the dynamics of the world outside the procedure that can influence the procedure. While recent systems for procedural text comprehension have focused on understanding the dynamics of the world inside the process, such as tracking entities and answering questions about what events happen, e.g., (Tandon et al., 2018; Bosselut et al., 2018; Henaff et al., 2017), the extent to which they understand the influences of outside events remains unclear. In particular, if a system fully understands a process, it should be able to predict what would happen if it was perturbed in some way due to an event from the outside world. Such counterfactual reasoning is particularly challenging because, rather than asking what happened (described in text), it asks about what would happen in an alternative world where the change occurred. Recently, Tandon et al. (2019) introduced the WIQA dataset that contains such problems,"
2020.findings-emnlp.300,D19-1629,1,0.733966,"s and answering questions about what events happen, e.g., (Tandon et al., 2018; Bosselut et al., 2018; Henaff et al., 2017), the extent to which they understand the influences of outside events remains unclear. In particular, if a system fully understands a process, it should be able to predict what would happen if it was perturbed in some way due to an event from the outside world. Such counterfactual reasoning is particularly challenging because, rather than asking what happened (described in text), it asks about what would happen in an alternative world where the change occurred. Recently, Tandon et al. (2019) introduced the WIQA dataset that contains such problems, requiring prediction of the effect of perturbations in a procedural text. They also presented several strong models on this task. However, it is unclear whether those high scores indicate that the mod3345 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3345–3355 c November 16 - 20, 2020. 2020 Association for Computational Linguistics els fully understand the described procedures, i.e., that the models have knowledge of the causal chain from perturbation to effect. To test this, Tandon et al. (2019) also prop"
2020.findings-emnlp.300,D19-1002,0,0.0325042,". However, existing models do not explain the effects of perturbations. Explanations: Despite large-scale QA benchmarks, high scores do not necessarily reflect understanding (Min et al., 2019). Current models may not be robust or exploit annotation artifacts (Gururangan et al., 2018). This makes explanations desirable for interpretation (Selvaraju et al., 2017). Attention based explanation has been successfully used in vision tasks such as object detection (Petsiuk et al., 2018) because pixel information is explainable to humans. These and other token level attention models used in NLP tasks (Wiegreffe and Pinter, 2019) do not provide full-sentence explanations of a model’s decisions. Recently, several datasets with natural language explanations have been introduced, e.g., in natural language inference (Camburu et al., 2018), visual question answering (Park et al., 2018), and multihop reading comprehension (HotpotQA dataset) (Yang et al., 2018). In contrast to these datasets, we explain the effects of perturbations in procedural text. HotpotQA contains explanations based on two sentences from a Wikipedia paragraph. Models on the HotpotQA would not be directly applicable to our task and require substantial mo"
2020.findings-emnlp.300,D18-1259,0,0.0616642,"Selvaraju et al., 2017). Attention based explanation has been successfully used in vision tasks such as object detection (Petsiuk et al., 2018) because pixel information is explainable to humans. These and other token level attention models used in NLP tasks (Wiegreffe and Pinter, 2019) do not provide full-sentence explanations of a model’s decisions. Recently, several datasets with natural language explanations have been introduced, e.g., in natural language inference (Camburu et al., 2018), visual question answering (Park et al., 2018), and multihop reading comprehension (HotpotQA dataset) (Yang et al., 2018). In contrast to these datasets, we explain the effects of perturbations in procedural text. HotpotQA contains explanations based on two sentences from a Wikipedia paragraph. Models on the HotpotQA would not be directly applicable to our task and require substantial modification for the following reasons: (i) HotpotQA models are not trained to predict the qualitative structure (more or less of chosen explanation sentences in Figure 1). (ii) HotpotQA involves reasoning over named entities, whereas the current task focuses on common nouns and actions (models that work well on named entities need"
2020.findings-emnlp.300,D16-1264,0,0.139086,"Missing"
2020.findings-emnlp.300,Q18-1015,0,0.0238835,"the effects of perturbations in procedural text. HotpotQA contains explanations based on two sentences from a Wikipedia paragraph. Models on the HotpotQA would not be directly applicable to our task and require substantial modification for the following reasons: (i) HotpotQA models are not trained to predict the qualitative structure (more or less of chosen explanation sentences in Figure 1). (ii) HotpotQA involves reasoning over named entities, whereas the current task focuses on common nouns and actions (models that work well on named entities need to be adapted to common nouns and actions (Sedghi and Sabharwal, 2018)). (iii) explanation paragraphs in HotpotQA are not 3346 ears less protected → (MORE/+) sound enters the ear → (MORE/+) sound hits ear drum → (MORE/+) more sound detected blood clotting disorder → (LESS/-) blood clots → (LESS/-) scab forms → (MORE/+) less scab formation breathing exercise → (MORE/+) air enters lungs → (MORE/+) air enters windpipe → (MORE/+) oxygen enters bloodstream squirrels store food → (MORE/+) squirrels eat more → (MORE/+) squirrels gain weight → (MORE/+) hard survival in winter less trucks run → (LESS/-) trucks go to refineries → (LESS/-) trucks carry oil → (MORE/+) less"
2020.findings-emnlp.300,N18-1023,0,\N,Missing
2020.findings-emnlp.300,P16-1162,0,\N,Missing
2021.emnlp-main.508,P19-1470,0,0.0292449,"rk draws on these ideas, using inference graphs to represent the machine’s “mental model” of the problem at hand. Building the inference graph can be viewed as first asking clarification questions about the context before answering. This is similar to self-talk (Shwartz et al., 2020) but directed towards eliciting chains of influence. Injecting Commonsense Knowledge Many prior systems use commonsense knowledge to aid question-answering, e.g., using sentences retrieved from a corpus (Yang et al., 2019; Guu et al., 2020), or with knowledge generated from a separate source (Shwartz et al., 2020; Bosselut et al., 2019); and injected either as extra sentences fed directly to the model (Clark et al., 2020), via the loss function (Tandon et al., 2018), or via attention (Ma et al., 2019). Unlike prior work, we use conditional language generation techniques to create graphs that are relevant to answering a question. use a graph generated from the query for answering the commonsense question. The graphs consumed by these works contain entities grounded in knowledge graphs like ConceptNet (Speer et al., 2017), whereas we perform reasoning over event inference graphs where each node describes an event. Our best mod"
2021.emnlp-main.508,D15-1075,0,0.0338193,"s, and scenario respectively, and y denotes whether S strengthens/weakens the plausible conclusion that H follows from P, as described in Section 1. 3 Figure 2: An overview of CURIOUS vealed that the graphs can often be erroneous, and CURIOUS also includes an error correction module to generate higher quality inference graphs. This was important because we found that better graphs are more helpful in the downstream QA task. The generated inference graph is then used for the QA task on three existing defeasible inference datasets from diverse domains, viz., δ-SNLI (natural language inference) (Bowman et al., 2015), δ-SOCIAL (reasoning about social norms) (Forbes et al., 2020), and δ-ATOMIC (commonsense reasoning) (Sap et al., 2019). We show that the way the graph is encoded for input is important. If we simply augment the question with the generated graphs, there are some gains on all datasets. However, the accuracy improves substantially across all datasets with a more judicious encoding of the graph-augmented question that accounts for interactions between the graph nodes. To achieve this, we use the mixture of experts approach (Jacobs et al., 1991) to include a mixture of experts layers during encod"
2021.emnlp-main.508,P19-1299,0,0.101858,"on. We encode the defeasible query x and the nodes of the graph using L. Query representation is computed as hx = L(x), and we similarly obtain a matrix of node representations hV by encoding each node v in G with L as follows: 3.4.2 Graph representations using MoE Recently, mixture-of-experts (Jacobs et al., 1991; Shazeer et al., 2017; Fedus et al., 2021) has emerged as a promising method of combining multiple feature types. Mixture-of-experts (MoE) is especially useful when the input consists of multiple facets, where each facet has a specific semantic meaning. Previously, Gu et al. (2018); Chen et al. (2019) have used the ability of MoE to pool disparate features on low-resource and cross-lingual language tasks. Since each node in the inference graphs used by us plays a specific role in defeasible reasoning (contextualizer, situation node, or mediator), we take inspiration from these works to design a hierarchical MoE model (Jordan and Xu, 1995) to pool node representations hV into a graph representation hG . An MoE consists of n expert networks E1 , E2 , . . . , En and a gating network M. Given an input x ∈ Rd , each expert network Ei : Rd → Rk learns a transform over the input. The gating netwo"
2021.emnlp-main.508,P18-1128,0,0.0123958,"we report the published numbers for this baseline. For an additional baseline, we directly use the initial inference graph G0 generated by GENinit , and provide it to the model simply as a string (i.e., sequence of tokens; a simple, often-used approach). This baseline is called E 2 E-STR. We use the same hyperparameters as Rudinger et al. (2020), and add a detailed description of the hyperparameters in Appendix §C. For all the QA experiments, we report the accuracy on the test set using the checkpoint with the highest accuracy on the development set. We use the McNemar’s test (McNemar, 1947; Dror et al., 2018) and use p &lt; 0.05 as a threshold for statistical significance. All the p-values are reported in Appendix §G. Experiments In this section, we empirically investigate if CURI OUS can improve defeasible inference by first modeling the question scenario using inference graphs. We also study the reasons for any improvements. 4.1 Datasets Our end task performance is measured on the three existing datasets for defeasible inference provided by Rudinger et al. (2020):3 δATOMIC , δ- SNLI , δ- SOCIAL (Table 1). These datasets exhibit substantial diversity because of their domains: δ-SNLI (natural languag"
2021.emnlp-main.508,2020.emnlp-main.48,0,0.0268856,"ns/weakens the plausible conclusion that H follows from P, as described in Section 1. 3 Figure 2: An overview of CURIOUS vealed that the graphs can often be erroneous, and CURIOUS also includes an error correction module to generate higher quality inference graphs. This was important because we found that better graphs are more helpful in the downstream QA task. The generated inference graph is then used for the QA task on three existing defeasible inference datasets from diverse domains, viz., δ-SNLI (natural language inference) (Bowman et al., 2015), δ-SOCIAL (reasoning about social norms) (Forbes et al., 2020), and δ-ATOMIC (commonsense reasoning) (Sap et al., 2019). We show that the way the graph is encoded for input is important. If we simply augment the question with the generated graphs, there are some gains on all datasets. However, the accuracy improves substantially across all datasets with a more judicious encoding of the graph-augmented question that accounts for interactions between the graph nodes. To achieve this, we use the mixture of experts approach (Jacobs et al., 1991) to include a mixture of experts layers during encoding, enabling the ability to attend to specific nodes while cap"
2021.emnlp-main.508,N18-1032,0,0.0935709,"uence representation. We encode the defeasible query x and the nodes of the graph using L. Query representation is computed as hx = L(x), and we similarly obtain a matrix of node representations hV by encoding each node v in G with L as follows: 3.4.2 Graph representations using MoE Recently, mixture-of-experts (Jacobs et al., 1991; Shazeer et al., 2017; Fedus et al., 2021) has emerged as a promising method of combining multiple feature types. Mixture-of-experts (MoE) is especially useful when the input consists of multiple facets, where each facet has a specific semantic meaning. Previously, Gu et al. (2018); Chen et al. (2019) have used the ability of MoE to pool disparate features on low-resource and cross-lingual language tasks. Since each node in the inference graphs used by us plays a specific role in defeasible reasoning (contextualizer, situation node, or mediator), we take inspiration from these works to design a hierarchical MoE model (Jordan and Xu, 1995) to pool node representations hV into a graph representation hG . An MoE consists of n expert networks E1 , E2 , . . . , En and a gating network M. Given an input x ∈ Rd , each expert network Ei : Rd → Rk learns a transform over the inp"
2021.emnlp-main.508,2020.acl-main.386,0,0.0237982,"Missing"
2021.emnlp-main.508,D19-1282,0,0.052542,"Missing"
2021.emnlp-main.508,2021.ccl-1.108,0,0.0392943,"Missing"
2021.emnlp-main.508,D19-6003,0,0.0152107,"larification questions about the context before answering. This is similar to self-talk (Shwartz et al., 2020) but directed towards eliciting chains of influence. Injecting Commonsense Knowledge Many prior systems use commonsense knowledge to aid question-answering, e.g., using sentences retrieved from a corpus (Yang et al., 2019; Guu et al., 2020), or with knowledge generated from a separate source (Shwartz et al., 2020; Bosselut et al., 2019); and injected either as extra sentences fed directly to the model (Clark et al., 2020), via the loss function (Tandon et al., 2018), or via attention (Ma et al., 2019). Unlike prior work, we use conditional language generation techniques to create graphs that are relevant to answering a question. use a graph generated from the query for answering the commonsense question. The graphs consumed by these works contain entities grounded in knowledge graphs like ConceptNet (Speer et al., 2017), whereas we perform reasoning over event inference graphs where each node describes an event. Our best model uses a mixture-of-experts (MoE) (Jacobs et al., 1991) model to pool multifaceted input. Prior work has shown the effectiveness of using MoE for graph classification"
2021.emnlp-main.508,2021.findings-acl.456,1,0.384764,"h inference graphs help machines in defeasible reasoning? Our approach is as follows. First, given a question, generate an inference graph describing important influences between question elements. Then, use that graph as an additional input when answering the defeasible reasoning query. Our proposed system, CURIOUS, comprises a graph generation module and a graph encoding module to use the generated graph for the query (Figure 2). To generate inference graphs, we build upon past 1 work that uses a sequence to sequence approach Data and code located at https://github.com/ madaan/thinkaboutit (Madaan et al., 2021). However, our analysis re6291 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6291–6310 c November 7–11, 2021. 2021 Association for Computational Linguistics sentences describing a premise, hypothesis, and scenario respectively, and y denotes whether S strengthens/weakens the plausible conclusion that H follows from P, as described in Section 1. 3 Figure 2: An overview of CURIOUS vealed that the graphs can often be erroneous, and CURIOUS also includes an error correction module to generate higher quality inference graphs. This was important becaus"
2021.emnlp-main.508,2021.naacl-main.67,1,0.722496,"017). Here we consider the specific formulation and challenge in Rudinger et al. (2020): Given that some premise P plausibly implies a hypothesis H, does new information that the situation is S weaken or strengthen the conclusion H? For example, consider the premise “The drinking glass fell” with a possible implication “The glass broke”. New information that “The glass fell on a pillow” here weakens the implication. We borrow ideas from the cognitive science literature that supports defeasible reasoning for humans with an inference graph (Pollock, 2009, 1987). Inference graphs formulation in (Madaan and Yang, 2021), which we use in this paper, draws connections between the P, H, and S through mediating Figure 1: CURIOUS improves defeasible reasoning by modeling the question scenario with an inference graph G adapted from cognitive science literature. The graph is encoded judiciously using our graph encoder h(.), resulting in end task performance improvement. events. This can be seen as a mental model of the question scenario before answering the question (Johnson-Laird, 1983). This paper asks the natural question: can modeling the question scenario with inference graphs help machines in defeasible reaso"
2021.emnlp-main.508,2020.findings-emnlp.418,0,0.0538706,"Missing"
2021.emnlp-main.508,2020.emnlp-main.373,0,0.0431211,"Missing"
2021.emnlp-main.508,D18-1455,0,0.0622376,"Missing"
2021.emnlp-main.508,N19-4013,0,0.0467657,"Missing"
2021.emnlp-main.508,D18-1006,1,0.735186,"ce graph can be viewed as first asking clarification questions about the context before answering. This is similar to self-talk (Shwartz et al., 2020) but directed towards eliciting chains of influence. Injecting Commonsense Knowledge Many prior systems use commonsense knowledge to aid question-answering, e.g., using sentences retrieved from a corpus (Yang et al., 2019; Guu et al., 2020), or with knowledge generated from a separate source (Shwartz et al., 2020; Bosselut et al., 2019); and injected either as extra sentences fed directly to the model (Clark et al., 2020), via the loss function (Tandon et al., 2018), or via attention (Ma et al., 2019). Unlike prior work, we use conditional language generation techniques to create graphs that are relevant to answering a question. use a graph generated from the query for answering the commonsense question. The graphs consumed by these works contain entities grounded in knowledge graphs like ConceptNet (Speer et al., 2017), whereas we perform reasoning over event inference graphs where each node describes an event. Our best model uses a mixture-of-experts (MoE) (Jacobs et al., 1991) model to pool multifaceted input. Prior work has shown the effectiveness of"
2021.emnlp-main.508,D19-1629,1,0.832164,"ng a system to “think about&quot; a question before answering can improve performance. Approach Inspired by past results (Madaan et al., 2021) that humans found inference graphs useful for defeasible inference, we investigate whether neural models can benefit from envisioning the question scenario using an inference graph before answering a defeasible inference query. Inference graphs As inference graphs are central to our work, we give a brief description of their structure next. Inference graphs were introduced in philosophy by Pollock (2009) to aid defeasible reasoning for humans, and in NLP by Tandon et al. (2019) for a counterfactual reasoning task. We interpret the inference graphs as having four kinds of nodes (Pollock, 2009; Madaan et al., 2021): i. Contextualizers (C-, C+): these nodes set the context around a situation and connect to the P. ii. Situations (S, S-): these nodes are new situations that emerge which might overturn an inference. iii. Hypothesis (H-, H+): Hypothesis nodes describe the outcome/conclusion of the situation. iv. Mediators (M-, M+): Mediators are nodes that help bridge the knowledge gap between a situation and a hypothesis node by explaining their connection explicitly. The"
2021.emnlp-main.582,N19-1245,0,0.022538,"rojects have probed the limits of transformers to solve pure math problems (Saxton et al., 2019; Lample and Charton, 2019; Hendrycks et al., 2021). FPs differ from these problems in two important ways. First, due to the heuristic nature of their solutions, FPs do not have a unique, precise answer with formal proof, in the way that normal mathematical problems do. Second, FPs are stated in natural language (NL) rather than a formal, mathematical notation. FPs are perhaps closer to algebra word problems, where a NL question, e.g., “How many cookies were left?”, is asked about a simple NL story (Amini et al., 2019; Ling et al., 2017; Koncel-Kedziorski et al., 2015). However, in algebra word problems, answers are again uniquely defined and provable. In addition, all required information is provided in the story, while in FPs the solver must find/recall required information.5 Finally, in story problems, the space of possible solution equations is typically small and well-defined enough that it can be exhaustively searched, while FPs can have arbitrarily complex solutions (e.g., Figure 1). Question Decomposition FPs require problem decomposition, in a way loosely similar to multihop inference. However, fo"
2021.emnlp-main.582,D13-1160,0,0.0445409,"d information is provided in the story, while in FPs the solver must find/recall required information.5 Finally, in story problems, the space of possible solution equations is typically small and well-defined enough that it can be exhaustively searched, while FPs can have arbitrarily complex solutions (e.g., Figure 1). Question Decomposition FPs require problem decomposition, in a way loosely similar to multihop inference. However, for FPs, the appropriate decomposition is not explicit in the question itself, unlike early multihop datasets such as HotpotQA (Yang et al., 2018) or WebQuestions (Berant et al., 2013). Later multihop datasets, e.g., OBQA (Mihaylov et al., 2018), contained questions where the decomposition was not explicit in the question (e.g., “Does a suit of armor conduct electricity?”, implicitly requiring a subquestion about materials), but typically into just two (or at most three) steps. In contrast, FPs typically require multiple levels of decomposition, significantly increasing complexity. This in turn requires identifying a solution strategy, namely how to factor an unknown quantity into a function of known (or recursively factorable) quantities. The StrategyQA (Geva et al., 2021)"
2021.emnlp-main.582,2020.emnlp-main.385,0,0.0270922,"Missing"
2021.emnlp-main.582,P19-1470,0,0.0214358,"ble, and without recursive decomposition, a key feature of FPs. Commonsense In addition to mathematical reasoning, FPs require significant commonsense knowledge, both for estimating quantities and for decomposing problems. For example, “How many pizza delivery trucks are in Chicago?” requires significant commonsense about human behavior (How often do people order pizza? How many deliveries can a truck make per day?) to even begin to decompose the problem, let alone estimating basic quantities (Population of Chicago?). While new resources of commonsense knowledge are becoming available, e.g., (Bosselut et al., 2019; Sap et al., 2019), substantial development is still needed for the kind of world modeling that many FPs require. Numeric Estimation Large-scale language models trained on web-scale data have been shown to contain common numerical facts – e.g. number of days in a year, distance from earth to moon, number of hairs on a human head, etc. We leverage one such model (T5 (Raffel et al., 2020)) for our baselines. More recently, researchers have shown that models can also perform estimation to some degree (Zhang et al., 2020), and have proposed novel encoding strategies to improve number prediction a"
2021.emnlp-main.582,D19-1410,0,0.0210447,").10 Task 2, distractor-context: Q, {F ∪ Fd } → A |P . This setting extends Task 2 by adding Fd , a set of distractor facts to the input, bringing the total number of facts to 20. This requires the model to also identify which facts are actually useful for the solution. F ∪ Fd here is akin to the “context” in the typical Reading Comprehension setting studied in the QA literature. It should be noted that the set of distractor facts Fd are chosen from facts corresponding to similar questions in the dataset;11 similarity is defined using the question embedding as given by a sentence transformer (Reimers and Gurevych, 2019). Task 3, full: Q → A |P . When the input is only the question, we are in the original Fermi problem setting. Again, we define two subtasks (a) generate an answer A directly (b) synthesize a program P which is then used to compute its implied answer PAns. Note that when the explanation program P needs to be outputted, the model is not presented with any facts F unlike the previous tasks. Therefore, the model has the freedom to avail information from any other source – e.g. a knowledge base or via information already part of its parameters. Given the unconstrained nature of this task, there are"
2021.emnlp-main.582,N19-1423,0,0.00989196,"rming a logarithmic sweep between 10−10 − 1010 , we find that the constant prediction of 1000 (for every FP) achieves an average score of 0.22, indicating that this prediction is, on average, Results. Based on predicted answer (A), two to three orders of magnitude off. we find that the T5 model finetuned only on Regression. This baseline uses a 3-layer MLP, R EAL FP performs slightly better than other which regresses to a number, given an encoding of variants. However, the best score is achieved the question (obtained using a pre-trained BERT when the explanation program is evaluated – model (Devlin et al., 2019)). We train this model in 12 from the Huggingface library. three settings: (1) on S YNTH FP, (2) on R EAL FP, 7325 Q: Imagine the earth is at one end of the school oval and the moon is at the other end. How far away is the sun? Predicted P : Mul(Distance of earth-sun? 2e+11 km, Distance of earth-moon? 2e+11 km) Target P : Mul(Length of school oval? 0.1 km, Div(Distance from earth-sun? 151e+6 km , Distance from earth-moon? 384400 km)) Scores: Valid?: Yes, Ans: 0 Facts: 0.67 Q: How many punctuation marks are in a book? Predicted P : Div(Number of sentences in a book? 5000, Avg. number of punctua"
2021.emnlp-main.582,P18-1196,0,0.0136383,"019), substantial development is still needed for the kind of world modeling that many FPs require. Numeric Estimation Large-scale language models trained on web-scale data have been shown to contain common numerical facts – e.g. number of days in a year, distance from earth to moon, number of hairs on a human head, etc. We leverage one such model (T5 (Raffel et al., 2020)) for our baselines. More recently, researchers have shown that models can also perform estimation to some degree (Zhang et al., 2020), and have proposed novel encoding strategies to improve number prediction and estimation (Spithourakis and Riedel, 2018; BergKirkpatrick and Spokoyny, 2020). Such techniques would be valuable for improved solutions to FPs. program → statement* statement → comp-expr |support-expr comp-expr → qn-id &quot;-&gt;&quot; {math-expr |value-expr} math-expr → operator &quot;(&quot; qn-id* &quot;)&quot; operator → &quot;Add&quot; |&quot;Sub&quot; |&quot;Mul&quot; |&quot;Div&quot; value-expr → val-id &quot;because&quot; fact-id support-expr → question-expr |fact-expr |val-expr question-expr → qn-id &quot;: &quot; question fact-expr† → fact-id &quot;: &quot; sentence val-expr → val-id &quot;: &quot; number [units] Figure 2: Grammar for FP explanation programs. † The proposed FP tasks (proposed in section 4.2.3) separate out fact-expr"
2021.emnlp-main.582,D18-1259,0,0.0293915,"d provable. In addition, all required information is provided in the story, while in FPs the solver must find/recall required information.5 Finally, in story problems, the space of possible solution equations is typically small and well-defined enough that it can be exhaustively searched, while FPs can have arbitrarily complex solutions (e.g., Figure 1). Question Decomposition FPs require problem decomposition, in a way loosely similar to multihop inference. However, for FPs, the appropriate decomposition is not explicit in the question itself, unlike early multihop datasets such as HotpotQA (Yang et al., 2018) or WebQuestions (Berant et al., 2013). Later multihop datasets, e.g., OBQA (Mihaylov et al., 2018), contained questions where the decomposition was not explicit in the question (e.g., “Does a suit of armor conduct electricity?”, implicitly requiring a subquestion about materials), but typically into just two (or at most three) steps. In contrast, FPs typically require multiple levels of decomposition, significantly increasing complexity. This in turn requires identifying a solution strategy, namely how to factor an unknown quantity into a function of known (or recursively factorable) quantiti"
2021.emnlp-main.582,2020.findings-emnlp.439,0,0.0176589,"ile new resources of commonsense knowledge are becoming available, e.g., (Bosselut et al., 2019; Sap et al., 2019), substantial development is still needed for the kind of world modeling that many FPs require. Numeric Estimation Large-scale language models trained on web-scale data have been shown to contain common numerical facts – e.g. number of days in a year, distance from earth to moon, number of hairs on a human head, etc. We leverage one such model (T5 (Raffel et al., 2020)) for our baselines. More recently, researchers have shown that models can also perform estimation to some degree (Zhang et al., 2020), and have proposed novel encoding strategies to improve number prediction and estimation (Spithourakis and Riedel, 2018; BergKirkpatrick and Spokoyny, 2020). Such techniques would be valuable for improved solutions to FPs. program → statement* statement → comp-expr |support-expr comp-expr → qn-id &quot;-&gt;&quot; {math-expr |value-expr} math-expr → operator &quot;(&quot; qn-id* &quot;)&quot; operator → &quot;Add&quot; |&quot;Sub&quot; |&quot;Mul&quot; |&quot;Div&quot; value-expr → val-id &quot;because&quot; fact-id support-expr → question-expr |fact-expr |val-expr question-expr → qn-id &quot;: &quot; question fact-expr† → fact-id &quot;: &quot; sentence val-expr → val-id &quot;: &quot; number [units] F"
2021.emnlp-main.582,Q15-1042,1,0.811081,"formers to solve pure math problems (Saxton et al., 2019; Lample and Charton, 2019; Hendrycks et al., 2021). FPs differ from these problems in two important ways. First, due to the heuristic nature of their solutions, FPs do not have a unique, precise answer with formal proof, in the way that normal mathematical problems do. Second, FPs are stated in natural language (NL) rather than a formal, mathematical notation. FPs are perhaps closer to algebra word problems, where a NL question, e.g., “How many cookies were left?”, is asked about a simple NL story (Amini et al., 2019; Ling et al., 2017; Koncel-Kedziorski et al., 2015). However, in algebra word problems, answers are again uniquely defined and provable. In addition, all required information is provided in the story, while in FPs the solver must find/recall required information.5 Finally, in story problems, the space of possible solution equations is typically small and well-defined enough that it can be exhaustively searched, while FPs can have arbitrarily complex solutions (e.g., Figure 1). Question Decomposition FPs require problem decomposition, in a way loosely similar to multihop inference. However, for FPs, the appropriate decomposition is not explicit"
2021.emnlp-main.582,P17-1015,0,0.0217954,"the limits of transformers to solve pure math problems (Saxton et al., 2019; Lample and Charton, 2019; Hendrycks et al., 2021). FPs differ from these problems in two important ways. First, due to the heuristic nature of their solutions, FPs do not have a unique, precise answer with formal proof, in the way that normal mathematical problems do. Second, FPs are stated in natural language (NL) rather than a formal, mathematical notation. FPs are perhaps closer to algebra word problems, where a NL question, e.g., “How many cookies were left?”, is asked about a simple NL story (Amini et al., 2019; Ling et al., 2017; Koncel-Kedziorski et al., 2015). However, in algebra word problems, answers are again uniquely defined and provable. In addition, all required information is provided in the story, while in FPs the solver must find/recall required information.5 Finally, in story problems, the space of possible solution equations is typically small and well-defined enough that it can be exhaustively searched, while FPs can have arbitrarily complex solutions (e.g., Figure 1). Question Decomposition FPs require problem decomposition, in a way loosely similar to multihop inference. However, for FPs, the appropri"
2021.emnlp-main.582,D18-1260,1,0.832239,"lver must find/recall required information.5 Finally, in story problems, the space of possible solution equations is typically small and well-defined enough that it can be exhaustively searched, while FPs can have arbitrarily complex solutions (e.g., Figure 1). Question Decomposition FPs require problem decomposition, in a way loosely similar to multihop inference. However, for FPs, the appropriate decomposition is not explicit in the question itself, unlike early multihop datasets such as HotpotQA (Yang et al., 2018) or WebQuestions (Berant et al., 2013). Later multihop datasets, e.g., OBQA (Mihaylov et al., 2018), contained questions where the decomposition was not explicit in the question (e.g., “Does a suit of armor conduct electricity?”, implicitly requiring a subquestion about materials), but typically into just two (or at most three) steps. In contrast, FPs typically require multiple levels of decomposition, significantly increasing complexity. This in turn requires identifying a solution strategy, namely how to factor an unknown quantity into a function of known (or recursively factorable) quantities. The StrategyQA (Geva et al., 2021) dataset illustrates this problem but for a different 5 We la"
2021.emnlp-main.585,N19-1423,0,0.0419032,"Missing"
2021.emnlp-main.585,2021.ccl-1.108,0,0.0355089,"Missing"
2021.emnlp-main.585,D19-1244,0,0.0485396,"Missing"
2021.emnlp-main.585,P19-1487,0,0.0164784,"tences (the leaves of the gold entailment tree), (b) all relevant and some distractor sentences, or (c) a full corpus. 2 Related Work In the context of QA, there are multiple notions of explanation/justification, including showing an Our focus here is on generating the derivation authoritative, answer-bearing sentence (Perez et al., (line of reasoning) showing how the evidence leads 2019), an attention map over a passage (Seo et al., to the answer, rather than the pragmatics of decid2016), a synthesized phrase connecting question ing which parts of that to then show the user. This and answer (Rajani et al., 2019), or the syntactic allows us to separate two (typically confounded) pattern used to locate the answer (Ye et al., 2020; explanation requirements, namely correctness (of Hancock et al., 2018). These methods are primarily the derivation) from utility, allowing us to evaluate designed for answers to “lookup” questions, to exderivations with a more objective measure (correctplain where/how an answer was found in a corpus. ness). This also sets the stage for future work on the For questions requiring inference, the focus of pragmatics of what to show users (Miller, 2019). this paper, an explanation"
2021.emnlp-main.585,2020.emnlp-main.9,0,0.0972,"Missing"
2021.emnlp-main.585,2020.acl-main.704,0,0.0259801,"Missing"
2021.emnlp-main.585,P18-1175,0,0.0620509,"Missing"
2021.emnlp-main.697,2020.tacl-1.3,0,0.027822,"988), and uncertainty (Pearl, 1986). Similarly, work in cognitive science has promoted mental models – coherent, constructed representations of the way the world is believed to be – as central to understanding and communication (Johnson-Laird, 1983; Gentner and Stevens, 1983; Hilton, 1996). We draw on these ideas, proposing how they can be layered on top of PTLMs, here representing beliefs as NL statements rather than formal structures. Although PTLMs contain extensive world knowledge (Petroni et al., 2019; Roberts et al., 2020), they can be inconsistent in their answers to probing questions (Ettinger, 2020; Davison et al., 2019; Ravichander et al., 2020; Elazar et al., 2021; Subramanian et al., 2020), making their “world model” unclear. Although various approaches have improved answer consistency, mainly through modified model training, e.g., (Ribeiro et al., 2019; Minervini and Riedel, 2018; Li et al., 2019; Asai and Hajishirzi, 2020), they have not solved the problem. Current PTLMs still behave as a source of noisy knowledge, rather than projecting a coherent picture of the world (Kassner and Schütze, 2020). A close analogy to our task is in knowledge graph (KG) construction. Pujara et al. (2"
2021.emnlp-main.697,K18-1007,0,0.0878341,"d with an original PTLM. In contrast to prior work, this system does not rely on fine-tuning the PTLM. Fine-tuning requires expensive training data, and risks destabilizing the model’s performance on other tasks outside the scope of training. Instead, our system functions without training data, explicitly reasoning about beliefs using an external mechanism, thus allowing both controllability and interpretability. Most significantly, we find that improving consistency in this way improves accuracy, while earlier finetuning-based approaches report either no accuracy gains (Ribeiro et al., 2019; Minervini and Riedel, 2018; Li et al., 2019) or only slight gains (Asai and Hajishirzi, 2020). Specifically, we show that two mechanisms – constraint reasoning and feedback – improve the overall system’s accuracy and consistency over time. 2. We contribute a targeted dataset to measure a system’s consistency against given constraints. 3. We provide an analysis of the failure modes and directions for future work. This work is significant as it is a first step towards PTLM-based architectures that have a systematic notion of belief, allowing them to construct a more coherent picture of the world, and improve over time wi"
2021.emnlp-main.697,D19-1250,0,0.153537,"w is not a fish”) as the query context. We find that both consistency and accuracy of the overall system improve. Example: The model M shown in the figure incorrectly answers “yes”, when asked “a swallow has gills?”. But (as shown above) if reminded of its previous answer “a swallow is not a fish”, M correctly answers ""no"". conditional on its other knowledge and reasoning abilities. Maintaining a consistent set of beliefs (a “belief system”) is a key facet of intelligence, as it can help debug errors and encourage rational behavior. However, although PTLMs contain substantial world knowledge (Petroni et al., 2019), their 1 Introduction answers to probing questions can be inconsistent (Elazar et al., 2021; Kassner and Schütze, 2020), Intelligent agents are typically considered to have even after specialized training to reduce inconsisbeliefs about the world – propositions that they take as true (Genin and Huber, 2021). In general, a sys- tency (Ribeiro et al., 2019; Li et al., 2019). As a result, it is sometimes hard to pin down what a tem can be said to (appear to) believe a proposition PTLM actually “believes”, making them susceptip, e.g., “eagles are birds”, if it produces answers consistent with p ("
2021.emnlp-main.697,2020.starsem-1.10,0,0.0233858,"imilarly, work in cognitive science has promoted mental models – coherent, constructed representations of the way the world is believed to be – as central to understanding and communication (Johnson-Laird, 1983; Gentner and Stevens, 1983; Hilton, 1996). We draw on these ideas, proposing how they can be layered on top of PTLMs, here representing beliefs as NL statements rather than formal structures. Although PTLMs contain extensive world knowledge (Petroni et al., 2019; Roberts et al., 2020), they can be inconsistent in their answers to probing questions (Ettinger, 2020; Davison et al., 2019; Ravichander et al., 2020; Elazar et al., 2021; Subramanian et al., 2020), making their “world model” unclear. Although various approaches have improved answer consistency, mainly through modified model training, e.g., (Ribeiro et al., 2019; Minervini and Riedel, 2018; Li et al., 2019; Asai and Hajishirzi, 2020), they have not solved the problem. Current PTLMs still behave as a source of noisy knowledge, rather than projecting a coherent picture of the world (Kassner and Schütze, 2020). A close analogy to our task is in knowledge graph (KG) construction. Pujara et al. (2013) We make the following contributions: 1. We"
2021.emnlp-main.697,P19-1621,0,0.355107,"ge and reasoning abilities. Maintaining a consistent set of beliefs (a “belief system”) is a key facet of intelligence, as it can help debug errors and encourage rational behavior. However, although PTLMs contain substantial world knowledge (Petroni et al., 2019), their 1 Introduction answers to probing questions can be inconsistent (Elazar et al., 2021; Kassner and Schütze, 2020), Intelligent agents are typically considered to have even after specialized training to reduce inconsisbeliefs about the world – propositions that they take as true (Genin and Huber, 2021). In general, a sys- tency (Ribeiro et al., 2019; Li et al., 2019). As a result, it is sometimes hard to pin down what a tem can be said to (appear to) believe a proposition PTLM actually “believes”, making them susceptip, e.g., “eagles are birds”, if it produces answers consistent with p (and its other beliefs). Pragmati- ble to inconsistent and/or irrational behavior. Our goal is a first step to avoid these problems by emcally, we expect the system to (a) give a consistent bedding a PTLM in a broader system with a clearer answer to different paraphrases of the question ""p?"" (""Are eagles birds?"", ""Is an eagle a type of bird?"", notion of be"
2021.emnlp-main.697,2020.emnlp-main.437,0,0.0223052,"nd Nilsson, 1987; Moore, 1983), belief revision (De Kleer, 1986; Dechter and Dechter, 1988), and uncertainty (Pearl, 1986). Similarly, work in cognitive science has promoted mental models – coherent, constructed representations of the way the world is believed to be – as central to understanding and communication (Johnson-Laird, 1983; Gentner and Stevens, 1983; Hilton, 1996). We draw on these ideas, proposing how they can be layered on top of PTLMs, here representing beliefs as NL statements rather than formal structures. Although PTLMs contain extensive world knowledge (Petroni et al., 2019; Roberts et al., 2020), they can be inconsistent in their answers to probing questions (Ettinger, 2020; Davison et al., 2019; Ravichander et al., 2020; Elazar et al., 2021; Subramanian et al., 2020), making their “world model” unclear. Although various approaches have improved answer consistency, mainly through modified model training, e.g., (Ribeiro et al., 2019; Minervini and Riedel, 2018; Li et al., 2019; Asai and Hajishirzi, 2020), they have not solved the problem. Current PTLMs still behave as a source of noisy knowledge, rather than projecting a coherent picture of the world (Kassner and Schütze, 2020). A clo"
2021.emnlp-main.697,2020.acl-main.698,1,0.935731,"xample: The model M shown in the figure incorrectly answers “yes”, when asked “a swallow has gills?”. But (as shown above) if reminded of its previous answer “a swallow is not a fish”, M correctly answers ""no"". conditional on its other knowledge and reasoning abilities. Maintaining a consistent set of beliefs (a “belief system”) is a key facet of intelligence, as it can help debug errors and encourage rational behavior. However, although PTLMs contain substantial world knowledge (Petroni et al., 2019), their 1 Introduction answers to probing questions can be inconsistent (Elazar et al., 2021; Kassner and Schütze, 2020), Intelligent agents are typically considered to have even after specialized training to reduce inconsisbeliefs about the world – propositions that they take as true (Genin and Huber, 2021). In general, a sys- tency (Ribeiro et al., 2019; Li et al., 2019). As a result, it is sometimes hard to pin down what a tem can be said to (appear to) believe a proposition PTLM actually “believes”, making them susceptip, e.g., “eagles are birds”, if it produces answers consistent with p (and its other beliefs). Pragmati- ble to inconsistent and/or irrational behavior. Our goal is a first step to avoid thes"
2021.emnlp-main.697,D19-1405,0,0.371899,"ties. Maintaining a consistent set of beliefs (a “belief system”) is a key facet of intelligence, as it can help debug errors and encourage rational behavior. However, although PTLMs contain substantial world knowledge (Petroni et al., 2019), their 1 Introduction answers to probing questions can be inconsistent (Elazar et al., 2021; Kassner and Schütze, 2020), Intelligent agents are typically considered to have even after specialized training to reduce inconsisbeliefs about the world – propositions that they take as true (Genin and Huber, 2021). In general, a sys- tency (Ribeiro et al., 2019; Li et al., 2019). As a result, it is sometimes hard to pin down what a tem can be said to (appear to) believe a proposition PTLM actually “believes”, making them susceptip, e.g., “eagles are birds”, if it produces answers consistent with p (and its other beliefs). Pragmati- ble to inconsistent and/or irrational behavior. Our goal is a first step to avoid these problems by emcally, we expect the system to (a) give a consistent bedding a PTLM in a broader system with a clearer answer to different paraphrases of the question ""p?"" (""Are eagles birds?"", ""Is an eagle a type of bird?"", notion of belief (see Figure 1"
2021.emnlp-main.697,2020.emnlp-main.373,0,0.0543443,"Missing"
2021.emnlp-main.697,2020.acl-main.495,0,0.0267304,"d mental models – coherent, constructed representations of the way the world is believed to be – as central to understanding and communication (Johnson-Laird, 1983; Gentner and Stevens, 1983; Hilton, 1996). We draw on these ideas, proposing how they can be layered on top of PTLMs, here representing beliefs as NL statements rather than formal structures. Although PTLMs contain extensive world knowledge (Petroni et al., 2019; Roberts et al., 2020), they can be inconsistent in their answers to probing questions (Ettinger, 2020; Davison et al., 2019; Ravichander et al., 2020; Elazar et al., 2021; Subramanian et al., 2020), making their “world model” unclear. Although various approaches have improved answer consistency, mainly through modified model training, e.g., (Ribeiro et al., 2019; Minervini and Riedel, 2018; Li et al., 2019; Asai and Hajishirzi, 2020), they have not solved the problem. Current PTLMs still behave as a source of noisy knowledge, rather than projecting a coherent picture of the world (Kassner and Schütze, 2020). A close analogy to our task is in knowledge graph (KG) construction. Pujara et al. (2013) We make the following contributions: 1. We show that a PTLM-based system can be given a con"
2021.findings-acl.317,D19-1250,0,0.0746348,"Missing"
2021.findings-emnlp.184,2020.nuse-1.7,0,0.0553705,"Missing"
2021.findings-emnlp.184,W17-0905,0,0.0757418,"dataset, and models offer a new research direction for learning script knowledge. Scenario: bake a cake find the cake recipe gather the ingredients turn on the oven mix the ingredients put the cake batter in the oven bake for the right amount of time take the cake out of the oven Figure 1: We collected 6.4k of partially ordered scripts (proScript) and developed models that take a scenario (e.g., bake a cake) as the input and generate a (possibly partial-order) script. In proScript, an event (node) requires that all the precedent events and paths are happened/executed in advance. et al., 2010; Chambers, 2017; Ostermann, 2020). In this work, we show for the first time that pre-trained neural language models (LMs) can be adapted to generate high-quality scripts, including appropriately partial ordering events where a specific temporal ordering is required only when it is necessary. LMs have previously been shown to successfully generate stories (Rashkin et al., 2020), summaries (Lewis et al., 2020), and commonsense facts (Bosselut et al., 2019; Hwang et al., 2020). 1 Introduction Here we investigate their application to script genScripts (Schank and Abelson, 1975) represent struc- eration. First, w"
2021.findings-emnlp.184,P08-1090,0,0.371077,"ipt generation, the best model obtains a graph edit distance of 4.97 (i.e., number of human edits), while human-created scripts achieve 2.98 on average. Our contributions are thus: • A new dataset (proScript) of crowdsourced scripts that is substantially larger than prior (manually crafted) datasets • Two complementary task definitions against proScript • Two new models for these task, providing the first demonstration that generative models can be successfully applied, although it is still significantly below human levels. 2 Related Work Script as narrative chain Mooney and DeJong (1985) and Chambers and Jurafsky (2008, inter alia) have investigated automatically inducing scripts from (unstructured) corpus. In particular, Chambers and Jurafsky (2008) introduced scripts as narrative chain, where verbs with the participants information (e.g., (claimed, subj), and (accused, obj) ) named narrative events are partially ordered according to causal and temporal relations. They also introduced narrative cloze task, where a model is expected to predict one removed narrative event, given all the other narrative events, while our proposed task requires to generate scripts as a partial-order graph for a given scenario."
2021.findings-emnlp.184,2021.naacl-demos.2,0,0.0320653,"3 We focus on events and the partial-ordering for the protagonist (Chambers and Jurafsky, 2008), and leave the identification of other participants for future work. Source of Scenarios We collected scenarios from DeScript (Wanzare et al., 2016), VirtualHome (Puig et al., 2018), and ROCStories (Mostafazadeh et al., 2016). DeScript consists of 40 daily scenarios (e.g., making coffee) and we 2140 Crowdsourcing proScript For the collected scenarios, we crowdsource the corresponding proScript on the Amazon Mechanical Turk. Our crowdsourcing procedure (Figure 2) is similar but simplified method to (Ciosici et al., 2021). First, given a scenario (e.g., bake a cake), each crowdworker is required to describe five to seven core events that they are essential for the scenario (Chambers, 2017) with the estimated time it takes to complete each event.4 In the second question, crowdworkers confirm the set of steps and they are asked to create a flowchart (DAG) by connecting the steps possibly in partial order. When crowdworkers make a submission, validation function is executed to check if the created flowchart is a valid (transitive reduction of) DAG that does not contain a cycle/loop and any short cut edge. Due to"
2021.findings-emnlp.184,N18-1144,1,0.788883,"limited amount of data hinders learning script knowledge by models. Furthermore, they provide no evaluation metric on the dataset for assessing model’s script knowledge. Story generation and tracking state changes Neural models have been demonstrated to success1 fully generate stories (Kiddon et al., 2016; Peng The dataset and code are available at https:// proscript.allenai.org/ et al., 2018; Zhai et al., 2019; Rashkin et al., 2020) 2139 Suppose a scenario where someone wants to “bake a cake”. as well as tracking state changes in procedural texts (Henaff et al., 2017; Bosselut et al., 2018; Dalvi et al., 2018; Tandon et al., 2020). Our work is related in terms of generating higher-level agenda (or plot) of a story and understanding latent preconditions and effects between events. However, a main difference between these studies and scripts is that story generation and state change tracking explicitly generate and/or predict character’s mental states and entity’s physical attributes (e.g., temperature), whereas scripts focuses on essential core events (Chambers, 2017) in partial order. 3 Preliminary Question: How long will it take for this scenario? 1.5 second(s) minute(s) hour(s) day(s) month(s) y"
2021.findings-emnlp.184,N19-1423,0,0.0105722,"and edges of the script. Given a scenario (s) and the number of events to generate in the script, proScriptgen generates events and edges for the partial-order script (G) in DOT language (Figure 6). Formally, we use the same encoder-decoder framework (eq.2) except that a scenario and number of steps to generate are described in natural text as x and the decoder is expected to generate both events and the edges (as y) in the script jointly. Transfer learning from WikiHow data Transfer learning often helps improve the performance when it is (pre-)trained on a similar task (Peters et al., 2018; Devlin et al., 2019). As additional resource for pre-training proScriptgen , we use procedural texts extracted from WikiHow,12 which contains 130k instances of a sequence of essential steps for a given topic in various categories (e.g., health, finance, hobbies, etc.). It is important to note that all the procedures in WikiHow are formatted as sequences rather than a partialorder, and therefore the model is biased towards generating sequences. We refer to this approach as proScriptgen-transfer . proScriptgen The proScript generation task combines natural language generation (i.e. generating events in natural lang"
2021.findings-emnlp.184,E12-1034,0,0.0388184,"Missing"
2021.findings-emnlp.184,D16-1032,1,0.822338,"shortcoming of this approach is the scalability; it is not easy to scale because of the cost for manual data collection (Chambers, 2017; Ostermann, 2020). In fact, Modi et al. (2016) crowdsourced 1000 stories that cover only 10 scripts, and similarly Regneri et al. (2010) end up with collecting 40 scripts. The limited amount of data hinders learning script knowledge by models. Furthermore, they provide no evaluation metric on the dataset for assessing model’s script knowledge. Story generation and tracking state changes Neural models have been demonstrated to success1 fully generate stories (Kiddon et al., 2016; Peng The dataset and code are available at https:// proscript.allenai.org/ et al., 2018; Zhai et al., 2019; Rashkin et al., 2020) 2139 Suppose a scenario where someone wants to “bake a cake”. as well as tracking state changes in procedural texts (Henaff et al., 2017; Bosselut et al., 2018; Dalvi et al., 2018; Tandon et al., 2020). Our work is related in terms of generating higher-level agenda (or plot) of a story and understanding latent preconditions and effects between events. However, a main difference between these studies and scripts is that story generation and state change tracking ex"
2021.findings-emnlp.184,2020.acl-main.703,0,0.0251896,"(e.g., bake a cake) as the input and generate a (possibly partial-order) script. In proScript, an event (node) requires that all the precedent events and paths are happened/executed in advance. et al., 2010; Chambers, 2017; Ostermann, 2020). In this work, we show for the first time that pre-trained neural language models (LMs) can be adapted to generate high-quality scripts, including appropriately partial ordering events where a specific temporal ordering is required only when it is necessary. LMs have previously been shown to successfully generate stories (Rashkin et al., 2020), summaries (Lewis et al., 2020), and commonsense facts (Bosselut et al., 2019; Hwang et al., 2020). 1 Introduction Here we investigate their application to script genScripts (Schank and Abelson, 1975) represent struc- eration. First, we collect large amount (6.4k) of tured commonsense knowledge about prototypi- partially ordered script from crowdsourcing with cal events in everyday situations/scenarios such as a similar but simplified collection method (Ciosici bake a cake (Figure 1). However, while scripts et al., 2021). We call the dataset as proScript have been shown to help understand narratives by (PaRtial Order SCRIPT"
2021.findings-emnlp.184,2021.ccl-1.108,0,0.0600469,"Missing"
2021.findings-emnlp.184,L16-1555,0,0.0954658,"Weber et al., 2018; Belyy and Van Durme, 2020), but it has its drawbacks. First, the source corpora is mainly from a news domain rather than everyday scenarios, and induced narrative chains contain a number of non-script events such as reporting verbs (Mostafazadeh et al., 2016; Chambers, 2017). Second, events are highly abstracted as tuples of verb and the dependency (subj or obj) (Ostermann, 2020). Third, the evaluation scheme for the narrative cloze task is insufficient to evaluate script knowledge (Chambers, 2017). Script as paraphrase sets Script as paraphrase sets (Regneri et al., 2010; Modi et al., 2016; Wanzare et al., 2016) is more recent approach to gather script knowledge, where crowd workers are asked to write down a sequence of events for a given everyday scenario (e.g., bake a cake) and the collected sequences (called event sequence description) are aligned with paraphrased events being clustered. The collected (partially ordered) scripts cover wide variety of everyday situations compared to narrative chains (news domain), but one shortcoming of this approach is the scalability; it is not easy to scale because of the cost for manual data collection (Chambers, 2017; Ostermann, 2020). I"
2021.findings-emnlp.184,W14-1606,0,0.0231252,"s. In particular, Chambers and Jurafsky (2008) introduced scripts as narrative chain, where verbs with the participants information (e.g., (claimed, subj), and (accused, obj) ) named narrative events are partially ordered according to causal and temporal relations. They also introduced narrative cloze task, where a model is expected to predict one removed narrative event, given all the other narrative events, while our proposed task requires to generate scripts as a partial-order graph for a given scenario. The “script as narrative chain” approach has been actively studied (Jans et al., 2012; Modi and Titov, 2014; Pichotta and Mooney, 2014; Rudinger et al., 2015; Granroth-Wilding and Clark, 2016; Weber et al., 2018; Belyy and Van Durme, 2020), but it has its drawbacks. First, the source corpora is mainly from a news domain rather than everyday scenarios, and induced narrative chains contain a number of non-script events such as reporting verbs (Mostafazadeh et al., 2016; Chambers, 2017). Second, events are highly abstracted as tuples of verb and the dependency (subj or obj) (Ostermann, 2020). Third, the evaluation scheme for the narrative cloze task is insufficient to evaluate script knowledge (Chambe"
2021.findings-emnlp.184,Q17-1003,0,0.0150327,"mmonsense knowledge about prototypi- partially ordered script from crowdsourcing with cal events in everyday situations/scenarios such as a similar but simplified collection method (Ciosici bake a cake (Figure 1). However, while scripts et al., 2021). We call the dataset as proScript have been shown to help understand narratives by (PaRtial Order SCRIPT), and this is substantially providing expectations, resolving ambiguity, and larger and more diverse than prior (crowdsourced) filling in unstated information (Chambers and Ju- dataset such as DeScript (Regneri et al., 2010) that rafsky, 2008; Modi et al., 2017, inter alia), they has 40 scripts. In proScript, all the events/paths have proved hard to author or extract from text, need to be happened/executed (cf. AND arcs in with only small script databases available (Regneri AND/OR graphs), whereas prior work on scripts 2138 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2138–2149 November 7–11, 2021. ©2021 Association for Computational Linguistics do not distinct core and optional/alternative events explicitly. Additionally, temporal duration of each event is annotated (e.g., take the cake out of the oven typically take"
2021.findings-emnlp.184,1985.tmi-1.17,0,0.174662,"n achieves 89.28, and for script generation, the best model obtains a graph edit distance of 4.97 (i.e., number of human edits), while human-created scripts achieve 2.98 on average. Our contributions are thus: • A new dataset (proScript) of crowdsourced scripts that is substantially larger than prior (manually crafted) datasets • Two complementary task definitions against proScript • Two new models for these task, providing the first demonstration that generative models can be successfully applied, although it is still significantly below human levels. 2 Related Work Script as narrative chain Mooney and DeJong (1985) and Chambers and Jurafsky (2008, inter alia) have investigated automatically inducing scripts from (unstructured) corpus. In particular, Chambers and Jurafsky (2008) introduced scripts as narrative chain, where verbs with the participants information (e.g., (claimed, subj), and (accused, obj) ) named narrative events are partially ordered according to causal and temporal relations. They also introduced narrative cloze task, where a model is expected to predict one removed narrative event, given all the other narrative events, while our proposed task requires to generate scripts as a partial-o"
2021.findings-emnlp.184,N16-1098,0,0.154238,"arrative event, given all the other narrative events, while our proposed task requires to generate scripts as a partial-order graph for a given scenario. The “script as narrative chain” approach has been actively studied (Jans et al., 2012; Modi and Titov, 2014; Pichotta and Mooney, 2014; Rudinger et al., 2015; Granroth-Wilding and Clark, 2016; Weber et al., 2018; Belyy and Van Durme, 2020), but it has its drawbacks. First, the source corpora is mainly from a news domain rather than everyday scenarios, and induced narrative chains contain a number of non-script events such as reporting verbs (Mostafazadeh et al., 2016; Chambers, 2017). Second, events are highly abstracted as tuples of verb and the dependency (subj or obj) (Ostermann, 2020). Third, the evaluation scheme for the narrative cloze task is insufficient to evaluate script knowledge (Chambers, 2017). Script as paraphrase sets Script as paraphrase sets (Regneri et al., 2010; Modi et al., 2016; Wanzare et al., 2016) is more recent approach to gather script knowledge, where crowd workers are asked to write down a sequence of events for a given everyday scenario (e.g., bake a cake) and the collected sequences (called event sequence description) are al"
2021.findings-emnlp.184,W18-1505,0,0.0578474,"Missing"
2021.findings-emnlp.184,N18-1202,0,0.0182608,"a sequence of events and edges of the script. Given a scenario (s) and the number of events to generate in the script, proScriptgen generates events and edges for the partial-order script (G) in DOT language (Figure 6). Formally, we use the same encoder-decoder framework (eq.2) except that a scenario and number of steps to generate are described in natural text as x and the decoder is expected to generate both events and the edges (as y) in the script jointly. Transfer learning from WikiHow data Transfer learning often helps improve the performance when it is (pre-)trained on a similar task (Peters et al., 2018; Devlin et al., 2019). As additional resource for pre-training proScriptgen , we use procedural texts extracted from WikiHow,12 which contains 130k instances of a sequence of essential steps for a given topic in various categories (e.g., health, finance, hobbies, etc.). It is important to note that all the procedures in WikiHow are formatted as sequences rather than a partialorder, and therefore the model is biased towards generating sequences. We refer to this approach as proScriptgen-transfer . proScriptgen The proScript generation task combines natural language generation (i.e. generating"
2021.findings-emnlp.184,E14-1024,0,0.023016,"bers and Jurafsky (2008) introduced scripts as narrative chain, where verbs with the participants information (e.g., (claimed, subj), and (accused, obj) ) named narrative events are partially ordered according to causal and temporal relations. They also introduced narrative cloze task, where a model is expected to predict one removed narrative event, given all the other narrative events, while our proposed task requires to generate scripts as a partial-order graph for a given scenario. The “script as narrative chain” approach has been actively studied (Jans et al., 2012; Modi and Titov, 2014; Pichotta and Mooney, 2014; Rudinger et al., 2015; Granroth-Wilding and Clark, 2016; Weber et al., 2018; Belyy and Van Durme, 2020), but it has its drawbacks. First, the source corpora is mainly from a news domain rather than everyday scenarios, and induced narrative chains contain a number of non-script events such as reporting verbs (Mostafazadeh et al., 2016; Chambers, 2017). Second, events are highly abstracted as tuples of verb and the dependency (subj or obj) (Ostermann, 2020). Third, the evaluation scheme for the narrative cloze task is insufficient to evaluate script knowledge (Chambers, 2017). Script as paraph"
2021.findings-emnlp.184,2020.emnlp-main.349,1,0.886511,"Missing"
2021.findings-emnlp.184,D15-1195,0,0.04212,"Missing"
2021.findings-emnlp.184,2020.emnlp-main.520,1,0.717003,"ata hinders learning script knowledge by models. Furthermore, they provide no evaluation metric on the dataset for assessing model’s script knowledge. Story generation and tracking state changes Neural models have been demonstrated to success1 fully generate stories (Kiddon et al., 2016; Peng The dataset and code are available at https:// proscript.allenai.org/ et al., 2018; Zhai et al., 2019; Rashkin et al., 2020) 2139 Suppose a scenario where someone wants to “bake a cake”. as well as tracking state changes in procedural texts (Henaff et al., 2017; Bosselut et al., 2018; Dalvi et al., 2018; Tandon et al., 2020). Our work is related in terms of generating higher-level agenda (or plot) of a story and understanding latent preconditions and effects between events. However, a main difference between these studies and scripts is that story generation and state change tracking explicitly generate and/or predict character’s mental states and entity’s physical attributes (e.g., temperature), whereas scripts focuses on essential core events (Chambers, 2017) in partial order. 3 Preliminary Question: How long will it take for this scenario? 1.5 second(s) minute(s) hour(s) day(s) month(s) year(s) Main Question 1"
2021.findings-emnlp.184,L16-1556,0,0.104406,"Belyy and Van Durme, 2020), but it has its drawbacks. First, the source corpora is mainly from a news domain rather than everyday scenarios, and induced narrative chains contain a number of non-script events such as reporting verbs (Mostafazadeh et al., 2016; Chambers, 2017). Second, events are highly abstracted as tuples of verb and the dependency (subj or obj) (Ostermann, 2020). Third, the evaluation scheme for the narrative cloze task is insufficient to evaluate script knowledge (Chambers, 2017). Script as paraphrase sets Script as paraphrase sets (Regneri et al., 2010; Modi et al., 2016; Wanzare et al., 2016) is more recent approach to gather script knowledge, where crowd workers are asked to write down a sequence of events for a given everyday scenario (e.g., bake a cake) and the collected sequences (called event sequence description) are aligned with paraphrased events being clustered. The collected (partially ordered) scripts cover wide variety of everyday situations compared to narrative chains (news domain), but one shortcoming of this approach is the scalability; it is not easy to scale because of the cost for manual data collection (Chambers, 2017; Ostermann, 2020). In fact, Modi et al. (20"
2021.findings-emnlp.184,D18-1413,0,0.0168986,"participants information (e.g., (claimed, subj), and (accused, obj) ) named narrative events are partially ordered according to causal and temporal relations. They also introduced narrative cloze task, where a model is expected to predict one removed narrative event, given all the other narrative events, while our proposed task requires to generate scripts as a partial-order graph for a given scenario. The “script as narrative chain” approach has been actively studied (Jans et al., 2012; Modi and Titov, 2014; Pichotta and Mooney, 2014; Rudinger et al., 2015; Granroth-Wilding and Clark, 2016; Weber et al., 2018; Belyy and Van Durme, 2020), but it has its drawbacks. First, the source corpora is mainly from a news domain rather than everyday scenarios, and induced narrative chains contain a number of non-script events such as reporting verbs (Mostafazadeh et al., 2016; Chambers, 2017). Second, events are highly abstracted as tuples of verb and the dependency (subj or obj) (Ostermann, 2020). Third, the evaluation scheme for the narrative cloze task is insufficient to evaluate script knowledge (Chambers, 2017). Script as paraphrase sets Script as paraphrase sets (Regneri et al., 2010; Modi et al., 2016;"
2021.findings-emnlp.184,W19-3404,0,0.0193821,"collection (Chambers, 2017; Ostermann, 2020). In fact, Modi et al. (2016) crowdsourced 1000 stories that cover only 10 scripts, and similarly Regneri et al. (2010) end up with collecting 40 scripts. The limited amount of data hinders learning script knowledge by models. Furthermore, they provide no evaluation metric on the dataset for assessing model’s script knowledge. Story generation and tracking state changes Neural models have been demonstrated to success1 fully generate stories (Kiddon et al., 2016; Peng The dataset and code are available at https:// proscript.allenai.org/ et al., 2018; Zhai et al., 2019; Rashkin et al., 2020) 2139 Suppose a scenario where someone wants to “bake a cake”. as well as tracking state changes in procedural texts (Henaff et al., 2017; Bosselut et al., 2018; Dalvi et al., 2018; Tandon et al., 2020). Our work is related in terms of generating higher-level agenda (or plot) of a story and understanding latent preconditions and effects between events. However, a main difference between these studies and scripts is that story generation and state change tracking explicitly generate and/or predict character’s mental states and entity’s physical attributes (e.g., temperatu"
2021.findings-emnlp.184,P10-1100,0,0.17898,"e collect large amount (6.4k) of tured commonsense knowledge about prototypi- partially ordered script from crowdsourcing with cal events in everyday situations/scenarios such as a similar but simplified collection method (Ciosici bake a cake (Figure 1). However, while scripts et al., 2021). We call the dataset as proScript have been shown to help understand narratives by (PaRtial Order SCRIPT), and this is substantially providing expectations, resolving ambiguity, and larger and more diverse than prior (crowdsourced) filling in unstated information (Chambers and Ju- dataset such as DeScript (Regneri et al., 2010) that rafsky, 2008; Modi et al., 2017, inter alia), they has 40 scripts. In proScript, all the events/paths have proved hard to author or extract from text, need to be happened/executed (cf. AND arcs in with only small script databases available (Regneri AND/OR graphs), whereas prior work on scripts 2138 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2138–2149 November 7–11, 2021. ©2021 Association for Computational Linguistics do not distinct core and optional/alternative events explicitly. Additionally, temporal duration of each event is annotated (e.g., take th"
2021.naacl-main.99,N16-1181,0,0.0762567,"2 Related Work Many early QA systems were designed as a combination of distinct modules, often composing outputs of lower-level language tasks to solve higherlevel tasks (Moldovan et al., 2000; Harabagiu and Hickl, 2006). However, much of this prior work is limited to pre-determined composition structures (Berant et al., 2013; Seo et al., 2015; Neelakantan et al., 2017; Roy and Roth, 2018). Various modular network architectures have been proposed to exploit compositionality (Rosenbaum et al., 2018; Kirsch et al., 2018). The closest models to our work are based on neural module networks (NMN) (Andreas et al., 2016) which compose task-specific simple neural modules. We to as ‘bridge’ questions. Complementation refers to questions such as ‘What percentage of X is not Y?’ M ODULAR QA can be easily extended to other reasoning types by defining the corresponding hints (§4.3). 1265 compare against formulations of NMNs for HotpotQA (Jiang and Bansal, 2019) and DROP (Gupta et al., 2020), both of which target only one dataset and do not reuse existing QA systems. Moreover, they provide attention-based explanations whose interpretability is unclear (Serrano and Smith, 2019; Brunner et al., 2020; Wiegreffe and Pin"
2021.naacl-main.99,D13-1160,0,0.0634388,"A,1 an interpretable system that learns to automatically decompose multi-hop and discrete reasoning questions. (3) Experiments on DROP and HotpotQA demonstrating M ODU LAR QA’s cross-dataset versatility, robustness, sample efficiency and ability to explain its reasoning in natural language. 2 Related Work Many early QA systems were designed as a combination of distinct modules, often composing outputs of lower-level language tasks to solve higherlevel tasks (Moldovan et al., 2000; Harabagiu and Hickl, 2006). However, much of this prior work is limited to pre-determined composition structures (Berant et al., 2013; Seo et al., 2015; Neelakantan et al., 2017; Roy and Roth, 2018). Various modular network architectures have been proposed to exploit compositionality (Rosenbaum et al., 2018; Kirsch et al., 2018). The closest models to our work are based on neural module networks (NMN) (Andreas et al., 2016) which compose task-specific simple neural modules. We to as ‘bridge’ questions. Complementation refers to questions such as ‘What percentage of X is not Y?’ M ODULAR QA can be easily extended to other reasoning types by defining the corresponding hints (§4.3). 1265 compare against formulations of NMNs fo"
2021.naacl-main.99,2020.acl-main.703,0,0.0322315,". The resulting sequence of sub-questions and their answers provides a human-interpretable description of the model’s neuro-symbolic reasoning (McCarthy, 1988; Smolensky, 1988), as illustrated in Fig. 1. Notably, TMNs learn to produce these decompositions using only distant supervision, without the need for any explicit human annotation. One of our key insights is that the capabilities of existing sub-models can be captured by training a text-to-text system to generate the questions in the sub-model’s training dataset (e.g., SQuAD), given appropriate hints. In our case, we train a BART model (Lewis et al., 2020) to generate questions given the context, answer, and preferred vocabulary as hints. We then use these sub-task question models to generate sub-questions (and identify appropriate sub-models) that could lead to the likely intermediate answers extracted for each step of the complex question (“Raymond S.” and “American” in the HotpotQA example in Fig. 1). The resulting sub-questions, by virtue of our training, are in the language (i.e., within-scope) of the corresponding sub-models. These sub-question sequences can now be used to train the next-question generator to sequentially produce the next"
2021.naacl-main.99,2020.emnlp-main.711,1,0.884598,"Missing"
2021.naacl-main.99,P19-1416,0,0.352531,"ularQA Goldberg, 2020) of its reasoning as a composition of simpler sub-tasks, as shown in Fig. 1. Motivated by this, we ask the following question: Given a set of existing QA models, can one leverage them to answer complex questions by communicating with these existing models? We propose a general framework, Text Modular Networks (TMNs), that answers this question by learning to decompose complex questions (of any form) into sub-questions that are answerable by existing QA models—symbolic or neural (henceforth referred to as sub-models ).2 Unlike previous approaches (Talmor and Berant, 2018; Min et al., 2019a), the decompositions are not based on splits of the complex questions and aren’t built independent of the sub-model. Instead, our framework learns to generate sub-questions in the scope 2 TMNs, in fact, treat sub-models as blackboxes, and can thus use any model or function as a module. 1264 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1264–1279 June 6–11, 2021. ©2021 Association for Computational Linguistics of existing models. For instance, the second subquestion in the DROP dataset exam"
2021.naacl-main.99,P06-1114,0,0.0378308,"verages existing simpler models—neural and symbolic— as blackboxes for answering complex questions. (2) M ODULAR QA,1 an interpretable system that learns to automatically decompose multi-hop and discrete reasoning questions. (3) Experiments on DROP and HotpotQA demonstrating M ODU LAR QA’s cross-dataset versatility, robustness, sample efficiency and ability to explain its reasoning in natural language. 2 Related Work Many early QA systems were designed as a combination of distinct modules, often composing outputs of lower-level language tasks to solve higherlevel tasks (Moldovan et al., 2000; Harabagiu and Hickl, 2006). However, much of this prior work is limited to pre-determined composition structures (Berant et al., 2013; Seo et al., 2015; Neelakantan et al., 2017; Roy and Roth, 2018). Various modular network architectures have been proposed to exploit compositionality (Rosenbaum et al., 2018; Kirsch et al., 2018). The closest models to our work are based on neural module networks (NMN) (Andreas et al., 2016) which compose task-specific simple neural modules. We to as ‘bridge’ questions. Complementation refers to questions such as ‘What percentage of X is not Y?’ M ODULAR QA can be easily extended to oth"
2021.naacl-main.99,P17-1141,0,0.0201648,"100 - X; if_then(X &lt;op&gt; Y, Z, W) that returns Z if X &lt;op&gt; Y is true, otherwise returns W. answer from the dataset as the input context and answer. For the estimated question vocabulary, we select essential words5 from the gold questions (referred as the function Φ) with additional irrelevant words sampled from other questions.6 To train the text-to-text BARTS model, we use a simple concatenation of the passage, vocabulary, and answer (with markers such as “H:” and “A:” to indicate each field) as the input sequence and the question as the output sequence. While a constrained-decoding approach (Hokamp and Liu, 2017; Hu et al., 2019a) could be used here to further promote the use of the vocabulary hints, this simple approach was effective and more generally applicable to other hints in our use-case. Once this model is trained, we use it with nucleus sampling (Holtzman et al., 2020) to generate k sub-questions, Q, and filter out those that lead an incorrect or no answer using AS : GS (p, a, v) = {q ∈ Q |overlaps(AS (p, q), a)} Math Sub-task Question Model, GC . Given the symbolic nature of this solver, rather than training a neural generator, we simply generate all possible numeric questions given the con"
2021.naacl-main.99,J13-2005,0,0.0160581,"e use these generated sub-question decompositions as the training data for the next-question generator model. 4 M ODULAR QA System To generate training decompositions for a complex question using a sub-task question model, we ex- We next describe a specific instantiation of the Text tract distant-supervision hints z corresponding to Modular Network: M ODULAR QA – a new QA each reasoning step. This is akin to the distant su- system that works across HotpotQA and DROP. To pervision approaches used to extract logical forms handle these datasets, we first introduce the two QA in semantic parsing (Liang et al., 2013; Berant et al., sub-models(§4.1), the sub-task question models for 2013) and the intermediate entities in a reasoning these models(§4.2), our approach to build training chain (Gupta et al., 2020; Jiang and Bansal, 2019). data (§4.3), and the inference procedure used for In our running DROP example, under the defi- question-answering(§4.4). nition of z = hp, a, vi, we would need to provide 4 the context, answer, and question vocabulary for As mentioned before, these are soft hints and the model each reasoning step. We can derive intermediate can be trained to handle noise in these hints. 1267"
2021.naacl-main.99,P19-1613,0,0.322756,"ularQA Goldberg, 2020) of its reasoning as a composition of simpler sub-tasks, as shown in Fig. 1. Motivated by this, we ask the following question: Given a set of existing QA models, can one leverage them to answer complex questions by communicating with these existing models? We propose a general framework, Text Modular Networks (TMNs), that answers this question by learning to decompose complex questions (of any form) into sub-questions that are answerable by existing QA models—symbolic or neural (henceforth referred to as sub-models ).2 Unlike previous approaches (Talmor and Berant, 2018; Min et al., 2019a), the decompositions are not based on splits of the complex questions and aren’t built independent of the sub-model. Instead, our framework learns to generate sub-questions in the scope 2 TMNs, in fact, treat sub-models as blackboxes, and can thus use any model or function as a module. 1264 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1264–1279 June 6–11, 2021. ©2021 Association for Computational Linguistics of existing models. For instance, the second subquestion in the DROP dataset exam"
2021.naacl-main.99,P00-1071,0,0.0901028,"neral framework that leverages existing simpler models—neural and symbolic— as blackboxes for answering complex questions. (2) M ODULAR QA,1 an interpretable system that learns to automatically decompose multi-hop and discrete reasoning questions. (3) Experiments on DROP and HotpotQA demonstrating M ODU LAR QA’s cross-dataset versatility, robustness, sample efficiency and ability to explain its reasoning in natural language. 2 Related Work Many early QA systems were designed as a combination of distinct modules, often composing outputs of lower-level language tasks to solve higherlevel tasks (Moldovan et al., 2000; Harabagiu and Hickl, 2006). However, much of this prior work is limited to pre-determined composition structures (Berant et al., 2013; Seo et al., 2015; Neelakantan et al., 2017; Roy and Roth, 2018). Various modular network architectures have been proposed to exploit compositionality (Rosenbaum et al., 2018; Kirsch et al., 2018). The closest models to our work are based on neural module networks (NMN) (Andreas et al., 2016) which compose task-specific simple neural modules. We to as ‘bridge’ questions. Complementation refers to questions such as ‘What percentage of X is not Y?’ M ODULAR QA c"
2021.naacl-main.99,2020.tacl-1.13,0,0.0508985,"Berant, 2018) and HotpotQA. Both approaches (Talmor and Berant, 2018; Min et al., 2019b) focus on directly training a model to produce sub-questions using question spans—an approach not suitable for DROP questions (as illustrated in Fig. 1). Our nextquestion generator overcomes this limitation by generating free-form sub-questions in the language of existing models. Perez et al. (2020) also use a text-to-text model to generate sub-questions for HotpotQA. However, they generate simpler questions without capturing the requisite reasoning, and hence use them mainly for evidence retrieval. BREAK (Wolfson et al., 2020) follows an alternative paradigm of collecting full question decomposition meaning representations (QDMR) annotations. While this can be effective, it relies on costly human annotation that may not generalize to domains with new decomposition operations. Its decompositions are generated in a model-agnostic way and still need QA systems to answer the subquestions, e.g, high-level QDMR questions such as “Which is earlier?” and “Which is longer?” would need special systems that can map these to symbolic comparisons. In contrast, TMNs start with pre-determined models and learn to generate decompos"
2021.naacl-main.99,D18-1259,0,0.0303371,"rican) Hey S! Who directed Little Big Girl? Raymond S. ModularQA Hey S! What was Raymond S&apos;s nationality? ModularQA ■ Okay, the answer is &quot;American&quot; ModularQA Existing QA system S American Existing QA system S Figure 1: M ODULAR QA learns to ask sub-questions to existing simple QA models, including a symbolic calculator, to answer a given complex question. Notably, the approach does not rely on annotated decompositions. Despite this, the system learned to add “start to take a dip” in the DROP dataset question. An intuitive way to solve more complex tasks, such as multi-hop question-answering (Yang et al., 2018; Khashabi et al., 2018; Khot et al., 2020) and numerical reasoning (Dua et al., 2019), would be to decompose them into already solved simpler problems, e.g., single-fact QA (Rajpurkar et al., 2016). Besides allowing reuse of existing simpler models, this approach would yield an interpretable system that provides a faithful explanation (Jacovi and https://github.com/allenai/modularqa Existing QA system S 2002 Hey C! diff(2003, 2002)=? Introduction 1 2003 ModularQA Goldberg, 2020) of its reasoning as a composition of simpler sub-tasks, as shown in Fig. 1. Motivated by this, we ask the following"
2021.naacl-main.99,D16-1264,0,0.0476719,"sting QA system S Figure 1: M ODULAR QA learns to ask sub-questions to existing simple QA models, including a symbolic calculator, to answer a given complex question. Notably, the approach does not rely on annotated decompositions. Despite this, the system learned to add “start to take a dip” in the DROP dataset question. An intuitive way to solve more complex tasks, such as multi-hop question-answering (Yang et al., 2018; Khashabi et al., 2018; Khot et al., 2020) and numerical reasoning (Dua et al., 2019), would be to decompose them into already solved simpler problems, e.g., single-fact QA (Rajpurkar et al., 2016). Besides allowing reuse of existing simpler models, this approach would yield an interpretable system that provides a faithful explanation (Jacovi and https://github.com/allenai/modularqa Existing QA system S 2002 Hey C! diff(2003, 2002)=? Introduction 1 2003 ModularQA Goldberg, 2020) of its reasoning as a composition of simpler sub-tasks, as shown in Fig. 1. Motivated by this, we ask the following question: Given a set of existing QA models, can one leverage them to answer complex questions by communicating with these existing models? We propose a general framework, Text Modular Networks (TM"
2021.naacl-main.99,D19-1251,0,0.131377,"ostly human annotation that may not generalize to domains with new decomposition operations. Its decompositions are generated in a model-agnostic way and still need QA systems to answer the subquestions, e.g, high-level QDMR questions such as “Which is earlier?” and “Which is longer?” would need special systems that can map these to symbolic comparisons. In contrast, TMNs start with pre-determined models and learn to generate decompositions in their language. While many multi-hop QA models exist for HotpotQA and DROP, these are often equally complex models (Tu et al., 2020; Fang et al., 2020; Ran et al., 2019) focusing on just one of these datasets. Only on HotpotQA, where supporting sentences are annotated, can these models also produce post-hoc explanations, but these explanations are often not faithful and shown to be gameable (Trivedi et al., 2020). TMNs are able to produce explanations for multiple datasets without needing such annotations, making it more generalizable to future datasets. qc: How many years did it take for the services sector to rebound? q1: In what year did the services sector rebound? a1: 2003 NextGen q2: When did the services sector start to take a dip? a2: 2002 QAS q3: dif"
2021.naacl-main.99,Q18-1012,0,0.0206626,"e multi-hop and discrete reasoning questions. (3) Experiments on DROP and HotpotQA demonstrating M ODU LAR QA’s cross-dataset versatility, robustness, sample efficiency and ability to explain its reasoning in natural language. 2 Related Work Many early QA systems were designed as a combination of distinct modules, often composing outputs of lower-level language tasks to solve higherlevel tasks (Moldovan et al., 2000; Harabagiu and Hickl, 2006). However, much of this prior work is limited to pre-determined composition structures (Berant et al., 2013; Seo et al., 2015; Neelakantan et al., 2017; Roy and Roth, 2018). Various modular network architectures have been proposed to exploit compositionality (Rosenbaum et al., 2018; Kirsch et al., 2018). The closest models to our work are based on neural module networks (NMN) (Andreas et al., 2016) which compose task-specific simple neural modules. We to as ‘bridge’ questions. Complementation refers to questions such as ‘What percentage of X is not Y?’ M ODULAR QA can be easily extended to other reasoning types by defining the corresponding hints (§4.3). 1265 compare against formulations of NMNs for HotpotQA (Jiang and Bansal, 2019) and DROP (Gupta et al., 2020)"
2021.naacl-main.99,2020.emnlp-main.248,0,0.0340074,"nerated perturbations. Importantly, M OD ULAR QA provides easy-to-interpret explanations of its reasoning. It is the first system that decomposes DROP questions into textual sub-questions and can be applied to both DROP and HotpotQA. Extending this model to more question classes such as counting (“How many touchdowns were scored by X?”) and Boolean conjunction (“Are both X and Y musicians?”) are interesting avenues for future work. To handle the former class, the first challenge is building models that can return a list of answers—a relatively unexplored task until recently (Hu et al., 2019b; Segal et al., 2020). For Boolean questions, the challenge is identifying good sub-questions as there is a large space of questions such as “Did musicians work for X?” that may have the expected yes/no answer but are not part of the true decomposition. Semantic parsing faces similar issues when questions have a large number of possible logical forms (Dasigi et al., 2019). Finally, end-to-end training of the next-question generator and QA models via REINFORCE (Williams, 1992) can further improve the score and allow for faster greedy inference. Acknowledgements We thank the Aristo team at AI2 for helpful input, Bea"
2021.naacl-main.99,D15-1171,0,0.026046,"system that learns to automatically decompose multi-hop and discrete reasoning questions. (3) Experiments on DROP and HotpotQA demonstrating M ODU LAR QA’s cross-dataset versatility, robustness, sample efficiency and ability to explain its reasoning in natural language. 2 Related Work Many early QA systems were designed as a combination of distinct modules, often composing outputs of lower-level language tasks to solve higherlevel tasks (Moldovan et al., 2000; Harabagiu and Hickl, 2006). However, much of this prior work is limited to pre-determined composition structures (Berant et al., 2013; Seo et al., 2015; Neelakantan et al., 2017; Roy and Roth, 2018). Various modular network architectures have been proposed to exploit compositionality (Rosenbaum et al., 2018; Kirsch et al., 2018). The closest models to our work are based on neural module networks (NMN) (Andreas et al., 2016) which compose task-specific simple neural modules. We to as ‘bridge’ questions. Complementation refers to questions such as ‘What percentage of X is not Y?’ M ODULAR QA can be easily extended to other reasoning types by defining the corresponding hints (§4.3). 1265 compare against formulations of NMNs for HotpotQA (Jiang"
2021.naacl-main.99,P19-1282,0,0.0268505,"are based on neural module networks (NMN) (Andreas et al., 2016) which compose task-specific simple neural modules. We to as ‘bridge’ questions. Complementation refers to questions such as ‘What percentage of X is not Y?’ M ODULAR QA can be easily extended to other reasoning types by defining the corresponding hints (§4.3). 1265 compare against formulations of NMNs for HotpotQA (Jiang and Bansal, 2019) and DROP (Gupta et al., 2020), both of which target only one dataset and do not reuse existing QA systems. Moreover, they provide attention-based explanations whose interpretability is unclear (Serrano and Smith, 2019; Brunner et al., 2020; Wiegreffe and Pinter, 2019). Question decomposition has been pursued before for ComplexWebQuestions (Talmor and Berant, 2018) and HotpotQA. Both approaches (Talmor and Berant, 2018; Min et al., 2019b) focus on directly training a model to produce sub-questions using question spans—an approach not suitable for DROP questions (as illustrated in Fig. 1). Our nextquestion generator overcomes this limitation by generating free-form sub-questions in the language of existing models. Perez et al. (2020) also use a text-to-text model to generate sub-questions for HotpotQA. Howev"
C10-2020,W10-0902,0,0.0138759,"aphrasing to expand the space of interpretations considered, and thus improve interpretation. Underspecified representations (e.g., van Deemter and Peters, 1996; Pinkal, 1999) allow ambiguity (in particular scope ambiguity) to be preserved in a single structure and commitments deferred until later, allowing multiple interpretations to be carried through the system. Similarly, a system can defer commitment by simply carrying multiple, alternative interpretations forward as individual structures, or packed together into a single structure (e.g., Alshawi and van Eijck, 1989, Bobrow et al., 2005; Kim et al., 2010a,b). Finally, canonicalized representations are often used to represent (and hence carry through the system) multiple, equivalent surface forms as a single structure, e.g., normalizing active and passive forms, or alternative forms of noun modification (Rinaldi et al., 2003). All these techniques help avoid premature commitment in interpretation. As well as avoiding early rejection of interpretations in these ways, there has been substantial, recent work on expanding the space of possible interpretations considered through the use of paraphases (e.g., Sekine and Inui, 2007). Paraphrasing is b"
C10-2020,C10-1066,0,0.0209393,"aphrasing to expand the space of interpretations considered, and thus improve interpretation. Underspecified representations (e.g., van Deemter and Peters, 1996; Pinkal, 1999) allow ambiguity (in particular scope ambiguity) to be preserved in a single structure and commitments deferred until later, allowing multiple interpretations to be carried through the system. Similarly, a system can defer commitment by simply carrying multiple, alternative interpretations forward as individual structures, or packed together into a single structure (e.g., Alshawi and van Eijck, 1989, Bobrow et al., 2005; Kim et al., 2010a,b). Finally, canonicalized representations are often used to represent (and hence carry through the system) multiple, equivalent surface forms as a single structure, e.g., normalizing active and passive forms, or alternative forms of noun modification (Rinaldi et al., 2003). All these techniques help avoid premature commitment in interpretation. As well as avoiding early rejection of interpretations in these ways, there has been substantial, recent work on expanding the space of possible interpretations considered through the use of paraphases (e.g., Sekine and Inui, 2007). Paraphrasing is b"
C10-2020,W03-1604,0,0.0354522,"Missing"
C10-2020,P89-1004,0,\N,Missing
C16-1278,W16-1303,1,0.778456,"ured: Study Guides: A collection of free text from six resources: study guides for two elementary science exams, a teacher’s manual, a set of flashcards, and two dictionary resources: a science dictionary for kids, and the open-domain Simple English Wiktionary6 . A total of 3,832 science-domain sentences and 17,473 open-domain definition sentences were included. Aristo TableStore: An open collection7 of approximately 100 semi-formal tables (approximately 10k rows, 30k cells) containing knowledge tailored to elementary science exams, constructed using a mixture of manual and automatic methods (Dalvi et al., 2016). The table knowledge spans across knowledge types, from properties and taxonomic knowledge to causality, processes, and domain models. Each table encodes an aspect of the science domain (e.g., animal adaptations, measuring instruments, energy conversions, etc.), where variations are typically enumerated (e.g. “a <grill&gt; converts <chemical energy&gt; to <heat energy&gt;”, “a <flashlight&gt; converts <electrical energy&gt; into <light energy&gt;”, etc.). 4.2 Solvers We characterize QA approaches from two families: a baseline that uses “learning to rank” (L2R) with information retrieval (IR) features, and more"
C16-1278,Q15-1015,1,0.491036,"Missing"
C16-1278,P14-1092,1,0.856906,"ere variations are typically enumerated (e.g. “a <grill&gt; converts <chemical energy&gt; to <heat energy&gt;”, “a <flashlight&gt; converts <electrical energy&gt; into <light energy&gt;”, etc.). 4.2 Solvers We characterize QA approaches from two families: a baseline that uses “learning to rank” (L2R) with information retrieval (IR) features, and more recent inference models. Retrieval Model: We use an L2R model which finds answers by scoring passage level evidence for each answer choice from the unstructured textual knowledge sources. Our implementation is based on the candidate ranking (CR) model described in Jansen et al. (2014). Short passages are scored based on how similar they are to the words in the question and the corresponding answer choice. The similarity scores are computed using cosine similarity of tf.idf representations of the question and passages, and used in a L2R framework to produce the final ranking of the answer choices. We created two versions of the solver: one that uses the study guide collection, and the other with a textual representation of the Aristo TableStore. Apache Lucene8 is used to index and retrieve passages. Inference Models: For inference, we use two models that operate over a stru"
C16-1278,N16-3020,0,0.00981887,"e a sub-categorization of these. Here we build upon these prior works and provide both a more fine-grained characterization of the knowledge types required to answer these questions, along with manually curated answer explanations. 2957 This allows us to compare the relative strengths and weaknesses of different QA systems from knowledge and inference requirements identified using both bottom-up (from explanations) and top-down (from questions) approaches. More broadly and with respect to explanations, there is a recent trend towards emphasizing interpretable models for machine learning (e.g. Ribeiro et al. (2016)) that are able to produce human-readable explanations for their reasoning, both to improve human trust in automated inference, as well as to verify that a given model is accurately capturing the aspects of complex reasoning required for a given task. We view this work as complementary, here characterizing the knowledge and inference requirements that an automated reasoning method for science exams must meet to assemble compelling human-readable explanations as part of the inference process. 3 Knowledge and Inference Analysis Estimating knowledge and inference requirements is challenging for m"
C16-1278,roberts-hickl-2008-scaling,0,0.0801933,"Missing"
C16-1278,D15-1080,1,\N,Missing
D13-1056,2009.eamt-1.23,0,0.092581,"Missing"
D13-1056,D12-1032,0,0.0163741,"edness Feature is a single scaled number in [0, 1] from the best performing system (Han et al., 2013) of the *Sem 2013 Semantic Textual Similarity (STS) task. We included this feature mainly to deal with cases where “related” words cannot be well measured by either paraphrases or distributional similarities. For instance, in one alignment dataset annotators aligned married with wife. Adding a few other words as comparison, the Han et al. (2013) system gives the following similarity scores: married/wife: 0.85 married/husband: 0.84 married/child: 0.10 married/stone: 0.01 Name Phylogeny Feature (Andrews et al., 2012) is a similarity feature with a string transducer to model how one name evolves to another. Examples below show how similar is the name Bill associated with other names in log probability: Bill/Bill: -0.8 Bill/Billy: -5.2 Bill/William: -13.6 Bill/Mary: -18.6 Finally, one decision we made during feature design was not to use any parsing-based features, with a permissive assumption that the input might not be well-formed English, or even not complete sentences (such as fragmented snippets from web search). The “deepest” linguistic processing stays at the level of tagging and chunking, making the"
D13-1056,P11-1131,0,0.0343557,"Missing"
D13-1056,P06-1009,0,0.241342,"translation) rather than monolingual. Moreover, most work has considered token-based approaches over phrase-based.1 Here we seek to address this imbalance by proposing better phrase-based models for monolingual word alignment. ∗ Performed while faculty at Johns Hopkins University. In this paper we use the term token-based alignment for one-to-one alignment and phrase-based for non one-to-one alignment, and word alignment in general for both. 1 The token aligner jacana-align (Yao et al., 2013a) has achieved state-of-the-art result on the task of monolingual alignment, based on previous work of Blunsom and Cohn (2006). It employs a Conditional Random Field (Lafferty et al., 2001) to align tokens from the source sentence to tokens in the target sentence, by treating source tokens as “observation” and target tokens as “hidden states”. However, it is not designed to handle phrase-based alignment, largely due to the Markov nature of the underlying model: a state can only span one token each time, making it unable to align multiple consecutive tokens (i.e. a phrase). We extend this model by introducing semiMarkov states for phrase-based alignment: a state can instead span multiple consecutive time steps, thus a"
D13-1056,J93-2003,0,0.0472056,"e. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can be easily added, significantly improving exact alignment match rate for whole sentence pairs. Besides the common application of textual entailment and question answering, monolingual alignment has also been applied in the field of text generation (Barzilay and Lee, 2003; Pang et al., 2003). Word alignment has been more explored in machine translation. The IBM models (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2 http://code.google.com/p/jacana/ 591 bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-p"
D13-1056,W07-1427,0,0.102732,"Missing"
D13-1056,J08-4005,1,0.880184,"Missing"
D13-1056,P09-1053,0,0.2096,"Missing"
D13-1056,W11-2107,0,0.0285654,"Missing"
D13-1056,C04-1051,0,0.429672,"and phrase-only alignment (subscript p). it is another topic, we simply show in this section using just alignment scores in binary prediction problems. Specifically, we pick the tasks of recognizing textual entailment (RTE), paraphrase identification (PP), and question answering sentence ranking (QA) described in Heilman and Smith (2010): RTE: predicting whether a hypothesis can be inferred from the premise, with training data from RTE-1/2 and RTE-3 dev, and test from RTE-3 test. PP: predicting whether two sentences are paraphrases, with training and test data from the MSR Paraphrase Corpus (Dolan et al., 2004). QA: predicting whether a sentence contains the answer to the question, with training data from TREC-8 to TREC-12 and test data from TREC-13. For each aligned pair, we can compute a normalized decoding score. Following MacCartney et al. (2008), we select a threshold score and predict true if the decoding score is above this threshold. For the tasks of RTE and PP, we tuned this threshold w.r.t the maximal accuracy on the training set, then reported performance on the test set. For the task of QA, since the evaluation methods in Mean Average Precision and Mean Reciprocal Rank only need a ranked"
D13-1056,N13-1092,1,0.387664,"Missing"
D13-1056,N10-1112,0,0.0196559,"ue labels. It is only computed during training in the denominator because in the numerator cost(ay , ay ) = 0. Hamming cost is used in practice without learning the weights (i.e., uniform weights). The more inconsistence there is between ay and ˆ a, the more penalized is the decoding sequence ˆ a through the cost function. 3.2 1 6 7..14 Z(s, t) X Shops ...-... P exp( i,k λk fk (ai−1 , ai , s, t)) This assumes a first-order Conditional Random Field (Lafferty et al., 2001). Since the word alignment task is evaluated over F1 , instead of directly optimizing it, we choose a much easier objective (Gimpel and Smith, 2010) and add a cost function to the normalizing function Z(s, t) in the denominator: Z(s, t) = 0 Phrase-based Model The token-based model supports 1 : 1 alignment. We first extend it in the direction of ls : 1, where a target state spans ls words on the source side (ls source words align to 1 target word). Then we extend it in the direction of 1 : lt , where lt is the target phrase length a source word aligns to (1 source word aligns to lt target words). The final combined 592 15 shops Shops are closed up for now until March Figure 1: A semi-Markov phrase-based model example and the desired Viterb"
D13-1056,S13-1005,0,0.0116787,"s is the left hand side syntactic non-terminal symbol. We did not use the syntactic part (e.g., the N P of N N S ↔ the N N S of N P ) of PPDB as we did not make the assumption that the input sentence pairs were well-formed (and newswire-like) English, or 594 even of a language with a parser available. Also, for phrasal alignments, we ruled out those paraphrases spanning multiple syntactic structures, or of different syntactic structures (indicated as [X] in PPDB), for instance, and crazy ↔ , mad. Semantic Relatedness Feature is a single scaled number in [0, 1] from the best performing system (Han et al., 2013) of the *Sem 2013 Semantic Textual Similarity (STS) task. We included this feature mainly to deal with cases where “related” words cannot be well measured by either paraphrases or distributional similarities. For instance, in one alignment dataset annotators aligned married with wife. Adding a few other words as comparison, the Han et al. (2013) system gives the following similarity scores: married/wife: 0.85 married/husband: 0.84 married/child: 0.10 married/stone: 0.01 Name Phylogeny Feature (Andrews et al., 2012) is a similarity feature with a string transducer to model how one name evolves"
D13-1056,N10-1145,0,0.119622,"on Empirical Methods in Natural Language Processing, pages 590–600, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics CRF model on the task of phrase-based alignment, and then show a basic application in the NLP tasks of recognizing textual entailment, paraphrase identification, and question answering sentence ranking. The final phrase-based aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron lea"
D13-1056,N06-1014,0,0.0177325,"08) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens. Andr´es-Ferrer and Juan (2009) extended the HMM-based method to Hidden Semi-Markov Models (HSMM) (Ostendorf et al., 1996), allowing phrasal alignments on the source side. Finally, Bansal et al. (2011) unified the HSMM models with the alignment by agreement framework (Liang et al., 2006), achieving phrasal alignment that agreed in both directions. Despite successful usage of generative semiMarkov models in bilingual alignment, this has not been followed with models in discriminative monolingual alignment. Essentially monolingual alignment would benefit more from discriminative models with various feature extractions (just like those defined in MANLI) than generative models without any predefined feature (just like how they were used in bilingual alignment). To combine the strengths of both semi-Markov models and discriminative training, we propose to use the semi-Markov Condi"
D13-1056,C08-1066,0,0.0315246,"odels (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2 http://code.google.com/p/jacana/ 591 bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens. Andr´es-Ferrer and Juan (2009) extended the HMM-based method to Hidden Semi-Markov Models (HSMM) (Ostendorf et al., 1996), allowing phrasal alignments on the source side. Finally, Bansal et al. (2011) unified the HSMM models with the alignment by agreement framework (Lia"
D13-1056,D08-1084,0,0.228637,"d aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can b"
D13-1056,W02-1018,0,0.0337279,"sentence pairs. Besides the common application of textual entailment and question answering, monolingual alignment has also been applied in the field of text generation (Barzilay and Lee, 2003; Pang et al., 2003). Word alignment has been more explored in machine translation. The IBM models (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2 http://code.google.com/p/jacana/ 591 bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens."
D13-1056,P09-2073,0,0.0234686,"ttle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics CRF model on the task of phrase-based alignment, and then show a basic application in the NLP tasks of recognizing textual entailment, paraphrase identification, and question answering sentence ranking. The final phrase-based aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment o"
D13-1056,J03-1002,0,0.0465458,"Missing"
D13-1056,N03-1024,0,0.0311421,"nt of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can be easily added, significantly improving exact alignment match rate for whole sentence pairs. Besides the common application of textual entailment and question answering, monolingual alignment has also been applied in the field of text generation (Barzilay and Lee, 2003; Pang et al., 2003). Word alignment has been more explored in machine translation. The IBM models (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2 http://code.google.com/p/jacana/ 591 bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences"
D13-1056,D12-1016,0,0.0332604,"rase-based alignment, and then show a basic application in the NLP tasks of recognizing textual entailment, paraphrase identification, and question answering sentence ranking. The final phrase-based aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadan"
D13-1056,P11-2044,0,0.385701,"work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can be easily added, significantly improving exact a"
D13-1056,C12-2120,0,0.278948,"Missing"
D13-1056,C96-2141,0,0.435527,"essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2 http://code.google.com/p/jacana/ 591 bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens. Andr´es-Ferrer and Juan (2009) extended the HMM-based method to Hidden Semi-Markov Models (HSMM) (Ostendorf et al., 1996), allowing phrasal alignments on the source side. Finally, Bansal et al. (2011) unified the HSMM models with the alignment by agreement framework (Liang et al., 2006), achieving phrasal alignment that agreed in both directions"
D13-1056,U06-1019,0,0.0376029,"Missing"
D13-1056,C10-1131,0,0.102753,"Missing"
D13-1056,D07-1003,0,0.114867,"Missing"
D13-1056,P13-2123,1,0.662726,"Missing"
D13-1056,N13-1106,1,0.881841,"Missing"
D13-1056,N03-1003,0,\N,Missing
D13-1056,U04-1000,0,\N,Missing
D13-1177,D08-1073,0,0.65788,"ens # of events # of non-N ONE relations Avg 3.80 89.98 6.20 5.64 Min 1 19 2 1 Max 15 319 15 24 Table 1: Process statistics over 148 process descriptions. N ONE is used to indicate no relation. relation set R. We chose a relation set R that captures the essential aspects of temporal relations between events in a process, while keeping the annotation as simple as possible. For instance, we include the S UPER relation that appears in temporal annotations such as the Timebank corpus (Pustejovsky et al., 2003) and Allen’s work, but in practice was not considered by many temporal ordering systems (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012). Importantly, our relation set also includes the relations C AUSES and E NABLES, which are fundamental to modeling processes and go beyond simple temporal ordering. We also added event coreference (S AME) to R. Do et al. (2012) used event coreference information in a temporal ordering task to modify probabilities provided by pairwise classifiers prior to joint inference. In this paper, we simply treat S AME as another event-event relation, which allows us to easily perform joint inference and employ structural constraints that combine both coreference"
D13-1177,P11-1098,0,0.14816,"and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one another and their semantic roles are extracted. Recently, Cheung et al. (2013) proposed an unsupervised generative model for inducing such templates. A major difference in our work is that we do not learn typical event relations from a large and redundant corpus, but are given a paragraph and have a “one-shot” chance to extract the process structure. We showed in this paper that global structural properties lead to significant improvements in extraction accuracy, and ILP is an effective framework CAUSES CAUSES shifts CA"
D13-1177,N13-1104,0,0.0243669,"ons between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one another and their semantic roles are extracted. Recently, Cheung et al. (2013) proposed an unsupervised generative model for inducing such templates. A major difference in our work is that we do not learn typical event relations from a large and redundant corpus, but are given a paragraph and have a “one-shot” chance to extract the process structure. We showed in this paper that global structural properties lead to significant improvements in extraction accuracy, and ILP is an effective framework CAUSES CAUSES shifts CAUSES skipped CAUSES deleted COTEMP secrete PREV CAUSES CAUSES CAUSES used duplicated bind CAUSES PREV ENABLES SAME NONE binds Figure 3: Process graph fra"
D13-1177,de-marneffe-etal-2006-generating,1,0.126092,"Missing"
D13-1177,D12-1062,0,0.42201,"turing, economical developments, and various phenomena in life and social sciences can all be viewed as types of processes. Processes are complicated objects; consider for example the biological process of ATP synthesis described in Figure 1. This process involves 12 entities and 8 events. Additionally, it describes relations between events and entities, and the relationship between events (e.g., the second occurrence of the event ‘enter’, causes the event ‘changing’). ∗ Both authors equally contributed to the paper Timeline construction involves identifying temporal relations between events (Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and is thus related to process extraction as both focus on event-event relations spanning multiple sentences. However, events in processes are tightly coupled in ways that go beyond simple temporal ordering, and these dependencies are central for the process extraction task. Hence, capturing process structure requires modeling a larger set of relations that includes temporal, causal and co-reference relations. In this paper, we formally define the task of process extraction and present automatic extraction methods. Our approach handles an op"
D13-1177,N13-1112,0,0.0818844,"Missing"
D13-1177,P08-2012,1,0.92408,"fier and the gold standard disagree. We are interested in triads that never occur in training data but are predicted by the classifier, and vice versa. Figure 2 illustrates some of the triads found and Equations 8-12 provide the corresponding ILP formulations. Equations 8-10 were formulated as soft constraints (expanding the set K) and were incorporated by defining a reward αk for each triad type.3 On the other hand, Equations 11-12 were formulated as hard constraints to prevent certain structures. 1. S AME transitivity (Figure 2a, Eqn. 8): Coreference transitivity has been used in past work (Finkel and Manning, 2008) and we incorporate it by a constraint that encourages triads that respect transitivity. 2. C AUSE-C OTEMP (Figure 2b, Eqn. 9): If ti causes both tj and tk , then often tj and tk are co-temporal. E.g, in “genetic drift has led to a loss of genetic variation and an increase in the frequency of . . .”, a single event causes two subsequent events that occur simultaneously. 3. C OTEMP transitivity (Figure 2c, Eqn. 10): If ti is co-temporal with tj and tj is co-temporal with tk , then usually ti and tk are either cotemporal or denote the same event. 4. S AME contradiction (Figure 2d, Eqn. 11): If t"
D13-1177,W11-1801,0,0.226726,"hich exploits these structural properties by performing joint inference over the set of extracted relations. On a novel dataset containing 148 descriptions of biological processes (released with this paper), we show significant improvement comparing to baselines that disregard process structure. 1 Process extraction is related to two recent lines of work in Information Extraction – event extraction and timeline construction. Traditional event extraction focuses on identifying a closed set of events within a single sentence. For example, the BioNLP 2009 and 2011 shared tasks (Kim et al., 2009; Kim et al., 2011) consider nine events types related to proteins. In practice, events are currently almost always extracted from a single sentence. Process extraction, on the other hand, is centered around discovering relations between events that span multiple sentences. The set of possible event types in process extraction is also much larger. Introduction A process is defined as a series of inter-related events that involve multiple entities and lead to an end result. Product manufacturing, economical developments, and various phenomena in life and social sciences can all be viewed as types of processes. Pr"
D13-1177,P03-1054,1,0.0154475,"gy” and marking any contiguous sequence of sentences that describes a process, i.e., a series of events that lead towards some objective. Then, each process description was annotated by a biologist. The annotator was first presented with annotation guidelines and annotated 20 descriptions. The annotations were then discussed with the authors, after which all process descriptions were annotated. After training a second biologist, we measured inter-annotator agreement κ = 0.69, on 30 random process descriptions. Process descriptions were parsed with Stanford constituency and dependency parsers (Klein and Manning, 2003; de Marneffe et al., 2006), and 35 process descriptions were set aside as a test set (number of training set trigger pairs: 1932, number of test set trigger pairs: 906). We performed 10fold cross validation over the training set for feature selection and tuning of constraint parameters. For each constraint type (connectivity, chain-structure, and five triad constraints) we introduced a parameter and tuned the seven parameters by coordinatewise ascent, where for hard constraints a binary parameter controls whether the constraint is used, and for soft constraints we attempted 10 different rewar"
D13-1177,P09-1039,0,0.019495,"for a relation r between the trigger pair (ti , tj ) (e.g., θijr = log pijr ), and yijr be the corresponding indicator variable. Our goal is to find an assignment for the indicators y = {yijr |1 ≤ i &lt; j ≤ n, r ∈ R}. With no global constraints this can be formulated as the following ILP: arg max y X θijr yijr (1) ijr s.t.∀i,j X yijr = 1 r where the constraint ensures exactly one relation between each event pair. We now describe constraints that result in a coherent global process structure. Connectivity Our ILP formulation for enforcing connectivity is a minor variation of the one suggested by Martins et al. (2009) for dependency parsing. In our setup, we want P to be a connected undirected graph, and not a directed tree. However, an undirected graph P is connected iff there exists a directed tree that is a subgraph of P when edge directions are ignored. Thus the resulting formulation is almost identical and is based on flow constraints which ensure that there is a path from a designated root in the graph to all other nodes. ¯ be the set R  N ONE. An edge (ti , tj ) is Let R in E iff there is some non-N P ONE relation between ti and tj , i.e. iff yij := ¯ yijr is equal to 1. r∈R For each variable yij w"
D13-1177,D12-1080,1,0.93945,"l developments, and various phenomena in life and social sciences can all be viewed as types of processes. Processes are complicated objects; consider for example the biological process of ATP synthesis described in Figure 1. This process involves 12 entities and 8 events. Additionally, it describes relations between events and entities, and the relationship between events (e.g., the second occurrence of the event ‘enter’, causes the event ‘changing’). ∗ Both authors equally contributed to the paper Timeline construction involves identifying temporal relations between events (Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and is thus related to process extraction as both focus on event-event relations spanning multiple sentences. However, events in processes are tightly coupled in ways that go beyond simple temporal ordering, and these dependencies are central for the process extraction task. Hence, capturing process structure requires modeling a larger set of relations that includes temporal, causal and co-reference relations. In this paper, we formally define the task of process extraction and present automatic extraction methods. Our approach handles an open set of event types and wo"
D13-1177,P11-1163,1,0.901643,"Missing"
D13-1177,N10-1123,0,0.0211054,"nalized for both the false negative of the correct class (just as before), and also for the false positive of the predicted class. Despite that we see that Global improves overall performance by 3.7 F1 points on the test set. 5 Related Work A related line of work is biomedical event extraction in recent BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011). Earlier work employed a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky ("
D13-1177,N12-1008,0,0.0193823,"the predictions of Global and Local are identical. Original text, Left: “... the template shifts . . . , and a part of the template strand is either skipped by the replication machinery or used twice as a template. As a result, a segment of DNA is deleted or duplicated.” Right: “Cells of mating type A secrete a signaling molecule, which can bind to specific receptor proteins on nearby cells. At the same time, cells secrete factor, which binds to receptors on A cells.” for modeling global constraints. Similar observations and techniques have been proposed in other information extraction tasks. Reichart and Barzilay (2012) tied information from multiple sequence models that describe the same event by using global higher-order potentials. Berant et al. (2011) proposed a global inference algorithm to identify entailment relations. There is an abundance of examples of enforcing global constraints in other NLP tasks, such as in coreference resolution (Finkel and Manning, 2008), parsing (Rush et al., 2012) and named entity recognition (Wang et al., 2013). 6 Conclusion Developing systems that understand process descriptions is an important step towards building applications that require deeper reasoning, such as biol"
D13-1177,D11-1001,0,0.0317982,"the false positive of the predicted class. Despite that we see that Global improves overall performance by 3.7 F1 points on the test set. 5 Related Work A related line of work is biomedical event extraction in recent BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011). Earlier work employed a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one anothe"
D13-1177,D12-1131,0,0.0299692,"t the same time, cells secrete factor, which binds to receptors on A cells.” for modeling global constraints. Similar observations and techniques have been proposed in other information extraction tasks. Reichart and Barzilay (2012) tied information from multiple sequence models that describe the same event by using global higher-order potentials. Berant et al. (2011) proposed a global inference algorithm to identify entailment relations. There is an abundance of examples of enforcing global constraints in other NLP tasks, such as in coreference resolution (Finkel and Manning, 2008), parsing (Rush et al., 2012) and named entity recognition (Wang et al., 2013). 6 Conclusion Developing systems that understand process descriptions is an important step towards building applications that require deeper reasoning, such as biological process models from text, intelligent tutoring systems, and non-factoid QA systems. In this paper we have presented the task of process extraction, and developed methods for extracting relations between process events. Processes contain events that are tightly coupled through strong dependencies. We have shown that exploiting these structural dependencies and performing joint"
D13-1177,J11-2003,0,0.0141453,"iversity, Stanford Justin Lewis and Brittany Harding University of Washington, Seattle Peter Clark Allen Institute for Artificial Intelligence, Seattle Abstract Automatically extracting the structure of processes from text is crucial for applications that require reasoning, such as non-factoid QA. For instance, answering a question on ATP synthesis, such as “How do H+ ions contribute to the production of ATP?” requires a structure that links H+ ions (Figure 1, sentence 1) to ATP (Figure 1, sentence 4) through a sequence of intermediate events. Such “How?” questions are common on FAQ websites (Surdeanu et al., 2011), which further supports the importance of process extraction. Biological processes are complex phenomena involving a series of events that are related to one another through various relationships. Systems that can understand and reason over biological processes would dramatically improve the performance of semantic applications involving inference such as question answering (QA) – specifically “How?” and “Why?” questions. In this paper, we present the task of process extraction, in which events within a process and the relations between the events are automatically extracted from text. We rep"
D13-1177,J08-2002,1,0.92479,"e gold standard disagree. We are interested in triads that never occur in training data but are predicted by the classifier, and vice versa. Figure 2 illustrates some of the triads found and Equations 8-12 provide the corresponding ILP formulations. Equations 8-10 were formulated as soft constraints (expanding the set K) and were incorporated by defining a reward αk for each triad type.3 On the other hand, Equations 11-12 were formulated as hard constraints to prevent certain structures. 1. S AME transitivity (Figure 2a, Eqn. 8): Coreference transitivity has been used in past work (Finkel and Manning, 2008) and we incorporate it by a constraint that encourages triads that respect transitivity. 2. C AUSE-C OTEMP (Figure 2b, Eqn. 9): If ti causes both tj and tk , then often tj and tk are co-temporal. E.g, in “genetic drift has led to a loss of genetic variation and an increase in the frequency of . . .”, a single event causes two subsequent events that occur simultaneously. 3. C OTEMP transitivity (Figure 2c, Eqn. 10): If ti is co-temporal with tj and tj is co-temporal with tk , then usually ti and tk are either cotemporal or denote the same event. 4. S AME contradiction (Figure 2d, Eqn. 11): If t"
D13-1177,P09-1046,0,0.281107,"E relations Avg 3.80 89.98 6.20 5.64 Min 1 19 2 1 Max 15 319 15 24 Table 1: Process statistics over 148 process descriptions. N ONE is used to indicate no relation. relation set R. We chose a relation set R that captures the essential aspects of temporal relations between events in a process, while keeping the annotation as simple as possible. For instance, we include the S UPER relation that appears in temporal annotations such as the Timebank corpus (Pustejovsky et al., 2003) and Allen’s work, but in practice was not considered by many temporal ordering systems (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012). Importantly, our relation set also includes the relations C AUSES and E NABLES, which are fundamental to modeling processes and go beyond simple temporal ordering. We also added event coreference (S AME) to R. Do et al. (2012) used event coreference information in a temporal ordering task to modify probabilities provided by pairwise classifiers prior to joint inference. In this paper, we simply treat S AME as another event-event relation, which allows us to easily perform joint inference and employ structural constraints that combine both coreference and temporal relations"
D13-1177,W09-1401,0,\N,Missing
D13-1177,J12-1003,1,\N,Missing
D14-1159,D13-1160,1,0.289643,"contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choice reading comprehensio"
D14-1159,D08-1073,0,0.0214459,"the set of regular expressions that are currently used by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCallum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al.,"
D14-1159,W06-0602,0,0.0205921,"urafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke e"
D14-1159,W10-2903,0,0.0252125,"., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choic"
D14-1159,clarke-etal-2012-nlp,1,0.779928,"Missing"
D14-1159,W02-1001,0,0.0492796,"ossible assignments to the inference variables are not feasible and there are both structural as well as prior knowledge constraints over the output space. Table 2 states the constraints we include, which are expressed as linear inequalities over output variables using standard techniques (e.g., (Roth and Yih, 2004)). 4.4 Learning in the Joint Model We train both the trigger classifier and the argument identifier using L2 -regularized logistic regression. For the joint model, we use a linear model for the scoring functions, and train jointly using the structured averaged perceptron algorithm (Collins, 2002). Since argument labeling is similar to semantic role labeling (SRL), we extract standard SRL features given the trigger and argument from the syntactic tree for the corresponding sentence. In addition, we add features extracted from an off-theshelf SRL system. We also include all feature conjunctions. For event relations, we include the features described in Scaria et al. (2013), as well as context features for both triggers, and the dependency path between them, if one exists. 5 Question Answering via Structures This section describes our question answering system that, given a process struc"
D14-1159,D12-1062,0,0.0400506,"d by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCallum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank a"
D14-1159,doddington-etal-2004-automatic,0,0.188353,"Missing"
D14-1159,D11-1142,0,0.015657,"in the paragraph. Then, we map the question to a formal query, which is executed against the predicted structure. We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations. 1 Introduction The goal of machine reading is to develop programs that read text to learn about the world and make decisions based on accumulated knowledge. Work in this field has focused mostly on macro-reading, i.e., processing large text collections and extracting knowledge bases of facts (Etzioni et al., 2006; Carlson et al., 2010; Fader et al., 2011). Such methods rely on redundancy, and are thus suitable for answering common factoid questions which have ample evidence in text (Fader et al., 2013). However, reading a single document (micro-reading) to answer comprehension questions that require deep reasoning is currently beyond the scope of state-of-the-art systems. In this paper, we introduce a task where given a paragraph describing a process, the goal is to ∗ Both authors equally contributed to the paper. answer reading comprehension questions that test understanding of the underlying structure. In particular, we consider processes in"
D14-1159,P13-1158,0,0.00703221,"ons via predicted structures substantially improves accuracy over baselines that use shallower representations. 1 Introduction The goal of machine reading is to develop programs that read text to learn about the world and make decisions based on accumulated knowledge. Work in this field has focused mostly on macro-reading, i.e., processing large text collections and extracting knowledge bases of facts (Etzioni et al., 2006; Carlson et al., 2010; Fader et al., 2011). Such methods rely on redundancy, and are thus suitable for answering common factoid questions which have ample evidence in text (Fader et al., 2013). However, reading a single document (micro-reading) to answer comprehension questions that require deep reasoning is currently beyond the scope of state-of-the-art systems. In this paper, we introduce a task where given a paragraph describing a process, the goal is to ∗ Both authors equally contributed to the paper. answer reading comprehension questions that test understanding of the underlying structure. In particular, we consider processes in biology textbooks such as this excerpt and the question that follows: “. . . Water is split, providing a source of electrons and protons (hydrogen io"
D14-1159,P99-1042,0,0.0251544,"ap natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choice reading comprehension tests are a natural choice for evaluating machine reading. Hirschman et al. (1999) presented a bag-ofwords approach to retrieving sentences for reading comprehension. Richardson et al. (2013) recently released the MCTest reading comprehension dataset that examines understanding of fictional stories. Their work shares our goal of advancing micro-reading, but they do not focus on process understanding. Developing programs that perform deep reasoning over complex descriptions of processes is an important step on the road to fulfilling the higher goals of machine reading. In this paper, we present an end-to-end system for reading comprehension of paragraphs which describe biolo"
D14-1159,W11-1801,0,0.0399034,"Missing"
D14-1159,Q13-1016,0,0.0240224,"e do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choice reading comprehension tests are a natural choice for evaluating mach"
D14-1159,P14-1026,0,0.0192904,"he text to the question. We propose a novel method that tackles this challenging problem (see Figure 1). First, we train a supervised structure predictor that learns to extract entities, events and their relations describing the biological process. This is a difficult problem because events have complex interactions that span multiple sentences. Then, treating this structure as a small knowledge-base, we map questions to formal queries that are executed against the structure to provide the answer. Micro-reading is an important aspect of natural language understanding (Richardson et al., 2013; Kushman et al., 2014). In this work, we focus specifically on modeling processes, where events and entities relate to one another through complex interactions. While we work in the biology “. . . Water is split, providing a source of electrons and protons (hydrogen ions, H+ ) and giving off O2 as a by-product. Light absorbed by chlorophyll drives a transfer of water T HEME Step 1 split absorb E NABLE C AUSE transfer the electrons and hydrogen ions from water T HEME T HEME light ions to an acceptor called NADP+ . . . ” Step 3: Answer = b Q What can the splitting of water lead to? a Light absorption b Transfer of io"
D14-1159,P14-5010,1,0.0110213,"Missing"
D14-1159,P09-1039,0,0.00814841,"iption Every argument candidate and trigger pair has exactly one label. Two arguments of the same trigger cannot overlap. The S AME relation is symmetric. All other relations are anti-symmetric, i.e., for any relation label other than S AME, at most one of (ti , tj ) or (tj , ti ) can take that label and the other is assigned the label NULL - REL. Every trigger can have no more than two arguments with the same label. The same span of text can not be an argument for more than two triggers. The triggers must form a connected graph, framed as flow constraints as in Magnanti and Wolsey (1995) and Martins et al. (2009). If the same span of text is an argument of two triggers, then the triggers must be connected by a relation that is not NULL - REL. This ensures that triggers that share arguments are related. For any trigger, at most one outgoing edge can be labeled S UPER. Table 2: Constraints for joint inference. Formulation Given the two sets of variables, the objective of inference is to find a global assignment that maximizes the score. That is, the objective can be stated as follows: max y,z X t,a∈At ,A bt,a,A · yt,a,A + X ct1 ,t2 ,R · zt1 ,t2 ,R t1 ,t2 ,R Here, y and z refer to all the argument and re"
D14-1159,D12-1080,1,0.842646,"m. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCallum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow"
D14-1159,N03-1022,0,0.0797767,"igure 1). Our strategy is to treat the process structure as a small knowledge-base. We map each answer along with the question into a structured query that we compare against the structure. The query can prove either the correctness or incorrectness of the answer being considered. That is, either we get a valid match for an answer (proving that the corresponding answer is correct), or we get a refutation in the form of a contradicted causal chain (thus proving that the other answer is correct). This is similar to theorem proving approaches suggested in the past for factoid question answering (Moldovan et al., 2003). The rest of this section is divided into three parts: Section 5.1 defines the queries we use, Section 5.2 describes a rule-based algorithm for converting a question and an answer into a query and finally, 5.3 describes the overall algorithm. 5.1 Queries over Processes We model a query as a directed graph path with regular expressions over edge labels. The bottom right portion of Figure 1 shows examples of queries for our running example. In general, given a question and one of the answer candidates, one end of the path is populated by a trigger/argument found in the question and the other is"
D14-1159,J05-1004,0,0.128022,"tudied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of superv"
D14-1159,J08-2005,0,0.144807,"Missing"
D14-1159,D13-1020,0,0.248644,"ong answer is closer in the text to the question. We propose a novel method that tackles this challenging problem (see Figure 1). First, we train a supervised structure predictor that learns to extract entities, events and their relations describing the biological process. This is a difficult problem because events have complex interactions that span multiple sentences. Then, treating this structure as a small knowledge-base, we map questions to formal queries that are executed against the structure to provide the answer. Micro-reading is an important aspect of natural language understanding (Richardson et al., 2013; Kushman et al., 2014). In this work, we focus specifically on modeling processes, where events and entities relate to one another through complex interactions. While we work in the biology “. . . Water is split, providing a source of electrons and protons (hydrogen ions, H+ ) and giving off O2 as a by-product. Light absorbed by chlorophyll drives a transfer of water T HEME Step 1 split absorb E NABLE C AUSE transfer the electrons and hydrogen ions from water T HEME T HEME light ions to an acceptor called NADP+ . . . ” Step 3: Answer = b Q What can the splitting of water lead to? a Light abso"
D14-1159,D11-1001,0,0.0772427,"Missing"
D14-1159,W04-2401,0,0.0455766,"es, the objective of inference is to find a global assignment that maximizes the score. That is, the objective can be stated as follows: max y,z X t,a∈At ,A bt,a,A · yt,a,A + X ct1 ,t2 ,R · zt1 ,t2 ,R t1 ,t2 ,R Here, y and z refer to all the argument and relation variables respectively. Clearly, all possible assignments to the inference variables are not feasible and there are both structural as well as prior knowledge constraints over the output space. Table 2 states the constraints we include, which are expressed as linear inequalities over output variables using standard techniques (e.g., (Roth and Yih, 2004)). 4.4 Learning in the Joint Model We train both the trigger classifier and the argument identifier using L2 -regularized logistic regression. For the joint model, we use a linear model for the scoring functions, and train jointly using the structured averaged perceptron algorithm (Collins, 2002). Since argument labeling is similar to semantic role labeling (SRL), we extract standard SRL features given the trigger and argument from the syntactic tree for the corresponding sentence. In addition, we add features extracted from an off-theshelf SRL system. We also include all feature conjunctions."
D14-1159,D13-1177,1,0.941392,"and the accompanying dataset. We will use the example in Figure 1 as our running example throughout the paper. Our goal is to tackle a complex reading comprehension setting that centers on understanding the underlying meaning of a process description. We target a multiple-choice setting in which each input consists of a paragraph of text describing a biological process, a question, and two possible answers. The goal is to identify the correct answer using the text (Figure 1, left). We used the 148 paragraphs from the textbook Biology (Campbell and Reece, 2005) that were manually identified by Scaria et al. (2013). We extended this set to 200 paragraphs by including additional paragraphs that describe biological processes. Each paragraph in the collection represents a single biological process and describes a set of events, their participants and their interactions. Because we target understanding of paragraph meaning, we use the following desiderata for building the corpus of questions and answers: 1. The questions should focus on the events and entities participating in the process described in the paragraph, and answering the questions should require reasoning about the relations between those event"
D14-1159,N06-1056,0,0.0533243,"and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future res"
D14-1159,P09-1046,0,0.0164746,"s that are currently used by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCallum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contr"
D14-1159,W09-1401,0,\N,Missing
D14-1159,E12-2021,0,\N,Missing
D15-1080,P14-1114,0,0.0310312,"Missing"
D15-1080,D09-1001,0,0.0215396,"Missing"
D15-1080,S13-1002,0,\N,Missing
D15-1080,P07-2009,0,\N,Missing
D15-1236,P88-1012,0,0.418413,"n et al., 2004). These elaborations reflect the mental process of “filling in the gaps”, and multiple choice questions can then be answered by finding which answer option creates the most coherent scene. The notion of NLU as constructing a most coherent scene is not new, and has has been studied in several contexts including work on scripts (Schank and Abelson, 1977), interpretation as ab2007 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2007–2012, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. duction (Hobbs et al., 1988; Hobbs, 1979; Ovchinnikova et al., 2014), bridging anaphora (Asher and Lascarides, 1998; Fan et al., 2005), and paragraph understanding (Zadrozny and Jensen, 1991; Harabagiu and Moldovan, 1997). These methods are inspiring, but have previously been limited by the lack of background knowledge to supply implicit information, and with the complexity of their representations. To make progress, we have chosen to work with a simple “knowledge graph” representation of nodes (words) and edges (relations). Although we lose some subtlety of expression, we gain the ability to leverage several vast resou"
D15-1236,P14-1092,1,0.856686,"Missing"
D15-1236,P14-1090,0,0.0819356,"Missing"
D15-1236,J91-2003,0,0.534068,"answer option creates the most coherent scene. The notion of NLU as constructing a most coherent scene is not new, and has has been studied in several contexts including work on scripts (Schank and Abelson, 1977), interpretation as ab2007 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2007–2012, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. duction (Hobbs et al., 1988; Hobbs, 1979; Ovchinnikova et al., 2014), bridging anaphora (Asher and Lascarides, 1998; Fan et al., 2005), and paragraph understanding (Zadrozny and Jensen, 1991; Harabagiu and Moldovan, 1997). These methods are inspiring, but have previously been limited by the lack of background knowledge to supply implicit information, and with the complexity of their representations. To make progress, we have chosen to work with a simple “knowledge graph” representation of nodes (words) and edges (relations). Although we lose some subtlety of expression, we gain the ability to leverage several vast resources of world knowledge to supply implicit information. The significance of this work is thus to show that, by working with a simple “knowledge graph” representati"
D15-1236,W11-0124,0,\N,Missing
D16-1014,D14-1067,0,0.060924,"Missing"
D16-1014,J93-2003,0,0.0544067,"Missing"
D16-1014,D14-1082,0,0.017627,"Missing"
D16-1014,D15-1112,0,0.0413018,"Missing"
D16-1014,Q15-1015,1,0.838997,"Missing"
D16-1014,C92-2082,0,0.290538,"Missing"
D16-1014,W09-2415,0,0.0627835,"Missing"
D16-1014,P15-1162,0,0.0490213,"Missing"
D16-1014,P14-1092,1,0.907147,"Missing"
D16-1014,D15-1242,0,0.0608293,"Missing"
D16-1014,P14-2050,0,0.106352,"Missing"
D16-1014,N15-1098,0,0.0556435,"Missing"
D16-1014,P14-5010,1,0.0560812,"Missing"
D16-1014,W12-3018,0,0.0430808,"Missing"
D16-1014,J03-1002,0,0.0131617,"Missing"
D16-1014,P13-1170,0,0.22825,"Missing"
D16-1014,N13-1008,0,0.0471301,"Missing"
D16-1014,P07-1059,0,0.0875959,"Missing"
D16-1014,J11-2003,1,0.90094,"Missing"
D16-1014,L16-1050,1,0.798163,"Missing"
D16-1014,D15-1295,0,0.0532995,"Missing"
D16-1014,D14-1071,0,0.0491014,"Missing"
D16-1014,D13-1056,1,0.904776,"Missing"
D16-1014,P13-1171,0,0.0333684,"Missing"
D16-1014,P03-1003,0,\N,Missing
D16-1014,W16-2506,0,\N,Missing
D16-1151,P98-1013,0,0.163459,"This setting presents a unique opportunity, where the goal is to perform semantic role labeling on a set of closely related sentences, sentences that describe the same process. This allows us to design a joint inference method that can promote expectations of consistency amongst the extracted role fillers. There is no large scale training data that can be readily used for this task. Because we target process-specific and not verb-specific semantic roles, existing ProbBank (Kingsbury and Palmer, 2003) trained SRL systems cannot be used directly. Frame-semantic parsers trained on FrameNet data (Baker et al., 1998) are also not directly usable because FrameNet lacks coverage for many of the processes discussed in the science domain. Therefore, we create a process dataset that covers a relatively small number of processes, but demonstrate that the role extraction generalizes to previously unseen processes as well. 3.1 Cross-Sentence Inference Given a set of sentences about a process, we want to extract role fillers that are globally consistent i.e., we want role assignments that are compatible. Our approach is based on two observations: First, any given role is likely to have similar fillers for a partic"
D16-1151,D14-1159,1,0.895585,"Missing"
D16-1151,W09-1206,0,0.045565,"Missing"
D16-1151,N10-1138,0,0.0259923,"n 3.3, the within sentence and the cross sentence inference models. We tune the ILP parameter λ for cross sentence inference based on a coarsegrained sweep on the training folds. We also compared with a simple baseline that learned a mapping from PropBank roles produced by EasySRL system to the process roles by using the roles and the verb as features. We also add the FrameNet frames invoked by the lexical unit in the sentence. Note this is essentially a subset of the features we use in our role classifier. As a second baseline, we compare with a (nearly) out-of-thebox application of SEMAFOR (Das et al., 2010), a FrameNet based frame-semantic parser. We modified SEMAFOR to override the frame identification step since the process frame information is already associated with the test sentences. 0.9 Role Classifier + within sent. + cross sent. 0.8 0.7 0.0 0.2 0.4 0.6 0.8 1.0 Recall Figure 3: Precision/Recall trade-offs for process role inference. y-axis is truncated at 0.7 to better visualize the differences. 4.3 †† indicates significant improvement over Role + within sentence system. inference, shown as (+within sent.), improves over the baseline which does not use any consistency. It gains precision"
D16-1151,D09-1002,0,0.0683162,"Missing"
D16-1151,J12-1005,0,0.0510265,"Missing"
D16-1151,P16-1045,0,0.0435972,"Missing"
D16-1151,C12-1161,0,0.0259043,"Missing"
D16-1151,P15-2036,0,0.107014,"and semi-supervised approaches have been proposed to address these issues for PropBank style predicate-specific roles (Swier and Stevenson, 2004; Lang and Lapata, 2011; F¨urstenau and Lapata, 2009; F¨urstenau and Lapata, 2012; Lang and Lapata, 2010; Klementiev, 2012). A key idea here is to cluster syntactic signatures of the arguments and use the discovered clusters as roles. Another line of research has sought to perform joint training for syntactic parsing and semantic role labeling (Lewis et al., 2015), and in using PropBank role labels to improve FrameNet processing using pivot features (Kshirsagar et al., 2015). Some SRL methods account for context information from multiple sentences (Ruppenhofer et al., 2010; Roth and Lapata, 2015). They focus on anProcess evaporation weathering photosynthesis Undergoer liquid water rock solid material carbon dioxide CO2 Enabler heat heat energy weather heating solar energy light energy Action changes convert disintegration breaking down convert transforms Result gas water vapor smaller rocks smaller particles energy food Table 2: Examples of Target Knowledge Roles notating individual event mentions in a document using discourse-level evidence such as co-reference"
D16-1151,N10-1137,0,0.0386629,"Missing"
D16-1151,D11-1122,0,0.047175,"Missing"
D16-1151,D15-1169,0,0.0921854,"n instance of the process evaporation using a macrolevel role knowledge: Among others, the typical undergoer of evaporation is a kind of liquid (the puddle), and the enabler is usually a heat source (the sun). Our goal is to acquire this kind of role-based knowledge about processes from sentence-level descriptions in grade level texts. Semantic role labeling (SRL) systems can be trained to identify these process specific roles. However, these were developed for sentence-level interpretation and only ensure within sentence consistency of labels (Punyakanok et al., 2004; Toutanova et al., 2005; Lewis et al., 2015), limiting their ability to generate coherent characterizations of the process overall. In particular, the same process participant may appear in text at different syntactic positions, with different wording, and with different verbs, which makes it hard to extract globally consistent descriptions. In this work, we propose a cross sentence inference method to address this problem. To illustrate the challenge consider some example sentences on evaporation shown in Table 1.The underlined spans correspond to fillers for an undergoer role i.e., the main entity that is undergoing evaporation. Howev"
D16-1151,P14-5010,0,0.00408807,"e more than one argument with the same label, except for the NONE role. We use an off-the-shelf solver in Gurobi (www.gurobi.com) to find an approximate solution to the resulting optimization problem. standard SRL features such as lexical and syntactic contexts of arguments (e.g., head word, its POS tag) and predicate-argument path features (e.g, dependency paths). We also add features that are specific to the nature of the process sentences. In particular, we encode syntactic relationships of arguments with respect to the process name mention in the sentence. We use Stanford CoreNLP toolkit (Manning et al., 2014) to obtain POS tags, and dependency parses to build these features. ii) PropBank roles – While they do not have a 1to-1 correspondence with process roles, we use the EasySRL roles coupled with the specific predicate as a feature to provide useful evidence towards the process role. iii) Framenet Frames – We use the frames evoked by the words in the sentence to allow better feature sharing among related processes. For instance, the contexts of undergoers in evaporation and condensation are likely to be similar as they are both state changes which evoke the same Undergo Change frame in FrameNet."
D16-1151,W08-1810,0,0.045955,"Missing"
D16-1151,C04-1197,0,0.216566,"e drying in the sun”, one can recognize this as an instance of the process evaporation using a macrolevel role knowledge: Among others, the typical undergoer of evaporation is a kind of liquid (the puddle), and the enabler is usually a heat source (the sun). Our goal is to acquire this kind of role-based knowledge about processes from sentence-level descriptions in grade level texts. Semantic role labeling (SRL) systems can be trained to identify these process specific roles. However, these were developed for sentence-level interpretation and only ensure within sentence consistency of labels (Punyakanok et al., 2004; Toutanova et al., 2005; Lewis et al., 2015), limiting their ability to generate coherent characterizations of the process overall. In particular, the same process participant may appear in text at different syntactic positions, with different wording, and with different verbs, which makes it hard to extract globally consistent descriptions. In this work, we propose a cross sentence inference method to address this problem. To illustrate the challenge consider some example sentences on evaporation shown in Table 1.The underlined spans correspond to fillers for an undergoer role i.e., the main"
D16-1151,Q15-1032,0,0.0545562,"Missing"
D16-1151,S10-1008,0,0.0615206,"Missing"
D16-1151,D07-1002,0,0.0973936,"Missing"
D16-1151,P05-1073,0,0.0818344,"Missing"
D16-1151,C98-1013,0,\N,Missing
D18-1006,D14-1159,1,0.83622,"entific processes (Dalvi et al., 2018; Clark et al., 2018), tracking ingredients in cooking recipes (Bosselut et al., 2018), and tracking the emotional reactions and motivations of characters in simple stories (Rashkin et al., 2018). Our work extends these methods and addresses their most common issues by using background knowledge about entities to prune the set of state changes they can experience as the model reads new text. Prior to these neural approaches, some earlier systems for process comprehension did make use of world knowledge, and motivated this work. Like us, the system ProRead (Berant et al., 2014; Scaria et al., 2013) also treated process comprehension as structure prediction, using an Integer Linear Programming (ILP) formalism to enforce global constraints (e.g., if the result of event1 is the agent of event2, then event1 must enable event2). Similarly, Kiddon et al. (2015) used corpus-based priors to guide extraction of an “action graph” from recipes. 3 Problem Definition We first define the general task that we are addressing, before presenting our approach. 3.1 General Formulation We define the task as follows. Given: • A paragraph of procedural text S = an ordered set of sentence"
D18-1006,N18-1144,1,0.679082,"locations at each sentence, but the implied movements violate commonsense constraints (e.g., an object cannot move from itself (1)) and corpus-based preferences (e.g., it is rare to see turbines move (2)). 2018), and NPN (Bosselut et al., 2018) – has focused on learning to predict individual entity states at various points in the text, thereby approximating the underlying dynamics of the world. However, while these models can learn to make local predictions with fair accuracy, their results are often globally unlikely or inconsistent. For example, in Figure 1, the neural ProGlobal model from Dalvi et al. (2018) learns to predict the impossible action of an object moving from itself (1), and the unlikely action of a turbine changing location (2). We observe similar mistakes in other neural models, indicating that these models have little notion of global consistency. Unsurprisingly, mistakes in local predictions compound as the process becomes longer, further reducing the plausibility of the overall result. Introduction Procedural text is ubiquitous (e.g., scientific protocols, news articles, how-to guides, recipes), but is challenging to comprehend because of the dynamic nature of the world being de"
D18-1006,T87-1035,0,0.755647,"Missing"
D18-1006,P16-5005,0,0.0195,"te structures using hard constraints and soft KB priors. Previous work in Neural Machine Translation (NMT) has used sets of example-specific lexical constraints in beam search decoding to only produce translations that satisfy every constraint in the set (Hokamp and Liu, 2017). In contrast, our work uses a set of global example-free constraints to prune the set of possible paths the search algorithm can explore. Simultaneously, a recent body of work has explored encoding soft constraints as an additional loss term in the training objective for dialogue (Wen et al., 2015), machine translation (Tu et al., 2016), and recipe generation (Kiddon et al., 2016). Our work instead uses soft constraints to re-rank candidate structures and is not directly encoded in the loss function. Related Work Our work builds off a recent body of work that focuses on using neural networks to explicitly track the states of entities while reading long texts. These works have focused on answering simple commonsense questions (Henaff et al., 2017), tracking entity states in scientific processes (Dalvi et al., 2018; Clark et al., 2018), tracking ingredients in cooking recipes (Bosselut et al., 2018), and tracking the emotional"
D18-1006,P17-1141,0,0.0210284,"until an answer can be provided. Although this context is somewhat different (the primary goal being state discovery from weak dialog evidence), state tracking techniques originally designed for procedural text have been successfully applied in this context also (Liu and Perez, 2017). Finally, our model learns to search over the best candidate structures using hard constraints and soft KB priors. Previous work in Neural Machine Translation (NMT) has used sets of example-specific lexical constraints in beam search decoding to only produce translations that satisfy every constraint in the set (Hokamp and Liu, 2017). In contrast, our work uses a set of global example-free constraints to prune the set of possible paths the search algorithm can explore. Simultaneously, a recent body of work has explored encoding soft constraints as an additional loss term in the training objective for dialogue (Wen et al., 2015), machine translation (Tu et al., 2016), and recipe generation (Kiddon et al., 2016). Our work instead uses soft constraints to re-rank candidate structures and is not directly encoded in the loss function. Related Work Our work builds off a recent body of work that focuses on using neural networks"
D18-1006,D15-1199,0,0.0191058,"l learns to search over the best candidate structures using hard constraints and soft KB priors. Previous work in Neural Machine Translation (NMT) has used sets of example-specific lexical constraints in beam search decoding to only produce translations that satisfy every constraint in the set (Hokamp and Liu, 2017). In contrast, our work uses a set of global example-free constraints to prune the set of possible paths the search algorithm can explore. Simultaneously, a recent body of work has explored encoding soft constraints as an additional loss term in the training objective for dialogue (Wen et al., 2015), machine translation (Tu et al., 2016), and recipe generation (Kiddon et al., 2016). Our work instead uses soft constraints to re-rank candidate structures and is not directly encoded in the loss function. Related Work Our work builds off a recent body of work that focuses on using neural networks to explicitly track the states of entities while reading long texts. These works have focused on answering simple commonsense questions (Henaff et al., 2017), tracking entity states in scientific processes (Dalvi et al., 2018; Clark et al., 2018), tracking ingredients in cooking recipes (Bosselut et"
D18-1006,D15-1114,0,0.0926547,"r most common issues by using background knowledge about entities to prune the set of state changes they can experience as the model reads new text. Prior to these neural approaches, some earlier systems for process comprehension did make use of world knowledge, and motivated this work. Like us, the system ProRead (Berant et al., 2014; Scaria et al., 2013) also treated process comprehension as structure prediction, using an Integer Linear Programming (ILP) formalism to enforce global constraints (e.g., if the result of event1 is the agent of event2, then event1 must enable event2). Similarly, Kiddon et al. (2015) used corpus-based priors to guide extraction of an “action graph” from recipes. 3 Problem Definition We first define the general task that we are addressing, before presenting our approach. 3.1 General Formulation We define the task as follows. Given: • A paragraph of procedural text S = an ordered set of sentences {s1 , ..., sT } describing 58 dataset. Figure 2 gives a (simplified) example of the data, visualized as an (entity x sentence) grid, where each column tracks a different entity (time progressing vertically downwards), and each row denotes the entities’ state (existence and location"
D18-1006,D16-1032,0,0.0476308,"soft KB priors. Previous work in Neural Machine Translation (NMT) has used sets of example-specific lexical constraints in beam search decoding to only produce translations that satisfy every constraint in the set (Hokamp and Liu, 2017). In contrast, our work uses a set of global example-free constraints to prune the set of possible paths the search algorithm can explore. Simultaneously, a recent body of work has explored encoding soft constraints as an additional loss term in the training objective for dialogue (Wen et al., 2015), machine translation (Tu et al., 2016), and recipe generation (Kiddon et al., 2016). Our work instead uses soft constraints to re-rank candidate structures and is not directly encoded in the loss function. Related Work Our work builds off a recent body of work that focuses on using neural networks to explicitly track the states of entities while reading long texts. These works have focused on answering simple commonsense questions (Henaff et al., 2017), tracking entity states in scientific processes (Dalvi et al., 2018; Clark et al., 2018), tracking ingredients in cooking recipes (Bosselut et al., 2018), and tracking the emotional reactions and motivations of characters in s"
D18-1006,D16-1137,0,0.027991,"nd nonsensical predictions. Second, we present a novel, end-to-end model that integrates these constraints and achieves state-of-the-art performance on an existing process comprehension dataset (Dalvi et al., 2018). 2 Our work here can viewed as incorporating these approaches within the neural paradigm. Neural methods for structure prediction have been used extensively in other areas of NLP, and we leverage these methods here. In particular we use a neural encoder-decoder architecture with beam search decoding, representative of several current state-of-the-art systems (Bahdanau et al., 2014; Wiseman and Rush, 2016; Vinyals et al., 2015). As our model’s only supervision signal comes from the final prediction (of state changes), our work is similar to previous work in semantic parsing that extracts structured outputs from text with no intermediate supervision (Krishnamurthy et al., 2017). State tracking also appears in other areas of AI, such as dialog. A typical dialog state tracking task (e.g., the DSTC competitions) involves gradually uncovering the user’s state (e.g., their constraints, preferences, and goals for booking a restaurant), until an answer can be provided. Although this context is somewha"
D18-1006,D17-1160,0,0.131181,"ches within the neural paradigm. Neural methods for structure prediction have been used extensively in other areas of NLP, and we leverage these methods here. In particular we use a neural encoder-decoder architecture with beam search decoding, representative of several current state-of-the-art systems (Bahdanau et al., 2014; Wiseman and Rush, 2016; Vinyals et al., 2015). As our model’s only supervision signal comes from the final prediction (of state changes), our work is similar to previous work in semantic parsing that extracts structured outputs from text with no intermediate supervision (Krishnamurthy et al., 2017). State tracking also appears in other areas of AI, such as dialog. A typical dialog state tracking task (e.g., the DSTC competitions) involves gradually uncovering the user’s state (e.g., their constraints, preferences, and goals for booking a restaurant), until an answer can be provided. Although this context is somewhat different (the primary goal being state discovery from weak dialog evidence), state tracking techniques originally designed for procedural text have been successfully applied in this context also (Liu and Perez, 2017). Finally, our model learns to search over the best candid"
D18-1006,P17-1041,0,0.0280129,"te change is predicted, then the model also must predict its parameter values from the paragraph. Previous models for process comprehension make a sequence of local predictions about the entities’ states, one sentence at a time, maintaining a (typically neural) state at each sentence. However, none have the ability to reverse earlier predictions should an inconsistency arise later in the sequence. ProStruct overcomes this limitation by reformulating the task as structured prediction. To do this, it uses a neural encoder-decoder from the semantic parsing literature (Krishnamurthy et al., 2017; Yin and Neubig, 2017) combined with a search procedure that integrates soft and hard constraints for finding the best candidate structure. For each sentence and entity, the encoder first uses a bidirectional LSTM to encode the sentence and indicator variables identifying which entity is currently being considered (Figure 3). It then produces a (distributed) representation of the action that the sentence describes as being applied to that entity. During decoding, the model decodes each action embedding into a distribution over possible state changes that might result, then performs actions1 a sequence of about a gi"
D18-1006,E17-1029,0,0.0265098,"utputs from text with no intermediate supervision (Krishnamurthy et al., 2017). State tracking also appears in other areas of AI, such as dialog. A typical dialog state tracking task (e.g., the DSTC competitions) involves gradually uncovering the user’s state (e.g., their constraints, preferences, and goals for booking a restaurant), until an answer can be provided. Although this context is somewhat different (the primary goal being state discovery from weak dialog evidence), state tracking techniques originally designed for procedural text have been successfully applied in this context also (Liu and Perez, 2017). Finally, our model learns to search over the best candidate structures using hard constraints and soft KB priors. Previous work in Neural Machine Translation (NMT) has used sets of example-specific lexical constraints in beam search decoding to only produce translations that satisfy every constraint in the set (Hokamp and Liu, 2017). In contrast, our work uses a set of global example-free constraints to prune the set of possible paths the search algorithm can explore. Simultaneously, a recent body of work has explored encoding soft constraints as an additional loss term in the training objec"
D18-1006,D14-1162,0,0.0924963,"ly destroy one entity and create another. Figure 3 shows the encoder operating on s4 : “The generator spins, and produces electricity” and e3 : electricity from Figure 1. Without loss of generality, we define an arbitrary sentence in S as st = {w0 , ..., wI }. Each word wi in the input sentence is encoded as a vector xi = [vw : ve : vv ], which is the concatenation of a pre-trained word embedding vw for wi , an indicator variable ve for whether wi is a reference to the specified entity e j , and an indicator variable vv for whether wi is a verb. We use GloVe vectors as pre-trained embeddings (Pennington et al., 2014) and a POS tagger to extract verbs (Spacy, 2018). Then, a BiLSTM is used to encode the word representations extracted above, yielding a contextualized vector hi for each embedded word xi that is the concatenated output of the backward and forward hidden states produced by the BiLSTM for word wi . An attention over the contextualized embeddings hi is performed to predict a distribution of weights over the sentence: Figure 3: The encoder, illustrated for the ProPara domain with the paragraph from Figure 1. During encoding, ProStruct creates an action embedding ct j representing the action at ste"
D18-1006,P18-1213,1,0.813229,"ead uses soft constraints to re-rank candidate structures and is not directly encoded in the loss function. Related Work Our work builds off a recent body of work that focuses on using neural networks to explicitly track the states of entities while reading long texts. These works have focused on answering simple commonsense questions (Henaff et al., 2017), tracking entity states in scientific processes (Dalvi et al., 2018; Clark et al., 2018), tracking ingredients in cooking recipes (Bosselut et al., 2018), and tracking the emotional reactions and motivations of characters in simple stories (Rashkin et al., 2018). Our work extends these methods and addresses their most common issues by using background knowledge about entities to prune the set of state changes they can experience as the model reads new text. Prior to these neural approaches, some earlier systems for process comprehension did make use of world knowledge, and motivated this work. Like us, the system ProRead (Berant et al., 2014; Scaria et al., 2013) also treated process comprehension as structure prediction, using an Integer Linear Programming (ILP) formalism to enforce global constraints (e.g., if the result of event1 is the agent of e"
D18-1006,D13-1177,1,0.77852,"lvi et al., 2018; Clark et al., 2018), tracking ingredients in cooking recipes (Bosselut et al., 2018), and tracking the emotional reactions and motivations of characters in simple stories (Rashkin et al., 2018). Our work extends these methods and addresses their most common issues by using background knowledge about entities to prune the set of state changes they can experience as the model reads new text. Prior to these neural approaches, some earlier systems for process comprehension did make use of world knowledge, and motivated this work. Like us, the system ProRead (Berant et al., 2014; Scaria et al., 2013) also treated process comprehension as structure prediction, using an Integer Linear Programming (ILP) formalism to enforce global constraints (e.g., if the result of event1 is the agent of event2, then event1 must enable event2). Similarly, Kiddon et al. (2015) used corpus-based priors to guide extraction of an “action graph” from recipes. 3 Problem Definition We first define the general task that we are addressing, before presenting our approach. 3.1 General Formulation We define the task as follows. Given: • A paragraph of procedural text S = an ordered set of sentences {s1 , ..., sT } desc"
D18-1260,P17-1171,0,0.184145,"below. Reading Comprehension (RC) datasets have been proposed as benchmarks to evaluate the ability of systems to understand a document by answering factoid-style questions over this document. These datasets have taken various forms: multiple-choice (Richardson et al., 2013), clozestyle (Hermann et al., 2015; Onishi et al., 2016; Hill et al., 2016), and span prediction (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017) However, analysis (Chen et al., 2016; Sugawara et al., 2017) of these datasets has shown that many of the questions can be solved with context token matching (Chen et al., 2017a; Weissenborn et al., 2017) or relatively simple paraphrasing. To focus on the more challenging problem of reasoning across sentences, new datasets have been proposed for multi-step RC. QAngaroo (Welbl et al., 2018) have used a knowledgebase to identify entity pairs (s, o) with a known relation, r, which is also supported by a multihop path in a set of documents. They use structured tuple queries (s, r, ?) and use all the documents along the path as the input passage. NarrativeQA (Kocisk´y et al., 2017) is an RC dataset that has been shown to require an iterative reasoning about the narrative"
D18-1260,P17-1152,0,0.183292,"below. Reading Comprehension (RC) datasets have been proposed as benchmarks to evaluate the ability of systems to understand a document by answering factoid-style questions over this document. These datasets have taken various forms: multiple-choice (Richardson et al., 2013), clozestyle (Hermann et al., 2015; Onishi et al., 2016; Hill et al., 2016), and span prediction (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017) However, analysis (Chen et al., 2016; Sugawara et al., 2017) of these datasets has shown that many of the questions can be solved with context token matching (Chen et al., 2017a; Weissenborn et al., 2017) or relatively simple paraphrasing. To focus on the more challenging problem of reasoning across sentences, new datasets have been proposed for multi-step RC. QAngaroo (Welbl et al., 2018) have used a knowledgebase to identify entity pairs (s, o) with a known relation, r, which is also supported by a multihop path in a set of documents. They use structured tuple queries (s, r, ?) and use all the documents along the path as the input passage. NarrativeQA (Kocisk´y et al., 2017) is an RC dataset that has been shown to require an iterative reasoning about the narrative"
D18-1260,D17-1070,0,0.0146208,"y to be the correct answer to a science question without negation (which is the case for OpenBookQA). We implement a plausible answer detector using a choice-only model for predicting the answer by obtaining a score αci as: αci = WcT rcctx ∈ R1 , where WcT ∈ R2h is a i weights vector optimized during training, i = {1..4} is the index of the choice. To obtain the answer choice from the set of choice scores αc1..4 using arg max(softmax(αc1..4 )), exp(αci ) where softmax(αci ) = P4 exp(α as usual. ) j=1 cj 5d+4 BiLSTM Max-Out Baselines. As a simple neural baseline, we adapt BiLSTM max-out model (Conneau et al., 2017) to our QA task. That is, we first encode the question tokens and choice s tokens w1...n , independently with a bi-directional s context encoder (LSTM) to obtain a context (ctx) s representation hctx s1...ns = BiLSTM(e1...ns ) ∈ Rns ×2h Next, we perform an element-wise aggregation operation max on the encoded representations hctx s1..ns to construct a single vector: 2h rsctx = max(hctx s1..ns ) ∈ R . (1) Given the contextual representations for each token sequence, we experiment with three configurations for using these representations for QA: (b) Odd-One-Out Solver. It considers all 4 answer"
D18-1260,N18-2017,0,0.0595382,"Missing"
D18-1260,C16-1278,1,0.891345,"Missing"
D18-1260,L18-1433,0,0.191692,"urn, often requires combining a fact in the book (e.g., metals conduct electricity) with additional common knowledge the test taker is expected to have acquired by this stage (e.g., a suit of armor is made of metal). Motivated by this setting, we present a new kind of question answering dataset, OpenBookQA,1 that consists of two parts: Q, a set of 5957 multiple-choice questions, and F, a set of 1326 diverse facts about elementary level science. F has three key characteristics of an ‘open book’: (a) it forms the basis for generating Q; (b) it has been deemed central to scientific explanations (Jansen et al., 2018); and (c) by itself, F is generally insufficient to answer questions in Q. Faced with a question q ∈ Q, a student or system S is expected retrieve a relevant fact f ∈ F, and appeal to their own common knowledge, KS , when applying f to answer q. Figure 1 provides an example. Here, metals are thermal conductors is a core scientific fact available in F. One way to apply this fact to decide whether a steel spoon would let the most heat travel through is to appeal to common knowledge that steel is metallic and heat travels through thermal conductors. In general, the expected common knowledge is re"
D18-1260,P17-1147,0,0.151791,"Missing"
D18-1260,N18-1023,0,0.149036,"garoo (Welbl et al., 2018) have used a knowledgebase to identify entity pairs (s, o) with a known relation, r, which is also supported by a multihop path in a set of documents. They use structured tuple queries (s, r, ?) and use all the documents along the path as the input passage. NarrativeQA (Kocisk´y et al., 2017) is an RC dataset that has been shown to require an iterative reasoning about the narrative of a story. Similar to OpenBookQA, the questions were generated to ensure that the answer is not a direct match or paraphrase 2382 that can be retrieved with an IR approach. Most recently, Khashabi et al. (2018) proposed MultiRC, a multiple-choice RC dataset that is designed to require multi-sentence reasoning and can have multiple correct answers. Again, like most RC datasets, it is self-contained. Tasks with external knowledge. While many of the RC datasets could benefit from commonsense or background knowledge, they are designed to be self-contained, i.e., solvable by the document context alone. Datasets such as the Story Cloze Test (Mostafazadeh et al., 2016), MCScript,4 and ProPara (Mishra et al., 2018) do require additional domain knowledge about everyday events, scripts, and processes, respect"
D18-1260,P17-2049,1,0.865227,"Missing"
D18-1260,K16-2014,1,0.739267,"⇒ ground(rhs)] Table 3: Example training questions (with their correct choices marked) along with the facts and reasoning needed. In the last example, the science fact states that lhs=“source of light becomes closer” implies rhs=“source will appear brighter”. Grounding this rule based on the common-knowledge fact, produces a new rule: “As headlights of the car come closer, headlights will appear brighter” {1, 2, 3, 4}, where the true label is the correct answer index. Embeddings + Similarities as Features. We first experiment with a simple logistic regression model (Mihaylov and Nakov, 2016; Mihaylov and Frank, 2016, 2017) that uses centroid vectors rsemb of the word embeddings of tokens in s, and then computes the cosine similarities between the quescos : tion and each answer choice, rq,c i rsemb ns 1 X e sj ∈ R d = ns j=1 cos rq,c i = cos(rqemb , rcemb ) ∈ R1 i For each training instance, we build a feature representations f~ by concatenating these vectors and train an L2 logistic regression classifier: f~ = cos [rqemb ; rcemb ; rq,c ] 1..4 1..4 ∈R (a) Plausible Answer Detector. This baseline goes to the extreme of completely ignoring q and trying to learn how plausible it is for ci to be the correct a"
D18-1260,W17-0913,1,0.855037,"Missing"
D18-1260,P18-1076,1,0.827816,"the index ac2cr = arg min(softmax(αc1..4 2cr )). (c) Question Match. This solver tries to predict which choice best matches the question (Nakov et al., 2016), without relying on external knowledge. To achieve that, we compute an attention score αq,ci between q and each of the choices qi as αq,ci = Att(rqctx , rcctx ), and select the one with the i highest score. We also experiment with a model where rqctx and rcctx are obtained using token-wise i interaction proposed in ESIM (Chen et al., 2017b). 4.4 Knowledge-Enhanced Reader. As a base knowledge-aware model, we use a variant of the model of Mihaylov and Frank (2018), implemented by extending our BiLSTM max-out question-match baseline (c). For each instance the model reads the question q and answers c1..4 independently and attends to the set of retrieved external knowledge facts KQ,C . We encode each fact kj from KQ,C = k1..Nk (Nk is the number of facts) with same BiLSTM as used for q and c1..4 and construct a single vector rkctx ∈ R2h using Eq. 1. Having such j representations for each kj results in knowledge memory matrix Mk = rkctx ∈ RNk ×2h . Note 1..Nk that Mk is dynamic memory, specific for each instance in the batch and is encoded in each step duri"
D18-1260,S16-1136,1,0.756819,"[lhs ⇒ rhs] ⇒ [ground(lhs) ⇒ ground(rhs)] Table 3: Example training questions (with their correct choices marked) along with the facts and reasoning needed. In the last example, the science fact states that lhs=“source of light becomes closer” implies rhs=“source will appear brighter”. Grounding this rule based on the common-knowledge fact, produces a new rule: “As headlights of the car come closer, headlights will appear brighter” {1, 2, 3, 4}, where the true label is the correct answer index. Embeddings + Similarities as Features. We first experiment with a simple logistic regression model (Mihaylov and Nakov, 2016; Mihaylov and Frank, 2016, 2017) that uses centroid vectors rsemb of the word embeddings of tokens in s, and then computes the cosine similarities between the quescos : tion and each answer choice, rq,c i rsemb ns 1 X e sj ∈ R d = ns j=1 cos rq,c i = cos(rqemb , rcemb ) ∈ R1 i For each training instance, we build a feature representations f~ by concatenating these vectors and train an L2 logistic regression classifier: f~ = cos [rqemb ; rcemb ; rq,c ] 1..4 1..4 ∈R (a) Plausible Answer Detector. This baseline goes to the extreme of completely ignoring q and trying to learn how plausible it is"
D18-1260,W17-5034,0,0.0190505,"t is self-contained. Tasks with external knowledge. While many of the RC datasets could benefit from commonsense or background knowledge, they are designed to be self-contained, i.e., solvable by the document context alone. Datasets such as the Story Cloze Test (Mostafazadeh et al., 2016), MCScript,4 and ProPara (Mishra et al., 2018) do require additional domain knowledge about everyday events, scripts, and processes, respectively. However, these datasets need domain-specific modeling of events, whereas OpenBookQA appeals to broad common knowledge cutting across a variety of types and topics. Stasaski and Hearst (2017) explore the creation of multi-hop questions and propose generating stronger distractors for the multiple-choice setting. Their work, however, starts with structured knowledge, specifically a Biology ontology. Lastly, many Science Question Answering datasets (e.g. Clark et al., 2016, 2018) have been released that need broad external knowledge to answer the questions. However, these questions are not associated with a core set of facts, i.e., an “open book” used to define these questions. As a result, the questions vary widely in style and complexity (Clark et al., 2018). In contrast, OpenBookQ"
D18-1260,N18-1144,1,0.83343,"a direct match or paraphrase 2382 that can be retrieved with an IR approach. Most recently, Khashabi et al. (2018) proposed MultiRC, a multiple-choice RC dataset that is designed to require multi-sentence reasoning and can have multiple correct answers. Again, like most RC datasets, it is self-contained. Tasks with external knowledge. While many of the RC datasets could benefit from commonsense or background knowledge, they are designed to be self-contained, i.e., solvable by the document context alone. Datasets such as the Story Cloze Test (Mostafazadeh et al., 2016), MCScript,4 and ProPara (Mishra et al., 2018) do require additional domain knowledge about everyday events, scripts, and processes, respectively. However, these datasets need domain-specific modeling of events, whereas OpenBookQA appeals to broad common knowledge cutting across a variety of types and topics. Stasaski and Hearst (2017) explore the creation of multi-hop questions and propose generating stronger distractors for the multiple-choice setting. Their work, however, starts with structured knowledge, specifically a Biology ontology. Lastly, many Science Question Answering datasets (e.g. Clark et al., 2016, 2018) have been released"
D18-1260,N16-1098,0,0.0375293,"ons were generated to ensure that the answer is not a direct match or paraphrase 2382 that can be retrieved with an IR approach. Most recently, Khashabi et al. (2018) proposed MultiRC, a multiple-choice RC dataset that is designed to require multi-sentence reasoning and can have multiple correct answers. Again, like most RC datasets, it is self-contained. Tasks with external knowledge. While many of the RC datasets could benefit from commonsense or background knowledge, they are designed to be self-contained, i.e., solvable by the document context alone. Datasets such as the Story Cloze Test (Mostafazadeh et al., 2016), MCScript,4 and ProPara (Mishra et al., 2018) do require additional domain knowledge about everyday events, scripts, and processes, respectively. However, these datasets need domain-specific modeling of events, whereas OpenBookQA appeals to broad common knowledge cutting across a variety of types and topics. Stasaski and Hearst (2017) explore the creation of multi-hop questions and propose generating stronger distractors for the multiple-choice setting. Their work, however, starts with structured knowledge, specifically a Biology ontology. Lastly, many Science Question Answering datasets (e.g"
D18-1260,W17-2623,0,0.0388017,"’, (ii) broader understanding about the world (common or commonsense knowledge), and (iii) an ability to combine these facts (reasoning). This setup differs from several existing QA tasks, as summarized below. Reading Comprehension (RC) datasets have been proposed as benchmarks to evaluate the ability of systems to understand a document by answering factoid-style questions over this document. These datasets have taken various forms: multiple-choice (Richardson et al., 2013), clozestyle (Hermann et al., 2015; Onishi et al., 2016; Hill et al., 2016), and span prediction (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017) However, analysis (Chen et al., 2016; Sugawara et al., 2017) of these datasets has shown that many of the questions can be solved with context token matching (Chen et al., 2017a; Weissenborn et al., 2017) or relatively simple paraphrasing. To focus on the more challenging problem of reasoning across sentences, new datasets have been proposed for multi-step RC. QAngaroo (Welbl et al., 2018) have used a knowledgebase to identify entity pairs (s, o) with a known relation, r, which is also supported by a multihop path in a set of documents. They use structured tuple queries ("
D18-1260,D16-1241,0,0.0470796,"g OpenBookQA questions requires (i) some base science facts from a provided ‘open book’, (ii) broader understanding about the world (common or commonsense knowledge), and (iii) an ability to combine these facts (reasoning). This setup differs from several existing QA tasks, as summarized below. Reading Comprehension (RC) datasets have been proposed as benchmarks to evaluate the ability of systems to understand a document by answering factoid-style questions over this document. These datasets have taken various forms: multiple-choice (Richardson et al., 2013), clozestyle (Hermann et al., 2015; Onishi et al., 2016; Hill et al., 2016), and span prediction (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017) However, analysis (Chen et al., 2016; Sugawara et al., 2017) of these datasets has shown that many of the questions can be solved with context token matching (Chen et al., 2017a; Weissenborn et al., 2017) or relatively simple paraphrasing. To focus on the more challenging problem of reasoning across sentences, new datasets have been proposed for multi-step RC. QAngaroo (Welbl et al., 2018) have used a knowledgebase to identify entity pairs (s, o) with a known relation, r, which is als"
D18-1260,K17-1028,0,0.0130079,"ehension (RC) datasets have been proposed as benchmarks to evaluate the ability of systems to understand a document by answering factoid-style questions over this document. These datasets have taken various forms: multiple-choice (Richardson et al., 2013), clozestyle (Hermann et al., 2015; Onishi et al., 2016; Hill et al., 2016), and span prediction (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017) However, analysis (Chen et al., 2016; Sugawara et al., 2017) of these datasets has shown that many of the questions can be solved with context token matching (Chen et al., 2017a; Weissenborn et al., 2017) or relatively simple paraphrasing. To focus on the more challenging problem of reasoning across sentences, new datasets have been proposed for multi-step RC. QAngaroo (Welbl et al., 2018) have used a knowledgebase to identify entity pairs (s, o) with a known relation, r, which is also supported by a multihop path in a set of documents. They use structured tuple queries (s, r, ?) and use all the documents along the path as the input passage. NarrativeQA (Kocisk´y et al., 2017) is an RC dataset that has been shown to require an iterative reasoning about the narrative of a story. Similar to Open"
D18-1260,Q18-1021,0,0.107853,"aken various forms: multiple-choice (Richardson et al., 2013), clozestyle (Hermann et al., 2015; Onishi et al., 2016; Hill et al., 2016), and span prediction (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017) However, analysis (Chen et al., 2016; Sugawara et al., 2017) of these datasets has shown that many of the questions can be solved with context token matching (Chen et al., 2017a; Weissenborn et al., 2017) or relatively simple paraphrasing. To focus on the more challenging problem of reasoning across sentences, new datasets have been proposed for multi-step RC. QAngaroo (Welbl et al., 2018) have used a knowledgebase to identify entity pairs (s, o) with a known relation, r, which is also supported by a multihop path in a set of documents. They use structured tuple queries (s, r, ?) and use all the documents along the path as the input passage. NarrativeQA (Kocisk´y et al., 2017) is an RC dataset that has been shown to require an iterative reasoning about the narrative of a story. Similar to OpenBookQA, the questions were generated to ensure that the answer is not a direct match or paraphrase 2382 that can be retrieved with an IR approach. Most recently, Khashabi et al. (2018) pro"
D18-1260,D14-1162,0,0.0796332,"Missing"
D18-1260,N18-1202,0,0.0658219,"Missing"
D18-1260,D16-1264,0,0.287215,"Missing"
D18-1260,D13-1020,0,0.16455,"the NLP community. 2 Related Work By construction, answering OpenBookQA questions requires (i) some base science facts from a provided ‘open book’, (ii) broader understanding about the world (common or commonsense knowledge), and (iii) an ability to combine these facts (reasoning). This setup differs from several existing QA tasks, as summarized below. Reading Comprehension (RC) datasets have been proposed as benchmarks to evaluate the ability of systems to understand a document by answering factoid-style questions over this document. These datasets have taken various forms: multiple-choice (Richardson et al., 2013), clozestyle (Hermann et al., 2015; Onishi et al., 2016; Hill et al., 2016), and span prediction (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017) However, analysis (Chen et al., 2016; Sugawara et al., 2017) of these datasets has shown that many of the questions can be solved with context token matching (Chen et al., 2017a; Weissenborn et al., 2017) or relatively simple paraphrasing. To focus on the more challenging problem of reasoning across sentences, new datasets have been proposed for multi-step RC. QAngaroo (Welbl et al., 2018) have used a knowledgebase to identify ent"
D18-1535,D15-1075,0,0.228213,"ain has successfully used Open IE to extract facts from sentences (Khot et al., 2017), one of the key reasons for errors was the lossy nature of Open IE. 4941 2.2 Modules We use a Neural Entailment model to compute the entailment score based on the premise, as well as two symbolic models, Symbolic Matcher and Symbolic Lookup, to compute entailment scores based on the premise and the KB respectively (middle layer in Fig. 2). Neural Entailment We use a simple neural entailment model, Decomposable Attention (Parikh et al., 2016), one of the state-of-the-art models on the SNLI entailment dataset (Bowman et al., 2015). However, our architecture can just as easily use any other neural entailment model. We initialize the model parameters by training it on the Science Entailment dataset. Given the sub-facts from the hypothesis, we use this model to compute an entailment score n(hi , p) from the premise to each sub-fact hi . Symbolic Matcher In our initial experiments, we noticed that the neural entailment models would often either get distracted by similar words in the distributional space (false positives) or completely miss an exact mention of hi in a long premise (false negatives). To mitigate these errors"
D18-1535,Q17-1017,1,0.830201,"he following sub-sections. 2.1 Inputs We decompose the hypothesis and identify relevant KB facts in the bottom “inputs” layer (Fig. 2). Hypothesis Decomposition: To identify knowledge gaps, we must first identify the facts stated in the hypothesis h = (h1 , h2 ..). We use ClausIE (Del et al., 2013) to break h into sub-facts. ClausIE tuples need not be verb-mediated and generate multiple tuples derived from conjunctions, leading to higher recall than alternatives such as Open 1 Knowledge Base (KB): To verify these facts, we use the largest available clean knowledge base for the science domain (Dalvi et al., 2017), with 294K simple facts, as input to our system. The knowledge base contains subject-verb-object (SVO) tuples with short, one or two word arguments (e.g., hydrogen; is; element). Using these simple facts ensures that the KB is only used to fill the basic knowledge gaps and not directly prove the hypothesis irrespective of the premise. KB Retrieval: The large number of tuples in the knowledge base makes it infeasible to evaluate each hypothesis sub-fact against the entire KB. Hence, we retrieve the top-100 relevant knowledge tuples, K ′ , for each sub-fact based on a simple Jaccard word overla"
D18-1535,D16-1244,0,0.460588,"mation extraction. In one of the earliest works on entailment, the PASCAL Recognizing Textual Entailment Challenge, Dagan et al. (2005) define entailment as follows: text (or premise) P entails a hypothesis H if typically a human reading P would infer that H is most likely true. They note that this informal definition is “based on (and assumes) common human understanding of language as well as common background knowledge”. While current entailment systems have achieved impressive performance by focusing on the language understanding aspect, these systems, especially recent neural models (e.g. Parikh et al., 2016; Khot et al., 2018), do not directly address the need for filling knowledge gaps by leveraging common background knowledge. Figure 1 illustrates an example of P and H from SciTail, a recent science entailment dataset (Khot et al., 2018), that highlights the challenge of knowledge gaps—sub-facts of H that aren’t stated in P but are universally true. In this example, an entailment system that is strong at filling lexical gaps may align large blood vessel with major artery to help conclude that P entails H. Such a system, however, would equally well—but incorrectly—conclude that P entails a hypo"
D18-1535,D14-1162,0,0.0813371,", p) from the symbolic matcher, and l(hi ) from the symbolic lookup model. The task of the Aggregator network is to combine these to produce a single entailment score. However, we found that using only the final predictions from the three modules was not eﬀective. Inspired by recent work on skip/highway connections (He et al., 2016; Srivastava et al., 2015), we supplement these scores with intermediate, higher-dimensional representations from two of the modules. From the Symbolic Lookup model, we use the representation of each sub-fact henc = Enc(hi ) i obtained by averaging word embeddings (Pennington et al., 2014) and individual similarity scores over the top-100 KB tuples embi = [. . . , Sim f (hi , kb j ), . . .]. From the neural entailment model, we use the intermediate representation of both the sub-fact of hypothesis and premise text from the final layer (before the softmax computation), nv (hi , p) = [v1 ; v2 ]. We define a hybrid layer that takes as input a simple concatenation of these representation vectors from the diﬀerent modules: in(hi , p) =[henc i ; l(hi ); m(hi , p); n(hi , p); embi ; nv (hi , p)] The hybrid layer is a single layer MLP for each sub-fact hi that outputs a sub-representat"
D18-1535,P18-2086,0,0.0235031,"Missing"
D18-1535,N18-2017,0,0.0462412,"Missing"
D18-1535,D17-1292,1,0.902545,"Missing"
D18-1535,P18-1225,1,0.880386,"Missing"
D18-1535,P17-2049,1,0.897481,"ord arguments (e.g., hydrogen; is; element). Using these simple facts ensures that the KB is only used to fill the basic knowledge gaps and not directly prove the hypothesis irrespective of the premise. KB Retrieval: The large number of tuples in the knowledge base makes it infeasible to evaluate each hypothesis sub-fact against the entire KB. Hence, we retrieve the top-100 relevant knowledge tuples, K ′ , for each sub-fact based on a simple Jaccard word overlap score. 1 While prior work on question answering in the science domain has successfully used Open IE to extract facts from sentences (Khot et al., 2017), one of the key reasons for errors was the lossy nature of Open IE. 4941 2.2 Modules We use a Neural Entailment model to compute the entailment score based on the premise, as well as two symbolic models, Symbolic Matcher and Symbolic Lookup, to compute entailment scores based on the premise and the KB respectively (middle layer in Fig. 2). Neural Entailment We use a simple neural entailment model, Decomposable Attention (Parikh et al., 2016), one of the state-of-the-art models on the SNLI entailment dataset (Bowman et al., 2015). However, our architecture can just as easily use any other neur"
D19-1281,P16-1042,0,0.0310754,"er hand, open domain question answering datasets (Clark et al., 2016, 2018) come with no context, and require first retrieving relevant knowledge before reasoning with it. Retrieving this knowledge from noisy textual corpora, while simultaneously solving the reasoning problem, can be challenging, especially when questions require multiple facts. This results in simple approaches (e.g. word-overlap/PMI-based approaches), that do not heavily rely on the retrieval quality, being competitive with other complex reasoning methods that assume clean knowledge (Clark et al., 2016; Jansen et al., 2017; Angeli et al., 2016). To mitigate this issue, semistructured tables (Khashabi et al., 2016; Jansen et al., 2018) have been manually authored targeting a subset of these questions. However, these tables are expensive to create and these questions often need multiple hops (sometimes up to 1 The code and associated dataset are available at https://github.com/allenai/missing-fact. 2815 16 (Jansen et al., 2018)), making reasoning much more complex. OpenBookQA dataset (Mihaylov et al., 2018) was proposed to limit the retrieval problem by providing a set of ∼1300 facts as an ‘open book‘ for the system to use. Every ques"
D19-1281,P19-1615,0,0.0322296,"Missing"
D19-1281,D13-1160,0,0.0580915,"2018)), making reasoning much more complex. OpenBookQA dataset (Mihaylov et al., 2018) was proposed to limit the retrieval problem by providing a set of ∼1300 facts as an ‘open book‘ for the system to use. Every question is based on one of the core facts, and in addition requires basic external knowledge such as hypernymy, definition, and causality. We focus on the task of question answering under partial context, where the core fact for each question is available to the system. Knowledge-Based QA. Another line of research is answering questions (Bordes et al., 2015; Pasupat and Liang, 2015; Berant et al., 2013) over a structured knowledge base (KB) such as Freebase (Bollacker et al., 2008). Depending on the task, systems map questions to a KB query with varying complexity: from complex semantic parses (Krishnamurthy et al., 2017) to simple relational lookup (Petrochuk and Zettlemoyer, 2018). Our sub-task of filling the knowledge gap can be viewed as KB QA task with knowledge present in a KB or expected to be inferred from text. Some RC systems (Mihaylov and Frank, 2018; Kadlec et al., 2016) and Textual Entailment (TE) models (Weissenborn et al., 2017; Inkpen et al., 2018) incorporate external KBs to"
D19-1281,P18-1224,0,0.0245561,"5; Pasupat and Liang, 2015; Berant et al., 2013) over a structured knowledge base (KB) such as Freebase (Bollacker et al., 2008). Depending on the task, systems map questions to a KB query with varying complexity: from complex semantic parses (Krishnamurthy et al., 2017) to simple relational lookup (Petrochuk and Zettlemoyer, 2018). Our sub-task of filling the knowledge gap can be viewed as KB QA task with knowledge present in a KB or expected to be inferred from text. Some RC systems (Mihaylov and Frank, 2018; Kadlec et al., 2016) and Textual Entailment (TE) models (Weissenborn et al., 2017; Inkpen et al., 2018) incorporate external KBs to provide additional context to the model for better language understanding. However, we take a different approach of using this background knowledge in an explicit inference step (i.e. hop) as part of a multihop QA model. 3 Knowledge Gaps We now take a deeper look at categorizing knowledge gaps into various classes. While grounded in OpenBookQA, this categorization is relevant for other multi-hop question sets as well.2 We will then discuss how to effectively annotate such gaps. 3.1 Understanding Gaps: Categorization We analyzed the additional facts needed for answe"
D19-1281,C16-1278,1,0.842685,"o reduce noise, we only use knowledge gap annotations where at least two of three workers found a contiguous span from the core fact and a relation from our list. The final 4 This was also noticed by the original authors of OpenBookQA dataset (Mihaylov et al., 2018). 5 Workers preferably chose from a selected list of nine most common relations: {causes, definedAs, enables, isa, located in, made of, part of, provides, synonym of} and their inverses (except synonymy). These relations have also been found to be useful by prior approaches for science QA (Clark et al., 2014; Khashabi et al., 2016; Jansen et al., 2016, 2018). 2817 Total #questions Total #question-facts Avg. # spans Avg. # relations Train Dev Test 1151 1531 1.43 3.31 117 157 1.46 2.45 121 165 1.45 2.45 relations in our gaps. Since ConceptNet can be incomplete or vague (e.g. /r/RelatedTo relation), we also use the ARC corpus of 14M science-relevant sentences to improve our recall. Table 2: Statistics of the train/dev/test split of the KGD dataset. The #question-fact pairs is higher than #questions as some questions may be supported by multiple facts. The average statistic computes the average number of unique spans and relations per question"
D19-1281,J17-2005,1,0.837221,"l., 2018). On the other hand, open domain question answering datasets (Clark et al., 2016, 2018) come with no context, and require first retrieving relevant knowledge before reasoning with it. Retrieving this knowledge from noisy textual corpora, while simultaneously solving the reasoning problem, can be challenging, especially when questions require multiple facts. This results in simple approaches (e.g. word-overlap/PMI-based approaches), that do not heavily rely on the retrieval quality, being competitive with other complex reasoning methods that assume clean knowledge (Clark et al., 2016; Jansen et al., 2017; Angeli et al., 2016). To mitigate this issue, semistructured tables (Khashabi et al., 2016; Jansen et al., 2018) have been manually authored targeting a subset of these questions. However, these tables are expensive to create and these questions often need multiple hops (sometimes up to 1 The code and associated dataset are available at https://github.com/allenai/missing-fact. 2815 16 (Jansen et al., 2018)), making reasoning much more complex. OpenBookQA dataset (Mihaylov et al., 2018) was proposed to limit the retrieval problem by providing a set of ∼1300 facts as an ‘open book‘ for the sys"
D19-1281,N19-1240,0,0.020819,"Missing"
D19-1281,L18-1433,0,0.0125509,"text, and require first retrieving relevant knowledge before reasoning with it. Retrieving this knowledge from noisy textual corpora, while simultaneously solving the reasoning problem, can be challenging, especially when questions require multiple facts. This results in simple approaches (e.g. word-overlap/PMI-based approaches), that do not heavily rely on the retrieval quality, being competitive with other complex reasoning methods that assume clean knowledge (Clark et al., 2016; Jansen et al., 2017; Angeli et al., 2016). To mitigate this issue, semistructured tables (Khashabi et al., 2016; Jansen et al., 2018) have been manually authored targeting a subset of these questions. However, these tables are expensive to create and these questions often need multiple hops (sometimes up to 1 The code and associated dataset are available at https://github.com/allenai/missing-fact. 2815 16 (Jansen et al., 2018)), making reasoning much more complex. OpenBookQA dataset (Mihaylov et al., 2018) was proposed to limit the retrieval problem by providing a set of ∼1300 facts as an ‘open book‘ for the system to use. Every question is based on one of the core facts, and in addition requires basic external knowledge su"
D19-1281,N19-1405,0,0.172135,"substantially improves accuracy, we observe that using KGD to fine-tune BiDAF pretrained on SQuAD results in the best F1 (78.55) and EM (63.99) scores on the Dev set. All subsequent experiments use this fine-tuned model. 10 54.67 72.99 78.55 41.40 58.60 63.69 Choice 5.2 BiDAF Dev EM Table 3: BiDAF model performance on the span prediction task, under different choices of training data Sc(kj) kk1 j Dev F1 OpenBookQA Results We compare with three previous state-of-the-art models reported by Mihaylov et al. (2018). Two of these are Knowledge-free models (also referred to as No Context Baselines (Chen and Durrett, 2019)): (a) Question-to-Choice (Q2Choice) computes attention between the question and the answer choice, (b) ESIM + ELMo, uses ESIM (Chen et al., 2017) with ELMo (Peters et al., 2018) embeddings to compute questionchoice entailment. The third is Knowledge Enhanced Reader (KER), which uses the core fact (f) and knowledge retrieved from ConceptNet to compute cross-attentions between the question, knowledge, and answer choices. For knowledge, we consider four sources: (1) ConceptNet (CN), the English subset of ConceptNet v5.6.0 tuples;11 (2) WordNet, the WordNet subset of ConceptNet used by Mihaylov e"
D19-1281,P17-1152,0,0.022192,"on the Dev set. All subsequent experiments use this fine-tuned model. 10 54.67 72.99 78.55 41.40 58.60 63.69 Choice 5.2 BiDAF Dev EM Table 3: BiDAF model performance on the span prediction task, under different choices of training data Sc(kj) kk1 j Dev F1 OpenBookQA Results We compare with three previous state-of-the-art models reported by Mihaylov et al. (2018). Two of these are Knowledge-free models (also referred to as No Context Baselines (Chen and Durrett, 2019)): (a) Question-to-Choice (Q2Choice) computes attention between the question and the answer choice, (b) ESIM + ELMo, uses ESIM (Chen et al., 2017) with ELMo (Peters et al., 2018) embeddings to compute questionchoice entailment. The third is Knowledge Enhanced Reader (KER), which uses the core fact (f) and knowledge retrieved from ConceptNet to compute cross-attentions between the question, knowledge, and answer choices. For knowledge, we consider four sources: (1) ConceptNet (CN), the English subset of ConceptNet v5.6.0 tuples;11 (2) WordNet, the WordNet subset of ConceptNet used by Mihaylov et al. (2018); (3) OMCS, the Open Mind Common Sense subset of ConceptNet used by Mihaylov et al. (2018); and (4) ARC, with 14M sciencerelevant sent"
D19-1281,P17-1147,0,0.0392248,"ial knowledge. On the OpenBookQA dataset, given partial knowledge, explicitly identifying what’s missing substantially outperforms previous approaches. 1 Knowledge Gap (similar gaps for other choices): steel spoon in a cafeteria metal. Filled Gap (relation identified using KB): steel spoon in a cafeteria is made of metal. Figure 1: A sample OpenBookQA question, the identified knowledge gap based on partial information in the core fact, and relation (is made of ) identified from a KB to fill that gap. Introduction Reading Comprehension datasets (Richardson et al., 2013; Rajpurkar et al., 2016; Joshi et al., 2017) have gained interest as benchmarks to evaluate a system’s ability to understand a document via question answering (QA). Since many of these early datasets only required a system to understand a single sentence, new datasets were specifically designed to focus on the problem of multi-hop QA, i.e., reasoning across sentences (Khashabi et al., 2018; Welbl et al., 2018; Yang et al., 2018). While this led to improved language understanding, the tasks still assume that a system is provided with all knowledge necessary to answer the question. In practice, however, we often only have access to partia"
D19-1281,P16-1086,0,0.0220972,"ledge-Based QA. Another line of research is answering questions (Bordes et al., 2015; Pasupat and Liang, 2015; Berant et al., 2013) over a structured knowledge base (KB) such as Freebase (Bollacker et al., 2008). Depending on the task, systems map questions to a KB query with varying complexity: from complex semantic parses (Krishnamurthy et al., 2017) to simple relational lookup (Petrochuk and Zettlemoyer, 2018). Our sub-task of filling the knowledge gap can be viewed as KB QA task with knowledge present in a KB or expected to be inferred from text. Some RC systems (Mihaylov and Frank, 2018; Kadlec et al., 2016) and Textual Entailment (TE) models (Weissenborn et al., 2017; Inkpen et al., 2018) incorporate external KBs to provide additional context to the model for better language understanding. However, we take a different approach of using this background knowledge in an explicit inference step (i.e. hop) as part of a multihop QA model. 3 Knowledge Gaps We now take a deeper look at categorizing knowledge gaps into various classes. While grounded in OpenBookQA, this categorization is relevant for other multi-hop question sets as well.2 We will then discuss how to effectively annotate such gaps. 3.1 U"
D19-1281,N18-1023,0,0.0506382,"nBookQA question, the identified knowledge gap based on partial information in the core fact, and relation (is made of ) identified from a KB to fill that gap. Introduction Reading Comprehension datasets (Richardson et al., 2013; Rajpurkar et al., 2016; Joshi et al., 2017) have gained interest as benchmarks to evaluate a system’s ability to understand a document via question answering (QA). Since many of these early datasets only required a system to understand a single sentence, new datasets were specifically designed to focus on the problem of multi-hop QA, i.e., reasoning across sentences (Khashabi et al., 2018; Welbl et al., 2018; Yang et al., 2018). While this led to improved language understanding, the tasks still assume that a system is provided with all knowledge necessary to answer the question. In practice, however, we often only have access to partial knowledge when dealing with such multi-hop questions, and must retrieve additional facts (the knowledge “gaps”) based on the question and the provided knowledge. Our goal is to identify such gaps and fill them using an external knowledge source. The recently introduced challenge of open book question answering (Mihaylov et al., 2018) highlights"
D19-1281,N19-1423,0,0.053253,"Missing"
D19-1281,D17-1082,0,0.0337219,"Missing"
D19-1281,N18-2007,0,0.0211773,"Comprehension (RC) datasets probe language understanding via question answering. While several RC datasets (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017) can be addressed with single sentence understanding, newer datasets (Weston et al., 2015; Welbl et al., 2018; Khashabi et al., 2018; Yang et al., 2018) specifically target multi-hop reasoning. In both cases, all relevant information, barring some linguistic knowledge, is provided or the questions are unanswerable (Rajpurkar et al., 2018). This allows using an attention-based approach of indirectly combining information (Dhingra et al., 2018; Cao et al., 2019; Song et al., 2018). On the other hand, open domain question answering datasets (Clark et al., 2016, 2018) come with no context, and require first retrieving relevant knowledge before reasoning with it. Retrieving this knowledge from noisy textual corpora, while simultaneously solving the reasoning problem, can be challenging, especially when questions require multiple facts. This results in simple approaches (e.g. word-overlap/PMI-based approaches), that do not heavily rely on the retrieval quality, being competitive with other complex reasoning methods that assume clean kn"
D19-1281,D18-1260,1,0.808489,"sentences (Khashabi et al., 2018; Welbl et al., 2018; Yang et al., 2018). While this led to improved language understanding, the tasks still assume that a system is provided with all knowledge necessary to answer the question. In practice, however, we often only have access to partial knowledge when dealing with such multi-hop questions, and must retrieve additional facts (the knowledge “gaps”) based on the question and the provided knowledge. Our goal is to identify such gaps and fill them using an external knowledge source. The recently introduced challenge of open book question answering (Mihaylov et al., 2018) highlights this phenomenon. The questions in the corresponding dataset, OpenBookQA, are derived from a science fact in an “open book” of about 1300 facts. To answer these questions, a system must not only identify a relevant “core” science fact from this small book, but then also retrieve additional common knowledge from large external sources in order to successfully apply this core fact to the question. Consider the example in Figure 1. The core science fact metal lets heat to travel through points to metal as the correct answer, but it is not one of the 4 answer choices. Given this core fa"
D19-1281,D17-1160,0,0.0138705,"stion is based on one of the core facts, and in addition requires basic external knowledge such as hypernymy, definition, and causality. We focus on the task of question answering under partial context, where the core fact for each question is available to the system. Knowledge-Based QA. Another line of research is answering questions (Bordes et al., 2015; Pasupat and Liang, 2015; Berant et al., 2013) over a structured knowledge base (KB) such as Freebase (Bollacker et al., 2008). Depending on the task, systems map questions to a KB query with varying complexity: from complex semantic parses (Krishnamurthy et al., 2017) to simple relational lookup (Petrochuk and Zettlemoyer, 2018). Our sub-task of filling the knowledge gap can be viewed as KB QA task with knowledge present in a KB or expected to be inferred from text. Some RC systems (Mihaylov and Frank, 2018; Kadlec et al., 2016) and Textual Entailment (TE) models (Weissenborn et al., 2017; Inkpen et al., 2018) incorporate external KBs to provide additional context to the model for better language understanding. However, we take a different approach of using this background knowledge in an explicit inference step (i.e. hop) as part of a multihop QA model. 3"
D19-1281,P18-1076,0,0.0217744,"ilable to the system. Knowledge-Based QA. Another line of research is answering questions (Bordes et al., 2015; Pasupat and Liang, 2015; Berant et al., 2013) over a structured knowledge base (KB) such as Freebase (Bollacker et al., 2008). Depending on the task, systems map questions to a KB query with varying complexity: from complex semantic parses (Krishnamurthy et al., 2017) to simple relational lookup (Petrochuk and Zettlemoyer, 2018). Our sub-task of filling the knowledge gap can be viewed as KB QA task with knowledge present in a KB or expected to be inferred from text. Some RC systems (Mihaylov and Frank, 2018; Kadlec et al., 2016) and Textual Entailment (TE) models (Weissenborn et al., 2017; Inkpen et al., 2018) incorporate external KBs to provide additional context to the model for better language understanding. However, we take a different approach of using this background knowledge in an explicit inference step (i.e. hop) as part of a multihop QA model. 3 Knowledge Gaps We now take a deeper look at categorizing knowledge gaps into various classes. While grounded in OpenBookQA, this categorization is relevant for other multi-hop question sets as well.2 We will then discuss how to effectively ann"
D19-1281,D19-5804,0,0.0687801,"Missing"
D19-1281,P15-1142,0,0.015982,". 2815 16 (Jansen et al., 2018)), making reasoning much more complex. OpenBookQA dataset (Mihaylov et al., 2018) was proposed to limit the retrieval problem by providing a set of ∼1300 facts as an ‘open book‘ for the system to use. Every question is based on one of the core facts, and in addition requires basic external knowledge such as hypernymy, definition, and causality. We focus on the task of question answering under partial context, where the core fact for each question is available to the system. Knowledge-Based QA. Another line of research is answering questions (Bordes et al., 2015; Pasupat and Liang, 2015; Berant et al., 2013) over a structured knowledge base (KB) such as Freebase (Bollacker et al., 2008). Depending on the task, systems map questions to a KB query with varying complexity: from complex semantic parses (Krishnamurthy et al., 2017) to simple relational lookup (Petrochuk and Zettlemoyer, 2018). Our sub-task of filling the knowledge gap can be viewed as KB QA task with knowledge present in a KB or expected to be inferred from text. Some RC systems (Mihaylov and Frank, 2018; Kadlec et al., 2016) and Textual Entailment (TE) models (Weissenborn et al., 2017; Inkpen et al., 2018) incor"
D19-1281,D14-1162,0,0.0845796,"asticSearch8 with the query: sˆ + ci (refer to Appendix D for more details). Similar to ConceptNet, we pick top 5 sentences for each answer choice. To ensure a consistent formatting of all knowledge sources, we convert the tuples into sentences using few hand-defined rules(described in Appendix C). Finally all the retrieved sentences are combined to produce the input KB for the model, K. 4.3 Question Answering Model The question answering model takes as input the question qs , answer choices c, fact f , predicted span, sˆ and retrieved knowledge K. We use 300dimensional 840B GloVe embeddings (Pennington et al., 2014) to embed each word in the inputs. We use a Bi-LSTM with 100-dimensional hidden states to compute the contextual encodings for each string, e.g., Ef ∈ Rfm ×h . The question answering model selects the right answer using two components: (1) Fact Relevance module (2) Relation Prediction module. Fact Relevance. This module is motivated by the intuition that a relevant fact will often capture a relation between concepts that align with the question and the correct answer (the cyan and magenta regions in Figure 2). To deal with the gaps between these concepts, this module relies purely on word embe"
D19-1281,N18-1202,0,0.0197436,"t experiments use this fine-tuned model. 10 54.67 72.99 78.55 41.40 58.60 63.69 Choice 5.2 BiDAF Dev EM Table 3: BiDAF model performance on the span prediction task, under different choices of training data Sc(kj) kk1 j Dev F1 OpenBookQA Results We compare with three previous state-of-the-art models reported by Mihaylov et al. (2018). Two of these are Knowledge-free models (also referred to as No Context Baselines (Chen and Durrett, 2019)): (a) Question-to-Choice (Q2Choice) computes attention between the question and the answer choice, (b) ESIM + ELMo, uses ESIM (Chen et al., 2017) with ELMo (Peters et al., 2018) embeddings to compute questionchoice entailment. The third is Knowledge Enhanced Reader (KER), which uses the core fact (f) and knowledge retrieved from ConceptNet to compute cross-attentions between the question, knowledge, and answer choices. For knowledge, we consider four sources: (1) ConceptNet (CN), the English subset of ConceptNet v5.6.0 tuples;11 (2) WordNet, the WordNet subset of ConceptNet used by Mihaylov et al. (2018); (3) OMCS, the Open Mind Common Sense subset of ConceptNet used by Mihaylov et al. (2018); and (4) ARC, with 14M sciencerelevant sentences from the AI2 Reasoning Cha"
D19-1281,D18-1051,0,0.0188066,"requires basic external knowledge such as hypernymy, definition, and causality. We focus on the task of question answering under partial context, where the core fact for each question is available to the system. Knowledge-Based QA. Another line of research is answering questions (Bordes et al., 2015; Pasupat and Liang, 2015; Berant et al., 2013) over a structured knowledge base (KB) such as Freebase (Bollacker et al., 2008). Depending on the task, systems map questions to a KB query with varying complexity: from complex semantic parses (Krishnamurthy et al., 2017) to simple relational lookup (Petrochuk and Zettlemoyer, 2018). Our sub-task of filling the knowledge gap can be viewed as KB QA task with knowledge present in a KB or expected to be inferred from text. Some RC systems (Mihaylov and Frank, 2018; Kadlec et al., 2016) and Textual Entailment (TE) models (Weissenborn et al., 2017; Inkpen et al., 2018) incorporate external KBs to provide additional context to the model for better language understanding. However, we take a different approach of using this background knowledge in an explicit inference step (i.e. hop) as part of a multihop QA model. 3 Knowledge Gaps We now take a deeper look at categorizing know"
D19-1281,N19-1270,0,0.0958141,"Missing"
D19-1281,W17-2623,0,0.0354702,"Missing"
D19-1281,Q18-1021,0,0.0694029,"dentified knowledge gap based on partial information in the core fact, and relation (is made of ) identified from a KB to fill that gap. Introduction Reading Comprehension datasets (Richardson et al., 2013; Rajpurkar et al., 2016; Joshi et al., 2017) have gained interest as benchmarks to evaluate a system’s ability to understand a document via question answering (QA). Since many of these early datasets only required a system to understand a single sentence, new datasets were specifically designed to focus on the problem of multi-hop QA, i.e., reasoning across sentences (Khashabi et al., 2018; Welbl et al., 2018; Yang et al., 2018). While this led to improved language understanding, the tasks still assume that a system is provided with all knowledge necessary to answer the question. In practice, however, we often only have access to partial knowledge when dealing with such multi-hop questions, and must retrieve additional facts (the knowledge “gaps”) based on the question and the provided knowledge. Our goal is to identify such gaps and fill them using an external knowledge source. The recently introduced challenge of open book question answering (Mihaylov et al., 2018) highlights this phenomenon. Th"
D19-1281,D18-1259,0,0.0600104,"gap based on partial information in the core fact, and relation (is made of ) identified from a KB to fill that gap. Introduction Reading Comprehension datasets (Richardson et al., 2013; Rajpurkar et al., 2016; Joshi et al., 2017) have gained interest as benchmarks to evaluate a system’s ability to understand a document via question answering (QA). Since many of these early datasets only required a system to understand a single sentence, new datasets were specifically designed to focus on the problem of multi-hop QA, i.e., reasoning across sentences (Khashabi et al., 2018; Welbl et al., 2018; Yang et al., 2018). While this led to improved language understanding, the tasks still assume that a system is provided with all knowledge necessary to answer the question. In practice, however, we often only have access to partial knowledge when dealing with such multi-hop questions, and must retrieve additional facts (the knowledge “gaps”) based on the question and the provided knowledge. Our goal is to identify such gaps and fill them using an external knowledge source. The recently introduced challenge of open book question answering (Mihaylov et al., 2018) highlights this phenomenon. The questions in the c"
D19-1281,P18-2124,0,0.0718241,"Missing"
D19-1281,D16-1264,0,0.665078,"t with the provided partial knowledge. On the OpenBookQA dataset, given partial knowledge, explicitly identifying what’s missing substantially outperforms previous approaches. 1 Knowledge Gap (similar gaps for other choices): steel spoon in a cafeteria metal. Filled Gap (relation identified using KB): steel spoon in a cafeteria is made of metal. Figure 1: A sample OpenBookQA question, the identified knowledge gap based on partial information in the core fact, and relation (is made of ) identified from a KB to fill that gap. Introduction Reading Comprehension datasets (Richardson et al., 2013; Rajpurkar et al., 2016; Joshi et al., 2017) have gained interest as benchmarks to evaluate a system’s ability to understand a document via question answering (QA). Since many of these early datasets only required a system to understand a single sentence, new datasets were specifically designed to focus on the problem of multi-hop QA, i.e., reasoning across sentences (Khashabi et al., 2018; Welbl et al., 2018; Yang et al., 2018). While this led to improved language understanding, the tasks still assume that a system is provided with all knowledge necessary to answer the question. In practice, however, we often only"
D19-1281,D13-1020,0,0.0395134,"Missing"
D19-1457,D14-1159,1,0.876692,"background knowledge, and proved difficult to scale. More recent work on event extraction and script learning has proved more effective at extracting event/action sequences from text using statistical methods, e.g., (Chambers and Jurafsky, 2008), and neural techniques, e.g., (Modi and Titov, 2014; Modi, 2016; Pichotta and Mooney, 2016), but have largely focused on what happens and in what order, rather than why. For example, such representations cannot answer questions about which events would fail if an earlier action in a sequence did not occur. The 2014 ProRead system (Scaria et al., 2013; Berant et al., 2014) included dependency relationships between events that it extracted, but assessed dependencies based on surface language cues, hence could not explain why those dependencies held. There has also been a line of research in reading procedural text in which the goal has been to track how entities’ states change with time, e.g., EntNet (Henaff et al., 2017), ProStruct (Tandon et al., 2018), and Neural Process Networks (Bosselut et al., 2018), applied to procedural text such as cooking recipes (Kiddon et al., 2016), science processes (Dalvi et al., 2018), or toy stories (Weston et al., 2015). Using"
D19-1457,P08-1090,0,0.0462856,"ic representations (“scripts”) of event sequences (or partial orders) from text, including representations of the goals, effects, and purpose of actions, e.g., (Schank and Abelson, 1977; DeJong, 1979; Cullingford, 1986; Mooney, 1986). Some of these systems could explain why actions occurred in text, similar to our goals here, but only on a handful of toy examples using hand-coded background knowledge, and proved difficult to scale. More recent work on event extraction and script learning has proved more effective at extracting event/action sequences from text using statistical methods, e.g., (Chambers and Jurafsky, 2008), and neural techniques, e.g., (Modi and Titov, 2014; Modi, 2016; Pichotta and Mooney, 2016), but have largely focused on what happens and in what order, rather than why. For example, such representations cannot answer questions about which events would fail if an earlier action in a sequence did not occur. The 2014 ProRead system (Scaria et al., 2013; Berant et al., 2014) included dependency relationships between events that it extracted, but assessed dependencies based on surface language cues, hence could not explain why those dependencies held. There has also been a line of research in rea"
D19-1457,P16-1223,0,0.0515435,"Missing"
D19-1457,N18-1144,1,0.916324,"ehension with an additional task of predicting the dependencies between steps, in the form of their effects and which subsequent action(s) become possible. Building upon the state-ofthe-art framework for predicting effects of actions, we present a new model, called XPAD (“eXPlaining Action Dependencies”) that also considers the purpose of those effects. Specifically, XPAD biases those predictions towards those that (1) explain more of the actions in the paragraph and (2) are more plausible with respect to background knowledge. On a benchmark dataset for procedural text comprehension, ProPara (Dalvi et al., 2018), XPAD significantly improves on the prediction and explanation of action dependencies compared to prior systems, while also matching state-of-the-art results on the original tasks. We thus contribute: 1. A new task for procedural text comprehension, namely predicting and explaining the dependencies between actions (“what depends on what, and why”), including an additional dependency graph dataset for ProPara. 2. A model, XPAD, that significantly outperforms prior systems at predicting and explaining action dependencies, while maintaining its performance on the original tasks in ProPara. 2 Rel"
D19-1457,W18-2501,0,0.0327805,"Missing"
D19-1457,W19-1502,0,0.138633,"dependency layer. P R F1 ProLocal QRN EntNet ProStruct ProGlobal 24.7 32.6 32.8 76.3 43.4 18.0 30.3 38.6 21.3 37.0 20.8 31.4 35.5 33.4 39.9 XPAD 62.0 32.9 43.0 Table 1: Results on the dependency task (test set). Table 1 reports results of all models on the new dependency task. XPAD significantly outperforms the strongest baselines, ProGlobal and ProStruct, by more than 3 points F1 . XPAD has much higher precision than ProGlobal with similar recall, sug3 Since XPAD was developed, two higher unpublished results of 57.6 and 62.5 on the state-change task have appeared on arXiv (Das et al., 2019; Gupta and Durrett, 2019), their systems developed contemporaneously with XPAD. In principle XPAD’s approach of jointly learning both state changes and dependencies could be also applied in these new systems. Our main contribution is to show that jointly learning state changes and dependencies can produce more rational (explainable) results, without damaging (here, slightly improving) the state change predictions themselves. 4502 ProStruct XPAD P R F1 74.3 70.5 43.0 45.3 54.5 55.2 Table 3: Results on the state-change task (test set), comparing with the best published prior result. 7 7.1 Analysis and Discussion Depende"
D19-1457,D16-1014,1,0.853147,"sily generates πtrain , and then apply the heuristics from Section 4 to obtain Gtrain . The distribution in Gtrain can be quite different from that in ProPara (the current task), so we append Gtrain with dependency graphs obtained from the training set in ProPara. We then decompose Gtrain into its Ei j edges (negative examples are created by reversing these edges). This leads to 324,462 training examples. We then add 2,201 examples derived from the ProPara train set. This use of hand-written rules to generate a large number of potentially noisy examples follows others in the literature, e.g. (Sharp et al., 2016; Ahn et al., 2016; Bosselut et al., 2018). To accommodate for lexical variations, we embed the database of training data in a neural model. Our model itself takes as input Ei j and outputs the likelihood of Ei j . To do this, an embedding for Ei j is created using a deep network of biLSTMs, producing a contextual embedding based on the token level embeddings of si , s j and the state change vector πik . This contextual embedding is then decoded using a feedforward network to predict a score for whether si enabled s j through πik . The loss function is designed such that errors on the training"
D19-1457,D18-1006,1,0.874235,"Missing"
D19-1457,D16-1032,0,0.0330512,"er action in a sequence did not occur. The 2014 ProRead system (Scaria et al., 2013; Berant et al., 2014) included dependency relationships between events that it extracted, but assessed dependencies based on surface language cues, hence could not explain why those dependencies held. There has also been a line of research in reading procedural text in which the goal has been to track how entities’ states change with time, e.g., EntNet (Henaff et al., 2017), ProStruct (Tandon et al., 2018), and Neural Process Networks (Bosselut et al., 2018), applied to procedural text such as cooking recipes (Kiddon et al., 2016), science processes (Dalvi et al., 2018), or toy stories (Weston et al., 2015). Using annotated data, these systems learn to predict the effects of actions from text, allowing simple simulations of how the world changes throughout the process. We build on this line of work to identify the purpose of actions, by connecting their effects to subsequent actions. In addition to signal from training data, many systems use background knowledge to help bias predictions towards those consistent with that knowledge. For example, (McLauchlan, 2004) predicts prepositional phrase attachments that are more"
D19-1457,W04-2410,0,0.020795,", applied to procedural text such as cooking recipes (Kiddon et al., 2016), science processes (Dalvi et al., 2018), or toy stories (Weston et al., 2015). Using annotated data, these systems learn to predict the effects of actions from text, allowing simple simulations of how the world changes throughout the process. We build on this line of work to identify the purpose of actions, by connecting their effects to subsequent actions. In addition to signal from training data, many systems use background knowledge to help bias predictions towards those consistent with that knowledge. For example, (McLauchlan, 2004) predicts prepositional phrase attachments that are more consistent with unambiguous attachments derived from thesaurii; (Clark and Harrison, 2009) tune a parser to prefer bracketings consistent with frequent bracketings stored a text-derived corpus of bracketings; and (Tandon et al., 2018) predicts state changes consistent with those seen in a large text corpus. For our purposes, while KBs such as (Speer and Havasi, 2013; Chu et al., 2017; Park and Nezhad, 2018) contain useful information about action dependencies (e.g., “smoking can cause cancer”), they lack explanations for those links. Ins"
D19-1457,K16-1008,0,0.0130658,"ncluding representations of the goals, effects, and purpose of actions, e.g., (Schank and Abelson, 1977; DeJong, 1979; Cullingford, 1986; Mooney, 1986). Some of these systems could explain why actions occurred in text, similar to our goals here, but only on a handful of toy examples using hand-coded background knowledge, and proved difficult to scale. More recent work on event extraction and script learning has proved more effective at extracting event/action sequences from text using statistical methods, e.g., (Chambers and Jurafsky, 2008), and neural techniques, e.g., (Modi and Titov, 2014; Modi, 2016; Pichotta and Mooney, 2016), but have largely focused on what happens and in what order, rather than why. For example, such representations cannot answer questions about which events would fail if an earlier action in a sequence did not occur. The 2014 ProRead system (Scaria et al., 2013; Berant et al., 2014) included dependency relationships between events that it extracted, but assessed dependencies based on surface language cues, hence could not explain why those dependencies held. There has also been a line of research in reading procedural text in which the goal has been to track how ent"
D19-1457,W14-1606,0,0.0246656,"l orders) from text, including representations of the goals, effects, and purpose of actions, e.g., (Schank and Abelson, 1977; DeJong, 1979; Cullingford, 1986; Mooney, 1986). Some of these systems could explain why actions occurred in text, similar to our goals here, but only on a handful of toy examples using hand-coded background knowledge, and proved difficult to scale. More recent work on event extraction and script learning has proved more effective at extracting event/action sequences from text using statistical methods, e.g., (Chambers and Jurafsky, 2008), and neural techniques, e.g., (Modi and Titov, 2014; Modi, 2016; Pichotta and Mooney, 2016), but have largely focused on what happens and in what order, rather than why. For example, such representations cannot answer questions about which events would fail if an earlier action in a sequence did not occur. The 2014 ProRead system (Scaria et al., 2013; Berant et al., 2014) included dependency relationships between events that it extracted, but assessed dependencies based on surface language cues, hence could not explain why those dependencies held. There has also been a line of research in reading procedural text in which the goal has been to t"
D19-1457,D17-1108,0,0.0621705,"Missing"
D19-1457,D14-1162,0,0.0834093,"to a ‘mixture’ (Figure 2). 5.4 Training and Testing XPAD At training time, XPAD follows only the correct (gold) path through the search space, and learns to minimize the joint loss of predicting the correct state changes and dependency explanation graph for the paragraph. At test time, XPAD performs a beam search to predict the most likely state changes and dependency explanation graph. 5.5 Implementation Details for XPAD We implement XPAD in PyTorch using AllenNLP (Gardner et al., 2018). We use the dataset reader published in ProStruct’s publicly available code. We use 100D Glove embeddings (Pennington et al., 2014), trained on Wikipedia 2014 and Gigaword 5 corpora (6B tokens, 400K vocab, uncased). Starting from glove embeddings appended by entity and verb indicators, we use bidirectional LSTM layer to create contextual representation for every word. We use 100D hidden representations for the bidirectional LSTM (Hochreiter and Schmidhuber, 1997) shared between all inputs (each direction uses 50D hidden vectors). The attention layer on top of BiLSTM, uses a bilinear similarity function similar to (Chen et al., 2016) to compute attention weights over the contextual embedding for every word. To compute the"
D19-1457,P16-1027,0,0.01866,"resentations of the goals, effects, and purpose of actions, e.g., (Schank and Abelson, 1977; DeJong, 1979; Cullingford, 1986; Mooney, 1986). Some of these systems could explain why actions occurred in text, similar to our goals here, but only on a handful of toy examples using hand-coded background knowledge, and proved difficult to scale. More recent work on event extraction and script learning has proved more effective at extracting event/action sequences from text using statistical methods, e.g., (Chambers and Jurafsky, 2008), and neural techniques, e.g., (Modi and Titov, 2014; Modi, 2016; Pichotta and Mooney, 2016), but have largely focused on what happens and in what order, rather than why. For example, such representations cannot answer questions about which events would fail if an earlier action in a sequence did not occur. The 2014 ProRead system (Scaria et al., 2013; Berant et al., 2014) included dependency relationships between events that it extracted, but assessed dependencies based on surface language cues, hence could not explain why those dependencies held. There has also been a line of research in reading procedural text in which the goal has been to track how entities’ states change with ti"
D19-1457,D13-1177,1,0.740913,"les using hand-coded background knowledge, and proved difficult to scale. More recent work on event extraction and script learning has proved more effective at extracting event/action sequences from text using statistical methods, e.g., (Chambers and Jurafsky, 2008), and neural techniques, e.g., (Modi and Titov, 2014; Modi, 2016; Pichotta and Mooney, 2016), but have largely focused on what happens and in what order, rather than why. For example, such representations cannot answer questions about which events would fail if an earlier action in a sequence did not occur. The 2014 ProRead system (Scaria et al., 2013; Berant et al., 2014) included dependency relationships between events that it extracted, but assessed dependencies based on surface language cues, hence could not explain why those dependencies held. There has also been a line of research in reading procedural text in which the goal has been to track how entities’ states change with time, e.g., EntNet (Henaff et al., 2017), ProStruct (Tandon et al., 2018), and Neural Process Networks (Bosselut et al., 2018), applied to procedural text such as cooking recipes (Kiddon et al., 2016), science processes (Dalvi et al., 2018), or toy stories (Westo"
D19-1608,D13-1160,0,0.257991,"Missing"
D19-1608,P18-1078,1,0.859212,"Missing"
D19-1608,N19-1423,0,0.0439343,"Missing"
D19-1608,D17-1160,1,0.898162,"Missing"
D19-1608,N19-1270,0,0.0166905,"he full task. First, a sentence Ki is retrieved from K using Qi as a search query. This is then supplied to BERT as [CLS] Ki [SEP] question-stem [SEP] answeroption [SEP] for each option. The [CLS] output token is projected to a single logit and fed through a softmax layer across answer options, using cross entropy loss, the highest being selected. This model is fine-tuned using Q UA RT Z (only). 4. BERT (IR upper bound): Same, but using the ideal (annotated) Ki rather than retrieved Ki . 5. BERT-PFT (no knowledge): BERT first finetuned (“pre-fine-tuned”) on the RACE dataset (Lai et al., 2017; Sun et al., 2019), and then fine-tuned on Q UA RT Z (questions only, no K, both train and test). Questions are supplied as [CLS] questionstem [SEP] answer-option [SEP]. 6. BERT-PFT (IR): Same as BERT (IR), except starting with the pre-fine-tuned BERT. All models were implemented using AllenNLP (Gardner et al., 2018). 6 Results The results are shown in Table 3, and provide insights into both the data and the models: 1. The dataset is hard. Our best model, BERTPFT (IR), scores only 73.7, over 20 points behind human performance (95.0), suggesting there are significant linguistic and semantic challenges to overcom"
D19-1608,D18-1259,0,0.0703663,"s plus a background corpus of 400 qualitative knowledge sentences; and (2) an analysis of the dataset, performance of BERT-based models, and a catalog of the challenges it poses, pointing the way towards solutions. 2 Related Work Despite rapid progress in general questionanswering (QA), e.g., (Clark and Gardner, 2018), and formal models for qualitative reasoning (QR), e.g., (Forbus, 1984; Weld and De Kleer, 2013), there has been little work on reasoning with textual qualitative knowledge, and no dataset available in this area. Although many datasets include a few qualitative questions, e.g., (Yang et al., 2018; Clark et al., 2018), the only one directly probing 5941 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5941–5946, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Differing Comparatives: Q1 Jan is comparing stars, specifically a small star and the larger Sun. Given the size of each, Jan can tell that the Sun puts out heat that is (A) greater (B) lesser K1 Bigger stars produce more energy, so their surfaces are hotter. Discrete Property"
D19-1629,D16-1032,0,0.0510862,"Missing"
D19-1629,D17-1108,0,0.0500867,"Missing"
D19-1629,D16-1244,0,0.044415,"#hops=3 Total 6754 8969 4149 19872 1510 2145 941 4596 835 1153 545 2533 9099 12267 5635 27001 in-para out-of-para no-effect Total 935 1598 1460 3993 45.46 76.31 49.41 56.31 60.32 79.68 49.47 53.59 36.61 48.56 43.74 56.13 0.55 0.27 48.42 73.42 84.18 89.38 30.66 39.43 43.93 59.48 62.41 73.80 96.33 Table 2: Comparing models on WIQA test partition Table 1: Dataset statistics cannot be made. Adaboost (Freund and Schapire, 1995) was used to make the 3-way classification using several bag-of-word features computed from change and effect. Decomp-Attn applies the Decomposable Attention (DA) model of (Parikh et al., 2016) to our task. The original DA model computes entailment, i.e., the confidence that a premise entails (or contradicts) a hypothesis. We recast WIQA as an entailment task where cause-effect becomes premise-hypothesis, and (correct/opposite/no-effect) correspond to (entailment/contradiction/neutral). We retrain the DA model on WIQA using this mapping. BERT (Devlin et al., 2018) is a pre-trained transformer-based language model that has achieved state of the art performance on many NLP tasks. We supply questions to BERT as [CLS] paragraph [SEP] question [SEP] answer-option for each of the three op"
D19-1629,D18-1006,1,0.869111,"Missing"
D19-5808,N18-1023,0,0.114328,"Missing"
D19-5808,2021.ccl-1.108,0,0.246324,"Missing"
D19-5808,D18-1260,1,0.912028,"Missing"
D19-5808,D16-1264,0,0.107978,"Missing"
D19-5808,D13-1020,0,0.207718,"Missing"
D19-5808,D15-1075,0,0.126445,"Missing"
D19-5808,D18-1233,0,0.0634668,"Missing"
D19-5808,P16-1223,0,0.0240296,"ation than category A flower? Answer: more Question: Would category A flower have more or less efficient fertilization than category B flower? Answer: less Question: Which category of flowers would be more likely to have brightly colored petals? Answer: Category B Question: Which category of flowers would be less likely to have brightly colored petals? Answer: Category A Figure 1: Example questions in ROPES. Introduction Recent work in reading comprehension has seen impressive results, with models reaching human performance on well-established datasets (Devlin et al., 2019; Wang et al., 2017; Chen et al., 2016), but so far has mostly focused on extracting local predicate-argument structure, without the need to apply what was read to outside context. We introduce ROPES1 , a reading comprehension challenge that focuses on understanding causes and effects in an expository paragraph, requiring systems to apply this understanding to Comprehending a passage of text requires being able to understand the implications of the passage on other text that is read. For example, after reading a background passage about how animal pollinators increase the efficiency of fertilization in flowers, a human can easily d"
D19-5808,N19-1423,0,0.0238704,"wer have more or less efficient fertilization than category A flower? Answer: more Question: Would category A flower have more or less efficient fertilization than category B flower? Answer: less Question: Which category of flowers would be more likely to have brightly colored petals? Answer: Category B Question: Which category of flowers would be less likely to have brightly colored petals? Answer: Category A Figure 1: Example questions in ROPES. Introduction Recent work in reading comprehension has seen impressive results, with models reaching human performance on well-established datasets (Devlin et al., 2019; Wang et al., 2017; Chen et al., 2016), but so far has mostly focused on extracting local predicate-argument structure, without the need to apply what was read to outside context. We introduce ROPES1 , a reading comprehension challenge that focuses on understanding causes and effects in an expository paragraph, requiring systems to apply this understanding to Comprehending a passage of text requires being able to understand the implications of the passage on other text that is read. For example, after reading a background passage about how animal pollinators increase the efficiency of fertili"
D19-5808,P17-1018,0,0.0150372,"efficient fertilization than category A flower? Answer: more Question: Would category A flower have more or less efficient fertilization than category B flower? Answer: less Question: Which category of flowers would be more likely to have brightly colored petals? Answer: Category B Question: Which category of flowers would be less likely to have brightly colored petals? Answer: Category A Figure 1: Example questions in ROPES. Introduction Recent work in reading comprehension has seen impressive results, with models reaching human performance on well-established datasets (Devlin et al., 2019; Wang et al., 2017; Chen et al., 2016), but so far has mostly focused on extracting local predicate-argument structure, without the need to apply what was read to outside context. We introduce ROPES1 , a reading comprehension challenge that focuses on understanding causes and effects in an expository paragraph, requiring systems to apply this understanding to Comprehending a passage of text requires being able to understand the implications of the passage on other text that is read. For example, after reading a background passage about how animal pollinators increase the efficiency of fertilization in flowers,"
D19-5808,N19-1246,1,0.869684,"Missing"
D19-5808,D18-1259,0,0.151555,"Missing"
D19-5808,D19-1107,0,0.0834166,"his phone remotely. He called this test as case C. Which experiment would be less appropriate for case C, case A or case B? case A Table 4: Example questions and answers from ROPES, showing the relevant parts of the associated passage and the reasoning required to answer the question. In the last example, the situation grounds the desired outcome and asks which of two cases would achieve the desired outcome. Dataset split In initial experiments, we found splitting the dataset based on the situations resulted in high scores due to annotator bias from prolific workers generating many examples (Geva et al., 2019). We follow their proposal and separate training set annotators from test set annotators, and find that models have difficulty generalizing to new workers. 5 Development EM F1 EM Test F1 RoBERTa BASE - background 38.0 40.7 53.5 59.3 35.8 33.7 45.5 46.1 RoBERTa LARGE - background + RACE 59.7 48.7 60.1 70.2 55.2 73.5 55.4 53.6 55.5 61.1 60.4 61.6 Human - - 82.7 89.0 Table 5: Performance of baselines and human performance on the dev and test set. Baseline performance We use the RoBERTa question answering model proposed by Liu et al. (2019) as our baseline and concatenate the background and situat"
D19-5808,Q19-1026,0,\N,Missing
D19-6007,P16-1223,0,0.0623605,"Missing"
D19-6007,D19-6008,0,0.0508671,"odel for the actual machine comprehension task. Participants • JDA (Da, 2019) use three different knowledge bases, namely ConceptNet (Speer et al., 2017), ATOMIC (Sap et al., 2019) and WebChild (Tandon et al., 2017). They extract In total, five teams submitted systems in task 1, and one team participated in task 2. All submitted models were neural networks, and all made use of pretrained Transformer language models 69 relevant edges from the knowledge bases and compute relation embeddings, which are combined with BERT-based word representations with a diadic multiplication operation. • KARNA (Jain and Singh, 2019) use a BERT model, but they enhance the text representation with edges that are extracted from ConceptNet. Following Wang et al. (2018), they extract relations between words in the text and the question/answer, and append them to the text representation. Instead of computing relational embeddings, they append a specific string that describes the relation. 5 # Team Name acc acccs accOOD 1 2 3 4 5 PSH–SJTU IIT-KGP BLCU-NLP JDA KARNA 0.906 0.905∗ 0.842∗ 0.807∗ 0.733∗ 0.903 0.894 0.812 0.775 0.697 0.915 0.931 0.838 0.796 0.729 - 0.715 0.651 0.666 0.634 0.673 0.619 - TriAN Attentive Reader Logistic"
D19-6007,P17-1147,0,0.0248495,"blation tests indicating the importance of including commonsense knowledge. In comparison to ATOMIC and WebChild, Da (2019) report that ConceptNet is most beneficial for performance on the task 1 data, which can be explained with its domain: The OMCS (Singh et al., 2002) data are part of the ConceptNet database, and OMCS scenarios were also used to collect texts for the task 1 data. All in all, powerful pretrained models such as XLNet still outperform approaches that make use of structured knowledge bases, which indicates that they are (at least to some extent) capable of Web texts. TriviaQA (Joshi et al., 2017) is a corpus of webcrawled trivia and quiz-league websites together with evidence documents from the web. A large part of questions requires a system to make use of factual commonsense knowledge for finding an answer. CommonsenseQA (Talmor et al., 2018) consists of 9,000 crowdsourced multiplechoice questions with a focus on relations between entities that appear in ConceptNet (Speer et al., 2017). Evidence documents were webcrawled based on the question and added after the crowdsourcing step. Pretraining and finetuning on other data. Several participants reported effects of pretraining/finetun"
D19-6007,Q18-1023,0,0.0444937,"Missing"
D19-6007,L18-1564,1,0.883216,"Missing"
D19-6007,D17-1082,0,0.233399,"Webchild ConceptNet - 1 Table 2: Overview of participating systems DocQA (Clark and Gardner, 2018) is a strong baseline model for extractive QA. It consists of components such as bi-directional attention flow (Seo et al., 2016) and self-attention, both of which are widely used in MC models. We also evaluated a variant of DocQA with ELMo (Peters et al., 2018) to analyze the impact of ELMo on this task. such as BERT (Devlin et al., 2019). The participants used a wide range of external corpora and resources to augment their models, ranging from other machine comprehension data sets such as RACE (Lai et al., 2017) or MCScript (Ostermann et al., 2018a), up to commonsense knowledge databases such as ConceptNet (Speer et al., 2017), WebChild (Tandon et al., 2017) or ATOMIC (Sap et al., 2019). Table 2 gives a summary of the participating systems. Random Guess acts as the lower bound of the evaluated models, which randomly picks a named entity from the passage as the answer. The reported results are averaged over 5 runs. 3.3 • PSH–SJTU (Li et al., 2019) participated in both tasks with a Transformer model based on XLNet (Yang et al., 2019b). For task 1, they pretrain the model in several steps, first on the"
D19-6007,S19-1012,1,0.863993,"kers were then asked to write questions about noun or verb phrases that were highlighted in the texts. After collecting questions, the sentences containing the noun or verb phrases were deleted from the texts. During the answer collection, crowd workers thus had to infer the information required for finding an answer from background knowledge. Five turkers wrote correct and incorrect answer candidates for each question, and the most difficult incorrect candidates were selected via adversarial filtering (Zellers et al., 2018). For our shared task, we use the same data split as Ostermann et al. (2019): 14,191 questions on 2,500 texts for the training set, 2,020 questions on 355 texts for the development set and 3,610 questions on 632 texts for the test set. All texts for five scenarios were reserved for the test set only to increase difficulty. Data and Tasks Text understanding systems are often evaluated by means of a reading comprehension task, which is also referred to as machine (reading) comprehension (MC). The central idea is that a system has to process a text and then find a correct answer to a question that is asked on the text. Our shared tasks follow this paradigm and use machin"
D19-6007,D19-6011,0,0.246941,"participants used a wide range of external corpora and resources to augment their models, ranging from other machine comprehension data sets such as RACE (Lai et al., 2017) or MCScript (Ostermann et al., 2018a), up to commonsense knowledge databases such as ConceptNet (Speer et al., 2017), WebChild (Tandon et al., 2017) or ATOMIC (Sap et al., 2019). Table 2 gives a summary of the participating systems. Random Guess acts as the lower bound of the evaluated models, which randomly picks a named entity from the passage as the answer. The reported results are averaged over 5 runs. 3.3 • PSH–SJTU (Li et al., 2019) participated in both tasks with a Transformer model based on XLNet (Yang et al., 2019b). For task 1, they pretrain the model in several steps, first on the RACE data (Lai et al., 2017) and then on SWAG (Zellers et al., 2018). For task 2, they do not conduct specific pretraining steps, but implement a range of simple rulebased answer verification strategies to verify the output of the model. Evaluation Task 1. The evaluation measure for task 1 is accuracy, computed as the number of correctly answered questions divided by the number of all questions. We also report accuracy values on questions"
D19-6007,N18-1202,0,0.0255822,"ure Commonsense Other Resources Tasks 1 2 PSH–SJTU IIT-KGP - RACE, SWAG RACE 1, 2 1 3 4 BLCU-NLP JDA Transformer (XLNet) Transformer (BERT + XLNet) Transformer (BERT) Transformer (BERT) ReCoRD, RACE Wikipedia 1 1 5 KARNA Transformer (BERT) ConceptNet, Atomic, Webchild ConceptNet - 1 Table 2: Overview of participating systems DocQA (Clark and Gardner, 2018) is a strong baseline model for extractive QA. It consists of components such as bi-directional attention flow (Seo et al., 2016) and self-attention, both of which are widely used in MC models. We also evaluated a variant of DocQA with ELMo (Peters et al., 2018) to analyze the impact of ELMo on this task. such as BERT (Devlin et al., 2019). The participants used a wide range of external corpora and resources to augment their models, ranging from other machine comprehension data sets such as RACE (Lai et al., 2017) or MCScript (Ostermann et al., 2018a), up to commonsense knowledge databases such as ConceptNet (Speer et al., 2017), WebChild (Tandon et al., 2017) or ATOMIC (Sap et al., 2019). Table 2 gives a summary of the participating systems. Random Guess acts as the lower bound of the evaluated models, which randomly picks a named entity from the pa"
D19-6007,D19-6012,0,0.0641609,"pretrained on the RACE data (Lai et al., 2017), and their output is averaged for a final prediction. Task 2. We use two evaluation metrics, EM and F1, similar to those used by SQuAD (Rajpurkar et al., 2016b). Exact Match (EM) measures the percentage of predictions that match a reference answer exactly. (Macro-averaged) F1 measures the average overlap between model predictions and reference answers. For computing F1 , we treat prediction and reference answers as bags of tokens. We take the maximum F1 over all reference answers for a given query, and then average over all queries. 4 • BLCU-NLP (Liu et al., 2019) use a Transformer model based on BERT, which is finetuned in two stages: they first tune the BERTbased language model on the RACE and ReCoRD datasets and then (further) train the model for the actual machine comprehension task. Participants • JDA (Da, 2019) use three different knowledge bases, namely ConceptNet (Speer et al., 2017), ATOMIC (Sap et al., 2019) and WebChild (Tandon et al., 2017). They extract In total, five teams submitted systems in task 1, and one team participated in task 2. All submitted models were neural networks, and all made use of pretrained Transformer language models"
D19-6007,P18-1157,0,0.0138699,"of three baseline models. KT-NET (Yang et al., 2019a) employs an attention mechanism to adaptively select desired knowledge from knowledge bases, and then fuses selected knowledge with BERT to enable contextand knowledge-aware predictions for machine reading comprehension. Logistic Regression Model. Merkhofer et al. (2018) presented a logistic regression classifier for the SemEval 2018 shared task 11, which used simple overlap features and word patterns on MCScript, a predecessor of the dataset used for this task. Their model outperformed many neural networks in spite of its simplicity. SAN (Liu et al., 2018) is a top-ranked MC model. It shares many components with other MC models, and employs a stochastic answer module. As we used SAN to filter out queries in the data collection, it is necessary to verify that the collected queries are hard for not only SAN but also other MC architectures. 3.1 Task 1 Baselines 68 Rank Team Name Architecture Commonsense Other Resources Tasks 1 2 PSH–SJTU IIT-KGP - RACE, SWAG RACE 1, 2 1 3 4 BLCU-NLP JDA Transformer (XLNet) Transformer (BERT + XLNet) Transformer (BERT) Transformer (BERT) ReCoRD, RACE Wikipedia 1 1 5 KARNA Transformer (BERT) ConceptNet, Atomic, Webc"
D19-6007,D16-1264,0,0.384056,"1 is accuracy, computed as the number of correctly answered questions divided by the number of all questions. We also report accuracy values on questions that crowd workers explicitly annotated as requiring commonsense as well as performance on the five held-out scenarios. • IIT-KGP (Sharma and Roychowdhury, 2019) present an ensemble of different pretrained language models, namely BERT and XLNet. Both models are pretrained on the RACE data (Lai et al., 2017), and their output is averaged for a final prediction. Task 2. We use two evaluation metrics, EM and F1, similar to those used by SQuAD (Rajpurkar et al., 2016b). Exact Match (EM) measures the percentage of predictions that match a reference answer exactly. (Macro-averaged) F1 measures the average overlap between model predictions and reference answers. For computing F1 , we treat prediction and reference answers as bags of tokens. We take the maximum F1 over all reference answers for a given query, and then average over all queries. 4 • BLCU-NLP (Liu et al., 2019) use a Transformer model based on BERT, which is finetuned in two stages: they first tune the BERTbased language model on the RACE and ReCoRD datasets and then (further) train the model fo"
D19-6007,S18-1181,0,0.0538854,"Missing"
D19-6007,L16-1555,1,0.846494,"stions has two answer candidates, one of which is correct. Questions in the data were annotated for reasoning types, i.e. according to whether the answer to a question can be found in the text or needs to be inferred from commonsense knowledge. Roughly half of the questions do require inferences over commonsense knowledge. The texts in MCScript2.0 are short narrations (164.4 tokens on average) on a total of 200 different everyday activities. All texts were crowdsourced on Amazon Mechanical Turk1 , by asking crowd workers to tell a story about one of the 200 scenarios as if talking to a child (Modi et al., 2016; Ostermann et al., 2018a), resulting in simple texts which explicitly mention many details of a scenario. In the question collection, which was also conducted via crowdsourcing, turkers were then asked to write questions about noun or verb phrases that were highlighted in the texts. After collecting questions, the sentences containing the noun or verb phrases were deleted from the texts. During the answer collection, crowd workers thus had to infer the information required for finding an answer from background knowledge. Five turkers wrote correct and incorrect answer candidates for each ques"
D19-6007,N16-1098,0,0.0788234,"Missing"
D19-6007,D18-1009,0,0.17611,"of a scenario. In the question collection, which was also conducted via crowdsourcing, turkers were then asked to write questions about noun or verb phrases that were highlighted in the texts. After collecting questions, the sentences containing the noun or verb phrases were deleted from the texts. During the answer collection, crowd workers thus had to infer the information required for finding an answer from background knowledge. Five turkers wrote correct and incorrect answer candidates for each question, and the most difficult incorrect candidates were selected via adversarial filtering (Zellers et al., 2018). For our shared task, we use the same data split as Ostermann et al. (2019): 14,191 questions on 2,500 texts for the training set, 2,020 questions on 355 texts for the development set and 3,610 questions on 632 texts for the test set. All texts for five scenarios were reserved for the test set only to increase difficulty. Data and Tasks Text understanding systems are often evaluated by means of a reading comprehension task, which is also referred to as machine (reading) comprehension (MC). The central idea is that a system has to process a text and then find a correct answer to a question tha"
D19-6007,P19-1472,0,0.035309,"Missing"
D19-6007,P17-4020,0,0.0453697,"Missing"
D19-6007,W17-2623,0,0.0520537,"Missing"
D19-6007,P19-1226,0,0.0546566,"resent five baselines for ReCoRD: BERT (Devlin et al., 2019) is a new language representation model. Recently fine-tuning the pre-trained BERT with an additional output layer has created state-of-the-art models on a wide range of NLP tasks. We formalized ReCoRD as an extractive QA task like SQuAD, and then reused the fine-tuning script for SQuAD to fine-tune BERT for ReCoRD. Shared Task Setup The baselines for our shared tasks were adapted from Ostermann et al. (2019) and Zhang et al. (2018), respectively. Following Ostermann et al. (2019), we present results of three baseline models. KT-NET (Yang et al., 2019a) employs an attention mechanism to adaptively select desired knowledge from knowledge bases, and then fuses selected knowledge with BERT to enable contextand knowledge-aware predictions for machine reading comprehension. Logistic Regression Model. Merkhofer et al. (2018) presented a logistic regression classifier for the SemEval 2018 shared task 11, which used simple overlap features and word patterns on MCScript, a predecessor of the dataset used for this task. Their model outperformed many neural networks in spite of its simplicity. SAN (Liu et al., 2018) is a top-ranked MC model. It share"
D19-6007,C00-2137,0,0.398033,"Missing"
J17-2005,W13-2322,0,0.00982113,"n. Closer to the other end of the formality continuum, several approaches were proposed to not only select a correct answer, but also provide a formally valid justification for that answer. For example, some QA systems have sought to answer questions by creating formal proofs driven by logic reasoning (Moldovan et al. 2003a, 2007; Balduccini, Baral, and Lierler 2008; MacCartney 2009; Lewis and Steedman 2013; Liang, Jordan, and Klein 2013) answer-set programming (Tari and Baral 2006; Baral, Liang, and Nguyen 2011; Baral and Liang 2012; Baral, Vo, and Liang 2012), or connecting semantic graphs (Banarescu et al. 2013; Sharma et al. 2015). However, the formal representations used in these systems (e.g., logic forms) are both expensive to generate and tend to be brittle because they rely extensively on imperfect tools such as complete syntactic analysis and word sense disambiguation. We offer the lightly structured sentence representation generated by our approach (see Section 5) as a shallower and consequently more robust approximation of those logical forms, and show that they are well-suited for the complexity of our questions. Our approach allows us to robustly aggregate information from a variety of kn"
J17-2005,J08-1001,0,0.061091,"Missing"
J17-2005,J05-3002,0,0.0367337,"ur learning models to linear classifiers that learn efficiently at this scale. However, as discussed, tree kernels offer distinct advantages over linear models. We leave the adaptation of tree kernels to the problem discussed here as future work. Information aggregation (or fusion) is broadly defined as the assembly of knowledge from different sources, and has been used in several NLP applications, including summarization and QA. In the context of summarization, information aggregation has been used to assemble summaries from non-contiguous text fragments (Barzilay, McKeown, and Elhadad 1999; Barzilay and McKeown 2005, inter alia), whereas in QA, aggregation has been used to assemble answers to both factoid questions (Pradhan et al. 2002) and definitional questions (Blair-Goldensohn, McKeown, and Schlaikjer 2003). Critical to the current work, in an in-depth open-domain QA error analysis, Moldovan et al. (2003b) identified a subset of questions for which information from a single source is not sufficient, and designated a separate class within their taxonomy of QA systems for those systems that were capable of performing answer fusion. Combining multiple sources, however, creates the need for context disam"
J17-2005,P99-1071,0,0.151159,"Missing"
J17-2005,P14-1005,0,0.0147402,"ons are assembled, our method selects the answer that corresponds to the best (i.e., highest-scoring) justification. We learn which justifications are indicative of a correct answer by extending ranking perceptrons (Shen and Joshi 2005) that have been previously used in QA (Surdeanu, Ciaramita, and Zaragoza 2011) to include a latent layer that models the correctness of the justifications. Latent-variable perceptrons have been proposed for several other NLP tasks (Liang et al. 2006; Zettlemoyer and Collins 2007; Sun et al. 2009; Hoffmann et al. 2011; Fernandes, ¨ Dos Santos, and Milidiu´ 2012; Bjorkelund and Kuhn 2014), but to our knowledge, we are the first to adapt them to reranking scenarios. Finally, we round out our discussion of question answering systems with a comparison to the famous Watson QA system, which achieved performance on par with the 411 Computational Linguistics Volume 43, Number 2 human champions in the Jeopardy! game (Ferrucci 2012). Several of the ideas proposed in our work are reminiscent of Watson. For example, our component that generates text aggregation graphs (Section 5) shares functionality with the Prismatic engine used in Watson. Similar to Watson, we extract evidence from mu"
J17-2005,D14-1067,0,0.0127029,"Missing"
J17-2005,P16-1223,0,0.0349813,"Missing"
J17-2005,D14-1082,0,0.0212775,"vector), and continue to use the implementation of P that uses only the TAG with highest score. In this way, for each pair of candidate answers we have two justifications (one for the correct answer and one for the incorrect), and so perform a model update (i.e., backpropagation) for each. b) The combination of embeddings with explicit features that come from the CR system and the TAG features. This strategy of mixing latent and 432 Jansen et al. Framing QA as Building and Ranking Intersentence Answer Justifications explicit features has been demonstrated to be successful in other NLP tasks (Chen and Manning 2014; Suggu et al. 2016). Despite their advantages, neural networks have many hyperparameters that need to be tuned. Continuing the inspiration of Iyyer et al. (2015), we lightly turned the network on development, both in terms of network shape as well as additional parameters. Doing so, we arrived at a network that has a single dense layer with length 100 followed by the output layer of length one.14 For our word embeddings, we used a recurrent neural network language model (Mikolov et al. 2010, 2013) trained over a concatenation of all of our in-domain elementary science resources (i.e., the stu"
J17-2005,W02-1001,0,0.0379275,"cussed there, this performed far worse. Inference: During inference, the algorithm ranks all candidate answers for a given question in descending order of their score (as given by F, but using the averaged parameter vector, Θ avg ) and returns the top answer. Practical concerns: Two practical issues were omitted in Algorithm 1 for clarity, but improved performance in practice. First, we used the averaged percetron at inference 9 In fact, we found that performance consistently dropped as the number of justifications chosen by P increased. 423 Computational Linguistics Volume 43, Number 2 time (Collins 2002). That is, instead of using the latest Θ after training, we averaged all parameter vectors produced during training, and used the averaged vector, Θ avg , for inference. Second, we used a large-margin perceptron, similar to Surdeanu, Ciaramita, and Zaragoza (2011). In particular, we update the model not only when the predicted answer is incorrect (line 5), but also when the current model is not confident enough—that is, when the predicted answer is correct, but the difference in F scores between this answer and the second predicted answer is smaller than a small positive hyper parameter τ. 8."
J17-2005,P07-1033,0,0.0351743,"Missing"
J17-2005,P15-1026,0,0.0235445,"in a TAG. Eliminating this structure (i.e., considering each sentence as a bag of words, which is equivalent to a graphlet with a single nugget) substantially reduces the performance of the 1G + 2G model from 38.69 to 28.90 P@1 and the performance of the 1GCT + 2GCT model from 42.88 to 28.08 P@1. 8.5 From Latent Perceptron to Latent Neural Networks Neural networks (NNs) have recently received much attention in many NLP tasks and have been shown to be quite effective in certain question answering tasks (Bordes, Chopra, and Weston 2014; Iyyer et al. 2014; Bordes et al. 2015; Iyyer et al. 2015; Dong et al. 2015; Wang and Nyberg 2015; Yih et al. 2015; He and Lin 2016; Suggu et al. 2016). To determine if a deep learning approach would benefit our system, we explored providing our text aggregation graph features and the candidate retrieval scores to a neural ranking architecture. Under this framework we were also able to straightforwardly experiment with including vectorized representations (i.e., embeddings) of the question, candidate answer, and corresponding justification, in hopes of accommodating some of the lexical variation in natural language. Neural network systems have varied dramatically in"
J17-2005,P03-1003,0,0.0184502,"uss the results, and analyze the error classes observed, respectively. We conclude in Section 11. 409 Computational Linguistics Volume 43, Number 2 2. Related Work In one sense, QA systems can be described in terms of their position along a formality continuum ranging from shallow models that rely on information retrieval, lexical semantics, or alignment to highly structured models based on first-order logic. On the shallower end of the spectrum, QA models can be constructed either from structured text, such as question–answer pairs, or unstructured text. Alignment models (Berger et al. 2000; Echihabi and Marcu 2003; Soricut and Brill 2006; Riezler et al. 2007; Surdeanu, Ciaramita, and Zaragoza 2011; Yao et al. 2013) require aligned question– answer pairs for training, a burden that often limits their practical usage (though Sharp et al. [2015] recently proposed a method for using the discourse structure of free text as a surrogate for this alignment structure). Lexical semantic models such as neural-network language models (Yih et al. 2013; Jansen, Surdeanu, and Clark 2014; Sultan, Bethard, and Sumner 2014), on the other hand, have the advantage of being readily constructed from free text. Fried et al."
J17-2005,W12-4502,0,0.0178075,"Missing"
J17-2005,P10-1074,0,0.0158212,"dent, and 14 that are affixed by each of the connection types. For example, for a given feature such as numFocusQ, we create 15 copies: numFocusQ, numFocusQ1 ... numFocusQ14 , where the subscript (when present) indicates a specific connection type. For a TAG of connection type i, only values for the type-specific feature numFocusQi as well as the general feature numFocusQ are populated—all other numFocusQj features are set to a value of zero. This allows the learning algorithm in the next section to learn whether each feature generalizes across connection types, or not. As shown in Finkel and Manning (2010), this approach is equivalent to a joint model with a hierarchical prior. The features introduced in Table 4 apply to justifications containing any number of sentences, but characterizing justifications that are longer than two sentences is not straightforward, as the number of connection types (Figure 4) would become prohibitively large. We handle three-sentence justifications by treating each as a set of three two-sentence TAGs, and characterize each two-sentence connection individually. In the case where two groups of sentences have the same connection type, we take the highest scoring vers"
J17-2005,Q15-1015,1,0.523338,"nd Marcu 2003; Soricut and Brill 2006; Riezler et al. 2007; Surdeanu, Ciaramita, and Zaragoza 2011; Yao et al. 2013) require aligned question– answer pairs for training, a burden that often limits their practical usage (though Sharp et al. [2015] recently proposed a method for using the discourse structure of free text as a surrogate for this alignment structure). Lexical semantic models such as neural-network language models (Yih et al. 2013; Jansen, Surdeanu, and Clark 2014; Sultan, Bethard, and Sumner 2014), on the other hand, have the advantage of being readily constructed from free text. Fried et al. (2015) called these approaches first-order models because associations are explicitly learned, and introduced a higher-order lexical semantics QA model where indirect associations are detected through traversals of the association graph. Other recent efforts have applied deep learning architectures to QA to learn nonlinear answer scoring functions that model lexical semantics (Iyyer et al. 2014; Hermann et al. 2015). However, although lexical semantic approaches to QA have shown robust performance across a variety of tasks, a disadvantage of these methods is that, even when a correct answer is selec"
J17-2005,N16-1108,0,0.0200563,"each sentence as a bag of words, which is equivalent to a graphlet with a single nugget) substantially reduces the performance of the 1G + 2G model from 38.69 to 28.90 P@1 and the performance of the 1GCT + 2GCT model from 42.88 to 28.08 P@1. 8.5 From Latent Perceptron to Latent Neural Networks Neural networks (NNs) have recently received much attention in many NLP tasks and have been shown to be quite effective in certain question answering tasks (Bordes, Chopra, and Weston 2014; Iyyer et al. 2014; Bordes et al. 2015; Iyyer et al. 2015; Dong et al. 2015; Wang and Nyberg 2015; Yih et al. 2015; He and Lin 2016; Suggu et al. 2016). To determine if a deep learning approach would benefit our system, we explored providing our text aggregation graph features and the candidate retrieval scores to a neural ranking architecture. Under this framework we were also able to straightforwardly experiment with including vectorized representations (i.e., embeddings) of the question, candidate answer, and corresponding justification, in hopes of accommodating some of the lexical variation in natural language. Neural network systems have varied dramatically in terms of their primary architecture (feed-forward, convo"
J17-2005,P11-1055,0,0.0371042,"Missing"
J17-2005,D14-1070,0,0.0448051,"Missing"
J17-2005,P15-1162,0,0.0146041,"Missing"
J17-2005,J13-4004,1,0.725101,"hat question. We believe this intuition may be general and applicable to other domains. 414 Jansen et al. Framing QA as Building and Ranking Intersentence Answer Justifications Table 3 Syntactic patterns used to detect answer type words. Square brackets represent optional elements. Pattern Example (SBARQ (WHNP (WHNP (WDT) (NN)) [(PP)]... (SBARQ (WHNP (WP)) (SQ (VBZ is) (NP)... (S (NP (NP (DT A) (NN)) (SBAR (WHNP (WDT that)) ... (S (NP (NP) (PP)) (VP (VBZ is) ... What kind of energy ... What is one method that ... A tool that .. The main function of ... is to ... decreasing order of precision (Lee et al. 2013). Each of the five sieves attempts to assign question words into one of the following categories:4 1. Lists and sequences: Lists in questions generally contain highly important terms. We identify comma delimited lists of the form X, Y, ..., <and/or> Z (e.g., sleet, rain, and hail). Given the prevalence of questions that involve causal or process knowledge, we also identify from/to sequences (e.g., from a solid to a liquid) using paired prep from and prep to Stanford dependencies (De Marneffe and Manning 2008). 2. Focus words: Content lemmas (nouns, verbs, adjectives, and adverbs) with concrete"
J17-2005,P06-1096,0,0.144756,"Missing"
J17-2005,J13-2005,0,0.0223269,"Missing"
J17-2005,P14-5010,1,0.0129513,"Missing"
J17-2005,N03-1022,0,0.0955541,"Missing"
J17-2005,P01-1052,0,0.0742007,"Missing"
J17-2005,P04-1043,0,0.0376045,"a shallower and consequently more robust approximation of those logical forms, and show that they are well-suited for the complexity of our questions. Our approach allows us to robustly aggregate information from a variety of knowledge sources to create human-readable answer justifications. It is these justifications that are then ranked in order to choose the correct answer, using a reranking perceptron with a latent layer that models the correctness of those justifications. Covering the middle ground between shallow and formal representations, learning to rank methods based on tree-kernels (Moschitti 2004) perform well for various QA tasks, including passage reranking, answer sentence selection, or answer extraction (Moschitti et al. 2007; Moschitti and Quarteroni 2011; Severyn and Moschitti 2012, 2013; Severyn, Nicosia, and Moschitti 2013; Tymoshenko and Moschitti 2015, inter alia). The key to tree kernels’ success is their ability to automate feature engineering rather than having to rely on hand-crafted features, which allows them to explore a larger representation space. Further, tree kernels operate over structures that encode syntax and/or shallow semantics such as semantic role labeling"
J17-2005,P07-1098,0,0.0526244,"ity of our questions. Our approach allows us to robustly aggregate information from a variety of knowledge sources to create human-readable answer justifications. It is these justifications that are then ranked in order to choose the correct answer, using a reranking perceptron with a latent layer that models the correctness of those justifications. Covering the middle ground between shallow and formal representations, learning to rank methods based on tree-kernels (Moschitti 2004) perform well for various QA tasks, including passage reranking, answer sentence selection, or answer extraction (Moschitti et al. 2007; Moschitti and Quarteroni 2011; Severyn and Moschitti 2012, 2013; Severyn, Nicosia, and Moschitti 2013; Tymoshenko and Moschitti 2015, inter alia). The key to tree kernels’ success is their ability to automate feature engineering rather than having to rely on hand-crafted features, which allows them to explore a larger representation space. Further, tree kernels operate over structures that encode syntax and/or shallow semantics such as semantic role labeling (Severyn and Moschitti 410 Jansen et al. Framing QA as Building and Ranking Intersentence Answer Justifications 2012), knowledge from s"
J17-2005,P07-1059,0,0.0134058,"served, respectively. We conclude in Section 11. 409 Computational Linguistics Volume 43, Number 2 2. Related Work In one sense, QA systems can be described in terms of their position along a formality continuum ranging from shallow models that rely on information retrieval, lexical semantics, or alignment to highly structured models based on first-order logic. On the shallower end of the spectrum, QA models can be constructed either from structured text, such as question–answer pairs, or unstructured text. Alignment models (Berger et al. 2000; Echihabi and Marcu 2003; Soricut and Brill 2006; Riezler et al. 2007; Surdeanu, Ciaramita, and Zaragoza 2011; Yao et al. 2013) require aligned question– answer pairs for training, a burden that often limits their practical usage (though Sharp et al. [2015] recently proposed a method for using the discourse structure of free text as a surrogate for this alignment structure). Lexical semantic models such as neural-network language models (Yih et al. 2013; Jansen, Surdeanu, and Clark 2014; Sultan, Bethard, and Sumner 2014), on the other hand, have the advantage of being readily constructed from free text. Fried et al. (2015) called these approaches first-order mo"
J17-2005,D13-1044,0,0.0364372,"Missing"
J17-2005,W13-3509,0,0.0729068,"Missing"
J17-2005,N15-1025,1,0.912841,"Missing"
J17-2005,C16-1135,0,0.0588818,"Missing"
J17-2005,Q14-1018,0,0.0274658,"Missing"
J17-2005,J11-2003,1,0.51277,"Missing"
J17-2005,N03-2037,0,0.0353155,"ce-level context. Our approach works in two steps, both of which are performed offline (i.e., before the actual QA process). First, we decompose sentences that are likely to justify science exam answers (e.g., from knowledge bases such as study guides) into smaller units based on clausal and prepositional phrase boundaries. Intuitively, this allows us to maintain sufficient context to control semantic drift, while mitigating the sparsity of complete sentences. Following previous QA terminology, we call these smaller units, which represent clauses or prepositional phrases, information nuggets (Voorhees 2003). We connect two information nuggets within the same sentence if there are any syntactic dependencies that connect any words in these nuggets. We call the resulting graph of these information nuggets, which represents an entire sentence, a graphlet. Figure 2 shows a number of example graphlets. Formally, we transform sentences into graphlets using the following algorithm: 1. Parse knowledge base sentences: All knowledge base sentences are syntactically parsed using the Stanford CoreNLP toolkit (Manning et al. 2014). 2. Decompose sentences into information nuggets: For each sentence, beginning"
J17-2005,P15-2116,0,0.0745859,"Missing"
J17-2005,D13-1056,1,0.891877,"Missing"
J17-2005,P15-1128,0,0.0197819,".e., considering each sentence as a bag of words, which is equivalent to a graphlet with a single nugget) substantially reduces the performance of the 1G + 2G model from 38.69 to 28.90 P@1 and the performance of the 1GCT + 2GCT model from 42.88 to 28.08 P@1. 8.5 From Latent Perceptron to Latent Neural Networks Neural networks (NNs) have recently received much attention in many NLP tasks and have been shown to be quite effective in certain question answering tasks (Bordes, Chopra, and Weston 2014; Iyyer et al. 2014; Bordes et al. 2015; Iyyer et al. 2015; Dong et al. 2015; Wang and Nyberg 2015; Yih et al. 2015; He and Lin 2016; Suggu et al. 2016). To determine if a deep learning approach would benefit our system, we explored providing our text aggregation graph features and the candidate retrieval scores to a neural ranking architecture. Under this framework we were also able to straightforwardly experiment with including vectorized representations (i.e., embeddings) of the question, candidate answer, and corresponding justification, in hopes of accommodating some of the lexical variation in natural language. Neural network systems have varied dramatically in terms of their primary architecture (fe"
J17-2005,P13-1171,0,0.154955,"spectrum, QA models can be constructed either from structured text, such as question–answer pairs, or unstructured text. Alignment models (Berger et al. 2000; Echihabi and Marcu 2003; Soricut and Brill 2006; Riezler et al. 2007; Surdeanu, Ciaramita, and Zaragoza 2011; Yao et al. 2013) require aligned question– answer pairs for training, a burden that often limits their practical usage (though Sharp et al. [2015] recently proposed a method for using the discourse structure of free text as a surrogate for this alignment structure). Lexical semantic models such as neural-network language models (Yih et al. 2013; Jansen, Surdeanu, and Clark 2014; Sultan, Bethard, and Sumner 2014), on the other hand, have the advantage of being readily constructed from free text. Fried et al. (2015) called these approaches first-order models because associations are explicitly learned, and introduced a higher-order lexical semantics QA model where indirect associations are detected through traversals of the association graph. Other recent efforts have applied deep learning architectures to QA to learn nonlinear answer scoring functions that model lexical semantics (Iyyer et al. 2014; Hermann et al. 2015). However, alt"
J17-2005,D07-1071,0,0.0171921,"ntext, and as we demonstrate in Section 8, this boosts overall performance. Once the candidate answer justifications are assembled, our method selects the answer that corresponds to the best (i.e., highest-scoring) justification. We learn which justifications are indicative of a correct answer by extending ranking perceptrons (Shen and Joshi 2005) that have been previously used in QA (Surdeanu, Ciaramita, and Zaragoza 2011) to include a latent layer that models the correctness of the justifications. Latent-variable perceptrons have been proposed for several other NLP tasks (Liang et al. 2006; Zettlemoyer and Collins 2007; Sun et al. 2009; Hoffmann et al. 2011; Fernandes, ¨ Dos Santos, and Milidiu´ 2012; Bjorkelund and Kuhn 2014), but to our knowledge, we are the first to adapt them to reranking scenarios. Finally, we round out our discussion of question answering systems with a comparison to the famous Watson QA system, which achieved performance on par with the 411 Computational Linguistics Volume 43, Number 2 human champions in the Jeopardy! game (Ferrucci 2012). Several of the ideas proposed in our work are reminiscent of Watson. For example, our component that generates text aggregation graphs (Section 5)"
J17-2005,Q13-1015,0,\N,Missing
J17-2005,P11-1060,0,\N,Missing
J17-2005,P14-1092,1,\N,Missing
K17-1009,P15-1162,0,0.0715512,"Missing"
K17-1009,C16-1278,1,0.732006,"Missing"
K17-1009,P16-1223,0,0.04917,"Missing"
K17-1009,J17-2005,1,0.833369,"tionally lose all justification improvements from our system (see Section 6.2), demonstrating that learning this reranking is key to our approach. Additionally, we tracked the number of times a new justification was chosen by the model as it trained. We found that our system converges to a stable set of justifications during training, shown in Figure 3. the top-scoring justifications as re-ranked by our model. Each of these justifications was composed of a single sentence from our corpus, though a future version could use multi-sentence passages, or aggregate several sentences together, as in Jansen et al. (2017). Following the methodology of Jansen et al. (2017), each justification received a rating of either Good (if the connection between the question and correct answer was fully covered), Half (if there was a missing link), Topical (if the justification was simply of the right topic), or Off-Topic (if the justification was completely unrelated to the question). Examples of each rating are provided in Table 4. Results of this analysis are shown using three evaluation metrics in Table 5. The first two columns show the percentage of questions which had a Good justification at position 1 (Good@1), and"
K17-1009,D14-1082,0,0.333881,"n (Wang and Manning, 2010; Severyn and Moschitti, 2012, 2013; Severyn et al., 2013; Severyn and Moschitti, 2015; Wang and Nyberg, 2015, inter alia). Answer sentence selection is typically framed as a fully or semi-supervised task for factoid questions, where Related work In many ways, deep learning has become the canonical example of the ”black box” of machine learning and many of the approaches to explaining it can be loosely categorized into two types: approaches that try to interpret the parameters themselves (e.g., with visualizations and heat maps 70 Additionally, similar to the strategy Chen and Manning (2014) applied to parsing, we combine representation-based features with explicit features that capture additional information that is difficult to model through embeddings, especially with limited training data. The architecture of our approach is summarized in Figure 1. Given a question and a candidate answer, we first query an textual knowledge base (KB) to retrieve a pool of potential justifications for that answer candidate. For each justification, we extract a set of features designed to model the relations between questions, answers, and answer justifications based on word embeddings, lexical"
K17-1009,P14-1092,1,0.809068,"tification words that do not appear in either the question or the answer; and the length of the justification in words.4 Feature Extraction For each retrieved candidate justification, we extract a set of features based on (a) distributed representations of the question, candidate answer, and justification terms; (b) strict lexical overlap; (c) discourse relations present in the justification; and (d) the IR scores for the justification. Semi-Lexicalized Discourse features (lexDisc): These features use the discourse structure of the justification text, which has been shown to be useful for QA (Jansen et al., 2014; Sharp et al., 2015; Sachan et al., 2016). We use the discourse parser of Surdeanu et al. (2015) to fragment the text into elementary discourse units (EDUs) and then recursively connect neighboring EDUs with binary discourse relations. For each of the 18 possible relation labels, we create a set of semi-lexicalized discourse features that indicate the presence of a given discourse relation as well as whether or not the head Representation-based features (Emb): To model the similarity between the text of each question (Q), candidate answer (A), and candidate justification (J), we include a set"
K17-1009,P17-2049,1,0.847548,"Missing"
K17-1009,D16-1011,0,0.0482648,"Missing"
K17-1009,P14-2050,0,0.0145418,"beddings, we found performance was improved when using pre-trained embeddings, and in this low-data domain, fixing these embeddings to not update during training substantially reduced the amount of model over-fitting. In order to pretrain domain-relevant embeddings for our vocabulary, we used the documents from the StudyStack and Quizlet corpora, supplemented by the newly released Aristo MINI corpus (December 2016 release)9 , which contains 1.2M science-related sentences from various web sources. The training was done using the word2vec algorithm (Mikolov et al., 2010, 2013) as implemented by Levy and Goldberg (2014), such that the context for each word in a sentence is composed of all the other words in the same sentence. We used embeddings of size 50 as we did not see a performance improvement with higher dimensionality. 5.4 8 9 Model Random IR Baseline IR++ Iyyer et al. (2015) Khot et al. (2017) Our approach w/o IR P@1 Val 25 47.2 50.7∗∗ – – 50.54∗ P@1 Test 25 47 36.35 32.52 46.17 48.66 7 Our approach 54.0∗∗†† 53.3∗∗† Table 2: Performance on the AI2 Kaggle questions, measured by precision-at-one (P@1). ∗ s indicate that the difference between the corresponding model and the IR baseline is statistically"
K17-1009,P15-1026,0,0.073672,"Missing"
K17-1009,N16-1082,0,0.0398038,"Missing"
K17-1009,D16-1166,0,0.0664065,"Missing"
K17-1009,N15-1025,1,0.858117,"do not appear in either the question or the answer; and the length of the justification in words.4 Feature Extraction For each retrieved candidate justification, we extract a set of features based on (a) distributed representations of the question, candidate answer, and justification terms; (b) strict lexical overlap; (c) discourse relations present in the justification; and (d) the IR scores for the justification. Semi-Lexicalized Discourse features (lexDisc): These features use the discourse structure of the justification text, which has been shown to be useful for QA (Jansen et al., 2014; Sharp et al., 2015; Sachan et al., 2016). We use the discourse parser of Surdeanu et al. (2015) to fragment the text into elementary discourse units (EDUs) and then recursively connect neighboring EDUs with binary discourse relations. For each of the 18 possible relation labels, we create a set of semi-lexicalized discourse features that indicate the presence of a given discourse relation as well as whether or not the head Representation-based features (Emb): To model the similarity between the text of each question (Q), candidate answer (A), and candidate justification (J), we include a set of features that ut"
K17-1009,P14-5010,1,0.00874152,"esting dataset is not publicly available. 5.3 Corpora For our pool of candidate justifications (as well as the scores for our IR baselines) we used the corpora that were cited as being most helpful to the top-performing systems of the Kaggle challenge. These consisted of short, flash-card style texts gathered from two online resources: about 700K sentences from StudyStack7 and 25K sentences from Quizlet8 . From these corpora, we use the top 50 sentences retrieved by the IR model as our set of candidate justifications. All of our corpora were annotated using using the Stanford CoreNLP toolkit (Manning et al., 2014), the dependency parser of Chen and Manning (2014), and the discourse parser of Surdeanu et al. (2015). While our model is able to learn a set of embeddings, we found performance was improved when using pre-trained embeddings, and in this low-data domain, fixing these embeddings to not update during training substantially reduced the amount of model over-fitting. In order to pretrain domain-relevant embeddings for our vocabulary, we used the documents from the StudyStack and Quizlet corpora, supplemented by the newly released Aristo MINI corpus (December 2016 release)9 , which contains 1.2M sc"
K17-1009,N15-3001,1,0.874346,"Missing"
K17-1009,P16-1044,0,0.0439936,"Missing"
K17-1009,D16-1244,0,0.092624,"Missing"
K17-1009,N16-3020,0,0.206761,"Missing"
K17-1009,P15-2116,0,0.0612714,"Missing"
K17-1009,P16-2076,0,0.0260312,"ther the question or the answer; and the length of the justification in words.4 Feature Extraction For each retrieved candidate justification, we extract a set of features based on (a) distributed representations of the question, candidate answer, and justification terms; (b) strict lexical overlap; (c) discourse relations present in the justification; and (d) the IR scores for the justification. Semi-Lexicalized Discourse features (lexDisc): These features use the discourse structure of the justification text, which has been shown to be useful for QA (Jansen et al., 2014; Sharp et al., 2015; Sachan et al., 2016). We use the discourse parser of Surdeanu et al. (2015) to fragment the text into elementary discourse units (EDUs) and then recursively connect neighboring EDUs with binary discourse relations. For each of the 18 possible relation labels, we create a set of semi-lexicalized discourse features that indicate the presence of a given discourse relation as well as whether or not the head Representation-based features (Emb): To model the similarity between the text of each question (Q), candidate answer (A), and candidate justification (J), we include a set of features that utilize distributed repr"
K17-1009,C10-1131,0,0.0917657,"Missing"
K17-1009,D13-1044,0,0.0592355,"Missing"
K17-1009,W13-3509,0,0.0707555,"Missing"
N13-1106,P08-1081,0,0.00871982,"Missing"
N13-1106,N03-2010,0,0.00784703,"type as clearly as how much does. Some extra features are designed for what/which questions per required answer types. The question dependency tree is analyzed and the Lexical Answer Type (LAT) is extracted. The following are some examples of LAT for what questions: • color: what is Crips’ gang color? • animal: what kind of animal is an agouti? The extra LAT=? feature is also used with chunking features for what/which questions. There is significant prior work in building specialized templates or classifiers for labeling question types (Hermjakob, 2001; Li and Roth, 2002; Zhang and Lee, 2003; Hacioglu and Ward, 2003; Metzler and Croft, 2005; Blunsom et al., 2006; Moschitti et al., 2007). We designed our shallow question type features based on the intuitions of these prior work, with the goal of having a relatively compact approach that still extracts useful predictive signal. One possible drawback, however, is that if an LAT is not observed during training but shows up in testing, the sequence tagger would not know which answer type to associate with the question. In this case it falls back to the more general qword=? feature and will most likely pick the type of answers that are mostly associated with w"
N13-1106,N10-1145,0,0.87532,"ake public (together with the software) to the community.3 Related prior work is interspersed throughout the paper. Feature distance renNoun renVerb renOther insN, insV, insPunc, insDet, insOtherPos delN, delV, ... ins{N,V,P}Mod insSub, insObj insOtherRel delNMod, ... renNMod, ... XEdits alignNodes, alignNum, alignN, alignV, alignProper Tree Edit Distance Model Tree Edit Distance (§2.1) models have been shown effective in a variety of applications, including textual entailment, paraphrase identification, answer ranking and information retrieval (Reis et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Augsten et al., 2010). We chose the variant proposed by Heilman and Smith (2010), inspired by its simplicity, generality, and effectiveness. Our approach differs from those authors in their reliance on a greedy search routine to make use of a complex tree kernel. With speed a consideration, we opted for the dynamic-programming solution of Zhang and Shasha (1989) (§2.1). We added new lexicalsemantic features §(2.2) to the model and then evaluated our implementation on the QASR task, showing strong results §(2.3). 859 # edits inserting a noun, verb, punctuation mark, determiner or other POS ty"
N13-1106,W01-1203,0,0.0196754,"ow questions, even though they do not indicate an answer type as clearly as how much does. Some extra features are designed for what/which questions per required answer types. The question dependency tree is analyzed and the Lexical Answer Type (LAT) is extracted. The following are some examples of LAT for what questions: • color: what is Crips’ gang color? • animal: what kind of animal is an agouti? The extra LAT=? feature is also used with chunking features for what/which questions. There is significant prior work in building specialized templates or classifiers for labeling question types (Hermjakob, 2001; Li and Roth, 2002; Zhang and Lee, 2003; Hacioglu and Ward, 2003; Metzler and Croft, 2005; Blunsom et al., 2006; Moschitti et al., 2007). We designed our shallow question type features based on the intuitions of these prior work, with the goal of having a relatively compact approach that still extracts useful predictive signal. One possible drawback, however, is that if an LAT is not observed during training but shows up in testing, the sequence tagger would not know which answer type to associate with the question. In this case it falls back to the more general qword=? feature and will most"
N13-1106,C02-1150,0,0.249239,"n though they do not indicate an answer type as clearly as how much does. Some extra features are designed for what/which questions per required answer types. The question dependency tree is analyzed and the Lexical Answer Type (LAT) is extracted. The following are some examples of LAT for what questions: • color: what is Crips’ gang color? • animal: what kind of animal is an agouti? The extra LAT=? feature is also used with chunking features for what/which questions. There is significant prior work in building specialized templates or classifiers for labeling question types (Hermjakob, 2001; Li and Roth, 2002; Zhang and Lee, 2003; Hacioglu and Ward, 2003; Metzler and Croft, 2005; Blunsom et al., 2006; Moschitti et al., 2007). We designed our shallow question type features based on the intuitions of these prior work, with the goal of having a relatively compact approach that still extracts useful predictive signal. One possible drawback, however, is that if an LAT is not observed during training but shows up in testing, the sequence tagger would not know which answer type to associate with the question. In this case it falls back to the more general qword=? feature and will most likely pick the typ"
N13-1106,P02-1054,0,0.0315697,"Missing"
N13-1106,P05-1012,0,0.00925047,"against the TREC answer pattern (in the form of Perl regular expressions). If a sentence matched, then it was deemed a (noisy) positive example. Finally, TRAIN, DEV and TEST were manually corrected for errors. Those authors decided to limit candidate source sen861 tences to be no longer than 40 words.7 Keeping with prior work, those questions with only positive or negative examples were removed, leaving 94 of the original 100 questions for evaluation. The data was processed by Wang et al. (2007) with the following tool chain: POS tags via MXPOST (Ratnaparkhi, 1996); parse trees via MSTParser (McDonald et al., 2005) with 12 coarsegrained dependency relation labels; and named entities via Identifinder (Bikel et al., 1999). Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) are reported in Table 3. Our implementation gives state of the art performance, and is furthered improved by our inclusion of semantic features drawn from WordNet.8 3 CRF with TED for Answer Extraction In this section we move from ranking source sentences, to the next QA stage: answer extraction. Given our competitive TED-based alignment model, the most obvious solution to extraction would be to report those spans aligned from"
N13-1106,P07-1098,0,0.0123631,"hat/which questions per required answer types. The question dependency tree is analyzed and the Lexical Answer Type (LAT) is extracted. The following are some examples of LAT for what questions: • color: what is Crips’ gang color? • animal: what kind of animal is an agouti? The extra LAT=? feature is also used with chunking features for what/which questions. There is significant prior work in building specialized templates or classifiers for labeling question types (Hermjakob, 2001; Li and Roth, 2002; Zhang and Lee, 2003; Hacioglu and Ward, 2003; Metzler and Croft, 2005; Blunsom et al., 2006; Moschitti et al., 2007). We designed our shallow question type features based on the intuitions of these prior work, with the goal of having a relatively compact approach that still extracts useful predictive signal. One possible drawback, however, is that if an LAT is not observed during training but shows up in testing, the sequence tagger would not know which answer type to associate with the question. In this case it falls back to the more general qword=? feature and will most likely pick the type of answers that are mostly associated with what questions in training. Edit script Our TED module produces an edit t"
N13-1106,W96-0213,0,0.0416997,"the task collection, which were then compared against the TREC answer pattern (in the form of Perl regular expressions). If a sentence matched, then it was deemed a (noisy) positive example. Finally, TRAIN, DEV and TEST were manually corrected for errors. Those authors decided to limit candidate source sen861 tences to be no longer than 40 words.7 Keeping with prior work, those questions with only positive or negative examples were removed, leaving 94 of the original 100 questions for evaluation. The data was processed by Wang et al. (2007) with the following tool chain: POS tags via MXPOST (Ratnaparkhi, 1996); parse trees via MSTParser (McDonald et al., 2005) with 12 coarsegrained dependency relation labels; and named entities via Identifinder (Bikel et al., 1999). Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) are reported in Table 3. Our implementation gives state of the art performance, and is furthered improved by our inclusion of semantic features drawn from WordNet.8 3 CRF with TED for Answer Extraction In this section we move from ranking source sentences, to the next QA stage: answer extraction. Given our competitive TED-based alignment model, the most obvious solution to extr"
N13-1106,N03-2029,0,0.0254606,"n an aligned adjective. Alignment distance We observed that a candidate answer often appears close to an aligned word (i.e., answer tokens tend to be located “nearby” portions of text that align across the pair), especially in compound noun constructions, restrictive clauses, preposition phrases, etc. For instance, in the following pair, the answer Limp Bizkit comes from the leading compound noun: 863 • What is the name of Durst ’s group? • Limp Bizkit lead singer Fred Durst did a lot ... Past work has designed large numbers of specific templates aimed at these constructions (Soubbotin, 2001; Ravichandran et al., 2003; Clark et al., 2003; Sneiders, 2002). Here we use a single general feature that we expect to pick up much of this signal, without the significant feature engineering. Thus we incorporated a simple feature to roughly model this phenomenon. It is defined as the distance to the nearest aligned nonstop word in the original word order. In the above example, the only aligned nonstop word is Durst. Then this nearest alignment distance feature for the word Limp is: nearest dist to align(Limp):5 This is the only integer-valued feature. All other features are binary-valued. Note this feature does not s"
N13-1106,W06-3104,0,0.00950297,"ractice this gave stopwords “too much say” in guiding the overall edit sequence. The resultant system is fast in practice, processing 10,000 pre-parsed tree pairs per second on a contemporary machine.6 2.2 TED for Sentence Ranking The task of Question Answer Sentence Ranking (QASR) takes a question and a set of source sentences, returning a list sorted by the probability likelihood that each sentence contains an appropriate answer. Prior work in this includes that of: Punyakanok et al. (2004), based on mapping syntactic dependency trees; Wang et al. (2007) utilizing Quasi-Synchronous Grammar (Smith and Eisner, 2006); Heilman and Smith (2010) using TED; and Shima et al. (2008), Ding et al. (2008) and Wang and Manning (2010), who each employed a CRF in various ways. Wang et al. (2007) made their dataset public, which we use here for system validation. To date, models based on TED have shown the best performance for this task. Our implementation follows Heilman and Smith (2010), with the addition of 15 new features beyond their original 33 (see Table 1). Based on results 6 In later tasks, feature extraction and decoding will slow down the system, but the final system was still able to process 200 pairs per"
N13-1106,C10-1131,0,0.743788,"Missing"
N13-1106,D07-1003,0,0.439691,"erences, tagging or parsing errors. 860 tent terms. In practice this gave stopwords “too much say” in guiding the overall edit sequence. The resultant system is fast in practice, processing 10,000 pre-parsed tree pairs per second on a contemporary machine.6 2.2 TED for Sentence Ranking The task of Question Answer Sentence Ranking (QASR) takes a question and a set of source sentences, returning a list sorted by the probability likelihood that each sentence contains an appropriate answer. Prior work in this includes that of: Punyakanok et al. (2004), based on mapping syntactic dependency trees; Wang et al. (2007) utilizing Quasi-Synchronous Grammar (Smith and Eisner, 2006); Heilman and Smith (2010) using TED; and Shima et al. (2008), Ding et al. (2008) and Wang and Manning (2010), who each employed a CRF in various ways. Wang et al. (2007) made their dataset public, which we use here for system validation. To date, models based on TED have shown the best performance for this task. Our implementation follows Heilman and Smith (2010), with the addition of 15 new features beyond their original 33 (see Table 1). Based on results 6 In later tasks, feature extraction and decoding will slow down the system,"
N15-1025,J93-2003,0,0.0745619,"done using a shallow candidate retrieval (CR) component.6 Then, these answers are reranked using a more expressive model that incorporates alignment features alongside the CR score. As a learning framework we use SVMrank , a Support Vector Machine tailored for ranking.7 We compare this alignment-based reranking model against one that uses a state-of-the-art recurrent neural network language model (RNNLM) (Mikolov et al., 2010; Mikolov et al., 2013), which has been successfully applied to QA previously (Yih et al., 2013). Alignment Model: The alignment matrices were generated with IBM Model 1 (Brown et al., 1993) using GIZA++ (Och and Ney, 2003), and the corresponding models were implemented as per Surdeanu et al. (2011) with a global alignment probability. We extend this alignment model with features from Fried et al. (In press) that treat each (source) word’s probability distribution (over destination words) in the alignment matrix as a distributed semantic representation, and make use the Jensen-Shannon distance (JSD)8 between these conditional distributions. A summary of all these features is shown in Table 1. RNNLM: We learned word embeddings using the word2vec RNNLM of Mikolov et al. (2013), and"
N15-1025,P03-1003,0,0.119724,"al model, but the sequential model is an acceptable approximation when a discourse parser is not available. 3. We evaluate the proposed methods on two corpora, including a low-resource domain where training data is expensive (biology). 4. We experimentally demonstrate that monolingual alignment models trained using our method considerably outperform state-of-theart neural network language models in low resource domains. 2 Related Work Lexical semantic models have shown promise in bridging Berger et al.’s (2000) ”lexical chasm.” In general, these models can be classified into alignment models (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013) which require structured training data, and language models (Jansen et al., 2014; Sultan et al., 2014; Yih et al., 2013), which operate over free text. Here, we close this gap in resource availability by developing a method to train an alignment model over free text by making use of discourse structures. Discourse has been previously applied to QA to help identify answer candidates that contain explanatory text (e.g. Verberne et al. (2007)). Jansen et al. (2014) proposed a reranking model that used both s"
N15-1025,P12-1007,0,0.13135,"pensive training data requirements, requiring a large set of aligned indomain question-answer pairs for training. For lowresource languages or specialized domains like science or biology, often the only option is to enlist a domain expert to generate gold QA pairs – a process that is both expensive and time consuming. All of this means that only in rare cases are we accorded the luxury of having enough high-quality QA pairs to properly train an alignment model, and so these models are often underutilized or left struggling for resources. Making use of recent advancements in discourse parsing (Feng and Hirst, 2012), here we address this issue, and investigate whether alignment models for QA can be trained from artificial question-answer pairs generated from discourse structures imposed on free text. We evaluate our methods on two corpora, generating alignment models for an opendomain community QA task using Gigaword2 , and for a biology-domain QA task using a biology textbook. 1 In practice, alignment for QA is often done from answer to question, as answers tend to be longer and provide more opportunity for association (Surdeanu et al., 2011). 2 LDC catalog number LDC2012T21 231 Human Language Technolog"
N15-1025,P14-1092,1,0.922176,"-resource domain where training data is expensive (biology). 4. We experimentally demonstrate that monolingual alignment models trained using our method considerably outperform state-of-theart neural network language models in low resource domains. 2 Related Work Lexical semantic models have shown promise in bridging Berger et al.’s (2000) ”lexical chasm.” In general, these models can be classified into alignment models (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013) which require structured training data, and language models (Jansen et al., 2014; Sultan et al., 2014; Yih et al., 2013), which operate over free text. Here, we close this gap in resource availability by developing a method to train an alignment model over free text by making use of discourse structures. Discourse has been previously applied to QA to help identify answer candidates that contain explanatory text (e.g. Verberne et al. (2007)). Jansen et al. (2014) proposed a reranking model that used both shallow and deep discourse features to identify answer structures in large answer collections across different tasks and genres. Here we use discourse to impose structure"
N15-1025,W12-3018,0,0.0204879,"Missing"
N15-1025,J03-1002,0,0.0135375,"rieval (CR) component.6 Then, these answers are reranked using a more expressive model that incorporates alignment features alongside the CR score. As a learning framework we use SVMrank , a Support Vector Machine tailored for ranking.7 We compare this alignment-based reranking model against one that uses a state-of-the-art recurrent neural network language model (RNNLM) (Mikolov et al., 2010; Mikolov et al., 2013), which has been successfully applied to QA previously (Yih et al., 2013). Alignment Model: The alignment matrices were generated with IBM Model 1 (Brown et al., 1993) using GIZA++ (Och and Ney, 2003), and the corresponding models were implemented as per Surdeanu et al. (2011) with a global alignment probability. We extend this alignment model with features from Fried et al. (In press) that treat each (source) word’s probability distribution (over destination words) in the alignment matrix as a distributed semantic representation, and make use the Jensen-Shannon distance (JSD)8 between these conditional distributions. A summary of all these features is shown in Table 1. RNNLM: We learned word embeddings using the word2vec RNNLM of Mikolov et al. (2013), and include the cosine similarity-ba"
N15-1025,P07-1059,0,0.0766621,"approximation when a discourse parser is not available. 3. We evaluate the proposed methods on two corpora, including a low-resource domain where training data is expensive (biology). 4. We experimentally demonstrate that monolingual alignment models trained using our method considerably outperform state-of-theart neural network language models in low resource domains. 2 Related Work Lexical semantic models have shown promise in bridging Berger et al.’s (2000) ”lexical chasm.” In general, these models can be classified into alignment models (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013) which require structured training data, and language models (Jansen et al., 2014; Sultan et al., 2014; Yih et al., 2013), which operate over free text. Here, we close this gap in resource availability by developing a method to train an alignment model over free text by making use of discourse structures. Discourse has been previously applied to QA to help identify answer candidates that contain explanatory text (e.g. Verberne et al. (2007)). Jansen et al. (2014) proposed a reranking model that used both shallow and deep discourse features to identify"
N15-1025,N03-1030,0,0.102502,"in-house parser (Surdeanu et al., 2015), which follows the architecture introduced by Hernault et al. (2010) and Feng and Hirst (2012). The parser first segments text into elementary discourse units (EDUs), which may be at sub-sentence granularity, then recursively connects neighboring units with binary discourse relations, such as Elaboration or Contrast.4 Our parser differs from previous work with respect to feature generation in that we implement all features that rely on syntax using solely dependency syntax. For example, a crucial feature used by the parser is the dominance relations of Soricut and Marcu (2003), which capture syntactic dominance between discourse units located in the same sentence. While originally these dominance relations were implemented using constituent syntax, we provide an equivalent implementation that relies on dependency syntax. The main advantage to this approach is speed: the resulting parser performs at least an order of magnitude faster than the parser of Feng and Hirst (2012). Importantly, we generate artificial alignment pairs from this imposed structure by aligning the governing text (nucleus) with its dependent text (satellite).5 Turning again to the example in Fig"
N15-1025,Q14-1018,0,0.0310099,"e training data is expensive (biology). 4. We experimentally demonstrate that monolingual alignment models trained using our method considerably outperform state-of-theart neural network language models in low resource domains. 2 Related Work Lexical semantic models have shown promise in bridging Berger et al.’s (2000) ”lexical chasm.” In general, these models can be classified into alignment models (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013) which require structured training data, and language models (Jansen et al., 2014; Sultan et al., 2014; Yih et al., 2013), which operate over free text. Here, we close this gap in resource availability by developing a method to train an alignment model over free text by making use of discourse structures. Discourse has been previously applied to QA to help identify answer candidates that contain explanatory text (e.g. Verberne et al. (2007)). Jansen et al. (2014) proposed a reranking model that used both shallow and deep discourse features to identify answer structures in large answer collections across different tasks and genres. Here we use discourse to impose structure on free text to creat"
N15-1025,J11-2003,1,0.965745,"urces. Making use of recent advancements in discourse parsing (Feng and Hirst, 2012), here we address this issue, and investigate whether alignment models for QA can be trained from artificial question-answer pairs generated from discourse structures imposed on free text. We evaluate our methods on two corpora, generating alignment models for an opendomain community QA task using Gigaword2 , and for a biology-domain QA task using a biology textbook. 1 In practice, alignment for QA is often done from answer to question, as answers tend to be longer and provide more opportunity for association (Surdeanu et al., 2011). 2 LDC catalog number LDC2012T21 231 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 231–237, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The contributions of this work are: 1. We demonstrate that by exploiting the discourse structure of free text, monolingual alignment models can be trained to surpass the performance of models built from expensive indomain question-answer pairs. 2. We compare two methods of discourse parsing: a simple sequential model, and a deep model based on Rhetorical St"
N15-1025,N15-3001,1,0.879427,"Missing"
N15-1025,D13-1056,1,0.788159,"Missing"
N15-1025,P13-1171,0,0.242378,"pensive (biology). 4. We experimentally demonstrate that monolingual alignment models trained using our method considerably outperform state-of-theart neural network language models in low resource domains. 2 Related Work Lexical semantic models have shown promise in bridging Berger et al.’s (2000) ”lexical chasm.” In general, these models can be classified into alignment models (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013) which require structured training data, and language models (Jansen et al., 2014; Sultan et al., 2014; Yih et al., 2013), which operate over free text. Here, we close this gap in resource availability by developing a method to train an alignment model over free text by making use of discourse structures. Discourse has been previously applied to QA to help identify answer candidates that contain explanatory text (e.g. Verberne et al. (2007)). Jansen et al. (2014) proposed a reranking model that used both shallow and deep discourse features to identify answer structures in large answer collections across different tasks and genres. Here we use discourse to impose structure on free text to create inexpensive knowl"
N15-1025,Q15-1015,1,\N,Missing
N15-1086,D11-1039,0,0.026743,"However, for our domain, each distinct question warrants its own actions, slots, and values. Such a complex model would require abundant training data or laboriously handcrafted interpretation rules. In contrast, an open dialog system can usefully interpret, learn from, and respond to user utterances without a comprehensive dialog model. Domainindependent dialog systems with the flexibility to accept novel user utterances are a longstanding goal in dialog research (Polifroni et al., 2003). Recent work to address more open dialog includes bootstrapping a semantic parser from unlabeled dialogs (Artzi and Zettlemoyer, 2011), extracting potential user goals and system responses from backend databases (Hixon and Passonneau, 2013), and inducing slots and slot-fillers from a corpus of humanhuman dialogs with the use of FrameNet (Chen et al., 2014). These works focus on systems that learn about their domain prior to any human-system dialog. Our system learns about its domain during the dialog. While we rely on a limited number of templates to generate system responses, unscripted user utterances can usefully progress the dialog. This allows relation extraction from complex natural language utterances without a closed"
N15-1086,D14-1159,1,0.496981,"Missing"
N15-1086,N13-1092,0,0.0122334,"Missing"
N15-1086,N13-1128,1,0.794506,"model would require abundant training data or laboriously handcrafted interpretation rules. In contrast, an open dialog system can usefully interpret, learn from, and respond to user utterances without a comprehensive dialog model. Domainindependent dialog systems with the flexibility to accept novel user utterances are a longstanding goal in dialog research (Polifroni et al., 2003). Recent work to address more open dialog includes bootstrapping a semantic parser from unlabeled dialogs (Artzi and Zettlemoyer, 2011), extracting potential user goals and system responses from backend databases (Hixon and Passonneau, 2013), and inducing slots and slot-fillers from a corpus of humanhuman dialogs with the use of FrameNet (Chen et al., 2014). These works focus on systems that learn about their domain prior to any human-system dialog. Our system learns about its domain during the dialog. While we rely on a limited number of templates to generate system responses, unscripted user utterances can usefully progress the dialog. This allows relation extraction from complex natural language utterances without a closed set of recognized actions and known slot-value decompositions. 7 Discussion and Future Work K NOWBOT acqu"
N15-1086,D14-1058,1,0.100391,"ing is both flexible and powerful (Liu and Singh, 2004). However, in a small number of cases, relations that align known facts with question-answer statements are unlikely to lead to the correct answer. For example, our question set contains a single math problem, How long does it take for Earth to rotate on its axis seven times? (A) one day (B) one week (C) one month (D) one year. The multiplication operation necessary to infer the answer from the S CITEXT fact “The Earth rotates, or spins, on its axis once every 24 hours” is not easily represented by our model and requires other techniques (Hosseini et al., 2014). We observed only slight transfer of knowledge between questions. A larger question set with multiple questions per topic will allow us to better evaluate knowledge transfer. Our long-term goal is learning through any conversational interaction in a com859 pletely open domain, but because the fundamental trick that enables model-free NLU is computing progress towards an explicit dialog goal as a function of possible extractions, our current method is limited to tasks with explicit goals. The simple redundancy filter we use effectively distinguishes salient from noisy relations, but could be i"
N15-1086,C10-2065,0,0.221053,"Missing"
N15-1086,D14-1108,0,0.0180464,"alignment score is a Jaccard overlap modified to use relations, which makes it fast and practical, but results in many ties which we score as incorrect, and also ignores word order. For example, the bag-of-keywords is identical for contradicting answers “changing from liquid to solid” and “changing from solid to liquid.” To make this distinction, we could use an alignment score that is sensitive to word order such as an edit distance. We could expand our simple pruning constraints to take more advantage of syntax, for example by using dependency parsers optimized for conversational language (Kong et al., 2014). The relational model for reasoning is both flexible and powerful (Liu and Singh, 2004). However, in a small number of cases, relations that align known facts with question-answer statements are unlikely to lead to the correct answer. For example, our question set contains a single math problem, How long does it take for Earth to rotate on its axis seven times? (A) one day (B) one week (C) one month (D) one year. The multiplication operation necessary to infer the answer from the S CITEXT fact “The Earth rotates, or spins, on its axis once every 24 hours” is not easily represented by our mode"
N15-1086,W14-4326,0,0.105717,"ional agents. Williams et al (2015) combine active learning and dialog to efficiently label training data for dialog act classifiers. Relatively little work integrates relation extraction and dialog systems. Attribute-value pairs from restaurant reviews can generate system prompts (Reschke et al., 2013), and single-turn exchanges with search engines can populate a knowledge graph (Hakkani-Tur et al., 2014). Dependency relations extracted from individual dialog utterances by a parser also make effective features for dialog act classification (Kl¨uwer et al., 2010). The work closest to our own, Pappu and Rudnicky (2014a; 2014b), investigates knowledge acquisition strategies for academic events. Their system asks its users open-ended questions in order to elicit information about academic events of interest. They compare strategies by how many new vocabulary words are acquired, so that the best strategy prompts the user to mention the most OOV words. In their most recent work (2014b), they group the acquired researcher names by their interests to form a bipartite graph, and use acquired keywords for query ex858 pansion in a simple information retrieval task. Our present contribution builds on this general id"
N15-1086,P13-2089,0,0.0164603,"87) and its prototypes (Leal and Pearl, 1977) learn decision structures through stylized but conversational dialogs. An interactive interface for CYC (Witbrock et al., 2003) learns from experts but don’t use natural language. Fern´andez et al (2011) argue the importance of interactive language learning for conversational agents. Williams et al (2015) combine active learning and dialog to efficiently label training data for dialog act classifiers. Relatively little work integrates relation extraction and dialog systems. Attribute-value pairs from restaurant reviews can generate system prompts (Reschke et al., 2013), and single-turn exchanges with search engines can populate a knowledge graph (Hakkani-Tur et al., 2014). Dependency relations extracted from individual dialog utterances by a parser also make effective features for dialog act classification (Kl¨uwer et al., 2010). The work closest to our own, Pappu and Rudnicky (2014a; 2014b), investigates knowledge acquisition strategies for academic events. Their system asks its users open-ended questions in order to elicit information about academic events of interest. They compare strategies by how many new vocabulary words are acquired, so that the best"
N15-1086,P06-1101,0,0.0336763,"s the best supporting sentence in a text corpus. Equation (1) scores each questionanswer statement by using domain relations to align question concepts to support concepts. The next section describes sources of domain relations. 5.1 Sources of domain knowledge We compare relations from five sources: I DENTITY: An edgeless knowledge graph. The only relations are between identical concepts, equivalent to Jaccard overlap of concept keyword roots. W ORDNET: Paraphrase relations from Wordnet. Wordnet (Fellbaum, 1998) is a lexical database of synonyms and hypernyms common in NLP tasks. For example, Snow et al (2006) use Wordnet as training data for ontology induction. To build W ORDNET, we draw an edge between every pair of Wordnet concepts (ws , wq ) for which the WuPalmer Similarity (WUP) (Wu and Palmer, 1994) of the first sense in each concept’s synset exceeds 0.9, the best-performing WUP threshold we found. Concepts in the Wordnet hierarchy have a higher WUP when they have a closer common ancestor. If a known fact is Heat energy causes snow to melt, but a question asks if ice melts, then Wordnet should provide the missing knowledge that ice acts like snow. PPDB: Paraphrase relations from PPDB (Ganitk"
N15-1086,H89-1033,0,0.528703,"Missing"
N15-1086,P94-1019,0,\N,Missing
N18-1144,C16-1107,0,0.0227546,"t real-world processes, and (2) we propose two new models that learn to infer and propagate entity states in novel ways, and outperform existing methods on this dataset. Related Work Datasets: Large-scale reading comprehension datasets, e.g., SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), have successfully driven progress in question answering, but largely targeting explicitly stated facts. Often, the resulting systems can be fooled (Jia and Liang, 2017), prompting efforts to create harder datasets where a deeper understanding of the text appears necessary (Welbl et al., 2017; Araki et al., 2016). Procedural text is a genre that is particularly challenging, because the worlds they describe are largely implicit and changing. While there are few large datasets in this genre, two exceptions are bAbI (Weston et al., 2015) and SCoNE (Long et al., 2016), described earlier2 . bAbI has helped advance methods for reasoning over text, such as memory network architectures (Weston et al., 2014), but has also been criticized for using machine-generated text over a simulated domain. SCoNE is closer to our goal, but has a different task (given a perfect world model of the initial state, predict the"
N18-1144,D13-1160,0,0.0477917,"oftmax. The detailed comparisons in their design are shown in Table 2. We use the released implementations10 (with default hyper-parameter values), and retrained them on our dataset, adapted to the standard bAbI QA format. Specifically, we create three separate variations of data by adding three bAbI-style questions after each step in a paragraph, respectively: Q1. “Does e exist?” (yes/no) Q2. “Is the location of e known?” (yes/no) Q3. “Where is e?” (span) The template Q1 is applied to all participants. Q2 9 This approach has been adopted previously for questions with multiple answers (e.g., (Berant et al., 2013)). For questions with only one answer, F1 is equivalent to accuracy. 10 https://github.com/siddk/entity-network and https://github.com/uwnlp/qrn will only be present in the training data if Q1 is “yes”, and similarly Q3 is only present if Q2 is “yes”. These three variations of data are used to train three different models from the same method. At test time, we cascade the questions (e.g., ask Q2 only if the answer to the Q1 model is “yes”), and combine the model outputs accordingly to answer the questions in our target tasks (Section 5.1). We also compare against a rule-based baseline and a fe"
N18-1144,D14-1159,1,0.800052,"over text, such as memory network architectures (Weston et al., 2014), but has also been criticized for using machine-generated text over a simulated domain. SCoNE is closer to our goal, but has a different task (given a perfect world model of the initial state, predict the end state) and different motivation (handling ellipsis and coreference in context). It also used a deterministic, simulated world to generate data. Models: For answering questions about procedural text, early systems attempted to extract a process structure (events, arguments, relations) from the paragraph, e.g., ProRead (Berant et al., 2014) and for newswire (Caselli et al., 2017). This allowed questions about event ordering to be answered, but not about state changes, unmodelled by these approaches. More recently, several neural systems have been developed to answer questions about the world state after a process, inspired in part by the bAbI dataset. Building on the general Memory Network architecture (Weston et al., 2014) and gated recurrent models such as GRU (Cho et al., 2014), Recurrent Entity Networks (EntNet) (Henaff et al., 2016) is a state-of-the-art method for bAbI. EntNet uses a dynamic memory of hidden states (memory"
N18-1144,P16-1223,0,0.10149,"Missing"
N18-1144,W14-4012,0,0.0586297,"Missing"
N18-1144,D17-1215,0,0.124468,"re challenging because questions (e.g., the one shown here) often require inference about the process states. http://data.allenai.org/propara. 1 Introduction Building a reading comprehension (RC) system that is able to read a text document and to answer questions accordingly has been a long-standing goal in NLP and AI research. Impressive progress has been made in factoid-style reading comprehension, e.g., (Seo et al., 2017a; Clark and Gardner, 2017), enabled by well-designed datasets and modern neural network models. However, these models still struggle with questions that require inference (Jia and Liang, 2017). Consider the paragraph in Figure 1 about photosynthesis. While top systems on SQuAD (Rajpurkar et al., 2016) can reliably answer lookup questions such as: Q1: What do the roots absorb? (A: water, minerals) they struggle when answers are not explicit, e.g., Q2: Where is sugar produced? (A: in the leaf)1 ∗* Bhavana Dalvi Mishra and Lifu Huang contributed equally to this work. 1 For example, the RC system BiDAF (Seo et al., 2017a) answers “glucose” to this question. To answer Q2, it appears that a system needs knowledge of the world and the ability to reason with state transitions in multiple s"
N18-1144,P17-1147,0,0.0624669,"Missing"
N18-1144,P16-1138,0,0.219513,"in Figure 1. Understanding what is happening in such texts is important for many tasks, e.g., procedure execution and validation, effect prediction. However, it is also difficult because the world state is changing, and the causal effects of actions on that state are often implicit. To address this challenging style of reading comprehension problem, researchers have created several datasets. The bAbI dataset (Weston et al., 2015) includes questions about objects moved throughout a paragraph, using machine-generated language over a deterministic domain with a small lexicon. The SCoNE dataset (Long et al., 2016) contains paragraphs describing a changing world state in three synthetic, deterministic domains, and 1595 Proceedings of NAACL-HLT 2018, pages 1595–1604 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Figure 2: A (simplified) annotated paragraph from ProPara. Each filled row shows the existence and location of participants between each step (“?” denotes “unknown”, “-” denotes “does not exist”). For example in state0, water is located at the soil. assumes that a complete and correct model of the initial state is given for each task. However, approac"
N18-1144,D14-1162,0,0.0951594,"eural network architecture (biLSTM) with attention for input encoding. The prediction tasks are handled by two different output layers. We give the detail of these layers below. The design of ProLocal consists of two main components: local prediction and commonsense persistence. The former infers all direct effects of individual sentences and the latter algorithmically propagates known values forwards and backwards to fill in any remaining unknown states. Input Encoding: Each word wi in the input sentence is encoded with a vector xi = [vw : ve : vv ], the concatenation of a pre-trained GloVe (Pennington et al., 2014) word embedding vw , indicator variables ve on whether wi is the specified participant and vv on whether wi is a verb (via a POS tagger). Table 1: ProPara vs. other procedural datasets. was then split 80/10/10 into train/dev/test by process prompt, ensuring that the test paragraphs were all about processes unseen in train and dev. Table 1 compares our dataset with bAbI and SCoNE. 4 Models We present two new models for this task. The first, ProLocal, makes local state predictions and then algorithmically propagates them through the process. The second, ProGlobal, is an end-to-end neural model t"
N18-1144,D16-1264,0,0.0641623,"s. http://data.allenai.org/propara. 1 Introduction Building a reading comprehension (RC) system that is able to read a text document and to answer questions accordingly has been a long-standing goal in NLP and AI research. Impressive progress has been made in factoid-style reading comprehension, e.g., (Seo et al., 2017a; Clark and Gardner, 2017), enabled by well-designed datasets and modern neural network models. However, these models still struggle with questions that require inference (Jia and Liang, 2017). Consider the paragraph in Figure 1 about photosynthesis. While top systems on SQuAD (Rajpurkar et al., 2016) can reliably answer lookup questions such as: Q1: What do the roots absorb? (A: water, minerals) they struggle when answers are not explicit, e.g., Q2: Where is sugar produced? (A: in the leaf)1 ∗* Bhavana Dalvi Mishra and Lifu Huang contributed equally to this work. 1 For example, the RC system BiDAF (Seo et al., 2017a) answers “glucose” to this question. To answer Q2, it appears that a system needs knowledge of the world and the ability to reason with state transitions in multiple sentences: If carbon dioxide enters the leaf (stated), then it will be at the leaf (unstated), and as it is the"
N18-1144,H89-1033,0,0.45403,"Missing"
N19-1244,D14-1159,1,0.853396,"inimization for data graph. Talukdar et al. (2008) propose a graphbased semi-supervised label propagation algorithm for acquiring open-domain labeled classes and their instances from a combination of unstructured and structured text sources. Our framework extends these ideas by introducing the notion of groups (examples that are expected to be similar) and summaries (what similarities are expected), applied in an end-to-end-framework. Procedural Text Understanding and Reading Comprehension There has been a growing interest in procedural text understanding/QA recently. The ProcessBank dataset (Berant et al., 2014) asks questions about event ordering and event arguments for biology processes. bAbI (Weston et al., 2015) includes questions about movement of entities, however it’s synthetically generated and with a small lexicon. Kiddon et al. (2015)’s RECIPES dataset introduces the task of predicting the locations of cooking ingredients, and Kiddon et al. (2016) for recipe generation. In this paper, we continue this line of exploration using ProPara, and illustrate how the previous two lines of work (label consistency and semi-supervised learning) can be integrated. 3 3.1 Problem Definition ferent news st"
N19-1244,P16-1223,0,0.0514622,"Missing"
N19-1244,N18-1144,1,0.91825,"agraph about photosynthesis, a recipe). This task is challenging as the world is changing throughout the text, and despite recent advances, current systems still struggle with this task. Our approach is to leverage the fact that, for many procedural texts, multiple independent descriptions are readily available, and that predictions from them should be consistent (label consistency). We present a new learning framework that leverages label consistency during training, allowing consistency bias to be built into the model. Evaluation on a standard benchmark dataset for procedural text, ProPara (Dalvi et al., 2018), shows that our approach significantly improves prediction performance (F1) over prior state-of-the-art systems. 1 Figure 1: Fragments from three independent texts about photosynthesis. Although (1) is ambiguous as to whether oxygen is being created or merely moved, evidence from (2) and (3) suggests it is being created, helping to correctly interpret (1). More generally, encouraging consistency between predictions from different paragraphs about the same process/procedure can improve performance. Introduction We address the task of procedural text comprehension, namely tracking how the prope"
N19-1244,W18-2501,0,0.0285905,"Missing"
N19-1244,P18-1075,0,0.0217954,"ork is related to several important branches of work in both NLP and ML, as we now summarize. Leveraging Label Consistency Leveraging information about label consistency (i.e., similar instances should have consistent labels at a certain granularity) is an effective idea. It has been studied in computer vision (Haeusser et al., 2017; Chen et al., 2018) and IR (Clarke et al., 2001; Dumais et al., 2002). Learning by association (Haeusser et al., 2017) establishes implicit cross-modal links between similar descriptions and leverage more unlabeled data during training. Schütze et al. (2018); 2348 Hangya et al. (2018) adapt the similar idea to exploit unlabeled data for the cross-lingual classification. We extend this line of research in two ways: by developing a framework allowing it to be applied to the task of structure prediction; and by incorporating label consistency into the model itself via end-to-end training, rather than enforcing consistency as a post-processing step. Semi-supervised Learning Approaches Besides utilizing the label consistency knowledge, our learning framework is also able to use unlabeled paragraphs, which fits in the literature of semisupervised learning approaches (for NLP). Z"
N19-1244,D15-1114,0,0.0604539,"ces. Our framework extends these ideas by introducing the notion of groups (examples that are expected to be similar) and summaries (what similarities are expected), applied in an end-to-end-framework. Procedural Text Understanding and Reading Comprehension There has been a growing interest in procedural text understanding/QA recently. The ProcessBank dataset (Berant et al., 2014) asks questions about event ordering and event arguments for biology processes. bAbI (Weston et al., 2015) includes questions about movement of entities, however it’s synthetically generated and with a small lexicon. Kiddon et al. (2015)’s RECIPES dataset introduces the task of predicting the locations of cooking ingredients, and Kiddon et al. (2016) for recipe generation. In this paper, we continue this line of exploration using ProPara, and illustrate how the previous two lines of work (label consistency and semi-supervised learning) can be integrated. 3 3.1 Problem Definition ferent news stories about a political meeting, we expect top-level features (e.g., where the meeting took place, who attended) to be similar; for different recipes for the same item, we expect loosely similar ingredients and steps; and for different i"
N19-1244,D16-1032,0,0.0264777,"r) and summaries (what similarities are expected), applied in an end-to-end-framework. Procedural Text Understanding and Reading Comprehension There has been a growing interest in procedural text understanding/QA recently. The ProcessBank dataset (Berant et al., 2014) asks questions about event ordering and event arguments for biology processes. bAbI (Weston et al., 2015) includes questions about movement of entities, however it’s synthetically generated and with a small lexicon. Kiddon et al. (2015)’s RECIPES dataset introduces the task of predicting the locations of cooking ingredients, and Kiddon et al. (2016) for recipe generation. In this paper, we continue this line of exploration using ProPara, and illustrate how the previous two lines of work (label consistency and semi-supervised learning) can be integrated. 3 3.1 Problem Definition ferent news stories about a political meeting, we expect top-level features (e.g., where the meeting took place, who attended) to be similar; for different recipes for the same item, we expect loosely similar ingredients and steps; and for different images of the same person, we expect some high-level characteristics (e.g., height, face shape) to be similar. Note"
N19-1244,D14-1162,0,0.0816999,"ncoder ProStruct uses an encoder-decoder architecture that takes procedural text as input and predicts the state changes of entities E in the text as output. During encoding, each step st is encoded using |E |embeddings, one for each entity e j ∈ E. Each embedding represents the action that st describes, applied to ek . The model thus allows the same action to have different effects on different entities (e.g., a transformation destroys one entity, and creates another). For each (st , e j ) ∈ S × E pair, the step is fed into a BiLSTM (Hochreiter and Schmidhuber, 1997), using pretrained GloVe (Pennington et al., 2014) vectors vw for each word wi concatenated with two indicator variables, one indicating whether wi is a word referring to e j , and one indicating whether wi is a verb. A bilinear attention layer then computes attention over the contextualized vectors hi output by the BiLSTM: ai = hi ∗ B∗hev +b , where B and b are learned parameters, and hev is the concatenation of he (the averaged contextualized embedding for the entity words we ) and hv (the averaged contextualized embedding for the verb words wv ). Finally, the output vector ct j is the attentionPI weighted sum of the hi : ct j = i=1 ai ∗ hi"
N19-1244,D08-1061,0,0.0331192,"rediction; and by incorporating label consistency into the model itself via end-to-end training, rather than enforcing consistency as a post-processing step. Semi-supervised Learning Approaches Besides utilizing the label consistency knowledge, our learning framework is also able to use unlabeled paragraphs, which fits in the literature of semisupervised learning approaches (for NLP). Zhou et al. (2003) propose an iterative label propagation algorithm similar to spectral clustering. Zhu et al. (2003) propose a semi-supervised learning framework via harmonic energy minimization for data graph. Talukdar et al. (2008) propose a graphbased semi-supervised label propagation algorithm for acquiring open-domain labeled classes and their instances from a combination of unstructured and structured text sources. Our framework extends these ideas by introducing the notion of groups (examples that are expected to be similar) and summaries (what similarities are expected), applied in an end-to-end-framework. Procedural Text Understanding and Reading Comprehension There has been a growing interest in procedural text understanding/QA recently. The ProcessBank dataset (Berant et al., 2014) asks questions about event or"
N19-1244,D18-1006,1,0.889929,"Missing"
P13-2029,J08-4004,0,0.0141711,"by Lin and Katz (2006) contains 109 questions from TREC 2002 and provides a nearexhaustive judgment of relevant documents for each question. We removed 10 questions that do not have an answer by matching the TREC answer patterns. Then we call this test set MIT99. Training Set for QA We used Amazon Mechanical Turk to collect training data for the QA system by issuing answer-bearing queries for TREC19992003 questions. For the top 10 retrieved sentences for each question, three Turkers judged whether each sentence contained the answer. The inter-coder agreement rate was 0.81 (Krippendorff, 2004; Artstein and Poesio, 2008). The 99 questions of MIT99 were extracted from the Turk collection as our TESTgold with the remaining as TRAIN, with statistics shown in Table 2. Note that only 88 questions out of MIT99 have an answer from the top 10 query results. Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependencyparsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set. Corpus Preprocessing for IR The AQUAINT (LDC2002T31) corpus, on which the MIT99 q"
P13-2029,W08-1801,0,0.0248203,"distinction between our method and the technique of learning to rank applied in 3 4 160 Rarely are all three aspects presented in concert (see §2). http://code.google.com/p/jacana/ Callan (2008) extended this work with approximate matching and smoothing. Most research uses parsing to assign deep structures. Compared to shallow (POS, NER) structured retrieval, deep structures need more processing power and smoothing, but might also be more precise. 5 Most of the above (except Kaisser (2012)) only reported on IR or QA, but not both, assuming that improvement in one naturally improves the other. Bilotti and Nyberg (2008) challenged this assumption and called for tighter coupling between IR and QA. This paper is aimed at that challenge. 3 (non-GPE ). Both of them are learned to be important to where questions. Error tolerance along the NLP pipeline. IR and QA share the same processing pipeline. Systematic errors made by the processing tools are tolerated, in the sense that if the same preprocessing error is made on both the question and sentence, an answer may still be found. Take the previous where question, besides NER [0]= GPE and NER [0]= LOC, we also found oddly NER [0]= PERSON an important feature, due t"
P13-2029,P04-3031,0,0.0626902,"ystem by issuing answer-bearing queries for TREC19992003 questions. For the top 10 retrieved sentences for each question, three Turkers judged whether each sentence contained the answer. The inter-coder agreement rate was 0.81 (Krippendorff, 2004; Artstein and Poesio, 2008). The 99 questions of MIT99 were extracted from the Turk collection as our TESTgold with the remaining as TRAIN, with statistics shown in Table 2. Note that only 88 questions out of MIT99 have an answer from the top 10 query results. Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependencyparsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set. Corpus Preprocessing for IR The AQUAINT (LDC2002T31) corpus, on which the MIT99 questions are based, was processed in exactly the same manner as was the QA training set. But only sentence boundaries, POS tags and NER labels were kept as the annotation of the corpus. Figure 1: Coupled retrieval with queries directly constructed from highest weighted features of downstream QA. The retrieved and ranked list of sentences is POS"
P13-2029,W09-1119,0,0.0435715,"whether each sentence contained the answer. The inter-coder agreement rate was 0.81 (Krippendorff, 2004; Artstein and Poesio, 2008). The 99 questions of MIT99 were extracted from the Turk collection as our TESTgold with the remaining as TRAIN, with statistics shown in Table 2. Note that only 88 questions out of MIT99 have an answer from the top 10 query results. Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependencyparsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set. Corpus Preprocessing for IR The AQUAINT (LDC2002T31) corpus, on which the MIT99 questions are based, was processed in exactly the same manner as was the QA training set. But only sentence boundaries, POS tags and NER labels were kept as the annotation of the corpus. Figure 1: Coupled retrieval with queries directly constructed from highest weighted features of downstream QA. The retrieved and ranked list of sentences is POS and NER tagged, but only query-relevant tags are shown due to space limit. A bag-of-words retrieval approach would have the sentence shown above"
P13-2029,E12-1010,0,0.021652,"d entities: we are already using POS tags as a demonstration. We can potentially use any helpful answer features in retrieval. For instance, if the QA system learns that in order to is highly correlated with why question through lexicalized features, or some certain dependency relations are helpful in answering questions with specific structures, then it is natural and easy for the IR component to incorporate them. 2 Background Besides Predictive Annotation, our work is closest to structured retrieval, which covers techniques of dependency path mapping (Lin and Pantel, 2001; Cui et al., 2005; Kaisser, 2012), graph matching with Semantic Role Labeling (Shen and Lapata, 2007) and answer type checking (Pinchak et al., 2009), etc. Specifically, Bilotti et al. (2007) proposed indexing text with their semantic roles and named entities. Queries then include constraints of semantic roles and named entities for the predicate and its arguments in the question. Improvements in recall of answer-bearing sentences were shown over the bag-of-words baseline. Zhao and There is also a distinction between our method and the technique of learning to rank applied in 3 4 160 Rarely are all three aspects presented in"
P13-2029,D07-1002,0,0.0684965,". We can potentially use any helpful answer features in retrieval. For instance, if the QA system learns that in order to is highly correlated with why question through lexicalized features, or some certain dependency relations are helpful in answering questions with specific structures, then it is natural and easy for the IR component to incorporate them. 2 Background Besides Predictive Annotation, our work is closest to structured retrieval, which covers techniques of dependency path mapping (Lin and Pantel, 2001; Cui et al., 2005; Kaisser, 2012), graph matching with Semantic Role Labeling (Shen and Lapata, 2007) and answer type checking (Pinchak et al., 2009), etc. Specifically, Bilotti et al. (2007) proposed indexing text with their semantic roles and named entities. Queries then include constraints of semantic roles and named entities for the predicate and its arguments in the question. Improvements in recall of answer-bearing sentences were shown over the bag-of-words baseline. Zhao and There is also a distinction between our method and the technique of learning to rank applied in 3 4 160 Rarely are all three aspects presented in concert (see §2). http://code.google.com/p/jacana/ Callan (2008) ext"
P13-2029,P03-1054,0,0.00622919,"ns. For the top 10 retrieved sentences for each question, three Turkers judged whether each sentence contained the answer. The inter-coder agreement rate was 0.81 (Krippendorff, 2004; Artstein and Poesio, 2008). The 99 questions of MIT99 were extracted from the Turk collection as our TESTgold with the remaining as TRAIN, with statistics shown in Table 2. Note that only 88 questions out of MIT99 have an answer from the top 10 query results. Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependencyparsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set. Corpus Preprocessing for IR The AQUAINT (LDC2002T31) corpus, on which the MIT99 questions are based, was processed in exactly the same manner as was the QA training set. But only sentence boundaries, POS tags and NER labels were kept as the annotation of the corpus. Figure 1: Coupled retrieval with queries directly constructed from highest weighted features of downstream QA. The retrieved and ranked list of sentences is POS and NER tagged, but only query-relevant tags are shown due to space"
P13-2029,N13-1106,1,0.819007,"Missing"
P13-2029,P06-4018,0,\N,Missing
P13-2029,W02-0109,0,\N,Missing
P13-2123,W07-1427,0,0.175196,"Missing"
P13-2123,N10-1044,0,0.0384138,"Missing"
P13-2123,P09-2073,0,0.0414792,"in the task of tagging mail addresses, a feature of “5 consecutive digits” is highly indicative of a POSTCODE. However, in the alignment model, it does not make sense to design features based on a hard-coded state, say, a feature of “source word lemma matching target word lemma” fires for state index 6. To avoid this data sparsity problem, all features are defined implicitly with respect to the state. For instance: yields the lowest tree edit distance. Other tree or graph matching work for alignment includes that of (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Chambers et al., 2007; Mehdad, 2009; Roth and Frank, 2012). Finally, feature and model design in monolingual alignment is often inspired by bilingual work, including distortion modeling, phrasal alignment, syntactic constraints, etc (Och and Ney, 2003; DeNero and Klein, 2007; Bansal et al., 2011). 3 The Alignment Model 3.1 Model Design Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006). Given a source sentence s of length M , and a target sentence t of length N , the alignment from s to t is a sequence of target word indices a, where am∈["
P13-2123,J03-1002,0,0.0065229,"te, say, a feature of “source word lemma matching target word lemma” fires for state index 6. To avoid this data sparsity problem, all features are defined implicitly with respect to the state. For instance: yields the lowest tree edit distance. Other tree or graph matching work for alignment includes that of (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Chambers et al., 2007; Mehdad, 2009; Roth and Frank, 2012). Finally, feature and model design in monolingual alignment is often inspired by bilingual work, including distortion modeling, phrasal alignment, syntactic constraints, etc (Och and Ney, 2003; DeNero and Klein, 2007; Bansal et al., 2011). 3 The Alignment Model 3.1 Model Design Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006). Given a source sentence s of length M , and a target sentence t of length N , the alignment from s to t is a sequence of target word indices a, where am∈[1,M ] ∈ [0, N ]. We specify that when am = 0, source word st is aligned to a NULL state, i.e., deleted. This models a many-to-one alignment from source to target. Multiple source words can be aligned to the same targ"
P13-2123,D12-1016,0,0.0743357,"tagging mail addresses, a feature of “5 consecutive digits” is highly indicative of a POSTCODE. However, in the alignment model, it does not make sense to design features based on a hard-coded state, say, a feature of “source word lemma matching target word lemma” fires for state index 6. To avoid this data sparsity problem, all features are defined implicitly with respect to the state. For instance: yields the lowest tree edit distance. Other tree or graph matching work for alignment includes that of (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Chambers et al., 2007; Mehdad, 2009; Roth and Frank, 2012). Finally, feature and model design in monolingual alignment is often inspired by bilingual work, including distortion modeling, phrasal alignment, syntactic constraints, etc (Och and Ney, 2003; DeNero and Klein, 2007; Bansal et al., 2011). 3 The Alignment Model 3.1 Model Design Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006). Given a source sentence s of length M , and a target sentence t of length N , the alignment from s to t is a sequence of target word indices a, where am∈[1,M ] ∈ [0, N ]. We spe"
P13-2123,P11-2044,0,0.496604,"ilizes arbitrary features (to make use of word similarity measure and lexical resources) and exploits deeper sentence structures (especially in the case of major languages where robust parsers are available). In this setting the balance between precision and speed becomes an issue: while we might leverage an extensive NLP pipeline for a ∗ 2 Related Work The MANLI aligner (MacCartney et al., 2008) was first proposed to align premise and hypothesis sentences for the task of natural language inference. It applies perceptron learning and handles phrase-based alignment of arbitrary phrase lengths. Thadani and McKeown (2011) optimized this model by decoding via Integer Linear Programming (ILP). Benefiting from modern ILP solvers, this led to an order-of-magnitude speedup. With extra syntactic constraints added, the exact alignment match rate for whole sentence pairs was also significantly improved. Besides the above supervised methods, indirect supervision has also been explored. Among them, Wang and Manning (2010) extended the work of McCallum et al. (2005) and modeled alignment as latent variables. Heilman and Smith (2010) used tree kernels to search for the alignment that 1 Performed while faculty at Johns Hop"
P13-2123,C10-1131,0,0.0630786,"Missing"
P13-2123,N10-1145,0,\N,Missing
P13-2123,W08-1301,0,\N,Missing
P13-2123,D08-1084,0,\N,Missing
P13-2123,P11-1131,0,\N,Missing
P13-2123,P07-1003,0,\N,Missing
P13-2123,J08-4005,1,\N,Missing
P13-2123,N10-1112,0,\N,Missing
P13-2123,P06-1009,0,\N,Missing
P14-1092,P12-1007,0,0.0709132,"ng model for Japanense why QA. In terms of discourse parsing, Verberne et al. (2007) conducted an initial evaluation of the utility of RST structures to why QA by evaluating Figure 1: Architecture of the reranking framework for QA. performance on a small sample of seven WSJ articles drawn from the RST Treebank (Carlson et al., 2003). They later concluded that while discourse parsing appears to be useful for QA, automated discourse parsing tools are required before this approach can be tested at scale (Verberne et al., 2010). Inspired by this previous work and recent work in discourse parsing (Feng and Hirst, 2012), our work is the first to systematically explore structured discourse features driven by several discourse representations, combine discourse with lexical semantic models, and evaluate these representations on thousands of questions using both in-domain and cross-domain experiments. 3 Approach The proposed answer reranking component is embedded in the QA framework illustrated in Figure 1. This framework functions in two distinct scenarios, which use the same AR model, but differ in the way candidate answers are retrieved: CQA: In this scenario, the task is defined as reranking all the user-po"
P14-1092,P07-1059,0,0.0858365,"S, Yih et al. (2013) recently addressed the problem of answer sentence selection and demonstrated that LS models, including recurrent neural network language models (RNNLM), have a higher contribution to overall performance than exploiting syntactic analysis. We extend this work by showing that discourse models coupled with LS achieve the best performance for NF AR. The related work on NF QA is considerably more scarce, but several trends are clear. First, most NF QA approaches tend to use multiple similarity models (information retrieval or alignment) as features in discriminative rerankers (Riezler et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2010; Surdeanu et al., 2011). Second, and more relevant to this work, all these approaches focus either on bag-of-word representations or linguistic structures that are restricted to single sentences (e.g., syntactic dependencies, semantic roles, or standalone discourse cue phrases). Answering how questions using a single discourse marker, by, was previously explored by Prager et al. (2000), who searched for by followed by a present participle (e.g. by *ing) to elevate answer candidates in a ranking framework. Verberne et al. (2011) extracted 4"
P14-1092,I08-1055,0,0.258695,"ecently addressed the problem of answer sentence selection and demonstrated that LS models, including recurrent neural network language models (RNNLM), have a higher contribution to overall performance than exploiting syntactic analysis. We extend this work by showing that discourse models coupled with LS achieve the best performance for NF AR. The related work on NF QA is considerably more scarce, but several trends are clear. First, most NF QA approaches tend to use multiple similarity models (information retrieval or alignment) as features in discriminative rerankers (Riezler et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2010; Surdeanu et al., 2011). Second, and more relevant to this work, all these approaches focus either on bag-of-word representations or linguistic structures that are restricted to single sentences (e.g., syntactic dependencies, semantic roles, or standalone discourse cue phrases). Answering how questions using a single discourse marker, by, was previously explored by Prager et al. (2000), who searched for by followed by a present participle (e.g. by *ing) to elevate answer candidates in a ranking framework. Verberne et al. (2011) extracted 47 cue phrases such as because f"
P14-1092,J11-2003,1,0.622072,"tion and demonstrated that LS models, including recurrent neural network language models (RNNLM), have a higher contribution to overall performance than exploiting syntactic analysis. We extend this work by showing that discourse models coupled with LS achieve the best performance for NF AR. The related work on NF QA is considerably more scarce, but several trends are clear. First, most NF QA approaches tend to use multiple similarity models (information retrieval or alignment) as features in discriminative rerankers (Riezler et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2010; Surdeanu et al., 2011). Second, and more relevant to this work, all these approaches focus either on bag-of-word representations or linguistic structures that are restricted to single sentences (e.g., syntactic dependencies, semantic roles, or standalone discourse cue phrases). Answering how questions using a single discourse marker, by, was previously explored by Prager et al. (2000), who searched for by followed by a present participle (e.g. by *ing) to elevate answer candidates in a ranking framework. Verberne et al. (2011) extracted 47 cue phrases such as because from a small collection of web documents, and us"
P14-1092,P13-1171,0,0.205167,"he 52nd Annual Meeting of the Association for Computational Linguistics, pages 977–986, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics perform single-sentence models when answers span multiple sentences. 4. We demonstrate good domain transfer performance between these corpora, suggesting that answer discourse structures are largely independent of domain, and thus broadly applicable to NF QA. 2 Related Work The body of work on factoid QA is too broad to be discussed here (see, e.g., the TREC workshops for an overview). However, in the context of LS, Yih et al. (2013) recently addressed the problem of answer sentence selection and demonstrated that LS models, including recurrent neural network language models (RNNLM), have a higher contribution to overall performance than exploiting syntactic analysis. We extend this work by showing that discourse models coupled with LS achieve the best performance for NF AR. The related work on NF QA is considerably more scarce, but several trends are clear. First, most NF QA approaches tend to use multiple similarity models (information retrieval or alignment) as features in discriminative rerankers (Riezler et al., 2007"
P14-1092,P13-1170,0,0.0651051,"Missing"
P14-1092,W01-1605,0,\N,Missing
P14-1092,J10-2003,0,\N,Missing
P17-2049,D13-1161,0,0.013902,"ticSearch1 query against S. We take the top 200 hits, run Open IE v4,2 and aggregate the resulting tuples over all a 2 A and over all questions in Qtr to create the tuple KB (T ).3 Related Work We discuss two classes of related work: retrievalbased web question-answering (simple reasoning with large scale KB) and science questionanswering (complex reasoning with small KB). Web QA: There exist several systems for retrieval-based Web QA problems (Ferrucci et al., 2010; Brill et al., 2002). While structured KBs such as Freebase have been used in many (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013), such approaches are limited by the coverage of the data. QA systems using semistructured Open IE tuples (Fader et al., 2013, 2014; Yin et al., 2015) or automatically extracted web tables (Sun et al., 2016; Pasupat and Liang, 2015) have broader coverage but are limited to simple questions with a single query. 3.2 Tuple Selection Given a multiple-choice question qa with question text q and answer choices A={ai }, we select the most relevant tuples from T and S as follows. Selecting from Tuple KB: We use an inverted index to find the 1,000 tuples that have the most overlapping tokens with quest"
P17-2049,P15-1142,0,0.0192547,"retrievalbased web question-answering (simple reasoning with large scale KB) and science questionanswering (complex reasoning with small KB). Web QA: There exist several systems for retrieval-based Web QA problems (Ferrucci et al., 2010; Brill et al., 2002). While structured KBs such as Freebase have been used in many (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013), such approaches are limited by the coverage of the data. QA systems using semistructured Open IE tuples (Fader et al., 2013, 2014; Yin et al., 2015) or automatically extracted web tables (Sun et al., 2016; Pasupat and Liang, 2015) have broader coverage but are limited to simple questions with a single query. 3.2 Tuple Selection Given a multiple-choice question qa with question text q and answer choices A={ai }, we select the most relevant tuples from T and S as follows. Selecting from Tuple KB: We use an inverted index to find the 1,000 tuples that have the most overlapping tokens with question tokens tok (qa).4 We also filter out any tuples that overlap only with tok (q) as they do not support any answer. We compute the normalized TF-IDF score by treating the question, q, as a query and each tuple, t, as a Science QA:"
P17-2049,D13-1160,0,0.394715,"subgraph that connects terms in the question and answer via rows in a set of curated tables, and solves the optimization problem using Integer Linear Programming (ILP). We similarly want to search for an optimal subgraph. However, a large, automatically extracted tuple KB makes the reasoning context different on three fronts: (a) unlike reasoning with tables, chaining tuples is less important and reliable as join rules aren’t Introduction Effective question answering (QA) systems have been a long-standing quest of AI research. Structured curated KBs have been used successfully for this task (Berant et al., 2013; Berant and Liang, 2014). However, these KBs are expensive to build and typically domain-specific. Automatically constructed open vocabulary (subject; predicate; object) style tuples have broader coverage, but have only been used for simple questions where a single tuple suffices (Fader et al., 2014; Yin et al., 2015). Our goal in this work is to develop a QA system that can perform reasoning with Open IE (Banko 311 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 311–316 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Associati"
P17-2049,W02-1033,0,0.0898658,"or each multiplechoice question (q, A) 2 Qtr and each choice a 2 A, we use all non-stopword stemmed tokens in q and a as an ElasticSearch1 query against S. We take the top 200 hits, run Open IE v4,2 and aggregate the resulting tuples over all a 2 A and over all questions in Qtr to create the tuple KB (T ).3 Related Work We discuss two classes of related work: retrievalbased web question-answering (simple reasoning with large scale KB) and science questionanswering (complex reasoning with small KB). Web QA: There exist several systems for retrieval-based Web QA problems (Ferrucci et al., 2010; Brill et al., 2002). While structured KBs such as Freebase have been used in many (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013), such approaches are limited by the coverage of the data. QA systems using semistructured Open IE tuples (Fader et al., 2013, 2014; Yin et al., 2015) or automatically extracted web tables (Sun et al., 2016; Pasupat and Liang, 2015) have broader coverage but are limited to simple questions with a single query. 3.2 Tuple Selection Given a multiple-choice question qa with question text q and answer choices A={ai }, we select the most relevant tuples from T and S a"
P17-2049,P13-1158,0,0.00985614,"l questions in Qtr to create the tuple KB (T ).3 Related Work We discuss two classes of related work: retrievalbased web question-answering (simple reasoning with large scale KB) and science questionanswering (complex reasoning with small KB). Web QA: There exist several systems for retrieval-based Web QA problems (Ferrucci et al., 2010; Brill et al., 2002). While structured KBs such as Freebase have been used in many (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013), such approaches are limited by the coverage of the data. QA systems using semistructured Open IE tuples (Fader et al., 2013, 2014; Yin et al., 2015) or automatically extracted web tables (Sun et al., 2016; Pasupat and Liang, 2015) have broader coverage but are limited to simple questions with a single query. 3.2 Tuple Selection Given a multiple-choice question qa with question text q and answer choices A={ai }, we select the most relevant tuples from T and S as follows. Selecting from Tuple KB: We use an inverted index to find the 1,000 tuples that have the most overlapping tokens with question tokens tok (qa).4 We also filter out any tuples that overlap only with tok (q) as they do not support any answer. We comp"
P17-2049,P14-1133,0,\N,Missing
P17-2049,D15-1080,1,\N,Missing
P19-1263,D13-1160,0,0.0553557,"al structure from text. Instead of an ILP program, Jansen et al. (2017) train a latent ranking perceptron using features from aggregated syntactic structures from multiple sentences. However, their system operates at the detailed (and often noisy) level of dependency graphs, whereas we identify entities and let the model learn implicit relations and their compositions. Knowledge Graph QA. QA datasets on knowledge graphs such as Freebase (Bollacker et al., 2008), require systems to map queries to a single relation (Bordes et al., 2015), a path (Guu et al., 2015), or complex structured queries (Berant et al., 2013) over these graphs. While early models (Lao et al., 2011; Gardner and Mitchell, 2015) focused on creating path-based features, recent neural models (Guu et al., 2015; Das et al., 2017; Toutanova et al., 2016) encode the entities and relations along a path and compose them using recurrent networks. Importantly, the input knowledge graphs have entities and relations that are shared across all training and test examples, which the model can exploit during learning (e.g., via learned entity and relation embeddings). When reasoning with text, our model must learn these representations purely based"
P19-1263,N19-1240,0,0.0200561,"Missing"
P19-1263,E17-1013,0,0.107967,"knowledge from multiple sentences or passages, a key aspect of natural language understanding. To remedy this, new datasets (Weston et al., 2015; Welbl et al., 2018; Khashabi et al., 2018a; Mihaylov et al., 2018) have been proposed, ∗ Work performed while doing an internship at the Allen Institute for Artificial Intelligence. requiring a system to combine information from multiple sentences in order to arrive at the answer, referred to as multi-hop reasoning. Multi-hop reasoning has been studied for question answering (QA) over structured knowledge graphs (Lao et al., 2011; Guu et al., 2015; Das et al., 2017). Many of the successful models explicitly identify paths in the knowledge graph that led to the answer. A strength of these models is high interpretability, arising from explicit pathbased reasoning over the underlying graph structure. However, they cannot be directly applied to QA in the absence of such structure. Consequently, most multi-hop RC models over unstructured text (Dhingra et al., 2017; Hu et al., 2018) extend standard attention-based models from RC by iteratively updating the attention to indirectly “hop” over different parts of the text. Recently, graph-based models (Song et al."
P19-1263,N18-2007,0,0.106605,"et al., 2018) have encouraged research in multi-hop QA over text. The resulting multi-hop models can be categorized into state-based and graph-based reasoning models. State-based reasoning models (Dhingra et al., 2017; Shen et al., 2017; Hu et al., 2018) are closer to a standard attention-based RC model with an additional “state” representation that is iteratively updated. The changing state representation results in the model focusing on different parts of the passage during each iteration, allowing it to combine information from different parts of the passage. Graph-based reasoning models (Dhingra et al., 2018; Cao et al., 2018; Song et al., 2018), on the other hand, create graphs over entities within the passages and update entity representations via recurrent or convolutional networks. In contrast, our approach explicitly identifies paths connecting entities in the question to the answer choices. Semi-structured QA. Our model is closer to Integer Linear Programming (ILP) based methods (Khashabi et al., 2016; Khot et al., 2017; Khashabi et al., 2018b), which define an ILP program to find optimal support graphs for connecting the question to the choices through a semi-structured knowledge represent"
P19-1263,P17-1168,0,0.377572,"order to arrive at the answer, referred to as multi-hop reasoning. Multi-hop reasoning has been studied for question answering (QA) over structured knowledge graphs (Lao et al., 2011; Guu et al., 2015; Das et al., 2017). Many of the successful models explicitly identify paths in the knowledge graph that led to the answer. A strength of these models is high interpretability, arising from explicit pathbased reasoning over the underlying graph structure. However, they cannot be directly applied to QA in the absence of such structure. Consequently, most multi-hop RC models over unstructured text (Dhingra et al., 2017; Hu et al., 2018) extend standard attention-based models from RC by iteratively updating the attention to indirectly “hop” over different parts of the text. Recently, graph-based models (Song et al., 2018; Cao et al., 2018) have been proposed for the WikiHop dataset (Welbl et al., 2018). Nevertheless, these models still only implicitly combine knowl2737 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2737–2747 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics edge from all passages, and are therefore unab"
P19-1263,D17-1082,0,0.0658973,"Missing"
P19-1263,Q15-1015,1,0.874689,"ed entity and relation embeddings). When reasoning with text, our model must learn these representations purely based on their local context. 3 Approach Overview We focus on the multiple-choice RC setting: given a question and a set of passages, the task is to find the correct answer among a predefined set of candidates. The proposed approach can be applied to m-hop reasoning, as discussed briefly in the corresponding sections for path extraction, encoding, and scoring. Since our target datasets primarily need 2-hop reasoning3 and the potential of semantic drift with increased number of hops (Fried et al., 2015; Khashabi et al., 2019), we focus on and assess the case of 2-hop paths (m = 2). As discussed later (see Footnote 4), our path-extraction step scales exponentially with m. Using m = 2 keeps this step tractable, while still covering almost all examples in our target datasets. In WikiHop, a question Q is given in the form of a tuple (he , r, ?), where he represents the head en3 We found that most WikiHop questions can be answered with 2 hops and OpenBookQA also targets 2-hop questions. tity and r represents the relation between he and the unknown tail entity. The task is to select the unknown t"
P19-1263,D11-1049,0,0.309664,"ese tasks may not be able to compose knowledge from multiple sentences or passages, a key aspect of natural language understanding. To remedy this, new datasets (Weston et al., 2015; Welbl et al., 2018; Khashabi et al., 2018a; Mihaylov et al., 2018) have been proposed, ∗ Work performed while doing an internship at the Allen Institute for Artificial Intelligence. requiring a system to combine information from multiple sentences in order to arrive at the answer, referred to as multi-hop reasoning. Multi-hop reasoning has been studied for question answering (QA) over structured knowledge graphs (Lao et al., 2011; Guu et al., 2015; Das et al., 2017). Many of the successful models explicitly identify paths in the knowledge graph that led to the answer. A strength of these models is high interpretability, arising from explicit pathbased reasoning over the underlying graph structure. However, they cannot be directly applied to QA in the absence of such structure. Consequently, most multi-hop RC models over unstructured text (Dhingra et al., 2017; Hu et al., 2018) extend standard attention-based models from RC by iteratively updating the attention to indirectly “hop” over different parts of the text. Rece"
P19-1263,D15-1173,0,0.0241678,"a latent ranking perceptron using features from aggregated syntactic structures from multiple sentences. However, their system operates at the detailed (and often noisy) level of dependency graphs, whereas we identify entities and let the model learn implicit relations and their compositions. Knowledge Graph QA. QA datasets on knowledge graphs such as Freebase (Bollacker et al., 2008), require systems to map queries to a single relation (Bordes et al., 2015), a path (Guu et al., 2015), or complex structured queries (Berant et al., 2013) over these graphs. While early models (Lao et al., 2011; Gardner and Mitchell, 2015) focused on creating path-based features, recent neural models (Guu et al., 2015; Das et al., 2017; Toutanova et al., 2016) encode the entities and relations along a path and compose them using recurrent networks. Importantly, the input knowledge graphs have entities and relations that are shared across all training and test examples, which the model can exploit during learning (e.g., via learned entity and relation embeddings). When reasoning with text, our model must learn these representations purely based on their local context. 3 Approach Overview We focus on the multiple-choice RC settin"
P19-1263,D15-1038,0,0.124102,"be able to compose knowledge from multiple sentences or passages, a key aspect of natural language understanding. To remedy this, new datasets (Weston et al., 2015; Welbl et al., 2018; Khashabi et al., 2018a; Mihaylov et al., 2018) have been proposed, ∗ Work performed while doing an internship at the Allen Institute for Artificial Intelligence. requiring a system to combine information from multiple sentences in order to arrive at the answer, referred to as multi-hop reasoning. Multi-hop reasoning has been studied for question answering (QA) over structured knowledge graphs (Lao et al., 2011; Guu et al., 2015; Das et al., 2017). Many of the successful models explicitly identify paths in the knowledge graph that led to the answer. A strength of these models is high interpretability, arising from explicit pathbased reasoning over the underlying graph structure. However, they cannot be directly applied to QA in the absence of such structure. Consequently, most multi-hop RC models over unstructured text (Dhingra et al., 2017; Hu et al., 2018) extend standard attention-based models from RC by iteratively updating the attention to indirectly “hop” over different parts of the text. Recently, graph-based"
P19-1263,P82-1020,0,0.809148,"Missing"
P19-1263,J17-2005,1,0.853118,"e models require a manually authored and tuned ILP program, and need to convert text into a semi-structured representation—a process that is often noisy (such as using Open IE tu2 1 The source code is available at https://github. com/allenai/PathNet Other systems, such as by Zhong et al. (2019), have recently appeared on the WikiHop leaderboard (http:// qangaroo.cs.ucl.ac.uk/leaderboard.html). 2738 ples (Khot et al., 2017), SRL frames (Khashabi et al., 2018b)). Our model, on the other hand, is trained end-to-end, and discover relevant relational structure from text. Instead of an ILP program, Jansen et al. (2017) train a latent ranking perceptron using features from aggregated syntactic structures from multiple sentences. However, their system operates at the detailed (and often noisy) level of dependency graphs, whereas we identify entities and let the model learn implicit relations and their compositions. Knowledge Graph QA. QA datasets on knowledge graphs such as Freebase (Bollacker et al., 2008), require systems to map queries to a single relation (Bordes et al., 2015), a path (Guu et al., 2015), or complex structured queries (Berant et al., 2013) over these graphs. While early models (Lao et al.,"
P19-1263,P17-1147,0,0.0227441,"We show that our approach outperforms prior models on the multi-hop Wikihop dataset, and also can be generalized to apply to the OpenBookQA dataset, matching stateof-the-art performance. 1 Candidates: records, ... chrysalis records, emi group, virgin Answer: chrysalis records Paths: (“Always Breaking My Heart” ... single from ... A Woman and a Man) (A Woman and a Man ... released ... by ... Chrysalis Records) Figure 1: Example illustrating our proposed path extraction and reasoning approach. Introduction Many reading comprehension (RC) datasets (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017) have been proposed recently to evaluate a system’s ability to answer a question from a given text passage. However, most of the questions in these datasets can be answered by using only a single sentence or passage. As a result, systems designed for these tasks may not be able to compose knowledge from multiple sentences or passages, a key aspect of natural language understanding. To remedy this, new datasets (Weston et al., 2015; Welbl et al., 2018; Khashabi et al., 2018a; Mihaylov et al., 2018) have been proposed, ∗ Work performed while doing an internship at the Allen Institute for Artific"
P19-1263,N18-1023,0,0.286557,"d reasoning approach. Introduction Many reading comprehension (RC) datasets (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017) have been proposed recently to evaluate a system’s ability to answer a question from a given text passage. However, most of the questions in these datasets can be answered by using only a single sentence or passage. As a result, systems designed for these tasks may not be able to compose knowledge from multiple sentences or passages, a key aspect of natural language understanding. To remedy this, new datasets (Weston et al., 2015; Welbl et al., 2018; Khashabi et al., 2018a; Mihaylov et al., 2018) have been proposed, ∗ Work performed while doing an internship at the Allen Institute for Artificial Intelligence. requiring a system to combine information from multiple sentences in order to arrive at the answer, referred to as multi-hop reasoning. Multi-hop reasoning has been studied for question answering (QA) over structured knowledge graphs (Lao et al., 2011; Guu et al., 2015; Das et al., 2017). Many of the successful models explicitly identify paths in the knowledge graph that led to the answer. A strength of these models is high interpretability, arising from"
P19-1263,P17-2049,1,0.935411,"el focusing on different parts of the passage during each iteration, allowing it to combine information from different parts of the passage. Graph-based reasoning models (Dhingra et al., 2018; Cao et al., 2018; Song et al., 2018), on the other hand, create graphs over entities within the passages and update entity representations via recurrent or convolutional networks. In contrast, our approach explicitly identifies paths connecting entities in the question to the answer choices. Semi-structured QA. Our model is closer to Integer Linear Programming (ILP) based methods (Khashabi et al., 2016; Khot et al., 2017; Khashabi et al., 2018b), which define an ILP program to find optimal support graphs for connecting the question to the choices through a semi-structured knowledge representation. However, these models require a manually authored and tuned ILP program, and need to convert text into a semi-structured representation—a process that is often noisy (such as using Open IE tu2 1 The source code is available at https://github. com/allenai/PathNet Other systems, such as by Zhong et al. (2019), have recently appeared on the WikiHop leaderboard (http:// qangaroo.cs.ucl.ac.uk/leaderboard.html). 2738 ples"
P19-1263,D18-1260,1,0.813308,"troduction Many reading comprehension (RC) datasets (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017) have been proposed recently to evaluate a system’s ability to answer a question from a given text passage. However, most of the questions in these datasets can be answered by using only a single sentence or passage. As a result, systems designed for these tasks may not be able to compose knowledge from multiple sentences or passages, a key aspect of natural language understanding. To remedy this, new datasets (Weston et al., 2015; Welbl et al., 2018; Khashabi et al., 2018a; Mihaylov et al., 2018) have been proposed, ∗ Work performed while doing an internship at the Allen Institute for Artificial Intelligence. requiring a system to combine information from multiple sentences in order to arrive at the answer, referred to as multi-hop reasoning. Multi-hop reasoning has been studied for question answering (QA) over structured knowledge graphs (Lao et al., 2011; Guu et al., 2015; Das et al., 2017). Many of the successful models explicitly identify paths in the knowledge graph that led to the answer. A strength of these models is high interpretability, arising from explicit pathbased reason"
P19-1263,D14-1162,0,0.0912184,"(ck ) = score(pkj ). (1) Figure 2: Architecture of the proposed model. Q, passages p1 and p2 , candidate ck , and the locations of he , e1 , e01 , ck in these passages: (1) Embedding and Encoding (§ 5.1) (2) Path Encoding (§ 5.2) (3) Path Scoring (§ 5.3). In Figure 3, we present the model architecture for these three components used for scoring the paths. 5.1 Embedding and Encoding We start by describing how we embed and contextually encode all pieces of text: question, supporting passages, and candidate answer choices. For word embedding, we use pretrained 300 dimensional vectors from GloVe (Pennington et al., 2014), randomly initializing vectors for out of vocabulary (OOV) words. For contextual encoding, we use bi-directional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997). Let T , U , and V represent the number of tokens in the p-th supporting passage, question, and k-th answer candidate, respectively. The final encoded representation for the p-th supporting passage can be obtained by stacking these vectors into Sp ∈ RT ×H , where H is the number of hidden units for the BiLSTMs. The sequence level encoding for the question, Q ∈ RU ×H , and for the k-th candidate answer, Ck ∈ RV ×H , are obtained simil"
P19-1263,N18-1202,0,0.0165766,"several recently proposed multi-hop 5 https://spacy.io/api/tokenizer QA models. We show the best results from each of the competing entries. Welbl et al. (2018) presented the results of BiDAF (Seo et al., 2017) on the WikiHop dataset. Dhingra et al. (2018) incorporated coreference connections inside GRU network to capture coreference links while obtaining the contextual representation. Recently, Cao et al. (2018) and Song et al. (2018) proposed graph neural network approaches for multi-hop reading comprehension. While the high level idea is similar for these work, Cao et al. (2018) used ELMo (Peters et al., 2018) for a contextual embedding, which has proven to be very useful in the recent past in many NLP tasks. As seen in Table 1, our proposed model PathNet significantly outperforms prior approaches on WikiHop. Additionally, we benefit from interpretability: unlike these prior methods, our model allows identifying specific entity chains that led to the predicted answer. Table 2 presents results on the OpenBookQA dataset. We compare with the Knowledge Enhanced Reader (KER) model (Mihaylov et al., 2018). The variants reflect the source from which the model retrieves relevant knowledge: the open book (O"
P19-1263,D16-1264,0,0.0502027,"via these explicit paths through the passages. We show that our approach outperforms prior models on the multi-hop Wikihop dataset, and also can be generalized to apply to the OpenBookQA dataset, matching stateof-the-art performance. 1 Candidates: records, ... chrysalis records, emi group, virgin Answer: chrysalis records Paths: (“Always Breaking My Heart” ... single from ... A Woman and a Man) (A Woman and a Man ... released ... by ... Chrysalis Records) Figure 1: Example illustrating our proposed path extraction and reasoning approach. Introduction Many reading comprehension (RC) datasets (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017) have been proposed recently to evaluate a system’s ability to answer a question from a given text passage. However, most of the questions in these datasets can be answered by using only a single sentence or passage. As a result, systems designed for these tasks may not be able to compose knowledge from multiple sentences or passages, a key aspect of natural language understanding. To remedy this, new datasets (Weston et al., 2015; Welbl et al., 2018; Khashabi et al., 2018a; Mihaylov et al., 2018) have been proposed, ∗ Work performed while doing an"
P19-1263,N19-1270,0,0.255314,"Missing"
P19-1263,P16-1136,0,0.0175644,"operates at the detailed (and often noisy) level of dependency graphs, whereas we identify entities and let the model learn implicit relations and their compositions. Knowledge Graph QA. QA datasets on knowledge graphs such as Freebase (Bollacker et al., 2008), require systems to map queries to a single relation (Bordes et al., 2015), a path (Guu et al., 2015), or complex structured queries (Berant et al., 2013) over these graphs. While early models (Lao et al., 2011; Gardner and Mitchell, 2015) focused on creating path-based features, recent neural models (Guu et al., 2015; Das et al., 2017; Toutanova et al., 2016) encode the entities and relations along a path and compose them using recurrent networks. Importantly, the input knowledge graphs have entities and relations that are shared across all training and test examples, which the model can exploit during learning (e.g., via learned entity and relation embeddings). When reasoning with text, our model must learn these representations purely based on their local context. 3 Approach Overview We focus on the multiple-choice RC setting: given a question and a set of passages, the task is to find the correct answer among a predefined set of candidates. The"
P19-1263,W17-2623,0,0.0230114,"s through the passages. We show that our approach outperforms prior models on the multi-hop Wikihop dataset, and also can be generalized to apply to the OpenBookQA dataset, matching stateof-the-art performance. 1 Candidates: records, ... chrysalis records, emi group, virgin Answer: chrysalis records Paths: (“Always Breaking My Heart” ... single from ... A Woman and a Man) (A Woman and a Man ... released ... by ... Chrysalis Records) Figure 1: Example illustrating our proposed path extraction and reasoning approach. Introduction Many reading comprehension (RC) datasets (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017) have been proposed recently to evaluate a system’s ability to answer a question from a given text passage. However, most of the questions in these datasets can be answered by using only a single sentence or passage. As a result, systems designed for these tasks may not be able to compose knowledge from multiple sentences or passages, a key aspect of natural language understanding. To remedy this, new datasets (Weston et al., 2015; Welbl et al., 2018; Khashabi et al., 2018a; Mihaylov et al., 2018) have been proposed, ∗ Work performed while doing an internship at the Allen"
P19-1263,Q18-1021,0,0.334167,"d path extraction and reasoning approach. Introduction Many reading comprehension (RC) datasets (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017) have been proposed recently to evaluate a system’s ability to answer a question from a given text passage. However, most of the questions in these datasets can be answered by using only a single sentence or passage. As a result, systems designed for these tasks may not be able to compose knowledge from multiple sentences or passages, a key aspect of natural language understanding. To remedy this, new datasets (Weston et al., 2015; Welbl et al., 2018; Khashabi et al., 2018a; Mihaylov et al., 2018) have been proposed, ∗ Work performed while doing an internship at the Allen Institute for Artificial Intelligence. requiring a system to combine information from multiple sentences in order to arrive at the answer, referred to as multi-hop reasoning. Multi-hop reasoning has been studied for question answering (QA) over structured knowledge graphs (Lao et al., 2011; Guu et al., 2015; Das et al., 2017). Many of the successful models explicitly identify paths in the knowledge graph that led to the answer. A strength of these models is high interpre"
Q15-1015,E09-1005,0,0.0194689,"Missing"
Q15-1015,P14-1023,0,0.0289107,"nalysis already performed by the CR model. Mikolov et al. (2013) to this QA task. In particular, we use their skip-gram model with hierarchical sampling. This model predicts the context of a word given the word itself, and through this process embeds words into a latent conceptual space with a fixed number of dimensions. Consequently, related words tend to have vector representations that are close to each other in this space. This type of predictive algorithm has been found to perform considerably better than count-based approaches to distributional similarity on a variety of semantic tasks (Baroni et al., 2014). We derive four LS measures from these vectors, which are then are included as features in the reranker. The first is a measure of the overall similarity of the question and answer candidate, which is computed as the cosine similarity between two composite vectors. These composite vectors are assembled by summing the vectors for individual question (or answer candidate) words, and re-normalizing this composite vector to unit length.5 In addition to this overall similarity score, we compute the pairwise similarities between each word in the question and answer candidates, and include as featur"
Q15-1015,S13-1002,0,0.0187129,"-order NNLM: In the NNLM setting, we use the cosine similarity of vectors as interpolation weights and to choose the nearest neighbors: scos (w(i), w(j)) = w(i) · w(j) ||w(i) |w(j)|| (9) We found that applying the softmax function to each term’s vector of k-highest scos similarities, to ensure all interpolation weights are positive and have a consistent range across terms, improved performance. As such, all higher-order NNLM models use this softmax normalization. 203 The resulting interpolation can be conceptualized in several ways. Viewing cosine similarity as a representation of entailment (Beltagy et al., 2013), the higher-order NNLM model reflects multiple-hop inference on top of the corresponding association graph, similar to the higher-order alignment model. The interpolation could also be viewed as smoothing term representations in vector space, averaging each term’s vector with its nearest neighbors according to their cosine similarity. Higher-order Hybrid Model: We also implement a hybrid model, which interpolates the alignment distribution vectors, but using the pairwise cosine similarities from the NNLM setting: X w ˆ A (i) = scos (wN (i), wN (j)) wA (j) j∈Nk (wN (i)) where wN (i) and wA (i)"
Q15-1015,J93-2003,0,0.0585777,"individual question (or answer candidate) words, and re-normalizing this composite vector to unit length.5 In addition to this overall similarity score, we compute the pairwise similarities between each word in the question and answer candidates, and include as features the average, minimum, and maximum pairwise similarities. 4.2 Alignment Models Berger et al. (2000) showed that learning questionto-answer transformations using a statistical machine translation (SMT) model begins to “bridge the lexical chasm” between questions and answers. We build upon this observation, using the IBM Model 1 (Brown et al., 1993) variant of Surdeanu et al. (2011) to determine the probability that a question Q is a translation of an answer A, P (Q|A): Y P (Q|A) = P (q|A) (1) q∈Q P (q|A) = (1 − λ)Pml (q|A) + λPml (q|C) X Pml (q|A) = (T (q|a)Pml (a|A)) the probabilities that the question term q is a translation of any answer term a, T (q|a), weighted by the probability that a is generated from A. The translation table for T (q|a) is computed using GIZA++ (Och and Ney, 2003). Similar to Surdeanu et al. (2011) we: (a) set Pml (q|C) to a small value for out-of-vocabulary words; (b) modify the T (.|.) distributions to guaran"
Q15-1015,de-marneffe-etal-2006-generating,0,0.0294551,"Missing"
Q15-1015,P03-1003,0,0.29025,"ent models capture complementary information, and can be combined to improve the performance of the CQA system for manner questions. 2 Related Work We focus on statistical LS methods for opendomain QA, in particular CQA tasks, such as the ones driven by Yahoo! Answers datasets (Wang et al., 2009; Jansen et al., 2014). Berger et al. (2000) were the first to propose a statistical LS model to “bridge the lexical chasm” between questions and answers for a QA task. Building off this work, a number of LS models using either words or other syntactic and semantic structures have been proposed for QA (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013; Jansen et al., 2014), or related tasks, such as semantic textual similarity (Sultan et al., 2014). However, all of these models fall into the class of first-order models. To our knowledge, this work is the first to investigate higher-order LS models for QA, which can take advantage of indirect evidence, i.e., the “neighbors of neighbors” in the association graph. Second-order lexical models have recently been brought to bear on a variety of other NLP tasks. Zapirain et al. (2013) use a second-order model"
Q15-1015,P14-1092,1,0.661709,"els, over both words and syntactic structures, can be adapted to the proposed higher-order formalism. In this latter respect we introduce a novel syntax-based variant of the neural network language model (NNLM) of Mikolov et al. (2013) that models syntactic dependencies rather than words, which allows it to capture knowledge that is complementary to that of word-based NNLMs. 3. The training process for alignment models requires a large corpus of QA pairs. Due to these resource requirements, we evaluate our higher-order LS models on a community question answering (CQA) task (Wang et al., 2009; Jansen et al., 2014) across thousands of how questions, and show that most higher-order models perform significantly better than their first-order variants. 4. We demonstrate that language models and alignment models capture complementary information, and can be combined to improve the performance of the CQA system for manner questions. 2 Related Work We focus on statistical LS methods for opendomain QA, in particular CQA tasks, such as the ones driven by Yahoo! Answers datasets (Wang et al., 2009; Jansen et al., 2014). Berger et al. (2000) were the first to propose a statistical LS model to “bridge the lexical c"
Q15-1015,D11-1049,0,0.0276153,"model based on distributional similarity (Lin, 1998) to improve a selectional preference model, and Lee et al. (2012) use a similar approach for coreference resolution. Our work expands on these ideas with models of arbitrary order, an approach to control semantic drift, and an application to QA. This work falls under the larger umbrella of algorithms for graph-based inference, which have been successfully applied to other NLP and information retrieval problems, such as relation extraction (Chakrabarti and Agarwal, 2006; Chakrabarti, 2007; Lao and Cohen, 2010), inference over knowledge bases (Lao et al., 2011), name disambiguation (Minkov et al., 2006), and search (Bhalotia et al., 2002; Tong et al., 2006). While many of these approaches use random walk algorithms, in pilot experiments we observed that random walks, such as PageRank (PR) (Page et al., 1999), tend to accumulate semantic drift from the originating node because they consider all possible paths in the graph. This semantic drift reduces the quality of the higher-order associations, which impacts QA performance. Here we implement a conservative graph traversal algorithm, similar in spirit to the “cautious bootstrapping” algorithm of Yaro"
Q15-1015,D12-1045,1,0.49906,"en et al., 2014), or related tasks, such as semantic textual similarity (Sultan et al., 2014). However, all of these models fall into the class of first-order models. To our knowledge, this work is the first to investigate higher-order LS models for QA, which can take advantage of indirect evidence, i.e., the “neighbors of neighbors” in the association graph. Second-order lexical models have recently been brought to bear on a variety of other NLP tasks. Zapirain et al. (2013) use a second-order model based on distributional similarity (Lin, 1998) to improve a selectional preference model, and Lee et al. (2012) use a similar approach for coreference resolution. Our work expands on these ideas with models of arbitrary order, an approach to control semantic drift, and an application to QA. This work falls under the larger umbrella of algorithms for graph-based inference, which have been successfully applied to other NLP and information retrieval problems, such as relation extraction (Chakrabarti and Agarwal, 2006; Chakrabarti, 2007; Lao and Cohen, 2010), inference over knowledge bases (Lao et al., 2011), name disambiguation (Minkov et al., 2006), and search (Bhalotia et al., 2002; Tong et al., 2006)."
Q15-1015,P14-2050,0,0.0139523,"ergence (JSD) between their conditional distributions.6 Let m(i, j) be the element-wise average of the vectors w(i) and w(j), that is (w(i) + w(j))/2, and let K(w, v) be the Kullback-Leibler divergence between distributions (represented as vectors) w and v: (2) (3) K(w, v) = i=0 a∈A where the probability that the question term q is generated from answer A, P (q|A), is smoothed using the prior probability that the term q is generated from the entire collection of answers C, Pml (q|C). Pml (q|A) is computed as the sum of 5 We also tried the multiplicative strategy for word-vector combination of Levy and Goldberg (2014b), but it did not improve our results. 200 |V | X wi ln wi vi (4) where |V |is the vocabulary size (and the dimensionality of w). Then the distance between words i and j is J(w(i), w(j)) = r K(w(i), m(i, j)) + K(w(j), m(i, j)) 2 (5) 6 We use the square root of the Jensen-Shannon divergence, derived from Kullback-Leibler divergence, since it is a distance metric (in particular it is finite and symmetric). We derive four additional LS features from these alignment vector representations, which parallel the features derived from the NNLM vector representations (§4.1). The first is the JSD betwee"
Q15-1015,W14-1618,0,0.0604173,"ergence (JSD) between their conditional distributions.6 Let m(i, j) be the element-wise average of the vectors w(i) and w(j), that is (w(i) + w(j))/2, and let K(w, v) be the Kullback-Leibler divergence between distributions (represented as vectors) w and v: (2) (3) K(w, v) = i=0 a∈A where the probability that the question term q is generated from answer A, P (q|A), is smoothed using the prior probability that the term q is generated from the entire collection of answers C, Pml (q|C). Pml (q|A) is computed as the sum of 5 We also tried the multiplicative strategy for word-vector combination of Levy and Goldberg (2014b), but it did not improve our results. 200 |V | X wi ln wi vi (4) where |V |is the vocabulary size (and the dimensionality of w). Then the distance between words i and j is J(w(i), w(j)) = r K(w(i), m(i, j)) + K(w(j), m(i, j)) 2 (5) 6 We use the square root of the Jensen-Shannon divergence, derived from Kullback-Leibler divergence, since it is a distance metric (in particular it is finite and symmetric). We derive four additional LS features from these alignment vector representations, which parallel the features derived from the NNLM vector representations (§4.1). The first is the JSD betwee"
Q15-1015,P98-2127,0,0.0467549,"t al., 2007; Surdeanu et al., 2011; Yao et al., 2013; Jansen et al., 2014), or related tasks, such as semantic textual similarity (Sultan et al., 2014). However, all of these models fall into the class of first-order models. To our knowledge, this work is the first to investigate higher-order LS models for QA, which can take advantage of indirect evidence, i.e., the “neighbors of neighbors” in the association graph. Second-order lexical models have recently been brought to bear on a variety of other NLP tasks. Zapirain et al. (2013) use a second-order model based on distributional similarity (Lin, 1998) to improve a selectional preference model, and Lee et al. (2012) use a similar approach for coreference resolution. Our work expands on these ideas with models of arbitrary order, an approach to control semantic drift, and an application to QA. This work falls under the larger umbrella of algorithms for graph-based inference, which have been successfully applied to other NLP and information retrieval problems, such as relation extraction (Chakrabarti and Agarwal, 2006; Chakrabarti, 2007; Lao and Cohen, 2010), inference over knowledge bases (Lao et al., 2011), name disambiguation (Minkov et al"
Q15-1015,P14-5010,1,0.0355837,"compared the effects of using either words or lemmas as the base lexical unit for the LS models, and found that words achieved higher P@1 scores in both the alignment and NNLM models on the development dataset. As such, all results reported here use words for the syntax-independent models, and tuples of words for the syntax-driven models. Content Filtering: We investigated using partof-speech (POS) tags to filter the content consid12 ered by the lexical similarity models, by excluding certain non-informative classes of words such as determiners. Using POS tags generated by Stanford’s CoreNLP (Manning et al., 2014), we filtered content to only include nouns, adjectives, verbs, and adverbs for the word-based models, and tuples where both words have one of these four POS tags for the syntax-based models. We found that this increased P@1 scores for all wordbased alignment and NNLM models (including the Levy-Goldberg (L-G) model13 ), but did not improve performance for models that used dependency representations.14 Results reported in the remainder of this paper use this POS filtering for all word-based alignment and NNLM models (including L-G’s) as well as the dependency alignment model, but not for our de"
Q15-1015,H05-1086,0,0.0431779,"a translation of an answer A, P (Q|A): Y P (Q|A) = P (q|A) (1) q∈Q P (q|A) = (1 − λ)Pml (q|A) + λPml (q|C) X Pml (q|A) = (T (q|a)Pml (a|A)) the probabilities that the question term q is a translation of any answer term a, T (q|a), weighted by the probability that a is generated from A. The translation table for T (q|a) is computed using GIZA++ (Och and Ney, 2003). Similar to Surdeanu et al. (2011) we: (a) set Pml (q|C) to a small value for out-of-vocabulary words; (b) modify the T (.|.) distributions to guarantee that the probability of translating a word to itself, i.e., T (w|w), is highest (Murdock and Croft, 2005); and (c) tune the smoothing parameter λ on a development corpus. QA systems generally use the above model to determine the global alignment probability between a given question and answer candidate, P (Q|A). A novel contribution of our work is that we also use the alignment model’s probability distributions (from a source word to destination words) as distributed representations for source words, based on the observation that words with similar alignment distributions are likely to have a similar meaning. Formally, we denote the alignment vector representation for the ith word in the vocabula"
Q15-1015,W12-3018,0,0.0275503,"Missing"
Q15-1015,J03-1002,0,0.0307592,"e translation (SMT) model begins to “bridge the lexical chasm” between questions and answers. We build upon this observation, using the IBM Model 1 (Brown et al., 1993) variant of Surdeanu et al. (2011) to determine the probability that a question Q is a translation of an answer A, P (Q|A): Y P (Q|A) = P (q|A) (1) q∈Q P (q|A) = (1 − λ)Pml (q|A) + λPml (q|C) X Pml (q|A) = (T (q|a)Pml (a|A)) the probabilities that the question term q is a translation of any answer term a, T (q|a), weighted by the probability that a is generated from A. The translation table for T (q|a) is computed using GIZA++ (Och and Ney, 2003). Similar to Surdeanu et al. (2011) we: (a) set Pml (q|C) to a small value for out-of-vocabulary words; (b) modify the T (.|.) distributions to guarantee that the probability of translating a word to itself, i.e., T (w|w), is highest (Murdock and Croft, 2005); and (c) tune the smoothing parameter λ on a development corpus. QA systems generally use the above model to determine the global alignment probability between a given question and answer candidate, P (Q|A). A novel contribution of our work is that we also use the alignment model’s probability distributions (from a source word to destinat"
Q15-1015,P07-1059,0,0.0600946,"an be combined to improve the performance of the CQA system for manner questions. 2 Related Work We focus on statistical LS methods for opendomain QA, in particular CQA tasks, such as the ones driven by Yahoo! Answers datasets (Wang et al., 2009; Jansen et al., 2014). Berger et al. (2000) were the first to propose a statistical LS model to “bridge the lexical chasm” between questions and answers for a QA task. Building off this work, a number of LS models using either words or other syntactic and semantic structures have been proposed for QA (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013; Jansen et al., 2014), or related tasks, such as semantic textual similarity (Sultan et al., 2014). However, all of these models fall into the class of first-order models. To our knowledge, this work is the first to investigate higher-order LS models for QA, which can take advantage of indirect evidence, i.e., the “neighbors of neighbors” in the association graph. Second-order lexical models have recently been brought to bear on a variety of other NLP tasks. Zapirain et al. (2013) use a second-order model based on distributional similarity (Lin, 1998)"
Q15-1015,Q14-1018,0,0.0177403,"domain QA, in particular CQA tasks, such as the ones driven by Yahoo! Answers datasets (Wang et al., 2009; Jansen et al., 2014). Berger et al. (2000) were the first to propose a statistical LS model to “bridge the lexical chasm” between questions and answers for a QA task. Building off this work, a number of LS models using either words or other syntactic and semantic structures have been proposed for QA (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013; Jansen et al., 2014), or related tasks, such as semantic textual similarity (Sultan et al., 2014). However, all of these models fall into the class of first-order models. To our knowledge, this work is the first to investigate higher-order LS models for QA, which can take advantage of indirect evidence, i.e., the “neighbors of neighbors” in the association graph. Second-order lexical models have recently been brought to bear on a variety of other NLP tasks. Zapirain et al. (2013) use a second-order model based on distributional similarity (Lin, 1998) to improve a selectional preference model, and Lee et al. (2012) use a similar approach for coreference resolution. Our work expands on thes"
Q15-1015,J11-2003,1,0.749423,"search, Berger (2000) observed that lexical matching methods are generally insufficient for QA, where questions and answers often have little to no lexical overlap (as in the case of Where should we go for breakfast? and Zoe’s Diner has great pancakes). Previous work has shown that lexical semantics (LS) models are well suited to bridging this “lexical chasm”, and at least two flavors of lexical semantics have been successfully applied to QA. The first treats QA as a monolingual alignment problem, learning associations between words (or other structures) that appear in question-answer pairs (Surdeanu et al., 2011; Yao et al., 2013). The second computes the semantic similarity between question and answer using language models acquired from relevant texts (Yih et al., 2013; Jansen et al., 2014). Here we argue that while these models begin to bridge the “lexical chasm”, many still suffer from sparsity and only capitalize on direct evidence. Returning to our example question, if we also train on the QA pair What goes well with pancakes? and hashbrowns and toast, we can use the 197 Transactions of the Association for Computational Linguistics, vol. 3, pp. 197–210, 2015. Action Editor: Sharon Goldwater. Sub"
Q15-1015,P12-1065,0,0.0121701,"disambiguation (Minkov et al., 2006), and search (Bhalotia et al., 2002; Tong et al., 2006). While many of these approaches use random walk algorithms, in pilot experiments we observed that random walks, such as PageRank (PR) (Page et al., 1999), tend to accumulate semantic drift from the originating node because they consider all possible paths in the graph. This semantic drift reduces the quality of the higher-order associations, which impacts QA performance. Here we implement a conservative graph traversal algorithm, similar in spirit to the “cautious bootstrapping” algorithm of Yarowsky (Whitney and Sarkar, 2012; Yarowsky, 1995). By constraining the traversal paths, our algorithm runs two orders of magnitude faster than PR, while controlling for semantic drift. 3 Approach The architecture of our proposed QA framework is illustrated in Figure 2. Here we evaluate both firstorder and higher-order LS models in the context of community question answering (CQA), using a large dataset of QA pairs from Yahoo! Answers1 . We use a standard CQA evaluation task (Jansen et al., 2014), where one must rank a set of usergenerated answers to a given question, such that the community-selected best answer appears in th"
Q15-1015,D13-1056,1,0.846443,"Missing"
Q15-1015,P95-1026,0,0.461793,"al., 2006), and search (Bhalotia et al., 2002; Tong et al., 2006). While many of these approaches use random walk algorithms, in pilot experiments we observed that random walks, such as PageRank (PR) (Page et al., 1999), tend to accumulate semantic drift from the originating node because they consider all possible paths in the graph. This semantic drift reduces the quality of the higher-order associations, which impacts QA performance. Here we implement a conservative graph traversal algorithm, similar in spirit to the “cautious bootstrapping” algorithm of Yarowsky (Whitney and Sarkar, 2012; Yarowsky, 1995). By constraining the traversal paths, our algorithm runs two orders of magnitude faster than PR, while controlling for semantic drift. 3 Approach The architecture of our proposed QA framework is illustrated in Figure 2. Here we evaluate both firstorder and higher-order LS models in the context of community question answering (CQA), using a large dataset of QA pairs from Yahoo! Answers1 . We use a standard CQA evaluation task (Jansen et al., 2014), where one must rank a set of usergenerated answers to a given question, such that the community-selected best answer appears in the top position. M"
Q15-1015,P13-1171,0,0.536462,"as in the case of Where should we go for breakfast? and Zoe’s Diner has great pancakes). Previous work has shown that lexical semantics (LS) models are well suited to bridging this “lexical chasm”, and at least two flavors of lexical semantics have been successfully applied to QA. The first treats QA as a monolingual alignment problem, learning associations between words (or other structures) that appear in question-answer pairs (Surdeanu et al., 2011; Yao et al., 2013). The second computes the semantic similarity between question and answer using language models acquired from relevant texts (Yih et al., 2013; Jansen et al., 2014). Here we argue that while these models begin to bridge the “lexical chasm”, many still suffer from sparsity and only capitalize on direct evidence. Returning to our example question, if we also train on the QA pair What goes well with pancakes? and hashbrowns and toast, we can use the 197 Transactions of the Association for Computational Linguistics, vol. 3, pp. 197–210, 2015. Action Editor: Sharon Goldwater. Submission batch: 12/2014; Revision batch 3/2015; Published 4/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY-NC-SA 4.0 license. t"
Q15-1015,J13-3006,1,\N,Missing
Q15-1015,C98-2122,0,\N,Missing
Q17-1017,P11-1062,0,0.0442145,"Tij , AUij , Sij represent the scores produced by Moby, WordNet, AMIE-typed, AMIEuntyped and Specificity features respectively for the schema mapping rule Si → Sj . The objective function maximizes the weighted combination of these scores. Further, the solution picked by this ILP satisfies constraints such as asymmetry, transitive closure and at most one parent per schema. We also apply an L1 sparsity penalty on X, retaining only those schema mapping edges for which the model is reasonably confident. For n schemas, there are O(n3 ) transitivity constraints which make the ILP very inefficient. Berant et al. (2011) proposed two approximations to handle a large number of transitivity rules by decomposing 240 the ILP or solving it in an incremental way. Instead we re-write the ILP rules in such a way that we can efficiently solve our mapping problem without introducing any approximations. The last two constraints of this ILP can be rewritten as follows: P j Xij ≤ 1, ∀i  AND Xij + Xjk − Xik ≤ 1, ∀hi, j, ki =⇒ If(Xij = 1) then Xjk = 0 ∀k This results in O(n2 ) constraints and makes the ILP efficient. Impact of this technique in terms of runtime is described in Section 5.3. We then use an off-the-shelf ILP"
Q17-1017,W16-1303,1,0.832991,"can be modified to do this also. We have implemented this modification (called AMIE*, described in Section 5.3), and we use it as a baseline to compare our schema clustering method (CASI) against. Finally, interactive methods have been used to create common sense knowledge bases, for example ConceptNet (Speer and Havasi, 2013; Liu and Singh, 2004) includes a substantial amount of knowledge manually contributed by people through a Web-based interface, and used in numerous applications (Faaborg and Lieberman, 2006; Dinakar et al., 2012). More recently there has been work on interactive methods (Dalvi et al., 2016; Wolfe et al., 2015; Soderland et al., 2013), which can be seen as a “machine teaching” approach to KB construction. These approaches focus on human-in-theloop methods to create domain specific knowledge bases. Such approaches are proven to be effective on domains where expert human input is available. In contrast, our goal is to create extraction techniques that need little human supervision, and result in comprehensive coverage of the target domain. 3 The Extraction Pipeline We first describe the overall extraction pipeline. The pipeline is a chain of filters and transformations, outputting"
Q17-1017,D14-1042,0,0.0131761,"in a domain they are often polysemous out of context (e.g., “have”). To handle this, we refer to verbs along with their argument types, the combination expressed as a verbal schema, e.g., (Animal,“have”,BodyPart). This allows us to distinguish 3 There are exceptions, e.g., in 4th Grade Science “bat” can refer to either the animal or the sporting implement, but these cases are rare. 236 different contextual uses of a verb without introducing a proliferation of verb sense symbols. Others have taken a similar approach of using type restrictions to express verb semantics (Pantel et al., 2007; Del Corro et al., 2014). 3.3 The Pipeline The pipeline is sketched in Figure 1 and exemplified in Table 1, and consists of six steps: 3.3.1 Sentence Selection The first step is to construct a collection of (loosely) domain-appropriate sentences from the larger corpus. There are multiple ways this could be done, but in our case we found the most effective way was as follows: a. List the core topics in the domain of interest (science), here producing 81 topics derived from syllabus guides. b. For each topic, author 1-3 query templates, parameterized using one or more of the 45 domain types. For example, for the topic"
Q17-1017,D11-1142,0,0.0789456,"ligence 2157 N Northlake Way Suite 110, Seattle, WA 98103 {bhavanad,nikett,peterc}@allenai.org Abstract remains elusive. Specifically, our goal is a large, high precision body of (subject,predicate,object) statements relevant to elementary science, to support a downstream QA application task. Although there are several impressive, existing resources that can contribute to our endeavor, e.g., NELL (Carlson et al., 2010), ConceptNet (Speer and Havasi, 2013), WordNet (Fellbaum, 1998), WebChild (Tandon et al., 2014), Yago (Suchanek et al., 2007), FreeBase (Bollacker et al., 2008), and ReVerb-15M (Fader et al., 2011), their applicability is limited by both Our goal is to construct a domain-targeted, high precision knowledge base (KB), containing general (subject,predicate,object) statements about the world, in support of a downstream question-answering (QA) application. Despite recent advances in information extraction (IE) techniques, no suitable resource for our task already exists; existing resources are either too noisy, too named-entity centric, or too incomplete, and typically have not been constructed with a clear scope or purpose. To address these, we have created a domaintargeted, high precision"
Q17-1017,C14-1207,0,0.0145501,"ded sequence of open information extraction, crowdsourcing, and learning predicate relationships are used to produce high precision tuples relevant to the domain of interest. output’s precision, and learn and apply relationships between predicates. The task of finding and exploiting relationships between different predicates requires identifying both equivalence between relations (e.g., clustering to find paraphrases), and implication (hierarchical organization of relations). One class of approach is to use existing resources, e.g., verb taxonomies, as a source of verbal relationships, e.g., (Grycner and Weikum, 2014), (Grycner et al., 2015). However, the hierarchical relationship between verbs, out of context, is often unclear, and some verbs, e.g., “have”, are ambiguous. To address this, we characterize semantic relationships not only by a verb but also by the types of its arguments. A second class of approach is to induce semantic equivalence from data, e.g., using algorithms such as DIRT (Lin and Pantel, 2001), RESOLVER (Yates and Etzioni, 2009), WiseNet (Moro and Navigli, 2012), and AMIE (Gal´arraga et al., 2013). These allow relational equivalences to be inferred, but are also noisy. In our pipeline,"
Q17-1017,D15-1113,0,0.0142641,"tion extraction, crowdsourcing, and learning predicate relationships are used to produce high precision tuples relevant to the domain of interest. output’s precision, and learn and apply relationships between predicates. The task of finding and exploiting relationships between different predicates requires identifying both equivalence between relations (e.g., clustering to find paraphrases), and implication (hierarchical organization of relations). One class of approach is to use existing resources, e.g., verb taxonomies, as a source of verbal relationships, e.g., (Grycner and Weikum, 2014), (Grycner et al., 2015). However, the hierarchical relationship between verbs, out of context, is often unclear, and some verbs, e.g., “have”, are ambiguous. To address this, we characterize semantic relationships not only by a verb but also by the types of its arguments. A second class of approach is to induce semantic equivalence from data, e.g., using algorithms such as DIRT (Lin and Pantel, 2001), RESOLVER (Yates and Etzioni, 2009), WiseNet (Moro and Navigli, 2012), and AMIE (Gal´arraga et al., 2013). These allow relational equivalences to be inferred, but are also noisy. In our pipeline, we combine these two ap"
Q17-1017,D12-1048,0,0.03026,"es is to steer the search engine to domain-relevant text. c. For each template, automatically instantiate its type(s) in all possible ways using the domain vocabulary members of those types. d. Use each instantiation as a search query over the corpus, and collect sentences in the top (here, 10) documents retrieved. In our case, this resulted in a generally domainrelevant corpus of 7M sentences. 3.3.2 Tuple Generation Second, we run an open information extraction system over the sentences to generate an initial set of (np, vp, np) tuples. In our case, we use OpenIE 4.2 (Soderland et al., 2013; Mausam et al., 2012). 3.3.3 Headword Extraction and Filtering Third, the np arguments are replaced with their headwords, by applying a simple headword filtering utility. We discard tuples with infrequent vps or verbal schemas (here vp frequency &lt; 10, schema frequency &lt; 5). Pipeline Example Outputs: Inputs: corpus + vocabulary + types 1. Sentence selection: “In addition, green leaves have chlorophyll.”) 2. Tuple Generation: (“green leaves” “have” “chlorophyll”) 3. Headword Extraction: (“leaf” “have” “chlorophyll”) 4. Refinement and Scoring: (“leaf” “have” “chlorophyll”) @0.89 (score) 5. Phrasal tuple generation: ("
Q17-1017,N07-1071,0,0.0391396,"enging, because even within a domain they are often polysemous out of context (e.g., “have”). To handle this, we refer to verbs along with their argument types, the combination expressed as a verbal schema, e.g., (Animal,“have”,BodyPart). This allows us to distinguish 3 There are exceptions, e.g., in 4th Grade Science “bat” can refer to either the animal or the sporting implement, but these cases are rare. 236 different contextual uses of a verb without introducing a proliferation of verb sense symbols. Others have taken a similar approach of using type restrictions to express verb semantics (Pantel et al., 2007; Del Corro et al., 2014). 3.3 The Pipeline The pipeline is sketched in Figure 1 and exemplified in Table 1, and consists of six steps: 3.3.1 Sentence Selection The first step is to construct a collection of (loosely) domain-appropriate sentences from the larger corpus. There are multiple ways this could be done, but in our case we found the most effective way was as follows: a. List the core topics in the domain of interest (science), here producing 81 topics derived from syllabus guides. b. For each topic, author 1-3 query templates, parameterized using one or more of the 45 domain types. Fo"
Q17-1017,W16-1308,0,0.05778,"Missing"
Q17-1017,N13-1008,0,0.0142423,", 2013). These allow relational equivalences to be inferred, but are also noisy. In our pipeline, we combine these two approaches together, by clustering relations using a similarity measure computed from both existing resources and data. A novel feature of our approach is that we not only cluster the (typed) relations, but also identify a canonical relation that all the other relations in a cluster can be mapped to, without recourse to human annotated training data or a target relational vocabulary (e.g., from Freebase). This makes our problem setting different from that of universal schema (Riedel et al., 2013) where the clusters of relations are not explicitly represented and mapping to canon235 ical relations can be achieved given an existing KB like Freebase. Although no existing methods can be directly applied in our problem setting, the AMIEbased schema clustering method of (Gal´arraga et al., 2014) can be modified to do this also. We have implemented this modification (called AMIE*, described in Section 5.3), and we use it as a baseline to compare our schema clustering method (CASI) against. Finally, interactive methods have been used to create common sense knowledge bases, for example Concept"
Q17-1017,D16-1252,0,0.0111098,"Missing"
Q17-1017,D11-1063,0,0.010007,".”) 2. Tuple Generation: (“green leaves” “have” “chlorophyll”) 3. Headword Extraction: (“leaf” “have” “chlorophyll”) 4. Refinement and Scoring: (“leaf” “have” “chlorophyll”) @0.89 (score) 5. Phrasal tuple generation: (“leaf” “have” “chlorophyll”) @0.89 (score) (“green leaf” “have” “chlorophyll”) @0.89 (score) 6. Relation Canonicalization: (“leaf” “have” “chlorophyll”) @0.89 (score) (“green leaf” “have” “chlorophyll”) @0.89 (score) (“leaf” “contain” “chlorophyll”) @0.89 (score) (“green leaf” “contain” “chlorophyll”) @0.89 (score) stract vs. concrete (using Turney et al’s abstractness database (Turney et al., 2011)), and whether there are any modal verbs (e.g. may, should etc.) in the original sentence. PMI features are derived from the count statistics of subject, predicate, object and entire triple in the Google n-gram corpus (Brants and Franz, 2006). 3.3.5 Phrasal Tuple Generation Fifth, for each headword tuple (n, vp, n), retrieve the original phrasal triples (np, vp, np) it was derived from, and add sub-phrase versions of these phrasal tuples to the KB. For example, if a headword tuple (cat, chase, mouse) was derived from (A black furry cat, chased, a grey mouse) then the algorithm considers adding"
S13-2045,S12-1059,0,0.00894071,"Missing"
S13-2045,P10-4003,1,0.705578,"in, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012). In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2way RTE-style tasks on educational data. In addition, a partial entailment task was piloted. We present and c"
S13-2045,N12-1021,1,0.552195,"ing textual entailment. Thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2way RTE-style tasks on educational data. In addition, a partial entailment task was piloted. We present and compare results from 9 participating teams, and discuss future directions. 1 Introduction One of the tasks in educational NLP systems is providing feedback to students in the context of exam questions, homework or intelligent tutoring. Much previous work has been devoted to the automated Since the task of making and testing a full educational dialog system is daunting, Dzikovska et al. (2012) identified a key subtask and proposed it as a new shared task for the NLP community. Student response analysis (henceforth SRA) is the task of labeling student answers with categories that could 263 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 263–274, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Example 1 Q UESTION R EF. A NS . S TUD . A NS . Example 2 Q UESTION R EF. A NS . S TUD . A NS . You used several methods to separate and identify the"
S13-2045,P11-1076,0,0.189172,"ar-Ilan University Israel dagan@cs.biu.ac.il Hoa Trang Dang NIST hoa.dang@nist.gov Abstract scoring of essays (Attali and Burstein, 2006; Shermis and Burstein, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012). In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the community a 5-way stud"
S13-2045,nielsen-etal-2008-annotating,1,0.393893,"@vulcan.com Ido Dagan Bar-Ilan University Israel dagan@cs.biu.ac.il Hoa Trang Dang NIST hoa.dang@nist.gov Abstract scoring of essays (Attali and Burstein, 2006; Shermis and Burstein, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012). In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the"
S13-2045,W05-0202,0,0.185532,"Clark Vulcan Inc. USA peterc@vulcan.com Ido Dagan Bar-Ilan University Israel dagan@cs.biu.ac.il Hoa Trang Dang NIST hoa.dang@nist.gov Abstract scoring of essays (Attali and Burstein, 2006; Shermis and Burstein, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012). In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. T"
S13-2045,R11-1063,1,0.693982,"ighted Average Precision, Recall and F1 , and computed as described in Section 4.2. We used only a majority class baseline, which labeled all facets as ‘Unaddressed’. Its performance is presented in Section 5.4 jointly with the system results. 5.4 Participants and results Only one participant, UKP-BIU, participated in the Partial Entailment Pilot task. The UKP-BIU system is a hybrid of two semantic relationship approaches, namely (i) computing semantic textual similarity by combining multiple content similarity measures (B¨ar et al., 2012), and (ii) recognizing textual entailment with BIUTEE (Stern and Dagan, 2011). The two approaches are combined by generating indicative features from each one and then applying standard supervised machine learning techniques to train a classifier. The system used several lexicalsemantic resources as part of the BIUTEE entailment system, together with S CI E NTS BANK dependency parses and ESA semantic relatedness indexes from Wikipedia. The team submitted the maximum allowed of 3 runs. Table 7 shows Weighted Average and Macro Average F1 scores respectively, also for the majority baseline. The system outperformed the majority baseline on both metrics. The best performanc"
S13-2045,C00-2137,0,0.0467226,"m one another, with performance varying from being the top rank to nearly the lowest. Hence, it seemed more appropriate to report two separate runs.3 In the rest of the discussion system is used to refer to a row in the tables as just described. Systems with performance that was not statistically different from the best results for a given TS are all shown in bold (significance was not calculated for the TS mean). Systems with performance statistically better than the lexical baseline are displayed in italics. Statistical significance tests were conducted using approximate randomization test (Yeh, 2000) with 10,000 iterations; p ≤ 0.05 was considered statistically significant. 4.4 Dataset: B EETLE 5way Run UA UQ CELI1 0.315 0.300 CNGL2 0.431 0.382 CoMeT1 0.569 0.300 EHUALM2 0.526 0.3703 ETS1 0.444 0.461 ETS2 0.619 0.552 LIMSIILES1 0.327 0.280 SoftCardinality1 0.455 0.436 UKP-BIU1 0.423 0.285 Median 0.444 0.370 Baselines: Lexical 0.424 0.414 Majority 0.114 0.118 Five-way Task The results for the five-way task are shown in Tables 2 and 3. Comparison to baselines All of the systems performed substantially better than the majority class baseline (“correct” for both B EETLE and S CI E NTS BANK),"
S13-2045,W07-1401,1,\N,Missing
W03-0901,J02-3001,0,\N,Missing
W07-1409,P98-1013,0,0.0581043,"rders Y 1 This seems to be an accidental gap; WordNet contains many interlinked disease-patient noun pairs, incl. ""diabetes-diabetic,"" ""epilepsy-eplileptic,"" etc. 58 721: X works on Y → X discusses Y And one that could be is not, namely: 705: X is under a contract with Y → X cooperates with Y (not in the database) Other examples are outside the scope of DIRT&apos;s approach (i.e., “X pattern1 Y” → “X pattern2 Y”), but nonetheless the coverage is encouraging. 3.3 FrameNet In our earlier analysis, we identified knowledge about stereotypical situations and their events as important for RTE. FrameNet (Baker et al, 1998) attempts to encode this knowledge. FrameNet was used with some success in RTE2 by Burchardt and Frank (2005). FrameNet&apos;s basic unit - a Frame - is a script-like conceptual schema that refers to a situation, object, or event along with its participants (Frame Elements), identified independent of their syntactic configuration. We earlier discussed how 538.T ""...the O. J. Simpson murder trial..."" might entail 538.H ""O. J. Simpson was accused of murder."" This case applies to FrameNet’s Trial frame, which includes the Frame Elements Defendant and Charges, with Charges being defined as ""The legal l"
W07-1409,J91-1003,0,0.100568,"Missing"
W07-1409,W06-3907,0,0.0744538,"mbing, which includes sending the bomb in the mail. Thus a person could also recognize alternative verbs in 358.H as valid (e.g., ""mailed"", ""delivered"") or invalid (e.g., ""thrown at"", ""dropped on""), even 56 Some RTE3 examples contain complement-taking verbs that make an implication (either positive or negative) about the complement. For example: 668 ""A survey shows that X..."" → ""X..."" 657 ""...X was seen..."" → ""...X..."" 725 “...decided to X..."" → ""...X..."" 716 ""...have been unable to X..."" → ""...do not X"" In the first 3 the implication is positive, but in the last the implication is negative. (Nairn et al, 2006) provide a detailed analysis of this type of behavior. In fact, this notion of implicature (one part of a sentence making an implication about another part) extends beyond single verbs, and there are some more complex examples in RTE3, e.g.: 453 ""...won the battle to X..."" → ""...X..."" 784.T ""X reassures Russia it has nothing to fear..."" 784.H ""Russia fears..."" In this last example the implication behavior is quite complex: (loosely) If X reassures Y of Z, then Y is concerned about not-Z. 2.12 Metonymy/Transfer In some cases, language allows us to replace a word (sense) with a closely related w"
W07-1409,C98-1013,0,\N,Missing
W08-2205,W07-1420,0,0.0110908,"Missing"
W08-2205,P98-1013,0,0.0707153,"n exactly the right way to fire a script). Second, two new approches for amassing knowledge are available today that were not available previously, namely automated learning from corpora, and use of Web volunteers (e.g., (Chklovski, 2005)), and may be applicable to script acquisition (Script work in the ’70s typically worked with tiny databases of scripts). Finally, techniques for language processing have substantially improved, making core tasks (e.g., parsing) less problematic, and opening the possibility to easy authoring of scripts in English, followed by machine interpretation. FrameNet (Baker et al., 1998) already provides a few small scripts, but does not currently encode the complex scenarios that we would like; a vastly expanded resource would be highly useful. We are in the early stages of exploring this avenue, encoding scripts as a list of simple English sentences, which are then automatically translated to WordNet-sense tagged logic using our software. For example, a “bombing” script looks: A building is bombed by an attacker. The attacker plants the bomb in the building. 54 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson The bomb explodes. The explosion damages or destroys the bu"
W08-2205,W07-1427,0,0.0257037,"Missing"
W08-2205,W08-2221,1,0.9137,"ng’s Language Understanding Engine, which we first describe. We then present the WordNet augmentations that we are developing, and our experience with these as well as with the DIRT paraphrase database. Augmenting WordNet for Deep Understanding of Text 47 The contribution of this paper is some preliminary insight into avenues and challenges for creating and leveraging more world knowledge, in the context of WordNet, for deeper language understanding. 2 Text Interpretation and Subsumption 2.1 Text Interpretation For text interpretation we are using BLUE, Boeing’s Language Understanding Engine (Clark and Harrison, 2008), comprising a parser, logical form (LF) generator, and final logic generator. Parsing is performed using SAPIR, a mature, bottom-up, broad coverage chart parser (Harrison and Maxwell, 1986). The parser’s cost function is biased by a database of manually and corpus-derived “tuples” (good parse fragments), as well as hand-coded preference rules. During parsing, the system also generates a logical form (LF), a semi-formal structure between a parse and full logic, loosely based on Schubert and Hwang (1993). The LF is a simplified and normalized tree structure with logic-type elements, generated b"
W08-2205,C98-1013,0,\N,Missing
W08-2205,W07-1401,0,\N,Missing
W08-2221,W08-2220,0,0.144806,"t inference. Generating a semantic representation is challenging, due to the wide variety of semantic phenomena which can occur in text. We identify seventeen such phenomena which occurred in the STEP 2008 &quot;shared task&quot; texts, comment on BLUE’s ability to handle them or otherwise, and discuss the more general question of what exactly constitutes a &quot;semantic representation&quot;, arguing that a spectrum of interpretations exist. 263 264 Clark and Harrison 1 System Description 1.1 Overview and Scope As our contribution to the 2008 STEP Symposium’s “shared task” of comparing semantic representations (Bos, 2008), we describe Boeing’s NLP system, BLUE (Boeing Language Understanding Engine), and subsequently analyze its performance on the task’s shared texts. BLUE consists of a pipeline of a parser, logical form (LF) generator, an initial logic generator, and subsequent processing modules. The parser has broad coverage and is domain general. The logical form generator currently deals with a (reasonably large) subset of linguistic phenomena, including simple sentences, prepositional phrases, compound nouns, ordinal modifiers, proper nouns, some simple types of coordination, adverbs, negation, comparativ"
W10-0901,W09-2201,0,0.0139311,"ence-supporting representations of text. One approach for selective extraction is the use of semantic templates (&quot;scripts&quot;, &quot;frames&quot;) to provide a set of roles (slots) and constraints on objects playing those roles (fillers) to be expected in text, and might be filled by methods ranging from simply skimming text, e.g., FRUMP (DeJong, 1979), to full language processing, e.g., (Dahlgren et al., 1991). Other work has looked at techniques for learning phrasal patterns likely to contain slot fillers (Riloff, 1996; Sekine, 2006) or contain information semantically similar to a set of seed examples (Carlson et al, 2009). Introduction Machine reading is not just a task of language processing, but an active interplay between knowledge and language; Prior knowledge should guide interpretation of new text, and new interpretations should augment that prior knowledge. Such interaction is essential if ambiguities in language are to be resolved &quot;correctly&quot; (with respect to what is At the other end of the spectrum, some systems attempt a full understanding of text, i.e., have the ambitious goal of building a complete representation of the text's contents (e.g., Zadrozny 1991, Hobbs et al, 1993). A common thread of th"
W10-0901,C10-2020,1,0.882558,"Missing"
W10-0901,M91-1025,0,0.25373,"useful for proposing knowledge base extensions, extracted from text, to a knowledge engineer. 1 &quot;Machine reading&quot; itself is a loosely-defined notion, ranging from extracting selective facts to constructing complex, inference-supporting representations of text. One approach for selective extraction is the use of semantic templates (&quot;scripts&quot;, &quot;frames&quot;) to provide a set of roles (slots) and constraints on objects playing those roles (fillers) to be expected in text, and might be filled by methods ranging from simply skimming text, e.g., FRUMP (DeJong, 1979), to full language processing, e.g., (Dahlgren et al., 1991). Other work has looked at techniques for learning phrasal patterns likely to contain slot fillers (Riloff, 1996; Sekine, 2006) or contain information semantically similar to a set of seed examples (Carlson et al, 2009). Introduction Machine reading is not just a task of language processing, but an active interplay between knowledge and language; Prior knowledge should guide interpretation of new text, and new interpretations should augment that prior knowledge. Such interaction is essential if ambiguities in language are to be resolved &quot;correctly&quot; (with respect to what is At the other end of"
W10-0901,P06-2094,0,0.0128266,"efined notion, ranging from extracting selective facts to constructing complex, inference-supporting representations of text. One approach for selective extraction is the use of semantic templates (&quot;scripts&quot;, &quot;frames&quot;) to provide a set of roles (slots) and constraints on objects playing those roles (fillers) to be expected in text, and might be filled by methods ranging from simply skimming text, e.g., FRUMP (DeJong, 1979), to full language processing, e.g., (Dahlgren et al., 1991). Other work has looked at techniques for learning phrasal patterns likely to contain slot fillers (Riloff, 1996; Sekine, 2006) or contain information semantically similar to a set of seed examples (Carlson et al, 2009). Introduction Machine reading is not just a task of language processing, but an active interplay between knowledge and language; Prior knowledge should guide interpretation of new text, and new interpretations should augment that prior knowledge. Such interaction is essential if ambiguities in language are to be resolved &quot;correctly&quot; (with respect to what is At the other end of the spectrum, some systems attempt a full understanding of text, i.e., have the ambitious goal of building a complete represent"
W10-0901,J08-2002,0,0.0167347,"wi is the word used for xi. An example from DIRT is: IF X is found in Y THEN X is inside Y The condition “X is found in Y” can be expressed as the clause chain: { object-of(x,f), &quot;find&quot;(f), &quot;in&quot;(f,y) } We use DIRT to explore alternative interpretations of the text, singling out those that help identify the facts in the text that are already known in the KB. 3.2 Deferred Sense Commitment Two common challenges for NLP are word sense disambiguation (WSD) and semantic role labeling (SRL). While there are a number of existing tools for performing these tasks based on the linguistic context (e.g., Toutanova et al., 2008, Erk and Pado, 2006), their performance is only moderate (e.g., Agirre et al, 2007). The problem is accentuated when trying to disambiguate in a way consistent with a particular KB, because there is often a degree of subjectivity in how the knowledge engineer chose to represent the world in that KB (e.g., whether some object is the &quot;agent&quot; or &quot;instrument&quot; or &quot;site&quot; of an activity is to a degree a matter of viewpoint). Trying to create a WSD or SRL module that reliably mimics the knowledge engineer’s decision procedure is difficult. To address this, we defer WSD and SRL commitment during the i"
W10-0901,J91-2003,0,0.0721796,"eedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 1–9, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics &quot;coherent&quot;, based on criteria such as maximizing coreference, minimizing redundancy, and avoiding contradictions. For example, Mulkar et al (2007) search for a set of abductive inferences on the (logical form of the) text that minimizes cost (maximizes coherence) of the result, where an abductive inference might be a word sense or coreference decision with an associated cost. Similarly, Zadrozny and Jensen (1991) search a space of disambiguations when interpreting paragraphs by elaborating each alternative (using dictionary definitions) and selecting the most coherent based on similar criteria. Work on model building is inspiring but also challenging due to the lack of constraint on the final models (even with substantial domain knowledge) and the difficulty of quantifying &quot;coherence&quot;. Our work falls somewhere between these two. We do not use templates for new knowledge, but rather use inference at run-time to identify what is known and thus what to expect that the text might be saying. However, unlik"
W12-3014,W11-2504,0,0.0217989,"Missing"
W12-3014,W08-2221,1,0.836258,"ess can be bootstrapped: QA can help build the KB, and the KB can provide the evidence for QA. We kickstart the process by initially seeding the KB with extractions from individual sentences in the book, and then use QA over those extractions to rescore and refine the knowledge (&quot;introspective QA&quot;). 2.1 Information Extraction Our first step is to process the textbook text and extract semi-structured representations of its content. We extract two forms of the textbook's information: Logical Forms (LFs): A parse-based logical form (LF) representation of the book sentences using the BLUE system (Clark and Harrison, 2008), e.g., from &quot;Metabolism sets limits on cell size&quot; we obtain: (S (SUBJ (&quot;metabolism&quot;)) (V (&quot;set&quot;)) (SOBJ (&quot;limit&quot; (&quot;on&quot; (&quot;size&quot; (MOD (&quot;cell&quot;))))) Triples: A set of arg1-predicate-arg2 triples extracted via a chunker applied to the book sentences, using Univ. Washington's ReVerb system (Fader et al, 2011), e.g., from &quot;Free ribosomes are suspended in the cytosol and synthesize proteins there.&quot; we obtain: [&quot;ribosomes&quot;] [&quot;are suspended in&quot;] [&quot;the cytosol&quot;] These extractions are the raw material for the initial textual KB. 2.2 Knowledge-Base Construction and Introspective Question-Answering As the"
W12-3014,D11-1142,1,0.634026,"tion Extraction Our first step is to process the textbook text and extract semi-structured representations of its content. We extract two forms of the textbook's information: Logical Forms (LFs): A parse-based logical form (LF) representation of the book sentences using the BLUE system (Clark and Harrison, 2008), e.g., from &quot;Metabolism sets limits on cell size&quot; we obtain: (S (SUBJ (&quot;metabolism&quot;)) (V (&quot;set&quot;)) (SOBJ (&quot;limit&quot; (&quot;on&quot; (&quot;size&quot; (MOD (&quot;cell&quot;))))) Triples: A set of arg1-predicate-arg2 triples extracted via a chunker applied to the book sentences, using Univ. Washington's ReVerb system (Fader et al, 2011), e.g., from &quot;Free ribosomes are suspended in the cytosol and synthesize proteins there.&quot; we obtain: [&quot;ribosomes&quot;] [&quot;are suspended in&quot;] [&quot;the cytosol&quot;] These extractions are the raw material for the initial textual KB. 2.2 Knowledge-Base Construction and Introspective Question-Answering As the ontology for the TKB, we are using the preexisting biology taxonomy (isa hierarchy) from the hand-build biology KB (part of the formal knowledge project). Initially, for each concept in that ontology, all the extractions &quot;about&quot; that concept are gathered together. An extraction is considered &quot;about&quot; a co"
W12-3014,P11-1062,0,\N,Missing
W12-3014,P10-1124,0,\N,Missing
W16-1303,H05-1071,1,0.915744,"Missing"
W16-1303,W09-2201,0,0.0312736,"Missing"
W16-1303,W99-0613,0,0.548716,"Missing"
W16-1303,D11-1133,0,0.0672581,"Missing"
W16-1303,W12-0702,0,0.0450558,"Missing"
W16-1303,W14-1611,0,0.0787971,"Missing"
W16-1303,C92-2082,0,0.445913,"Missing"
W16-1303,P11-1055,0,0.123146,"Missing"
W16-1303,P12-3019,0,0.0706265,"Missing"
W16-1303,E14-1048,0,0.0501259,"Missing"
W16-1303,J06-3003,0,0.0561851,"Missing"
W16-1303,P15-1034,0,\N,Missing
