2006.iwslt-evaluation.2,W00-0508,1,0.938698,"ese-English data sets. Sentence Aligned Corpus Alignment Word Alignment Bilanguage Transformation Bilanguage 1. Introduction Local Phrase Reordering The problem of machine translation can be viewed as consisting of two subproblems: (a) lexical selection, where appropriate target language lexical items are chosen for each source language lexical item and (b) lexical reordering, where the chosen target language lexical items are rearranged to produce a meaningful target language string. In previous work, we have proposed stochastic finite-state transducer (SFST) models for these two subproblems [1, 2] which can then be composed into a single SFST model for Statistical Machine Translation (SMT). SFST approach to SMT has gained momentum in recent years with several groups following this approach successfully [3, 4, 5, 6] for speech to speech translation. The attractions of this approach are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing. In this paper, we present the SFST model for SMT and detail the different pha"
2006.iwslt-evaluation.2,N01-1018,1,0.843925,"ese-English data sets. Sentence Aligned Corpus Alignment Word Alignment Bilanguage Transformation Bilanguage 1. Introduction Local Phrase Reordering The problem of machine translation can be viewed as consisting of two subproblems: (a) lexical selection, where appropriate target language lexical items are chosen for each source language lexical item and (b) lexical reordering, where the chosen target language lexical items are rearranged to produce a meaningful target language string. In previous work, we have proposed stochastic finite-state transducer (SFST) models for these two subproblems [1, 2] which can then be composed into a single SFST model for Statistical Machine Translation (SMT). SFST approach to SMT has gained momentum in recent years with several groups following this approach successfully [3, 4, 5, 6] for speech to speech translation. The attractions of this approach are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing. In this paper, we present the SFST model for SMT and detail the different pha"
2006.iwslt-evaluation.2,P04-1065,1,0.854062,"wo subproblems: (a) lexical selection, where appropriate target language lexical items are chosen for each source language lexical item and (b) lexical reordering, where the chosen target language lexical items are rearranged to produce a meaningful target language string. In previous work, we have proposed stochastic finite-state transducer (SFST) models for these two subproblems [1, 2] which can then be composed into a single SFST model for Statistical Machine Translation (SMT). SFST approach to SMT has gained momentum in recent years with several groups following this approach successfully [3, 4, 5, 6] for speech to speech translation. The attractions of this approach are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing. In this paper, we present the SFST model for SMT and detail the different phases in training the model and decoding using the model. We also present a novel approach to translation using a discriminatively trained model for lexical choice and a permutation automaton for lexical reordering. This mod"
2006.iwslt-evaluation.2,P02-1038,0,0.26339,"uction Lexical Reodering Decoding 3.1. Word Alignment The first stage in the process of training a lexical selection model is obtaining an alignment function that given a pair of source (s1 s2 . . . sn ) and target (t1 t2 . . . tm ) language sentences, maps source language word subsequences into target language word subsequences, as shown below. Permutation Lattice Target Model Language FSA Composition ∀i∃j(f (si ) = tj ∨ f (si ) = ) (1) For this purpose, in the past, we have used the tree alignment algorithm described in [7]. For the work reported in this paper, we have used the GIZA++ tool [8] which implements a string-alignment algorithm. GIZA++ alignment however is asymmetric in that the word mappings are different depending on the direction of alignment – source-to-target or target-to-source. Hence in addition to the functions f as shown in Equation 1 we train another alignment function g as shown in Equation 2. Target Sentence Figure 2: Decoding phases for our system aligned corpus as the input as shown in Figure 1. The sentence aligned corpus is used to infer a word alignment using an alignment training process. Word alignment results in a mapping between the words in the sour"
2006.iwslt-evaluation.2,W05-0831,1,0.848187,"ordering by permuting the words of the best translation and weighting the result by an n-gram language model (see also Figure 2): T ∗ = BestP ath(perm(T 0 ) ◦ LMt ) (7) Unfortunately, even the size of the minimal permutation automaton grows exponentially with the length of the input sequence. While decoding by composition simply resembles the principle of memoization (i.e. in this case: all state hypotheses necessary to decode a sentence are kept in memory), it is necessary to either use heuristic forward pruning or limit the window of the allowed permutations. Similar to the way described in [11] the latter was used here. Decoding ASR output in combination with global reordering uses either n-best lists or extracts n-best lists from lattices first. Decoding using global reordering is performed for each entry of the n-best list separately and the best decoded target sentence is picked from the union of the n intermediate results. 3.4. SFST Representation From the bilanguage corpus B, we train a n-gram language model using language modeling tools [9, 10]. The resulting language model is represented as a weighted finite-state automaton (S×T → [0, 1]). The symbols on the arcs of this auto"
2006.iwslt-evaluation.2,W02-2018,0,0.0280261,"(S×T → [0, 1]). The symbols on the arcs of this automaton (si ti ) are interpreted as having the source and target symbols (si :ti ), making it into a weighted finite-state transducer (S → T × [0, 1]) that provides a weighted string-tostring transduction from S into T (as shown in Equation 4). T ∗ = argmaxP (si , ti |si−1 , ti−1 . . . si−n−1 , ti−n−1 ) (4) T 18 5. Discriminant Models for Translation The procedures used to find the global maximum of this concave function include two major families of methods: Iterative Scaling (IS) and gradient descent procedures, in particular L-BFGS methods [15], which have been reported to be the fastest. We obtained faster convergence with a new Sequential L1-Regularized Maxent algorithm (SL1-Max) [16], compared to L-BFGS1 . We have adapted SL1-Max to conditional distributions for our purposes. Another advantage of the SL1-Max algorithm is that it provides L1-regularization as well as efficient heuristics to estimate the regularization meta-parameters. The computational requirements are O(V ) and as all the classes need to be trained simultaneously, memory requirements are also O(V ). Given that the actual number of non-zero weights is much lower t"
2006.iwslt-evaluation.2,J96-1002,0,\N,Missing
2007.mtsummit-papers.32,J84-3009,0,0.701915,"Missing"
2014.eamt-1.18,P11-2068,0,0.0632133,"Missing"
2014.eamt-1.18,H90-1045,0,0.844434,"e for text production and they seem to be the easiest input method when only minor changes are needed. However, in the context of post-editing, when the text requires major changes (e.g. editing larger segments of text), typing could be optimized using other input modalities. Moreover, if the posteditor is not a touch typist, then she has to switch visual attention back and forth between the screen and the keyboard making the task more complex. A possible solution for this profile of users could be the use of other input methods, such as ASR or hand-writing, in addition to traditional typing (Hauptmann and Rudnicky, 1990). Introduction The comparison between ASR and typing as input methods can be done based on task duration, i.e. measuring the time needed to type against the ASR rate including possible corrections to fix recognition inaccuracies. Studies on input durations have shown that ASR input can be faster (Chen, 2006; Vidal et al., 2006). Human-aided machine translation is gradually becoming a common practice for language service providers (LSPs) as opposed to machine-aided human translation. Depending on the nature of the text, more and more LSPs pre-translate the source text using existing translation"
2014.eamt-1.18,P06-2061,0,0.0118359,"t the translation process, but will also help to improve the output provided by the MT server base in gaze information coming from the user. the ASR server for the recognition of the speech signal uttered by the user. The C AS M AC AT logging functions have been extended with the information coming from ASR in order to be able to check when the ASR input starts and finishes. Figure 5 describes how the SEECAT components interact. 4 Experiments and results This section presents experimental data using the current version of the SEECAT workbench. 4.1 Integration of ASR and MT MT can improve ASR (Khadivi et al., 2006; Lecouteux et al., 2006) in a computer-assisted translation scenario. The same technique used to improve ASR through MT can be used with semantic information (Tammewar et al., 2013). In SEECAT, the hypotheses produced by ASR and MT are converted into lattices and are then composed using Edit Machine with the help of OpenFst toolkit (Allauzen et al., 2007). The synset information from WordNet is used while composing for the semantic matching of words. According to the edit distance scores, ASR hypotheses are rescored. We further extend this approach for the two language pairs Hindi-English and"
2014.eamt-1.18,2012.tc-1.7,1,0.783026,"Missing"
2014.iwslt-papers.2,E09-1081,0,0.0188895,"with a tempo-elastic speech synthesizer. 1. Introduction Today’s speech-to-speech translation solutions are a long way from transparent and ubiquitous universal translators as envisioned in science fiction literature (e. g. [1]), for a multitude of reasons. One of the shortcomings is translation latency, which in speech can be described as the latency between when a concept can be grasped from listening to the source utterance and producing it as part of the target utterance. For swift and seamless communication across language barriers, low translation latency is key. Incremental processing [2] is a technical means to implement interactive speech processing systems for online speech recognition [3], [4], [5], language understanding and generation [6], for speech synthesis [7]. Incremental processing has also been successfully applied to speech-to-speech translation (e. g. [8]), where it helps to bring down processing latency in an integrated system. An important aspect of incremental processing (and hence, incremental translation) is the granularity at which material is being added. A fine granularity of processing is a precondition to low latency, as smaller units can more quickly"
2014.iwslt-papers.2,N09-1043,1,0.713289,"re a long way from transparent and ubiquitous universal translators as envisioned in science fiction literature (e. g. [1]), for a multitude of reasons. One of the shortcomings is translation latency, which in speech can be described as the latency between when a concept can be grasped from listening to the source utterance and producing it as part of the target utterance. For swift and seamless communication across language barriers, low translation latency is key. Incremental processing [2] is a technical means to implement interactive speech processing systems for online speech recognition [3], [4], [5], language understanding and generation [6], for speech synthesis [7]. Incremental processing has also been successfully applied to speech-to-speech translation (e. g. [8]), where it helps to bring down processing latency in an integrated system. An important aspect of incremental processing (and hence, incremental translation) is the granularity at which material is being added. A fine granularity of processing is a precondition to low latency, as smaller units can more quickly be passed on to a next module. Previous work on incremental translation has focused on phrasing (based on"
2014.iwslt-papers.2,W11-2014,0,0.0216987,"long way from transparent and ubiquitous universal translators as envisioned in science fiction literature (e. g. [1]), for a multitude of reasons. One of the shortcomings is translation latency, which in speech can be described as the latency between when a concept can be grasped from listening to the source utterance and producing it as part of the target utterance. For swift and seamless communication across language barriers, low translation latency is key. Incremental processing [2] is a technical means to implement interactive speech processing systems for online speech recognition [3], [4], [5], language understanding and generation [6], for speech synthesis [7]. Incremental processing has also been successfully applied to speech-to-speech translation (e. g. [8]), where it helps to bring down processing latency in an integrated system. An important aspect of incremental processing (and hence, incremental translation) is the granularity at which material is being added. A fine granularity of processing is a precondition to low latency, as smaller units can more quickly be passed on to a next module. Previous work on incremental translation has focused on phrasing (based on inton"
2014.iwslt-papers.2,W10-4301,0,0.0233288,"al translators as envisioned in science fiction literature (e. g. [1]), for a multitude of reasons. One of the shortcomings is translation latency, which in speech can be described as the latency between when a concept can be grasped from listening to the source utterance and producing it as part of the target utterance. For swift and seamless communication across language barriers, low translation latency is key. Incremental processing [2] is a technical means to implement interactive speech processing systems for online speech recognition [3], [4], [5], language understanding and generation [6], for speech synthesis [7]. Incremental processing has also been successfully applied to speech-to-speech translation (e. g. [8]), where it helps to bring down processing latency in an integrated system. An important aspect of incremental processing (and hence, incremental translation) is the granularity at which material is being added. A fine granularity of processing is a precondition to low latency, as smaller units can more quickly be passed on to a next module. Previous work on incremental translation has focused on phrasing (based on intonation and somewhat related to meaning units) for"
2014.iwslt-papers.2,P12-3018,1,0.847891,"ed in science fiction literature (e. g. [1]), for a multitude of reasons. One of the shortcomings is translation latency, which in speech can be described as the latency between when a concept can be grasped from listening to the source utterance and producing it as part of the target utterance. For swift and seamless communication across language barriers, low translation latency is key. Incremental processing [2] is a technical means to implement interactive speech processing systems for online speech recognition [3], [4], [5], language understanding and generation [6], for speech synthesis [7]. Incremental processing has also been successfully applied to speech-to-speech translation (e. g. [8]), where it helps to bring down processing latency in an integrated system. An important aspect of incremental processing (and hence, incremental translation) is the granularity at which material is being added. A fine granularity of processing is a precondition to low latency, as smaller units can more quickly be passed on to a next module. Previous work on incremental translation has focused on phrasing (based on intonation and somewhat related to meaning units) for translation [9], as phras"
2014.iwslt-papers.2,N12-1048,1,0.791569,"ranslation latency, which in speech can be described as the latency between when a concept can be grasped from listening to the source utterance and producing it as part of the target utterance. For swift and seamless communication across language barriers, low translation latency is key. Incremental processing [2] is a technical means to implement interactive speech processing systems for online speech recognition [3], [4], [5], language understanding and generation [6], for speech synthesis [7]. Incremental processing has also been successfully applied to speech-to-speech translation (e. g. [8]), where it helps to bring down processing latency in an integrated system. An important aspect of incremental processing (and hence, incremental translation) is the granularity at which material is being added. A fine granularity of processing is a precondition to low latency, as smaller units can more quickly be passed on to a next module. Previous work on incremental translation has focused on phrasing (based on intonation and somewhat related to meaning units) for translation [9], as phrases can easily be passed on to speech synthesis as one unit. Recently, incremental speech synthesis is"
2014.iwslt-papers.2,2013.iwslt-papers.3,0,0.021799,"we propose that automatic simultaneous interpreting modules, just like human experts, must have recovery capabilities, which enable them to cope with situations in which already-delivered parts of a translation should be revoked and replaced by a different translation. Human experts use and combine various strategies to cope with the problem [15]. We experiment here with the simplest possible solution of dealing with changes: we ignore all changes to words that have already or are currently being spoken. This causes the translation performance to deteriorate, given a fixed delay (similarly to [16]), which will also be analyzed in Section 4. Finally, one intuitively important strategy of human experts is to vary the latency between input and output by varying speech delivery tempo. We report on our initial progress in determining overall latency and reducing it in Section 5. 3. Corpus and Experiment Setup We use the IWSLT 2011 test set of the TED talks corpus as provided by the Web Inventory of Transcribed and Translated Talks [17]. As translation quality and stability may depend to a large extent on languages, we include analyses for three language pairs: en → de, en → es, and de → en."
2014.iwslt-papers.2,2012.eamt-1.60,0,0.0144957,"l changes to words that have already or are currently being spoken. This causes the translation performance to deteriorate, given a fixed delay (similarly to [16]), which will also be analyzed in Section 4. Finally, one intuitively important strategy of human experts is to vary the latency between input and output by varying speech delivery tempo. We report on our initial progress in determining overall latency and reducing it in Section 5. 3. Corpus and Experiment Setup We use the IWSLT 2011 test set of the TED talks corpus as provided by the Web Inventory of Transcribed and Translated Talks [17]. As translation quality and stability may depend to a large extent on languages, we include analyses for three language pairs: en → de, en → es, and de → en.3 We tokenize the respective source material with WASTE [18], using the included models for German and English. We then feed each of the utterances to standard, per-se non-incremental translation systems in a restart-incremental fashion: first translating just the first token, then the first two, then the first three, and so on, ending with the full utterance. This results in a large processing overhead and may confuse the translation sys"
2014.iwslt-papers.2,W09-3943,1,0.848194,"is difference in the following experiments. 4. Evaluation of Basic Measures For a time-aligned source sentence and its corresponding time-aligned incremental translation output that represents the final target language sentence, we find the minimum necessary delay at which the target sentence can be delivered such that the partial translation hypotheses always match the final target language sentence (i. e., the synthesis would never be triggered to start saying a word that is later replaced by a different word during incremental translation). Using the incremental evaluation toolbox intelida [21], we compute the delay that is necessary in order to have all finally chosen target language words available before their scheduled 4 http://translate.google.com/ with the help of some PHPbased automation code. 5 Of course, we could have extracted more precise source language timing information from TED videos, but results would likely be similar and only be available for English as source language. 6 We thank Marcela Charfuelan for making a Spanish voice and linguistic resources available. delivery starts, and without intermittent interruptions from synthesis running out of words to speak. De"
2014.iwslt-papers.2,P06-1140,0,0.0184614,"asily be passed on to speech synthesis as one unit. Recently, incremental speech synthesis is progressing well at a wordby-word granularity, if some additional boundary and finality information is provided [10], [11]. In building language processing systems, joint analysis and optimization across module boundaries often greatly improves performance. The combination of speech recognition with understanding (e. g. [12]) or translation (e. g. [13]) is quite common, but this is less often done for the output side. (One notable exception is joint optimization of natural language generation and TTS [14], however not in an incremental setting.) In this paper, we analyze the timing properties of source and target speech in an incremental machine translation setting in order to evaluate the improvements possible when combining word-by-word incremental machine translation with speech synthesis, particularly with respect to delivery latency. We do not yet actually employ fully incremental synthesis but focus our analysis on the advantages of such a synthesis technique in this contribution. The remainder of this paper is structured as follows: in Section 2, we describe the interplay of incremental"
2020.findings-emnlp.166,Q16-1026,0,0.0162594,"2018)). For each (task, dataset) pair we use common embeddings and hyperparameters from the literature. The baseline models are biLSTM-CRFs with character compositional features based on convolutional neural networks (Dos Santos and Zadrozny, 2014) and our models are identical except we train with a crossentropy loss and use the encoding scheme constraints as transition probabilities instead of learning them with a CRF. Our hyper-parameters mostly follow Ma and Hovy (2016), except we use multiple pre-trained word embeddings concatenated together (Lester et al., 2020). For Ontonotes we follow Chiu and Nichols (2016). See Section A.7 or the configuration files in our implementation for more details. As seen in Table 1, in three out of four datasets constrained decoding performs comparably or better than the CRF in terms of F1. OntoNotes is 1842 Dataset CoNLL WNUT-17 Snips OntoNotes Model CRF Constrain CRF Constrain CRF Constrain CRF Constrain mean 91.61 91.44 40.33 40.59 96.04 96.07 87.43 86.13 std 0.25 0.23 1.13 1.06 0.28 0.17 0.26 0.17 max 92.00 91.90 41.99 41.71 96.35 96.29 87.57 86.72 Task NER Slot Filling Table 1: Tagging results on a variety of datasets. The CRF model is a standard biLSTM-CRF while"
2020.findings-emnlp.166,W17-4418,0,0.0250024,"Missing"
2020.findings-emnlp.166,N06-2015,0,0.0865775,"ed decoding model—avoiding the need for the CRF forward algorithm and the CRF loss. For constrained decoding, we leverage the IOBES tagging scheme rather than BIO tagging, allowing us to inject more structure into the decoding mask. Early experiments with BIO tagging failed to show the large gains we realized using IOBES tagging for the reasons mentioned in Section 4. 3 Experiments & Results To test if we can replace the CRF with constrained decoding we use two sequential prediction tasks: NER (CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003), WNUT-17 (Derczynski et al., 2017), and OntoNotes (Hovy et al., 2006)) and slot-filling (Snips (Coucke et al., 2018)). For each (task, dataset) pair we use common embeddings and hyperparameters from the literature. The baseline models are biLSTM-CRFs with character compositional features based on convolutional neural networks (Dos Santos and Zadrozny, 2014) and our models are identical except we train with a crossentropy loss and use the encoding scheme constraints as transition probabilities instead of learning them with a CRF. Our hyper-parameters mostly follow Ma and Hovy (2016), except we use multiple pre-trained word embeddings concatenated together (Leste"
2020.findings-emnlp.166,W03-0419,0,0.232818,"Missing"
2021.naacl-industry.27,L16-1503,0,0.120887,"Missing"
2021.naacl-industry.27,D14-1181,0,0.00293987,"ven appear sequentially, for example, the cancel intent is often followed by the refund intent, as users tend to request a can3.1.1 Convolutional Baseline cellation first and then ask for a refund. Therefore, we modeled our multi-intent system as a sequence The first approach was to assume that the feature tagging problem, where intent spans are encoded labels for an intent are local to that intent span, and, 216 therefore, each intent span can be fed into a classifier independently of the other intent spans. Under this assumption, we used a convolutional neural network with parallel filters (Kim, 2014), as it is a strong baseline used in several of our production systems. We used parallel filters of size 3, 4, and 5 with 100 filters each. Max-over-time pooling was used to produce a final span representation, which is projected into the label space. This model was trained using Adadelta (Zeiler, 2012) with an initial learning rate of 1.0 and a batch size of 50. However, this approach misses possible dependencies across spans. Some features (such as “tense”) are naturally co-dependent among spans; the use of a past tense verb in one span dictates that all spans in the utterance are past tense"
2021.naacl-industry.27,D17-1018,0,0.0541183,"Missing"
2021.naacl-industry.27,2020.findings-emnlp.166,1,0.824666,"Missing"
2021.naacl-industry.27,J00-3003,0,0.646951,"Missing"
2021.naacl-industry.27,P16-1101,0,0.0426155,"f just marking an event as “Asserted” or “Other”, our version of Modality distinguishes between different aspects of hypothetical events. 3 Modeling There are four different model types we explored for intent features that we detail below. However, before we can annotate an intent with a feature, we need to have an intent span. First, we describe our intent span extraction model whose predictions are used as intent spans. 3.1 Multi-Intent as Annotatable Spans as token level annotations with the IOBES tagging scheme (Ratinov and Roth, 2009). We used a standard BiLSTM-CRF architecture following Ma and Hovy (2016). Each input token is represented both as a character composition, by running a small convolutional neural network with a filter size of 3 over the characters and doing max-over-time pooling as in Dos Santos and Zadrozny (2014), and as a word embedding. We use the concatenation of multiple word embeddings, GloVe embeddings (Pennington et al., 2014), as well as 100 dimensional, in-domain embeddings trained in-house, following Lester et al. (2020a). The token sequence is then fed into an bidirectional LSTM (Graves et al., 2005), where the LSTM (Hochreiter and Schmidhuber, 1997) in each direction"
2021.naacl-industry.27,D14-1162,0,0.085986,"ribe our intent span extraction model whose predictions are used as intent spans. 3.1 Multi-Intent as Annotatable Spans as token level annotations with the IOBES tagging scheme (Ratinov and Roth, 2009). We used a standard BiLSTM-CRF architecture following Ma and Hovy (2016). Each input token is represented both as a character composition, by running a small convolutional neural network with a filter size of 3 over the characters and doing max-over-time pooling as in Dos Santos and Zadrozny (2014), and as a word embedding. We use the concatenation of multiple word embeddings, GloVe embeddings (Pennington et al., 2014), as well as 100 dimensional, in-domain embeddings trained in-house, following Lester et al. (2020a). The token sequence is then fed into an bidirectional LSTM (Graves et al., 2005), where the LSTM (Hochreiter and Schmidhuber, 1997) in each direction has a size of 200, and projected to the final label space. Finally a Conditional Random Field (CRF) (Lafferty et al., 2001) with constrained decoding (Lester et al., 2020b) is used to produce the final sequence of intents. This model was trained using SGD with momentum using 0.0015 as the learning rate, 0.9 for momentum, and a batch size of 10. Mo"
2021.naacl-industry.27,W18-2506,1,0.85666,"ssed by the local pooling function separately. Doing this efficiently in a batched computing environment, like TensorFlow (Abadi et al., 2015), is slightly tricky to implement. A much simpler model would feed the global utterance and the span separately, to be encoded and processed independently. Our ablations in the “– Shared Embedding” row of Table 4 shows that using a shared embedding space does yield performance gains, but it can be removed for the sake of easier model deployment and still maintain superior performance over the span-level model. All models were trained with Mead-Baseline (Pressel et al., 2018), an open-source library for the development, training, and export for deep neural networks for NLP. 6 Deployment We have deployed a NLU component of a taskoriented, production dialogue system that produces intent features. The dialogue system deals with cus5 Experiments tomer service in the retail software domain. The The F1 scores for these models are reported in Ta- dialogue manager currently makes use of several ble 3. The BiLSTM-CRF tagger without any infor- intent features. The easier feature to use is negamation about the intent boundaries has the lowest tion and it is critical to under"
2021.naacl-industry.27,H90-1020,0,0.176757,". Like us, they have regions on interest embedded in a larger context. However, our models differ in several key ways: their span representation is a hand-crafted combination of token features while ours is a learned pooling of token representations. Also, their model is restricted to operating on contiguous spans (possibly due to unavailability of spans a priori, or that noncontiguous spans would lead to a combinatorial explosion), while our model has no such restriction. 7 8 Previous Work Conclusion Most popular intent taxonomies such as ATIS Improvements in the complexity of conversations (Price, 1990) are domain-specific. Dialog Acts that a dialogue system can handle have put tremen(DA) (Stolcke et al., 2000) are more formalized dous pressure on NLU systems to capture fineand generalized versions of intents. The interna- grained and domain-specific information. Diffitional standard for DA annotations (Bunt et al., culty in the data generation process means the abil2010, 2012, 2016) defined the concept of commu- ity to share data across clients is critical. We define 219 intent features, a core set of general annotations, on intents that provide context and clarity on the exact nature of th"
2021.naacl-industry.27,W09-1119,0,0.0480731,"t in event extraction datasets like ACE 2005 (Consortium, 2005), but instead of just marking an event as “Asserted” or “Other”, our version of Modality distinguishes between different aspects of hypothetical events. 3 Modeling There are four different model types we explored for intent features that we detail below. However, before we can annotate an intent with a feature, we need to have an intent span. First, we describe our intent span extraction model whose predictions are used as intent spans. 3.1 Multi-Intent as Annotatable Spans as token level annotations with the IOBES tagging scheme (Ratinov and Roth, 2009). We used a standard BiLSTM-CRF architecture following Ma and Hovy (2016). Each input token is represented both as a character composition, by running a small convolutional neural network with a filter size of 3 over the characters and doing max-over-time pooling as in Dos Santos and Zadrozny (2014), and as a word embedding. We use the concatenation of multiple word embeddings, GloVe embeddings (Pennington et al., 2014), as well as 100 dimensional, in-domain embeddings trained in-house, following Lester et al. (2020a). The token sequence is then fed into an bidirectional LSTM (Graves et al., 2"
2021.naacl-industry.9,N18-3023,1,0.846214,"low SLU confidence are handed-off to human agents who label them in real-time instead of being automated using the SLU output. The rejection of an SLU output is based on comparing the overall confidence measure for each utterance to a threshold. This utterance-level semantic confidence score quantifies the reliability of the information extracted from a spoken utterance, including entities and intents. It has been shown that combining speech recognition scores with semantic features to train a confidence model is an effective approach for semantic confidence estimation (Sarikaya et al., 2005; Mehrabani et al., 2018; San-Segundo et al., 2001). We use a logistic regression confidence model that is trained by passing each utterance through the SLU pipeline and the predicted result (intents and entities) is compared with the reference label containing the spoken intents and entities. After this binary model is trained, the following is used as the confidence measure: Entity Extraction While increasingly accurate sequence tagging models for named entity recognition (NER) have been developed over the years, NER on speech input adds another complexity which cannot be mitigated by advanced algorithms developed"
2021.naacl-industry.9,W19-5906,0,0.0282272,"ities, and overall confidence score. either reprompt or to request human assistance. A variety of design considerations are discussed with insights drawn from real world EVA applications. We know of few previous studies having similar aim and scope of work as ours. Early work on industrial SLU systems sharing the aim of scalable SLU without human intervention was described in (Gupta et al., 2005), though without confidence modeling. An SLU pipeline is also addressed in (Coucke et al., 2018), but with design considerations made for CVA-like applications running on a device. While Gupta et al. (Gupta et al., 2019) does recognize that the needs of EVAs are different, their work primarily focuses on a framework for joint intent classification and slot filling that is modularized into different components. This paper presents a complete study of a deployed SLU pipeline for handling intents and entities. The models described have been deployed in applications for Fortune 500 companies and a variety of design considerations are discussed with insights drawn from these real world EVA applications. In particular, we focus on improving performance on entities and intents for several core subtasks in a goal dir"
2021.naacl-industry.9,H90-1020,0,0.405275,"ate transcription of the user’s speech, these virtual agents critically rely on interpreting the user’s utterance accurately. Interpretation of a user’s utterance – spoken language understanding (SLU) is broadly characterized as extracting intents – expressions that refer to actions, and entities – expressions that refer to objects. The entity expressions are further grounded to specific objects in the domain of the dialog (eg. latest iphone → iphone 11) or through world knowledge (eg. Christmas → 12/25). SLU has been a topic of research for the past three decades. Public data sets like ATIS (Price, 1990), SNIPS (Coucke et al., 2018), and recently FSC (Lugosch et al., 2019) have allowed for comparing various methodologies, including many recent developments driven by deep learning (Mesnil et al., 2014; Xu and Sarikaya, 2013; Liu and Lane, 2016; Price, 2020; Tomashenko et al., 2019). Such data sets are also a To address the several challenges that relate to SLU in EVAs, we describe a general and robust framework for intent and entity extraction. Our primary goal is to create accurate and scalable SLU that can be widely deployed for a large class of EVA applications with little need for human in"
2021.naacl-industry.9,H05-1057,0,0.19932,"Missing"
2021.naacl-industry.9,D14-1181,0,0.00373805,"elicit predictable user responses making them suitable for recognition with GLMs, in our goal-oriented dialogs deployed in EVAs we have observed that is not always the case. Statistical language models (SLMs) paired with intent classifiers and entity extraction methods can outperform GLMs. Therefore, we use SLMs built from n-grams or a hybrid LM combining SLMs and GLMs. 2.2 Intent Classification We employ a linear Support Vector Machine (SVM) for intent classification, using n-gram based TFIDF features. Although classifiers based on deep neural networks have gained popularity in recent years (Kim, 2014), linear classifiers remain as strong baselines (Wang and Manning, 2012), particularly on short text, with their ability to efficiently handle highFramework for Intent and Entity Extraction In this section we describe the framework for simultaneous intent and entity extraction with confidence modeling. An illustration of the overall pipeline is show in Figure 1. We introduce the main components consist64 dimensional sparse features, and their training stability through convex optimization. In SLU, the outputs from ASR are inherently uncertain and erroneous. For example, an utterance correspond"
2021.naacl-industry.9,P12-2018,0,0.0290699,"recognition with GLMs, in our goal-oriented dialogs deployed in EVAs we have observed that is not always the case. Statistical language models (SLMs) paired with intent classifiers and entity extraction methods can outperform GLMs. Therefore, we use SLMs built from n-grams or a hybrid LM combining SLMs and GLMs. 2.2 Intent Classification We employ a linear Support Vector Machine (SVM) for intent classification, using n-gram based TFIDF features. Although classifiers based on deep neural networks have gained popularity in recent years (Kim, 2014), linear classifiers remain as strong baselines (Wang and Manning, 2012), particularly on short text, with their ability to efficiently handle highFramework for Intent and Entity Extraction In this section we describe the framework for simultaneous intent and entity extraction with confidence modeling. An illustration of the overall pipeline is show in Figure 1. We introduce the main components consist64 dimensional sparse features, and their training stability through convex optimization. In SLU, the outputs from ASR are inherently uncertain and erroneous. For example, an utterance corresponding to “I want to buy a phone” may result in multiple recognition hypoth"
2021.naacl-industry.9,W09-3919,0,0.0447417,"alization is needed to convert the entity to the desired format. In any case, additional confidence p(ˆ y = y|~x) = 1  P 1 + exp − j λj xj (1) where ~x is the confidence predictor feature vector, yˆ is the predicted label (including all entities and intents) and y is the reference label. Confidence predictors xj depend on the inputs and outputs of the SLU system and the feature weights that are estimated during confidence model training are denoted by λj . We used a number of ASR confidence scores, based on posterior probabilities, as well as comparing the ASR best path to alternative paths (Williams and Balakrishnan, 2009). Basic statistics of word-level scores were computed to create utterance-level features. The number of ASR n-best was used as another feature as an indication of ASR uncertainty (larger number of n-best shows uncertainty). We also used the text classification scores as semantic features. Another semantic feature that we used was the predicted intent category encoded as a 1-hot vector over the intent classes. ASR confi65 dence for digits or the number of digits in the ASR n-best text were also added as features. Finally, since for number and date capture dialog states we utilized a text classi"
C00-1007,P98-1006,1,0.717962,"t . for our example input.  In the third experiment, as described in Section 3, we employ the supertag-based tree model whose parameters consist of whether a lexeme ld with supertag sd is a dependent of lm with supertag sm . Furthermore we use the supertag information provided by the XTAG grammar to order the dependents. This model generates There was no cost estimate for the second phase . for our example input, which is indeed the sentence found in the WSJ. As in the case of machine translation, evaluation in generation is a complex issue. We use two metrics suggested in the MT literature (Alshawi et al., 1998) based on string edit distance between the output of the generation system and the reference corpus string from the WSJ. These metrics, simple accuracy and generation accuracy, allow us to evaluate without human intervention, automatically and objectively.4 Simple accuracy is the number of insertion (I ), deletion (D) and substitutions (S ) errors between the target language strings in the test corpus and the strings produced by the generation model. The metric is summarized in Equation (1). R is the number of tokens in the target string. This metric is similar to the string distance metric us"
C00-1007,J99-2004,1,0.386226,"imate α there was α1 γ 3 no γ 1 2 cost for γ γ 2 4 phase α 1 the second γ 1 γ 5 Figure 3: Derivation tree for LTAG derivation of There was no cost estimate for the second phase 3 System Overview is composed of three modules: the Tree Chooser, the Unraveler, and the Linear Precedence (LP) Chooser. The input to the system is a dependency tree as shown in Figure 4. Note that the nodes are labeled only with lexemes, not with supertags.2 The Tree Chooser then uses a stochastic tree model to choose TAG trees for the nodes in the input structure. This step can be seen as analogous to supertagging"" (Bangalore and Joshi, 1999), except that now supertags (i.e., names of trees) must be found for words in a tree rather than for words in a linear sequence. The Unraveler then uses the XTAG grammar to produce a lattice of all possible linearizations that are compatible with the supertagged tree and the XTAG. The LP Chooser then chooses the most likely traversal of this lattice, given a language model. We discuss the three components in more detail. The Tree Chooser draws on a tree model, which is a representation of XTAG derivation for 1,000,000 words of the Wall Street Journal.3 The Tree Chooser makes the simplifying as"
C00-1007,W00-1401,1,0.513445,"ata and both models improve over the baseline LR model. Supertags incorporate richer information such as argument and adjunct distinction, and number and types of arguments. We expect to improve the performance of the supertag-based model by taking these features into account. In ongoing work, we have developed treebased metrics in addition to the string-based presented here, in order to evaluate stochastic generation models. We have also attempted to correlate these quantitative metrics with human qualitative judgements. A detailed discussion of these experiments and results is presented in (Bangalore et al., 2000). 5 Comparison with Langkilde & Knight Langkilde and Knight (1998a) use a handcrafted grammar that maps semantic representations to sequences of words with linearization constraints. A complex semantic structure is translated to a lattice, and a bigram language model then chooses among the possible surface strings encoded in the lattice. The system of Langkilde & Knight, Nitrogen, is similar to Fergus in that generation is divided into two phases, the rst of which results in a lattice from which a surface string is chosen during the second phase using a language model (in our case a trigram mo"
C00-1007,P98-1116,0,0.841501,". However, in other applications for NLG the variety of the output is much bigger, and the demands on the quality of the output somewhat less stringent. A typical example is NLG in the context of (interlingua- or transfer-based) machine translation. Another reason for relaxing the quality of the output may be that not enough time is available to develop a full grammar for a new target language in NLG. In all these cases, stochastic (empiricist"") methods provide an alternative to hand-crafted (ationalist"") approaches to NLG. To our knowledge, the rst to use stochastic techniques in NLG were Langkilde and Knight (1998a) and (1998b). In this paper, we present Fergus (Flexible Empiricist/Rationalist Generation Using Syntax). follows Langkilde and Knight's seminal work in using an n-gram language model, but we augment it with a tree-based stochastic model and a traditional tree-based syntactic grammar. More recent work on aspects of stochastic generation include (Langkilde and Knight, 2000), (Malouf, 1999) and (Ratnaparkhi, 2000). Before we describe in more detail how we use stochastic models in NLG, we recall the basic tasks in NLG (Rambow and Korelsky, 1992; Reiter, 1994). During text planning, content and"
C00-1007,W98-1426,0,0.875342,". However, in other applications for NLG the variety of the output is much bigger, and the demands on the quality of the output somewhat less stringent. A typical example is NLG in the context of (interlingua- or transfer-based) machine translation. Another reason for relaxing the quality of the output may be that not enough time is available to develop a full grammar for a new target language in NLG. In all these cases, stochastic (empiricist"") methods provide an alternative to hand-crafted (ationalist"") approaches to NLG. To our knowledge, the rst to use stochastic techniques in NLG were Langkilde and Knight (1998a) and (1998b). In this paper, we present Fergus (Flexible Empiricist/Rationalist Generation Using Syntax). follows Langkilde and Knight's seminal work in using an n-gram language model, but we augment it with a tree-based stochastic model and a traditional tree-based syntactic grammar. More recent work on aspects of stochastic generation include (Langkilde and Knight, 2000), (Malouf, 1999) and (Ratnaparkhi, 2000). Before we describe in more detail how we use stochastic models in NLG, we recall the basic tasks in NLG (Rambow and Korelsky, 1992; Reiter, 1994). During text planning, content and"
C00-1007,A00-2023,0,0.565882,"new target language in NLG. In all these cases, stochastic (empiricist"") methods provide an alternative to hand-crafted (ationalist"") approaches to NLG. To our knowledge, the rst to use stochastic techniques in NLG were Langkilde and Knight (1998a) and (1998b). In this paper, we present Fergus (Flexible Empiricist/Rationalist Generation Using Syntax). follows Langkilde and Knight's seminal work in using an n-gram language model, but we augment it with a tree-based stochastic model and a traditional tree-based syntactic grammar. More recent work on aspects of stochastic generation include (Langkilde and Knight, 2000), (Malouf, 1999) and (Ratnaparkhi, 2000). Before we describe in more detail how we use stochastic models in NLG, we recall the basic tasks in NLG (Rambow and Korelsky, 1992; Reiter, 1994). During text planning, content and structure of the target text are determined to achieve the overall communicative goal. During sentence planning, linguistic means { in particular, lexical and syntactic means { are determined to convey smaller pieces of meaning. During realization, the speci cation chosen in sentence planning is transformed into a surface string, by linearizing and in ecting words in the sen"
C00-1007,P85-1012,0,0.435381,"ociated with a whole tree (rather than just a phrase-structure rule, for example), we can specify both the predicate-argument structure of the lexeme (by including nodes at which its arguments must substitute) and morphosyntactic constraints such as subject-verb agreement within the structure associated with the lexeme. This property is referred to as TAG's extended domain of locality. Note that in an LTAG, there is no distinction between lexicon and grammar. A sample grammar is shown in Figure 1. We depart from XTAG in our treatment of trees for adjuncts (such as adverbs), and instead follow McDonald and Pustejovsky (1985). While in XTAG the elementary tree for an adjunct contains phrase structure that attaches the adjunct to nodes in another tree with the stag anchored by 1 Det 2 N 3 Aux 4 Prep or 5 Adj adjoins to direction NP right N right S, VP right NP, VP left S right N right Figure 2: Adjunction table for grammar fragment speci ed label (say, VP) from the speci ed direction (say, from the left), in our system the trees for adjuncts simply express their active valency, but not how they connect to the lexical item they modify. This information is kept in the adjunction table which is associated with the gra"
C00-1007,A92-1006,1,0.670138,"the rst to use stochastic techniques in NLG were Langkilde and Knight (1998a) and (1998b). In this paper, we present Fergus (Flexible Empiricist/Rationalist Generation Using Syntax). follows Langkilde and Knight's seminal work in using an n-gram language model, but we augment it with a tree-based stochastic model and a traditional tree-based syntactic grammar. More recent work on aspects of stochastic generation include (Langkilde and Knight, 2000), (Malouf, 1999) and (Ratnaparkhi, 2000). Before we describe in more detail how we use stochastic models in NLG, we recall the basic tasks in NLG (Rambow and Korelsky, 1992; Reiter, 1994). During text planning, content and structure of the target text are determined to achieve the overall communicative goal. During sentence planning, linguistic means { in particular, lexical and syntactic means { are determined to convey smaller pieces of meaning. During realization, the speci cation chosen in sentence planning is transformed into a surface string, by linearizing and in ecting words in the sentence (and typically, adding function words). As in the work by Langkilde and Knight, our work ignores the text planning stage, but it does address the sentence planning an"
C00-1007,A00-2026,0,0.190954,"stochastic (empiricist"") methods provide an alternative to hand-crafted (ationalist"") approaches to NLG. To our knowledge, the rst to use stochastic techniques in NLG were Langkilde and Knight (1998a) and (1998b). In this paper, we present Fergus (Flexible Empiricist/Rationalist Generation Using Syntax). follows Langkilde and Knight's seminal work in using an n-gram language model, but we augment it with a tree-based stochastic model and a traditional tree-based syntactic grammar. More recent work on aspects of stochastic generation include (Langkilde and Knight, 2000), (Malouf, 1999) and (Ratnaparkhi, 2000). Before we describe in more detail how we use stochastic models in NLG, we recall the basic tasks in NLG (Rambow and Korelsky, 1992; Reiter, 1994). During text planning, content and structure of the target text are determined to achieve the overall communicative goal. During sentence planning, linguistic means { in particular, lexical and syntactic means { are determined to convey smaller pieces of meaning. During realization, the speci cation chosen in sentence planning is transformed into a surface string, by linearizing and in ecting words in the sentence (and typically, adding function wo"
C00-1007,W94-0319,0,0.0144855,"techniques in NLG were Langkilde and Knight (1998a) and (1998b). In this paper, we present Fergus (Flexible Empiricist/Rationalist Generation Using Syntax). follows Langkilde and Knight's seminal work in using an n-gram language model, but we augment it with a tree-based stochastic model and a traditional tree-based syntactic grammar. More recent work on aspects of stochastic generation include (Langkilde and Knight, 2000), (Malouf, 1999) and (Ratnaparkhi, 2000). Before we describe in more detail how we use stochastic models in NLG, we recall the basic tasks in NLG (Rambow and Korelsky, 1992; Reiter, 1994). During text planning, content and structure of the target text are determined to achieve the overall communicative goal. During sentence planning, linguistic means { in particular, lexical and syntactic means { are determined to convey smaller pieces of meaning. During realization, the speci cation chosen in sentence planning is transformed into a surface string, by linearizing and in ecting words in the sentence (and typically, adding function words). As in the work by Langkilde and Knight, our work ignores the text planning stage, but it does address the sentence planning and the realizati"
C00-1007,C98-1112,0,\N,Missing
C00-1007,C98-1006,1,\N,Missing
C00-1054,W00-0508,1,0.374861,"bol in the input and generates the output symbol associated with the transition. In other words, an FST can be regarded as a 2-tape FSA with an input tape from which the input symbols are read and an output tape where the output symbols are written. Finite-state machines have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Joshi and Hopely, 1997; Bangalore, 1997), parsing (Roche, 1999), and machine translation (Bangalore and Riccardi, 2000). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding and (c) associated with a calculus for composing machines which allows for straightforward integration of constraints from various levels of language processing. Furthermore, software implementing the finite-state calculus is available for research purposes (Mohri et al., 1998). Another motivation for our choice of finite-state models is that they enable tight integration of language processing with speech and gesture recognition. 3 Fin"
C00-1054,P98-1101,0,0.0118239,"Missing"
C00-1054,P97-1036,1,0.73098,"ltiple modes. In that approach, speech and gesture recognition produce nbest lists of recognition results which are assigned typed feature structure representations (Carpenter, 1992) and passed to a multidimensional chart parser that uses a multimodal unification-based grammar to combine the representations assigned to the input elements. Possible multimodal interpretations are then ranked and the optimal interpretation is passed on for execution. This approach overcomes many of the limitations of previous approaches to multimodal integration such as (Bolt, 1980; Neal and Shapiro, 1991) (See (Johnston et al., 1997)(p. 282)). It supports speech with multiple gestures, visual parsing of unimodal gestures, and its declarative nature facilitates rapid prototyping and iterative development of multimodal systems. Also, the unification-based approach allows for mutual compensation of recognition errors in the individual modalities (Oviatt, 1999). However, the unification-based approach does not allow for tight-coupling of multimodal parsing with speech and gesture recognition. Compensation effects are dependent on the correct answer appearing in the n-best list of interpretations assigned to each mode. Multimo"
C00-1054,P98-1102,1,0.696654,"mputing devices (PDAs, nextgeneration phones) that offer limited screen real estate, and other keyboard-less platforms such as public information kiosks. To realize their full potential, multimodal interfaces need to support not just input from multiple modes, but synergistic multimodal utterances optimally distributed over the available modes (JohnSrinivas Bangalore AT&T Labs - Research Shannon Laboratory, 180 Park Ave Florham Park, NJ 07932, USA srini@research.att.com ston et al., 1997). In order to achieve this, an effective method for integration of content from different modes is needed. Johnston (1998b) shows how techniques from natural language processing (unification-based grammars and chart parsing) can be adapted to support parsing and interpretation of utterances distributed over multiple modes. In that approach, speech and gesture recognition produce nbest lists of recognition results which are assigned typed feature structure representations (Carpenter, 1992) and passed to a multidimensional chart parser that uses a multimodal unification-based grammar to combine the representations assigned to the input elements. Possible multimodal interpretations are then ranked and the optimal i"
C00-1054,J94-3001,0,0.015825,"are finite-state automata (FSA) where each transition consists of an input and an output symbol. The transition is traversed if its input symbol matches the current symbol in the input and generates the output symbol associated with the transition. In other words, an FST can be regarded as a 2-tape FSA with an input tape from which the input symbols are read and an output tape where the output symbols are written. Finite-state machines have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Joshi and Hopely, 1997; Bangalore, 1997), parsing (Roche, 1999), and machine translation (Bangalore and Riccardi, 2000). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding and (c) associated with a calculus for composing machines which allows for straightforward integration of constraints from various levels of language processing. Furthermore, software implementing the finite-state calculus is available for research purposes (Mohri"
C00-1054,P84-1038,0,0.177129,"re each transition consists of an input and an output symbol. The transition is traversed if its input symbol matches the current symbol in the input and generates the output symbol associated with the transition. In other words, an FST can be regarded as a 2-tape FSA with an input tape from which the input symbols are read and an output tape where the output symbols are written. Finite-state machines have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Joshi and Hopely, 1997; Bangalore, 1997), parsing (Roche, 1999), and machine translation (Bangalore and Riccardi, 2000). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding and (c) associated with a calculus for composing machines which allows for straightforward integration of constraints from various levels of language processing. Furthermore, software implementing the finite-state calculus is available for research purposes (Mohri et al., 1998). Another motivati"
C00-1054,J97-2003,0,0.0121382,"Figure 4 shows the speech and gesture streams and the resulting combined meaning. The elements on the meaning tape are concatenated and the buffer references are replaced to yield S: email this person G: Gp e1 M: email([ person( e1 ) and that organization , org( e2 ) ]) Go e2 Figure 4: Messaging domain example email([person(objid367); org(objid893)]). As more recursive semantic phenomena such as possessives and other complex noun phrases are added to the grammar the resulting machines become larger. However, the computational consequences of this can be lessened by lazy evaluation techniques (Mohri, 1997) and we believe that this finitestate approach to constructing semantic representations is viable for a broad range of sophisticated language interface tasks. We have implemented a sizeable multimodal CFG for VPQ (See Section 1): 417 rules and a lexicon of 2388 words. 3.2 Multimodal Finite-state Transducers While a three-tape finite-state automaton is feasible in principle (Rosenberg, 1964), currently available tools for finite-state language processing (Mohri et al., 1998) only support finite-state transducers (FSTs) (two tapes). Furthermore, speech recognizers typically do not support the us"
C00-1054,W94-0204,0,0.0282245,"a such as possessives and other complex noun phrases are added to the grammar the resulting machines become larger. However, the computational consequences of this can be lessened by lazy evaluation techniques (Mohri, 1997) and we believe that this finitestate approach to constructing semantic representations is viable for a broad range of sophisticated language interface tasks. We have implemented a sizeable multimodal CFG for VPQ (See Section 1): 417 rules and a lexicon of 2388 words. 3.2 Multimodal Finite-state Transducers While a three-tape finite-state automaton is feasible in principle (Rosenberg, 1964), currently available tools for finite-state language processing (Mohri et al., 1998) only support finite-state transducers (FSTs) (two tapes). Furthermore, speech recognizers typically do not support the use of a three-tape FSA as a language model. In order to implement our approach, we convert the three-tape FSA (Figure 3) into an FST, by decomposing the transition symbols into an input component (G  W ) and output component M , thus resulting in a function, T :(G  W ) ! M . This corresponds to a transducer in which gesture symbols and words are on the input tape and the meaning is on the"
C00-1054,C98-1098,0,\N,Missing
C00-1054,C98-1099,1,\N,Missing
C02-1134,J93-2003,0,\N,Missing
C02-1134,P99-1068,0,\N,Missing
C02-1134,P98-1006,1,\N,Missing
C02-1134,C98-1006,1,\N,Missing
C02-1134,P98-2186,0,\N,Missing
C02-1134,C98-2181,0,\N,Missing
C02-1138,J99-2004,1,0.680382,"lar ideas can be used to port FERGUS to different domains with little manual effort. 3.1 Description of the FERGUS Surface Realizer Given an underspecified dependency tree representing one sentence as input, FERGUS outputs the best surface string according to its stochastic modeling. Each node in the input tree corresponds to a lexeme. Nodes that are related by grammatical function are linked together. Surface ordering of the lexemes remains unspecified in the tree. FERGUS consists of three models: tree chooser, unraveler, and linear precedence chooser. The tree chooser associates a supertag (Bangalore and Joshi, 1999) from a treeadjoining grammar (TAG) with each node in the underspecified dependency tree. This partially specifies the output string’s surface order; it is constrained by grammatical constraints encoded by the supertags (e.g. subcategorization constraints, voice), but remains free otherwise (e.g. ordering of modifiers). The tree chooser uses a stochastic tree model (TM) to select a supertag for each node in the tree based on local tree context. The unraveler takes the resulting semi-specified TAG derivation tree and creates a word lattice corresponding to all of the potential surface orderings"
C02-1138,C00-1007,1,0.914188,"Missing"
C02-1138,W00-1401,1,0.891425,"rom all strings like these, with duplicates, in the Communicator system by replacing the slot names with fillers according to a probability distribution. Furthermore, dependency parses are assigned to the resulting strings by hand. In the first series of experiments, we ascertain the output quality of FERGUS using the XTAG grammar on different training corpora. We vary the TM’s training corpus to be either PTB or HH. We do the same for the LM’s training corpus. Assessing the output quality of a generator is a complex issue. Here, we select as our metric understandability accuracy, defined in (Bangalore et al., 2000) as quantifying the differPTB LM HH LM PTB TM 0.30 0.37 HH TM 0.38 0.41 Table 2: Average understandability accuracies using XTAG-Based FERGUS for various kinds of training data PTB LM HH LM PTB TM 0.39 0.33 Table 3: Average understandability accuracies using automatically-extracted grammar based FERGUS for various kinds of training data ence between the generator output, in terms of both dependency tree and surface string, and the desired reference output. (Bangalore et al., 2000) finds this metric to correlate well with human judgments of understandability and quality. Understandability accur"
C02-1138,W01-0520,1,0.913667,"a trigram language model (LM), specifying the output string completely. Certain resources are required in order to train FERGUS. A TAG grammar is needed— the source of the supertags with which the semi-specified TAG derivation tree is annotated. There needs to be a treebank in order to obtain the stochastic model TM driving the tree chooser. There also needs to be a corpus of sentences in order to train the language model LM required for the LP chooser. 3.2 Labor-Minimizing Approaches to Training FERGUS The resources that are needed to train FERGUS seem quite labor intensive to develop. But (Bangalore et al., 2001) show that automatically generated version of these resources can be used by FERGUS to obtain quality output. Two kinds of TAG grammar are used in (Bangalore et al., 2001). One kind is a manually developed, broad-coverage grammar for English: the XTAG grammar (XTAG-Group, 2001). It consists of approximately 1000 tree frames. Disadvantages of using XTAG are the considerable amount of human labor expended in its development and the lack of a treebank based on XTAG—the only way to estimate parameters in the TM is to rely on a heuristic mapping of XTAG tree frames onto a pre-existing treebank (Ban"
C02-1138,A00-2018,0,0.00691146,"disadvantages of using the XTAG grammar, but they introduce potential complications— notably, an extracted grammar’s size is often much larger than that of XTAG, typically more than 2000 tree frames, potentially leading to a larger sparse data problem, and also the resulting grammar is not hand-checked. Two kinds of treebank are used in (Bangalore et al., 2001). One kind is the Penn Treebank (Marcus et al., 1993). It consists of approximately 1,000,000 words of hand-checked, bracketed text. The text consists of Wall Street Journal news articles. The other kind of treebank is the BLLIP corpus (Charniak, 2000). It consists of approximately 40,000,000 words of text that has been parsed by a broad-coverage statistical parser. The text consists of Wall Street Journal news and newswire articles. The advantage of the former is that it has been handchecked, whereas the latter has the advantage of being easily produced and hence can easily be enlarged. (Bangalore et al., 2001) experimentally determine how the quality and quantity of the resources used in training FERGUS affect the output quality of the generator. They find that while a better quality annotated corpus (Penn Treebank) results in better mode"
C02-1138,P00-1058,0,0.0261357,"ed in (Bangalore et al., 2001). One kind is a manually developed, broad-coverage grammar for English: the XTAG grammar (XTAG-Group, 2001). It consists of approximately 1000 tree frames. Disadvantages of using XTAG are the considerable amount of human labor expended in its development and the lack of a treebank based on XTAG—the only way to estimate parameters in the TM is to rely on a heuristic mapping of XTAG tree frames onto a pre-existing treebank (Bangalore and Joshi, 1999). Another kind of grammar is a TAG automatically extracted from a treebank using the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). These techniques extract a linguistically motivated TAG using heuristics programmed using a modicum of human labor. They nullify the disadvantages of using the XTAG grammar, but they introduce potential complications— notably, an extracted grammar’s size is often much larger than that of XTAG, typically more than 2000 tree frames, potentially leading to a larger sparse data problem, and also the resulting grammar is not hand-checked. Two kinds of treebank are used in (Bangalore et al., 2001). One kind is the Penn Treebank (Marcus et al., 1993). It consists of approximately 1,00"
C02-1138,P95-1034,0,0.0350299,"se stochastic NLG components can be made to work together, that they can be ported to new domains with apparent ease, and that such NLG components can be integrated in a real-time dialog system. 1 Introduction Systems that interact with the user via natural language are in their infancy. As these systems mature and become more complex, it would be desirable for a system developer if there were automatic methods for creating natural language generation (NLG) components that can produce quality output efficiently. Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimoto et al., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. In contrast, this paper investigates how different stochastic NLG components can be made to work together effectively, whether they can easily be ported to new domains, and whether they can be integrated in a real-time dialog system. DM Dialog Manager SPoT Sentence Planner Implicit−confirm(NEWARK) Implicit−confirm(DALLAS) Request(DEPART−DATE) period"
C02-1138,P98-1116,0,0.0580485,"ade to work together, that they can be ported to new domains with apparent ease, and that such NLG components can be integrated in a real-time dialog system. 1 Introduction Systems that interact with the user via natural language are in their infancy. As these systems mature and become more complex, it would be desirable for a system developer if there were automatic methods for creating natural language generation (NLG) components that can produce quality output efficiently. Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimoto et al., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. In contrast, this paper investigates how different stochastic NLG components can be made to work together effectively, whether they can easily be ported to new domains, and whether they can be integrated in a real-time dialog system. DM Dialog Manager SPoT Sentence Planner Implicit−confirm(NEWARK) Implicit−confirm(DALLAS) Request(DEPART−DATE) period soft−merge Request(D−D) Imp−co"
C02-1138,J93-2004,0,0.0246778,"sing the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). These techniques extract a linguistically motivated TAG using heuristics programmed using a modicum of human labor. They nullify the disadvantages of using the XTAG grammar, but they introduce potential complications— notably, an extracted grammar’s size is often much larger than that of XTAG, typically more than 2000 tree frames, potentially leading to a larger sparse data problem, and also the resulting grammar is not hand-checked. Two kinds of treebank are used in (Bangalore et al., 2001). One kind is the Penn Treebank (Marcus et al., 1993). It consists of approximately 1,000,000 words of hand-checked, bracketed text. The text consists of Wall Street Journal news articles. The other kind of treebank is the BLLIP corpus (Charniak, 2000). It consists of approximately 40,000,000 words of text that has been parsed by a broad-coverage statistical parser. The text consists of Wall Street Journal news and newswire articles. The advantage of the former is that it has been handchecked, whereas the latter has the advantage of being easily produced and hence can easily be enlarged. (Bangalore et al., 2001) experimentally determine how the"
C02-1138,W00-0306,0,0.203755,"y can be ported to new domains with apparent ease, and that such NLG components can be integrated in a real-time dialog system. 1 Introduction Systems that interact with the user via natural language are in their infancy. As these systems mature and become more complex, it would be desirable for a system developer if there were automatic methods for creating natural language generation (NLG) components that can produce quality output efficiently. Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimoto et al., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. In contrast, this paper investigates how different stochastic NLG components can be made to work together effectively, whether they can easily be ported to new domains, and whether they can be integrated in a real-time dialog system. DM Dialog Manager SPoT Sentence Planner Implicit−confirm(NEWARK) Implicit−confirm(DALLAS) Request(DEPART−DATE) period soft−merge Request(D−D) Imp−conf(N) Imp−conf(D) FERGUS"
C02-1138,C00-2126,0,0.0197615,"mains with apparent ease, and that such NLG components can be integrated in a real-time dialog system. 1 Introduction Systems that interact with the user via natural language are in their infancy. As these systems mature and become more complex, it would be desirable for a system developer if there were automatic methods for creating natural language generation (NLG) components that can produce quality output efficiently. Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimoto et al., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. In contrast, this paper investigates how different stochastic NLG components can be made to work together effectively, whether they can easily be ported to new domains, and whether they can be integrated in a real-time dialog system. DM Dialog Manager SPoT Sentence Planner Implicit−confirm(NEWARK) Implicit−confirm(DALLAS) Request(DEPART−DATE) period soft−merge Request(D−D) Imp−conf(N) Imp−conf(D) FERGUS Surface Generator Flying"
C02-1138,N01-1003,1,0.912551,"ve goal. During sentence planning, linguistic means—in particular, lexical and syntactic means—are determined to convey smaller pieces of meaning. During realization, the specification chosen in sentence planning is transformed into a surface string by linearizing and inflecting words in the sentence (and typically, adding function words). Figure 1 shows how such components cooperate to generate text corresponding to a set of communicative goals. Our work addresses both the sentence planning stage and the realization stage. The sentence planning stage is embodied by the SPoT sentence planner (Walker et al., 2001), while the surface realization stage is embodied by the FERGUS surface realizer (Bangalore and Rambow, 2000). We extend the work of (Walker et al., 2001) and (Bangalore and Rambow, 2000) in various ways. We show that apparently each Features Used all domain-independent task-independent task-dependent of SPoT and FERGUS can be ported to different domains with little manual effort. We then show that these two components can work together effectively. Finally, we show the on-line integration of FERGUS with a dialog system. 2 Testing the Domain Independence of Sentence Planning In this section, w"
C02-1138,C98-1112,0,\N,Missing
C02-1138,C98-1114,1,\N,Missing
C02-1138,P98-1118,1,\N,Missing
C02-2026,P97-1003,0,0.0133068,"antics One type of Natural Language Understanding (NLU) application is exemplified by the database access problem: the user may type in free source language text, but the NLU component must map this text to a fixed set of actions dictated by the underlying application program. We will call such NLU applications “applicationsemantic NLU”. Other examples of applicationsemantic NLU include interfaces to commandbased applications (such as airline reservation systems), often in the guise of dialog systems. Several general-purpose off-the-shelf (OTS) parsers have become widely available (Lin, 1994; Collins, 1997). For application-semantic NLU, it is possible to use such an OTS parser in conjunction with a post-processor which transfers the output of the parser (be it phrase structure or dependency) to the domain semantics. In addition to mapping the parser output to application semantics, the post-processor often must also “correct” the output of the parser: the parser may be tailored for a particular domain (such as main presents linguistic constructions not found in the original domain (such as questions). It may also be the case that the OTS parser consistently misanalyzes certain lexemes because t"
C02-2026,C94-1079,0,0.0338234,"pecific Semantics One type of Natural Language Understanding (NLU) application is exemplified by the database access problem: the user may type in free source language text, but the NLU component must map this text to a fixed set of actions dictated by the underlying application program. We will call such NLU applications “applicationsemantic NLU”. Other examples of applicationsemantic NLU include interfaces to commandbased applications (such as airline reservation systems), often in the guise of dialog systems. Several general-purpose off-the-shelf (OTS) parsers have become widely available (Lin, 1994; Collins, 1997). For application-semantic NLU, it is possible to use such an OTS parser in conjunction with a post-processor which transfers the output of the parser (be it phrase structure or dependency) to the domain semantics. In addition to mapping the parser output to application semantics, the post-processor often must also “correct” the output of the parser: the parser may be tailored for a particular domain (such as main presents linguistic constructions not found in the original domain (such as questions). It may also be the case that the OTS parser consistently misanalyzes certain l"
C02-2026,W02-2214,1,0.870509,"Missing"
C02-2026,J00-1003,0,0.0131956,"beled by the name of an elementary tree machine by the lexicalized version of that tree machine. Of course, in each iteration, there are many more replacements than in the previous iteration. We use 5 rounds of iteration; obviously, the number of iterations restrict the syntactic complexity (but not the length) of recognized input. However, because we output brackets in the FSTs, we obtain a parse with full syntactic/lexical semantic (i.e., dependency) structure, not a “shallow parse”. This construction is in many ways similar to similar constructions proposed for CFGs, in particular that of (Nederhof, 2000). One difference is that, since we start from TAG, recursion is already factored, and we need not find cycles in the rules of the grammar. 5 Experimental Results We present results in which our classes are defined entirely with respect to syntactic behavior. This is because we do not have available an important corpus annotated with semantics. We train on the Wall Street Journal (WSJ) corpus. We evaluate by taking a list of 205 sentences which are chosen at random from entries to W ORDS E YE made by the developers (who were testing the graphical component using a different parser). Their avera"
C12-1013,I11-1048,1,0.795853,"Missing"
C12-1013,C04-1151,0,0.0291225,"Web has also focused primarily on a specific pair of languages. Even though in principle the algorithm and framework can be extended to other pairs of languages, it requires either parallel webpages or comparable documents to trigger the process. For example, (Resnik and Smith, 2003) used a crawling procedure to harvest parallel text in English-Arabic starting from the Internet Archive and using several language specific patterns in the URLs. The work in (Munteanu and Marcu, 2005) matches comparable documents in English-Chinese and English-Arabic and subsequently extracts parallel text while (Fung and Cheung, 2004) extract parallel text from quasi-comparable documents in English-Chinese. The work in (Hong et al., 2010; Shi et al., 2006) uses several web crawling strategies for harvesting parallel text in English-Chinese and the work in (Utiyama et al., 2009) addresses the extraction of Japanese-English parallel text from mixed language pages. Conventionally, the crawling procedure to detect websites containing parallel text has been through a query-based approach (Resnik and Smith, 2003; Chen and Nie, 2000; Hong et al., 2010). They submit carefully constructed queries to a search engine that might yield"
C12-1013,C10-1054,0,0.437845,"utomatically by exploiting multilingual webpages provides a low-cost, scalable alternative to expensive expert human translations. Moreover, bilingual subject matter experts may be extremely difficult to find for certain 1 http://news.netcraft.com/archives/2011/09/06/september-2011-web-server-survey.html Proceedings of COLING 2012: Technical Papers, pages 201–214, COLING 2012, Mumbai, December 2012. 201 language pairs. As a result, web harvesting of parallel text has been addressed extensively in the recent past (Resnik and Smith, 2003; Shi et al., 2006; Pomikálek, 2008; Utiyama et al., 2009; Hong et al., 2010; Almeida and Simões, 2010; Uszkoreit et al., 2010). Parallel text acquisition can be performed by examining the Web in either an unstructured or structured way. In the unstructured view of the Web, a large Web index is used as a starting point and the webpages are matched using cross-language document retrieval (see Figure 1(a)). The basic idea behind such an approach is to use a seed translation model to translate the entire index in one particular language and then match the pages using document retrieval techniques (Uszkoreit et al., 2010). The feasibility of such an approach is dependent"
C12-1013,W04-3250,0,0.0497571,"and we show that parallel data harvested from the Web provides significant improvement. 7 Unsupervised parallel text acquisition: English-Hindi In the previous section, we demonstrated the utility of our multilingual crawler for languages with reasonable resources. However, such data may not be present for several other language pairs. One of the main objectives of our Web crawling scheme is to harvest parallel text 208 Figure 5: Translation quality as measured through BLEU score for various test sets with and without web crawled parallel text. Hatched bars indicate insignificant difference (Koehn, 2004) with respect to the baseline model built from Europarl data. for language pairs with limited resources, i.e., lack of publicly available large database or language resources. As an instantiation of this goal, we conducted a detailed study on English-Hindi. We used the collocated link extraction procedure described in Section 4.1 to compile 1638 potential entry points in English-Hindi. Unlike the language pairs used in the previous section, we did not have access to parallel text or a bilingual dictionary for English-Hindi. Hence, we used a completely unsupervised scheme to harvest parallel te"
C12-1013,2005.mtsummit-papers.11,0,0.31701,"esent semi-supervised and unsupervised approaches to harvesting multilingual text that rely on a key observation of link collocation, i.e., entry points to different languages on multilingual websites are often collocated on the HTML DOM tree. Subsequently, we use an intra-site crawler and a suite of alignment procedures to generate parallel text across multiple pairs of languages and evaluate their utility in machine translation. We demonstrate significant improvements in translation quality for almost all of the 20 language pairs (with English as the source language) in the Europarl corpus (Koehn, 2005) through the addition of parallel text harvested through our approach. We also report experiments in English-Hindi translation 202 by using a completely unsupervised approach. The rest of the paper is organized as follows. In Section 2, we describe related work in web mining for parallel text and contrast our work with prior efforts. We describe the semi-supervised approach for obtaining multilingual entry points in a website in Sections 3 and 4 followed by a description of the overall framework for harvesting parallel text from the entry points. In Section 6 we present statistical machine tra"
C12-1013,P07-2045,0,0.00436595,"line translation engines (cosine distance ≥ 0.8). This step is performed to avoid the use of machine translated parallel text. A detailed description of the intra-site mining procedure can be found in (Rangarajan Sridhar et al., 2011). 6 Machine translation experiments In this section, we validate the quality of the parallel text obtained using our multilingual crawling approach through machine translation experiments. Our objective is to evaluate the translation quality with and without the parallel text harvested from the Web for a large number of language pairs. We used the Moses3 toolkit (Koehn et al., 2007) for performing phrase-based translation experiments. The standard pipeline (sentence alignment using GIZA++, phrase extraction with maximum phrase length of 7 using grow-diag-final option, lexicalized reordering model with msd-bidirectional-fe option) was used to build the 3 http://www.statmt.org/moses 207 models. The language models were interpolated Kneser-Ney discounted trigram models, all constructed using the SRILM toolkit (Stolcke, 2002). The language models were constructed only from the target side of the bitext for each language pair. We performed minimum error rate training (MERT) o"
C12-1013,ma-2006-champollion,0,0.0323119,"detectors constructed using the semi-supervised entry point detector approach 5 Parallel text acquisition The multilingual crawler generates pairs of entry points for multiple language pairs that subsequently need to be mined for parallel text. We adopt a recursive intra-site crawling approach that aligns text and URLs across the initial entry points to harvest parallel text. Our framework uses a document matching framework to align the URLs (Munteanu and Marcu, 2005) and a bilingual dictionary based dynamic programming match to align the sentences across the hypothesized parallel documents (Ma, 2006). The document matching process is constrained by a window size that is dependent on the total number of parallel URLs that need to be aligned. The framework also enables us to highly parallelize mining across multiple language pairs. The bilingual dictionary in this work was obtained by performing automatic word alignment on seed parallel data (Och and Ney, 2003). The harvested bitext was then filtered using a word-overlap filter as well as a source and target vocabulary restriction filter. By varying the thresholds for the various filters, one can control the amount and quality of the bitext"
C12-1013,J05-4003,0,0.643318,"ection 8 and conclude in Section 9 along with directions for future work. 2 Related Work Prior research on acquiring parallel text from the Web has also focused primarily on a specific pair of languages. Even though in principle the algorithm and framework can be extended to other pairs of languages, it requires either parallel webpages or comparable documents to trigger the process. For example, (Resnik and Smith, 2003) used a crawling procedure to harvest parallel text in English-Arabic starting from the Internet Archive and using several language specific patterns in the URLs. The work in (Munteanu and Marcu, 2005) matches comparable documents in English-Chinese and English-Arabic and subsequently extracts parallel text while (Fung and Cheung, 2004) extract parallel text from quasi-comparable documents in English-Chinese. The work in (Hong et al., 2010; Shi et al., 2006) uses several web crawling strategies for harvesting parallel text in English-Chinese and the work in (Utiyama et al., 2009) addresses the extraction of Japanese-English parallel text from mixed language pages. Conventionally, the crawling procedure to detect websites containing parallel text has been through a query-based approach (Resn"
C12-1013,J03-1002,0,0.00273954,"harvest parallel text. Our framework uses a document matching framework to align the URLs (Munteanu and Marcu, 2005) and a bilingual dictionary based dynamic programming match to align the sentences across the hypothesized parallel documents (Ma, 2006). The document matching process is constrained by a window size that is dependent on the total number of parallel URLs that need to be aligned. The framework also enables us to highly parallelize mining across multiple language pairs. The bilingual dictionary in this work was obtained by performing automatic word alignment on seed parallel data (Och and Ney, 2003). The harvested bitext was then filtered using a word-overlap filter as well as a source and target vocabulary restriction filter. By varying the thresholds for the various filters, one can control the amount and quality of the bitext. We also check for the fidelity of the translation by matching a subset of the harvested text for each website against translations from Google Translate and Microsoft Bing. We omit the data from the entire website if the translations have a high correlation with the online translation engines (cosine distance ≥ 0.8). This step is performed to avoid the use of ma"
C12-1013,J03-3002,0,0.685769,"arallel text, i.e., translations of text across languages. Harvesting such data automatically by exploiting multilingual webpages provides a low-cost, scalable alternative to expensive expert human translations. Moreover, bilingual subject matter experts may be extremely difficult to find for certain 1 http://news.netcraft.com/archives/2011/09/06/september-2011-web-server-survey.html Proceedings of COLING 2012: Technical Papers, pages 201–214, COLING 2012, Mumbai, December 2012. 201 language pairs. As a result, web harvesting of parallel text has been addressed extensively in the recent past (Resnik and Smith, 2003; Shi et al., 2006; Pomikálek, 2008; Utiyama et al., 2009; Hong et al., 2010; Almeida and Simões, 2010; Uszkoreit et al., 2010). Parallel text acquisition can be performed by examining the Web in either an unstructured or structured way. In the unstructured view of the Web, a large Web index is used as a starting point and the webpages are matched using cross-language document retrieval (see Figure 1(a)). The basic idea behind such an approach is to use a seed translation model to translate the entire index in one particular language and then match the pages using document retrieval techniques"
C12-1013,P06-1062,0,0.454747,"slations of text across languages. Harvesting such data automatically by exploiting multilingual webpages provides a low-cost, scalable alternative to expensive expert human translations. Moreover, bilingual subject matter experts may be extremely difficult to find for certain 1 http://news.netcraft.com/archives/2011/09/06/september-2011-web-server-survey.html Proceedings of COLING 2012: Technical Papers, pages 201–214, COLING 2012, Mumbai, December 2012. 201 language pairs. As a result, web harvesting of parallel text has been addressed extensively in the recent past (Resnik and Smith, 2003; Shi et al., 2006; Pomikálek, 2008; Utiyama et al., 2009; Hong et al., 2010; Almeida and Simões, 2010; Uszkoreit et al., 2010). Parallel text acquisition can be performed by examining the Web in either an unstructured or structured way. In the unstructured view of the Web, a large Web index is used as a starting point and the webpages are matched using cross-language document retrieval (see Figure 1(a)). The basic idea behind such an approach is to use a seed translation model to translate the entire index in one particular language and then match the pages using document retrieval techniques (Uszkoreit et al."
C12-1013,C10-1124,0,0.0841732,"ges provides a low-cost, scalable alternative to expensive expert human translations. Moreover, bilingual subject matter experts may be extremely difficult to find for certain 1 http://news.netcraft.com/archives/2011/09/06/september-2011-web-server-survey.html Proceedings of COLING 2012: Technical Papers, pages 201–214, COLING 2012, Mumbai, December 2012. 201 language pairs. As a result, web harvesting of parallel text has been addressed extensively in the recent past (Resnik and Smith, 2003; Shi et al., 2006; Pomikálek, 2008; Utiyama et al., 2009; Hong et al., 2010; Almeida and Simões, 2010; Uszkoreit et al., 2010). Parallel text acquisition can be performed by examining the Web in either an unstructured or structured way. In the unstructured view of the Web, a large Web index is used as a starting point and the webpages are matched using cross-language document retrieval (see Figure 1(a)). The basic idea behind such an approach is to use a seed translation model to translate the entire index in one particular language and then match the pages using document retrieval techniques (Uszkoreit et al., 2010). The feasibility of such an approach is dependent on the availability of a large Web index as well as"
C12-1013,2009.mtsummit-papers.18,0,0.881491,"Harvesting such data automatically by exploiting multilingual webpages provides a low-cost, scalable alternative to expensive expert human translations. Moreover, bilingual subject matter experts may be extremely difficult to find for certain 1 http://news.netcraft.com/archives/2011/09/06/september-2011-web-server-survey.html Proceedings of COLING 2012: Technical Papers, pages 201–214, COLING 2012, Mumbai, December 2012. 201 language pairs. As a result, web harvesting of parallel text has been addressed extensively in the recent past (Resnik and Smith, 2003; Shi et al., 2006; Pomikálek, 2008; Utiyama et al., 2009; Hong et al., 2010; Almeida and Simões, 2010; Uszkoreit et al., 2010). Parallel text acquisition can be performed by examining the Web in either an unstructured or structured way. In the unstructured view of the Web, a large Web index is used as a starting point and the webpages are matched using cross-language document retrieval (see Figure 1(a)). The basic idea behind such an approach is to use a seed translation model to translate the entire index in one particular language and then match the pages using document retrieval techniques (Uszkoreit et al., 2010). The feasibility of such an app"
C14-1092,C02-1134,1,0.628254,"d canonical form. A beam search decoder for normalizing social media text was presented in (Wang and Tou Ng, 2013). All these approaches rely on supervised training data to train the normalization model. In contrast, we use an unsupervised approach to learn the normalization lexicon of word forms in SMS to standard text. While several works have addressed the problem of normalizing SMS using machine translation, there has been little to no work on the translation of SMS messages across languages on a large scale. Machine translation of instant messages from English-to-Spanish was proposed in (Bangalore et al., 2002) where multiple translation hypotheses from several off-the-shelf translation engines were combined using consensus decoding. However, the approach did not consider any specific strategies for normalization and the fidelity of training bitext is questionable since it was obtained using automatic machine translation. Several products that enable multilingual communication with the aid of machine translation in conventional chat, email, etc., are available in the market. However, most of these models are trained on relatively clean bitext. 3 Problem Formulation The objective in SMS translation i"
C14-1092,P10-1079,0,0.0184663,"mparison with standard French. More recently, (Pennell and Liu, 2011) have used twitter data as a surrogate for SMS messages. Most of these previous efforts have focused on normalization, i.e., translation of SMS text to canonical text while we are interested in translating SMS messages from one language into another (Eidelman et al., 2011). Several works have addressed the problem of normalizing SMS text. A majority of these works have used statistical machine translation (character-level) to translate SMS text into standard text (Pennell and Liu, 2011; Aw et al., 2009; Kobus et al., 2008). (Beaufort et al., 2010) used a finite-state framework to learn the mapping between SMS and canonical form. A beam search decoder for normalizing social media text was presented in (Wang and Tou Ng, 2013). All these approaches rely on supervised training data to train the normalization model. In contrast, we use an unsupervised approach to learn the normalization lexicon of word forms in SMS to standard text. While several works have addressed the problem of normalizing SMS using machine translation, there has been little to no work on the translation of SMS messages across languages on a large scale. Machine transla"
C14-1092,2012.eamt-1.60,0,0.0160681,".01 27.32 es2en 31.83 33.48 32.01 33.14 32.96 Table 5: BLEU scores obtained using different alignment strategies. Only the statistical translation model was used in the evaluation. aligner (Liang et al., 2006) and the Phrasal ITG aligner (Pialign) (Neubig et al., 2011). We combined the alignments in two different ways, taking the union of alignments or majority vote for each target word. For training the translation model, we used a total of 28.5 million parallel sentences obtained from the following sources: Opensubtitles (Tiedemann and Lars Nygaard, 2004), Europarl (Koehn, 2005), TED talks (Cettolo et al., 2012) and Web. The bitext was processed to eliminate spurious pairs by restricting the English and Spanish vocabularies to the top 150k frequent words as evidenced in a large collection of monolingual corpora. We also eliminated bitext with ratio of English to Spanish words less than 0.5. The initial model was optimized using MERT over 1000 parallel sentences from the SMS domain. Results of the machine translation experiments are shown in Table 5. The test set used was 456 messages collected in a real SMS interaction (see Section 6.1). The results indicate that consensus alignment procedure is not"
C14-1092,W11-2140,0,0.0254728,".org/licenses/by/4.0/ 974 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 974–983, Dublin, Ireland, August 23-29 2014. was collected in (Fairon and Paumier, 2006) to study the idiosyncrasies of SMS language in comparison with standard French. More recently, (Pennell and Liu, 2011) have used twitter data as a surrogate for SMS messages. Most of these previous efforts have focused on normalization, i.e., translation of SMS text to canonical text while we are interested in translating SMS messages from one language into another (Eidelman et al., 2011). Several works have addressed the problem of normalizing SMS text. A majority of these works have used statistical machine translation (character-level) to translate SMS text into standard text (Pennell and Liu, 2011; Aw et al., 2009; Kobus et al., 2008). (Beaufort et al., 2010) used a finite-state framework to learn the mapping between SMS and canonical form. A beam search decoder for normalizing social media text was presented in (Wang and Tou Ng, 2013). All these approaches rely on supervised training data to train the normalization model. In contrast, we use an unsupervised approach to le"
C14-1092,fairon-paumier-2006-translated,0,0.0337658,"lation. Finally, we describe a SMS translation service built using our pipeline in Section 6 along with results from a user trial. We provide some discussion in Section 7 and conclude in Section 8. 2 Related Work One of the main challenges of building a machine translation system for SMS messages is the lack of training data in this domain. Typically, there are several legal restrictions in using consumer SMS data that precludes one from either using it completely or forces one to use it in limited capacity. Only a handful of such corpora are publicly available on the Web (Chen and Kan, 2013; Fairon and Paumier, 2006; Treurniet et al., 2012; Sanders, 2012; Tagg, 2009); they are limited in size and restricted to a few language pairs. The NUS SMS corpus (Chen and Kan, 2013) is probably the largest English SMS corpus consisting of around 41000 messages. However, these messages are characteristic of Singaporean chat lingo and not an accurate reflection of SMS style in other parts of the world. A corpus of 30000 French SMS messages This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creati"
C14-1092,C08-1056,0,0.0241218,"of SMS language in comparison with standard French. More recently, (Pennell and Liu, 2011) have used twitter data as a surrogate for SMS messages. Most of these previous efforts have focused on normalization, i.e., translation of SMS text to canonical text while we are interested in translating SMS messages from one language into another (Eidelman et al., 2011). Several works have addressed the problem of normalizing SMS text. A majority of these works have used statistical machine translation (character-level) to translate SMS text into standard text (Pennell and Liu, 2011; Aw et al., 2009; Kobus et al., 2008). (Beaufort et al., 2010) used a finite-state framework to learn the mapping between SMS and canonical form. A beam search decoder for normalizing social media text was presented in (Wang and Tou Ng, 2013). All these approaches rely on supervised training data to train the normalization model. In contrast, we use an unsupervised approach to learn the normalization lexicon of word forms in SMS to standard text. While several works have addressed the problem of normalizing SMS using machine translation, there has been little to no work on the translation of SMS messages across languages on a lar"
C14-1092,2005.mtsummit-papers.11,0,0.0213147,"n2es 28.45 28.08 27.82 28.01 27.32 es2en 31.83 33.48 32.01 33.14 32.96 Table 5: BLEU scores obtained using different alignment strategies. Only the statistical translation model was used in the evaluation. aligner (Liang et al., 2006) and the Phrasal ITG aligner (Pialign) (Neubig et al., 2011). We combined the alignments in two different ways, taking the union of alignments or majority vote for each target word. For training the translation model, we used a total of 28.5 million parallel sentences obtained from the following sources: Opensubtitles (Tiedemann and Lars Nygaard, 2004), Europarl (Koehn, 2005), TED talks (Cettolo et al., 2012) and Web. The bitext was processed to eliminate spurious pairs by restricting the English and Spanish vocabularies to the top 150k frequent words as evidenced in a large collection of monolingual corpora. We also eliminated bitext with ratio of English to Spanish words less than 0.5. The initial model was optimized using MERT over 1000 parallel sentences from the SMS domain. Results of the machine translation experiments are shown in Table 5. The test set used was 456 messages collected in a real SMS interaction (see Section 6.1). The results indicate that con"
C14-1092,2005.iwslt-1.19,0,0.0726532,"Missing"
C14-1092,2006.iwslt-papers.1,0,0.0476894,"Missing"
C14-1092,P11-1064,0,0.0191169,"458 how:que/1.822 how^are^you:how^are^you 0/0 step1.fsm WIP LM) ptable.fst Figure 1: Illustration of the hybrid translation approach using FSTs. WIP and LM refer to the finite state automata for word insertion penalty and language model, respectively. Alignment strategy GIZA++ Pialign Berkeley aligner Union Majority voting en2es 28.45 28.08 27.82 28.01 27.32 es2en 31.83 33.48 32.01 33.14 32.96 Table 5: BLEU scores obtained using different alignment strategies. Only the statistical translation model was used in the evaluation. aligner (Liang et al., 2006) and the Phrasal ITG aligner (Pialign) (Neubig et al., 2011). We combined the alignments in two different ways, taking the union of alignments or majority vote for each target word. For training the translation model, we used a total of 28.5 million parallel sentences obtained from the following sources: Opensubtitles (Tiedemann and Lars Nygaard, 2004), Europarl (Koehn, 2005), TED talks (Cettolo et al., 2012) and Web. The bitext was processed to eliminate spurious pairs by restricting the English and Spanish vocabularies to the top 150k frequent words as evidenced in a large collection of monolingual corpora. We also eliminated bitext with ratio of Eng"
C14-1092,J03-1002,0,0.00563627,"ck phrases and hence caching these phrases may result in high accuracies instead of deriving the translation using a statistical model. We took the data created in Section 4 and created a FST to represent the sentences. The motivation is to increase the precision of common entries as well as reduce the latency involved in retrieving a translation from a statistical model. An example of the FST translation paradigm is shown in Figure 1 We experimented with the notion of using a consensus-based word alignment by combining the alignment obtained through different alignment tools. We used GIZA++ (Och and Ney, 2003), Berkeley 5 http://www.opensubtitles.org 978 Cached Table how^do^you^do:como^estas thanks:gracias hello:hola hello:hello 0 0 hello.fsm hello how are you ex.fst hello how are you hola como estas Statistical Model bestpath( how:how 0 1 are^you:are^you are:are how^are:how^are 2 you:you 3 you:tu/0.757 you:que/1.460 are^you:estas/0.757 are^you:estan/1.998 how^are^you:como^esta^usted/2.358 how^are^you:como^estas/1.106 how:como/0.458 how:que/1.822 how^are^you:how^are^you 0/0 step1.fsm WIP LM) ptable.fst Figure 1: Illustration of the hybrid translation approach using FSTs. WIP and LM refer to the fin"
C14-1092,I11-1109,0,0.239618,"on of SMS style in other parts of the world. A corpus of 30000 French SMS messages This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 974 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 974–983, Dublin, Ireland, August 23-29 2014. was collected in (Fairon and Paumier, 2006) to study the idiosyncrasies of SMS language in comparison with standard French. More recently, (Pennell and Liu, 2011) have used twitter data as a surrogate for SMS messages. Most of these previous efforts have focused on normalization, i.e., translation of SMS text to canonical text while we are interested in translating SMS messages from one language into another (Eidelman et al., 2011). Several works have addressed the problem of normalizing SMS text. A majority of these works have used statistical machine translation (character-level) to translate SMS text into standard text (Pennell and Liu, 2011; Aw et al., 2009; Kobus et al., 2008). (Beaufort et al., 2010) used a finite-state framework to learn the map"
C14-1092,N13-1023,1,0.868848,"Missing"
C14-1092,sanders-2012-collecting,0,0.0129097,"vice built using our pipeline in Section 6 along with results from a user trial. We provide some discussion in Section 7 and conclude in Section 8. 2 Related Work One of the main challenges of building a machine translation system for SMS messages is the lack of training data in this domain. Typically, there are several legal restrictions in using consumer SMS data that precludes one from either using it completely or forces one to use it in limited capacity. Only a handful of such corpora are publicly available on the Web (Chen and Kan, 2013; Fairon and Paumier, 2006; Treurniet et al., 2012; Sanders, 2012; Tagg, 2009); they are limited in size and restricted to a few language pairs. The NUS SMS corpus (Chen and Kan, 2013) is probably the largest English SMS corpus consisting of around 41000 messages. However, these messages are characteristic of Singaporean chat lingo and not an accurate reflection of SMS style in other parts of the world. A corpus of 30000 French SMS messages This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 974 Proc"
C14-1092,tiedemann-nygaard-2004-opus,0,0.0970909,"Missing"
C14-1092,P10-1040,0,0.00942462,"f all characters. We do not perform any other kind of tokenization. 5.2 Unsupervised SMS Normalization In Section 5.2, we described a static lookup table for expanding abbreviations and shorthands typically encountered in SMS messages, e.g., 4ever→forever. While a static lookup table provides a reasonable way of handling common SMS abbreviations, it has limited coverage. In order to build a larger normalization lexicon, we used distributed representation of words to induce the lexicon in an unsupervised manner. Distributed word representations (Bengio et al., 2003; Collobert and Weston, 2008; Turian et al., 2010) induced through deep neural networks have been shown to be useful in several natural language processing applications. We use the notion of distributional similarity that is automatically induced through the word representations for learning automatic normalization lexicons. Canonical form love starbucks once tomorrow forever because homework igualmente siempre adios contigo demasiado Noisy form loveeee, loveeeee, looove, love, wuv, wove, love, laffff, love, wuvvv, luhhhh, love, luvvv, luv starbs, sbucks oncee, 1ce tmrw, tomorrow, 2moro, tmrrw, tomarrow, tomoro, tomoz, 2mrw, tmr, tm, tmwr, 2m"
C14-1092,N13-1050,0,0.0260814,"Missing"
C14-1092,N04-1033,0,0.0848234,"Missing"
C14-1092,treurniet-etal-2012-collection,0,\N,Missing
C14-1092,P06-2005,0,\N,Missing
C14-1092,N06-1014,0,\N,Missing
C98-1006,1995.tmi-1.20,0,0.023415,"Missing"
C98-1006,J97-3002,0,0.13962,"Missing"
C98-1006,P97-1046,1,0.852115,"Missing"
C98-1006,P96-1023,1,0.88095,"unction froHl SOllree word subsequences to target word subsequences [&apos;or each transcribed uttera nce and its translation. The construction of states and transitions is specitied in Section 4; the method for selecting phrase head words is described in Section 5. The string comparison eva.htatiol~ metric we use. is described in Seclion 6, atl(l the results o[ testing the method in a limited domain of l&apos;;nglish-Spanish translation are reported in Section 7. 2 Overview 2.1 Lexlcal head t r a n s d u c e r s In our training method, we follow tile simple lexical head transduction model described by Alshawi (1996b) which can be regarded as a type of .~ta.tistical dependency gra.ntmar transduction. This type of transduction model consists of a collection of head transducers; the purpose of a particular transducer is to translate a specific source word w into a target word v, and further to translate tile pair of sequences of dependent words to the left and right of w to sequences of dependents to the left and right of c. When applied recursively, a set of such transducer> effects a hierarchical transduction of the source string into the target string. A distinguishing property of head transducers, as c"
C98-1006,J90-2002,0,0.0652812,"tion i n e a s u r e (cf the use of 0 2 h: Gale and Church (1991)) computed as follows: c) = (be - a d) ,/(a + b)(c + d)(a + + ,l) 43 w here (t ~ 7IV -- I/W,V b ~ &apos;/l ~v v c -- IV - d ~- ll, j v &apos;tz V - - ~,!.I&apos; + 7tW, V &apos;ii,,{.v, V N is tile total nunfl)er of bitexts, nv the number of bitexts in which V appears in the target, nw the number of bitexts in which FV appears ill the source, and ~twy the nulnber of bitexts ill which IU a p p e a r s in t h e s o u r c e a n d V a l ) p e a r s ill the target. We tried using the log probabilities of target subsequences given source subsequences (cf Brown et al. (1990)) as a cost function instead of05 but 05 resulted in better performance of our translation models. The second cost. function used is a distance measure which penalizes pairings in which the source subsequence and target subsequence are in very different positions in their respective sentences, l)ifferent weightings of distance to correlation costs can be used to bias the model towards more or less parallel alignments for different language pairs. 3.2 A l i g n m e n t search The agenda-based alignment search makes use of dynamic programming to record the best cost seen for all partial alignmen"
C98-1006,J93-2003,0,0.0280187,"Missing"
C98-1006,P96-1025,0,0.043579,"d as event observation counts for a statistical head transduction model. More specifically, they are used as counts for maxinmm likelihood estimation of the transducer start., transition, and stop probabilities specified in Section 2. ,5 Head selection We have l>een using the following monolingual metrics which can t)e at)plied to either the s o u r c e o r t a r g e t Iangllage. t o predict the likelihood of a word being the head word of a string. DistaT~ce: The distance between a dependent and its head. In general, the likelihood of a head:dependent relation decreases as distance increases (Collins, 1996). I&apos;l/ord frequcncg: The frequency of occurrence of a word in the training corpus. IVor<t &apos;complexity&apos;: I&apos;k}r languages with phonetic orthography such as t~;nglish, &apos;complexity&apos; of a word can be measured in terms of number of characters in that word. Optionalit9: This metric is intended to identify optional modifiers which are less likely to be heads. For each word we lind trigrams with the w<)r(t of interest as the middle word and compare tile distribution of these trigrams with the distribution of the bigrams formed from the outer pairs of words. If these two distributions are strongly corre"
C98-1006,H91-1026,0,0.019555,"mapping word subsequences W in the source to word subsequences V in the target. In this process a.n alignment model is constructed which specifies a cost for each pairing (!IV, V) of s o u r c e a n d t a r g e t s u b s e q t l e n c e s , a n d an alignment search is carried out to minimize the sum of the costs of a set of pairings which completely maps the bitext source to its target. 3.1 Alignment model The cost of a pairing is composed of a weighted combination of cost, functioIls. We currently use two. The first cost function is tile 05 correlation i n e a s u r e (cf the use of 0 2 h: Gale and Church (1991)) computed as follows: c) = (be - a d) ,/(a + b)(c + d)(a + + ,l) 43 w here (t ~ 7IV -- I/W,V b ~ &apos;/l ~v v c -- IV - d ~- ll, j v &apos;tz V - - ~,!.I&apos; + 7tW, V &apos;ii,,{.v, V N is tile total nunfl)er of bitexts, nv the number of bitexts in which V appears in the target, nw the number of bitexts in which FV appears ill the source, and ~twy the nulnber of bitexts ill which IU a p p e a r s in t h e s o u r c e a n d V a l ) p e a r s ill the target. We tried using the log probabilities of target subsequences given source subsequences (cf Brown et al. (1990)) as a cost function instead of05 but 05 resul"
D08-1028,C96-1043,0,0.0299051,"ents. In Section 5, we present a new approach to automatically identify emerging templates – texts that are repeatedly created by agents and are similar to each other but distinct from the current template text. We use AHT as the metric to minimize for automatic identification of emerging templates. We discuss some of the issues concerning this work in Section 6 and conclude in Section 7. 2 Related Work There are few threads of research that are relevant to the work presented in this paper. First, the topic of email response generation in the context of customer care has been investigated by (Coch, 1996; Lapalme and Kosseim, 2003; Zukerman and Marom, 2007). In (Coch, 1996), the authors model multi-sentence generation of response letters to customer complaints in French. The generation model is carefully crafted for the domain using domain-specific rules for conceptual planning, rhetorical relations and surface word order operators. They show that their 265 approach performs better than predefined templates and slightly worse than human generated responses. In (Lapalme and Kosseim, 2003), the authors explore three different approaches based on classification, case-based reasoning and question"
D08-1028,W02-1020,0,0.0332681,"sitory of documents or the return results of a search query. These maps are primarily designed for exploration and navigation through the document space. While the underlying algorithm we use to illustrate the text edits is similar to the one used in text map visualizations, our focus in this paper is to provide a mechanism for template designers to quickly identify the variants of a template sentence created by the agents. A third thread is in the context of humanassisted machine translation, where a human translator post-edits the output of a machine translation system (Foster et al., 1997; Foster et al., 2002; Och et al., 2003). In order to improve the efficiency of a human translator, the k-best output of a translation system could be displayed as word or phrase choices which are color coded based on the confidence value assigned by the translation model. While the approach we follow is partly motivated by the postediting paradigm, there are significant differences in the context we apply this approach. In the context of this paper, the template designer is presented a summary of the set of variants created by each agent for each sentence of the template. The task of the template designer is to u"
D08-1028,E03-1032,0,0.0137326,"r the return results of a search query. These maps are primarily designed for exploration and navigation through the document space. While the underlying algorithm we use to illustrate the text edits is similar to the one used in text map visualizations, our focus in this paper is to provide a mechanism for template designers to quickly identify the variants of a template sentence created by the agents. A third thread is in the context of humanassisted machine translation, where a human translator post-edits the output of a machine translation system (Foster et al., 1997; Foster et al., 2002; Och et al., 2003). In order to improve the efficiency of a human translator, the k-best output of a translation system could be displayed as word or phrase choices which are color coded based on the confidence value assigned by the translation model. While the approach we follow is partly motivated by the postediting paradigm, there are significant differences in the context we apply this approach. In the context of this paper, the template designer is presented a summary of the set of variants created by each agent for each sentence of the template. The task of the template designer is to use this tool to sel"
D08-1028,P02-1040,0,0.0822897,"se. The time taken for each of these phases typically varies depending on the customer’s account and the problem category. Nevertheless, we assume that the times for these phases is mostly a constant for a given problem category, and hence the results presented in this paper need to be interpreted on a per problem category basis. A second limitation of the approach presented in this paper is that the metric used to measure the similarity between strings (n-gram overlap) is only a crude approximation of an ideal semantic similarity metric. There are however other similarity metrics (e.g. BLEU (Papineni et al., 2002)) which could be used equally well. The purpose of this paper is to illustrate the possibility of analysis of responses using one particular instantiation of the similarity metric. In spite of the several directions that this work can 272 Acknowledgments We would like to thanks Mazin Gilbert, Junlan Feng, Narendra Gupta and Wenling Hsu for the discussions during the course of this work. We also thank the members who generously offered to their support to provide us with data used in this study without which this work would not have been possible. We thank the anonymous reviewers for their usef"
E06-1046,N04-1005,1,0.913042,"s-Research 180 Park Ave Florham Park, NJ 07932 johnston@research.att.com lore, 2005), we have shown that such grammars can be compiled into finite-state transducers enabling effective processing of lattice input from speech and gesture recognition and mutual compensation for errors and ambiguities. However, like other approaches based on handcrafted grammars, multimodal grammars can be brittle with respect to extra-grammatical, erroneous and disfluent input. For speech recognition, a corpus-driven stochastic language model (SLM) with smoothing or a combination of grammarbased and -gram model (Bangalore and Johnston, 2004; Wang et al., 2002) can be built in order to overcome the brittleness of a grammar-based language model. Although the corpus-driven language model might recognize a user’s utterance correctly, the recognized utterance may not be assigned a semantic representation by the multimodal grammar if the utterance is not part of the grammar. There have been two main approaches to improving robustness of the understanding component in the spoken language understanding literature. First, a parsing-based approach attempts to recover partial parses from the parse chart when the input cannot be parsed in i"
E06-1046,J93-2003,0,0.00770653,"ure 1: Editing Example   In this paper, we develop further this edit-based approach to finite-state multimodal language understanding and show how when appropriately tuned it can provide a substantial improvement in concept accuracy. We also explore learning edits from data and present an approach of modeling this process as a machine translation problem. We learn a model to translate from out of grammar or misrecognized language (such as ‘ASR:’ above) to the closest language the system can understand (‘Grammar:’ above). To this end, we adopt techniques from statistical machine translation (Brown et al., 1993; Och and Ney, 2003) and use statistical alignment to learn the edit patterns. Here we evaluate these different techniques on data from the MATCH multimodal conversational system (Johnston et al., 2002) but the same techniques are more broadly applicable to spoken language systems in general whether unimodal or multimodal. The layout of the paper is as follows. In Sections 2 and 3, we briefly describe the MATCH application and the finite-state approach to multimodal language understanding. In Section 4, we discuss the limitations of the methods used for robust understanding in spoken language"
E06-1046,P93-1008,0,0.0386644,"ased language model. Although the corpus-driven language model might recognize a user’s utterance correctly, the recognized utterance may not be assigned a semantic representation by the multimodal grammar if the utterance is not part of the grammar. There have been two main approaches to improving robustness of the understanding component in the spoken language understanding literature. First, a parsing-based approach attempts to recover partial parses from the parse chart when the input cannot be parsed in its entirety due to noise, in order to construct a (partial) semantic representation (Dowding et al., 1993; Allen et al., 2001; Ward, 1991). Second, a classification-based approach views the problem of understanding as extracting certain bits of information from the input. It attempts to classify the utterance and identifies substrings of the input as slot-filler values to construct a frame-like semantic representation. Both approaches have shortcomings. Although in the first approach, the grammar can encode richer semantic representations, the method for combining the fragmented parses is quite ad hoc. In the second approach, the robustness is derived from training classifiers on annotated data,"
E06-1046,C00-1054,1,0.900303,"ate and compare the performance of the hand-crafted and learned edit machines in the context of a multimodal conversational system (MATCH). 1 Introduction Over the years, there have been several multimodal systems that allow input and/or output to be conveyed over multiple channels such as speech, graphics, and gesture, for example, put that there (Bolt, 1980), CUBRICON (Neal and Shapiro, 1991), QuickSet (Cohen et al., 1998), SmartKom (Wahlster, 2002), Match (Johnston et al., 2002). Multimodal integration and interpretation for such interfaces is elegantly expressed using multimodal grammars (Johnston and Bangalore, 2000). These grammars support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation. In (Bangalore and Johnston, 2000; Johnston and BangaMichael Johnston AT&T Labs-Research 180 Park Ave Florham Park, NJ 07932 johnston@research.att.com lore, 2005), we have shown that such grammars can be compiled into finite-state transducers enabling effective processing of lattice input from speech and gesture recognition and mutual com"
E06-1046,J03-1002,0,0.00970568,"le   In this paper, we develop further this edit-based approach to finite-state multimodal language understanding and show how when appropriately tuned it can provide a substantial improvement in concept accuracy. We also explore learning edits from data and present an approach of modeling this process as a machine translation problem. We learn a model to translate from out of grammar or misrecognized language (such as ‘ASR:’ above) to the closest language the system can understand (‘Grammar:’ above). To this end, we adopt techniques from statistical machine translation (Brown et al., 1993; Och and Ney, 2003) and use statistical alignment to learn the edit patterns. Here we evaluate these different techniques on data from the MATCH multimodal conversational system (Johnston et al., 2002) but the same techniques are more broadly applicable to spoken language systems in general whether unimodal or multimodal. The layout of the paper is as follows. In Sections 2 and 3, we briefly describe the MATCH application and the finite-state approach to multimodal language understanding. In Section 4, we discuss the limitations of the methods used for robust understanding in spoken language understanding litera"
E06-1046,W95-0107,0,0.020146,") is the set of arguments to the predicate. We determine the predicate (;< ) for a = token multimodal utterance ( >@A? ) by maximizing the posterior probability as shown in Equation 2. ; <CB K ? O D$E9F&GH J D$I E,L ;NM-> AN (2) We view the problem of identifying and extracting arguments from a multimodal input as a problem of associating each token of the input with a specific tag that encodes the label of the argument and the span of the argument. These tags are drawn from a tagset which is constructed by 363 extending each argument label by three additional symbols P,QSRTQVU , following (Ramshaw and Marcus, 1995). These symbols correspond to cases when a token is inside (P ) an argument span, outside ( R ) an argument span or at the boundary of two argument spans (U ) (See Table 1). User Utterance Argument Annotation IOB Encoding cheap thai upper west side     W         6  price cheap /price cuisine thai /cuisine place upper west side /place cheap price B thai cuisine B upper place I west place I side place I      Table 1: The X I,O,B Y encoding for argument extraction. Given this encoding, the problem of extracting the arguments is a search for the most likely sequence of tags ("
E06-1046,H93-1008,0,\N,Missing
E06-1046,P02-1048,1,\N,Missing
E09-1012,W05-0613,0,0.0191088,"ing (Soricut and Marcu, 2003), hierarchical agglomerative clustering (Sporleder and Lascarides, 2004), parsing from lexicalized tree-adjoining grammars (Cristea, 2000), and rulebased approaches that use rhetorical relations and discourse cues (Forbes et al., 2003; Polanyi et al., 2004; LeThanh et al., 2004). With the exception of Cristea (2000), most of this research has been limited to non-incremental parsing of textual monologues where, in contrast to incremental dialog parsing, predicting a system action is not relevant. The work on discourse parsing that is most similar to ours is that of Baldridge and Lascarides (2005). They used a probabilistic headdriven parsing method (described in (Collins, 2003)) to construct rhetorical structure trees for a spoken dialog corpus. However, their parser was Proceedings of the 12th Conference of the European Chapter of the ACL, pages 94–102, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 94 Order Placement Dialog Opening Task Task Contact Info Order Item Shipping Info Topic/Subtask DialogAct,Pred!Args Utterance Topic/Subtask DialogAct,Pred!Args Utterance Payment Info Summary Closing Task Delivery Info Topic/Subtask Figure 2: Samp"
E09-1012,P06-1026,1,0.865703,"Missing"
E09-1012,J98-4001,0,0.298606,"stics 94 Order Placement Dialog Opening Task Task Contact Info Order Item Shipping Info Topic/Subtask DialogAct,Pred!Args Utterance Topic/Subtask DialogAct,Pred!Args Utterance Payment Info Summary Closing Task Delivery Info Topic/Subtask Figure 2: Sample output (subtask tree) from a parse-based model for the catalog ordering domain. DialogAct,Pred!Args to parse plans. Their parser, however, was not probabilistic or targeted at dialog processing. Utterance 3 Clause Dialog Structure We consider a task-oriented dialog to be the result of incremental creation of a shared plan by the participants (Lochbaum, 1998). The shared plan is represented as a single tree T that incorporates the task/subtask structure, dialog acts, syntactic structure and lexical content of the dialog, as shown in Figure 1. A task is a sequence of subtasks ST ∈ S. A subtask is a sequence of dialog acts DA ∈ D. Each dialog act corresponds to one clause spoken by one speaker, customer (cu ) or agent (ca ) (for which we may have acoustic, lexical, syntactic and semantic representations). Figure 2 shows the subtask tree for a sample dialog in our domain (catalog ordering). An order placement task is typically composed of the sequenc"
E09-1012,J96-1002,0,0.0236082,"e ...... ......... Closing Ack may we deliver this order to your home thank you yes i would like yes one thank for calling to place an order second you yes please please XYZ catalog ...... this is mary how may I help you can i have your home telephone number with area code ...... may we deliver this order to your home yes please ...... Figure 4: An illustration of incremental evolution of dialog structure a feature vector containing contextual information for the parsing action (see Section 5.1). These feature vectors and the associated parser actions are used to train maximum entropy models (Berger et al., 1996). These models are then used to incrementally incorporate the utterances for a new dialog into that dialog’s subtask tree as the dialog progresses, as shown in Figure 3. 4.1 ing of this dialog using our shift-reduce dialog parser would proceed as follows: the STP model predicts shift for sta ; the DAP model predicts YNP(Promotions) for daa ; the generator outputs would you like a free magazine?; and the parser shifts a token representing this utterance onto the stack. Then, the customer says no. The DAC model classies dau as No; the STC model classies stu as shift and binary-reduce-special-o"
E09-1012,W04-2322,0,0.0225122,"Missing"
E09-1012,W97-0301,0,0.0311247,"d of the dialog, the output is a binary branching subtask tree. Consider the example subdialog A: would you like a free magazine? U: no. The process4.2 Start-Complete Method In the shift-reduce method, the dialog tree is constructed as a side effect of the actions performed on the stack: each reduce action on the stack introduces a non-terminal in the tree. By contrast, in the start-complete method the instructions to build the tree are directly encoded in the parser actions. A stack is used to maintain the global parse state. The actions the parser can take are similar to those described in (Ratnaparkhi, 1997). The parser must decide whether to join each new terminal onto the existing left-hand edge of the tree, or start a new subtree. The actions for the parser include start-X, n-start-X, complete-X, u-completeX and b-complete-X, where X is each of the nonterminals (subtask labels) in the tree. Start-X pushes a token representing the current utterance onto the stack; n-start-X pushes non-terminal X onto the stack; complete-X pushes a token representing the current utterance onto the stack, then 97 Type Call-level pops the top two tokens off the stack and pushes the non-terminal X; u-complete-X pop"
E09-1012,J03-4003,0,0.0354499,"), parsing from lexicalized tree-adjoining grammars (Cristea, 2000), and rulebased approaches that use rhetorical relations and discourse cues (Forbes et al., 2003; Polanyi et al., 2004; LeThanh et al., 2004). With the exception of Cristea (2000), most of this research has been limited to non-incremental parsing of textual monologues where, in contrast to incremental dialog parsing, predicting a system action is not relevant. The work on discourse parsing that is most similar to ours is that of Baldridge and Lascarides (2005). They used a probabilistic headdriven parsing method (described in (Collins, 2003)) to construct rhetorical structure trees for a spoken dialog corpus. However, their parser was Proceedings of the 12th Conference of the European Chapter of the ACL, pages 94–102, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 94 Order Placement Dialog Opening Task Task Contact Info Order Item Shipping Info Topic/Subtask DialogAct,Pred!Args Utterance Topic/Subtask DialogAct,Pred!Args Utterance Payment Info Summary Closing Task Delivery Info Topic/Subtask Figure 2: Sample output (subtask tree) from a parse-based model for the catalog ordering domain."
E09-1012,P06-2089,0,0.0169641,"the generator outputs would you like a free magazine?; and the parser shifts a token representing this utterance onto the stack. Then, the customer says no. The DAC model classies dau as No; the STC model classies stu as shift and binary-reduce-special-offer; and the parser shifts a token representing the utterance onto the stack, before popping the top two elements off the stack and adding the subtree for special-order into the dialog’s subtask tree. Shift-Reduce Method In this method, the subtask tree is recovered through a right-branching shift-reduce parsing process (Hall et al., 2006; Sagae and Lavie, 2006). The parser shifts each utterance on to the stack. It then inspects the stack and decides whether to do one or more reduce actions that result in the creation of subtrees in the subtask tree. The parser maintains two data structures – a stack and a tree. The actions of the parser change the contents of the stack and create nodes in the dialog tree structure. The actions for the parser include unaryreduce-X, binary-reduce-X and shift, where X is each of the non-terminals (subtask labels) in the tree. Shift pushes a token representing the utterance onto the stack; binary-reduce-X pops two token"
E09-1012,P04-1088,0,0.0758876,"Missing"
E09-1012,N03-1030,0,0.0413694,") includes the task structure of the dialog in the context, (4) can be trained from dialog data, and (5) runs incrementally, parsing the dialog as it occurs and interleaving generation and interpretation. At the core of our model is a parser that incrementally builds the dialog task structure as the Discourse Parsing Discourse parsing is the process of building a hierarchical model of a discourse from its basic elements (sentences or clauses), as one would build a parse of a sentence from its words. There has now been considerable work on discourse parsing using statistical bottom-up parsing (Soricut and Marcu, 2003), hierarchical agglomerative clustering (Sporleder and Lascarides, 2004), parsing from lexicalized tree-adjoining grammars (Cristea, 2000), and rulebased approaches that use rhetorical relations and discourse cues (Forbes et al., 2003; Polanyi et al., 2004; LeThanh et al., 2004). With the exception of Cristea (2000), most of this research has been limited to non-incremental parsing of textual monologues where, in contrast to incremental dialog parsing, predicting a system action is not relevant. The work on discourse parsing that is most similar to ours is that of Baldridge and Lascarides (200"
E09-1012,J86-3001,0,0.703203,"ordering domain that has been annotated for dialog acts and task/subtask information. We contrast the amount of context provided by each method and its impact on performance. 1 2 Introduction Related Work There are two threads of research that are relevant to our work: work on parsing (written and spoken) discourse, and work on plan-based dialog models. Corpora of spoken dialog are now widely available, and frequently come with annotations for tasks/games, dialog acts, named entities and elements of syntactic structure. These types of information provide rich clues for building dialog models (Grosz and Sidner, 1986). Dialog models can be built ofine (for dialog mining and summarization), or online (for dialog management). A dialog manager is the component of a dialog system that is responsible for interpreting user actions in the dialog context, and for generating system actions. Needless to say, a dialog manager operates incrementally as the dialog progresses. In typical commercial dialog systems, the interpretation and generation processes operate independently of each other, with only a small amount of shared context. By contrast, in this paper we describe a dialog model that (1) tightly integrates i"
E09-1012,C04-1007,0,0.0143956,"an be trained from dialog data, and (5) runs incrementally, parsing the dialog as it occurs and interleaving generation and interpretation. At the core of our model is a parser that incrementally builds the dialog task structure as the Discourse Parsing Discourse parsing is the process of building a hierarchical model of a discourse from its basic elements (sentences or clauses), as one would build a parse of a sentence from its words. There has now been considerable work on discourse parsing using statistical bottom-up parsing (Soricut and Marcu, 2003), hierarchical agglomerative clustering (Sporleder and Lascarides, 2004), parsing from lexicalized tree-adjoining grammars (Cristea, 2000), and rulebased approaches that use rhetorical relations and discourse cues (Forbes et al., 2003; Polanyi et al., 2004; LeThanh et al., 2004). With the exception of Cristea (2000), most of this research has been limited to non-incremental parsing of textual monologues where, in contrast to incremental dialog parsing, predicting a system action is not relevant. The work on discourse parsing that is most similar to ours is that of Baldridge and Lascarides (2005). They used a probabilistic headdriven parsing method (described in (C"
E09-1012,P06-2041,0,0.0146227,"omotions) for daa ; the generator outputs would you like a free magazine?; and the parser shifts a token representing this utterance onto the stack. Then, the customer says no. The DAC model classies dau as No; the STC model classies stu as shift and binary-reduce-special-offer; and the parser shifts a token representing the utterance onto the stack, before popping the top two elements off the stack and adding the subtree for special-order into the dialog’s subtask tree. Shift-Reduce Method In this method, the subtask tree is recovered through a right-branching shift-reduce parsing process (Hall et al., 2006; Sagae and Lavie, 2006). The parser shifts each utterance on to the stack. It then inspects the stack and decides whether to do one or more reduce actions that result in the creation of subtrees in the subtask tree. The parser maintains two data structures – a stack and a tree. The actions of the parser change the contents of the stack and create nodes in the dialog tree structure. The actions for the parser include unaryreduce-X, binary-reduce-X and shift, where X is each of the non-terminals (subtask labels) in the tree. Shift pushes a token representing the utterance onto the stack; binary"
E09-1012,C04-1048,0,\N,Missing
E09-1012,W04-0211,0,\N,Missing
E09-1028,H05-1062,0,0.0280662,"on in the same step (Natarajan et al., 2002). There are two other threads of research literature relevant to our work. Named entity (NE) extraction attempts to identify entities of interest in speech or text. Typical entities include locations, persons, organizations, dates, times monetary amounts and percentages (Kubala et al., 1998). Most approaches for NE tasks rely on machine learning approaches using annotated data. These algorithms include a hidden Markov model, support vector machines, maximum entropy, and conditional random fields. With the goal of improving robustness to ASR errors, (Favre et al., 2005) described a finite-state machine based approach to take as input ASR n-best strings and extract the NEs. Although our task of query segmentation has similarity with NE tasks, it is arguable whether the SearchTerm is a well-defined entity, since a user can provide varied expressions as they would for a general web search. Also, it is not clear how the current best performing NE methods based on maximum entropy or conditional random fields models can be extended to apply on weighted lattices produced by ASR. The other related literature is natural language interface to databases (NLIDBs), which"
E99-1025,P96-1023,0,0.0340024,"r we present two approaches: contextual models, which exploit a variety of features in order to improve supertag performance, and class-based models, which assign sets of supertags to words in order to substantially improve accuracy with only a slight increase in ambiguity. 1 Introduction Many natural language applications are beginning to exploit some underlying structure of the language. Roukos (1996) and Jurafsky et al. (1995) use structure-based language models in the context of speech applications. Grishman (1995) and Hobbs et al. (1995) use phrasal information in information extraction. Alshawi (1996) uses dependency information in a machine translation system. The need to impose structure leads to the need to have robust parsers. There have been two main robust parsing paradigms: Finite State Grammar-based approaches (such as Abney (1990), Grishman (1995), and Hobbs et al. (1997)) and Statistical Parsing (such as Charniak (1996), Magerman (1995), and Collins (1996)). Srinivas (1997a) has presented a different approach called supertagging that integrates linguistically motivated lexical descriptions with the robustness of statistical techniques. The idea underlying the approach is that the"
E99-1025,W98-0102,1,0.826513,"fectively a parse (almost parse). Supertagging has been found useful for a number of applications. For instance, it can be used to speed up conventional chart parsers because it reduces the ambiguity which a parser must face, as described in Srinivas (1997a). Chandrasekhar and Srinivas (1997) has shown that supertagging may be employed in information retrieval. Furthermore, given a sentence aligned parallel corpus of two languages and almost parse information for the sentences of one of the languages, one can rapidly develop a grammar for the other language using supertagging, as suggested by Bangalore (1998). In contrast to the aforementioned work in supertag disambiguation, where the objective was to provide a-direct comparison between trigram models for part-of-speech tagging and supertagging, in this paper our goal is to improve the performance of supertagging using local techniques which avoid full parsing. These supertag disambiguation models can be grouped into contextual models and class based models. Contextual models use different features in frameworks that exploit the information those features provide in order to achieve higher accuracies in supertagging. For class based models, super"
E99-1025,P96-1025,0,0.0502073,"ure of the language. Roukos (1996) and Jurafsky et al. (1995) use structure-based language models in the context of speech applications. Grishman (1995) and Hobbs et al. (1995) use phrasal information in information extraction. Alshawi (1996) uses dependency information in a machine translation system. The need to impose structure leads to the need to have robust parsers. There have been two main robust parsing paradigms: Finite State Grammar-based approaches (such as Abney (1990), Grishman (1995), and Hobbs et al. (1997)) and Statistical Parsing (such as Charniak (1996), Magerman (1995), and Collins (1996)). Srinivas (1997a) has presented a different approach called supertagging that integrates linguistically motivated lexical descriptions with the robustness of statistical techniques. The idea underlying the approach is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (Supertags) that impose complex constraints in a local context. Supertag disambiguation is resolved &quot;Supported by NSF grants ~SBR-9710411 and ~GER-9354869 K. Vijay-Shanker Department of Computer and Information Sciences University of Delaware Newark, DE 19716 vij"
E99-1025,P98-1061,0,0.0582341,"Missing"
E99-1025,P98-1081,0,0.100226,"Missing"
E99-1025,C94-1024,0,0.444025,"Missing"
E99-1025,P95-1037,0,0.0424616,"ome underlying structure of the language. Roukos (1996) and Jurafsky et al. (1995) use structure-based language models in the context of speech applications. Grishman (1995) and Hobbs et al. (1995) use phrasal information in information extraction. Alshawi (1996) uses dependency information in a machine translation system. The need to impose structure leads to the need to have robust parsers. There have been two main robust parsing paradigms: Finite State Grammar-based approaches (such as Abney (1990), Grishman (1995), and Hobbs et al. (1997)) and Statistical Parsing (such as Charniak (1996), Magerman (1995), and Collins (1996)). Srinivas (1997a) has presented a different approach called supertagging that integrates linguistically motivated lexical descriptions with the robustness of statistical techniques. The idea underlying the approach is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (Supertags) that impose complex constraints in a local context. Supertag disambiguation is resolved &quot;Supported by NSF grants ~SBR-9710411 and ~GER-9354869 K. Vijay-Shanker Department of Computer and Information Sciences University of Delaware"
E99-1025,1997.iwpt-1.22,0,0.754821,"ge. Roukos (1996) and Jurafsky et al. (1995) use structure-based language models in the context of speech applications. Grishman (1995) and Hobbs et al. (1995) use phrasal information in information extraction. Alshawi (1996) uses dependency information in a machine translation system. The need to impose structure leads to the need to have robust parsers. There have been two main robust parsing paradigms: Finite State Grammar-based approaches (such as Abney (1990), Grishman (1995), and Hobbs et al. (1997)) and Statistical Parsing (such as Charniak (1996), Magerman (1995), and Collins (1996)). Srinivas (1997a) has presented a different approach called supertagging that integrates linguistically motivated lexical descriptions with the robustness of statistical techniques. The idea underlying the approach is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (Supertags) that impose complex constraints in a local context. Supertag disambiguation is resolved &quot;Supported by NSF grants ~SBR-9710411 and ~GER-9354869 K. Vijay-Shanker Department of Computer and Information Sciences University of Delaware Newark, DE 19716 vijay~cis.udel.edu b"
E99-1025,J93-2006,0,0.0618123,"Missing"
E99-1025,J92-4003,0,\N,Missing
E99-1025,C98-1078,0,\N,Missing
E99-1025,C98-1059,0,\N,Missing
H01-1055,P00-1059,1,0.873365,"Missing"
H01-1055,C00-1007,1,0.920937,"dels of what should be learned. In this respect, the work reported in this paper differs from other recent work on generation in the context of dialog systems [12, 16], which does not modularize the generation process and proposes a single stochastic model for the entire process. We start out by reviewing the generation architecture (Section 2). In Section 3, we discuss the issue of text planning for Communicator. In Section 4, we summarize some initial work in using machine learning for sentence planning [19]. Finally, in Section 5 we summarize work using stochastic tree models in generation [2]. 2. TEXT GENERATION ARCHITECTURE . NLG is conceptualized as a process leading from a high-level communicative goal to a sequence of communicative acts which accomplish this communicative goal. A communicative goal is a goal to affect the user’s cognitive state, e.g., his or her beliefs about the world, desires with respect to the world, or intentions about his or her actions in the world. Following (at least) [13], it has been customary to divide the generation process into three phases, the first two of which are planning phases. Reiter [17] calls this architecture a “consensus architecture”"
H01-1055,W00-1401,1,0.834193,"he intended dialogs is not as relevant: we can try and mimic the human-human transcripts as closely as possible. To show this, we have performed some initial experiments using FERGUS (Flexible Empiricist-Rationalist Generation Using Syntax), a stochastic surface realizer which incorporates a tree model and a linear language model [2]. We have developed a metric which can be computed automatically from the syntactic dependency structure of the sentence and the linear order chosen by the realizer, and we have shown that this metric correlates with human judgments of the felicity of the sentence [3]. Using this metric, we have shown that the use of both the tree model and the linear language model improves the quality of the output of FERGUS over the use of only one or the other of these resources. FERGUS was originally trained on the Penn Tree Bank corpus consisting of Wall Street Journal text (WSJ). The results on an initial set of Communicator sentences were not encouraging, presumably because there are few questions in the WSJ corpus, and furthermore, specific constructions (including what as determiner) appear to be completely absent (perhaps due to a newspaper style file). In an in"
H01-1055,W00-1407,0,0.0206978,"system response to users has been studied by the Natural Language Generation (NLG) community, though rarely in the context of dialog systems. What have emerged from this work are a “consensus architecture” [17] which modularizes the large number of tasks performed during NLG in a particular way, and a range of linguistic representations which can be used in accomplishing these tasks. Many systems have been built using NLG technology, including report generators [8, 7], system description generators [10], and systems that attempt to convince the user of a particular view through argumentation [20, 4]. In this paper, we claim that the work in NLG is relevant to dialog systems as well. We show how the results can be incorporated, and report on some initial work in adapting NLG approaches to dialog systems and their special needs. The dialog system we use is the AT&T Communicator travel planning system.We use machine learning and stochastic approaches where hand-crafting appears to be too complex an option, but we also use insight gained during previous work on NLG in order to develop models of what should be learned. In this respect, the work reported in this paper differs from other recent"
H01-1055,A97-1037,1,0.824416,"ity in terms of informativeness and of adaptation to the context. For a long time, the issue of system response to users has been studied by the Natural Language Generation (NLG) community, though rarely in the context of dialog systems. What have emerged from this work are a “consensus architecture” [17] which modularizes the large number of tasks performed during NLG in a particular way, and a range of linguistic representations which can be used in accomplishing these tasks. Many systems have been built using NLG technology, including report generators [8, 7], system description generators [10], and systems that attempt to convince the user of a particular view through argumentation [20, 4]. In this paper, we claim that the work in NLG is relevant to dialog systems as well. We show how the results can be incorporated, and report on some initial work in adapting NLG approaches to dialog systems and their special needs. The dialog system we use is the AT&T Communicator travel planning system.We use machine learning and stochastic approaches where hand-crafting appears to be too complex an option, but we also use insight gained during previous work on NLG in order to develop models of"
H01-1055,W00-0306,0,0.113299,"evant to dialog systems as well. We show how the results can be incorporated, and report on some initial work in adapting NLG approaches to dialog systems and their special needs. The dialog system we use is the AT&T Communicator travel planning system.We use machine learning and stochastic approaches where hand-crafting appears to be too complex an option, but we also use insight gained during previous work on NLG in order to develop models of what should be learned. In this respect, the work reported in this paper differs from other recent work on generation in the context of dialog systems [12, 16], which does not modularize the generation process and proposes a single stochastic model for the entire process. We start out by reviewing the generation architecture (Section 2). In Section 3, we discuss the issue of text planning for Communicator. In Section 4, we summarize some initial work in using machine learning for sentence planning [19]. Finally, in Section 5 we summarize work using stochastic tree models in generation [2]. 2. TEXT GENERATION ARCHITECTURE . NLG is conceptualized as a process leading from a high-level communicative goal to a sequence of communicative acts which accomp"
H01-1055,A92-1006,1,0.711248,"In Section 4, we summarize some initial work in using machine learning for sentence planning [19]. Finally, in Section 5 we summarize work using stochastic tree models in generation [2]. 2. TEXT GENERATION ARCHITECTURE . NLG is conceptualized as a process leading from a high-level communicative goal to a sequence of communicative acts which accomplish this communicative goal. A communicative goal is a goal to affect the user’s cognitive state, e.g., his or her beliefs about the world, desires with respect to the world, or intentions about his or her actions in the world. Following (at least) [13], it has been customary to divide the generation process into three phases, the first two of which are planning phases. Reiter [17] calls this architecture a “consensus architecture” in NLG. During text planning, a high-level communicative goal is broken down into a structured representation of atomic communicative goals, i.e., goals that can be attained with a single communicative act (in language, by uttering a single clause). The atomic communicative goals may be linked by rhetorical relations which show how attaining the atomic goals contributes to attaining the high-level goal. The work r"
H01-1055,A00-2026,0,0.0354348,"evant to dialog systems as well. We show how the results can be incorporated, and report on some initial work in adapting NLG approaches to dialog systems and their special needs. The dialog system we use is the AT&T Communicator travel planning system.We use machine learning and stochastic approaches where hand-crafting appears to be too complex an option, but we also use insight gained during previous work on NLG in order to develop models of what should be learned. In this respect, the work reported in this paper differs from other recent work on generation in the context of dialog systems [12, 16], which does not modularize the generation process and proposes a single stochastic model for the entire process. We start out by reviewing the generation architecture (Section 2). In Section 3, we discuss the issue of text planning for Communicator. In Section 4, we summarize some initial work in using machine learning for sentence planning [19]. Finally, in Section 5 we summarize work using stochastic tree models in generation [2]. 2. TEXT GENERATION ARCHITECTURE . NLG is conceptualized as a process leading from a high-level communicative goal to a sequence of communicative acts which accomp"
H01-1055,W94-0319,0,0.122438,"eraction, the user can supply more and different information at any time in the dialog. The dialog system must then support a mixed-initiative dialog strategy. While this strategy places greater requirements on ASR, it also increases the range of system responses and the requirements on their quality in terms of informativeness and of adaptation to the context. For a long time, the issue of system response to users has been studied by the Natural Language Generation (NLG) community, though rarely in the context of dialog systems. What have emerged from this work are a “consensus architecture” [17] which modularizes the large number of tasks performed during NLG in a particular way, and a range of linguistic representations which can be used in accomplishing these tasks. Many systems have been built using NLG technology, including report generators [8, 7], system description generators [10], and systems that attempt to convince the user of a particular view through argumentation [20, 4]. In this paper, we claim that the work in NLG is relevant to dialog systems as well. We show how the results can be incorporated, and report on some initial work in adapting NLG approaches to dialog syst"
H01-1055,W98-1415,0,0.0323366,"Missing"
H01-1055,N01-1003,1,\N,Missing
H01-1055,P01-1056,1,\N,Missing
H01-1055,A97-1039,0,\N,Missing
I11-1048,J05-4003,0,0.571712,"Barbosa AT&T Labs – Research 180 Park Ave Florham Park, NJ 07932 lbarbosa@research.att.com Srinivas Bangalore Vivek Kumar Sridhar Rangarajan AT&T Labs – Research AT&T Labs – Research 180 Park Ave 180 Park Ave Florham Park, NJ 07932 Florham Park, NJ 07932 srini@research.att.com Abstract or there is some restriction for using them. On the other hand, Web data is free and comprises data from different languages and domains. Previous research in the area of parallel Web data acquisition has mainly focused on the problems of document pair identification (Jiang et al., 2009; Uszkoreit et al., 2010; Munteanu and Marcu, 2005; Resnik and Smith, 2003; Melamed, 2001) and sentence alignment. Typically, document pairs are located by issuing queries to a search engine (Resnik and Smith, 2003; Hong et al., 2010). The sentences in the matched documents are then aligned using standard dynamic programming techniques. In this work, we model the problem of obtaining parallel text in two subtasks. First, locate the sites that contain bilingual data (bilingual sites). Here we assume that parallel texts are present in the same site (Chen and Nie, 2000). Second, extract parallel texts within these sites. While the latter problem"
I11-1048,P02-1040,0,0.0792442,"Missing"
I11-1048,J03-3002,0,0.824531,"ch 180 Park Ave Florham Park, NJ 07932 lbarbosa@research.att.com Srinivas Bangalore Vivek Kumar Sridhar Rangarajan AT&T Labs – Research AT&T Labs – Research 180 Park Ave 180 Park Ave Florham Park, NJ 07932 Florham Park, NJ 07932 srini@research.att.com Abstract or there is some restriction for using them. On the other hand, Web data is free and comprises data from different languages and domains. Previous research in the area of parallel Web data acquisition has mainly focused on the problems of document pair identification (Jiang et al., 2009; Uszkoreit et al., 2010; Munteanu and Marcu, 2005; Resnik and Smith, 2003; Melamed, 2001) and sentence alignment. Typically, document pairs are located by issuing queries to a search engine (Resnik and Smith, 2003; Hong et al., 2010). The sentences in the matched documents are then aligned using standard dynamic programming techniques. In this work, we model the problem of obtaining parallel text in two subtasks. First, locate the sites that contain bilingual data (bilingual sites). Here we assume that parallel texts are present in the same site (Chen and Nie, 2000). Second, extract parallel texts within these sites. While the latter problem of extracting of parall"
I11-1048,resnik-1998-parallel,0,0.149738,"ata. quality of the web crawled data on the test set considered in the experiments is mainly due to the style of the test set. Even though the domain of the crawler is travel and hospitality, the sentences in the test set are more conversational and better matched with Europarl in terms of BLEU metric. On the other hand, the METEOR metric that accounts for the overlapping unigrams is much closer for Europarl and Web data, i.e., the vocabulary coverage is comparable. 5 Related Work There are basically two main types of approaches to locate parallel corpora: query-based (Resnik and Smith, 2003; Resnik, 1998; Chen and Nie, 2000; Tom´as et al., 2005) and crawling-based (Ma and Liberman, 1999; Chen et al., 2004). Query-based approaches typically try to explore common patterns that occur in this kind of data by using them as search queries. For instance, STRAND (Resnik and Smith, 2003; Resnik, 1998) tries to locate candidate parallel pages by issuing queries like: (anchor:“english” OR anchor:“anglais”) AND (anchor:“french” OR anchor:“francais”). Chen and Nie (Chen and Nie, 2000) used a similar principle to obtain two sets of candidate sites by issuing queries as anchor:“english version” to a search"
I11-1048,2006.amta-papers.25,0,0.0198824,"Missing"
I11-1048,C10-1054,0,0.472669,"k Ave 180 Park Ave Florham Park, NJ 07932 Florham Park, NJ 07932 srini@research.att.com Abstract or there is some restriction for using them. On the other hand, Web data is free and comprises data from different languages and domains. Previous research in the area of parallel Web data acquisition has mainly focused on the problems of document pair identification (Jiang et al., 2009; Uszkoreit et al., 2010; Munteanu and Marcu, 2005; Resnik and Smith, 2003; Melamed, 2001) and sentence alignment. Typically, document pairs are located by issuing queries to a search engine (Resnik and Smith, 2003; Hong et al., 2010). The sentences in the matched documents are then aligned using standard dynamic programming techniques. In this work, we model the problem of obtaining parallel text in two subtasks. First, locate the sites that contain bilingual data (bilingual sites). Here we assume that parallel texts are present in the same site (Chen and Nie, 2000). Second, extract parallel texts within these sites. While the latter problem of extracting of parallel text from bilingual Web sites has received a lot of attention, the former problem of automatically locating high quality parallel Web pages is still an open"
I11-1048,P09-1098,0,0.0406446,"Missing"
I11-1048,C10-1124,0,0.181511,"Missing"
I11-1048,P07-2045,0,0.00606064,"alted. A final note regarding our crawling strategy is that even though we do not restrict it to any particular topic, as the crawling process evolves, it automatically focuses on topics where there is a higher concentration of parallel data, as travel, translator sites, etc. This is different from conventional approaches that explicitly constrain the crawl based on topics. 4 Machine Translation Experiments In this section, we exploit the parallel text obtained through our crawling strategy as augmented data in machine translation. We use a phrase-based statistical machine translation system (Koehn et al., 2007) in all the experiments. 4.1 Experimental Setup Web data. We focus on English and Spanish as the bilingual pair of languages. We used the crawling strategy presented in the previous section to obtain a set of 20186 bilingual sites. The parallel text from these sites was mined using the technique presented in (Rangarajan et al., 2011). A total of initial 4.84M bilingual sentence pairs were obtained from this process. We used length-based and word-based filters as well as a language model to filter these initial sentence pairs. After cleanup, a total of 2,039,272 bilingual sentence pairs was obt"
I11-1048,2005.mtsummit-papers.11,0,0.191789,"Missing"
I11-1048,1999.mtsummit-1.79,0,\N,Missing
I13-1141,N12-1048,1,0.891474,"Missing"
I13-1141,W11-2103,0,0.0258671,"we used the Moses toolkit (Koehn et al., 2007) for statistical machine translation. Minimum error rate training (MERT) was performed on the development set (dev2010) to optimize the feature weights of the log-linear model used in translation. During decoding, the unknown words were preserved in the hypotheses. The parallel text for building the English-French translation model – around 6.3 million parallel sentences – was obtained from several corpora: Europarl (Koehn, 2005), jrc-acquis corpus (Steinberger et al., 2006), Opensubtitle corpus (Tiedemann and Lars Nygaard, 2004), WMT11 Gigaword (Callison-Burch et al., 2011), WMT11 News (Callison-Burch et al., 2011), and Web crawling (Rangarajan Sridhar et al., 2011) as well as human translation of proprietary data. Second, we used a finite-state implementation of translation without reordering. We represent the phrase translation table as a weighted finite state transducer (FST) and the language model as a finite-state acceptor. The weight on the arcs of the FST is the dot product of the MERT weights with the translation scores. Our FST-based translation is the equivalent of phrase-based translation in Moses without reordering. In addition to Moses and FST decod"
I13-1141,W02-1001,0,0.0160512,"Missing"
I13-1141,P07-1062,0,0.0373605,"Missing"
I13-1141,W10-1733,1,0.911343,"interpreters are able to generate target speech incrementally with very low ear-voice span by using a variety of strategies (Chernov, 2004) such as anticipation, cognitive and linguistic inference, paraphrasing, etc. However, current methodologies for simultaneous translation are far from being able to exploit or model such complex phenomena. Quite often, models trained for consecutive translation are repurposed for incremental translation. One of the first attempts at incremental text translation was presented by Furuse and Iida (1996) using a transfer-based MT approach and more recently by Sankaran et al. (2010) using a phrase-based approach. On the other hand, incremental speech translation has been addressed in simultaneous translation of lectures and speeches (Hamon et al., 2009; F¨ugen et al., 2007). Some previous work (Cettolo and Federico, 2006; Rao et al., 2007; Matusov et al., 2007) addressed source text (reference or ASR hypothesis) segmentation strategies in speech translation. Constraining the search process during decoding to be monotonic (Tillmann and Ney, 2000) is one way of reducing latency and promoting incrementality. However, finding the optimal segmentation of the complete source s"
I13-1141,steinberger-etal-2006-jrc,0,0.0104454,"e development set. The average length of a segment using this strategy is 6.56±4.73 words. 4 First, we used the Moses toolkit (Koehn et al., 2007) for statistical machine translation. Minimum error rate training (MERT) was performed on the development set (dev2010) to optimize the feature weights of the log-linear model used in translation. During decoding, the unknown words were preserved in the hypotheses. The parallel text for building the English-French translation model – around 6.3 million parallel sentences – was obtained from several corpora: Europarl (Koehn, 2005), jrc-acquis corpus (Steinberger et al., 2006), Opensubtitle corpus (Tiedemann and Lars Nygaard, 2004), WMT11 Gigaword (Callison-Burch et al., 2011), WMT11 News (Callison-Burch et al., 2011), and Web crawling (Rangarajan Sridhar et al., 2011) as well as human translation of proprietary data. Second, we used a finite-state implementation of translation without reordering. We represent the phrase translation table as a weighted finite state transducer (FST) and the language model as a finite-state acceptor. The weight on the arcs of the FST is the dot product of the MERT weights with the translation scores. Our FST-based translation is the"
I13-1141,tiedemann-nygaard-2004-opus,0,0.0847162,"Missing"
I13-1141,C00-2123,0,0.141885,"Missing"
I13-1141,C96-1070,0,0.211273,"Missing"
I13-1141,E09-1040,0,0.0705868,"Missing"
I13-1141,P07-2045,0,0.00495648,"Missing"
I13-1141,2005.mtsummit-papers.11,0,\N,Missing
I13-1141,federico-etal-2012-iwslt,0,\N,Missing
I13-1141,2011.iwslt-evaluation.1,0,\N,Missing
J00-1004,W97-0408,0,0.0377039,"ansduction function for a head transducer to be the function that m a p s an input string to the o u t p u t string p r o d u c e d b y the lowest-cost valid derivation taken over all initial states and initial symbols. (Formally, the function is partial in that it is not defined on an input w h e n there are no derivations or w h e n there are multiple outputs with the same minimal cost.) In the transducers p r o d u c e d b y the training m e t h o d described in this paper, the source and target positions are in the set { - 1 , 0 , 1 } , t h o u g h we have also used handcoded transducers (Alshawi and Xia 1997) and automatically trained transducers (A1shawl and Douglas 2000) with a larger range of positions. 2.2 Relationship to Standard FSTs The operation of a traditional left-to-right transducer can be simulated by a head transducer b y starting at the leftmost input symbol and setting the positions of the first transition taken to a = 0 and fl = 0, and the positions for subsequent transitions to o~ = 1 and fl = 1. However, we can illustrate the fact that head transducers are more 47 Computational Linguistics Volume 26, Number 1 a:a a:a ~ b:b 0:0 Figure 2 Head transducer to reverse an input string"
J00-1004,J90-2002,0,0.178418,"Missing"
J00-1004,J93-2003,0,0.0511896,"e, a suitable statistical function needs to indicate the strength of cooccurrence correlation between source and target words, which we assume is indicative of carrying the same semantic content. Our preferred choice of statistical measure for assigning the costs is the ~ correlation measure (Gale and Church 1991). We apply this statistic to co-occurrence of the source word with all its possible translations in the data set examples. We have found that, at least for our data, this measure leads to better performance than the use of the log probabilities of target words given source words (cf. Brown et al. 1993). In addition to the correlation measure, the cost for a pairing includes a distance measure component that penalizes pairings proportionately to the difference between the (normalized) positions of the source and target words in their respective sentences. 4.2 Computing Hierarchical Alignments As noted earlier, dependency transduction models are generative probabilistic models; each derivation generates a pair of dependency trees. Such a pair can be represented as a synchronized hierarchical alignment of two strings. A hierarchical alignment consists of four functions. The first two functions"
J00-1004,J94-4004,0,0.0464369,"do not necessarily expect the dependency representations produced by the training method to be traditional dependency structures for the two languages. Instead, the aim is to produce bilingual (i.e., synchronized, see below) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus. For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching. This contrasts with one of the traditional approaches (e.g., Dorr 1994; Watanabe 1995) to posing the translation problem, i.e., the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language. The training method has four stages: (i) Compute co-occurrence statistics from the training data. (ii) Search for an optimal synchronized hierarchical alignment for each bitext. (iii) Construct a set of head transducers that can generate these alignments with transition weights derived from maximum likelihood estimation. 4.1 Computing Pairing Costs For each sourc"
J00-1004,P81-1022,0,0.092352,", vo) )P(Dwo,vo) where P(Dw,v) is the probability of a subderivation headed by w and v, that is P(Dw,v) = P(qo, qllw, v) H P(qi+l, Wi, Vi,~i, fli]w,v, qi)P(Dwi,vl) 1Kiln for a derivation in which the dependents of w and v are generated by n transitions. 3.2 Transduction Algorithm To carry out translation with a dependency transduction model, we apply a dynamic programming search to find the optimal derivation. This algorithm can take as input either word strings, or word lattices produced by a speech recognizer. The algorithm is similar to those for context-free parsing such as chart parsing (Earley 1970) and the CKY algorithm (Younger 1967). Since w o r d string input is a special case of word lattice input, we need only describe the case of lattices. We n o w present a sketch of the transduction algorithm. The algorithm works bottom-up, maintaining a set of configurations. A configuration has the form In1, n2, w, v, q, c, t] corresponding to a bottom-up partial derivation currently in state q covering an input sequence between nodes nl and n2 of the input lattice, w and v are the topmost 50 Alshawi, Bangalore, and Douglas Learning Dependency Translation Models nodes in the source and target"
J00-1004,H91-1026,0,0.0601108,"age. These translations of the source word may be zero, one, or several target language words (see Section 4.4 for discussion of the multiword case). The assignment of translation pairing costs (effectively a statistical bilingual dictionary) may be done using various statistical measures. For this purpose, a suitable statistical function needs to indicate the strength of cooccurrence correlation between source and target words, which we assume is indicative of carrying the same semantic content. Our preferred choice of statistical measure for assigning the costs is the ~ correlation measure (Gale and Church 1991). We apply this statistic to co-occurrence of the source word with all its possible translations in the data set examples. We have found that, at least for our data, this measure leads to better performance than the use of the log probabilities of target words given source words (cf. Brown et al. 1993). In addition to the correlation measure, the cost for a pairing includes a distance measure component that penalizes pairings proportionately to the difference between the (normalized) positions of the source and target words in their respective sentences. 4.2 Computing Hierarchical Alignments A"
J00-1004,J97-3002,0,0.32359,"Missing"
J09-3002,P03-1006,0,0.0607606,"Missing"
J09-3002,N04-1005,1,0.861089,"Missing"
J09-3002,J99-2004,1,0.691957,"y generalizing the corpus to remove information that is speciﬁc only to the other domain and instantiating the generalized corpus to our domain. Although there are a number of ways of generalizing the out-of-domain corpus, the generalization we have investigated involved identifying linguistic units, such as noun and verb chunks, in the out-of-domain corpus and treating them as classes. These classes are then instantiated to the corresponding linguistic units from the MATCH domain. The identiﬁcation of the linguistic units in the out-of-domain corpus is done automatically using a supertagger (Bangalore and Joshi 1999). We use a corpus collected in the context of a software help-desk application as an example out-of-domain corpus. In cases where the out-of-domain corpus is closely related to the domain at hand, a more semantically driven generalization might be more suitable. Figure 29 illustrates the process of migrating data from one domain to another. 5.6 Adapting the Switchboard Language Model We investigated the performance of a large-vocabulary conversational speech recognition system when applied to a speciﬁc domain such as MATCH. We used the Switchboard corpus (Cswbd ) as an example of a large-vocab"
J09-3002,W00-0508,1,0.785542,"ng information about them (INFO), requesting subway directions (ROUTE), and zooming the map (ZOOM). As in Johnston and Bangalore (2000, 2005), this multimodal grammar is compiled into a cascade of ﬁnite-state transducers. Finite-state machines have been extensively applied to many aspects of language processing, including speech recognition (Riccardi, Pieraccini, and Bocchieri 1996; Pereira and Riley 1997), phonology (Kartunnen 1991; Kaplan and Kay 1994), morphology (Koskenniemi 1984), chunking (Abney 1991; Joshi and Hopely 1997; Bangalore 1997), parsing (Roche 1999), and machine translation (Bangalore and Riccardi 2000). Finite-state models are attractive mechanisms for language processing since they are (a) efﬁciently learnable from data; (b) generally effective for decoding; and (c) associated with a calculus for composing machines which allows for straightforward integration of constraints from various levels of language processing. Furthermore, software implementing the ﬁnite-state calculus is available for research purposes (Noord 1997; Mohri, Pereira, and Riley 1998; Kanthak and Ney 2004; Allauzen et al. 2007). We compile the multimodal grammar into a ﬁnite-state device operating over two input streams"
J09-3002,P93-1008,0,0.0882454,"ltimodal inputs and their meanings. The assignment of meaning to a multimodal output is achieved by parsing the utterance using the grammar. In a grammar-based speech-only system, if the language model of ASR is derived directly from the grammar, then every ASR output can be parsed and assigned a meaning by the grammar. However, using an SLM results in ASR outputs that may not be parsable by the grammar and hence cannot be assigned a meaning by the grammar. Robustness in such cases is achieved by either (a) modifying the parser to accommodate for unparsable substrings in the input (Ward 1991; Dowding et al. 1993; Allen et al. 2001) or (b) modifying the meaning representation to make it learnable as a classiﬁcation task using robust machine learning techniques as is done in large scale human-machine dialog systems (e.g., Gorin, Riccardi, and Wright 1997). In our grammar-based multimodal system, the grammar serves as the speechgesture alignment model and assigns a meaning representation to the multimodal input. Failure to parse a multimodal input implies that the speech and gesture inputs are not fused together and consequently may not be assigned a meaning representation. In order to improve robustnes"
J09-3002,P98-1102,1,0.835597,"which have typically operated over linear sequences of speech or text, ∗ 180 Park Avenue, Florham Park, NJ 07932. E-mail: srini@research.att.com. ∗∗ 180 Park Avenue, Florham Park, NJ 07932. E-mail: johnston@research.att.com. Submission received: 26 May 2006; revised submission received: 6 May 2008; accepted for publication: 11 July 2008. © 2009 Association for Computational Linguistics Computational Linguistics Volume 35, Number 3 need to be extended in order to support integration and understanding of multimodal language distributed over multiple different input modes (Johnston et al. 1997; Johnston 1998b). Multimodal grammars provide an expressive mechanism for quickly creating language processing capabilities for multimodal interfaces supporting input modes such as speech and gesture (Johnston and Bangalore 2000). They support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation. Johnston and Bangalore (2005) show that such grammars can be compiled into ﬁnite-state transducers, enabling effective processing of l"
J09-3002,C00-1053,1,0.876038,"ither symbolic gestures or handwritten words. The word lattice and ink lattice are integrated and assigned a combined meaning representation by the multimodal integration and understanding component (Johnston and Bangalore 2000; Johnston et al. 2002b). Because we implement this component using ﬁnite-state transducers, we refer to this component as the Multimodal Finite State Transducer (MMFST). The approach used in the MMFST component for integrating and interpreting multimodal inputs (Johnston et al. 2002a, 2002b) is an extension of the ﬁnite-state approach previously proposed (Bangalore and Johnston 2000; Johnston and Bangalore 2000, 2005). (See Section 3 for details.) This provides as output a lattice encoding all of the potential meaning representations assigned to the user’s input. The meaning is represented in XML, facilitating parsing and logging by other system components. MMFST can receive inputs and generate outputs using multiple communication protocols, including the W3C EMMA standard for representation of multimodal inputs (Johnston et al. 2007). The meaning lattice is ﬂattened to an n-best list and passed to a multimodal dialog manager (MDM) (Johnston et al. 2002b), which reranks"
J09-3002,C00-1054,1,0.677119,"ohnston@research.att.com. Submission received: 26 May 2006; revised submission received: 6 May 2008; accepted for publication: 11 July 2008. © 2009 Association for Computational Linguistics Computational Linguistics Volume 35, Number 3 need to be extended in order to support integration and understanding of multimodal language distributed over multiple different input modes (Johnston et al. 1997; Johnston 1998b). Multimodal grammars provide an expressive mechanism for quickly creating language processing capabilities for multimodal interfaces supporting input modes such as speech and gesture (Johnston and Bangalore 2000). They support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation. Johnston and Bangalore (2005) show that such grammars can be compiled into ﬁnite-state transducers, enabling effective processing of lattice input from speech and gesture recognition and mutual compensation for errors and ambiguities. In this article, we show how multimodal grammars and their ﬁnite-state implementation can be extended to support m"
J09-3002,P04-3033,1,0.798203,"tion is most valuable if it can be delivered effectively while mobile, since users’ needs change rapidly and the information itself is dynamic (e.g., train times change and shows get cancelled). MATCH (Multimodal Access To City Help) is a working city guide and navigation system that enables mobile users to access restaurant and subway information for urban centers such as New York City and Washington, DC (Johnston et al. 2002a, 2002b). MATCH runs stand-alone on a tablet PC (Figure 1) or in client-server mode across a wireless network. There is also a kiosk version of the system (MATCHkiosk) (Johnston and Bangalore 2004) which incorporates a life-like talking head. In this article, we focus on the mobile version of MATCH, in which the user interacts with a graphical interface displaying restaurant listings and a dynamic map showing locations and street information. The inputs can be speech, drawings on the display with a stylus, or synchronous multimodal combinations of the two modes. The user can ask for reviews, cuisine, phone number, address, or other information about restaurants and for subway directions to restaurants and locations. The system responds with graphical callouts on the display, synchronize"
J09-3002,P97-1036,1,0.272604,"processing techniques, which have typically operated over linear sequences of speech or text, ∗ 180 Park Avenue, Florham Park, NJ 07932. E-mail: srini@research.att.com. ∗∗ 180 Park Avenue, Florham Park, NJ 07932. E-mail: johnston@research.att.com. Submission received: 26 May 2006; revised submission received: 6 May 2008; accepted for publication: 11 July 2008. © 2009 Association for Computational Linguistics Computational Linguistics Volume 35, Number 3 need to be extended in order to support integration and understanding of multimodal language distributed over multiple different input modes (Johnston et al. 1997; Johnston 1998b). Multimodal grammars provide an expressive mechanism for quickly creating language processing capabilities for multimodal interfaces supporting input modes such as speech and gesture (Johnston and Bangalore 2000). They support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation. Johnston and Bangalore (2005) show that such grammars can be compiled into ﬁnite-state transducers, enabling effective"
J09-3002,P04-1065,0,0.0211015,"chunking (Abney 1991; Joshi and Hopely 1997; Bangalore 1997), parsing (Roche 1999), and machine translation (Bangalore and Riccardi 2000). Finite-state models are attractive mechanisms for language processing since they are (a) efﬁciently learnable from data; (b) generally effective for decoding; and (c) associated with a calculus for composing machines which allows for straightforward integration of constraints from various levels of language processing. Furthermore, software implementing the ﬁnite-state calculus is available for research purposes (Noord 1997; Mohri, Pereira, and Riley 1998; Kanthak and Ney 2004; Allauzen et al. 2007). We compile the multimodal grammar into a ﬁnite-state device operating over two input streams (speech and gesture) and one output stream (meaning). The transition symbols of the FSA correspond to the terminals of the multimodal grammar. For the sake of illustration here and in the following examples we will only show the portion of the three-tape ﬁnite-state device which corresponds to the DEICNP rule in the grammar in Figure 15. The corresponding ﬁnite-state device is shown in Figure 16. This three-tape machine is then factored into two transducers: R:G → W and T :(G ×"
J09-3002,J94-3001,0,0.0526779,"this article. This grammar is simpliﬁed for ease of exposition. The rules capture spoken, multimodal, and pen-only commands for showing restaurants (SHOW), getting information about them (INFO), requesting subway directions (ROUTE), and zooming the map (ZOOM). As in Johnston and Bangalore (2000, 2005), this multimodal grammar is compiled into a cascade of ﬁnite-state transducers. Finite-state machines have been extensively applied to many aspects of language processing, including speech recognition (Riccardi, Pieraccini, and Bocchieri 1996; Pereira and Riley 1997), phonology (Kartunnen 1991; Kaplan and Kay 1994), morphology (Koskenniemi 1984), chunking (Abney 1991; Joshi and Hopely 1997; Bangalore 1997), parsing (Roche 1999), and machine translation (Bangalore and Riccardi 2000). Finite-state models are attractive mechanisms for language processing since they are (a) efﬁciently learnable from data; (b) generally effective for decoding; and (c) associated with a calculus for composing machines which allows for straightforward integration of constraints from various levels of language processing. Furthermore, software implementing the ﬁnite-state calculus is available for research purposes (Noord 1997;"
J09-3002,P84-1038,0,0.0912453,"Missing"
J09-3002,1997.iwpt-1.19,0,0.354789,"point of reference to compare the performance of language models trained on derived corpora. 374 Bangalore and Johnston Robust Understanding in Multimodal Interfaces 5.2 Grammar as Language Model The multimodal context-free grammar (CFG; a fragment is presented in Section 2 and a larger fragment is shown in Section 3.2) encodes the repertoire of language and gesture commands allowed by the system and their combined interpretations. The CFG can be approximated by a ﬁnite state machine (FSM) with arcs labeled with language, gesture, and meaning symbols, using well-known compilation techniques (Nederhof 1997). Selecting the language symbol of each arc (projecting the FSM on the speech component) results in an FSM that can be used as the language model acceptor (Ggram ) for speech recognition. Note that the resulting language model acceptor is unweighted if the grammar is unweighted and suffers from not being robust to language variations in users’ input. However, due to the tight coupling of the grammars used for recognition and interpretation, every recognized string can be assigned a meaning representation (though it may not necessarily be the intended interpretation). 5.3 Grammar-Based n-gram L"
J09-3002,J03-1002,0,0.00143086,"he past two time steps: trigram assumption for our purposes) to compute the joint probability P(Su , Sg ), shown in Equation (13). S∗g = argmax P(Sg |Su ) (10) Sg = argmax P(Sg , Su ) (11) Sg = argmax P(S0u , S0g ) ∗ P(S1u , S1g |S0u , S0g ) . . . ∗ P(Snu , Sng |S0u , S0g , . . . , Sun−1 , Sgn−1 ) (12) Sg S∗g = argmax  P(Siu , Sig |Siu−1 , Siu−2 , Sig−1 , Sig−2 ) (13) Sg where Su = S1u S2u . . . Snu and Sg = S1g S2g . . . Sm g. In order to compute the joint probability, we need to construct an alignment between tokens (Siu , Sig ). We use the Viterbi alignment provided by the GIZA++ toolkit (Och and Ney 2003) for this purpose. We convert the Viterbi alignment into a bilanguage representation that pairs words of the string Su with words of Sg . A few examples of bilanguage strings are shown in Figure 32. We compute the joint n-gram model using a language modeling toolkit (Gofﬁn et al. 2005). Equation (13) thus allows us to edit a user’s utterance to a string that can be interpreted by the grammar. Figure 32 A few examples of bilanguage strings. 384 Bangalore and Johnston Robust Understanding in Multimodal Interfaces 6.3.1 Deriving a Translation Corpus. Because our multimodal grammar is implemented"
J09-3002,W97-1401,0,0.0166651,"Missing"
J09-3002,W04-3008,0,0.0323667,"ata with meaning representations and alignment of the meaning representations with word strings. This can be complex and expensive, involving a detailed labeling guide and instructions for annotators. In contrast in this approach, if data is used, all that is needed is transcription of the audio, a far more straightforward annotation task. If no data is used then grammar sampling can be used instead and no annotation of data is needed whatsoever. 5. Although data-driven approaches to understanding are commonplace in research, rule-based techniques continue to dominate in much of the industry (Pieraccini 2004). See, for example, the W3C SRGS standard (www.w3.org/TR/speech-grammar/). Acknowledgments We dedicate this article to the memory of Candy Kamm whose continued support for multimodal research made this work possible. We thank Patrick Ehlen, Helen Hastie, Preetam Maloor, Amanda Stent, Gunaranjan Vasireddy, Marilyn Walker, and Steve Whittaker for their contributions to the MATCH system. We also thank Richard Cox and Mazin Gilbert for their ongoing support of multimodal research at AT&T Labs–Research. We would also like to thank the anonymous reviewers for their many helpful comments and suggesti"
J09-3002,W05-0625,0,0.0232926,"Missing"
J09-3002,C02-2026,1,0.780703,"s (supertags) (Bangalore and Joshi 1999) each associated with a lexical item (the head). Supertags encode predicate–argument relations of the head and the linear order of its arguments with respect to the head. In Figure 30, we show the supertag associated with the word show in an imperative sentence such as show the Empire State Building. A supertag can be represented as a ﬁnite-state machine with the head and its arguments as arc labels (Figure 31). The set of sentences generated by an LTAG can be obtained by combining supertags using substitution and adjunction operations. In related work (Rambow et al. 2002), it has been shown that for a restricted version of LTAG, the combinations of a set of supertags can be represented as an FSM. This FSM compactly encodes the set of sentences generated by an LTAG grammar. It is composed of two transducers, a lexical FST, and a syntactic FSM. The lexical FST transduces input words to supertags. We assume that as input to the construction of the lexical machine we have a list of words with their parts-of-speech. Once we have determined for each word the set of supertags they should be associated Figure 30 Supertag tree for the word show. The NP nodes permit sub"
J09-3002,W95-0107,0,0.0171531,"redicate (c∗ ) for a N token multimodal utterance (SN 1 ) by searching for the predicate (c) that maximizes the posterior probability as shown in Equation (6). c∗ = argmax P(c |SN 1 ) (6) c We view the problem of identifying and extracting arguments from a multimodal input as a problem of associating each token of the input with a speciﬁc tag that encodes 381 Computational Linguistics Volume 35, Number 3 the label of the argument and the span of the argument. These tags are drawn from a tagset which is constructed by extending each argument label by three additional symbols I, O, B, following Ramshaw and Marcus (1995). These symbols correspond to cases when a token is inside (I) an argument span, outside (O) an argument span, or at the boundary of two argument spans (B) (See Table 5). Given this encoding, the problem of extracting the arguments amounts to a search for the most likely sequence of tags (T∗ ) given the input multimodal utterance SN 1 as shown in Equation (7). We approximate the posterior probability P(T |SN 1 ) using independence assumptions to include the lexical context in an n-word window and the preceding two tag labels, as shown in Equation (8). T∗ = argmax P(T |SN 1 ) T ≈ argmax T  n+1"
J09-3002,P99-1024,0,0.0405247,"rfaces are intended for use in environments subject to noise. The other alternative, ”click-and-hold,” where the user has to hold down a button for the duration of their speech, is also problematic because it limits the ability of the user to use pen input while they are speaking. 4 See Johnston and Bangalore (2005) for a detailed explanation. 368 Bangalore and Johnston Robust Understanding in Multimodal Interfaces 3.6 Multimodal Dialog Management and Contextual Resolution The multimodal dialog manager (MDM) is based on previous work on speech-act based models of dialog (Rich and Sidner 1998; Stent et al. 1999). It uses a Java-based toolkit for writing dialog managers that is similar in philosophy to TrindiKit (Larsson et al. 1999). It includes several rule-based processes that operate on a shared state. The state includes system and user intentions and beliefs, a dialog history and focus space, and information about the speaker, the domain, and the available modalities. The processes include interpretation, update, selection, and generation. The interpretation process takes as input an n-best list of possible multimodal interpretations for a user input from the MMFST. It rescores them according to"
J09-3002,P01-1066,0,0.0142603,"Missing"
J09-3002,W02-2110,0,0.0118357,"required and optional roles for different types of actions); if it is not, then the system’s next move is to take the initiative and start an information-gathering subdialogue. If the input is fully speciﬁed, the system’s next move is to perform the command or answer the question; to do this, MDM communicates directly with the UI. The generation process performs template-based generation for simple responses and updates the system’s model of the user’s intentions after generation. A text planning component (TEXTPLAN) is used for more complex generation, such as the generation of comparisons (Walker et al. 2002, 2004). In the case of a navigational query, such as the example in Section 2, MDM ﬁrst receives a route query in which only the destination is speciﬁed: How do I get to this place?. In the selection phase it consults the domain ontology and determines that a source is also required for a route. It adds a request to query the user for the source to the system’s next moves. This move is selected and the generation process selects a prompt and sends it to the TTS component. The system asks Where do you want to go from?. If the user says or writes 25th Street and 3rd Avenue then the MMFST will a"
J09-3002,H93-1008,0,\N,Missing
J09-3002,C98-1099,1,\N,Missing
J09-3002,P02-1048,1,\N,Missing
J99-2004,J94-4005,0,0.0123151,"es are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probability information that is encoded in the rules. This allows the system to assign the most-likely structure to each input. The output of these systems consists of constituent analysis, the degree of detail of which is dependent on the detail of annotation present in the treebank that is used to train the system. There are also parsers that use probabilistic (weighting) information in conjunction with hand-crafted grammars, for example, Black et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991), Link Grammar (Sleator and Tempe"
J99-2004,P93-1005,0,0.00471353,"language. The resultant set of rules are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probability information that is encoded in the rules. This allows the system to assign the most-likely structure to each input. The output of these systems consists of constituent analysis, the degree of detail of which is dependent on the detail of annotation present in the treebank that is used to train the system. There are also parsers that use probabilistic (weighting) information in conjunction with hand-crafted grammars, for example, Black et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Jos"
J99-2004,P98-1022,0,0.0434909,"Missing"
J99-2004,P93-1035,0,0.00518333,"ees are combined by substitution and adjunction operations. The result of combining the elementary trees is the derived tree and the process of combining the elementary trees to yield a parse of the sentence is represented by the derivation tree. The derivation tree can also be interpreted as a dependency tree with unlabeled arcs between words of the sentence. A more detailed discussion of LTAGs with an example and some of the key properties of elementary trees is presented in Appendix A. 4. Supertags Part-of-speech disambiguation techniques (POS taggers) (Church 1988; Weischedel et al. 1993; Brill 1993) are often used prior to parsing to eliminate (or substantially reduce) the part-of-speech ambiguity. The POS taggers are all local in the sense that they use information from a limited context in deciding which tag(s) to choose for each word. As is well known, these taggers are quite successful. In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (tree). The elementary structures of LTAG localize dependencies, including long-distance dependencies, by requiring that all and only the dependent ele"
J99-2004,C96-2183,0,0.00538926,"Missing"
J99-2004,A88-1019,0,0.0171294,"ted by auxiliary trees. Elementary trees are combined by substitution and adjunction operations. The result of combining the elementary trees is the derived tree and the process of combining the elementary trees to yield a parse of the sentence is represented by the derivation tree. The derivation tree can also be interpreted as a dependency tree with unlabeled arcs between words of the sentence. A more detailed discussion of LTAGs with an example and some of the key properties of elementary trees is presented in Appendix A. 4. Supertags Part-of-speech disambiguation techniques (POS taggers) (Church 1988; Weischedel et al. 1993; Brill 1993) are often used prior to parsing to eliminate (or substantially reduce) the part-of-speech ambiguity. The POS taggers are all local in the sense that they use information from a limited context in deciding which tag(s) to choose for each word. As is well known, these taggers are quite successful. In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (tree). The elementary structures of LTAG localize dependencies, including long-distance dependencies, by requirin"
J99-2004,P96-1025,0,0.0103291,"tems use any statistical information to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it. These systems attempt to assign some structure to every input string. The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language. The resultant set of rules are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probability information that is encoded i"
J99-2004,C94-2149,0,0.0474949,"agging. 6.3 N-gram Models with Smoothing We have improved the performance of the trigram model by incorporating smoothing techniques into the model and training the model on a larger training corpus. We have also proposed some new models for supertag disambiguation. In this section, we discuss these developments in detail. Two sets of data are used for training and testing the models for supertag disambiguation. The first set has been collected by parsing the Wall Street Journal 7, IBM Manual, and ATIS corpora using the wide-coverage English grammar being developed as part of the XTAG system (Doran et al. 1994). The correct derivation from all the derivations produced by the XTAG system was picked for each sentence from these corpora. The second and larger data set was collected by converting the Penn Treebank parses of the Wall Street Journal sentences. The objective was to associate each lexical item of a sentence with a supertag, given the phrase structure parse of the sentence. This process involved a number of heuristics based on local tree contexts. The heuristics made use of information about the labels of a word's dominating nodes (parent, grandparent, and great-grandparent), labels of its s"
J99-2004,W89-0209,0,0.0704691,"e also Joshi (1998). 238 Bangalore and Joshi Supertagging matches the input string at a given position. At present none of these systems use any statistical information to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it. These systems attempt to assign some structure to every input string. The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language. The resultant set of rules are not linguistically transparen"
J99-2004,P84-1058,0,0.107157,", for example, Black et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991), Link Grammar (Sleator and Temperley 1991), and some version of GB (Chomsky 1992). Parsing, lexical semantics, and machine translation, to name a few areas, have all benefited from lexicalization. Lexicalization provides a clean interface for combining the syntactic and semantic information in the lexicon. We discuss the merits of lexicalization and other related issues in the context of partial parsing and briefly discuss Feature-based Lexicalized Tree Adjoining Grammars (LTAGs) as a representative of the class of lexicalized grammars. Feature-based Lexicalized"
J99-2004,H94-1052,0,0.00730541,"n position. At present none of these systems use any statistical information to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it. These systems attempt to assign some structure to every input string. The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language. The resultant set of rules are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probabil"
J99-2004,C94-1024,1,0.249145,"g can be seen as specifying dependency requirements of the supertag. The probability with which a supertag depends on another supertag is collected from a corpus of sentences annotated with derivation structures. Given a set of supertags for each word and the dependency information between pairs of supertags, the objective of the dependency model is to compute the most likely dependency linkage that spans the entire string. The result of producing the dependency linkage is a sequence of supertags, one for each word of the sentence along with the dependency information. Since first reported in Joshi and Srinivas (1994), we have not continued experiments using this model of supertagging, primarily for two reasons. We are restrained by the lack of a large corpus of LTAG parsed derivation structures that is needed to reliably estimate the various parameters of this model. We are currently in the process of collecting a large LTAG parsed WSJ corpus, with each sentence annotated with the correct derivation. A second reason for the disuse of the dependency model for supertagging is that the objective of supertagging is to see how far local techniques can be used to disambiguate supertags even before parsing begin"
J99-2004,P95-1013,0,0.013813,"Missing"
J99-2004,C94-1025,0,0.0324563,"Missing"
J99-2004,P95-1037,0,0.0275782,"none of these systems use any statistical information to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it. These systems attempt to assign some structure to every input string. The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language. The resultant set of rules are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probability information t"
J99-2004,J93-2004,0,0.0366461,"Missing"
J99-2004,C88-2121,1,0.411404,"Missing"
J99-2004,E93-1040,0,0.0187113,"Missing"
J99-2004,1997.iwpt-1.22,0,0.167114,"Missing"
J99-2004,1995.iwpt-1.27,0,0.0261713,"Missing"
J99-2004,C94-1104,0,0.0214218,"Missing"
J99-2004,J93-2006,0,0.0134038,"ary trees. Elementary trees are combined by substitution and adjunction operations. The result of combining the elementary trees is the derived tree and the process of combining the elementary trees to yield a parse of the sentence is represented by the derivation tree. The derivation tree can also be interpreted as a dependency tree with unlabeled arcs between words of the sentence. A more detailed discussion of LTAGs with an example and some of the key properties of elementary trees is presented in Appendix A. 4. Supertags Part-of-speech disambiguation techniques (POS taggers) (Church 1988; Weischedel et al. 1993; Brill 1993) are often used prior to parsing to eliminate (or substantially reduce) the part-of-speech ambiguity. The POS taggers are all local in the sense that they use information from a limited context in deciding which tag(s) to choose for each word. As is well known, these taggers are quite successful. In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (tree). The elementary structures of LTAG localize dependencies, including long-distance dependencies, by requiring that all and only the"
J99-2004,H92-1026,0,\N,Missing
J99-2004,H93-1047,0,\N,Missing
J99-2004,C98-1022,0,\N,Missing
K18-2015,W03-3017,0,0.0748643,"use a transliteration module to transcribe data into Roman form for efficient processing. • Segmentation We mainly use this module to identify word boundaries in certain languages such as Chinese where space is not used as a boundary marker. 153 Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 153–159 c Brussels, Belgium, October 31 – November 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/K18-2015 2 System Architecture 2.1 2.2 2.2.1 Parsing Algorithm We employ an arc-standard transition system (Nivre, 2003) as our parsing algorithm. A typical transition-based parsing system uses the shiftreduce decoding algorithm to map a parse tree onto a sequence of transitions. Throughout the decoding process a stack and a queue data structures are maintained. The queue stores the sequence of raw input, while the stack stores the partially processed input which may be linked with the rest of the words in the queue. The parse tree is build by consuming the words in the queue from left to right by applying a set of transition actions. There are three kinds of transition actions that are performed in the parsing"
K18-2015,P09-1040,0,0.0193072,"he decoding process a stack and a queue data structures are maintained. The queue stores the sequence of raw input, while the stack stores the partially processed input which may be linked with the rest of the words in the queue. The parse tree is build by consuming the words in the queue from left to right by applying a set of transition actions. There are three kinds of transition actions that are performed in the parsing process: Shift, Left-Arc, Right-Arc. Additionally, we use a Swap action which reorders top node in the stack and the top node in the queue for parsing non-projective arcs (Nivre, 2009). At training time, the transition actions are inferred from the gold parse trees and the mapping between the parser state and the transition action is learned using a simple LSTM-based neural networking architecture presented in Goldberg (2016). While training, we use the oracle presented in (Nivre et al., 2009) to restrict the number of Swap actions needed to parse non-projective arcs. Given that Bi-LSTMs capture global sentential context at any given time step, we use minimal set of features in our parsing model. At each parser state, we restrict our features to just two top nodes in the st"
K18-2015,W09-3811,0,0.0284613,"t by applying a set of transition actions. There are three kinds of transition actions that are performed in the parsing process: Shift, Left-Arc, Right-Arc. Additionally, we use a Swap action which reorders top node in the stack and the top node in the queue for parsing non-projective arcs (Nivre, 2009). At training time, the transition actions are inferred from the gold parse trees and the mapping between the parser state and the transition action is learned using a simple LSTM-based neural networking architecture presented in Goldberg (2016). While training, we use the oracle presented in (Nivre et al., 2009) to restrict the number of Swap actions needed to parse non-projective arcs. Given that Bi-LSTMs capture global sentential context at any given time step, we use minimal set of features in our parsing model. At each parser state, we restrict our features to just two top nodes in the stack. Since Swap action distorts the linear order of word sequence, it renders the LSTM representations irrelevant in case of non-projective sentences. To capture this distortion, we also use the top most word in the queue as an additional feature. Text Processing Given the nature of the shared task, sentence and"
K18-2015,K17-3009,0,0.0825605,"Missing"
K18-2015,P17-1159,0,0.0190567,"ost frequent words in the raw corpora. The distributed word representations for each language are learned separately from their monolingual corpora collected from Web to Corpus (W2C) (Majliˇs, 2011)2 and latest wiki dumps3 . The word representations are learned using Skipgram model with negative sampling which is implemented in word2vec toolkit (Mikolov et al., 2013). For our backoff character model we only use 64-dimension character Bi-LSTM embeddings in the input layer of the network. fer by incorporating the syntactic knowledge from resource-rich domain into resource-poor domain. Recently, Wang et al. (2017); Bhat et al. (2018) showed significant improvements in parsing social media texts by injecting syntactic knowledge from large cross-domain treebanks using neural stacking. As shown in Figure 3, we transfer both POS tagging and parsing information from the source model. For tagging, we augment the input layer of the target tagger with the hidden layer of multilayered perceptron (MLP) of the source tagger. For transferring parsing knowledge, hidden representations from the parser specific Bi-LSTM of the source parser are augmented with the input layer of the target parser which already includes"
K18-2015,K18-2001,0,0.0532698,"Missing"
K18-2015,P16-1147,0,0.0309867,"Missing"
N01-1018,J94-3001,0,\N,Missing
N01-1018,J99-2004,1,\N,Missing
N01-1018,woszczcyna-etal-1998-modular,0,\N,Missing
N01-1018,J01-1001,0,\N,Missing
N01-1018,W00-0508,1,\N,Missing
N01-1018,knight-al-onaizan-1998-translation,0,\N,Missing
N01-1018,P91-1032,0,\N,Missing
N01-1018,J97-3002,0,\N,Missing
N01-1018,J00-1003,0,\N,Missing
N01-1018,P98-1006,1,\N,Missing
N01-1018,C98-1006,1,\N,Missing
N04-1005,J99-2004,1,0.92922,"y generalizing the corpus to remove information specific only to the out-of-domain and instantiating the generalized corpus to the MATCH domain. Although there are a number of ways of generalizing the out-of-domain corpus, the generalization we have investigated involved identifying linguistic units, such as noun and verb chunks in the out-of-domain corpus and treating them as classes. These classes are then instantiated to the corresponding linguistic units from the MATCH domain. The identification of the linguistic units in the out-of-domain corpus is done automatically using a supertagger (Bangalore and Joshi, 1999). We use a corpus collected in the context of a software helpdesk application as an example out-of-domain corpus. In cases where the out-of-domain corpus is closely related to the domain at hand, a more semantically driven generalization might be more suitable. 3.6 Adapting the SwitchBoard Language Model We investigate the performance of a large vocabulary conversational speech recognition system when applied to a specific domain such as MATCH. We used the Switchboard corpus ( ;< .= < ) as an example of a large vocabulary conversational speech corpus. We built a trigram model ( > .= < ) usi"
N04-1005,P93-1008,0,0.281124,"sification-based approach relies on local information and is more conducive for identifying the simple predicates in MATCH. Second, the patternmatching approach uses the entire grammar as a model for matching while the classification approach is trained on the training data which is significantly smaller when compared to the number of examples encoded in the grammar. 6 Discussion Although we are not aware of any attempts to address the issue of robust understanding in the context of multimodal systems, this issue has been of great interest in the context of speech-only conversational systems (Dowding et al., 1993; Seneff, 1992; Allen et al., 2000; Lavie, 1996). The output of the recognizer in these systems usually is parsed using a handcrafted grammar that assigns a meaning representation suited for the downstream dialog component. The coverage problems of the grammar and parsing of extra-grammatical utterances is typically addressed by retrieving fragments from the parse chart and incorporating operations that combine fragments to derive a meaning of the recognized utterance. We have presented an approach that achieves robust multimodal utterance understanding using the edit-distance automaton in a f"
N04-1005,1997.iwpt-1.19,0,0.178767,"igram language model ( >1CDFEGIH ) using the 709 multimodal and speech-only utterances as the corpus ( ;=CDFEGIH ). The performance of this model serves as the point of reference to compare the performance of language models trained on derived corpora. 3.2 Grammar as Language Model The multimodal CFG (a fragment is presented in Section 2) encodes the repertoire of language and gesture commands allowed by the system and their combined interpretations. The CFG can be approximated by an FSM with arcs labeled with language, gesture and meaning symbols, using well-known compilation techniques (Nederhof, 1997). The resulting FSM can be projected on the language component and can be used as the language model acceptor (&9JLKNMPO ) for speech recognition. Note that the resulting language model acceptor is unweighted if the grammar is unweighted and suffers from not being robust to language variations in user’s input. However, due to the tight coupling of the grammar used for recognition and interpretion, every recognized string can be assigned an interpretation (though it may not necessarily be the intended interpretation). 3.3 Grammar-based N-gram Language Model As mentioned earlier, a hand-crafted"
N04-1005,C02-2026,1,0.46169,"ons of wide-coverage, domain-independent, syntactic grammars for English in various formalisms (XTAG, 2001; Clark and Hockenmaier, 2002; Flickinger et al., 2000). Here, we describe a method that exploits one such grammar implementation in the Lexicalized TreeAdjoining Grammar (LTAG) formalism, for deriving domain-specific corpora. An LTAG consists of a set of elementary trees (Supertags) (Bangalore and Joshi, 1999) each associated with a lexical item. The set of sentences generated by an LTAG can be obtained by combining supertags using substitution and adjunction operations. In related work (Rambow et al., 2002), it has been shown that for a restricted version of LTAG, the combinations of a set of supertags can be represented as an FSM. This FSM compactly encodes the set of sentences generated by an LTAG grammar. We derive a domain-specific corpus by constructing a lexicon consisting of pairings of words with their supertags that are relevant to that domain. We then compile the grammar to build an FSM of all sentences upto a given length. We sample this FSM and build a language model as discussed in Section 3.3. Given untranscribed utterances from a specific domain, we can also adapt the language mod"
N04-1005,W95-0107,0,0.0258097,"A    !  is the set of arguments to the predicate.  We determine the   predicate ( ) for a  token multimodal utterance ( ) by maximizing the posterior probability as shown in Equation 7.  ( 2    8 (7) We view the problem of identifying and extracting arguments from a multimodal input as a problem of associating each token of the input with a specific tag that encodes the label of the argument and the span of the argument. These tags are drawn from a tagset which is constructed by extending   each argument label by three additional symbols # , following (Ramshaw and Marcus, 1995). These symbols correspond to cases when a token is inside ( ) an argument span, outside ( # ) an argument span or at the boundary of two argument spans ( ) (See Table 1). Given this encoding, the problem of extracting the arguments is a search for the most likely sequence  of tags (  ) given the input multimodal utterance as shown in Equation (8). We approximate the posterior proba2    bility * ( 8 using independence assumptions as    *   shown in Equation (9). Figure 5: Edit transducer with insertion, deletion, substitution and identity arcs. "" and "" could be words or phones. T"
N04-1005,E03-1078,0,0.0616032,"The coverage problems of the grammar and parsing of extra-grammatical utterances is typically addressed by retrieving fragments from the parse chart and incorporating operations that combine fragments to derive a meaning of the recognized utterance. We have presented an approach that achieves robust multimodal utterance understanding using the edit-distance automaton in a finite-state-based interpreter without the need for combining fragments from a parser. The issue of combining rule-based and data-driven approaches has received less attention, with the exception of a few (Wang et al., 2000; Rayner and Hockey, 2003; Wang and Acero, 2003). In a recent paper (Rayner and Hockey, 2003), the authors address this issue by employing a decision-list-based speech understanding system as a means of progressing from rule-based models to data-driven models when data becomes available. The decision-list-based understanding system also provides a method for robust understanding. In contrast, the approach presented in this paper can be used on lattices of speech and gestures to produce a lattice of meaning representations. 7 Conclusion In this paper, we have addressed how to rapidly prototype multimodal conversational"
N04-1005,H92-1060,0,0.0211661,"ach relies on local information and is more conducive for identifying the simple predicates in MATCH. Second, the patternmatching approach uses the entire grammar as a model for matching while the classification approach is trained on the training data which is significantly smaller when compared to the number of examples encoded in the grammar. 6 Discussion Although we are not aware of any attempts to address the issue of robust understanding in the context of multimodal systems, this issue has been of great interest in the context of speech-only conversational systems (Dowding et al., 1993; Seneff, 1992; Allen et al., 2000; Lavie, 1996). The output of the recognizer in these systems usually is parsed using a handcrafted grammar that assigns a meaning representation suited for the downstream dialog component. The coverage problems of the grammar and parsing of extra-grammatical utterances is typically addressed by retrieving fragments from the parse chart and incorporating operations that combine fragments to derive a meaning of the recognized utterance. We have presented an approach that achieves robust multimodal utterance understanding using the edit-distance automaton in a finite-state-ba"
N04-1005,C00-1054,1,0.730882,"he underlying architecture that supports MATCH consists of a series of re-usable components which communicate over sockets through a facilitator (MCUBE) (Figure 2). Users interact with the system through a Multimodal User Interface Client (MUI). Their speech and ink are processed by speech recognition (Sharp et al., 1997) (ASR) and handwriting/gesture recognition (GESTURE, HW RECO) components respectively. These recognition processes result in lattices of potential words and gestures. These are then combined and assigned a meaning representation using a multimodal finite-state device (MMFST) (Johnston and Bangalore, 2000; Johnston et al., 2002b). This provides as output a lattice encoding all of the potential meaning representations assigned to the user inputs. This lattice is flattened to an N-best list and passed to a multimodal dialog manager (MDM) (Johnston et al., 2002b), which re-ranks them in accordance with the current dialogue state. If additional information or confirmation is required, the MDM enters into a short information gathering dialogue with the user. Once a command or query is complete, it is passed to the multimodal generation component (MMGEN), which builds a multimodal score indicating a"
N07-1001,J99-2004,1,0.885517,"tions is accurate prosody detection, the topic of the present work. In this paper, we describe our framework for building an automatic prosody labeler for English. We report results on the Boston University (BU) Radio Speech Corpus (Ostendorf et al., 1995) and Boston Directions Corpus (BDC) (Hirschberg and Nakatani, 1996), two publicly available speech corpora with manual ToBI annotations intended for experiments in automatic prosody labeling. We condition prosody not only on word strings and their parts-of-speech but also on richer syntactic information encapsulated in the form of Supertags (Bangalore and Joshi, 1999). We propose a maximum entropy modeling framework for the syntactic features. We model the acoustic-prosodic stream with two different models, a maximum entropy model and a more traditional hidden markov model (HMM). In an automatic prosody labeling task, one is essentially try1 Proceedings of NAACL HLT 2007, pages 1–8, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics ing to predict the correct prosody label sequence for a given utterance and a maximum entropy model offers an elegant solution to this learning problem. The framework is also robust in the selection of"
N07-1001,J96-1002,0,0.0132466,"k as follows: given a sequence of words wi in a sentence W = {w1 , · · · , wn } and a prosodic label vocabulary (li  L), we need to predict the best prosodic label sequence L∗ = {l1 , l2 , · · · , ln }. We approximate the conditional probability to be within a bounded n-gram context. Thus, L∗ = arg max P (L|W, T, S) L n Y i+k i+k i+k p(li |wi−k , ti−k , si−k ) L∗ = arg max L n Y P (li |Φ) (6) i To estimate the conditional distribution P (li |Φ) we use the general technique of choosing the maximum entropy (maxent) distribution that estimates the average of each feature over the training data (Berger et al., 1996). This can be written in terms of Gibbs distribution parameterized with weights λ, where V is the size of the prosodic label set. Hence, (4) eλli .Φ P (li |Φ) = PV λ .Φ li l=1 e (5) 1 function and content word features were obtained through a look-up table based on POS L ≈ arg max where W = {w1 , · · · , wn } is the word sequence and T = {t1 , · · · , tn }, S = {s1 , · · · , sn } are the corresponding part-of-speech and additional syntactic information sequences. The variable k controls the context. The BU corpus is automatically labeled (and hand-corrected) with part-of-speech (POS) tags. The"
N07-1001,P04-1086,0,0.451491,"Missing"
N07-1001,P96-1038,0,0.253073,"oneous. Speech understanding systems model both the lexical and acoustic features at the output of an ASR to improve natural language understanding. Another source of renewed interest has come from spoken language translation (N¨oth et al., 2000; Ag¨ uero et al., 2006). A prerequisite for all these applications is accurate prosody detection, the topic of the present work. In this paper, we describe our framework for building an automatic prosody labeler for English. We report results on the Boston University (BU) Radio Speech Corpus (Ostendorf et al., 1995) and Boston Directions Corpus (BDC) (Hirschberg and Nakatani, 1996), two publicly available speech corpora with manual ToBI annotations intended for experiments in automatic prosody labeling. We condition prosody not only on word strings and their parts-of-speech but also on richer syntactic information encapsulated in the form of Supertags (Bangalore and Joshi, 1999). We propose a maximum entropy modeling framework for the syntactic features. We model the acoustic-prosodic stream with two different models, a maximum entropy model and a more traditional hidden markov model (HMM). In an automatic prosody labeling task, one is essentially try1 Proceedings of NA"
N07-1001,W99-0619,0,0.0837919,"Missing"
N09-2047,J99-2004,1,0.793606,"A returns n-best parses for arbitrary n; parse trees are associated with probabilities. A packed forest can also be returned. • MICA is freely available2 , easy to install under Linux, and easy to use. (Input is one sentence per line with no special tokenization required.) There is an enormous amount of related work, and we can mention only the most salient, given space constraints. Our parser is very similar to the work of (Shen and Joshi, 2005). They do not employ a supertagging step, and we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2 http://www1.ccls.columbia.edu/˜rambow/mica.html Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse. All of these systems generate dependency"
N09-2047,W07-2213,1,0.853249,"d by a dependency extractor that relies on the TIG structure of the CFG. The Earley-like parser relies on Earley’s algorithm (Earley, 1970). However, several optimizations have been applied, including guiding techniques (Boullier, 2003), extensive static (offline) computations over the grammar, and efficient data structures. Moreover, Earley’s algorithm has been extended so as to handle input DAGs (and not only sequences of forms). A particular effort has been made to handle huge grammars (over 1 million symbol occurrences in the grammar), thanks to advanced dynamic lexicalization techniques (Boullier and Sagot, 2007). The resulting efficiency is satisfying: with standard ambiguous NLP grammars, huge shared parse forest (over 1010 trees) are often generated in a few dozens of milliseconds. Within MICA, the first module that is applied on top of the shared parse forest is S YNTAX’s n-best module. This module adapts and implements the algorithm of (Huang and Chiang, 2005) for efficient n-best trees extraction from a shared parse forest. In practice, and within the current version of MICA, this module is usually used with n = 1, which identifies the optimal tree w.r.t. the probabilistic model embedded in the"
N09-2047,W03-3005,1,0.86506,"et of the symbols are specialized. 4 Parser S YNTAX (Boullier and Deschamp, 1988) is a system used to generate lexical and syntactic analyzers (parsers) (both deterministic and non-deterministic) for all kind of context-free grammars (CFGs) as well as some classes of contextual grammars. It has been under development at INRIA for several decades. S YNTAX handles most classes of deterministic (unambiguous) grammars (LR, LALR, RLR) as well as general context-free grammars. The non-deterministic features include, among others, an Earley-like parser generator used for natural language processing (Boullier, 2003). Like most S YNTAX Earley-like parsers, the architecture of MICA’s PCFG-based parser is the following: • The Earley-like parser proper computes a shared parse forest that represents in a factorized (polynomial) way all possible parse trees according to the underlying (non-probabilistic) CFG that represents the TIG; • Filtering and/or decoration modules are applied on the shared parse forest; in MICA’s case, an nbest module is applied, followed by a dependency extractor that relies on the TIG structure of the CFG. The Earley-like parser relies on Earley’s algorithm (Earley, 1970). However, sev"
N09-2047,P04-1014,0,0.0349589,"are associated with probabilities. A packed forest can also be returned. • MICA is freely available2 , easy to install under Linux, and easy to use. (Input is one sentence per line with no special tokenization required.) There is an enormous amount of related work, and we can mention only the most salient, given space constraints. Our parser is very similar to the work of (Shen and Joshi, 2005). They do not employ a supertagging step, and we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2 http://www1.ccls.columbia.edu/˜rambow/mica.html Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse. All of these systems generate dependency structures which are slightly different from MIC"
N09-2047,P81-1022,0,0.788449,"processing (Boullier, 2003). Like most S YNTAX Earley-like parsers, the architecture of MICA’s PCFG-based parser is the following: • The Earley-like parser proper computes a shared parse forest that represents in a factorized (polynomial) way all possible parse trees according to the underlying (non-probabilistic) CFG that represents the TIG; • Filtering and/or decoration modules are applied on the shared parse forest; in MICA’s case, an nbest module is applied, followed by a dependency extractor that relies on the TIG structure of the CFG. The Earley-like parser relies on Earley’s algorithm (Earley, 1970). However, several optimizations have been applied, including guiding techniques (Boullier, 2003), extensive static (offline) computations over the grammar, and efficient data structures. Moreover, Earley’s algorithm has been extended so as to handle input DAGs (and not only sequences of forms). A particular effort has been made to handle huge grammars (over 1 million symbol occurrences in the grammar), thanks to advanced dynamic lexicalization techniques (Boullier and Sagot, 2007). The resulting efficiency is satisfying: with standard ambiguous NLP grammars, huge shared parse forest (over 101"
N09-2047,W05-1506,0,0.0313205,"een extended so as to handle input DAGs (and not only sequences of forms). A particular effort has been made to handle huge grammars (over 1 million symbol occurrences in the grammar), thanks to advanced dynamic lexicalization techniques (Boullier and Sagot, 2007). The resulting efficiency is satisfying: with standard ambiguous NLP grammars, huge shared parse forest (over 1010 trees) are often generated in a few dozens of milliseconds. Within MICA, the first module that is applied on top of the shared parse forest is S YNTAX’s n-best module. This module adapts and implements the algorithm of (Huang and Chiang, 2005) for efficient n-best trees extraction from a shared parse forest. In practice, and within the current version of MICA, this module is usually used with n = 1, which identifies the optimal tree w.r.t. the probabilistic model embedded in the original PCFG; other values can also be used. Once the n-best trees have been extracted, the dependency extractor module transforms each of these trees into a dependency tree, by exploiting the fact that the CFG used for parsing has been built from a TIG. 5 Evaluation We compare MICA to the MALT parser. Both parsers are trained on sections 02-21 of our depe"
N09-2047,P04-1042,0,0.0251422,"nd we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2 http://www1.ccls.columbia.edu/˜rambow/mica.html Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse. All of these systems generate dependency structures which are slightly different from MICA’s, so that direct comparison is difficult. For comparison purposes, we therefore use the MALT parser generator (Nivre et al., 2004), which allows us to train a dependency parser on our own dependency structures. MALT has been among the top performers in the CoNLL dependency parsing competitions. 2 Supertags and Supertagging Supertags are elementary trees of a lexicalized tree grammar such as a Tree-Adjoining Grammar (TAG) (Joshi, 1987)"
N09-2047,C94-1079,0,0.0973378,"rk of (Shen and Joshi, 2005). They do not employ a supertagging step, and we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2 http://www1.ccls.columbia.edu/˜rambow/mica.html Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse. All of these systems generate dependency structures which are slightly different from MICA’s, so that direct comparison is difficult. For comparison purposes, we therefore use the MALT parser generator (Nivre et al., 2004), which allows us to train a dependency parser on our own dependency structures. MALT has been among the top performers in the CoNLL dependency parsing competitions. 2 Supertags and Supertagging Supertags are elementary tree"
N09-2047,W04-2407,0,0.102365,"Missing"
N09-2047,J94-1004,0,0.118342,"es anchored by Xl from the symbol Xl∗ . No adjunction, the first adjunction, and the second adjunction are modeled explicitly in the grammar and the associated probabilistic model, while the third and all subsequent adjunctions are modeled together. This conversion method is basically the same as that presented in (Schabes and Waters, 1995), except that our PCFG models multiple adjunctions at the same node by positions (a concern Schabes and Waters (1995) do not share, of course). Our PCFG construction differs from that of Hwa (2001) in that she does not allow multiple adjunction at one node (Schabes and Shieber, 1994) (which we do since we are interested in the derivation structure as a representation of linguistic dependency). For more information about the positional model of adjunction and a discussion of an alternate model, the “bigram model”, see (Nasr and Rambow, 2006). Tree tdi from Section 2 gives rise to the following rule (where tdi and tCO are terminal symbols and the rest are nonterminals): S → S∗l NP VP∗l Vl∗ tdi Vr∗ NP PP∗l P∗l tCO P∗r NP PP∗r VP∗r S∗r The probabilities of the PCFG rules are estimated using maximum likelihood. The probabilistic model refers only to supertag names, not to word"
N09-2047,H05-1102,0,0.0299565,"which derives the syntactic structure from the n-best chosen supertags. Only the supertagger uses lexical information, the parser only sees the supertag hypotheses. • MICA returns n-best parses for arbitrary n; parse trees are associated with probabilities. A packed forest can also be returned. • MICA is freely available2 , easy to install under Linux, and easy to use. (Input is one sentence per line with no special tokenization required.) There is an enormous amount of related work, and we can mention only the most salient, given space constraints. Our parser is very similar to the work of (Shen and Joshi, 2005). They do not employ a supertagging step, and we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2 http://www1.ccls.columbia.edu/˜rambow/mica.html Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a dec"
N09-2047,W04-0307,0,0.115845,"Missing"
N10-1007,P01-1005,0,0.532599,"notated by human experts with topic labels. In contrast, to train a dynamic/static classifier, we experimented with the following three different techniques. Baseline: We treat questions as dynamic if they contain temporal indexicals, e.g. today, now, this week, two summers ago, currently, recently, which were based on the TimeML corpus. We also included spatial indexicals such as here, and other substrings such as cost of and how much is. A question is considered static if it does not contain any such words/phrases. Self-training with bagging: The general selftraining with bagging algorithm (Banko and Brill, 2001) is presented in Table 6 and illustrated in Figure 7(a). The benefit of self-training is that we can build a better classifier than that built from the small seed corpus by simply adding in the large unlabeled corpus without requiring hand-labeling. 1. Create k bags of data, each of size |L|, by sampling with replacement from labeled set L. 2. Train k classifiers; one classifier on each of k bags. 3. Each classifier predicts labels of the unlabeled set. 4. The N labeled instances that j of k classifiers agree on with the highest average confidence is added to the labeled set L, to produce a ne"
N10-1007,P02-1040,0,0.0828681,"with the following metrics. 1. TF-IDF metric: The user input query and the document (in our case, questions in the repository) are represented as bag-of-n-grams (aka terms). The term weights are computed using a combination of term frequency (tf ) and inverse document frequency (idf ) (Robertson, 2004). If Q = q1 , q2 , . . . , qn is a user query, then the 57 2. String Comparison Metrics: Since the length of the user query and the query to be retrieved are similar in length, we use string comparison methods such as Levenshtein edit distance (Levenshtein, 1966) and n-gram overlap (BLEU-score) (Papineni et al., 2002) as similarity metrics. We compare the search effectiveness of these similarity metrics in Section 5.3. 5 Tightly coupling ASR and Search Most of the speech-driven search systems use the 1-best output from the ASR as the query for the search component. Given that ASR 1-best output is likely to be erroneous, this serialization of the ASR and search components might result in suboptimal search accuracy. A lattice representation of the ASR output, in particular, a word-confusion network (WCN) transformation of the lattice, compactly encodes the n-best hypothesis with the flexibility of pruning al"
N10-1007,P04-1073,0,0.252202,"ew York?”. White Christmas is a seasonal play that plays in New York every year for a few weeks in December and January, but it does not necessarily at the same theater every year. So, depending when this question is asked, the answer will be different. Interest in temporal analysis for questionanswering has been growing since the late 1990’s. Early work on temporal expressions identification using a tagger led to the development of TimeML (Pustejovsky et al., 2001), a markup language for annotating temporal expressions and events in text. Other examples include QA-byDossier with Constraints (Prager et al., 2004), a method of improving QA accuracy by asking auxiliary questions related to the original question in order to temporally verify and restrict the original answer. (Moldovan et al., 2005) detect and represent temporally related events in natural language using logical form representation. (Saquete et al., 2009) use the temporal relations in a question to decompose it into simpler questions, the answers of which are recomposed to produce the answers to the original question. 6.1 Dynamic/Static Classification We automatically classify questions as dynamic and static questions. Answers to static q"
N10-1007,H89-1033,0,0.222848,"d to implement the system in Section 4. In Section 5, we discuss and evaluate our approach to tight coupling of speech recognition and search components. In Section 6, we present bootstrap techniques to distinguish dynamic questions from static questions, and evaluate the efficacy of these techniques on a test corpus. We conclude in Section 7. 2 Related Work Early question-answering (QA) systems, such as Baseball (Green et al., 1961) and Lunar (Woods, 1973) were carefully hand-crafted to answer questions in a limited domain, similar to the QA components of ELIZA (Weizenbaum, 1966) and SHRDLU (Winograd, 1972). However, there has been a resurgence of QA systems following the TREC conferences with an emphasis on answering factoid questions. This work on text-based questionanswering which is comprehensively summarized 55 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 55–63, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics in (Maybury, 2004), range widely in terms of linguistic sophistication. At one end of the spectrum, There are linguistically motivated systems (Katz, 1997; Waldinger et al., 2004) that analy"
N10-1007,W06-3005,0,0.0188124,"guistics in (Maybury, 2004), range widely in terms of linguistic sophistication. At one end of the spectrum, There are linguistically motivated systems (Katz, 1997; Waldinger et al., 2004) that analyze the user’s question and attempt to synthesize a coherent answer by aggregating the relevant facts. At the other end of the spectrum, there are data intensive systems (Dumais et al., 2002) that attempt to use the redundancy of the web to arrive at an answer for factoid style questions. There are also variants of such QA techniques that involve an interaction and use context to resolve ambiguity (Yang et al., 2006). In contrast to these approaches, our method matches the user’s query against the questions in a large corpus of question-answer pairs and retrieves the associated answer. In the information retrieval community, QA systems attempt to retrieve precise segments of a document instead of the entire document. In (Tomuro and Lytinen, 2004), the authors match the user’s query against a frequently-asked-questions (FAQ) database and select the answer whose question matches most closely to the user’s question. An extension of this idea is explored in (Xue et al., 2008; Jeon et al., 2005), where the aut"
N12-1048,C96-1070,0,0.568773,"se individual components to achieve S2S translation of acceptable quality. Prior work on S2S translation has primarily focused on providing either one-way or two-way translation on a single device (Waibel et al., 2003; Zhou In a conventional telephone conversation between two speakers of the same language, the interaction is real-time and the speakers process the information stream incrementally. Similarly, cross-lingual dialog between two remote participants will greatly benefit through incremental translation. While incremental decoding for text translation has been addressed previously in (Furuse and Iida, 1996; Sankaran et al., 2010), we address the problem in a speech-to-speech translation setting for enabling real-time cross-lingual dialog. We address the problem of incrementality in a novel session initiation protocol (SIP) based S2S translation system that enables two people to interact and engage in crosslingual dialog over a telephone (mobile phone or landline). Our system performs incremental speech recognition and translation, allowing for low latency interaction that provides an ideal setting for remote dialog aimed at accomplishing a task. We present previous work in this area in Section"
N12-1048,P07-2045,0,0.0247741,"elopment set using perplexity metric. The development set was 500 sentences selected randomly from the IWSLT corpus (Paul, 2006). The training vocabulary size for English acoustic model is 140k and for the language model is 300k. For the Spanish model, the training vocabulary size is 92k, while for testing, the language model includes 370k distinct words. In our experiments, the decoding and LM vocabularies were the same. 4.2 Data statistics # Sentences # Words Vocabulary Machine Translation The phrase-based translation experiments reported in this work was performed using the Moses2 toolkit (Koehn et al., 2007) for statistical machine translation. Training the translation model starts from the parallel sentences from which we learn word alignments by using GIZA++ toolkit (Och and Ney, 2003). The bidirectional word alignments obtained using GIZA++ were consolidated by using the grow-diag-final option in Moses. Subsequently, we learn phrases (maximum length of 7) from the consolidated word alignments. A lexicalized reordering model (msd-bidirectional-fe option in Moses) was used for reordering the phrases in addition to the standard distance based reordering (distortion-limit of 6). The language model"
N12-1048,2005.mtsummit-papers.11,0,0.00795009,"Missing"
N12-1048,lavie-etal-2002-nespole,0,0.06451,"tics: Human Language Technologies, pages 437–445, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics framework for real-time translation in Section 5. In Section 6, we describe the basic call flow of our system following which we present dialog experiments performed using our framework in Section 8. Finally, we conclude in Section 9 along with directions for future work. 2 Previous Work Most previous work on speech-to-speech translation systems has focused on a single device model, i.e., the user interface for translation is on one device (Waibel et al., 1991; Metze et al., 2002; Zhou et al., 2003; Waibel et al., 2003). The device typically supports multiple source-target language pairs. A user typically chooses the directionality of translation and a toggle feature is used to switch the directionality. However, this requires physical presence of the two conversants in one location. On the other hand, text chat between users over cell phones has become increasingly popular in the last decade. While the language used in the interaction is typically monolingual, there have been attempts to use statistical machine translation to enable cross-lingual text communication ("
N12-1048,J03-1002,0,0.00297197,"Missing"
N12-1048,P02-1040,0,0.104621,"Missing"
N12-1048,W10-1733,0,0.387415,"s to achieve S2S translation of acceptable quality. Prior work on S2S translation has primarily focused on providing either one-way or two-way translation on a single device (Waibel et al., 2003; Zhou In a conventional telephone conversation between two speakers of the same language, the interaction is real-time and the speakers process the information stream incrementally. Similarly, cross-lingual dialog between two remote participants will greatly benefit through incremental translation. While incremental decoding for text translation has been addressed previously in (Furuse and Iida, 1996; Sankaran et al., 2010), we address the problem in a speech-to-speech translation setting for enabling real-time cross-lingual dialog. We address the problem of incrementality in a novel session initiation protocol (SIP) based S2S translation system that enables two people to interact and engage in crosslingual dialog over a telephone (mobile phone or landline). Our system performs incremental speech recognition and translation, allowing for low latency interaction that provides an ideal setting for remote dialog aimed at accomplishing a task. We present previous work in this area in Section 2 and introduce the prob"
N12-1048,2006.amta-papers.25,0,0.070425,"Missing"
N12-1048,steinberger-etal-2006-jrc,0,0.0202173,"Missing"
N12-1048,tiedemann-nygaard-2004-opus,0,0.11249,"Missing"
N12-1048,1991.mtsummit-papers.18,0,0.511632,"Computational Linguistics: Human Language Technologies, pages 437–445, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics framework for real-time translation in Section 5. In Section 6, we describe the basic call flow of our system following which we present dialog experiments performed using our framework in Section 8. Finally, we conclude in Section 9 along with directions for future work. 2 Previous Work Most previous work on speech-to-speech translation systems has focused on a single device model, i.e., the user interface for translation is on one device (Waibel et al., 1991; Metze et al., 2002; Zhou et al., 2003; Waibel et al., 2003). The device typically supports multiple source-target language pairs. A user typically chooses the directionality of translation and a toggle feature is used to switch the directionality. However, this requires physical presence of the two conversants in one location. On the other hand, text chat between users over cell phones has become increasingly popular in the last decade. While the language used in the interaction is typically monolingual, there have been attempts to use statistical machine translation to enable cross-lingual"
N12-1048,N03-4015,0,0.0581937,"ncremental approach. 1 Introduction In recent years, speech-to-speech translation (S2S) technology has played an increasingly important role in narrowing the language barrier in crosslingual interpersonal communication. The improvements in automatic speech recognition (ASR), statistical machine translation (MT), and, text-to-speech synthesis (TTS) technology has facilitated the serial binding of these individual components to achieve S2S translation of acceptable quality. Prior work on S2S translation has primarily focused on providing either one-way or two-way translation on a single device (Waibel et al., 2003; Zhou In a conventional telephone conversation between two speakers of the same language, the interaction is real-time and the speakers process the information stream incrementally. Similarly, cross-lingual dialog between two remote participants will greatly benefit through incremental translation. While incremental decoding for text translation has been addressed previously in (Furuse and Iida, 1996; Sankaran et al., 2010), we address the problem in a speech-to-speech translation setting for enabling real-time cross-lingual dialog. We address the problem of incrementality in a novel session"
N12-1048,2006.iwslt-evaluation.1,0,\N,Missing
N13-1023,N12-1048,1,0.845391,"Missing"
N13-1023,2012.eamt-1.60,0,0.0100137,"of public talks from several speakers covering a variety of topics. Over the past couple of years, the International Workshop on Spoken Language Translation (IWSLT) has been conducting the evaluation of speech translation on TED talks for English-French. We leverage the IWSLT TED campaign by using identical development (dev2010) and test data (tst2010). However, English-Spanish is our target language pair as our internal projects are cater mostly to this pair. As a result, we created parallel text for English-Spanish based on the reference English segments released as part of the evaluation (Cettolo et al., 2012). We also harvested the audio data from the TED website for building an acoustic model. A total of 1308 talks in English were downloaded, out of which we used 1119 talks recorded prior to December 2011. We split the stereo audio file and duplicated the data to account for any variations in the channels. The data for the language models was also restricted to that permitted in the IWSLT 2011 evaluation. The parallel text for building the EnglishSpanish translation model was obtained from several corpora: Europarl (Koehn, 2005), JRC-Acquis corpus (Steinberger et al., 2006), Opensubtitle corpus ("
N13-1023,2011.iwslt-evaluation.1,0,0.0338831,"Missing"
N13-1023,E09-1040,0,0.184143,"Missing"
N13-1023,P07-2045,0,0.00427151,"ord to penalize short hypotheses. The decoding process consists of composing all possible segmentations of an input sentence with the phrase table FST and language model, followed by searching for the best path. Our FST-based translation is the equivalent of phrase-based translation in Moses without reordering. We present results using the independent chunk-wise strategy and chunk-wise translation conditioned on history in Table 3. The chunk-wise translation conditioned on history was performed using the continue-partialtranslation option in Moses. Translation Model We used the Moses toolkit (Koehn et al., 2007) for performing statistical machine translation. Minimum error rate training (MERT) was performed on the development set (dev2010) to optimize the feature weights of the log-linear model used in translation. During decoding, the unknown words were preserved in the hypotheses. The data used to train the model is summarized in Table 1. 3 We used the standard NIST scoring package as we did not have access to the IWSLT evaluation server that may normalize and score differently 233 The output of ASR for talks is a long string of words with no punctuation, capitalization or segmentation markers. In"
N13-1023,2005.mtsummit-papers.11,0,0.00247436,"erence English segments released as part of the evaluation (Cettolo et al., 2012). We also harvested the audio data from the TED website for building an acoustic model. A total of 1308 talks in English were downloaded, out of which we used 1119 talks recorded prior to December 2011. We split the stereo audio file and duplicated the data to account for any variations in the channels. The data for the language models was also restricted to that permitted in the IWSLT 2011 evaluation. The parallel text for building the EnglishSpanish translation model was obtained from several corpora: Europarl (Koehn, 2005), JRC-Acquis corpus (Steinberger et al., 2006), Opensubtitle corpus (Tiedemann and Lars Nygaard, 2004), Web crawling (Rangarajan Sridhar et al., 2011) as well as human translation of proprietary data. Table 1 summarizes the data used in building the models. It is important to note that the IWSLT evaluation on TED talks is completely offline. In this work, we perform the first investigation into the real-time translation of these talks. 232 5.1 Acoustic and Language Model We use the AT&T WATSONSM speech recognizer (Goffin et al., 2004). The speech recognition component consisted of a three-pass"
N13-1023,J93-2004,0,0.0453729,"Missing"
N13-1023,2005.iwslt-1.19,0,0.0476115,"artial output before sending it to MT. The plot shows the BLEU scores as a function of ASR timeouts used to generate the partial hypotheses. Figure 1 also shows the average latency involved in incremental speech translation. 7 Discussion The BLEU scores for the segmentation strategies over ASR hypotheses was computed at the talk level. Since the ASR hypotheses do not align with the reference source text, it is not feasible to evaluate the translation performance using the gold reference. While other studies have used an approximate edit distance algorithm for resegmentation of the hypotheses (Matusov et al., 2005), we simply concatenate all the segments and perform the evaluation at the talk level. The hold segmentation strategy yields the poorest translation performance. The significant drop in BLEU score can be attributed to relatively short segments (2-4 words) that was generated by the model. The scheme oversegments the text and since the translation and language models are trained on sentence like chunks, the performance is poor. For example, the input text the sea should be translated as el mar, but instead the hold segmenter chunks it as the·sea which MT’s chunk translation renders as el·el mar."
N13-1023,J03-1002,0,0.0129501,"Missing"
N13-1023,steinberger-etal-2006-jrc,0,0.0129598,"as part of the evaluation (Cettolo et al., 2012). We also harvested the audio data from the TED website for building an acoustic model. A total of 1308 talks in English were downloaded, out of which we used 1119 talks recorded prior to December 2011. We split the stereo audio file and duplicated the data to account for any variations in the channels. The data for the language models was also restricted to that permitted in the IWSLT 2011 evaluation. The parallel text for building the EnglishSpanish translation model was obtained from several corpora: Europarl (Koehn, 2005), JRC-Acquis corpus (Steinberger et al., 2006), Opensubtitle corpus (Tiedemann and Lars Nygaard, 2004), Web crawling (Rangarajan Sridhar et al., 2011) as well as human translation of proprietary data. Table 1 summarizes the data used in building the models. It is important to note that the IWSLT evaluation on TED talks is completely offline. In this work, we perform the first investigation into the real-time translation of these talks. 232 5.1 Acoustic and Language Model We use the AT&T WATSONSM speech recognizer (Goffin et al., 2004). The speech recognition component consisted of a three-pass decoding approach utilizing two acoustic mode"
N13-1023,tiedemann-nygaard-2004-opus,0,0.13568,"Missing"
N13-1023,2005.mtsummit-papers.34,0,0.0468457,"to the sentence units used in training the language and translation models. We propose several nonlinguistic and linguistic segmentation strategies for the segmentation of text (reference or ASR hypotheses) for machine translation. We address the problem of latency in real-time translation as a function of the segmentation strategy; i.e., we ask the question “what is the segmentation strategy that maximizes the number of segments while still maximizing translation accuracy?”. 2 Related Work Speech translation of European Parliamentary speeches has been addressed as part of the TCSTAR project (Vilar et al., 2005; F¨ugen et al., 2006). The project focused primarily on offline translation of speeches. Simultaneous translation of lectures and speeches has been addressed in (Hamon et al., 2009; F¨ugen et al., 2007). However, the work focused on a single speaker in a limited domain. Offline speech translation of TED1 talks has been addressed through the IWSLT 2011 and 2012 evaluation tracks. The talks are from a variety of speakers with varying dialects and cover a range of topics. The study presented in this work is the first effort on real-time speech translation of TED talks. In comparison with previou"
N13-1023,federico-etal-2012-iwslt,0,\N,Missing
P00-1059,J99-2004,1,\N,Missing
P00-1059,C00-1007,1,\N,Missing
P00-1059,P98-1116,0,\N,Missing
P00-1059,C98-1112,0,\N,Missing
P00-1059,J94-4004,0,\N,Missing
P00-1059,W00-1401,1,\N,Missing
P02-1048,C00-1054,1,0.503361,"n the user has hit the click-to-speak button, when a speech result arrives, and whether or not the user is inking on the display. When a speech lattice arrives, if inking is in progress MMFST waits for the ink meaning lattice, otherwise it applies a short timeout (1 sec.) and treats the speech as unimodal. When an ink meaning lattice arrives, if the user has tapped click-to-speak MMFST waits for the speech lattice to arrive, otherwise it applies a short timeout (1 sec.) and treats the ink as unimodal. MMFST uses the finite-state approach to multimodal integration and understanding proposed by Johnston and Bangalore (2000). Possibilities for multimodal integration and understanding are captured in a three tape device in which the first tape represents the speech stream (words), the second the ink stream (gesture symbols) and the third their combined meaning (meaning symbols). In essence, this device takes the speech and ink meaning lattices as inputs, consumes them using the first two tapes, and writes out a multimodal meaning lattice using the third tape. The three tape finite-state device is simulated using two transducers: G:W which is used to align speech and ink and G W:M which takes a composite alphabet o"
P02-1048,C00-1053,1,0.772153,"alues such as area, point, line, arrow. MEANING indicates the meaning of that form; for example an area can be either a loc(ation) or a sel(ection). NUMBER and TYPE indicate the number of entities in a selection (1,2,3, many) and their type (rest(aurant), theatre). SEM is a place holder for the specific content of the gesture, such as the points that make up an area or the identifiers of objects in a selection. When multiple selection gestures are present an aggregation technique (Johnston and Bangalore, 2001) is employed to overcome the problems with deictic plurals and numerals described in Johnston (2000). Aggregation augments the ink meaning lattice with aggregate gestures that result from combining adjacent selection gestures. This allows a deictic expression like these three restaurants to combine with two area gestures, one which selects one restaurant and the other two, as long as their sum is three. For example, if the user makes two area gestures, one around a single restaurant and the other around two restaurants (Figure 3), the resulting ink meaning lattice will be as in Figure 8. The first gesture (node numbers 0-7) is either a reference to a location (loc.) (0-3,7) or a reference to"
P02-1048,1997.iwpt-1.19,0,0.0197684,"e the corresponding I symbol is the specific interpretation. After multimodal integration a projection G:M is taken from the result G W:M machine and composed with the original I:G in order to reincorporate the specific contents that were left out of the finite-state process (I:G o G:M = I:M). The multimodal finite-state transducers used at runtime are compiled from a declarative multimodal context-free grammar which captures the structure Figure 8: Ink Meaning Lattice and interpretation of multimodal and unimodal commands, approximated where necessary using standard approximation techniques (Nederhof, 1997). This grammar captures not just multimodal integration patterns but also the parsing of speech and gesture, and the assignment of meaning. In Figure 9 we present a small simplified fragment capable of handling MATCH commands such as phone numbers for these three restaurants. A multimodal CFG differs from a normal CFG in that the terminals are triples: W:G:M, where W is the speech stream (words), G the ink stream (gesture symbols) and M the meaning stream (meaning symbols). An XML representation for meaning is used to facilate parsing and logging by other system components. The meaning tape sy"
P02-1048,P99-1024,1,0.628273,"ir specific contents in I:G (I:G o G:M = I:M). The meaning read off I:M is <cmd> <phone> <restaurant> [id1,id2,id3] </restaurant> </phone> </cmd>. This is passed to the multimodal dialog manager (MDM) and from there to the Multimodal UI resulting in a display like Figure 4 with coordinated TTS output. Since the speech input is a lattice and there is also potential for ambiguity in the multimodal grammar, the output from MMFST to MDM is an N-best list of potential multimodal interpretations. Multimodal Dialog Manager (MDM) The MDM is based on previous work on speech-act based models of dialog (Stent et al., 1999; Rich and Sidner, 1998). It uses a Java-based toolkit for writing dialog managers that is similar in philosophy to TrindiKit (Larsson et al., 1999). It includes several rule-based S CMD DEICTICNP DDETPL RESTPL NUM !! ! !!! < > < > eps:eps: cmd CMD eps:eps: /cmd phone:eps: phone numbers:eps:eps for:eps:eps DEICTICNP eps:eps: /phone DDETPL eps:area:eps eps:selection:eps NUM RESTPL eps:eps: restaurant eps:SEM:SEM eps:eps: /restaurant these:G:eps restaurants:restaurant:eps three:3:eps < < > > < < > > Figure 9: Multimodal grammar fragment processes that operate on a shared state. The state include"
P02-1048,W02-2110,0,0.226817,"e restaurants in Figure 3 and writes phone, the system responds with a graphical callout on the display, synchronized with a text-to-speech (TTS) prompt of the phone number, for each restaurant in turn (Figure 4). Figure 3: Two area gestures Figure 4: Phone query callouts The system also provides subway directions. If the user says How do I get to this place? and circles one Figure 5: Multimodal subway route User-tailored generation MATCH can also provide a user-tailored summary, comparison, or recommendation for an arbitrary set of restaurants, using a quantitative model of user preferences (Walker et al., 2002). The system will only discuss restaurants that rank highly according to the user’s dining preferences, and will only describe attributes of those restaurants the user considers important. This permits concise, targeted system responses. For example, the user could say compare these restaurants and circle a large set of restaurants (Figure 6). If the user considers inexpensiveness and food quality to be the most important attributes of a restaurant, the system response might be: Compare-A: Among the selected restaurants, the following offer exceptional overall value. Uguale’s price is 33 dolla"
P02-1048,P00-1020,0,\N,Missing
P04-3021,J99-2004,1,0.739986,"ext of the sentence. named-entities. Thus classification approaches need to be extended to be applicable on weighted packed representations of ambiguous input represented as a weighted lattice. The research direction we adopt here is to compile the model of a classifier into a weighted finite-state transducer (WFST) so that it can compose with the input lattice. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999), parsing (Roche, 1999; Oflazer, 1999) and machine translation (Vilar et al., 1999; Bangalore and Riccardi, 2000). Finitestate models are attractive mechanisms for language processing since they (a) provide an efficient data structure for representing weighted ambiguous hypotheses (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech and language processing.2 In this paper, we describe the compilation process for a particular classifier model into an WFST and validate th"
P04-3021,W00-0508,1,0.857652,"ighted packed representations of ambiguous input represented as a weighted lattice. The research direction we adopt here is to compile the model of a classifier into a weighted finite-state transducer (WFST) so that it can compose with the input lattice. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999), parsing (Roche, 1999; Oflazer, 1999) and machine translation (Vilar et al., 1999; Bangalore and Riccardi, 2000). Finitestate models are attractive mechanisms for language processing since they (a) provide an efficient data structure for representing weighted ambiguous hypotheses (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech and language processing.2 In this paper, we describe the compilation process for a particular classifier model into an WFST and validate the accuracy of the compilation process on a one-best input in a call-routing task. We view this as a first step to"
P04-3021,J94-3001,0,0.278739,"h a label that represents the syntactic information of the word given the context of the sentence. named-entities. Thus classification approaches need to be extended to be applicable on weighted packed representations of ambiguous input represented as a weighted lattice. The research direction we adopt here is to compile the model of a classifier into a weighted finite-state transducer (WFST) so that it can compose with the input lattice. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999), parsing (Roche, 1999; Oflazer, 1999) and machine translation (Vilar et al., 1999; Bangalore and Riccardi, 2000). Finitestate models are attractive mechanisms for language processing since they (a) provide an efficient data structure for representing weighted ambiguous hypotheses (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech and language processing.2 In this paper, we describe the"
P04-3021,P84-1038,0,0.0368447,"ctic information of the word given the context of the sentence. named-entities. Thus classification approaches need to be extended to be applicable on weighted packed representations of ambiguous input represented as a weighted lattice. The research direction we adopt here is to compile the model of a classifier into a weighted finite-state transducer (WFST) so that it can compose with the input lattice. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999), parsing (Roche, 1999; Oflazer, 1999) and machine translation (Vilar et al., 1999; Bangalore and Riccardi, 2000). Finitestate models are attractive mechanisms for language processing since they (a) provide an efficient data structure for representing weighted ambiguous hypotheses (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech and language processing.2 In this paper, we describe the compilation process for a parti"
P04-3021,P96-1031,0,0.0245822,"gs or words (not word graphs). By compiling these rule sets into WFSTs, we intend to extend their applicability to packed representations of ambiguous input such as word graphs. 4 Compilation We note that the weak learners selected at the end of the training process can be partitioned into one of three types based on the features that the learners test. u AtX u AT u AU : test features of the word : test features of the left context : test features of the right context We use the representation of context-dependent rewrite rules (Johnson, 1972; Kaplan and Kay, 1994) and their weighted version (Mohri and Sproat, 1996) to represent these weak learners. The (weighted) context-dependent rewrite rules have the general form Swv@xyaz { (6) where S , x , z and { are regular expressions on the alphabet of the rules. The interpretation of these rules are as follows: Rewrite S by x when it is preceded by z and followed by { . Furthermore, x can be extended to a rational power series which are weighted regular expressions where the weights encode preferences over the paths in x (Mohri and Sproat, 1996). Each weak learner can then be viewed as a set of weighted rewrite rules mapping the input word into each member =|"
P04-3021,P99-1033,0,0.0229604,"cation approaches need to be extended to be applicable on weighted packed representations of ambiguous input represented as a weighted lattice. The research direction we adopt here is to compile the model of a classifier into a weighted finite-state transducer (WFST) so that it can compose with the input lattice. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999), parsing (Roche, 1999; Oflazer, 1999) and machine translation (Vilar et al., 1999; Bangalore and Riccardi, 2000). Finitestate models are attractive mechanisms for language processing since they (a) provide an efficient data structure for representing weighted ambiguous hypotheses (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech and language processing.2 In this paper, we describe the compilation process for a particular classifier model into an WFST and validate the accuracy of the compilation process"
P04-3021,P96-1029,0,0.148731,"ure (Breiman et al., 1984; Freund and Schapire, 1996; Roth, 1998; Lafferty et al., 2001; McCallum et al., 2000) which take this approach and are well equipped to handle large number of features. The general framework for these approaches is to learn a model from pairs of associations of the form ( *<;>=?* ) where  * is a feature representation of  and =* ( @ ) is one of the members of the tag set. Although these approaches have been more effective than HMMs, there have not been many attempts to represent these models as a WFST, with the exception of the work on compiling decision trees (Sproat and Riley, 1996). In this paper, we consider the boosting (Freund and Schapire, 1996) approach (which outperforms decision trees) to Equation 1 and present a technique for compiling the classifier model into a WFST. 3 Boostexter Boostexter is a machine learning tool which is based on the boosting family of algorithms first proposed in (Freund and Schapire, 1996). The basic idea of boosting is to build a highly accurate classifier by combining many “weak” or “simple” base learner, each one of which may only be moderately accurate. CB G D A weak learner or a rule A is a triple ;EF D ;  , which B tests a predi"
P04-3033,N04-1005,1,0.656649,"bile device a close-talking headset or on-device microphone can be used, we found that a single microphone had very poor performance on the kiosk. Users stand in different positions with respect to the display and there may be more than one person standing in front. To overcome this problem we mounted an array microphone above the touchscreen which tracks the location of the talker. Robust Recognition and Understanding is particularly important for kiosks since they have so many first-time users. We utilize the techniques for robust language modelling and multimodal understanding described in Bangalore and Johnston (2004). Social Interaction For mobile multimodal interfaces, even those with graphical embodiment, we found there to be little or no need to support social greetings and small talk. However, for a public kiosk which different unknown users will approach those capabilities are important. We added basic support for social interaction to the language understanding and dialog components. The system is able to respond to inputs such as Hello, How are you?, Would you like to join us for lunch? and so on. Context-sensitive GUI Compared to mobile systems, on palmtops, phones, and tablets, kiosks can offer m"
P04-3033,C00-1054,1,0.849677,"s in tourist offices and museums, and more recently, automated check-out in retail stores. The majority of these systems provide a rigid structured graphical interface and user input by only touch or keypad, and as a result can only support a small number of simple tasks. As automated kiosks become more commonplace and have to support more complex tasks for a broader community of users, they will need to provide a more flexible and compelling user interface. One major motivation for developing multimodal interfaces for mobile devices is the lack of a keyboard or mouse (Oviatt and Cohen, 2000; Johnston and Bangalore, 2000). This limitation is also true of many different kinds of public information kiosks where security, hygiene, or space concerns make a physical keyboard or mouse impractical. Also, mobile users interacting with kiosks are often encumbered with briefcases, phones, or other equipment, leaving only one hand free for interaction. Kiosks often provide a touchscreen for input, opening up the possibility of an onscreen keyboard, but these can be awkward to use and occupy a considerable amount of screen real estate, generally leading to a more moded and cumbersome graphical interface. Srinivas Bangalor"
P04-3033,P02-1048,1,\N,Missing
P06-1026,J99-2004,1,0.313683,"ompare two models for recovering the subtask structure – a chunk-based model and a parse-based model. In the chunk-based model, we recover the precedence relations (sequence) of the subtasks but not dominance relations (subtask structure) among the subtasks. Figure 3 shows a sample output from the chunk model. In the parse model, we recover the complete task structure from the sequence of utterances as shown in Figure 2. Here, we describe our two models. We present our experiments on subtask segmentation and labeling in Section 6.4. We automatically annotate a user’s utterance with supertags (Bangalore and Joshi, 1999). Supertags encapsulate predicate-argument information in a local structure. They are composed with each other using the substitution and adjunction operations of Tree-Adjoining Grammars (Joshi, 1987) to derive a dependency analysis of an utterance and its predicate-argument structure. 4.3 Contact Info Dialog Act Tagging We use a domain-specific dialog act tagging scheme based on an adapted version of DAMSL (Core, 1998). The DAMSL scheme is quite comprehensive, but as others have also found (Jurafsky et al., 1998), the multi-dimensionality of the scheme makes the building of models from DAMSL-"
P06-1026,P92-1008,0,0.0582372,"Missing"
P06-1026,J96-1002,0,0.0109453,"Missing"
P06-1026,W03-2123,0,0.028735,"Missing"
P06-1026,J98-4001,0,0.0163483,"berry, 2001; Bohus and Rudnicky, 2003)) and information state-based approaches (e.g. (Larsson et al., 1999; Bos et al., 2003; Lemon and Gruenstein, 2004)). In recent years, there has been considerable research on how to automatically learn models of both types from data. Researchers who treat dialog as a sequence of information states have used reinforcement learning and/or Markov decision processes to build stochastic models for dialog management 4 Structural Analysis of a Dialog We consider a task-oriented dialog to be the result of incremental creation of a shared plan by the participants (Lochbaum, 1998). The shared plan is represented as a single tree that encapsulates the task structure (dominance and precedence relations among tasks), dialog act structure (sequences of dialog acts), and linguistic structure of utterances (inter-clausal relations and predicateargument relations within a clause), as illustrated in Figure 1. As the dialog proceeds, an utterance from a participant is accommodated into the tree in an incremental manner, much like an incremental syntactic parser accommodates the next word into a partial parse tree (Alexandersson and Reithinger, 1997). With this model, we can tig"
P06-1026,N01-1016,0,0.00960766,"tion, order-item, related-offers, summary. Subtasks can be nested; the nesting structure can be as deep as five levels. Most often the nesting is at the left or right frontier of the subtask tree. DialogAct,Pred−Args Utterance Clause Figure 1: Structural analysis of a dialog 4.1 Order Placement Utterance Segmentation The task of ”cleaning up” spoken language utterances by detecting and removing speech repairs and dysfluencies and identifying sentence boundaries has been a focus of spoken language parsing research for several years (e.g. (Bear et al., 1992; Seneff, 1992; Shriberg et al., 2000; Charniak and Johnson, 2001)). We use a system that segments the ASR output of a user’s utterance into clauses. The system annotates an utterance for sentence boundaries, restarts and repairs, and identifies coordinating conjunctions, filled pauses and discourse markers. These annotations are done using a cascade of classifiers, details of which are described in (Bangalore and Gupta, 2004). 4.2 Opening Order Item Payment Info Shipping Info Summary Closing Delivery Info Figure 2: A sample task structure in our application domain. Opening Contact Info Order Item Payment Info Shipping Info Summary Closing Delivery Info Figu"
P06-1026,J01-2004,0,0.0987769,"toolkit LLAMA (Haffner, 2006) to estimate the conditional distribution using maxent. LLAMA encodes multiclass maxent as binary maxent, in order to increase the speed of training and to scale this method to large data sets. Each of the g classes in the set z{> is encoded as a bit vector such that, in the vector for class |, the |B}~ bit is one and all other bits are zero. Then, g one-vs-other binary classifiers are used as follows. ; =+  r ; = ^ _ ¡ ; = 798x 7 9 x 8  ¡`w ¢ For real-time dialog management we use a topdown incremental parser that incorporates bottomup information (Roark, 2001). We rewrite equation (6) to exploit the subtask sequence provided by the chunk model as shown in Equation 7. For the purpose of this paper, we approximate Equation 7 using one-best (or k-best) chunk output.1 (2) to estimate the conditional distribution e In  order  D H  we use the general technique of choos_ _ 5.2 Parse-based Model As seen in Figure 3, the chunk model does not capture dominance relations among subtasks, which are important for resolving anaphoric references (Grosz and Sidner, 1986). Also, the chunk model is representationally inadequate for centerembedded nestings of s"
P06-1026,J86-3001,0,0.104962,"-time dialog management we use a topdown incremental parser that incorporates bottomup information (Roark, 2001). We rewrite equation (6) to exploit the subtask sequence provided by the chunk model as shown in Equation 7. For the purpose of this paper, we approximate Equation 7 using one-best (or k-best) chunk output.1 (2) to estimate the conditional distribution e In  order  D H  we use the general technique of choos_ _ 5.2 Parse-based Model As seen in Figure 3, the chunk model does not capture dominance relations among subtasks, which are important for resolving anaphoric references (Grosz and Sidner, 1986). Also, the chunk model is representationally inadequate for centerembedded nestings of subtasks, which do occur in our domain, although less frequently than the more prevalent “tail-recursive” structures. In this model, we are e interested in finding the most likely plan tree (  ) given the sequence of utterances: to cope with the prediction errors of the classifier, we approximate J3ML  with an P -gram language model on sequences of the refined tag labels: &:&apos; Q ) +R,/.S0/2K,4 &(&apos; Q ; &lt;*= 5 61T 798 5 6ST1U V WYX[Z _ ] ; d=  ,/.S0/2K,4 ^`_ T 798baBc 5 6 5 6ST1U V WYX[Z ; =+ In this"
P06-1026,H92-1060,0,0.0142709,"of subtasks opening, contact-information, order-item, related-offers, summary. Subtasks can be nested; the nesting structure can be as deep as five levels. Most often the nesting is at the left or right frontier of the subtask tree. DialogAct,Pred−Args Utterance Clause Figure 1: Structural analysis of a dialog 4.1 Order Placement Utterance Segmentation The task of ”cleaning up” spoken language utterances by detecting and removing speech repairs and dysfluencies and identifying sentence boundaries has been a focus of spoken language parsing research for several years (e.g. (Bear et al., 1992; Seneff, 1992; Shriberg et al., 2000; Charniak and Johnson, 2001)). We use a system that segments the ASR output of a user’s utterance into clauses. The system annotates an utterance for sentence boundaries, restarts and repairs, and identifies coordinating conjunctions, filled pauses and discourse markers. These annotations are done using a cascade of classifiers, details of which are described in (Bangalore and Gupta, 2004). 4.2 Opening Order Item Payment Info Shipping Info Summary Closing Delivery Info Figure 2: A sample task structure in our application domain. Opening Contact Info Order Item Payment I"
P06-1026,P04-1010,0,0.107697,"Missing"
P06-1026,J00-3003,0,0.522205,"Missing"
P06-1026,2005.sigdial-1.4,0,0.0600817,"Missing"
P06-1026,J97-1002,0,\N,Missing
P07-1020,W05-0831,1,0.93575,"compile some amount of (local) lexical reordering of the target words – those permitted by the size of the phrase. Most of the state-of-the-art machine translation systems use phrase-level associations in conjunction with a target language model to produce sentences. There is relatively little emphasis on (global) lexical reordering other than the local reorderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax-based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present an alternate approach to lexical selection and lexical reordering. For lexical selection, in contrast to the local approaches of associating target to source words, we associate target words to the entire source sentence. The intuition is that there may be lexico-syntactic features of the source sentence (not necessarily a single source word) that might trigger the presence of a target word in the target sentence. Furthermore, it might be difficult to exactly associate a target word to a source word in many situations – (a) when the translations are not exact but pa"
P07-1020,W02-2018,0,0.0173229,"Missing"
P07-1020,P04-1083,0,0.0199764,"Missing"
P07-1020,P98-1006,1,0.781926,"elevel alignments in order to increase the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words – those permitted by the size of the phrase. Most of the state-of-the-art machine translation systems use phrase-level associations in conjunction with a target language model to produce sentences. There is relatively little emphasis on (global) lexical reordering other than the local reorderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax-based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present an alternate approach to lexical selection and lexical reordering. For lexical selection, in contrast to the local approaches of associating target to source words, we associate target words to the entire source sentence. The intuition is that there may be lexico-syntactic features of the source sentence (not necessarily a single source word) that might trigger the presence of a target word in the target sentence. Furthermore, it might be difficult to exactly associate"
P07-1020,P02-1038,0,0.163673,"ertion cost. On composition as shown in Equation 5, the word insertion model penalizes or rewards paths which have more words depending on whether λ is positive or negative value. T ∗ = π1 (BestP ath(Is ◦T ransF ST ◦W IP )) (5) 154 tences. Now, we present our approach for a global lexical selection model based on discriminatively trained classification techniques. Discriminant modeling techniques have become the dominant method for resolving ambiguity in speech and other NLP tasks, outperforming generative models. Discriminative training has been used mainly for translation model combination (Och and Ney, 2002) and with the exception of (Wellington et al., 2006; Tillmann and Zhang, 2006), has not been used to directly train parameters of a translation model. We expect discriminatively trained global lexical selection models to outperform generatively trained local lexical selection models as well as provide a framework for incorporating rich morpho-syntactic information. Statistical machine translation can be formulated as a search for the best target sequence that maximizes P (T |S), where S is the source sentence and T is the target sentence. Ideally, P (T |S) should be estimated directly to maxim"
P07-1020,W05-0823,0,0.0113506,"language representation of each sentence in the bilingual corpus. The bilanguage string consists of source-target symbol pair sequences as shown in Equation 3. Note that the tokens of a bilanguage could be either ordered according to the word order of the source language or ordered according to the word order of the target language. 2 SFST Training and Decoding Bf bfi In this section, we describe each of the components of our SFST system shown in Figure 1. The SFST approach described here is similar to the one described in (Bangalore and Riccardi, 2000) which has subsequently been adopted by (Banchs et al., 2005). 2.1 Word Alignment The first stage in the process of training a lexical selection model is obtaining an alignment function (f ) that given a pair of source (s1 s2 . . . sn ) and target (t1 t2 . . . tm ) language sentences, maps source language word subsequences into target language word subsequences, as shown below. ∀i∃j(f (si ) = tj ∨ f (si ) = ) (1) For the work reported in this paper, we have used the GIZA++ tool (Och and Ney, 2003) which implements a string-alignment algorithm. GIZA++ alignment however is asymmetric in that the word mappings are different depending on the direction of a"
P07-1020,J03-1002,0,0.0128125,"om a finite-state based statistical machine translation system which relies on local lexical associations. 1 Introduction Machine translation can be viewed as consisting of two subproblems: (a) lexical selection, where appropriate target language lexical items are chosen for each source language lexical item and (b) lexical reordering, where the chosen target language lexical items are rearranged to produce a meaningful target language string. Most of the previous work on statistical machine translation, as exemplified in (Brown et al., 1993), employs word-alignment algorithm (such as GIZA++ (Och and Ney, 2003)) that provides local associations between source and target words. The source-to-target word alignments are sometimes augmented with target-to-source word alignments in order to improve precision. Further, the word-level alignments are extended to phraselevel alignments in order to increase the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words – those permitted by the size of the phrase. Most of the state-of-the-art machine translation systems use phrase-level associations in conjunction with a target language model to"
P07-1020,J99-2004,1,0.33873,"Missing"
P07-1020,W00-0508,1,0.750134,"ation From the alignment information (see Figure 3), we construct a bilanguage representation of each sentence in the bilingual corpus. The bilanguage string consists of source-target symbol pair sequences as shown in Equation 3. Note that the tokens of a bilanguage could be either ordered according to the word order of the source language or ordered according to the word order of the target language. 2 SFST Training and Decoding Bf bfi In this section, we describe each of the components of our SFST system shown in Figure 1. The SFST approach described here is similar to the one described in (Bangalore and Riccardi, 2000) which has subsequently been adopted by (Banchs et al., 2005). 2.1 Word Alignment The first stage in the process of training a lexical selection model is obtaining an alignment function (f ) that given a pair of source (s1 s2 . . . sn ) and target (t1 t2 . . . tm ) language sentences, maps source language word subsequences into target language word subsequences, as shown below. ∀i∃j(f (si ) = tj ∨ f (si ) = ) (1) For the work reported in this paper, we have used the GIZA++ tool (Och and Ney, 2003) which implements a string-alignment algorithm. GIZA++ alignment however is asymmetric in that th"
P07-1020,J96-1002,0,0.00714245,"red output approaches. 4.2 Geometric vs. Probabilistic Interpretation We separate the most popular classification techniques into two broad categories: • Geometric approaches maximize the width of a separation margin between the classes. The most popular method is the Support Vector Machine (SVM) (Vapnik, 1998). • Probabilistic approaches maximize the conditional likelihood of the output class given the input features. This logistic regression is 156 also called Maxent as it finds the distribution with maximum entropy that properly estimates the average of each feature over the training data (Berger et al., 1996). In previous studies, we found that the best accuracy is achieved with non-linear (or kernel) SVMs, at the expense of a high test time complexity, which is unacceptable for machine translation. Linear SVMs and regularized Maxent yield similar performance. In theory, Maxent training, which scales linearly with the number of examples, is faster than SVM training, which scales quadratically with the number of examples. In our first experiments with lexical choice models, we observed that Maxent slightly outperformed SVMs. Using a single threshold with SVMs, some classes of words were over-detect"
P07-1020,J93-2003,0,0.00923222,"ords. We compare the results of this approach against those obtained from a finite-state based statistical machine translation system which relies on local lexical associations. 1 Introduction Machine translation can be viewed as consisting of two subproblems: (a) lexical selection, where appropriate target language lexical items are chosen for each source language lexical item and (b) lexical reordering, where the chosen target language lexical items are rearranged to produce a meaningful target language string. Most of the previous work on statistical machine translation, as exemplified in (Brown et al., 1993), employs word-alignment algorithm (such as GIZA++ (Och and Ney, 2003)) that provides local associations between source and target words. The source-to-target word alignments are sometimes augmented with target-to-source word alignments in order to improve precision. Further, the word-level alignments are extended to phraselevel alignments in order to increase the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words – those permitted by the size of the phrase. Most of the state-of-the-art machine translation systems use ph"
P07-1020,P06-1091,0,0.013744,"model penalizes or rewards paths which have more words depending on whether λ is positive or negative value. T ∗ = π1 (BestP ath(Is ◦T ransF ST ◦W IP )) (5) 154 tences. Now, we present our approach for a global lexical selection model based on discriminatively trained classification techniques. Discriminant modeling techniques have become the dominant method for resolving ambiguity in speech and other NLP tasks, outperforming generative models. Discriminative training has been used mainly for translation model combination (Och and Ney, 2002) and with the exception of (Wellington et al., 2006; Tillmann and Zhang, 2006), has not been used to directly train parameters of a translation model. We expect discriminatively trained global lexical selection models to outperform generatively trained local lexical selection models as well as provide a framework for incorporating rich morpho-syntactic information. Statistical machine translation can be formulated as a search for the best target sequence that maximizes P (T |S), where S is the source sentence and T is the target sentence. Ideally, P (T |S) should be estimated directly to maximize the conditional likelihood on the training data (discriminant model). Howe"
P07-1020,2006.amta-papers.28,0,0.0102147,"on 5, the word insertion model penalizes or rewards paths which have more words depending on whether λ is positive or negative value. T ∗ = π1 (BestP ath(Is ◦T ransF ST ◦W IP )) (5) 154 tences. Now, we present our approach for a global lexical selection model based on discriminatively trained classification techniques. Discriminant modeling techniques have become the dominant method for resolving ambiguity in speech and other NLP tasks, outperforming generative models. Discriminative training has been used mainly for translation model combination (Och and Ney, 2002) and with the exception of (Wellington et al., 2006; Tillmann and Zhang, 2006), has not been used to directly train parameters of a translation model. We expect discriminatively trained global lexical selection models to outperform generatively trained local lexical selection models as well as provide a framework for incorporating rich morpho-syntactic information. Statistical machine translation can be formulated as a search for the best target sequence that maximizes P (T |S), where S is the source sentence and T is the target sentence. Ideally, P (T |S) should be estimated directly to maximize the conditional likelihood on the training data"
P07-1020,J97-3002,0,0.0113111,"d to phraselevel alignments in order to increase the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words – those permitted by the size of the phrase. Most of the state-of-the-art machine translation systems use phrase-level associations in conjunction with a target language model to produce sentences. There is relatively little emphasis on (global) lexical reordering other than the local reorderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax-based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present an alternate approach to lexical selection and lexical reordering. For lexical selection, in contrast to the local approaches of associating target to source words, we associate target words to the entire source sentence. The intuition is that there may be lexico-syntactic features of the source sentence (not necessarily a single source word) that might trigger the presence of a target word in the target sentence. Furthermore, it might be difficult"
P07-1020,P05-1033,0,0.0241716,"nt of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words – those permitted by the size of the phrase. Most of the state-of-the-art machine translation systems use phrase-level associations in conjunction with a target language model to produce sentences. There is relatively little emphasis on (global) lexical reordering other than the local reorderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax-based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present an alternate approach to lexical selection and lexical reordering. For lexical selection, in contrast to the local approaches of associating target to source words, we associate target words to the entire source sentence. The intuition is that there may be lexico-syntactic features of the source sentence (not necessarily a single source word) that might trigger the presence of a target word in the target sentence. Furthermore, it might be difficult to exactly associate a target word to a source word in many s"
P07-1020,P01-1067,0,0.04115,"rder to increase the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words – those permitted by the size of the phrase. Most of the state-of-the-art machine translation systems use phrase-level associations in conjunction with a target language model to produce sentences. There is relatively little emphasis on (global) lexical reordering other than the local reorderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax-based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present an alternate approach to lexical selection and lexical reordering. For lexical selection, in contrast to the local approaches of associating target to source words, we associate target words to the entire source sentence. The intuition is that there may be lexico-syntactic features of the source sentence (not necessarily a single source word) that might trigger the presence of a target word in the target sentence. Furthermore, it might be difficult to exactly associate a target word to a source"
P07-1020,W06-1628,0,0.0465185,"Missing"
P07-1020,N04-1033,0,0.151711,"Missing"
P07-1020,P05-1059,0,0.0159498,"Missing"
P07-1020,C98-1006,1,\N,Missing
P08-2057,koen-2004-pharaoh,0,0.274826,"We demonstrate the integration of the dialog acts in a phrase-based statistical translation framework, employing 3 limited domain parallel corpora (Farsi-English, Japanese-English and Chinese-English). For all three language pairs, in addition to producing interpretable DA enriched target language translations, we also obtain improvements in terms of objective evaluation metrics such as lexical selection accuracy and BLEU score. 1 Introduction Recent approaches to statistical speech translation have relied on improving translation quality with the use of phrase translation (Och and Ney, 2003; Koehn, 2004). The quality of phrase translation is typically measured using n-gram precision based metrics such as BLEU (Papineni et al., 2002) and NIST scores. However, in many dialog based speech translation scenarios, vital information beyond what is robustly captured by words and phrases is carried by the communicative act (e.g., question, acknowledgement, etc.) representing the function of the utterance. Our approach for incorporating dialog act tags in speech translation is motivated by the fact that it is important to capture and convey not only what is being communicated (the words) but how someth"
P08-2057,1995.tmi-1.15,0,0.107346,"context). Augmenting current statistical translation frameworks with dialog acts can potentially improve translation quality and facilitate successful crosslingual interactions in terms of improved information transfer. Dialog act tags have been previously used in the VERBMOBIL statistical speech-to-speech translation system (Reithinger et al., 1996). In that work, the predicted DA tags were mainly used to improve speech recognition, semantic evaluation, and information extraction modules. Discourse information in the form of speech acts has also been used in interlingua translation systems (Mayfield et al., 1995) to map input text to semantic concepts, which are then translated to target text. In contrast with previous work, in this paper we demonstrate how dialog act tags can be directly exploited in phrase based statistical speech translation systems (Koehn, 2004). The framework presented in this paper is particularly suited for human-human and human-computer interactions in a dialog setting, where information loss due to erroneous content may be compensated to some extent through the correct transfer of the appropriate dialog act. The dialog acts can also be potentially used for imparting correct u"
P08-2057,J03-1002,0,0.00759298,"tical translation. We demonstrate the integration of the dialog acts in a phrase-based statistical translation framework, employing 3 limited domain parallel corpora (Farsi-English, Japanese-English and Chinese-English). For all three language pairs, in addition to producing interpretable DA enriched target language translations, we also obtain improvements in terms of objective evaluation metrics such as lexical selection accuracy and BLEU score. 1 Introduction Recent approaches to statistical speech translation have relied on improving translation quality with the use of phrase translation (Och and Ney, 2003; Koehn, 2004). The quality of phrase translation is typically measured using n-gram precision based metrics such as BLEU (Papineni et al., 2002) and NIST scores. However, in many dialog based speech translation scenarios, vital information beyond what is robustly captured by words and phrases is carried by the communicative act (e.g., question, acknowledgement, etc.) representing the function of the utterance. Our approach for incorporating dialog act tags in speech translation is motivated by the fact that it is important to capture and convey not only what is being communicated (the words)"
P08-2057,P02-1040,0,0.077474,"d domain parallel corpora (Farsi-English, Japanese-English and Chinese-English). For all three language pairs, in addition to producing interpretable DA enriched target language translations, we also obtain improvements in terms of objective evaluation metrics such as lexical selection accuracy and BLEU score. 1 Introduction Recent approaches to statistical speech translation have relied on improving translation quality with the use of phrase translation (Och and Ney, 2003; Koehn, 2004). The quality of phrase translation is typically measured using n-gram precision based metrics such as BLEU (Papineni et al., 2002) and NIST scores. However, in many dialog based speech translation scenarios, vital information beyond what is robustly captured by words and phrases is carried by the communicative act (e.g., question, acknowledgement, etc.) representing the function of the utterance. Our approach for incorporating dialog act tags in speech translation is motivated by the fact that it is important to capture and convey not only what is being communicated (the words) but how something is being communicated (the context). Augmenting current statistical translation frameworks with dialog acts can potentially imp"
P08-2057,2006.iwslt-evaluation.1,0,\N,Missing
P10-4011,E09-1028,1,0.786806,"ite. For a yellow-pages type of query, Where is the Saigon Kitchen in Austin, Texas?, the pertinent search parameters that are parsed out are business-name: Saigon Kitchen, city: Austin, and state: Texas, which are used to construct a search string to search the Yellowpages website. These are just two examples of the kinds of dynamic user queries that we encounter. Within each broad category, there is a wide variety of the sub-types of user queries, and for each sub-type, we have to parse out different search parameters and use different web-forms. Details of this extraction are presented in (Feng and Bangalore, 2009). 5 Retrieving answers to static questions Answers to static user queries – questions whose answers do not change over time – are retrieved in a different way than answers to dynamic questions. A description of how our system retrieves the answers to static questions is presented in this section. It is quite likely that many of the dynamic queries may not have all the pertinent search parameters explicitly outlined. For example, a mass transit query may be When is the next train to Princeton?. The bare minimum search parameters needed to answer this query are a from-location, and a to-location"
P10-4011,N10-1007,1,0.778233,"ummers ago, currently, recently, which were based on the TimeML corpus. We also included spatial indexicals such as here, and other substrings such as cost of and how much is. A question is considered static if it does not contain any such words/phrases. 3.2.2 Dynamic/static Classification As mentioned before, we experimented with three different approaches to bootstrapping a dynamic/static question classifier. We evaluated these methods on a 250 question test set drawn from the broad topic of Movies. The error rates are summarized in Table 2. We provide further details of this experiment in (Mishra and Bangalore, 2010). Self-training with bagging: The general selftraining with bagging algorithm (Banko and Brill, 2001). The benefit of self-training is that we can build a better classifier than that built from the small seed corpus by simply adding in the large unlabeled corpus without requiring hand-labeling. Training approach Baseline “Supervised” learning Self-training Active-learning Active-learning: This is another popular method for training classifiers when not much annotated data is available. The key idea in active learning is to annotate only those instances of the dataset that are most difficult fo"
P10-4011,P04-1073,0,0.0847724,"Missing"
P10-4011,P01-1005,0,\N,Missing
P11-2107,P00-1030,0,0.0610258,"Missing"
P11-2107,W99-0619,0,0.0394397,"and, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Informativeness Measures We used the following five metrics to capture the individual and relative informativeness of nouns in each NN compound: • Unigram Predictability (UP): Defined as the predictability of a word given a text corpus, it is measured as the log probability of the word in the text corpus. Here, we use the maximum likelihood formulation of this measure. F req(wi ) UP = log P i F req(wi ) (1) This is a very simple measure of word informativeness that has been shown to be effective in a similar task (Pan and McKeown, 1999). • Bigram Predictability (BP): Defined as the predictability of a word given a previous word, it is measured as the log probability of noun N2 given noun N1. BP = log (P rob(N 2 |N 1)) (2) • Pointwise Mutual Information (PMI): Defined as a measure of how collocated two words are, it is measured as the log of the ratio of probability of the joint event of the two words occurring and the probability of them occurring independent of each other. PMI = log P rob(N 1, N 2) P rob(N 1)P rob(N 2) (3) • Dice Coefficient (DC): Dice is another collocation measure used in information retrieval. 2 × P rob("
P98-1006,1995.tmi-1.20,0,0.0236531,"Missing"
P98-1006,J97-3002,0,0.046021,"ver methods requiring hand-coding of linguistic information. However, there are disadvantages to the automatic approaches proposed so far. The various methods described by Brown et. al (1990; 1993) do not take into account the natural structuring of strings into phrases. Example-based translation, exemplified by the work of Sumita and Iida (1995), requires very large amounts of training material. The number of states in a simple finite state model such as those used by Vilar et al. (1996) becomes extremely large when faced with languages with large word order differences. The work reported in Wu (1997), which uses an inside-outside type of training algorithm to learn statistical contextfree transduction, has a similar motivation to the current work, but the models we describe 41 Douglas here, being fully lexical, are more suitable for direct statistical modelling. In this paper, we show that both the network topology and parameters of a head transducer translation model (Alshawi, 1996b) can be learned fully automatically from a bilingual corpus. It has already been shown (Alshawi et al., 1997) that a head transducer model with hand-coded structure can be trained to give better accuracy than"
P98-1006,P97-1046,1,0.850766,"Missing"
P98-1006,P96-1023,1,0.831541,"t function from source word subsequences to target word subsequences for each transcribed utterance and its translation. The construction of states and transitions is specified in Section 4; the method for selecting phrase head words is described in Section 5. The string comparison evaluation metric we use is described in Section 6, and the results of testing the method in a limited domain of English-Spanish translation are reported in Section 7. 2 Overview 2.1 L e x i c a l h e a d t r a n s d u c e r s In our training method, we follow the simple lexical head transduction model described by Alshawi (1996b) which can be regarded as a type of statistical dependency grammar transduction. This type of transduction model consists of a collection of head transducers; the purpose of a particular transducer is to translate a specific source word w into a target word v, and further to translate the pair of sequences of dependent words to the left and right of w to sequences of dependents to the left and right of c. When applied recursively, a set of such transducerb effects a hierarchical transduction of the source string into the target string. A distinguishing property of head transducers, as compar"
P98-1006,J90-2002,0,0.0615861,"f source and target subsequences, and all alignment search is carried out to minimize the sum of the costs of a set of pairings which completely maps the bitext source to its target. 3.1 Alignment n v -- n~,i~v b = nw, y c = N - d = nw nv - nw + nw, v - nw, v N is the total number of bitexts, n v the number of bitexts in which V appears in the target, n w the number of bitexts in which W appears in the source, and n w , y the number of bitexts in which W appears in the source and V appears in the target. We tried using the log probabilities of target subsequences given source subsequences (cf Brown et al. (1990)) as a cost function instead of ¢ but ¢ resulted in better performance of our translation models. model The cost of a pairing is composed of a weighted combination of cost functions. We currently use The second cost function used is a distance measure which penalizes pairings in which the source subsequence and target subsequence are in very different positions in their respective sentences. Different weightings of distance to correlation costs can be used to bias the model towards more or less parallel alignments for different language pairs. two. The first cost function is the ¢ correlation"
P98-1006,J93-2003,0,0.0277943,"Missing"
P98-1006,P96-1025,0,0.0423809,"or final, are treated as event observation counts for a statistical head transduction model. More specifically, they are used as counts for maximum likelihood estimation of the transducer start, transition, and stop probabilities specified in Section 2. 5 Head selection We have been using the following monolingual metrics which can be applied to either the source or target language to predict the likelihood of a word being the head word of a string. Distance: The distance between a dependent and its head. In general, the likelihood of a head-dependent relation decreases as distance increases (Collins, 1996). Word frequency: The frequency of occurrence of a word in the training corpus. IVord 'complezity': For languages with phonetic orthography such as English, 'complexity' of a word can be measured in terms of number of characters in that word. Optionality: This metric is intended to identify optional modifiers which are less likely to be heads. For each word we find trigrams with the word of interest as the middle word and compare the distribution of these trigrams with the distribution of the bigrams formed from the outer pairs of words. If these two distributions are strongly correlated then"
P98-1006,H91-1026,0,0.020961,"of ¢ but ¢ resulted in better performance of our translation models. model The cost of a pairing is composed of a weighted combination of cost functions. We currently use The second cost function used is a distance measure which penalizes pairings in which the source subsequence and target subsequence are in very different positions in their respective sentences. Different weightings of distance to correlation costs can be used to bias the model towards more or less parallel alignments for different language pairs. two. The first cost function is the ¢ correlation measure (cf the use of ¢2 in Gale and Church (1991)) computed as follows: (bc= ad) = x/(a + b)(c + d)(a + c)(b + d) 43 3.2 Alignment search The agenda-based alignment search makes use of dynamic programming to record the best cost seen for all partial alignments covering the same source and target subsequence; partial alignments coming off the agenda that have a higher cost for the same coverage are discarded and take 11o further part in the search. An effort limit on the number of agenda items processed is used to ensure reasonable speed in the search regardless of sentence length. An iterative broadening strategy is used, so that at breadth"
S14-2014,W03-0421,0,0.0577205,"Missing"
S14-2014,P97-1003,0,0.242141,"Missing"
S14-2014,N10-1138,0,0.0691305,"Missing"
S14-2014,W05-1506,0,0.0370293,"turquoise pyramid above the yellow cube. Word Move the turquoise pyramid above the yellow cube index 1 2 3 4 5 6 7 8 tag action O color type relation O color type ple in Figure 1. The parser generates multiple RCL parse tree hypotheses sorted in the order of their likelihood. The likelihood of a tree T given a sequence of tags T is determined using a probabilistic context free grammar (PCFG) G: label move cyan prism above yellow cube P (T |S) = PG (r) (1) r∈T The n-best parses are obtained using the CKY algorithm, recording the n-best hyperedge backpointers per constituent along the lines of (Huang and Chiang, 2005). G was obtained and PG was estimated from a corpus of non-lexical RCL trees generated by removing all nodes descendant from the tag nodes (action, color, etc.). Parses may contain empty nodes not corresponding to any tag in the input sequence. These are hypothesized by the parser at positions in between input tags and inserted as edges according to the PCFG, which has probabilistic rules for generating empty nodes. Table 1: Tagging labels for a sentence Move the turquoise pyramid above the yellow cube. ing a combined semantic tag and label (such as type cube) to each word in a command. The ta"
S14-2014,J93-2004,0,0.0450397,"s and labels for a sample sentence “Move the turquoise pyramid above the yellow cube” extracted from the RCL parse tree (see Figure 1). In some cases, a label is the same as a word (yellow, cube) while in other cases, it differs (turquoise - cyan, pyramid - prism). We train a sequence tagger using LLAMA maximum entropy (maxent) classification (Haffner, 2006) to predict the combined semantic tag and label of each word. Neighboring words, immediately neighboring semantic tags, and POS tags are used as features, where the POS tagger is another sequence tagging model trained on the Penn Treebank (Marcus et al., 1993). We also experimented with a tagger that assigns tags and labels in separate sequence tagging models, but it performed poorly. 2.2 Y 2.3 Reference Resolution Reference resolution identifies the most probable antecedent for each anaphor within a text (Hirschman and Chinchor, 1997). It applies when multiple candidates antecedents are present. For example, in a sentence “Pick up the red cube standing on a grey cube and place it on top of the yellow one”, the anaphor it has two candidate antecedents corresponding to entity segments the red cube and a grey cube. In our system, anaphor and antecede"
S14-2014,N03-1033,0,0.0169608,"Missing"
W00-0508,P98-1006,1,0.859074,"the chosen target language lexical items are reordered to produce a meaningful target language string. In our approach, we will represent these two phases using stochastic finitestate models which can be composed together to result in a single stochastic finite-state model for SMT. Thus our method can be viewed as a direct translation approach of transducing strings of the source language to strings of the target language. There are other approaches to statistical machine translation where translation is achieved through transduction of source language structure to target language structure (Alshawi et al., 1998b; Wu, 1997). There are also large international multi-site projects such as VERBMOBIL (Verbmobil, 2000) and CSTAR (Woszczyna et al., 1998; Lavie et al., 1999) that are involved in speech-to-speech translation in limited domains. The systems developed in these projects employ various techniques ranging from example-based to interlingua-based translation methods for translation between English, French, German, Italian, Japanese, and Korean. Abstract Stochastic finite-state models are efficiently learnable from data, effective for decoding and are associated with a calculus for composing models"
W00-0508,J94-3001,0,0.0286732,"c finite-state machine translation that is trained automatically from pairs of source and target utterances. We use this method to develop models for English-Japanese and Japanese-English translation. We have embedded the Japanese-English translation system in a call routing task of unconstrained speech utterances. We evaluate the efficacy of the translation system :in the context of this application. 1 Introduction Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Srinivas, 1997) and parsing (Roche, 1.999). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing. I In this paper, we develop stochastic finite-state models (SFSM) for statistical machine translation (SMT) and explore the performance limits of such models in the context of translati"
W00-0508,knight-al-onaizan-1998-translation,0,0.229362,"Missing"
W00-0508,W98-1122,1,0.824994,"Missing"
W00-0508,woszczcyna-etal-1998-modular,0,0.111101,"Missing"
W00-0508,J97-3002,0,0.0566754,"uage lexical items are reordered to produce a meaningful target language string. In our approach, we will represent these two phases using stochastic finitestate models which can be composed together to result in a single stochastic finite-state model for SMT. Thus our method can be viewed as a direct translation approach of transducing strings of the source language to strings of the target language. There are other approaches to statistical machine translation where translation is achieved through transduction of source language structure to target language structure (Alshawi et al., 1998b; Wu, 1997). There are also large international multi-site projects such as VERBMOBIL (Verbmobil, 2000) and CSTAR (Woszczyna et al., 1998; Lavie et al., 1999) that are involved in speech-to-speech translation in limited domains. The systems developed in these projects employ various techniques ranging from example-based to interlingua-based translation methods for translation between English, French, German, Italian, Japanese, and Korean. Abstract Stochastic finite-state models are efficiently learnable from data, effective for decoding and are associated with a calculus for composing models which allows"
W00-0508,J99-2004,1,\N,Missing
W00-0508,J93-2003,0,\N,Missing
W00-0508,J01-1001,0,\N,Missing
W00-0508,P02-1040,0,\N,Missing
W00-0508,N01-1018,1,\N,Missing
W00-0508,P91-1032,0,\N,Missing
W00-0508,J00-1003,0,\N,Missing
W00-0508,C98-1006,1,\N,Missing
W00-1401,P98-1006,1,0.753619,"use a second metric, G e n e r a t i o n S t r i n g A c c u r a c y , shown in Equation (3), which treats deleWe employ two metrics that measure the accuracy tion of a token at one location in the string and the of a generated string. The first metric, s i m p l e acinsertion of the same token at another location in c u r a c y , is the same string distance metric used for the string as one single movement error (M). This measuring speech recognition accuracy. This metis in addition to the remaining insertions (I&apos;) and ric has also been used to measure accuracy of MT deletions (D&apos;). systems (Alshawi et al., 1998). It is based on string edit distance between the output of the generation (3) G e n e r a t i o n S t r i n g A c c u r a c y = system and the reference corpus string. Simple ac( 1 -- M~-/~.P-~--~-~) curacy is the number of insertion (I), deletion (D) and substitutions (S) errors between the reference In our example sentence (2), we see that the inserstrings in the test corpus and the strings produced by tion and deletion of no can be collapsed into one the generation model. An alignment algorithm using substitution, insertion and deletion of tokens as move. However, the wrong positions of co"
W00-1401,J99-2004,1,0.215788,"Missing"
W00-1401,C00-1007,1,0.604742,"Missing"
W00-1401,P98-1116,0,0.215224,"tly cannot m a p large corpora of syntactic parses onto such semantic representations, and therefore cannot create the input representation for the evaluation. The second question is t h a t of fairness of the evaluation. FE[,tGt.&apos;S as described in this paper is of limited use. since it only chooses word order (and, to a certain extent, syntactic structure). Other realization and sentence planning tin{ks-which are needed for most applications and which may profit from a stochastic model include lexical choice, introduction of function words and punctuation, and generation of morphology. (See (Langkilde and Knight, 1998a) for a relevant discussion. FERGUS currently can perform punctuation and function word insertion, and morphology and lexical choice are under development.) The question arises whether our metrics will . fairly m e a s u r e the:quality,~of,a, more comp!ete real~ .... ization module (with some sentence planning). Once the range of choices t h a t the generation component makes expands, one quickly runs into the problem that, while the gold standard may be a good way of communicating the input structure, there are usually other good ways of doing so as well (using other words, other syntactic"
W00-1401,W98-1426,0,0.279927,"tly cannot m a p large corpora of syntactic parses onto such semantic representations, and therefore cannot create the input representation for the evaluation. The second question is t h a t of fairness of the evaluation. FE[,tGt.&apos;S as described in this paper is of limited use. since it only chooses word order (and, to a certain extent, syntactic structure). Other realization and sentence planning tin{ks-which are needed for most applications and which may profit from a stochastic model include lexical choice, introduction of function words and punctuation, and generation of morphology. (See (Langkilde and Knight, 1998a) for a relevant discussion. FERGUS currently can perform punctuation and function word insertion, and morphology and lexical choice are under development.) The question arises whether our metrics will . fairly m e a s u r e the:quality,~of,a, more comp!ete real~ .... ization module (with some sentence planning). Once the range of choices t h a t the generation component makes expands, one quickly runs into the problem that, while the gold standard may be a good way of communicating the input structure, there are usually other good ways of doing so as well (using other words, other syntactic"
W00-1401,A00-2023,0,0.0545747,"Missing"
W00-1401,W98-1400,0,0.060105,"enre, register, idiosyncratic choices, and so on). Assuming the test corpus is representative of the training corpus, we can then use our metrics to measure deviance from the corpus, whether it be merely in word order or in terms of more complex tasks such as lexical choice as well. Thus, as long as the goal of the realizer is to enmlate as closely as possible a given corpus (rather than provide a maximal range of paraphrastic capability), then our approach can be used for evaluation, r As in the case of machine translation, evaluation in generation is a complex issue. (For a discussion, see (Mellish and Dale, 1998).) Presumably, the quality of most generation systems can only be assessed at a system level in a task-oriented setting (rather than by taking quantitative measures or by asking humans for quality assessments). Such evaluations are costly, and they cannot be the basis of work in stochastic generation, for which evaluation is a frequent step in research and development. An advantage of our approach is that our quantitative metrics allow us to evaluate without human intervention, automatically and objectively (objectively with respect to the defined metric,-that is).- Independently, the use of t"
W00-1401,P97-1035,0,0.059365,"Missing"
W00-1401,J97-1004,0,\N,Missing
W00-1401,C98-1112,0,\N,Missing
W00-2004,P98-1006,1,0.671057,"Missing"
W00-2004,J99-2004,1,0.62835,"Missing"
W00-2004,W00-1401,1,0.403517,"Missing"
W00-2004,C00-1007,1,0.796749,"Missing"
W00-2004,C96-1034,0,0.0328893,"Missing"
W00-2004,P98-1116,0,0.107091,"Missing"
W00-2004,W98-1426,0,0.118114,"Missing"
W00-2004,P95-1021,1,0.346341,"Missing"
W00-2004,P97-1026,0,0.0186598,"Missing"
W00-2004,W98-0143,0,0.0411512,"Missing"
W00-2004,W98-1422,0,\N,Missing
W02-1035,J99-2004,1,\N,Missing
W02-1035,N01-1016,0,\N,Missing
W02-1035,H92-1060,0,\N,Missing
W02-1035,A88-1019,0,\N,Missing
W02-1035,P92-1008,0,\N,Missing
W02-1035,P99-1053,0,\N,Missing
W02-1035,J99-4003,0,\N,Missing
W02-2214,W02-2236,1,\N,Missing
W02-2214,J99-2004,1,\N,Missing
W02-2214,J95-4002,0,\N,Missing
W02-2214,1997.iwpt-1.11,0,\N,Missing
W02-2214,P00-1058,0,\N,Missing
W02-2214,P98-1106,1,\N,Missing
W02-2214,C98-1102,1,\N,Missing
W02-2214,P95-1037,0,\N,Missing
W02-2214,P98-1061,0,\N,Missing
W02-2214,C98-1059,0,\N,Missing
W02-2214,P98-2190,0,\N,Missing
W02-2214,C98-2185,0,\N,Missing
W02-2214,J94-1004,0,\N,Missing
W02-2236,W01-0520,1,0.885176,"Missing"
W02-2236,E99-1025,1,0.821222,"Missing"
W02-2236,2000.iwpt-1.9,1,0.941088,"Chen*, Srinivas Bangalore*, Michael Collins*, and Owen Rambowt *AT&T Labs-Research, t University ofPennsylvania {jchen,srini,mcollins}@research.att.com,rarnbow@unagi.cis.upenn.edu 1. Introduction As shown by Srinivas (1997), standard n-gram modeling may be used to perfonn supertag disambiguation with accuracy that is adequate for partial parsing, but in general not sufficient for füll parsing. A serious problem is that n-gram modeling usually considers a very small, fixed context and does not perfonn weil with large tag sets, such as those generated by automatic grammar extraction (Xia, 1999; Chen and Vijay-Shanker, 2000; Chlang, 2000). As an alternative, Chen, Bangalore and Vijay-Shanker (1999) introduce class-based supertagging. An example of class tagging is n-best trigram-based supertagging, which assigns to each word the top n most likely supertags as detennined by an n-gram supertagging model. Class-based supertagging can be performed much more accurately than supertagging with only a small increase in ambiguity. In a second phase, the most likely candidate from the class is chosen. In this paper, we investigate an approach to such a choice based on reranking a set of candidate supertags and their confi"
W02-2236,P00-1058,0,0.0381466,"g has been found tobe usefül in other applications such as infonnation retrieval (Chandrasekhar and Srinivas, l 997b) and text simplification (Chandrasekhar and Srinivas, 1997a). © 2002 John Chen, Srinivas Bangalore, Michael Collins, and Owen Rarnbow. Proceedings ofthe Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 259-268. Universitä di Venezia. 260 Proceedings ofTAG+6 2.2. Automatically Extracted Grammars Recently, procedures have been developed that automatically extract TAGs from broad coverage treebanks (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000). They have the advantage that linguistically motivated TAGs can be extracted from widely available treebanks without a huge investment in manual labor. Furthermore, because of their direct extraction from a treebank, parameters can be easily and accurately estimated for building statistical TAG models for parsing (Chiang, 2000; Sarkar, 2001) or geoeration (Bangalore, Chen and Rambow, 2001). In our experiments, we use an automatically extracted TAG grammar similar to the ones described by Chen and Vijay-Shanker (2000). This grammar has been extracted from Sections 02-21 of the Penn Treebank (M"
W02-2236,P97-1003,1,0.826017,"Missing"
W02-2236,J93-2004,0,0.022848,"Missing"
W02-2236,W96-0213,0,0.360487,"Missing"
W02-2236,W00-2027,0,0.029134,"r Output We claim that supertagging is a viable option to explore for use as a preprocessing step in order to speed up füll parsing. In order to substantiate this claim, we perform exploratory experiments that show the relationship between n-best supertagging and parsing performance. Using the grammar that is described in Section 2.2, we train n-best supertaggers on Sections 02-21 of the Perut Treebank. For each supertagger, we supertag Section 22, which consists of about 40,100 words in 1,700 sentences. We then feed the resulting output through the LEM parser, a head-driven TAG chart parser (Sarkar, 2000). Given an input sentence and a grammar, this parser either outputs nothing, or a packed derivation forest of every parse that can be assigned to the sentence by the grammar. lt does not retum partial parses. The results of these experiments are shown in Table 1. The input to the parser can be the output of either a 1, 2, or 4-best supertagger. lt can also be sentences where each word is associated with all of the supertags with that word's part of speech, as detennined by a trigram part of speech tagger. This is labeled as &quot;POS-tag&quot; in the table. Lastly, it can simply be sentences where each"
W02-2236,N01-1023,0,0.045366,"rks (TAG+6), pp. 259-268. Universitä di Venezia. 260 Proceedings ofTAG+6 2.2. Automatically Extracted Grammars Recently, procedures have been developed that automatically extract TAGs from broad coverage treebanks (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000). They have the advantage that linguistically motivated TAGs can be extracted from widely available treebanks without a huge investment in manual labor. Furthermore, because of their direct extraction from a treebank, parameters can be easily and accurately estimated for building statistical TAG models for parsing (Chiang, 2000; Sarkar, 2001) or geoeration (Bangalore, Chen and Rambow, 2001). In our experiments, we use an automatically extracted TAG grammar similar to the ones described by Chen and Vijay-Shanker (2000). This grammar has been extracted from Sections 02-21 of the Penn Treebank (Marcus, Santorini and Marcinkiewicz, 1993). lt contains 3964 tree frames (non-lexicalized elementary trees). The parameters of extraction are set as follows. Each tree frame contains nodes that are labeled using a label set similar to the XTAG (XTAG-Group, 2001) label set. Furthermore, tree frames are extracted corresponding to a &quot;moderate&quot; do"
W02-2236,C88-2121,0,0.206461,"Missing"
W02-2236,1997.iwpt-1.22,0,0.0396543,"hroughout this section, we describe the kinds oflinguistic resources that we use in all of our experiments and the kinds of notation that we will employ in the rest of this paper. 2.1. Supertagging Supertagging (Bangalore and Joshl, 1999) is the process of assigning the best TAG elementary tree, or supertag, to each word in the input sentence. lt performs the task of parsing disambiguation to such an extent that it may be characterized as providing an almost parse. There exist linear time approaches to supertagging, providing one promising route to linear time parsing disambiguation. However, Srinivas (1997) shows that standard n-grarn modeling may be used to perform supertagging with accuracy that is adequate for partial parsing, but not for füll parsing. On the other hand, n-gram modeling of supertagging has been found tobe usefül in other applications such as infonnation retrieval (Chandrasekhar and Srinivas, l 997b) and text simplification (Chandrasekhar and Srinivas, 1997a). © 2002 John Chen, Srinivas Bangalore, Michael Collins, and Owen Rarnbow. Proceedings ofthe Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 259-268. Universitä di Venezia. 260 Pr"
W02-2236,N01-1003,1,0.882532,"Missing"
W02-2236,W00-1208,0,0.0225814,"Missing"
W02-2236,W98-0143,0,0.0337296,"Missing"
W02-2236,J99-2004,1,\N,Missing
W02-2236,J03-4003,1,\N,Missing
W07-0413,P98-1006,1,0.748267,"ts in order to increase the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words—those permitted by the size of the phrase. Most of the state–of–the–art machine translation systems use these phrase–level associations in conjunction with a target language model to produce the target sentence. There is relatively little emphasis on (global) lexical reordering other than the local re-orderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax–based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present three models for doing discriminative machine translation using global lexical selection and lexical reordering. Machine translation of a source language sentence involves selecting appropriate target language words and ordering the selected words to form a well-formed target language sentence. Most of the previous work on statistical machine translation relies on (local) associations of target words/phrases with source words/phrases for lexical selection. In contrast,"
W07-0413,J96-1002,0,0.0487944,"in order to learn the local associations. Instead, we take the sentence aligned corpus as before but we treat the target sentence as a bag–of–words or BOW assigned to the source sentence. The goal is, given a source sentence S, to estimate the probability that we find a given word (tj ) in its translation ie.., we need to estimate the probabilities P (true|tj , S) and P (f alse|tj , S). To train such a model, we need to build binary classifiers for all the words in the target language vocabulary. The probability distributions of these binary classifiers are learnt using maximum entropy model (Berger et al., 1996; Haffner, 2006). For the word tj , the training sentence pairs are considered as positive examples where the word appears in the target, and negative otherwise. Thus, the number of training examples for each binary classifier equals the number of training examples. In this model, classifiers are training using n–gram features (BOgrams(S)). During decoding, instead of producing the target sentence directly, what we initially obtain is the target bag of words. Each word in the target vocabulary is detected independently, so we have here a very simple use of binary static classifiers. Given a se"
W07-0413,J93-2003,0,0.0163432,"guage words given a source sentence and then order the words appropriately. We show that a hierarchical model performs best when compared to the other two models. 1 Introduction The problem of machine translation can be viewed as consisting of two subproblems: (a) lexical selection, where appropriate target language lexical items are chosen for each source language lexical item and (b) lexical reordering, where the chosen target language lexical items are rearranged to produce a meaningful target language string. Most of the previous work on statistical machine translation, as exemplified in (Brown et al., 1993), employs word–alignment algorithm (such as GIZA++ (Och et al., 1999)) that provides local associations between source words and target words. The source–to–target word–alignments are 1. Bag–of–Words model : Given a source sentence, each of the target words are chosen by looking at the entire source sentence. The target language words are then permuted in various ways and then, the best permutation is chosen using the language model on the target side. The size of the search space of these permutations can be set by a parameter called the permutation window. This model does not allow long dist"
W07-0413,P05-1033,0,0.0341098,"ociations. The phrasal associations compile some amount of (local) lexical reordering of the target words—those permitted by the size of the phrase. Most of the state–of–the–art machine translation systems use these phrase–level associations in conjunction with a target language model to produce the target sentence. There is relatively little emphasis on (global) lexical reordering other than the local re-orderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax–based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present three models for doing discriminative machine translation using global lexical selection and lexical reordering. Machine translation of a source language sentence involves selecting appropriate target language words and ordering the selected words to form a well-formed target language sentence. Most of the previous work on statistical machine translation relies on (local) associations of target words/phrases with source words/phrases for lexical selection. In contrast, in this paper, we present a novel approa"
W07-0413,W05-0831,0,0.118003,"mount of (local) lexical reordering of the target words—those permitted by the size of the phrase. Most of the state–of–the–art machine translation systems use these phrase–level associations in conjunction with a target language model to produce the target sentence. There is relatively little emphasis on (global) lexical reordering other than the local re-orderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax–based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present three models for doing discriminative machine translation using global lexical selection and lexical reordering. Machine translation of a source language sentence involves selecting appropriate target language words and ordering the selected words to form a well-formed target language sentence. Most of the previous work on statistical machine translation relies on (local) associations of target words/phrases with source words/phrases for lexical selection. In contrast, in this paper, we present a novel approach to lexical selection where the target words are associa"
W07-0413,W99-0604,0,0.0875833,"ely. We show that a hierarchical model performs best when compared to the other two models. 1 Introduction The problem of machine translation can be viewed as consisting of two subproblems: (a) lexical selection, where appropriate target language lexical items are chosen for each source language lexical item and (b) lexical reordering, where the chosen target language lexical items are rearranged to produce a meaningful target language string. Most of the previous work on statistical machine translation, as exemplified in (Brown et al., 1993), employs word–alignment algorithm (such as GIZA++ (Och et al., 1999)) that provides local associations between source words and target words. The source–to–target word–alignments are 1. Bag–of–Words model : Given a source sentence, each of the target words are chosen by looking at the entire source sentence. The target language words are then permuted in various ways and then, the best permutation is chosen using the language model on the target side. The size of the search space of these permutations can be set by a parameter called the permutation window. This model does not allow long distance re-orderings of target words unless a very large permutation win"
W07-0413,J97-3002,0,0.0371823,"l alignments in order to increase the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words—those permitted by the size of the phrase. Most of the state–of–the–art machine translation systems use these phrase–level associations in conjunction with a target language model to produce the target sentence. There is relatively little emphasis on (global) lexical reordering other than the local re-orderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax–based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present three models for doing discriminative machine translation using global lexical selection and lexical reordering. Machine translation of a source language sentence involves selecting appropriate target language words and ordering the selected words to form a well-formed target language sentence. Most of the previous work on statistical machine translation relies on (local) associations of target words/phrases with source words/phrases for lexical se"
W07-0413,P01-1067,0,0.0597224,"e the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words—those permitted by the size of the phrase. Most of the state–of–the–art machine translation systems use these phrase–level associations in conjunction with a target language model to produce the target sentence. There is relatively little emphasis on (global) lexical reordering other than the local re-orderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax–based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present three models for doing discriminative machine translation using global lexical selection and lexical reordering. Machine translation of a source language sentence involves selecting appropriate target language words and ordering the selected words to form a well-formed target language sentence. Most of the previous work on statistical machine translation relies on (local) associations of target words/phrases with source words/phrases for lexical selection. In contrast, in this paper, we present"
W07-0413,C98-1006,1,\N,Missing
W08-1133,C00-1007,1,0.744769,"or both TUNA domains, while the use of recency constraints was not as effective for TUNA-style tasks. We then modified Dale and Reiter’s classic attribute selection algorithm (Dale and Reiter, 1995) to model speakerspecific constraints, and found performance gains in this more greedy approach as well. Then we looked at surface realization for referring expression generation. There are several approaches to surface realization described in the literature (Reiter and Dale, 2000) ranging from hand-crafted template-based realizers to data-driven syntax-based realizers (Langkilde and Knight, 2000; Bangalore and Rambow, 2000). Template-based realization involves the insertion of attribute values into predetermined templates. Data-driven syntax-based methods use syntactic relations between words (including long-distance relations) for word ordering. Other data-driven techniques exhaustively generate possible realizations with recourse to syntax in as much as it is reflected in local n-grams. Such techniques have the advantage of being robust although they are inadequate to capture long-range dependencies. In this paper, we explore three techniques for the task of referring expression generation that are different h"
W08-1133,2007.mtsummit-ucnlg.14,0,0.121161,"ng expression generation. However, most of this work did not take into account: a) stylistic differences between speakers; or b) trainable surface realization approaches that combine semantic and word order information. In this paper we describe and evaluate several end-to-end referring expression generation algorithms that take into consideration speaker style and use data-driven surface realization techniques. 1 Introduction There now exist numerous general-purpose algorithms for attribute selection used in referring expression generation (e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Belz and Gatt, 2007)). However, these algorithms by-and-large focus on the algorithmic aspects of referring expression generation rather than on psycholinguistic factors that influence language production. For example, we know that humans exhibit individual style differences during language production that can be quite pronounced (e.g. (Belz, 2007)). We also know that the language production process is subject to lexical priming, which means that words and concepts that have been used recently are likely to appear again (Levelt, 1989). In this paper, we first explore the impact of individual style and priming on"
W08-1133,N07-1021,0,0.0229201,"sideration speaker style and use data-driven surface realization techniques. 1 Introduction There now exist numerous general-purpose algorithms for attribute selection used in referring expression generation (e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Belz and Gatt, 2007)). However, these algorithms by-and-large focus on the algorithmic aspects of referring expression generation rather than on psycholinguistic factors that influence language production. For example, we know that humans exhibit individual style differences during language production that can be quite pronounced (e.g. (Belz, 2007)). We also know that the language production process is subject to lexical priming, which means that words and concepts that have been used recently are likely to appear again (Levelt, 1989). In this paper, we first explore the impact of individual style and priming on attribute selection for referring expression generation. To get an idea of the potential improvement when modeling these factors, we implemented a version of full brevity search (Dale, 1992) that uses speaker-specific constraints, and another version that also uses recency constraints. We found that using speaker-specific constr"
W08-1133,W05-0831,0,0.0208369,".01 .01 0 .02 Table 2: Results for realization rate speaker constraints, we again see a performance jump, although compared to the best possible case (full brevity) there is still room for improvement. Table 1: Results for attribute selection Unfortunately, the number of states of the minimal permutation automaton of even a linear automata (finite-state machine representation of a string) grows exponentially with the number of words of the string. So, instead of creating a full permutation automaton, we choose to constrain permutations to be within a local window of adjustable size (also see (Kanthak et al., 2005)). 4 Attribute Selection Experiments Data Preparation The training data were used to build the models outlined above. The development data were then processed one-by-one. For our final submissions, we use training and development data to build our models. Results Table 1 shows the results for variations of full brevity. As we would expect, all approaches achieve a perfect score on uniqueness. For both corpora, we see a large performance jump when we use speaker constraints. However, when we incorporate recency constraints as well performance declines slightly. We think this is due to two facto"
W08-1133,J03-1003,0,0.2085,"Missing"
W08-1133,A00-2023,0,0.0173701,"d to big performance gains for both TUNA domains, while the use of recency constraints was not as effective for TUNA-style tasks. We then modified Dale and Reiter’s classic attribute selection algorithm (Dale and Reiter, 1995) to model speakerspecific constraints, and found performance gains in this more greedy approach as well. Then we looked at surface realization for referring expression generation. There are several approaches to surface realization described in the literature (Reiter and Dale, 2000) ranging from hand-crafted template-based realizers to data-driven syntax-based realizers (Langkilde and Knight, 2000; Bangalore and Rambow, 2000). Template-based realization involves the insertion of attribute values into predetermined templates. Data-driven syntax-based methods use syntactic relations between words (including long-distance relations) for word ordering. Other data-driven techniques exhaustively generate possible realizations with recourse to syntax in as much as it is reflected in local n-grams. Such techniques have the advantage of being robust although they are inadequate to capture long-range dependencies. In this paper, we explore three techniques for the task of referring expression ge"
W08-1133,W04-3308,0,0.0199772,"a template for an input attribute list it is quite likely to be coherent. At generation time, we find all possible realizations of each attribute in the input attribute set, and fill in each possible template with each combination of the attribute realizations. We report results for two versions of this realizer: one with speakerspecific lexicon and templates (Template-S), and one without (Template). Dependency-Based Realizer To construct our dependency-based realizer, we first parse all the word strings from the training data using the dependency parser described in (Bangalore et al., 2005; Nasr and Rambow, 2004). Then, for every pair of words wi , wj that occur in the same referring expression (RE) in the training data, we compute: f req(i &lt; j), the frequency with which wi precedes wj in any RE; f req(i = j − 1), the frequency with which wi immediately precedes wj in any RE; f req(dep(wi , wj ) ∧ i &lt; j), the frequency with which wi depends on and precedes wj in any RE, and f req(dep(wi , wj ) ∧ j &lt; i), the frequency with which wi depends on and follows wj in any RE. At generation time, we find all possible realizations of each attribute in the input attribute set, and for each combination of attribut"
W08-2120,A00-2023,0,0.0252409,"Natural Language Learning, pages 151–158 Manchester, August 2008 cency constraints was not as effective for TUNAstyle tasks. We then modified Dale and Reiter’s classic attribute selection algorithm (Dale and Reiter, 1995) to model individual differences in style, and found performance gains in this more greedy approach as well. Then, we look at surface realization for referring expression generation. There are several approaches to surface realizations described in the literature (Reiter and Dale, 2000) ranging from hand-crafted template-based realizers to data-driven syntax-based realizers (Langkilde and Knight, 2000; Bangalore and Rambow, 2000). Template-based realization provides a straightforward method to fill out pre-defined templates with the current attribute values. Data-driven syntaxbased methods employ techniques that incorporate the syntactic relations between words which can potentially go beyond local adjacency relations. Syntactic information also helps in eliminating ungrammatical sentence realizations. At the other extreme, there are techniques that exhaustively generate possible realizations with recourse to syntax in as much as it is reflected in local n-grams. Such techniques have the a"
W08-2120,W04-3308,0,0.0243709,"s a weighted finite-state automaton. The weights are computed from the prior probability of each template and the prior probability of each lexical item realizing an attribute (Equation 2). We have two versions of this realizer: one with speaker-specific lexicons and templates (Template-S), and one without (Template). We report results for both. P (T |AS) = ! t P (t|AS) ∗ &quot;! a∈t P (l|a, t) (2) l 4.1.2 Dependency-Based Realizer To construct our dependency-based realizer, we first parse all the word strings from the training data using the dependency parser described in (Bangalore et al., 2005; Nasr and Rambow, 2004). Then, for every pair of words wi , wj that occur in the same referring expression (RE) in the training data, we compute: f req(i &lt; j), the frequency with which wi precedes wj in any RE; f req(dep(wi , wj ) ∧ i &lt; j), the frequency with which wi depends on and precedes wj in any RE, and f req(dep(wi , wj )∧j &lt; i), the frequency with which wi depends on and follows wj in any RE. At generation time, we find all possible realizations of each attribute value in the input attribute set, and for each combination of attribute realizations, we find the most likely set of dependencies and precedences g"
W08-2120,2001.mtsummit-papers.68,0,0.0265149,"e number of words that would have to be added, deleted, or replaced in order to transform the generated referring expression into the one produced by the human. As used in the REG 2008 shared challenge, it is unnormalized, so its values range from zero up. Accuracy (ACC) is binary-valued: 1 if the generated referring expression is identical to that produced by the human (after spelling correction and normalization), and 0 otherwise. Bleu is an n-gram based metric that counts the number of 1, 2 and 3 grams shared between the generated string and one or more (preferably more) reference strings (Papenini et al., 2001). Bleu values are normalized and range from 0 (no match) to 1 (perfect match). Finally, the NIST metric is a variation on the Bleu metric that, among other things, weights rare n-grams higher than frequently-occurring ones (Doddington, 2002). NIST values are unnormalized. 156 SED ACC Bleu NIST Furniture FB-sf DR-sf FB-sf DR-sf FB-sf DR-sf FB-sf DR-sf Permute&Rank 3.97 4.22 0.09 0.06 .291 .242 3.82 3.32 Dependency 4.80 5.03 0.04 0.03 .193 .105 3.32 2.46 Dependency-S 4.71 4.88 0.06 0.04 .201 .157 3.74 3.26 Template 3.89 4.56 0.09 0.05 .283 .213 3.48 3.22 Template-S 3.26 3.90 0.19 0.12 .362 .294"
W08-2120,C00-1007,1,0.736757,"pages 151–158 Manchester, August 2008 cency constraints was not as effective for TUNAstyle tasks. We then modified Dale and Reiter’s classic attribute selection algorithm (Dale and Reiter, 1995) to model individual differences in style, and found performance gains in this more greedy approach as well. Then, we look at surface realization for referring expression generation. There are several approaches to surface realizations described in the literature (Reiter and Dale, 2000) ranging from hand-crafted template-based realizers to data-driven syntax-based realizers (Langkilde and Knight, 2000; Bangalore and Rambow, 2000). Template-based realization provides a straightforward method to fill out pre-defined templates with the current attribute values. Data-driven syntaxbased methods employ techniques that incorporate the syntactic relations between words which can potentially go beyond local adjacency relations. Syntactic information also helps in eliminating ungrammatical sentence realizations. At the other extreme, there are techniques that exhaustively generate possible realizations with recourse to syntax in as much as it is reflected in local n-grams. Such techniques have the advantage of being robust alth"
W08-2120,P04-1052,0,0.0122269,"ibution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. parts of an NLG system. Traditionally, this problem is split into two parts. The task of selecting the attributes to use in referring to an entity is the attribute selection task, performed during content planning or sentence planning. The actual construction of the referring expression is part of surface realization. There now exist numerous general-purpose algorithms for attribute selection (e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Belz and Gatt, 2007; Siddharthan and Copestake, 2004)). However, these algorithms by-and-large focus on the algorithmic aspects of referring expression generation rather than on psycholinguistic factors that influence language production. For example, we know that humans exhibit individual differences in language production that can be quite pronounced (e.g. (Belz, 2007)). We also know that the language production process is subject to lexical priming, which means that words and concepts that have been used recently are likely to appear again (Levelt, 1989). In this paper, we look at attribute selection and surface realization for referring expr"
W08-2120,2007.mtsummit-ucnlg.14,0,0.0219509,"Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. parts of an NLG system. Traditionally, this problem is split into two parts. The task of selecting the attributes to use in referring to an entity is the attribute selection task, performed during content planning or sentence planning. The actual construction of the referring expression is part of surface realization. There now exist numerous general-purpose algorithms for attribute selection (e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Belz and Gatt, 2007; Siddharthan and Copestake, 2004)). However, these algorithms by-and-large focus on the algorithmic aspects of referring expression generation rather than on psycholinguistic factors that influence language production. For example, we know that humans exhibit individual differences in language production that can be quite pronounced (e.g. (Belz, 2007)). We also know that the language production process is subject to lexical priming, which means that words and concepts that have been used recently are likely to appear again (Levelt, 1989). In this paper, we look at attribute selection and surf"
W08-2120,N07-1021,0,0.0151406,"ng or sentence planning. The actual construction of the referring expression is part of surface realization. There now exist numerous general-purpose algorithms for attribute selection (e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Belz and Gatt, 2007; Siddharthan and Copestake, 2004)). However, these algorithms by-and-large focus on the algorithmic aspects of referring expression generation rather than on psycholinguistic factors that influence language production. For example, we know that humans exhibit individual differences in language production that can be quite pronounced (e.g. (Belz, 2007)). We also know that the language production process is subject to lexical priming, which means that words and concepts that have been used recently are likely to appear again (Levelt, 1989). In this paper, we look at attribute selection and surface realization for referring expression generation using the TUNA corpus 1 , an annotated corpus of human-produced referring expressions that describe furniture and people. We first explore the impact of individual style and priming on attribute selection for referring expression generation. To get an idea of the potential improvement when modeling th"
W08-2120,W05-0831,0,0.0196036,"possible surface realizations. In general, the number of states of the minimal permutation automaton of even a linear automaton (finite-state representation of a string) grows exponentially with the number of words of the string. Although creating the full permutation automaton for full natural language generation tasks could be computationally prohibitive, most attribute sets in our two domains contain no more than five attributes. So we choose to explore the full permutation space. A more general approach might constrain permutations to be within a local window of adjustable size (also see (Kanthak et al., 2005)). Figure 3 shows the minimal permutation automaton for an input sequence of 4 words and a window size of 2. Each state of the automaton is indexed by a bit vector of size equal to the number of words/phrases of the target sentence. Each bit of the bit vector is set to 1 if the word/phrase in that bit position is used on any path from the initial to the current state. The next word for permutation from a given state is restricted to be within the window size (2 in our case) positions counting from the first as-yet uncovered position in that state. For example, the state indexed with vector “10"
W08-2120,J03-1003,0,0.406206,"Missing"
W08-2120,P02-1040,0,\N,Missing
W10-3805,P07-1020,1,0.911222,"word alignment information from the IBM models and train source-target phrase pairs for lexical selection (phrase-table) and distortions of source phrases (reordering-table). These models are still relatively local, as the target phrases are tightly associated with their corresponding source phrases. In contrast to a phrase-based model, a discriminative model has the power to integrate much richer contextual information into the training model. Contextual information is extremely useful in making lexical selections of higher quality, as illustrated by the models for Global Lexical Selection (Bangalore et al., 2007; Venkatapathy and In this paper, we present an approach to statistical machine translation that combines the power of a discriminative model (for training a model for Machine Translation), and the standard beam-search based decoding technique (for the translation of an input sentence). A discriminative approach for learning lexical selection and reordering utilizes a large set of feature functions (thereby providing the power to incorporate greater contextual and linguistic information), which leads to an effective training of these models. This model is then used by the standard state-of-art"
W10-3805,J96-1002,0,0.0154432,"they use generic feature functions such as language model, cooccurence features such as presence of a lexical relationship in the lexicon. Their search algorithm limited the use of complex features. Direct Translation Model 2 (DTM2) (Ittycheriah and Roukos, 2007) expresses the phrasebased translation task in a unified log-linear probabilistic framework consisting of three components: 1. a prior conditional distribution P0 2. a number of feature functions Φi () that capture the effects of translation and language model 3. the weights of the features λi that are estimated using MaxEnt training (Berger et al., 1996) as shown in equation 2. P r(e|f ) = X P0 (e, j|f ) exp λi Φi (e, j, f ) (2) Z i In the above equation, j is the skip reordering factor for the phrase pair captured by Φi () and represents the jump from the previous source word. Z represents the per source sentence normalization term (Hassan et al., 2009). While a uniform prior on the set of futures results in a maximum entropy model, choosing other priors output a minimum divergence models. Normalized phrase count has been used as the prior P0 in the DTM2 model. The following decision rule is used to obtain optimal translation. pλ (y|x) = 3 e"
W10-3805,J93-2003,0,0.0200613,"Missing"
W10-3805,D09-1123,0,0.0314242,"Missing"
W10-3805,N07-1008,0,0.119677,"ative set-up for natural language understanding (and MT). They use a slightly modified equation (in comparison to IBM models) as shown in equation 1. In equation 1, they consider the translation model from f → e (p(e|f )), instead of the theoretically sound (after the application of Bayes’ rule), e → f (p(f |e)) and use grammatical features such as the presence of equal number of 35 verbs forms etc. eˆ = arg max pT M (e|f ) ∗ pLM (e) e a word aligned parallel corpora a set of minimal phrases such that no two phrases overlap with each other (Hassan et al., 2009). The decoding strategy in DTM2 (Ittycheriah and Roukos, 2007) is similar to a phrase-based decoder except that the score of a particular translation block is obtained from the maximum entropy model using the set of feature functions. In our approach, instead of providing the complete scoring function ourselves, we compute the parameters needed by a phrase based decoder, which in turn uses these parameters appropriately. In comparison with the DTM2, we also use minimal nonoverlapping blocks as the entries in the phrase table that we generate. Xiong et al. (2006) present a phrase reordering model under the ITG constraint using a maximum entropy framework."
W10-3805,D07-1091,0,0.032843,"ce construction and scoring approach (see section 4.1 for LCS Decoding) and the phrasebased decoding approach (see section 4.2). Another advantage of using a discriminative approach to construct the phrase table and the reordering table is the flexibility it provides to incorporate linguistic knowledge in the form of additional feature functions. In the past, factored phrase-based approaches for Machine Translation have allowed the use of linguistic feature functions. But, they are still bound by the locality of context, and definition of a fixed structure of dependencies between the factors (Koehn and Hoang, 2007). Furthermore, factored phrasebased approaches place constraints both on the type and number of factors that can be incorporated into the training. In this paper, though we do not extensively test this aspect, we show that using syntactic feature functions does improve the performance of our approach, which is likely to improve when much richer syntactic feature functions (such as information about the parse structure) are incorporated in the model. As the training model in a standard phrasebased system is relatively impoverished with respect to contextual/linguistic information, integration o"
W10-3805,N03-1017,0,0.0529195,"he standard EM Algorithm. The parameters used in these models are extremely restrictive, that is, a simple, small and closed set of feature functions is used to represent the translation process. Also, these feature functions are local and are word based. In spite of these limitations, these models perform very well for the task of word-alignment because of the restricted search space. However, they perform poorly during decoding (or translation) because of their limitations in the context of a much larger search space. To handle the contextual information, phrasebased models were introduced (Koehn et al., 2003). The phrase-based models use the word alignment information from the IBM models and train source-target phrase pairs for lexical selection (phrase-table) and distortions of source phrases (reordering-table). These models are still relatively local, as the target phrases are tightly associated with their corresponding source phrases. In contrast to a phrase-based model, a discriminative model has the power to integrate much richer contextual information into the training model. Contextual information is extremely useful in making lexical selections of higher quality, as illustrated by the mode"
W10-3805,P07-2045,0,0.0601833,"nd In this paper, we present an approach to statistical machine translation that combines the power of a discriminative model (for training a model for Machine Translation), and the standard beam-search based decoding technique (for the translation of an input sentence). A discriminative approach for learning lexical selection and reordering utilizes a large set of feature functions (thereby providing the power to incorporate greater contextual and linguistic information), which leads to an effective training of these models. This model is then used by the standard state-of-art Moses decoder (Koehn et al., 2007) for the translation of an input sentence. We conducted our experiments on Spanish-English language pair. We used maximum entropy model in our experiments. We show that the performance of our approach (using simple lexical features) is comparable to that of the state-of-art statistical MT system (Koehn et al., 2007). When additional syntactic features (POS tags in this paper) are used, there is a boost in the performance which is likely to improve when richer syntactic features are incorporated in the model. 34 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Trans"
W10-3805,P02-1038,0,0.094828,"te the approach. The paper is organized in the following sections. Section 2 presents the related work. In section 3, we describe the training of our model. In section 4, we present the decoding approaches (both LCS and phrase-based decoder). We describe the data used in our experiments in section 5. Section 6 consists of the experiments and results. Finally we conclude the paper in section 7. 2 Related Work In this section, we present approaches that are directly related to our approach. In Direct Translation Model (DTM) proposed for statistical machine translation by (Papineni et al., 1998; Och and Ney, 2002), the authors present a discriminative set-up for natural language understanding (and MT). They use a slightly modified equation (in comparison to IBM models) as shown in equation 1. In equation 1, they consider the translation model from f → e (p(e|f )), instead of the theoretically sound (after the application of Bayes’ rule), e → f (p(f |e)) and use grammatical features such as the presence of equal number of 35 verbs forms etc. eˆ = arg max pT M (e|f ) ∗ pLM (e) e a word aligned parallel corpora a set of minimal phrases such that no two phrases overlap with each other (Hassan et al., 2009)"
W10-3805,W99-0604,0,0.185333,"Missing"
W10-3805,P02-1040,0,0.0783539,"th the lowest score is considered the best possible target sentence for the given source sentence. Using this decoder, we conducted experiments on the development set by varying threshold values and the size of the permutation window. The best parameter values obtained using the development set were used for decoding the test corpus. 4.2 Dataset No. of sentences 200000 2525 2051 200000 Source 59591 10629 8888 n.a Target 36886 8905 7750 36886 Table 1: Corpus statistics for Spanish-English corpus. 6 Experiments and Results The output of our experiments was evaluated using two metrics, (1) BLEU (Papineni et al., 2002), and (2) Lexical Accuracy (LexAcc). Lexical accuracy measures the similarity between the unordered bag of words in the reference sentence Decoding with Moses Decoder In this approach, the phrase-table and the reordering-table are constructed using the dis39 against the unordered bag of words in the hypothesized translation. Lexical accuracy is a measure of the fidelity of lexical transfer from the source to the target sentence, independent of the syntax of the target language (Venkatapathy and Bangalore, 2009). We report lexical accuracies to show the performance of LCS decoding in comparison"
W10-3805,W07-0413,1,0.887549,"Missing"
W10-3805,P06-1066,0,0.0817896,"Missing"
W14-4006,N03-1017,0,0.016323,"core an improvement over the earlier baseline of 20.04 BLEU points. Baseline Components Baseline Model and Corpus Statistics We have used the ILCI corpora (Choudhary and Jha, 2011) for our experiments, which contains English-Hindi parallel sentences from tourism and health domain. We randomly divided the data into training (48970), development (500) and testing (500) sentences and for language modelling we used news corpus of English which is distributed as a part of WMT’14 translation task. The data is about 3 million sentences which also contains MT training data. We trained a phrase based (Koehn et al., 2003) MT system using the Moses toolkit with wordalignments extracted from GIZA++ (Och and Ney, 2000). We have used the SRILM (Stolcke and others, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) for training a language model for the first stage of decoding. The result of this baseline system is shown in Table 1. 3.2 Number of Development Sentences 500 4 Re-Ranking Experiments In this section, we describe the results of reranking the output of the translation model using Recurrent Neural Networks (RNN) based language models using the same data which is used for language modelling in the basel"
W14-4006,P07-1092,0,0.030931,"med entities, combination forms (e.g. widebody) and abbreviations. Apart from these issues, Hindi being a low-resourced language in terms of parallel corpora suffers from data sparsity. In the second part of the paper, we address the problem of data sparsity with the help of English WordNet (EWN) for English-Hindi PB-SMT. We increase the coverage of content words (excluding Named-Entities) by incorporating sysnset information in the source sentences. Combining Machine Translation (MT) systems has become an important part of statistical MT in past few years. Works by (Razmara and Sarkar, 2013; Cohn and Lapata, 2007) have shown that there is an increase in phrase coverage when combining different systems. To get more coverage of unigrams in phrase-table, we have explored system combination approaches to combine models trained with synset information and without synset information. We have explored two methodologies for system combination based on confusion matrix(dynamic) (Ghannay et al., 2014) and mixing models (Cohn and Lapata, 2007). 3 3.1 Number of Evaluation Sentences 500 BLEU 20.04 Table 1: Baseline Scores for Phrase-based Moses Model shown that pre-ordering source language to conform to target lang"
W14-4006,P05-1066,0,0.154505,"Missing"
W14-4006,P00-1056,0,0.19918,"Model and Corpus Statistics We have used the ILCI corpora (Choudhary and Jha, 2011) for our experiments, which contains English-Hindi parallel sentences from tourism and health domain. We randomly divided the data into training (48970), development (500) and testing (500) sentences and for language modelling we used news corpus of English which is distributed as a part of WMT’14 translation task. The data is about 3 million sentences which also contains MT training data. We trained a phrase based (Koehn et al., 2003) MT system using the Moses toolkit with wordalignments extracted from GIZA++ (Och and Ney, 2000). We have used the SRILM (Stolcke and others, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) for training a language model for the first stage of decoding. The result of this baseline system is shown in Table 1. 3.2 Number of Development Sentences 500 4 Re-Ranking Experiments In this section, we describe the results of reranking the output of the translation model using Recurrent Neural Networks (RNN) based language models using the same data which is used for language modelling in the baseline models. Unlike traditional n-gram based discrete language models, RNN do not make the Markov"
W14-4006,W14-3308,0,0.0166938,"eural Network based language model with different morphological features and second, we explore the use of lexical resources such as WordNet to overcome sparsity of content words. 1 2 Related Work In this paper, we present our efforts of reranking the n-best hypotheses produced by a PBMT (Phrase-Based MT) system using RNNLM (Mikolov et al., 2010) in the context of an EnglishHindi SMT system. The re-ranking task in machine translation can be defined as re-scoring the n-best list of translations, wherein a number of language models are deployed along with features of source or target language. (Dungarwal et al., 2014) described the benefits of re-ranking the translation hypothesis using simple n-gram based language model. In recent years, the use of RNNLM have shown significant improvements over the traditional n-gram models (Sundermeyer et al., 2013). (Mikolov et al., 2010) and (Liu et al., 2014) have shown significant improvements in speech recognition accuracy using RNNLM . Shi (2012) also showed the benefits of using RNNLM with contextual and linguistic features. We have also explored the use of morphological features (Hindi being a morphologically rich language) in RNNLM and deduced that these feature"
W14-4006,I08-1067,0,0.0140143,"t synset IDs. 5.2 Combining MT Models 6.1 Combination based on confusion networks We used the tool MANY (Barrault, 2010) for system combination. However, since the tool is configured to work with TERp evaluation metric, we modified it to use METEOR (Gupta et al., 2010) metric since it has been shown by (Kalyani et al., 2014), that METEOR evaluation metric is better correlated to human evaluation for morphologically rich Indian Languages. Factored Model Techniques such as factored modelling (Koehn and Hoang, 2007) are quite beneficial for Translation from English to Hindi language as shown by (Ramanathan et al., 2008). When we replace words in a source sentence with the synset IDˆas, we tend to lose morphological information associated with that word. We add inflections as features in a factored SMT model to minimize the impact of this replacement. 6.2 Linearly Interpolated Combination In this approach, we combined phrase-tables of the two models (Eng (sysnset) - Hindi and Baseline) using linear interpolation. We combined the two models with uniform weights – 0.5 for each model, in our case. We again tuned this model with the new interpolated phrase-table using standard algorithm MERT. We show the results"
W14-4006,W14-1002,0,0.0339724,"Missing"
W14-4006,I13-1029,0,0.0203587,"ror in source language, named entities, combination forms (e.g. widebody) and abbreviations. Apart from these issues, Hindi being a low-resourced language in terms of parallel corpora suffers from data sparsity. In the second part of the paper, we address the problem of data sparsity with the help of English WordNet (EWN) for English-Hindi PB-SMT. We increase the coverage of content words (excluding Named-Entities) by incorporating sysnset information in the source sentences. Combining Machine Translation (MT) systems has become an important part of statistical MT in past few years. Works by (Razmara and Sarkar, 2013; Cohn and Lapata, 2007) have shown that there is an increase in phrase coverage when combining different systems. To get more coverage of unigrams in phrase-table, we have explored system combination approaches to combine models trained with synset information and without synset information. We have explored two methodologies for system combination based on confusion matrix(dynamic) (Ghannay et al., 2014) and mixing models (Cohn and Lapata, 2007). 3 3.1 Number of Evaluation Sentences 500 BLEU 20.04 Table 1: Baseline Scores for Phrase-based Moses Model shown that pre-ordering source language t"
W14-4006,P07-1040,0,0.0285939,"ty 5.1.1 Intra-Category Sense Selection First Sense: Among the different senses,we select the first sense listed in EWN corresponding to the POS-tag of a given lexical item. The choice is motivated by our observation that the senses of a We have used the HCU morph-analyzer. 53 6 lexical item are ordered in the descending order of their frequencies of usage in the lexical resource. Combining Machine translation (MT) systems has become an important part of Statistical MT in the past few years. There are two dominant approaches. (1) a system combination approach based on confusion networks (CN) (Rosti et al., 2007), which can work dynamically in combining the systems. (2) Combine the models by linearly interpolating and then using MERT to tune the combined system. Merged Sense: In this approach, we merge all the senses listed in EWN corresponding to the POS-tag of the given lexical item. The motivation behind this strategy is that the senses in the EWN for a particular word-POS pair are too finely classified resulting in classification of words that may represent the same concept, are classified into different synsets. For example : travel and go can mean the same concept in a similar context but the fi"
W14-4006,2010.amta-papers.13,0,0.0608694,"Missing"
W14-4006,P03-1054,0,0.00744186,"d on confusion matrix(dynamic) (Ghannay et al., 2014) and mixing models (Cohn and Lapata, 2007). 3 3.1 Number of Evaluation Sentences 500 BLEU 20.04 Table 1: Baseline Scores for Phrase-based Moses Model shown that pre-ordering source language to conform to target language word order significantly improves translation quality (Collins et al., 2005). We created a re-ordering module for transforming an English sentence to be in the Hindi order based on reordering rules provided by Anusaaraka (Chaudhury et al., 2010). The reordering rules are based on parse output produced by the Stanford Parser (Klein and Manning, 2003). The transformation module requires the text to contain only surface form of words, however, we extended it to support surface form along with its factors such as lemma and Part of Speech (POS). Input : the girl in blue shirt is my sister Output : in blue shirt the girl is my sister. Hindi : neele shirt waali ladki meri bahen hai ( blue) ( shirt) (Mod)(girl)(my)(sister)(Vaux) With this transformation, the English sentence is structurally closer to the Hindi sentence which leads to better phrase alignments. The model trained with the transformed corpus produces a new baseline score of 21.84 BL"
W14-4006,D07-1091,0,\N,Missing
W14-4211,W14-1002,0,0.0239039,"Missing"
W14-4211,P08-2021,0,0.0248758,"rm weights. The combined triangulated phrase-table and direct src-tgt phrase table is then combined using uniform weights. In other words, we combined all the three systems, Ban-Mar, Ban-Hin-Mar, and Ban-Guj-Mar with 0.5, 0.25 and 0.25 weights respectively. This weight distribution reflects the intuition that the direct model is less noisy than the triangulated models. Combining different triangulated models and the direct model Combining Machine translation (MT) systems has become an important part of Statistical MT in the past few years. There have been several works by (Rosti et al., 2007; Karakos et al., 2008; Leusch and Ney, 2010); We followed two approaches In the experiments below, both weight settings produced comparable results. Since we performed triangulation only through two languages, we could not determine which approach would perform better. An ideal approach will be to train the weights for each system for each language pair using standard tuning algorithms such as MERT (Zaidan, 2009). 1. A system combination based on confusion network using open-source tool kit MANY (Barrault, 2010), which can work dynamically in combining the systems 4.2.3 In order to compare the approaches on our da"
W14-4211,P07-2045,0,0.0055531,"our experiment, the baseline translation model used was the direct system between the source and target languages which was trained on the same amount of data as the triangulated models. The parallel corpora for 4 Indian languages namely Hindi (hn), Marathi (mt), Gujarati (gj) and Bangla (bn) was taken from Indian Languages Corpora Initiative (ILCI) (Choudhary and Jha, 2011) . The parallel corpus used in our experiments belonged to two domains - health and tourism and the training set consisted of 28000 sentences. The development and evaluation set contained 500 sentences each. We used MOSES (Koehn et al., 2007) to train the baseline Phrase-based SMT system for all the language pairs on the above mentioned parallel corpus as training, development and evaluation data. Trigram language models were trained using SRILM (Stolcke and others, 2002). Table 1 below shows the BLEU score for all the trained pairs. 4.1 Language Pair bn-mt mt-bn bn-gj gj-mt gj-bn mt-gj hn-mt hn-bn bn-hn mt-hn hn-gj gj-hn Phrase-table triangulation Our emphasis is on building an enhanced phrase table that incorporates the translation phrase tables of different models. This combined phrase table will be used by the decoder during t"
W14-4211,P07-1092,0,0.0487476,"not sufficient to create high quality SMT systems. This paper aims at improving SMT systems trained on small parallel corpora using various recently developed techniques in the field of SMTs. Triangulation is a technique which has been found to be very useful in improving the translations when multilingual parallel corpora are present. 2 Related Works There are various works on combining the triangulated models obtained from different pivots with the direct model resulting in increased confidence score for translations and increased coverage by (Razmara and Sarkar, 2013; Ghannay et al., 2014; Cohn and Lapata, 2007). Among these techniques we explored two of the them. The first one is the technique based on the confusion matrix (dynamic) (Ghannay et al., 2014) and the other one is based on mixing the models as explored by (Cohn and Lapata, 2007). The paper also discusses the better choice of combination technique 85 Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 85–91, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 4 among these two when we have limitations on training data which in our case was small and restricted to a smal"
W14-4211,W10-1747,0,0.0140444,"ed triangulated phrase-table and direct src-tgt phrase table is then combined using uniform weights. In other words, we combined all the three systems, Ban-Mar, Ban-Hin-Mar, and Ban-Guj-Mar with 0.5, 0.25 and 0.25 weights respectively. This weight distribution reflects the intuition that the direct model is less noisy than the triangulated models. Combining different triangulated models and the direct model Combining Machine translation (MT) systems has become an important part of Statistical MT in the past few years. There have been several works by (Rosti et al., 2007; Karakos et al., 2008; Leusch and Ney, 2010); We followed two approaches In the experiments below, both weight settings produced comparable results. Since we performed triangulation only through two languages, we could not determine which approach would perform better. An ideal approach will be to train the weights for each system for each language pair using standard tuning algorithms such as MERT (Zaidan, 2009). 1. A system combination based on confusion network using open-source tool kit MANY (Barrault, 2010), which can work dynamically in combining the systems 4.2.3 In order to compare the approaches on our data, we performed experi"
W14-4211,I13-1029,0,0.070733,"wever the number of parallel sentences is still not sufficient to create high quality SMT systems. This paper aims at improving SMT systems trained on small parallel corpora using various recently developed techniques in the field of SMTs. Triangulation is a technique which has been found to be very useful in improving the translations when multilingual parallel corpora are present. 2 Related Works There are various works on combining the triangulated models obtained from different pivots with the direct model resulting in increased confidence score for translations and increased coverage by (Razmara and Sarkar, 2013; Ghannay et al., 2014; Cohn and Lapata, 2007). Among these techniques we explored two of the them. The first one is the technique based on the confusion matrix (dynamic) (Ghannay et al., 2014) and the other one is based on mixing the models as explored by (Cohn and Lapata, 2007). The paper also discusses the better choice of combination technique 85 Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 85–91, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 4 among these two when we have limitations on training data which"
W14-4211,P07-1040,0,0.0193981,"se-table using uniform weights. The combined triangulated phrase-table and direct src-tgt phrase table is then combined using uniform weights. In other words, we combined all the three systems, Ban-Mar, Ban-Hin-Mar, and Ban-Guj-Mar with 0.5, 0.25 and 0.25 weights respectively. This weight distribution reflects the intuition that the direct model is less noisy than the triangulated models. Combining different triangulated models and the direct model Combining Machine translation (MT) systems has become an important part of Statistical MT in the past few years. There have been several works by (Rosti et al., 2007; Karakos et al., 2008; Leusch and Ney, 2010); We followed two approaches In the experiments below, both weight settings produced comparable results. Since we performed triangulation only through two languages, we could not determine which approach would perform better. An ideal approach will be to train the weights for each system for each language pair using standard tuning algorithms such as MERT (Zaidan, 2009). 1. A system combination based on confusion network using open-source tool kit MANY (Barrault, 2010), which can work dynamically in combining the systems 4.2.3 In order to compare th"
W14-4211,N07-1061,0,0.0374704,"ntext. Each source phrase s is first translated to an intermediate (pivot) language i, and then to a target language t. This two stage translation process is termed as triangulation. Our basic approach involved making triangulated models by triangulating through different pivots and then interpolating triangulated models with the direct source-target model to make our combined model. In line with various previous works, we will be using multiple translation models to overcome the problems faced due to data sparseness and increase translational coverage. Rather than using sentence translation (Utiyama and Isahara, 2007) from source to pivot and then pivot to target, a phrase based translation model is built. Hence the main focus of our approach is on phrases rather than on sentences. Instead of using combination techniques on the output of several translation systems, we constructed a combined phrase table to be used by the decoder thus avoiding the additional inefficiencies observed while merging the output of various translation systems. Our method focuses on exploiting the availability of multi-parallel data, albeit small in size, to improve the phrase coverage and quality of our SMT system. Our approach"
W14-4211,D09-1141,0,\N,Missing
W16-3309,J99-2004,1,\N,Missing
W16-3626,P16-4012,1,0.821548,"in file generated by the web-based tool is composed of about fifteen rules responsible for (1) updating the slot values given the user inputs, (2) selecting the most appropriate system actions based on the current state, and (3) mapping these high-level actions to concrete system responses. The (probability and utility) parameters of these rules are initially fixed to reasonable System We rely on OpenDial as underlying framework (P. Lison, 2015) for dialogue management. OpenDial has been previously used for human–robot interactions, in-car driving assistants, and intelligent tutoring systems (Lison and Kennington, 2016). It is also a popular platform for teaching advanced courses on spoken dialogue systems. 2.1 Domain file Form-to-System Generation We created a web-based tool that generates an (XML-encoded) OpenDial dialogue domain from 217 Figure 2: Form for generating a dialogue with hotel information domain. defaults, but the user is free to modify the values of these parameters (or estimate them from data if such interaction data is available). The generated dialogue domain allows for mixed-initiative interactions where a user can choose any order and combination of fields for filling the form, including"
W16-3626,W10-4308,0,0.0274057,"s to easily specify and edit dialogue beIntroduction Dialogue systems research has witnessed the emergence of several important innovations in the last two decades, such as the development of information-state architectures (Larsson and Traum, 2000), the use of probabilistic reasoning to handle multiple state hypotheses (Young et al., 2013), the application of reinforcement learning to automatically derive dialogue policies from real or simulated interactions (Lemon and Pietquin, 2012), and the introduction of incremental processing methods to allow for more natural conversational behaviours (Schlangen et al., 2010). However, few of these innovations have so far made their way into dialogue systems deployed in commercial environments (Paek and Pieraccini, 2008; Williams, 2009). Indeed, the bulk of currently deployed dialogue systems continue 216 Proceedings of the SIGDIAL 2016 Conference, pages 216–219, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics a form specification. The web tool allows the dialogue designer to configure any number of form fields by specifying a field name, a corresponding semantic type, a natural language question for eliciting the field val"
W98-0102,P98-1006,1,0.854571,"Missing"
W98-0102,C94-1024,0,0.396881,"Missing"
W98-0102,C92-2065,0,0.0607762,"Missing"
W98-0102,C92-2066,0,0.04985,"Missing"
W98-0102,1997.iwpt-1.22,0,0.206326,"Missing"
W98-1122,J92-4003,0,0.0306469,"Missing"
W98-1122,P93-1024,0,0.171256,"t a t e s of America, etc..). A traditional word n-gram based language model can benefit greatly by using variable length units to capture long spanning dependencies, for any given order n of the model. Furthermore, language modeling based on longer length units is applicable to languages which do not have a predefined notion of a word. However, the problem of data sparseness is more acute in phrase-based language models than in word-based language models. Clustering words into classes has been used to overcome data sparseness in word-based language models (et.al., 1992; Kneser and Ney, 1993; Pereira et al., 1993; McCandless and Glass, 1993; Bellegarda et al., 1996; Saul and Pereira, 1997). Although the automatically acquired phrases can be later clustered into classes to overcome data sparseness, we present a novel approach 188 of combining the construction of classes during the acquisition of phrases. This integration of phrase acquisition and class construction results in the acquisition of phrase-grammar fragments. In (Gorin, 1996; Arai et al., 1997), grammar fragment acquisition is performed through Kullback-Liebler divergence techniques with application to topic classification from text. Althoug"
W98-1122,W97-0309,0,0.107987,"l can benefit greatly by using variable length units to capture long spanning dependencies, for any given order n of the model. Furthermore, language modeling based on longer length units is applicable to languages which do not have a predefined notion of a word. However, the problem of data sparseness is more acute in phrase-based language models than in word-based language models. Clustering words into classes has been used to overcome data sparseness in word-based language models (et.al., 1992; Kneser and Ney, 1993; Pereira et al., 1993; McCandless and Glass, 1993; Bellegarda et al., 1996; Saul and Pereira, 1997). Although the automatically acquired phrases can be later clustered into classes to overcome data sparseness, we present a novel approach 188 of combining the construction of classes during the acquisition of phrases. This integration of phrase acquisition and class construction results in the acquisition of phrase-grammar fragments. In (Gorin, 1996; Arai et al., 1997), grammar fragment acquisition is performed through Kullback-Liebler divergence techniques with application to topic classification from text. Although phrase-grammar fragments reduce the problem of data sparseness, they can res"
