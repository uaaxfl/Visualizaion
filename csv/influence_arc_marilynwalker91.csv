2020.acl-main.224,P18-1026,0,0.151557,"inspired by two lines of research: Seq2Seq generation and Graph2Seq generation. Graph neural networks (GNN) (Scarselli et al., 2009) aim to learn a latent state representation for each node in a graph by aggregating local information from its neighbors and the connected edges. Previous work has explored different ways of aggregating this local information, such as in GCNs (Kipf and Welling, 2017), gated graph neural networks (GGNNs) (Li et al., 2016), and Graph attention networks (GANs) (Veliˇckovi´c et al., 2018) Several works have applied GNNs instead of Seq2Seq models for text generation (Beck et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Guo et al., 2019; Li et al., 2019), and some of them outperform Seq2Seq models. However, Damonte and Cohen (2019) use both types of encoders and show that GCN can help LSTM capture reentrant structures and long-range dependencies, albeit on a different problem than ours. Our method also uses the two types of encoders but instead of using one to assist the other, it combines them simultaneously to capture their complementary effects. 2.1 3 2 Related Work Seq2Seq Generation Traditional data-to-text generation follows a planning and realization pipeline"
2020.acl-main.224,J16-2001,0,0.0138836,"2Seq models on certain tasks. However, these architectures broaden the structural gap between the encoder and decoder. That is, while the encoder receives the input data as a graph, the decoder has to create the output text as a linear chain structure. This structural gap increases the difficulty of establishing alignments between source and target, which is believed to play a key role in text generation. For example, in machine translation, pre-reordering the source words into a word order that is close to that of the target sentence can yield significant improvements in translation quality (Bisazza and Federico, 2016). This suggests a need for an intermediate “planning” stage (Reiter 2481 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2481–2491 c July 5 - 10, 2020. 2020 Association for Computational Linguistics and Dale, 2000; Puduppully et al., 2019) to help with organizing the output. In this work, we present a dual encoding model that is not only aware of the input graph structure but also incorporates a content planning stage. To encode the structural information in the input graph, we use a GCN based graph encoder. To narrow the ensuing structural gap, w"
2020.acl-main.224,W16-6626,0,0.0556513,"Missing"
2020.acl-main.224,N19-1366,0,0.0240177,"atent state representation for each node in a graph by aggregating local information from its neighbors and the connected edges. Previous work has explored different ways of aggregating this local information, such as in GCNs (Kipf and Welling, 2017), gated graph neural networks (GGNNs) (Li et al., 2016), and Graph attention networks (GANs) (Veliˇckovi´c et al., 2018) Several works have applied GNNs instead of Seq2Seq models for text generation (Beck et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Guo et al., 2019; Li et al., 2019), and some of them outperform Seq2Seq models. However, Damonte and Cohen (2019) use both types of encoders and show that GCN can help LSTM capture reentrant structures and long-range dependencies, albeit on a different problem than ours. Our method also uses the two types of encoders but instead of using one to assist the other, it combines them simultaneously to capture their complementary effects. 2.1 3 2 Related Work Seq2Seq Generation Traditional data-to-text generation follows a planning and realization pipeline (Reiter and Dale, 2000; Stent et al., 2004). More recent methods use Seq2Seq architecture (Sutskever et al., 2014) to combine planning and realization into"
2020.acl-main.224,D19-1052,0,0.235763,"Missing"
2020.acl-main.224,W17-3518,0,0.511375,"mories from GCN and RNN encoders, respectively, and yt−1 is the embedding of the previously generated token. The initial hidden state z0 is the summation of the final states from the two encoders. For the plan encoder, we use the final state HT of LSTM as the context representation. For the graph encoder, we use an average of all the hidden states following a two-layer perceptron to produce the final state. 2484 5 Experiments We conduct experiments to evaluate our Planner (Section 5.2) and the overall generation system (Section 5.3). 4 5.1 Dataset We conduct experiments on the WebNLG dataset (Gardent et al., 2017; Castro Ferreira et al., 2018) used in the WebNLG challenge.5 For each instance, the input is a set of up to 7 RDF triples from DBPedia, and the output is their text descriptions. Each triple-set is paired with a set of (up to three) humangenerated reference texts. Each reference is also paired with the order of triples it realized. We use them to train and evaluate our Planner. Overall, the dataset contains 9, 674 unique triple-sets and 25, 298 text references, and is divided into training, development, and test set. The test set contains two subsets, the S EEN part where the instances belon"
2020.acl-main.224,Q19-1019,0,0.0192416,"aph2Seq generation. Graph neural networks (GNN) (Scarselli et al., 2009) aim to learn a latent state representation for each node in a graph by aggregating local information from its neighbors and the connected edges. Previous work has explored different ways of aggregating this local information, such as in GCNs (Kipf and Welling, 2017), gated graph neural networks (GGNNs) (Li et al., 2016), and Graph attention networks (GANs) (Veliˇckovi´c et al., 2018) Several works have applied GNNs instead of Seq2Seq models for text generation (Beck et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Guo et al., 2019; Li et al., 2019), and some of them outperform Seq2Seq models. However, Damonte and Cohen (2019) use both types of encoders and show that GCN can help LSTM capture reentrant structures and long-range dependencies, albeit on a different problem than ours. Our method also uses the two types of encoders but instead of using one to assist the other, it combines them simultaneously to capture their complementary effects. 2.1 3 2 Related Work Seq2Seq Generation Traditional data-to-text generation follows a planning and realization pipeline (Reiter and Dale, 2000; Stent et al., 2004). More recent me"
2020.acl-main.224,P17-1019,0,0.0435122,"Figure 1: Illustration of the WebNLG challenge: the source data is an RDF graph and the target output is a text description of the graph. Introduction Data-to-text generation aims to create natural language text to describe the input data (Reiter and Dale, 2000). Here we focus on structured text input in a particular form such as a tree or a graph. Figure 1 shows an example where the input data is a mini knowledge graph, and the output text is its corresponding natural language description. Generating text from such data is helpful for many NLP tasks, such as question answering and dialogue (He et al., 2017; Liu et al., 2018; Moon et al., 2019). During generation, the structure of the data as well as the content inside the structure jointly determine the generated text. For example, the direction of the edge “capital” in Figure 1 determines that “London is the capital of U.K.” is an accurate description, but not vice versa. Current generation methods are based on sequence-to-sequence (Seq2Seq) encoder-decoder architecture (Sutskever et al., 2014), which requires the input data to be serialized as a sequence, resulting in a loss of structural information. Recent research has shown the utility of"
2020.acl-main.224,D19-1055,0,0.0151225,"forms all baselines on a variety of measures. on a variety of generation tasks (Lebret et al., 2016; Trisedya et al., 2018; Juraska et al., 2018; Reed et al., 2018). Despite the fair fluency and grammatical correctness, the generated text suffers from several problems such as repetition, omission, and unfaithfulness, which are less likely to happen in traditional planning-and-realization frameworks. Recent work has shown that neural models can also benefit from an explicit planning step to alleviate the above-mentioned problems. The input of these planners ranges from unstructured keyphrases (Hua and Wang, 2019) to structured tables (Puduppully et al., 2019) and graphs (Ferreira et al., 2019; Moryossef et al., 2019a). Our work also focuses on planning from graph data. Compared with previous methods, we show that our neural planning method is more feasible and accurate. More importantly, rather than serializing the planning and realization stages in a pipeline, our dual encoding method simultaneously captures information from the original data and the corresponding plan. 2.2 Graph2Seq Generation This work is inspired by two lines of research: Seq2Seq generation and Graph2Seq generation. Graph neural n"
2020.acl-main.224,N18-1014,1,0.848293,"rms the previous start-of-the-art on the generation task. The human evaluation confirms that the texts generated by our model are preferred over strong baselines. The contributions of this paper are three-fold: • We propose a dual encoding method to narrow the structural gap between data encoder and text decoder for data-to-text generation; • We propose a neural planner, which is more efficient and effective than previous methods; • Experiments show that our method outperforms all baselines on a variety of measures. on a variety of generation tasks (Lebret et al., 2016; Trisedya et al., 2018; Juraska et al., 2018; Reed et al., 2018). Despite the fair fluency and grammatical correctness, the generated text suffers from several problems such as repetition, omission, and unfaithfulness, which are less likely to happen in traditional planning-and-realization frameworks. Recent work has shown that neural models can also benefit from an explicit planning step to alleviate the above-mentioned problems. The input of these planners ranges from unstructured keyphrases (Hua and Wang, 2019) to structured tables (Puduppully et al., 2019) and graphs (Ferreira et al., 2019; Moryossef et al., 2019a). Our work also fo"
2020.acl-main.224,W06-3114,0,0.060289,".44 0.45 0.46 0.33 0.19 0.33 0.28 0.22 0.21 0.34 0.37 0.37 0.38 0.31 0.37 0.31 0.33 0.32 0.39 0.41 0.41 0.47 0.37 0.40 0.45 0.41 0.39 0.39 0.47 0.33 0.34 0.61 1.40 0.55 0.60 0.64 0.63 0.56 0.53 0.55 0.53 0.84 0.47 0.55 0.51 0.50 0.51 0.42 0.44 Table 2: Generation results evaluated by BLEU, METEOR, and TER. We compare our methods with different generation systems (SMT, Sequential NMT, Graph NMT, Pipeline). Both of our methods outperform all the baselines on all three measures. We highlight both results if there is no significant difference. perform the previous state-of-the-art (bootstrapping (Koehn and Monz, 2006), p &lt; 0.05). For the S EEN part, while no existing published work performed better than ADAPT, our P LAN E NC achieves a 3.83 performance gain on BLEU. It also outperforms the single GCN encoder by 8.52 BLEU, which confirms the advantage of the planning stage for bridging the structural gap between the encoder and decoder. For the U NSEEN part, P LAN E NC and D UAL E NC improve BLEU by 3.82 and 2.32 compared with the previous state-of-the-art. While it is difficult to distinguish the performance of D UA L E NC and P LAN E NC by automatic measures, our human experiments (see Section 5.3.4) show"
2020.acl-main.224,D16-1128,0,0.108369,"Missing"
2020.acl-main.224,P19-1479,0,0.019711,". Graph neural networks (GNN) (Scarselli et al., 2009) aim to learn a latent state representation for each node in a graph by aggregating local information from its neighbors and the connected edges. Previous work has explored different ways of aggregating this local information, such as in GCNs (Kipf and Welling, 2017), gated graph neural networks (GGNNs) (Li et al., 2016), and Graph attention networks (GANs) (Veliˇckovi´c et al., 2018) Several works have applied GNNs instead of Seq2Seq models for text generation (Beck et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Guo et al., 2019; Li et al., 2019), and some of them outperform Seq2Seq models. However, Damonte and Cohen (2019) use both types of encoders and show that GCN can help LSTM capture reentrant structures and long-range dependencies, albeit on a different problem than ours. Our method also uses the two types of encoders but instead of using one to assist the other, it combines them simultaneously to capture their complementary effects. 2.1 3 2 Related Work Seq2Seq Generation Traditional data-to-text generation follows a planning and realization pipeline (Reiter and Dale, 2000; Stent et al., 2004). More recent methods use Seq2Seq"
2020.acl-main.224,P18-1138,0,0.0190694,"ration of the WebNLG challenge: the source data is an RDF graph and the target output is a text description of the graph. Introduction Data-to-text generation aims to create natural language text to describe the input data (Reiter and Dale, 2000). Here we focus on structured text input in a particular form such as a tree or a graph. Figure 1 shows an example where the input data is a mini knowledge graph, and the output text is its corresponding natural language description. Generating text from such data is helpful for many NLP tasks, such as question answering and dialogue (He et al., 2017; Liu et al., 2018; Moon et al., 2019). During generation, the structure of the data as well as the content inside the structure jointly determine the generated text. For example, the direction of the edge “capital” in Figure 1 determines that “London is the capital of U.K.” is an accurate description, but not vice versa. Current generation methods are based on sequence-to-sequence (Seq2Seq) encoder-decoder architecture (Sutskever et al., 2014), which requires the input data to be serialized as a sequence, resulting in a loss of structural information. Recent research has shown the utility of incorporating stru"
2020.acl-main.224,W18-6501,0,0.13686,"nes of research: Seq2Seq generation and Graph2Seq generation. Graph neural networks (GNN) (Scarselli et al., 2009) aim to learn a latent state representation for each node in a graph by aggregating local information from its neighbors and the connected edges. Previous work has explored different ways of aggregating this local information, such as in GCNs (Kipf and Welling, 2017), gated graph neural networks (GGNNs) (Li et al., 2016), and Graph attention networks (GANs) (Veliˇckovi´c et al., 2018) Several works have applied GNNs instead of Seq2Seq models for text generation (Beck et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Guo et al., 2019; Li et al., 2019), and some of them outperform Seq2Seq models. However, Damonte and Cohen (2019) use both types of encoders and show that GCN can help LSTM capture reentrant structures and long-range dependencies, albeit on a different problem than ours. Our method also uses the two types of encoders but instead of using one to assist the other, it combines them simultaneously to capture their complementary effects. 2.1 3 2 Related Work Seq2Seq Generation Traditional data-to-text generation follows a planning and realization pipeline (Reiter and Dale, 2000; Stent et al., 200"
2020.acl-main.224,P19-1081,0,0.0341936,"LG challenge: the source data is an RDF graph and the target output is a text description of the graph. Introduction Data-to-text generation aims to create natural language text to describe the input data (Reiter and Dale, 2000). Here we focus on structured text input in a particular form such as a tree or a graph. Figure 1 shows an example where the input data is a mini knowledge graph, and the output text is its corresponding natural language description. Generating text from such data is helpful for many NLP tasks, such as question answering and dialogue (He et al., 2017; Liu et al., 2018; Moon et al., 2019). During generation, the structure of the data as well as the content inside the structure jointly determine the generated text. For example, the direction of the edge “capital” in Figure 1 determines that “London is the capital of U.K.” is an accurate description, but not vice versa. Current generation methods are based on sequence-to-sequence (Seq2Seq) encoder-decoder architecture (Sutskever et al., 2014), which requires the input data to be serialized as a sequence, resulting in a loss of structural information. Recent research has shown the utility of incorporating structural information d"
2020.acl-main.224,N19-1236,0,0.415727,"risedya et al., 2018; Juraska et al., 2018; Reed et al., 2018). Despite the fair fluency and grammatical correctness, the generated text suffers from several problems such as repetition, omission, and unfaithfulness, which are less likely to happen in traditional planning-and-realization frameworks. Recent work has shown that neural models can also benefit from an explicit planning step to alleviate the above-mentioned problems. The input of these planners ranges from unstructured keyphrases (Hua and Wang, 2019) to structured tables (Puduppully et al., 2019) and graphs (Ferreira et al., 2019; Moryossef et al., 2019a). Our work also focuses on planning from graph data. Compared with previous methods, we show that our neural planning method is more feasible and accurate. More importantly, rather than serializing the planning and realization stages in a pipeline, our dual encoding method simultaneously captures information from the original data and the corresponding plan. 2.2 Graph2Seq Generation This work is inspired by two lines of research: Seq2Seq generation and Graph2Seq generation. Graph neural networks (GNN) (Scarselli et al., 2009) aim to learn a latent state representation for each node in a grap"
2020.acl-main.224,W19-8645,0,0.351953,"risedya et al., 2018; Juraska et al., 2018; Reed et al., 2018). Despite the fair fluency and grammatical correctness, the generated text suffers from several problems such as repetition, omission, and unfaithfulness, which are less likely to happen in traditional planning-and-realization frameworks. Recent work has shown that neural models can also benefit from an explicit planning step to alleviate the above-mentioned problems. The input of these planners ranges from unstructured keyphrases (Hua and Wang, 2019) to structured tables (Puduppully et al., 2019) and graphs (Ferreira et al., 2019; Moryossef et al., 2019a). Our work also focuses on planning from graph data. Compared with previous methods, we show that our neural planning method is more feasible and accurate. More importantly, rather than serializing the planning and realization stages in a pipeline, our dual encoding method simultaneously captures information from the original data and the corresponding plan. 2.2 Graph2Seq Generation This work is inspired by two lines of research: Seq2Seq generation and Graph2Seq generation. Graph neural networks (GNN) (Scarselli et al., 2009) aim to learn a latent state representation for each node in a grap"
2020.acl-main.224,P02-1040,0,0.107891,"n https://github.com/ zhaochaocs/DualEnc 5 http://webnlg.loria.fr/pages/index. html • Step-By-Step (Moryossef et al., 2019a): a transition-based statistical ranking method; • Step-By-Step II (Moryossef et al., 2019b): a DFS-based method with a neural controller; • GRU & Transformer (Ferreira et al., 2019): two neural Seq2Seq methods with attention; We report the performance on three test sets: S EEN, U NSEEN, and A LL (S EEN & U NSEEN). We remove all one-triple instances for planner’s evaluation since the planning for these instances is trivial. Results are evaluated with accuracy and BLEU-n (Papineni et al., 2002). For accuracy, we regard a plan as correct only if it exactly matches one of the human-generated plans. BLEU-n is more forgiving than accuracy. It is also adopted in Yao et al. (2019) for plan evaluation. Here we choose n = 2. 5.2.2 Results Table 1 shows results of the planning experiments. Our GCN method significantly outperforms all the baselines (approximate randomization (Noreen, 1989; Chinchor, 1992), p &lt; 0.05) by a large margin on all the test sets and both measures, indicating the effectiveness of our planner. The most competitive baseline on A LL and U NSEEN sets is Step-By-Step, but"
2020.acl-main.224,W18-6535,1,0.903242,"parate nodes.2 Graph Representation and Encoding To make it easier for GCNs to encode information from both entities and predicates, we reconstruct the input graph by regarding both entities and predicates as nodes, which is different from Figure 1. Formally, for each RDF triple (s, p, o), we regard the s, p, and o as three kinds of nodes. s and o are identified by their entity mentions, and p is identified by a unique ID. That is, two entities from different triples that have the same mentions will Figure 3: The graph obtained from an RDF triple. We use the same edge structure as Beck et al. (2018). As Figure 3 shows, a triple contains four directed edges to connect its nodes: s → p, p → s, o → p, and p → o. These edges help in information exchange between arbitrary neighbor pairs. There is also a special self-loop edge n → n for each node n to enable information flow between adjacent iterations during feature aggregation. After building the graph G = (V, E) from the RDF data, we use a relational GCN (R-GCN) (Schlichtkrull et al., 2018) to encode the graph and learn a state representation hv ∈ Rd for each node v ∈ V using the following iterative method:   X X 1 htv = ρ  Wr h(t−1) + b"
2020.acl-main.224,P18-1150,0,0.0270328,"gure 1 determines that “London is the capital of U.K.” is an accurate description, but not vice versa. Current generation methods are based on sequence-to-sequence (Seq2Seq) encoder-decoder architecture (Sutskever et al., 2014), which requires the input data to be serialized as a sequence, resulting in a loss of structural information. Recent research has shown the utility of incorporating structural information during generation. By replacing the sequential encoder with a structureaware graph encoder, such as a graph convolutional network (GCNs) (Kipf and Welling, 2017) or graph-state LSTMs (Song et al., 2018), the resulting graph-to-sequence (Graph2Seq) methods can encode the structural information of the input and thus outperform Seq2Seq models on certain tasks. However, these architectures broaden the structural gap between the encoder and decoder. That is, while the encoder receives the input data as a graph, the decoder has to create the output text as a linear chain structure. This structural gap increases the difficulty of establishing alignments between source and target, which is believed to play a key role in text generation. For example, in machine translation, pre-reordering the source"
2020.acl-main.224,P04-1011,1,0.688994,"Beltrachini, 2018; Guo et al., 2019; Li et al., 2019), and some of them outperform Seq2Seq models. However, Damonte and Cohen (2019) use both types of encoders and show that GCN can help LSTM capture reentrant structures and long-range dependencies, albeit on a different problem than ours. Our method also uses the two types of encoders but instead of using one to assist the other, it combines them simultaneously to capture their complementary effects. 2.1 3 2 Related Work Seq2Seq Generation Traditional data-to-text generation follows a planning and realization pipeline (Reiter and Dale, 2000; Stent et al., 2004). More recent methods use Seq2Seq architecture (Sutskever et al., 2014) to combine planning and realization into an end-toend network and have achieved the state-of-the-art Problem Statement In this work we focus on text generation from RDF data.1 The input for this task is a set of RDF triples, where each triple (s, p, o) contains a subject, a predicate, and an object. For example, (“U.K.”, “cap2482 1 https://www.w3.org/TR/rdf-concepts/ Figure 2: The architecture of the proposed D UAL E NC model. The input triples are converted as a graph and then fed to two GCN encoders for plan and text gen"
2020.acl-main.224,P18-1151,0,0.178119,"significantly outperforms the previous start-of-the-art on the generation task. The human evaluation confirms that the texts generated by our model are preferred over strong baselines. The contributions of this paper are three-fold: • We propose a dual encoding method to narrow the structural gap between data encoder and text decoder for data-to-text generation; • We propose a neural planner, which is more efficient and effective than previous methods; • Experiments show that our method outperforms all baselines on a variety of measures. on a variety of generation tasks (Lebret et al., 2016; Trisedya et al., 2018; Juraska et al., 2018; Reed et al., 2018). Despite the fair fluency and grammatical correctness, the generated text suffers from several problems such as repetition, omission, and unfaithfulness, which are less likely to happen in traditional planning-and-realization frameworks. Recent work has shown that neural models can also benefit from an explicit planning step to alleviate the above-mentioned problems. The input of these planners ranges from unstructured keyphrases (Hua and Wang, 2019) to structured tables (Puduppully et al., 2019) and graphs (Ferreira et al., 2019; Moryossef et al., 20"
2020.sigdial-1.3,D18-1547,0,0.0128717,"e experiments presented here (Wen et al., 2016; Golovanov et al., 2019), but these methods do not produce NLG outputs that integrate attributes from two different sources into the same sentence. Our final results show that the ability of our self-training method to automatically construct new training instances results in high quality natural, coherent and grammatical outputs with high semantic accuracy. In future, we hope to generalize our novel selftraining method to build an NLG that can combine two distinct domains, e.g. hotels or movies combined with restaurants in multi-domain dialogue (Budzianowski et al., 2018; Gaˇsi´c et al., 2015; Hakkani-T¨ur et al., 2016; Cervone et al., 2019; Ultes et al., 2017). Ideally systems that cover multiple domains should be able to produce utterances that seamlessly integrate both domains, if data exists for each domain independently. However, there may be additional challenges in such combinations. Our results require the initial neural models to generate some combined outputs. It is not clear whether there are some aspects of our experimental setup that facilitate this, e.g. it may require some attributes to be shared across the two initial ontologies, or some share"
2020.sigdial-1.3,P17-4012,0,0.0181953,"were all RNN encoder-decoder systems. Here we also use a standard RNN Encoder–Decoder model (Sutskever et al., 2014) that maps a source sequence (the input MR) to a target sequence (the utterance text). We 3 The train and test data are available http://nlds.soe.ucsc.edu/source-blending-NLG http://nlds.soe.ucsc.edu/sentence-planning-NLG http://www.macs.hw.ac.uk/InteractionLab/E2E/ 23 at first implement a baseline model and then add three variations of model supervision that aim to improve semantic accuracy. All of the models are built with OpenNMT-py, a sequence-to-sequence modeling framework (Klein et al., 2017). Encoder. The MR is represented as a sequence of (attribute, value) pairs with separate vocabularies for attributes and values. Each attribute and each value are represented using 1-hot vectors. An (attribute, value) pair is represented by concatenating the two 1-hot vectors. The input sequence is processed using two single layer bidirectional-LSTM (Hochreiter and Schmidhuber, 1997) encoders. The first encoder operates at the pair level, producing a hidden state for each attribute-value pair of the input sequence. The second LSTM encoder is intended to produce utterance level context informat"
2020.sigdial-1.3,D16-1230,0,0.0233984,"Section 4.2. The retrofit MRs match the (errorful) NLG output: when these MR/NLG output pairs combine attributes from both sources, they provide novel corrected examples to add back into training. Text-to-Meaning Semantic Extractor Much previous work in NLG relies on a test set that provides gold reference outputs, and then applies automatic metrics such as BLEU that compare the gold reference to the model output (Papineni et al., 2002; Duˇsek et al., 2020), even though the limitations of BLEU for NLG are widely acknowledged (Belz and Reiter, 2006; Stent et al., 2005; Novikova et al., 2017b; Liu et al., 2016). To address these limitations, recent work has started to develop “referenceless” NLG evaluation metrics (Dusek et al., 2017; Kann et al., 2018; Tian et al., 2018; Mehri and Eskenazi, 2020). Since there are no reference outputs for the COM test set, we need a referenceless evaluation metric. We develop a rule-based text-to-MR semantic extractor (TTM) that allows us to compare the input MR to an MR automatically constructed from an NLG model textual output by the TTM, in order to calculate SER, the slot error rate. The TTM system is based on information extraction methods. We conduct a human e"
2020.sigdial-1.3,N18-3006,1,0.889013,"Missing"
2020.sigdial-1.3,D15-1166,0,0.00851762,"quence. The outputs of both encoders are combined via concatenation. That is, the final state of the second encoder is concatenated onto each hidden state output by the first encoder. The size of the pair level encoder is 46 units and the size of the MR encoder is 20 units. Model parameters are initialized using Glorot initialization (Glorot and Bengio, 2010) and optimized using Stochastic Gradient Descent with mini-batches of size 128. Decoder. The decoder is a uni-directional LSTM that uses global attention with input-feeding. Attention weights are calculated via the general scoring method (Luong et al., 2015). The decoder takes two inputs at each time step: the word embedding of the previous time step, and the attention weighted average of the encoder hidden states. The groundtruth previous word is used when training, and the predicted previous word when evaluating. Beam search with five beams is used during inference. Supervision. Figure 3 shows the baseline system architecture as well as three types of supervision, based on conditioning on source (E2E, NYC) information. The additional supervision is intended to help the model attend to the source domain information. We call the three types of su"
2020.sigdial-1.3,N19-1410,0,0.0132734,"we need a referenceless evaluation metric. We develop a rule-based text-to-MR semantic extractor (TTM) that allows us to compare the input MR to an MR automatically constructed from an NLG model textual output by the TTM, in order to calculate SER, the slot error rate. The TTM system is based on information extraction methods. We conduct a human evaluation of its accuracy below. A similar approach is used to calculate semantic accuracy in other work in NLG, including comparative system evaluation in the E2E Generation Challenge (Juraska et al., 2018; Duˇsek et al., 2020; Wiseman et al., 2017; Shen et al., 2019). The TTM relies on a rule-based automatic aligner that tags each output utterance with the attributes and values that it realizes. The aligner takes advantage of the fact that the RECOMMEND dialogue act, and the attributes and their values are typically realized from a domain-specific finite vocabulary. The output of the aligner is then used by the TTM extractor to construct an MR that matches the (potentially errorful) utterance that was generated by the NLG. We refer to this MR as the “retrofit MR”. The retrofit MR is then compared to the input MR in order to automatically calculate the slo"
2020.sigdial-1.3,2020.sigdial-1.28,0,0.164573,"into training. Text-to-Meaning Semantic Extractor Much previous work in NLG relies on a test set that provides gold reference outputs, and then applies automatic metrics such as BLEU that compare the gold reference to the model output (Papineni et al., 2002; Duˇsek et al., 2020), even though the limitations of BLEU for NLG are widely acknowledged (Belz and Reiter, 2006; Stent et al., 2005; Novikova et al., 2017b; Liu et al., 2016). To address these limitations, recent work has started to develop “referenceless” NLG evaluation metrics (Dusek et al., 2017; Kann et al., 2018; Tian et al., 2018; Mehri and Eskenazi, 2020). Since there are no reference outputs for the COM test set, we need a referenceless evaluation metric. We develop a rule-based text-to-MR semantic extractor (TTM) that allows us to compare the input MR to an MR automatically constructed from an NLG model textual output by the TTM, in order to calculate SER, the slot error rate. The TTM system is based on information extraction methods. We conduct a human evaluation of its accuracy below. A similar approach is used to calculate semantic accuracy in other work in NLG, including comparative system evaluation in the E2E Generation Challenge (Jura"
2020.sigdial-1.3,W17-5525,0,0.0314402,"Missing"
2020.sigdial-1.3,D17-1238,0,0.040736,"Missing"
2020.sigdial-1.3,W18-5019,1,0.887465,"Missing"
2020.sigdial-1.3,P02-1040,0,0.108015,"is .80 and the correlation with deletions, the most frequent error type, is .97. Retrofit MRs for Self-Training. The TTM is critical for our novel self-training method described in Section 4.2. The retrofit MRs match the (errorful) NLG output: when these MR/NLG output pairs combine attributes from both sources, they provide novel corrected examples to add back into training. Text-to-Meaning Semantic Extractor Much previous work in NLG relies on a test set that provides gold reference outputs, and then applies automatic metrics such as BLEU that compare the gold reference to the model output (Papineni et al., 2002; Duˇsek et al., 2020), even though the limitations of BLEU for NLG are widely acknowledged (Belz and Reiter, 2006; Stent et al., 2005; Novikova et al., 2017b; Liu et al., 2016). To address these limitations, recent work has started to develop “referenceless” NLG evaluation metrics (Dusek et al., 2017; Kann et al., 2018; Tian et al., 2018; Mehri and Eskenazi, 2020). Since there are no reference outputs for the COM test set, we need a referenceless evaluation metric. We develop a rule-based text-to-MR semantic extractor (TTM) that allows us to compare the input MR to an MR automatically constru"
2020.sigdial-1.3,W18-6512,0,0.0271224,"Missing"
2020.sigdial-1.3,W18-6535,1,0.853212,"ferent domain ontology in the restaurant domain, with novel attributes and dialogue acts not seen in the other dataset, e.g. only one has attributes representing family friendly and rating information, and only one has attributes for decor and service. Our aim is an NLG engine that can realize utterances for the extended combined ontology not seen in the training data, e.g. for MRs that specify values for family friendly, rating, decor and service. Figure 1 illustrates this task. Example E1 is from a training set referred to as NYC, from previous work on controllable sentence planning in NLG (Reed et al., 2018), while E2 is from the E2E NLG shared task (Novikova et al., 2017a). As we describe in detail in Section 2, E1 and E2 are based on two distinct ontologies. Example E3 illustrates the task addressed in this paper: we create a test set of novel MRs for the combined ontology, and train a model to generate high quality outputs where individual sentences realize attributes from both ontologies. To our knowledge, this is a completely novel task. While it is common practice in NLG to construct test sets of MRs that realize attribute combinations not seen in training, initial experiments Natural langu"
2020.sigdial-1.3,P17-4013,0,0.0323122,"Missing"
2020.sigdial-1.3,D19-1221,0,0.0173749,"existing within-domain training data. We show that we can combine two training datasets for the restaurant domain, that have different ontologies, and generate output that combines attributes from both sources, by applying a combination of neural supervision and a novel self-training method. While it is common practice to construct test sets with unseen attribute combinations, we know of no prior work based on constructing a new combined ontology. Our experiments show that the task is surprisingly adversarial, consistent with recent work suggesting that neural models often fail to generalize (Wallace et al., 2019; Feng et al., 2018; Ribeiro et al.; Goodfellow et al., 2014). Work on domain transfer shares similar goals to the experiments presented here (Wen et al., 2016; Golovanov et al., 2019), but these methods do not produce NLG outputs that integrate attributes from two different sources into the same sentence. Our final results show that the ability of our self-training method to automatically construct new training instances results in high quality natural, coherent and grammatical outputs with high semantic accuracy. In future, we hope to generalize our novel selftraining method to build an NLG"
2020.sigdial-1.3,N16-1015,0,0.0500517,"Missing"
2020.sigdial-1.3,D15-1199,0,0.0608914,"Missing"
2020.sigdial-1.3,D17-1239,0,0.0319969,"Missing"
2020.sigdial-1.3,W16-4620,0,0.024417,"Missing"
2021.emnlp-demo.15,W19-8623,1,0.883089,"Missing"
2021.emnlp-demo.15,2021.inlg-1.45,1,0.733562,"Missing"
2021.emnlp-demo.15,P19-1596,1,0.848212,"Missing"
2021.emnlp-demo.15,P97-1035,1,0.600293,"Missing"
2021.emnlp-demo.15,2020.sigdial-1.3,1,0.82851,"Missing"
2021.emnlp-demo.15,W18-5701,0,0.0606008,"Missing"
2021.emnlp-demo.15,W00-1433,0,0.466149,"Missing"
2021.inlg-1.45,W18-6555,0,0.0202599,"nce models is typically performed by creating an extensive set of rules, or by training a supplemental classifier, that indicates for each input slot whether it is present in the output utterance (Wen et al., 2015a; Duˇsek and Jurˇc´ıcˇ ek, 1 The code for S E A-G UI D E and heuristic semantic error evaluation can be found at https://github.com/ jjuraska/data2text-nlg. 416 Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 416–431, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics 2016; Juraska et al., 2018; Agarwal et al., 2018; Kedzie and McKeown, 2020; Harkous et al., 2020). Wen et al. (2015b) proposed an extension of the underlying LSTM cells of their sequence-tosequence model to explicitly track, at each decoding step, the information mentioned so far. The coverage mechanism (Tu et al., 2016; Mi et al., 2016; See et al., 2017) penalizes the model for attending to the same parts of the input based on the cumulative attention distribution in the decoder. Chisholm et al. (2017) and Shen et al. (2019) both introduce different sequence-to-sequence model architectures that jointly learn to generate text and reconstruc"
2021.inlg-1.45,D18-1431,0,0.021533,"ch leads to a reduction in semantic errors on both the E2E and ViGGO (Juraska et al., 2019) datasets. In contrast to the above methods, our approach does not rely on model modifications, data augmentation, or manual annotation. Our method is novel in that it utilizes information that is already present in the model itself to perform semantic reranking. Finally, related to our work is also controllable neural language generation, in which the constrained decoding strategy is often used, rescoring tokens at each decoding step based on a set of feature discriminators (Ghazvininejad et al., 2017; Baheti et al., 2018; Holtzman et al., 2018). Nevertheless, this method is typically used with unconditional generative LMs, and hence does not involve input-dependent constraints. 3 Semantic Attention-Guided Decoding While we will evaluate the S E A-G UI D E method on ViGGO, E2E, and MultiWOZ, we develop the method by careful analysis of the cross-attention behavior of different pretrained generative LMs fine-tuned on the ViGGO dataset. ViGGO is a parallel corpus of structured meaning representations (MRs) and corresponding natural-language utterances in the video game domain. The MRs consist of a dialogue act ("
2021.inlg-1.45,2020.acl-main.703,0,0.0306382,"ful analysis of the cross-attention behavior of different pretrained generative LMs fine-tuned on the ViGGO dataset. ViGGO is a parallel corpus of structured meaning representations (MRs) and corresponding natural-language utterances in the video game domain. The MRs consist of a dialogue act (DA) and a list of slot-andvalue pairs. The motivation for selecting ViGGO for developing the method was that it is the smallest dataset, but it provides a variety of DA and slot types (as shown in Table 1). The models used for the analysis were the smallest variants of T5 (Raffel et al., 2020) and BART (Lewis et al., 2020). We saved the larger variants of the models, as well as the other two datasets, for the evaluation. 3.1 Interpreting Cross-Attention Attention (Bahdanau et al., 2015; Luong et al., 2015) is a mechanism that was introduced in encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) to overcome the long-range dependencies problem of RNN-based models. It allows the decoder to effectively condition its output tokens on relevant parts of the encoder’s output at each decoding step. The term cross-attention is primarily used when referring to the more recent transformer-based encoder-decode"
2021.inlg-1.45,D15-1166,0,0.0685145,"and corresponding natural-language utterances in the video game domain. The MRs consist of a dialogue act (DA) and a list of slot-andvalue pairs. The motivation for selecting ViGGO for developing the method was that it is the smallest dataset, but it provides a variety of DA and slot types (as shown in Table 1). The models used for the analysis were the smallest variants of T5 (Raffel et al., 2020) and BART (Lewis et al., 2020). We saved the larger variants of the models, as well as the other two datasets, for the evaluation. 3.1 Interpreting Cross-Attention Attention (Bahdanau et al., 2015; Luong et al., 2015) is a mechanism that was introduced in encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) to overcome the long-range dependencies problem of RNN-based models. It allows the decoder to effectively condition its output tokens on relevant parts of the encoder’s output at each decoding step. The term cross-attention is primarily used when referring to the more recent transformer-based encoder-decoder models (Vaswani et al., 2017), to distinguish it from the self-attention layers present in both the encoder and the decoder transformer blocks. The crossattention layer ultimately provi"
2021.inlg-1.45,D16-1096,0,0.0218289,"rror evaluation can be found at https://github.com/ jjuraska/data2text-nlg. 416 Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 416–431, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics 2016; Juraska et al., 2018; Agarwal et al., 2018; Kedzie and McKeown, 2020; Harkous et al., 2020). Wen et al. (2015b) proposed an extension of the underlying LSTM cells of their sequence-tosequence model to explicitly track, at each decoding step, the information mentioned so far. The coverage mechanism (Tu et al., 2016; Mi et al., 2016; See et al., 2017) penalizes the model for attending to the same parts of the input based on the cumulative attention distribution in the decoder. Chisholm et al. (2017) and Shen et al. (2019) both introduce different sequence-to-sequence model architectures that jointly learn to generate text and reconstruct the input facts. An iterative self-training process using data augmentation (Nie et al., 2019; Kedzie and McKeown, 2019) was shown to reduce semantic NLG errors on the E2E dataset (Novikova et al., 2017). Among the most recent efforts, the jointly-learned segmentation and alignment metho"
2021.inlg-1.45,P19-1256,0,0.0169139,"an extension of the underlying LSTM cells of their sequence-tosequence model to explicitly track, at each decoding step, the information mentioned so far. The coverage mechanism (Tu et al., 2016; Mi et al., 2016; See et al., 2017) penalizes the model for attending to the same parts of the input based on the cumulative attention distribution in the decoder. Chisholm et al. (2017) and Shen et al. (2019) both introduce different sequence-to-sequence model architectures that jointly learn to generate text and reconstruct the input facts. An iterative self-training process using data augmentation (Nie et al., 2019; Kedzie and McKeown, 2019) was shown to reduce semantic NLG errors on the E2E dataset (Novikova et al., 2017). Among the most recent efforts, the jointly-learned segmentation and alignment method of Shen et al. (2020) improves semantic accuracy while simultaneously increasing output diversity. Kedzie and McKeown (2020) use segmentation for data augmentation and automatic utterance planning, which leads to a reduction in semantic errors on both the E2E and ViGGO (Juraska et al., 2019) datasets. In contrast to the above methods, our approach does not rely on model modifications, data augmentati"
2021.inlg-1.45,W17-5525,0,0.0435955,"Missing"
2021.inlg-1.45,P02-1040,0,0.109657,"Missing"
2021.inlg-1.45,2020.findings-emnlp.17,0,0.0144572,"which are identified as slot mentions with low confidence, due to the partial ambi419 Datasets. Besides ViGGO, which we use for finetuning the decoding (slot-tracking) parameters of the proposed S E A-G UI D E method, we evaluate its effectiveness for semantic error reduction on two unseen and out-of-domain datasets. While E2E (Novikova et al., 2017) is also a simple MR-totext generation dataset (in the restaurant domain), MultiWOZ 2.1 (Eric et al., 2020) is a dialogic corpus covering several domains from which we extract system turns only, along with their MR annotations, along the lines of Peng et al. (2020) and Kale and Rastogi (2020). Table 1 gives an overview of the datasets’ properties. Setup. In our experiments, we fine-tune T5 and BART models of varying sizes on the above datasets’ training partitions, select the best model checkpoints based on the BLEU score they achieve on the respective validation set, and evaluate them on the test sets while using different decoding methSize ViGGO E2E MultiWOZ Domains 6,900 51,426 70,530 1 1 7 DAs 9 1 13 Slots Verbatim st 14 8 27 Table 1: Dataset statistics, including the total number of dialogue act (DA) and slot types. For MultiWOZ, the numbers are ca"
A00-2028,P99-1025,1,0.895035,"s, plus a category called other for calls that cannot be automated and must be transferred to a human operator (Gorin et al., 1997). ~ Each category describes a different task, such as person-to-person dialing, or receiving credit for a misdialed number. The system determines which task the caller is requesting on the basis of its understanding of the cMler&apos;s response to the open-ended system greeting A T ~ T, How May I Help You?. Once the task has been determined, the information needed for completing the caller&apos;s request is obtained using dialogue submodules that are specific for each task (Abella and Gorin, 1999). In addition to the TASKSUCCESS dialogues in which HMIHY successfully automates the customer&apos;s call, illustrated in Figure 1, and the calls that are transferred to a human operator, there are three other possible outcomes for a call, all of which are problematic. The first category, which we call HANGUP, results from a customer&apos;s decision to hang up on the system. A sample HANGUP dialogue is in Figure 2. A caller may hang up because s/he is frustrated with the system; our goal is to learn from the corpus which system behaviors led to the caller&apos;s frustration. The second problematic category ("
A00-2028,J99-3003,0,0.0317013,"Missing"
A00-2028,P98-1122,0,0.0690774,"Missing"
A00-2028,P99-1040,1,0.829966,"Missing"
A00-2028,H92-1009,0,0.0372315,"Missing"
A00-2028,P98-2219,1,0.850525,"Missing"
A00-2028,C98-1117,0,\N,Missing
A00-2028,C98-2214,1,\N,Missing
C00-1073,P98-2219,1,0.809491,"Missing"
C00-1073,C98-2214,1,\N,Missing
C02-1138,J99-2004,1,0.680382,"lar ideas can be used to port FERGUS to different domains with little manual effort. 3.1 Description of the FERGUS Surface Realizer Given an underspecified dependency tree representing one sentence as input, FERGUS outputs the best surface string according to its stochastic modeling. Each node in the input tree corresponds to a lexeme. Nodes that are related by grammatical function are linked together. Surface ordering of the lexemes remains unspecified in the tree. FERGUS consists of three models: tree chooser, unraveler, and linear precedence chooser. The tree chooser associates a supertag (Bangalore and Joshi, 1999) from a treeadjoining grammar (TAG) with each node in the underspecified dependency tree. This partially specifies the output string’s surface order; it is constrained by grammatical constraints encoded by the supertags (e.g. subcategorization constraints, voice), but remains free otherwise (e.g. ordering of modifiers). The tree chooser uses a stochastic tree model (TM) to select a supertag for each node in the tree based on local tree context. The unraveler takes the resulting semi-specified TAG derivation tree and creates a word lattice corresponding to all of the potential surface orderings"
C02-1138,C00-1007,1,0.914188,"Missing"
C02-1138,W00-1401,1,0.891425,"rom all strings like these, with duplicates, in the Communicator system by replacing the slot names with fillers according to a probability distribution. Furthermore, dependency parses are assigned to the resulting strings by hand. In the first series of experiments, we ascertain the output quality of FERGUS using the XTAG grammar on different training corpora. We vary the TM’s training corpus to be either PTB or HH. We do the same for the LM’s training corpus. Assessing the output quality of a generator is a complex issue. Here, we select as our metric understandability accuracy, defined in (Bangalore et al., 2000) as quantifying the differPTB LM HH LM PTB TM 0.30 0.37 HH TM 0.38 0.41 Table 2: Average understandability accuracies using XTAG-Based FERGUS for various kinds of training data PTB LM HH LM PTB TM 0.39 0.33 Table 3: Average understandability accuracies using automatically-extracted grammar based FERGUS for various kinds of training data ence between the generator output, in terms of both dependency tree and surface string, and the desired reference output. (Bangalore et al., 2000) finds this metric to correlate well with human judgments of understandability and quality. Understandability accur"
C02-1138,W01-0520,1,0.913667,"a trigram language model (LM), specifying the output string completely. Certain resources are required in order to train FERGUS. A TAG grammar is needed— the source of the supertags with which the semi-specified TAG derivation tree is annotated. There needs to be a treebank in order to obtain the stochastic model TM driving the tree chooser. There also needs to be a corpus of sentences in order to train the language model LM required for the LP chooser. 3.2 Labor-Minimizing Approaches to Training FERGUS The resources that are needed to train FERGUS seem quite labor intensive to develop. But (Bangalore et al., 2001) show that automatically generated version of these resources can be used by FERGUS to obtain quality output. Two kinds of TAG grammar are used in (Bangalore et al., 2001). One kind is a manually developed, broad-coverage grammar for English: the XTAG grammar (XTAG-Group, 2001). It consists of approximately 1000 tree frames. Disadvantages of using XTAG are the considerable amount of human labor expended in its development and the lack of a treebank based on XTAG—the only way to estimate parameters in the TM is to rely on a heuristic mapping of XTAG tree frames onto a pre-existing treebank (Ban"
C02-1138,A00-2018,0,0.00691146,"disadvantages of using the XTAG grammar, but they introduce potential complications— notably, an extracted grammar’s size is often much larger than that of XTAG, typically more than 2000 tree frames, potentially leading to a larger sparse data problem, and also the resulting grammar is not hand-checked. Two kinds of treebank are used in (Bangalore et al., 2001). One kind is the Penn Treebank (Marcus et al., 1993). It consists of approximately 1,000,000 words of hand-checked, bracketed text. The text consists of Wall Street Journal news articles. The other kind of treebank is the BLLIP corpus (Charniak, 2000). It consists of approximately 40,000,000 words of text that has been parsed by a broad-coverage statistical parser. The text consists of Wall Street Journal news and newswire articles. The advantage of the former is that it has been handchecked, whereas the latter has the advantage of being easily produced and hence can easily be enlarged. (Bangalore et al., 2001) experimentally determine how the quality and quantity of the resources used in training FERGUS affect the output quality of the generator. They find that while a better quality annotated corpus (Penn Treebank) results in better mode"
C02-1138,P00-1058,0,0.0261357,"ed in (Bangalore et al., 2001). One kind is a manually developed, broad-coverage grammar for English: the XTAG grammar (XTAG-Group, 2001). It consists of approximately 1000 tree frames. Disadvantages of using XTAG are the considerable amount of human labor expended in its development and the lack of a treebank based on XTAG—the only way to estimate parameters in the TM is to rely on a heuristic mapping of XTAG tree frames onto a pre-existing treebank (Bangalore and Joshi, 1999). Another kind of grammar is a TAG automatically extracted from a treebank using the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). These techniques extract a linguistically motivated TAG using heuristics programmed using a modicum of human labor. They nullify the disadvantages of using the XTAG grammar, but they introduce potential complications— notably, an extracted grammar’s size is often much larger than that of XTAG, typically more than 2000 tree frames, potentially leading to a larger sparse data problem, and also the resulting grammar is not hand-checked. Two kinds of treebank are used in (Bangalore et al., 2001). One kind is the Penn Treebank (Marcus et al., 1993). It consists of approximately 1,00"
C02-1138,P95-1034,0,0.0350299,"se stochastic NLG components can be made to work together, that they can be ported to new domains with apparent ease, and that such NLG components can be integrated in a real-time dialog system. 1 Introduction Systems that interact with the user via natural language are in their infancy. As these systems mature and become more complex, it would be desirable for a system developer if there were automatic methods for creating natural language generation (NLG) components that can produce quality output efficiently. Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimoto et al., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. In contrast, this paper investigates how different stochastic NLG components can be made to work together effectively, whether they can easily be ported to new domains, and whether they can be integrated in a real-time dialog system. DM Dialog Manager SPoT Sentence Planner Implicit−confirm(NEWARK) Implicit−confirm(DALLAS) Request(DEPART−DATE) period"
C02-1138,P98-1116,0,0.0580485,"ade to work together, that they can be ported to new domains with apparent ease, and that such NLG components can be integrated in a real-time dialog system. 1 Introduction Systems that interact with the user via natural language are in their infancy. As these systems mature and become more complex, it would be desirable for a system developer if there were automatic methods for creating natural language generation (NLG) components that can produce quality output efficiently. Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimoto et al., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. In contrast, this paper investigates how different stochastic NLG components can be made to work together effectively, whether they can easily be ported to new domains, and whether they can be integrated in a real-time dialog system. DM Dialog Manager SPoT Sentence Planner Implicit−confirm(NEWARK) Implicit−confirm(DALLAS) Request(DEPART−DATE) period soft−merge Request(D−D) Imp−co"
C02-1138,J93-2004,0,0.0246778,"sing the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). These techniques extract a linguistically motivated TAG using heuristics programmed using a modicum of human labor. They nullify the disadvantages of using the XTAG grammar, but they introduce potential complications— notably, an extracted grammar’s size is often much larger than that of XTAG, typically more than 2000 tree frames, potentially leading to a larger sparse data problem, and also the resulting grammar is not hand-checked. Two kinds of treebank are used in (Bangalore et al., 2001). One kind is the Penn Treebank (Marcus et al., 1993). It consists of approximately 1,000,000 words of hand-checked, bracketed text. The text consists of Wall Street Journal news articles. The other kind of treebank is the BLLIP corpus (Charniak, 2000). It consists of approximately 40,000,000 words of text that has been parsed by a broad-coverage statistical parser. The text consists of Wall Street Journal news and newswire articles. The advantage of the former is that it has been handchecked, whereas the latter has the advantage of being easily produced and hence can easily be enlarged. (Bangalore et al., 2001) experimentally determine how the"
C02-1138,W00-0306,0,0.203755,"y can be ported to new domains with apparent ease, and that such NLG components can be integrated in a real-time dialog system. 1 Introduction Systems that interact with the user via natural language are in their infancy. As these systems mature and become more complex, it would be desirable for a system developer if there were automatic methods for creating natural language generation (NLG) components that can produce quality output efficiently. Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimoto et al., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. In contrast, this paper investigates how different stochastic NLG components can be made to work together effectively, whether they can easily be ported to new domains, and whether they can be integrated in a real-time dialog system. DM Dialog Manager SPoT Sentence Planner Implicit−confirm(NEWARK) Implicit−confirm(DALLAS) Request(DEPART−DATE) period soft−merge Request(D−D) Imp−conf(N) Imp−conf(D) FERGUS"
C02-1138,C00-2126,0,0.0197615,"mains with apparent ease, and that such NLG components can be integrated in a real-time dialog system. 1 Introduction Systems that interact with the user via natural language are in their infancy. As these systems mature and become more complex, it would be desirable for a system developer if there were automatic methods for creating natural language generation (NLG) components that can produce quality output efficiently. Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimoto et al., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. In contrast, this paper investigates how different stochastic NLG components can be made to work together effectively, whether they can easily be ported to new domains, and whether they can be integrated in a real-time dialog system. DM Dialog Manager SPoT Sentence Planner Implicit−confirm(NEWARK) Implicit−confirm(DALLAS) Request(DEPART−DATE) period soft−merge Request(D−D) Imp−conf(N) Imp−conf(D) FERGUS Surface Generator Flying"
C02-1138,N01-1003,1,0.912551,"ve goal. During sentence planning, linguistic means—in particular, lexical and syntactic means—are determined to convey smaller pieces of meaning. During realization, the specification chosen in sentence planning is transformed into a surface string by linearizing and inflecting words in the sentence (and typically, adding function words). Figure 1 shows how such components cooperate to generate text corresponding to a set of communicative goals. Our work addresses both the sentence planning stage and the realization stage. The sentence planning stage is embodied by the SPoT sentence planner (Walker et al., 2001), while the surface realization stage is embodied by the FERGUS surface realizer (Bangalore and Rambow, 2000). We extend the work of (Walker et al., 2001) and (Bangalore and Rambow, 2000) in various ways. We show that apparently each Features Used all domain-independent task-independent task-dependent of SPoT and FERGUS can be ported to different domains with little manual effort. We then show that these two components can work together effectively. Finally, we show the on-line integration of FERGUS with a dialog system. 2 Testing the Domain Independence of Sentence Planning In this section, w"
C02-1138,C98-1112,0,\N,Missing
C02-1138,C98-1114,1,\N,Missing
C02-1138,P98-1118,1,\N,Missing
C92-1054,P90-1010,1,0.816301,"Missing"
C92-1054,J86-3001,0,\N,Missing
C92-2122,A83-1006,0,0.145756,"modeling of accounts, brands, commercial and marketing manprocesses unique to the application task, or the partic- agers, each with different d a t a requirements. They ular relations in the application database. The process fit the user profile recommended for NLI&apos;s[8]. They of customising an NLI consists in adding the domain- were relatively infrequent computer users, who were dependent knowledge abont a particular application to experts in the domain with at least one year&apos;s experithe domain-independent knowledge t h a t comes with ence. None knew anything about database languages. the NLI[4]. Very little has been written about how Some of them had used a previously installed NLI, Intellect, as well as a menu-based interface t h a t accessed this eustomisation is done. AcrEs DECOLING-92, NANTES,23-28 AOt~n""1992 820 PROC.OF COLING-92, NANTES,AUG. 23-28, 1992 tile name set of data 1. They required ad hoe access to information that was difficult to support with standard reports. The NLI we worked with was considered state o f the art. It appeared to use a pipeline architecture consisting of morphological analysis, parser, semantic illterpretation, and database query translator. The s"
C92-2122,E89-1016,1,0.832772,"f • &apos;Fhe Product Hierarchy : Markets, Sectors, WSL. We didn&apos;t feel that the syntax of the transcripts was important since it reflected a degree of accommoBrands, etc. dation to Intellect, but the Intellect lexicon and the • The Customer IIierarchy : Corporations, Trading unknown word errors gave us a good basis for the reCompanies, Concerns quired lexical and conceptual coverage. In the absence of such information, a method to acqnire it, such as I In [9] we colnpalre tile menu system to Intellect. 2T|te customislng team comprised two computational fin- Wizard of Oz studies, would be necessary[10, 1]. gulsts, a computer scientiat aald two psychologists. 3. Constructing the customisation files ACRESDECOLING-92, NANTES,23-28 AO(rl 1992 82 1 I)ROC. OFCOLING-92, NANI&apos;ES,AUO. 23-28, 1992 5 M a p p i n g N L t e r m s o n t o an E - R diagram cators such as less than, greater than, equal to, at least, T h e steps we applied in this p a r t of the proposed m e t h o d are: (1) t a k e the E - R d i a g r a m provided by the d a t a b a s e designer at tim c u s t o m e r site as a conceptual representation of the domain, (2) associate each lexical i t e m f r o m the t r a n s c r i p t analysis"
C92-2122,E89-1039,0,0.0286032,"f • &apos;Fhe Product Hierarchy : Markets, Sectors, WSL. We didn&apos;t feel that the syntax of the transcripts was important since it reflected a degree of accommoBrands, etc. dation to Intellect, but the Intellect lexicon and the • The Customer IIierarchy : Corporations, Trading unknown word errors gave us a good basis for the reCompanies, Concerns quired lexical and conceptual coverage. In the absence of such information, a method to acqnire it, such as I In [9] we colnpalre tile menu system to Intellect. 2T|te customislng team comprised two computational fin- Wizard of Oz studies, would be necessary[10, 1]. gulsts, a computer scientiat aald two psychologists. 3. Constructing the customisation files ACRESDECOLING-92, NANTES,23-28 AO(rl 1992 82 1 I)ROC. OFCOLING-92, NANI&apos;ES,AUO. 23-28, 1992 5 M a p p i n g N L t e r m s o n t o an E - R diagram cators such as less than, greater than, equal to, at least, T h e steps we applied in this p a r t of the proposed m e t h o d are: (1) t a k e the E - R d i a g r a m provided by the d a t a b a s e designer at tim c u s t o m e r site as a conceptual representation of the domain, (2) associate each lexical i t e m f r o m the t r a n s c r i p t analysis"
C92-2122,A83-1004,0,\N,Missing
C94-2196,J93-4004,0,0.0867649,"Missing"
C94-2196,C82-1066,0,\N,Missing
C94-2196,P90-1010,1,\N,Missing
C94-2196,C92-1054,1,\N,Missing
C98-2124,P84-1029,0,0.326536,"literal or cooperative), other differences are a function of application task and task/strategy interactions. A PARADISE assessment of the contribution of each evaluation measure to overall performance shows that strategy-dependent dialogue phenomena as well as phenomena associated with speech recognition significantly predict performance. Our results identify the conditions under which TOOT' s cooperative response strategy leads to greater agent performance. 1 2 focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation TOOT TOOT allows users to access online AMTRAK train schedules via a tele"
C98-2124,J86-2002,0,0.12226,"Missing"
C98-2124,H92-1008,0,0.411086,"fferences are a function of application task and task/strategy interactions. A PARADISE assessment of the contribution of each evaluation measure to overall performance shows that strategy-dependent dialogue phenomena as well as phenomena associated with speech recognition significantly predict performance. Our results identify the conditions under which TOOT' s cooperative response strategy leads to greater agent performance. 1 2 focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation TOOT TOOT allows users to access online AMTRAK train schedules via a telephone dialogue, as in Figure 1.1 (A"
C98-2124,H92-1005,0,0.0710485,"pute a Mean Recognition score per dialogue. We also listened to the recordings to determine how many times the user interrupted the agent (Barge Ins). To measure dialogue efficiency, the number of System Turns and User Turns were extracted from the dialogue manager log, and the total Elapsed Time was determined from the recording. To measure user satisfaction 4, users responded to the web survey in Figure 4, which assessed their subjective evaluation of the agent's performance. Each question was designed to measure a pattie4Questionnaire-based user satisfaction ratings (Shriberg et al., 1992; Polifroni et al., 1992) have been frequentlyused in the literature as an external indicatorof agent usability. • Was the system easy to understand in this conversation? (TTS Performance) • In this conversation, did the system understand what you said? (ASR Performance) • In this conversation, was it easy to find the schedule you wanted? (Task Ease) • Was the pace of interaction with the system appropriate in this conversation? (Interaction Pace) o In this conversation, did you know what you could say at each point of the dialogue? (User Expertise) • How often was the system sluggish and slow to reply to you in this"
C98-2124,H92-1009,0,0.225674,"ASR rejections, to compute a Mean Recognition score per dialogue. We also listened to the recordings to determine how many times the user interrupted the agent (Barge Ins). To measure dialogue efficiency, the number of System Turns and User Turns were extracted from the dialogue manager log, and the total Elapsed Time was determined from the recording. To measure user satisfaction 4, users responded to the web survey in Figure 4, which assessed their subjective evaluation of the agent's performance. Each question was designed to measure a pattie4Questionnaire-based user satisfaction ratings (Shriberg et al., 1992; Polifroni et al., 1992) have been frequentlyused in the literature as an external indicatorof agent usability. • Was the system easy to understand in this conversation? (TTS Performance) • In this conversation, did the system understand what you said? (ASR Performance) • In this conversation, was it easy to find the schedule you wanted? (Task Ease) • Was the pace of interaction with the system appropriate in this conversation? (Interaction Pace) o In this conversation, did you know what you could say at each point of the dialogue? (User Expertise) • How often was the system sluggish and slow"
C98-2124,P97-1035,1,0.838789,"han literal strategy contributes to greater performance. database queries in TOOT, a spoken dialogue agent Introduction The notion of a cooperative response has been the for accessing online train schedules via a telephone conversation. We conduct an experiment in which 12 users carry out 4 tasks of varying difficulty with one of two versions of TOOT (literal and cooperative TOOT), resulting in a corpus of 48 dialogues. The values for a wide range of evaluation measures are then extracted from this corpus. We analyze our data using both traditional hypothesis testing methods and the PARADISE (Walker et al., 1997; Walker et al., 1998) methodology for estimating a performance function. Hypothesis testing shows that while differences among some evaluation measures depend on the response strategy (literal or cooperative), other differences are a function of application task and task/strategy interactions. A PARADISE assessment of the contribution of each evaluation measure to overall performance shows that strategy-dependent dialogue phenomena as well as phenomena associated with speech recognition significantly predict performance. Our results identify the conditions under which TOOT' s cooperative resp"
C98-2214,P95-1019,0,0.0494305,"Missing"
C98-2214,P89-1025,0,0.00999431,"per illustrates a novel technique by which an agent can learn to choose an optimal dialogue strategy. We illustrate our technique with ELVIS, an agent that supports access to email by phone, with strategies for initiative, and for reading and sum1350 Here, our method was illustrated by evaluating strategies for managing initiative and for message presentation. However there are numerous dialogue strategies that an agent might use, e.g. to gather information, handle errors, or manage the dialogue interaction (Chu-Carroll and Carberry, 1995; Danieli and Gerbino, 1995; Hovy, 1993; McKeown, 1985; Moore and Paris, 1989). Previous work in natural language generation has proposed heuristics to determine an agent&apos;s choice of dialogue strategy, based on factors such as discourse focus, medium, style, and the content of previous explanations (McKeown, 1985; Moore and Paris, 1989; Maybury, 1991; Hovy, 1993). It should be possible to test experimentally whether an agent can automatically learn these heuristics since the methodology we propose is general, and could be applied to any dialogue strategy choice that an agent might make. Previous work has also proposed that an agent&apos;s choice of dialogue strategy can be t"
C98-2214,P97-1035,1,0.721483,"agent can learn to choose an optimal dialogue strategy. While it is widely agreed that dialogue strategies should be formulated in terms of communicative intentions, there has been little work on automatically optimizing an agent&apos;s choices when there are multiple ways to realize a communicative intention. Our method is based on a combination of learning algorithms and empirical evaluation techniques. The learning component of our method is based on algorithms for reinforcement learning, such as dynamic programming and Q-learning. The empirical component uses the PARADISE evaluation framework (Walker et al., 1997) to identify the important peribrmance factors and to provide the performance function needed by the learning algorithm. We illustrate our method with a dialogue agent named ELVIS (EmaiL Voice Interactive System), that supports access to email over the phone. We show how ELVIS can learn to choose among alternate strategies for agent initiative, for reading messages, and for summarizing email folders. 1 Introduction This paper describes a novel method by which a dialogue agent can learn to choose an optimal dialogue strategy. The main problem for dialogue agents is deciding what information to"
D13-1036,D09-1030,0,0.0315412,"standard data and because we were not sure how difficult the task is. It is clearly subjective. To calculate the accuracy of each method, we computed the average correlation coefficient between each pair of raters and eliminate the 5 lowest scoring workers. We then used the perceptions of the 10 remaining workers to calculate accuracy as # of correct answers / total # of answers. In general, deciding when a MTurk worker is unreliable when the data is subjective is a difficult problem. In the future we plan to test other solutions to measuring annotator reliability as proposed in related work (Callison-Burch, 2009; Snow et al., 2008; Karger et al., 2011; Dawid and Skene, 1979; Welinder et al., 2010; Liu et al., 2012). 3.2 Results We report our results in terms of overall accuracy. Because the Mechanical Turk task is a chooseone question rather than a binary classification, Precision = Recall in our experimental results: True Positive = Number of Correct Answers True Negative = Number of Correct Answers False Positive = Number of Incorrect Answers False Positive = Number of Incorrect Answers True Positive Precision = True Positive + False Positive True Positive Recall = True Positive + False Negative Th"
D13-1036,P08-1090,0,0.622188,"2011; Chiarcos, 2012; Pitler et al., 2009; Lin et al., 2010), but we do not because the results have been mixed. In particular these discourse taggers are trained on The Wall Street Journal (WSJ) and are unlikely to work well on our data. We define an event as a verb lemma with its subject and object. Two events are considered equal if they have the same verb. We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should also be the same (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Riaz and Girju, 2010; Manshadi et al., 2008). Word sense ambiguities are also reduced in specific genres (Action and Romance) of film scenes. Our method for estimating the likelihood of a CONTINGENT relations between events consists of four steps: 1. TEXT PROCESSING: We use Stanford CoreNLP to annotate the corpus document by document and store the annotated text in XML format (Sec. 2.1); 2. COMPUTE EVENT REPRESENTATIONS: We form intermediate artifacts such as events, protagonists and event pairs from the annotated documents. Each event has its ar"
D13-1036,P09-1068,0,0.662759,"ff-hand towards him. Kirsten pulls on her robe, lights a cigarette, sits fishing for her slippers. Figure 1: Opening Scene from Total Recall Introduction Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010). Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Recent work in NLP has tackled the inference of relations between events from a broad range of perspectives: (1) as inference of a discourse relations (e.g. the Penn Discourse Treebank (PDTB) CON TINGENT relation and its specializations); (2) as a type of common sense reasoning; (3) as part of text understanding to support question-answering; and (4) as way of learning script-like or plot-like knowledge structures. All these lines of work aim to model narrative understanding, i.e. to enable systems to infer which events are likely to have happened even thoug"
D13-1036,P12-2042,0,0.0112765,"B →A should be much less frequent than A → B. We assume that both the CAUSE and CONDITION subtypes of the CONTIN GENCY relation will result in pairs of events that are likely to occur together and in a particular order. In particular we assume that the subtypes of the PDTB taxonomy of Contingency.Cause.Reason and Contingency.Cause.Result are the most likely to occur together as noted in previous work. Other related work has made use of discourse connectives or discourse taggers (implicit discourse relations) to provide additional evidence of CONTINGENCY (Do et al., 2011; Gordon et al., 2011; Chiarcos, 2012; Pitler et al., 2009; Lin et al., 2010), but we do not because the results have been mixed. In particular these discourse taggers are trained on The Wall Street Journal (WSJ) and are unlikely to work well on our data. We define an event as a verb lemma with its subject and object. Two events are considered equal if they have the same verb. We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should also be the same (Chambers and Jurafsk"
D13-1036,D11-1027,0,0.643853,"when it is not preceded by A and that B →A should be much less frequent than A → B. We assume that both the CAUSE and CONDITION subtypes of the CONTIN GENCY relation will result in pairs of events that are likely to occur together and in a particular order. In particular we assume that the subtypes of the PDTB taxonomy of Contingency.Cause.Reason and Contingency.Cause.Result are the most likely to occur together as noted in previous work. Other related work has made use of discourse connectives or discourse taggers (implicit discourse relations) to provide additional evidence of CONTINGENCY (Do et al., 2011; Gordon et al., 2011; Chiarcos, 2012; Pitler et al., 2009; Lin et al., 2010), but we do not because the results have been mixed. In particular these discourse taggers are trained on The Wall Street Journal (WSJ) and are unlikely to work well on our data. We define an event as a verb lemma with its subject and object. Two events are considered equal if they have the same verb. We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should a"
D13-1036,D10-1008,0,0.138117,"Missing"
D13-1036,W10-4310,0,0.0378755,"cal applications in commonsense reasoning, infor369 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 369–379, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics mation retrieval, question answering, narrative understanding and inferring discourse relations. We model this likelihood between events by drawing on the PTDB’s general definition of the CONTINGENT relation, which encapsulates relations elsewhere called CAUSE, CONDITION and ENABLE MENT (Prasad et al., 2008a; Lin et al., 2010; Pitler et al., 2009; Louis et al., 2010). Our aim in this paper is to implement and evaluate a range of different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We first utilize a corpus of scene descriptions from films because they are guaranteed to have an explicit narrative structure. Moreover, screenplay scene descriptions tend to be told in temporal order (Beamer and Girju, 2009; Gordon and Swanson, 2009), which makes them a good resource for learning about contingencies between events. In addition, scenes in film represent many typical sequences from real life, while providing a"
D13-1036,P09-1077,0,0.104416,"knowledge has practical applications in commonsense reasoning, infor369 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 369–379, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics mation retrieval, question answering, narrative understanding and inferring discourse relations. We model this likelihood between events by drawing on the PTDB’s general definition of the CONTINGENT relation, which encapsulates relations elsewhere called CAUSE, CONDITION and ENABLE MENT (Prasad et al., 2008a; Lin et al., 2010; Pitler et al., 2009; Louis et al., 2010). Our aim in this paper is to implement and evaluate a range of different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We first utilize a corpus of scene descriptions from films because they are guaranteed to have an explicit narrative structure. Moreover, screenplay scene descriptions tend to be told in temporal order (Beamer and Girju, 2009; Gordon and Swanson, 2009), which makes them a good resource for learning about contingencies between events. In addition, scenes in film represent many typical sequences from real lif"
D13-1036,prasad-etal-2008-penn,0,0.789822,"are likely to happen in the future. Such knowledge has practical applications in commonsense reasoning, infor369 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 369–379, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics mation retrieval, question answering, narrative understanding and inferring discourse relations. We model this likelihood between events by drawing on the PTDB’s general definition of the CONTINGENT relation, which encapsulates relations elsewhere called CAUSE, CONDITION and ENABLE MENT (Prasad et al., 2008a; Lin et al., 2010; Pitler et al., 2009; Louis et al., 2010). Our aim in this paper is to implement and evaluate a range of different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We first utilize a corpus of scene descriptions from films because they are guaranteed to have an explicit narrative structure. Moreover, screenplay scene descriptions tend to be told in temporal order (Beamer and Girju, 2009; Gordon and Swanson, 2009), which makes them a good resource for learning about contingencies between events. In addition, scenes in film repres"
D13-1036,D08-1027,0,0.0450439,"Missing"
D13-1036,walker-etal-2012-annotated,1,0.777989,"Missing"
D13-1036,C10-2172,0,0.0436439,"Missing"
D15-1019,P11-1016,0,0.0254261,"d “gift box” may represent two different concepts with different polarities in similes. We replaced personal pronouns (e.g., he, she) with a general PERSON token and other pronouns (e.g., it, this, that) with a general IT token. Table 2 presents examples of positive and negative similes in the annotated data set. timent, research has also focused on understanding people’s sentiment during specific events such as stock market fluctuations, presidential elections, Oscars, tsunamis, or toward entities such as movies, companies, or aspects of a product (Bollen et al., 2011; Thelwall et al., 2011; Jiang et al., 2011; Hu and Liu, 2004; Jo and Oh, 2011). To our knowledge, we are the first to explore recognition of affective polarity in similes as a whole, where the polarity relates to an act or state of the tenor. Unlike previous work, we do not rely on the presence of explicit properties. We also present a data set annotated with affective polarity in similes, and experiment with both manually annotated and automatically acquired training data. 3 Simile Data Set Creation One of the major challenges of supervised classification is acquiring sufficient labeled data for training, since manual annotation is t"
D15-1019,J96-2004,0,0.206256,"ire clause in place of the vehicle (e.g., “I feel like im gonna puke”). Other times, the informal text of Twitter makes the tweet hard to parse (e.g., “he is like whatttt”) or a verb occurs after “like” (e.g., “he is like hyperventilating”). The invalid label covers these types of erroneously extracted similes. The annotation task was first conducted on a small sample of 50 similes, to select workers that had high annotation agreement with each other and gold standard labels we prepared. The best three workers then all annotated the official set of 1500 similes. The average Cohen’s Kappa (κ) (Carletta, 1996) between each pair of annotators was 0.69. We then assigned the final label through majority vote. However, none of the annotators agreed on the same label for 78 of the 1500 similes, and 303 instances were labeled as invalid similes by the annotators. So we removed these 381 instances from the annotated data set. Finally, we randomly divided the remaining similes into an evaluation (Eval) set of 741 similes, and a development (Dev) set of 378 similes. Table 3 shows the label distribution of these sets. 3.2 # of Similes (Dev Data) 164 181 33 378 # of Similes (Eval Data) 312 343 86 741 Table 3:"
D15-1019,C10-2028,0,0.0318407,"Li et al. (2012) used similar patterns to retrieve similes and determine basic sentiment toward simile vehicles across different languages using the compared properties. One major difference with their work and ours is that they determine sentiment or affective perception toward entities or concepts extracted from simile vehicles. In contrast, our work is focused Our work is also related to sentiment analysis in general. The most common approach applies supervised classification with features such as ngrams, parts-of-speech, punctuation, lexicon features, etc. (e.g., (Kouloumpis et al., 2011; Davidov et al., 2010; Mohammad et al., 2013)). To overcome the challenge of acquiring manually labeled data, some work automatically collects noisy training data using emoticons and hashtags (e.g., (Go et al., 2009; Purver and Battersby, 2012)). In addition to determining overall sen191 longer tweet, and repeated the process. We used the UIUC Chunker (Punyakanok and Roth, 2001) to identify phrase sequences with the syntax of similes (e.g., NP1 + VP + PP-like + NP2 , or NP1 + VP + ADJP + PP-like + NP2 , where NP1 is the tenor, NP2 is the vehicle, VP is the event, and ADJP is any explicitly mentioned property). We"
D15-1019,P14-5010,0,0.0031431,"etermining overall sen191 longer tweet, and repeated the process. We used the UIUC Chunker (Punyakanok and Roth, 2001) to identify phrase sequences with the syntax of similes (e.g., NP1 + VP + PP-like + NP2 , or NP1 + VP + ADJP + PP-like + NP2 , where NP1 is the tenor, NP2 is the vehicle, VP is the event, and ADJP is any explicitly mentioned property). We generalized over the extracted similes by removing the comparator, and the optional explicit property component. Our simile representation is thus a triple of the tenor, event and vehicle. We also lemmatized all words using Stanford CoreNLP (Manning et al., 2014). For a tenor phrase, we kept only the head noun, which is usually sufficient to understand the affective polarity target. We kept the entire noun phrase for the vehicle, since vehicles like “ice box” and “gift box” may represent two different concepts with different polarities in similes. We replaced personal pronouns (e.g., he, she) with a general PERSON token and other pronouns (e.g., it, this, that) with a general IT token. Table 2 presents examples of positive and negative similes in the annotated data set. timent, research has also focused on understanding people’s sentiment during speci"
D15-1019,P13-1174,0,0.0401469,"tive and negative properties that appear with the vehicle in our corpus. We look for the property words in the combined AFINN and MPQA sentiment lexicons. Sentiment Classifier Label: We use 2 binary features (one for positive and one for negative) to represent the label that the NRC-Canada Sentiment Classifier assigns to a simile. Simile Connotation Polarity: We use 2 binary features (one for positive and one for negative) to indicate the overall connotation of a simile. We count whether the number of positive (or negative) connotation words is greater in a simile using a Connotation Lexicon (Feng et al., 2013), which contains 30,881 words with positive connotation and 33,724 words with negative connotation. 5 5.1 Evaluation Classification Performance with Manually Annotated Data Table 4 presents the results for supervised classification with our manually annotated data set using 10-fold cross-validation. As baselines, we used existing sentiment resources as described in Section 3.2, but now applied to evaluation data. We also used the connotation lexicon (Feng et al., 2013) the same way as the MPQA sentiment lexicon (Wilson et al., 2005) to compare as an additional baseline. The top section of Tabl"
D15-1019,S13-2053,0,0.0221242,"similar patterns to retrieve similes and determine basic sentiment toward simile vehicles across different languages using the compared properties. One major difference with their work and ours is that they determine sentiment or affective perception toward entities or concepts extracted from simile vehicles. In contrast, our work is focused Our work is also related to sentiment analysis in general. The most common approach applies supervised classification with features such as ngrams, parts-of-speech, punctuation, lexicon features, etc. (e.g., (Kouloumpis et al., 2011; Davidov et al., 2010; Mohammad et al., 2013)). To overcome the challenge of acquiring manually labeled data, some work automatically collects noisy training data using emoticons and hashtags (e.g., (Go et al., 2009; Purver and Battersby, 2012)). In addition to determining overall sen191 longer tweet, and repeated the process. We used the UIUC Chunker (Punyakanok and Roth, 2001) to identify phrase sequences with the syntax of similes (e.g., NP1 + VP + PP-like + NP2 , or NP1 + VP + ADJP + PP-like + NP2 , where NP1 is the tenor, NP2 is the vehicle, VP is the event, and ADJP is any explicitly mentioned property). We generalized over the ext"
D15-1019,S15-2080,0,0.0421555,"d sentiment expressed through metaphor. Rumbell et al. (2008) presented an analysis of animals that are metaphorically used to describe a person. Rentoumi et al. (2009) determined use of figurative language by disambiguating word senses, and then determined sentiment polarity at the sense level using ngram graph similarity. Wallington et al. (2011) identified affect in metaphor and similes when a comparison is made with an animal (e.g., dog, fox) or mythical creature (e.g., dragon, angel) by analyzing WordNet sense glosses of the compared terms. More recently, the SemEval-2015 Shared Task 11 (Ghosh et al., 2015) has addressed the sentiment analysis of figurative language such as irony, metaphor and sarcasm in Twitter. Niculae and Danescu-Niculescu-Mizil (2014) created a simile data set from Amazon product reviews, and determined when comparisons are figurative. They did not identify affective polarity, but showed that sentiment and figurative comparisons are correlated. Fishelov (2007) conducted a study of 16 similes where the connection between tenor and vehicle is obvious or not obvious, and when a conventional or unconventional explicit property is present or absent. Fishelov analyzed responses fr"
D15-1019,S13-2052,0,0.0426472,"itive/negative lexicon words. This method yields 629 positive and 522 negative similes. Using Sentiment Classifiers: We create our third training data set using a state-of-the-art sentiment classifier designed for tweets. For this, we reimplemented the NRC Canada sentiment classifier (Zhu et al., 2014) using the same set of features described by the authors. We use a Java implementation2 of SVM from LIBLINEAR (Fan et al., 2008), with the original parameter values used by the NRC Canada system. We trained the sentiment classifier with all of the tweet training data from SemEval 2013 subtask B (Nakov et al., 2013). We label a simile as positive or negative if the sentiment classifier labels it as positive or negative, respectively. This method yields 1185 positive and 402 negative similes. Using Sentiment in Surrounding Words: The previous approaches for labeling training instances will primarily identify similes that contain one or more strongly affective words. This Automatically Labeled Similes For any new domain (e.g., Amazon product reviews), manual annotations for supervised training 2 193 http://liblinear.bwaldvogel.de/ 4 can potentially bias the training data and limit the classifier’s ability"
D15-1019,D14-1215,0,0.240106,"nce (Li et al., 2012; Fishelov, 2007). Sometimes, the sentiment of a simile is expressed explicitly, such as “Jane swims beautifully like a dolphin!”. But in many cases the sentiment is implicit, evoked entirely from the comparison itself. “Jane swims like a dolphin” is easily understood to be a compliment toward Jane’s swimming ability because dolphins are known to be excellent swimmers. A simile consists of four key components: the topic or tenor (subject of the comparison), the vehicle (object of the comparison), the event (act or state), and a comparator (usually “as”, “like”, or “than”) (Niculae and Danescu-Niculescu-Mizil, 2014). A property (shared attribute) can be optionally included as well (e.g., “He is as red as Simile smells like bananas stinks like bananas smells like rotten bananas smells like garbage memory like an elephant memory like a sieve looks like a celebrity acts like a celebrity Polarity neutral negative negative negative positive negative positive negative Table 1: Simile Examples with Affective Polarity. However, the affective polarity of a simile often emerges from multiple component terms. For instance, all of the words in Examples (e) and (f) have neutral polarity. But Example (e) is positive b"
D15-1019,S14-2077,0,0.015085,"raining data set is created using the 2,718 positive words and 4,910 negative words from the MPQA lexicon (Wilson et al., 2005). We applied the CMU part-of-speech tagger for tweets (Owoputi et al., 2013) to match the MPQA parts-of-speech for each word. We assign positive/negative polarity to similes with more positive/negative lexicon words. This method yields 629 positive and 522 negative similes. Using Sentiment Classifiers: We create our third training data set using a state-of-the-art sentiment classifier designed for tweets. For this, we reimplemented the NRC Canada sentiment classifier (Zhu et al., 2014) using the same set of features described by the authors. We use a Java implementation2 of SVM from LIBLINEAR (Fan et al., 2008), with the original parameter values used by the NRC Canada system. We trained the sentiment classifier with all of the tweet training data from SemEval 2013 subtask B (Nakov et al., 2013). We label a simile as positive or negative if the sentiment classifier labels it as positive or negative, respectively. This method yields 1185 positive and 402 negative similes. Using Sentiment in Surrounding Words: The previous approaches for labeling training instances will prima"
D15-1019,N13-1039,0,0.0141311,"exicon (Nielsen, 2011) containing 2,477 manually labeled words with integer values ranging from -5 (negativity) to 5 (positivity). For each simile, we sum the sentiment scores for all lexicon words in the simile components, assigning positive/negative polarity depending on whether the sum is positive/negative. This method yields 460 positive and 423 negative similes. Using MPQA Sentiment Lexicon Words: Our second training data set is created using the 2,718 positive words and 4,910 negative words from the MPQA lexicon (Wilson et al., 2005). We applied the CMU part-of-speech tagger for tweets (Owoputi et al., 2013) to match the MPQA parts-of-speech for each word. We assign positive/negative polarity to similes with more positive/negative lexicon words. This method yields 629 positive and 522 negative similes. Using Sentiment Classifiers: We create our third training data set using a state-of-the-art sentiment classifier designed for tweets. For this, we reimplemented the NRC Canada sentiment classifier (Zhu et al., 2014) using the same set of features described by the authors. We use a Java implementation2 of SVM from LIBLINEAR (Fan et al., 2008), with the original parameter values used by the NRC Canad"
D15-1019,E12-1049,0,0.0265542,"is that they determine sentiment or affective perception toward entities or concepts extracted from simile vehicles. In contrast, our work is focused Our work is also related to sentiment analysis in general. The most common approach applies supervised classification with features such as ngrams, parts-of-speech, punctuation, lexicon features, etc. (e.g., (Kouloumpis et al., 2011; Davidov et al., 2010; Mohammad et al., 2013)). To overcome the challenge of acquiring manually labeled data, some work automatically collects noisy training data using emoticons and hashtags (e.g., (Go et al., 2009; Purver and Battersby, 2012)). In addition to determining overall sen191 longer tweet, and repeated the process. We used the UIUC Chunker (Punyakanok and Roth, 2001) to identify phrase sequences with the syntax of similes (e.g., NP1 + VP + PP-like + NP2 , or NP1 + VP + ADJP + PP-like + NP2 , where NP1 is the tenor, NP2 is the vehicle, VP is the event, and ADJP is any explicitly mentioned property). We generalized over the extracted similes by removing the comparator, and the optional explicit property component. Our simile representation is thus a triple of the tenor, event and vehicle. We also lemmatized all words using"
D15-1019,R09-1067,0,0.0712736,"for different domains. on determining affective polarity of a simile as a whole, where the affective polarity typically relates to an act or state of the tenor. In many cases, a simile vehicle does not have positive or negative polarity by itself. For example, “sauna” is not a positive or negative concept, but “room feels like a sauna” is a negative simile because it suggests that the room is humid and unpleasant. 2 Previous research has also explored sentiment expressed through metaphor. Rumbell et al. (2008) presented an analysis of animals that are metaphorically used to describe a person. Rentoumi et al. (2009) determined use of figurative language by disambiguating word senses, and then determined sentiment polarity at the sense level using ngram graph similarity. Wallington et al. (2011) identified affect in metaphor and similes when a comparison is made with an animal (e.g., dog, fox) or mythical creature (e.g., dragon, angel) by analyzing WordNet sense glosses of the compared terms. More recently, the SemEval-2015 Shared Task 11 (Ghosh et al., 2015) has addressed the sentiment analysis of figurative language such as irony, metaphor and sarcasm in Twitter. Niculae and Danescu-Niculescu-Mizil (201"
D15-1019,P12-2015,0,0.197003,"les. Related Work Although similes are a popular form of comparison, there has been relatively little prior research on understanding affective polarity in similes. Veale and Hao (2007) created a large simile case-base using the pattern “as ADJ as a/an NOUN”. They collected similes by querying the web after instantiating part of the pattern with adjectives, and then had a human annotate 30,991 of the extracted similes for validity. Their focus was on extracting salient properties associated with simile vehicles, and the affective perception on vehicles that the salient properties bring about. Veale (2012) took a step further and automatically recognized the affect toward vehicles when properties reinforce each other (e.g., hot and humid). They built a support graph of properties and determined how they connect to unambiguous positive and negative words. Li et al. (2012) used similar patterns to retrieve similes and determine basic sentiment toward simile vehicles across different languages using the compared properties. One major difference with their work and ours is that they determine sentiment or affective perception toward entities or concepts extracted from simile vehicles. In contrast,"
D15-1019,H05-1044,0,0.273041,"n Words: Our first training data set is created using the AFINN sentiment lexicon (Nielsen, 2011) containing 2,477 manually labeled words with integer values ranging from -5 (negativity) to 5 (positivity). For each simile, we sum the sentiment scores for all lexicon words in the simile components, assigning positive/negative polarity depending on whether the sum is positive/negative. This method yields 460 positive and 423 negative similes. Using MPQA Sentiment Lexicon Words: Our second training data set is created using the 2,718 positive words and 4,910 negative words from the MPQA lexicon (Wilson et al., 2005). We applied the CMU part-of-speech tagger for tweets (Owoputi et al., 2013) to match the MPQA parts-of-speech for each word. We assign positive/negative polarity to similes with more positive/negative lexicon words. This method yields 629 positive and 522 negative similes. Using Sentiment Classifiers: We create our third training data set using a state-of-the-art sentiment classifier designed for tweets. For this, we reimplemented the NRC Canada sentiment classifier (Zhu et al., 2014) using the same set of features described by the authors. We use a Java implementation2 of SVM from LIBLINEAR"
E17-1070,W11-1701,1,0.0637152,"ummaries increase belief change, summary tools for such arguments are still under development (Misra et al., 2015). However, perhaps high quality summaries may not be needed if compelling argument fragments can be automatically extracted (Misra et al., 2016b; Subba and Di Eugenio, 2007; Nguyen While here we used crowdsourced judgments to select arguments of particular types. Elsewhere, we present algorithms for automatically identifying and bootstrapping arguments with different properties. We have methods to extract arguments that represent different stances on an issue (Misra et al., 2016a; Anand et al., 2011; Sridhar et al., 2015; Walker et al., 2012a; Walker et al., 2012b), as well as argument exchanges that are agreements vs. disagreements (Misra and Walker, 2015), factual vs. emotional arguments (Oraby et al., 2015), sarcastic and not-sarcastic arguments, and nasty vs. nice arguments (Oraby et al., 2016; Lukin and Walker, 2013; Justo et al., 2014). An open question is to whether these effects are long term. Our approach limits us to examining belief change during a single session for practical reasons; long-term cross-session comparisons lead to significant participant retention issues. Our re"
E17-1070,W11-0707,0,0.0134619,"susceptibility to particular arguments or types of arguments (Anderson, 1971; Davies, 1998; Devine et al., 2000; Petty et al., 1981). Behavioral economics research shows that the cognitive style of the audience interacts with the argument’s emotional appeal: emphasizing personal losses is more persuasive for neurotics, whereas gains are effective for extraverts (Carver et al., 2000; Mann et al., 2004). The SOURCE is the speaker, whose influence may depend on factors such as attractiveness, expertise, trustworthiness or group identification or homophily (Eagly and Chaiken, 1975; Kelman, 1961; Bender et al., 2011; Luchok and McCroskey, 1978; Ludford et al., 2004; McPherson et al., 2001). We present experiments evaluating how properties of social media arguments interact with audience factors to affect belief change. We compare the effects of two aspects of the ARGUMENT: whether it is monologic or dialogic, and whether it is factual or emotional. We also examine how these factors interact with properties of the AUDI ENCE . We profile audience prior beliefs to test if • Can we mine social media to find arguments that change people’s beliefs? • Do different argument types have different effects on belief"
E17-1070,W16-6209,0,0.0765728,"nge has primarily focused on single, experimentally crafted, persuasive messages, rather than exploring whether usergenerated dialogic arguments can be repurposed to persuade. Recently however several papers have begun to investigate two challenges in argument mining: (1) understanding the structure of an argument and extracting argument components (Lippi and Torroni, 2015; Nguyen and Litman, 2015; Stab and Gurevych, 2014; Lippi and Torroni, 2015; Biran and Rambow, 2011); and (2) understanding what predicts the persuasiveness of web-sourced argumentative content (Habernal and Gurevych, 2016b; Fang et al., 2016; Wachsmuth et al., 2016; Habernal and Gurevych, 2016a; Tan et al., 2016). Tan et al. (2016) study belief change in the Reddit /r/ChangeMyView subreddit (CMV), in which an original poster (OP) challenges others to change his/her opinion. They build logistic regression models to predict argument success, identifying two conversational dynamic factors: a) early potential persuaders are more successful and b) after 4 exchanges, the chance of persuasion drops virtually to zero. Linguistic factors of persuasive posts include: a) dissimilar content words to the OP, b) similar stop words, c) being le"
E17-1070,W14-2107,0,0.0260422,"r different personality types are more open to different types of arguments, e.g., we hypothesize that people who are highly agreeable (A) might be more affected by the combative style of emotional arguments. We provide a new corpus for the research community of audience personality profiles, arguments, and belief change measurements.1 Audience factors have been explored in social psychological work on persuasion, but have been neglected in computational work, which has largely drawn from sentiment, rhetorical, or argument structure models (Habernal and Gurevych, ˇ 2016b; Conrad et al., 2012; Boltuzic and Snajder, 2014; Choi and Cardie, 2008). We demonstrate that, indeed, undecided people respond differently to arguments than entrenched people, and that the responses of undecided people correlate with personality. We show that this holds across an array of different arguments. Our research questions are: Curated Summary: Death Penalty PRO: Proponents of the death penalty say it is an important tool for preserving law and order, deters crime, and costs less than life imprisonment. They argue that retribution or ”an eye for an eye” honors the victim, helps console grieving families, and ensures that the perpe"
E17-1070,D08-1083,0,0.00628415,"s are more open to different types of arguments, e.g., we hypothesize that people who are highly agreeable (A) might be more affected by the combative style of emotional arguments. We provide a new corpus for the research community of audience personality profiles, arguments, and belief change measurements.1 Audience factors have been explored in social psychological work on persuasion, but have been neglected in computational work, which has largely drawn from sentiment, rhetorical, or argument structure models (Habernal and Gurevych, ˇ 2016b; Conrad et al., 2012; Boltuzic and Snajder, 2014; Choi and Cardie, 2008). We demonstrate that, indeed, undecided people respond differently to arguments than entrenched people, and that the responses of undecided people correlate with personality. We show that this holds across an array of different arguments. Our research questions are: Curated Summary: Death Penalty PRO: Proponents of the death penalty say it is an important tool for preserving law and order, deters crime, and costs less than life imprisonment. They argue that retribution or ”an eye for an eye” honors the victim, helps console grieving families, and ensures that the perpetrators of heinous crime"
E17-1070,D16-1129,0,0.505196,"b. Previous work on belief change has primarily focused on single, experimentally crafted, persuasive messages, rather than exploring whether usergenerated dialogic arguments can be repurposed to persuade. Recently however several papers have begun to investigate two challenges in argument mining: (1) understanding the structure of an argument and extracting argument components (Lippi and Torroni, 2015; Nguyen and Litman, 2015; Stab and Gurevych, 2014; Lippi and Torroni, 2015; Biran and Rambow, 2011); and (2) understanding what predicts the persuasiveness of web-sourced argumentative content (Habernal and Gurevych, 2016b; Fang et al., 2016; Wachsmuth et al., 2016; Habernal and Gurevych, 2016a; Tan et al., 2016). Tan et al. (2016) study belief change in the Reddit /r/ChangeMyView subreddit (CMV), in which an original poster (OP) challenges others to change his/her opinion. They build logistic regression models to predict argument success, identifying two conversational dynamic factors: a) early potential persuaders are more successful and b) after 4 exchanges, the chance of persuasion drops virtually to zero. Linguistic factors of persuasive posts include: a) dissimilar content words to the OP, b) similar sto"
E17-1070,W12-3810,0,0.0644287,"Missing"
E17-1070,P16-1150,0,0.412639,"b. Previous work on belief change has primarily focused on single, experimentally crafted, persuasive messages, rather than exploring whether usergenerated dialogic arguments can be repurposed to persuade. Recently however several papers have begun to investigate two challenges in argument mining: (1) understanding the structure of an argument and extracting argument components (Lippi and Torroni, 2015; Nguyen and Litman, 2015; Stab and Gurevych, 2014; Lippi and Torroni, 2015; Biran and Rambow, 2011); and (2) understanding what predicts the persuasiveness of web-sourced argumentative content (Habernal and Gurevych, 2016b; Fang et al., 2016; Wachsmuth et al., 2016; Habernal and Gurevych, 2016a; Tan et al., 2016). Tan et al. (2016) study belief change in the Reddit /r/ChangeMyView subreddit (CMV), in which an original poster (OP) challenges others to change his/her opinion. They build logistic regression models to predict argument success, identifying two conversational dynamic factors: a) early potential persuaders are more successful and b) after 4 exchanges, the chance of persuasion drops virtually to zero. Linguistic factors of persuasive posts include: a) dissimilar content words to the OP, b) similar sto"
E17-1070,N15-1046,1,0.327685,"ts however expressed. And Agreeable people may also be motivated to change belief by emotional arguments because they are less likely to be influenced by personal feelings. Our results have numerous implications that suggest further technical experimentation. The fact that we can induce belief change by extracting simple discussion fragments suggests that belief change can be induced without the application of sophisticated text processing tools. While our results for balanced monologs suggest that summaries increase belief change, summary tools for such arguments are still under development (Misra et al., 2015). However, perhaps high quality summaries may not be needed if compelling argument fragments can be automatically extracted (Misra et al., 2016b; Subba and Di Eugenio, 2007; Nguyen While here we used crowdsourced judgments to select arguments of particular types. Elsewhere, we present algorithms for automatically identifying and bootstrapping arguments with different properties. We have methods to extract arguments that represent different stances on an issue (Misra et al., 2016a; Anand et al., 2011; Sridhar et al., 2015; Walker et al., 2012a; Walker et al., 2012b), as well as argument exchang"
E17-1070,S16-1068,1,0.840639,"uenced by personal feelings. Our results have numerous implications that suggest further technical experimentation. The fact that we can induce belief change by extracting simple discussion fragments suggests that belief change can be induced without the application of sophisticated text processing tools. While our results for balanced monologs suggest that summaries increase belief change, summary tools for such arguments are still under development (Misra et al., 2015). However, perhaps high quality summaries may not be needed if compelling argument fragments can be automatically extracted (Misra et al., 2016b; Subba and Di Eugenio, 2007; Nguyen While here we used crowdsourced judgments to select arguments of particular types. Elsewhere, we present algorithms for automatically identifying and bootstrapping arguments with different properties. We have methods to extract arguments that represent different stances on an issue (Misra et al., 2016a; Anand et al., 2011; Sridhar et al., 2015; Walker et al., 2012a; Walker et al., 2012b), as well as argument exchanges that are agreements vs. disagreements (Misra and Walker, 2015), factual vs. emotional arguments (Oraby et al., 2015), sarcastic and not-sarc"
E17-1070,W16-3636,1,0.746343,"uenced by personal feelings. Our results have numerous implications that suggest further technical experimentation. The fact that we can induce belief change by extracting simple discussion fragments suggests that belief change can be induced without the application of sophisticated text processing tools. While our results for balanced monologs suggest that summaries increase belief change, summary tools for such arguments are still under development (Misra et al., 2015). However, perhaps high quality summaries may not be needed if compelling argument fragments can be automatically extracted (Misra et al., 2016b; Subba and Di Eugenio, 2007; Nguyen While here we used crowdsourced judgments to select arguments of particular types. Elsewhere, we present algorithms for automatically identifying and bootstrapping arguments with different properties. We have methods to extract arguments that represent different stances on an issue (Misra et al., 2016a; Anand et al., 2011; Sridhar et al., 2015; Walker et al., 2012a; Walker et al., 2012b), as well as argument exchanges that are agreements vs. disagreements (Misra and Walker, 2015), factual vs. emotional arguments (Oraby et al., 2015), sarcastic and not-sarc"
E17-1070,W15-0503,0,0.0142496,"instead exploring how persuasiveness relates to audience factors and argumentative style. Also our experiments are run online with hundreds of users, rather than as a controlled study in the lab. Previous work on belief change has primarily focused on single, experimentally crafted, persuasive messages, rather than exploring whether usergenerated dialogic arguments can be repurposed to persuade. Recently however several papers have begun to investigate two challenges in argument mining: (1) understanding the structure of an argument and extracting argument components (Lippi and Torroni, 2015; Nguyen and Litman, 2015; Stab and Gurevych, 2014; Lippi and Torroni, 2015; Biran and Rambow, 2011); and (2) understanding what predicts the persuasiveness of web-sourced argumentative content (Habernal and Gurevych, 2016b; Fang et al., 2016; Wachsmuth et al., 2016; Habernal and Gurevych, 2016a; Tan et al., 2016). Tan et al. (2016) study belief change in the Reddit /r/ChangeMyView subreddit (CMV), in which an original poster (OP) challenges others to change his/her opinion. They build logistic regression models to predict argument success, identifying two conversational dynamic factors: a) early potential persuaders"
E17-1070,W13-1104,1,0.776382,"ed judgments to select arguments of particular types. Elsewhere, we present algorithms for automatically identifying and bootstrapping arguments with different properties. We have methods to extract arguments that represent different stances on an issue (Misra et al., 2016a; Anand et al., 2011; Sridhar et al., 2015; Walker et al., 2012a; Walker et al., 2012b), as well as argument exchanges that are agreements vs. disagreements (Misra and Walker, 2015), factual vs. emotional arguments (Oraby et al., 2015), sarcastic and not-sarcastic arguments, and nasty vs. nice arguments (Oraby et al., 2016; Lukin and Walker, 2013; Justo et al., 2014). An open question is to whether these effects are long term. Our approach limits us to examining belief change during a single session for practical reasons; long-term cross-session comparisons lead to significant participant retention issues. Our results also suggest new empirical and theoretical methods for studying persuasion at scale. Only recently have studies of persuasion moved beyond small scale lab studies involving simple single arguments (Habernal and Gurevych, 2016b; Habernal and Gurevych, 2016a; Tan et al., 2016). Our research also suggests new methods and to"
E17-1070,W15-0515,1,0.803994,"Missing"
E17-1070,N06-2022,1,0.680159,"ent types of arguments have different effects: while balanced monologic summaries led to the greatest belief change, socio-emotional online exchanges also caused changes in belief. Our work also suggests the importance of personalization for persuasion: with different personality types being open to different styles of argument. Future work might be based on methods for profiling participant personality from simple online behaviors (Di Eugenio et al., 2013; Liu et al., 2016; Pan and Zhou, 2014; Yee et al., 2011), or from user-generated content such as first-person narratives or conversations (Mairesse and Walker, 2006a; Mairesse and Walker, 2006b; Rahimtoroghi et al., 2016; Rahimtoroghi et al., 2014). We could then select personalized arguments to meet a participant’s processing style. Although our short question/response pairs did not induce as much belief change as the curated balanced monologs, we believe that these are striking results given that the materials we extracted from online discussions are not balanced or professionally produced, but instead are simple fragments extracted from online discussions. Further, confirming prior work on persuasion (Eagly and Chaiken, 1975; Kelman, 1961; Petty et al"
E17-1070,W16-3604,1,0.254426,"e we used crowdsourced judgments to select arguments of particular types. Elsewhere, we present algorithms for automatically identifying and bootstrapping arguments with different properties. We have methods to extract arguments that represent different stances on an issue (Misra et al., 2016a; Anand et al., 2011; Sridhar et al., 2015; Walker et al., 2012a; Walker et al., 2012b), as well as argument exchanges that are agreements vs. disagreements (Misra and Walker, 2015), factual vs. emotional arguments (Oraby et al., 2015), sarcastic and not-sarcastic arguments, and nasty vs. nice arguments (Oraby et al., 2016; Lukin and Walker, 2013; Justo et al., 2014). An open question is to whether these effects are long term. Our approach limits us to examining belief change during a single session for practical reasons; long-term cross-session comparisons lead to significant participant retention issues. Our results also suggest new empirical and theoretical methods for studying persuasion at scale. Only recently have studies of persuasion moved beyond small scale lab studies involving simple single arguments (Habernal and Gurevych, 2016b; Habernal and Gurevych, 2016a; Tan et al., 2016). Our research also sug"
E17-1070,W15-4631,1,0.34486,"els for predicting belief change for Emotional arguments also benefit from information about Conscientiousness and Agreeableness. Row 17 (EMOT+O), Row 18 (EMOT+C) and Row 20 (EMOT+A) all show significant differences in • A new corpus of personality information and belief change in socio-political arguments; • A new method for identifying and deploying social media content to inform and engage the public about important social and political topics; • Results showing at scale (hundreds of users) that we can mine arguments from online discussions to change people’s beliefs; 749 and Litman, 2015; Swanson et al., 2015). • Results showing that different types of arguments have different effects: while balanced monologic summaries led to the greatest belief change, socio-emotional online exchanges also caused changes in belief. Our work also suggests the importance of personalization for persuasion: with different personality types being open to different styles of argument. Future work might be based on methods for profiling participant personality from simple online behaviors (Di Eugenio et al., 2013; Liu et al., 2016; Pan and Zhou, 2014; Yee et al., 2011), or from user-generated content such as first-perso"
E17-1070,W16-3644,1,0.824662,"anced monologic summaries led to the greatest belief change, socio-emotional online exchanges also caused changes in belief. Our work also suggests the importance of personalization for persuasion: with different personality types being open to different styles of argument. Future work might be based on methods for profiling participant personality from simple online behaviors (Di Eugenio et al., 2013; Liu et al., 2016; Pan and Zhou, 2014; Yee et al., 2011), or from user-generated content such as first-person narratives or conversations (Mairesse and Walker, 2006a; Mairesse and Walker, 2006b; Rahimtoroghi et al., 2016; Rahimtoroghi et al., 2014). We could then select personalized arguments to meet a participant’s processing style. Although our short question/response pairs did not induce as much belief change as the curated balanced monologs, we believe that these are striking results given that the materials we extracted from online discussions are not balanced or professionally produced, but instead are simple fragments extracted from online discussions. Further, confirming prior work on persuasion (Eagly and Chaiken, 1975; Kelman, 1961; Petty et al., 1981), we found that these effects depend on audience"
E17-1070,C16-1158,0,0.174104,"ocused on single, experimentally crafted, persuasive messages, rather than exploring whether usergenerated dialogic arguments can be repurposed to persuade. Recently however several papers have begun to investigate two challenges in argument mining: (1) understanding the structure of an argument and extracting argument components (Lippi and Torroni, 2015; Nguyen and Litman, 2015; Stab and Gurevych, 2014; Lippi and Torroni, 2015; Biran and Rambow, 2011); and (2) understanding what predicts the persuasiveness of web-sourced argumentative content (Habernal and Gurevych, 2016b; Fang et al., 2016; Wachsmuth et al., 2016; Habernal and Gurevych, 2016a; Tan et al., 2016). Tan et al. (2016) study belief change in the Reddit /r/ChangeMyView subreddit (CMV), in which an original poster (OP) challenges others to change his/her opinion. They build logistic regression models to predict argument success, identifying two conversational dynamic factors: a) early potential persuaders are more successful and b) after 4 exchanges, the chance of persuasion drops virtually to zero. Linguistic factors of persuasive posts include: a) dissimilar content words to the OP, b) similar stop words, c) being lengthy (in words, sentenc"
E17-1070,P15-1012,1,0.628215,"lief change, summary tools for such arguments are still under development (Misra et al., 2015). However, perhaps high quality summaries may not be needed if compelling argument fragments can be automatically extracted (Misra et al., 2016b; Subba and Di Eugenio, 2007; Nguyen While here we used crowdsourced judgments to select arguments of particular types. Elsewhere, we present algorithms for automatically identifying and bootstrapping arguments with different properties. We have methods to extract arguments that represent different stances on an issue (Misra et al., 2016a; Anand et al., 2011; Sridhar et al., 2015; Walker et al., 2012a; Walker et al., 2012b), as well as argument exchanges that are agreements vs. disagreements (Misra and Walker, 2015), factual vs. emotional arguments (Oraby et al., 2015), sarcastic and not-sarcastic arguments, and nasty vs. nice arguments (Oraby et al., 2016; Lukin and Walker, 2013; Justo et al., 2014). An open question is to whether these effects are long term. Our approach limits us to examining belief change during a single session for practical reasons; long-term cross-session comparisons lead to significant participant retention issues. Our results also suggest new"
E17-1070,N12-1072,1,0.074881,"articipants were pre-qualified using a reading comprehension task that checked their re744 sponses against a gold standard to ensure that they read the arguments carefully. Because we make many comparisons, and our experiments are conducted at large scale, all of our results incorporate Bonferroni corrections. 3.1 Factual: Abortion Q3: Dialog Selection: Identifying Socio-Emotional Arguments R3: Our work requires a new experimental corpus that is sensitive to readers’ prior beliefs and personalities. We utilize online dialogs from 4forums.com downloaded from The Internet Argument Corpus (IAC) (Walker et al., 2012c). The IAC contains quote/response pairs of targeted arguments between two people (Table 1) on topics such as: death penalty, gay marriage, climate change, abortion, evolution and gun control. Each argument is annotated to distinguish arguments making strong appeals to emotional factors versus straightforwardly factual arguments. We selected a subset of extreme exemplars of factual (FACT) versus emotional (EMOT) arguments, defined as Q/R pairs reliably annotated to be at the extreme ends of the fact/emotion scale, i.e. responses with an average ≥ 4 annotation were considered factual, and thos"
E17-1070,C14-1142,0,0.0148982,"suasiveness relates to audience factors and argumentative style. Also our experiments are run online with hundreds of users, rather than as a controlled study in the lab. Previous work on belief change has primarily focused on single, experimentally crafted, persuasive messages, rather than exploring whether usergenerated dialogic arguments can be repurposed to persuade. Recently however several papers have begun to investigate two challenges in argument mining: (1) understanding the structure of an argument and extracting argument components (Lippi and Torroni, 2015; Nguyen and Litman, 2015; Stab and Gurevych, 2014; Lippi and Torroni, 2015; Biran and Rambow, 2011); and (2) understanding what predicts the persuasiveness of web-sourced argumentative content (Habernal and Gurevych, 2016b; Fang et al., 2016; Wachsmuth et al., 2016; Habernal and Gurevych, 2016a; Tan et al., 2016). Tan et al. (2016) study belief change in the Reddit /r/ChangeMyView subreddit (CMV), in which an original poster (OP) challenges others to change his/her opinion. They build logistic regression models to predict argument success, identifying two conversational dynamic factors: a) early potential persuaders are more successful and b"
E17-1070,walker-etal-2012-corpus,1,0.939361,"articipants were pre-qualified using a reading comprehension task that checked their re744 sponses against a gold standard to ensure that they read the arguments carefully. Because we make many comparisons, and our experiments are conducted at large scale, all of our results incorporate Bonferroni corrections. 3.1 Factual: Abortion Q3: Dialog Selection: Identifying Socio-Emotional Arguments R3: Our work requires a new experimental corpus that is sensitive to readers’ prior beliefs and personalities. We utilize online dialogs from 4forums.com downloaded from The Internet Argument Corpus (IAC) (Walker et al., 2012c). The IAC contains quote/response pairs of targeted arguments between two people (Table 1) on topics such as: death penalty, gay marriage, climate change, abortion, evolution and gun control. Each argument is annotated to distinguish arguments making strong appeals to emotional factors versus straightforwardly factual arguments. We selected a subset of extreme exemplars of factual (FACT) versus emotional (EMOT) arguments, defined as Q/R pairs reliably annotated to be at the extreme ends of the fact/emotion scale, i.e. responses with an average ≥ 4 annotation were considered factual, and thos"
E17-1070,W10-4215,0,0.0266434,"nal arguments, and agreeable people are more persuaded by dialogic factual arguments. We describe how we use plan these findings to select and repurpose social media arguments to adapt them to people’s individual differences and thus maximize their educational impact. 1 743 nlds.soe.ucsc.edu/persuasion persona. 2 Related Work not explore factors of the audience or explicitly vary the style of the argument. Previous work also tests the hypothesis that dialogic exchanges might be more engaging, in the context of expository or car sales dialog (Andr´e et al., 2000; Lee, 2010; Craig et al., 2006; Stoyanchev and Piwek, 2010). Work comparing monologic vs. dialogic modes of providing information suggest that dialogs: (1) are more memorable and engaging, (2) stimulate the audience to formulate their own questions, and (3) allow audiences to be more successful at following communication (Lee et al., 1998; Fox Tree, 1999; Suzuki and Yamada, 2004; Driscoll et al., 2003; Fox Tree and Mayer, 2008; Fox Tree, 1999; Liu and Fox Tree, 2011). Other work (Vydiswaran et al., 2012) explores how user-interface factors (e.g., number and order of argument presentation, whether and how arguments are rated) affect how readers process"
E17-1070,swanson-etal-2014-getting,1,0.904313,"Missing"
H01-1015,J97-1002,0,0.0277592,"ndicating the system’s confusion about the destination. Note that the question within this pattern could easily be reformulated as a more typical instruction statement, such as Please specify which Springfield you mean, or Please say Missouri, Illinois or Ohio.. 3. THE SPEECH-ACT DIMENSION The SPEECH - ACT dimension characterizes the utterance’s communicative goal, and is motivated by the need to distinguish the communicative goal of an utterance from its form. As an example, consider the functional category of a REQUEST for information, found in many tagging schemes that annotate speech-acts [24, 18, 6]. Keeping the functional category of a REQUEST separate from the sentence modality distinction between question and statement makes it possible to capture the functional similarity between question and statement forms of requests, e.g., Can you tell me what time you would like to arrive? versus Please tell me what time you would like to arrive. In DATE, the speech-act dimension has ten categories. We use familiar speech-act labels, such as OFFER, REQUEST- INFO, PRESENTINFO , ACKNOWLEDGMENT , and introduce new ones designed to help us capture generalizations about communicative behavior in this"
H01-1015,P98-1052,0,0.0783931,"Missing"
H01-1015,J80-3003,0,0.346788,"cts requesting, or implicitly or explicitly confirming the date. A similar example is provided by the subtasks of CAR (rental) and HOTEL, which include dialogue acts requesting, confirming or acknowledging arrangements to rent a car or book a hotel room on the same trip. 1 This dimension is used as an elaboration of each speech-act type in other tagging schemes [24]. 2 It is tempting to also consider this dimension as a means of inferring discourse structure on the basis of utterance level labels, since it is widely believed that models of task structure drive the behavior of dialogue systems [23, 3, 22], and the relationship between discourse structure and task structure has been a core topic of research since Grosz’s thesis [15]. However, we leave the inference of discourse structure as a topic for future work because the multifunctionality of many utterances suggests that the correspondence between task structure and dialogue structure may not be as straightforward as has been proposed in Grosz’s work [30]. Task TOP - LEVEL TRIP ORIGIN DESTINATION DATE TIME AIRLINE TRIP - TYPE RETRIEVAL ITINERARY GROUND HOTEL CAR Example What are your travel plans? And, what city are you leaving from? And,"
H01-1015,C92-1054,1,0.826032,"are APOLO GIES that the system makes for misunderstandings (see Section 3 below), i.e. utterances such as I’m sorry. I’m having trouble understanding you., or My mistake again. I didn’t catch that. or I can see you are having some problems. The last category of ABOUT- COMMUNICATION utterances are the OPENINGS / CLOSINGS by which the system greets or says goodbye to the caller. (Again, see Section 3 below.) 2.2 About-Communication 2.3 About Situation-Frame The ABOUT- COMMUNICATION domain reflects the system goal of managing the verbal channel and providing evidence of what has been understood [29, 8, 25]. Although utterances of this type occur in human-human dialogue, they are more frequent in humancomputer dialogue, where they are motivated by the need to avoid potentially costly errors arising from imperfect speech recognition. In the COMMUNICATOR corpus, many systems use a conservative strategy of providing feedback indicating the system’s understanding of the information provided by the user after each user turn. A typical example is the repetition of the origin and destination cities in Figures 1 and 6. This type of repetition is the IMPLICIT- CONFIRMATION speech-act (see Section 3 below"
H01-1015,J96-2005,1,0.782684,"s a means of inferring discourse structure on the basis of utterance level labels, since it is widely believed that models of task structure drive the behavior of dialogue systems [23, 3, 22], and the relationship between discourse structure and task structure has been a core topic of research since Grosz’s thesis [15]. However, we leave the inference of discourse structure as a topic for future work because the multifunctionality of many utterances suggests that the correspondence between task structure and dialogue structure may not be as straightforward as has been proposed in Grosz’s work [30]. Task TOP - LEVEL TRIP ORIGIN DESTINATION DATE TIME AIRLINE TRIP - TYPE RETRIEVAL ITINERARY GROUND HOTEL CAR Example What are your travel plans? And, what city are you leaving from? And, where are you flying to? What day would you like to leave? Departing at what time?. Did you have an airline preference? Will you return to Boston from San Jose? Accessing the database; this might take a few seconds. The airfare for this trip is 390 dollars. Did you need to make any ground arrangements?. Would you like a hotel near downtown or near the airport?. Do you need a car in San Jose? Figure 5: Example"
H01-1015,P95-1016,0,\N,Missing
H01-1055,P00-1059,1,0.873365,"Missing"
H01-1055,C00-1007,1,0.920937,"dels of what should be learned. In this respect, the work reported in this paper differs from other recent work on generation in the context of dialog systems [12, 16], which does not modularize the generation process and proposes a single stochastic model for the entire process. We start out by reviewing the generation architecture (Section 2). In Section 3, we discuss the issue of text planning for Communicator. In Section 4, we summarize some initial work in using machine learning for sentence planning [19]. Finally, in Section 5 we summarize work using stochastic tree models in generation [2]. 2. TEXT GENERATION ARCHITECTURE . NLG is conceptualized as a process leading from a high-level communicative goal to a sequence of communicative acts which accomplish this communicative goal. A communicative goal is a goal to affect the user’s cognitive state, e.g., his or her beliefs about the world, desires with respect to the world, or intentions about his or her actions in the world. Following (at least) [13], it has been customary to divide the generation process into three phases, the first two of which are planning phases. Reiter [17] calls this architecture a “consensus architecture”"
H01-1055,W00-1401,1,0.834193,"he intended dialogs is not as relevant: we can try and mimic the human-human transcripts as closely as possible. To show this, we have performed some initial experiments using FERGUS (Flexible Empiricist-Rationalist Generation Using Syntax), a stochastic surface realizer which incorporates a tree model and a linear language model [2]. We have developed a metric which can be computed automatically from the syntactic dependency structure of the sentence and the linear order chosen by the realizer, and we have shown that this metric correlates with human judgments of the felicity of the sentence [3]. Using this metric, we have shown that the use of both the tree model and the linear language model improves the quality of the output of FERGUS over the use of only one or the other of these resources. FERGUS was originally trained on the Penn Tree Bank corpus consisting of Wall Street Journal text (WSJ). The results on an initial set of Communicator sentences were not encouraging, presumably because there are few questions in the WSJ corpus, and furthermore, specific constructions (including what as determiner) appear to be completely absent (perhaps due to a newspaper style file). In an in"
H01-1055,W00-1407,0,0.0206978,"system response to users has been studied by the Natural Language Generation (NLG) community, though rarely in the context of dialog systems. What have emerged from this work are a “consensus architecture” [17] which modularizes the large number of tasks performed during NLG in a particular way, and a range of linguistic representations which can be used in accomplishing these tasks. Many systems have been built using NLG technology, including report generators [8, 7], system description generators [10], and systems that attempt to convince the user of a particular view through argumentation [20, 4]. In this paper, we claim that the work in NLG is relevant to dialog systems as well. We show how the results can be incorporated, and report on some initial work in adapting NLG approaches to dialog systems and their special needs. The dialog system we use is the AT&T Communicator travel planning system.We use machine learning and stochastic approaches where hand-crafting appears to be too complex an option, but we also use insight gained during previous work on NLG in order to develop models of what should be learned. In this respect, the work reported in this paper differs from other recent"
H01-1055,A97-1037,1,0.824416,"ity in terms of informativeness and of adaptation to the context. For a long time, the issue of system response to users has been studied by the Natural Language Generation (NLG) community, though rarely in the context of dialog systems. What have emerged from this work are a “consensus architecture” [17] which modularizes the large number of tasks performed during NLG in a particular way, and a range of linguistic representations which can be used in accomplishing these tasks. Many systems have been built using NLG technology, including report generators [8, 7], system description generators [10], and systems that attempt to convince the user of a particular view through argumentation [20, 4]. In this paper, we claim that the work in NLG is relevant to dialog systems as well. We show how the results can be incorporated, and report on some initial work in adapting NLG approaches to dialog systems and their special needs. The dialog system we use is the AT&T Communicator travel planning system.We use machine learning and stochastic approaches where hand-crafting appears to be too complex an option, but we also use insight gained during previous work on NLG in order to develop models of"
H01-1055,W00-0306,0,0.113299,"evant to dialog systems as well. We show how the results can be incorporated, and report on some initial work in adapting NLG approaches to dialog systems and their special needs. The dialog system we use is the AT&T Communicator travel planning system.We use machine learning and stochastic approaches where hand-crafting appears to be too complex an option, but we also use insight gained during previous work on NLG in order to develop models of what should be learned. In this respect, the work reported in this paper differs from other recent work on generation in the context of dialog systems [12, 16], which does not modularize the generation process and proposes a single stochastic model for the entire process. We start out by reviewing the generation architecture (Section 2). In Section 3, we discuss the issue of text planning for Communicator. In Section 4, we summarize some initial work in using machine learning for sentence planning [19]. Finally, in Section 5 we summarize work using stochastic tree models in generation [2]. 2. TEXT GENERATION ARCHITECTURE . NLG is conceptualized as a process leading from a high-level communicative goal to a sequence of communicative acts which accomp"
H01-1055,A92-1006,1,0.711248,"In Section 4, we summarize some initial work in using machine learning for sentence planning [19]. Finally, in Section 5 we summarize work using stochastic tree models in generation [2]. 2. TEXT GENERATION ARCHITECTURE . NLG is conceptualized as a process leading from a high-level communicative goal to a sequence of communicative acts which accomplish this communicative goal. A communicative goal is a goal to affect the user’s cognitive state, e.g., his or her beliefs about the world, desires with respect to the world, or intentions about his or her actions in the world. Following (at least) [13], it has been customary to divide the generation process into three phases, the first two of which are planning phases. Reiter [17] calls this architecture a “consensus architecture” in NLG. During text planning, a high-level communicative goal is broken down into a structured representation of atomic communicative goals, i.e., goals that can be attained with a single communicative act (in language, by uttering a single clause). The atomic communicative goals may be linked by rhetorical relations which show how attaining the atomic goals contributes to attaining the high-level goal. The work r"
H01-1055,A00-2026,0,0.0354348,"evant to dialog systems as well. We show how the results can be incorporated, and report on some initial work in adapting NLG approaches to dialog systems and their special needs. The dialog system we use is the AT&T Communicator travel planning system.We use machine learning and stochastic approaches where hand-crafting appears to be too complex an option, but we also use insight gained during previous work on NLG in order to develop models of what should be learned. In this respect, the work reported in this paper differs from other recent work on generation in the context of dialog systems [12, 16], which does not modularize the generation process and proposes a single stochastic model for the entire process. We start out by reviewing the generation architecture (Section 2). In Section 3, we discuss the issue of text planning for Communicator. In Section 4, we summarize some initial work in using machine learning for sentence planning [19]. Finally, in Section 5 we summarize work using stochastic tree models in generation [2]. 2. TEXT GENERATION ARCHITECTURE . NLG is conceptualized as a process leading from a high-level communicative goal to a sequence of communicative acts which accomp"
H01-1055,W94-0319,0,0.122438,"eraction, the user can supply more and different information at any time in the dialog. The dialog system must then support a mixed-initiative dialog strategy. While this strategy places greater requirements on ASR, it also increases the range of system responses and the requirements on their quality in terms of informativeness and of adaptation to the context. For a long time, the issue of system response to users has been studied by the Natural Language Generation (NLG) community, though rarely in the context of dialog systems. What have emerged from this work are a “consensus architecture” [17] which modularizes the large number of tasks performed during NLG in a particular way, and a range of linguistic representations which can be used in accomplishing these tasks. Many systems have been built using NLG technology, including report generators [8, 7], system description generators [10], and systems that attempt to convince the user of a particular view through argumentation [20, 4]. In this paper, we claim that the work in NLG is relevant to dialog systems as well. We show how the results can be incorporated, and report on some initial work in adapting NLG approaches to dialog syst"
H01-1055,W98-1415,0,0.0323366,"Missing"
H01-1055,N01-1003,1,\N,Missing
H01-1055,P01-1056,1,\N,Missing
H01-1055,A97-1039,0,\N,Missing
hastie-etal-2002-automatic,H92-1005,0,\N,Missing
hastie-etal-2002-automatic,H92-1009,0,\N,Missing
hastie-etal-2002-automatic,W02-0221,1,\N,Missing
hastie-etal-2002-automatic,P01-1066,1,\N,Missing
hastie-etal-2002-automatic,bonneau-maynard-etal-2000-predictive,0,\N,Missing
J11-3002,C00-1007,0,0.0119532,"existing paradigms for statistical language generation. 1.1 Previous Statistical Language Generation Methods Previous work on SNLG has focused on three main approaches: (a) learning statistical language models (SLMs) from corpora in order to rerank a set of pre-generated 457 Computational Linguistics Volume 37, Number 3 utterances; (b) learning utterance reranking models from user feedback rather than corpora; and (c) learning generation parameters directly from data. The ﬁrst approach has used SLMs to rerank a large set of candidate utterances, and focused on grammaticality and naturalness (Bangalore and Rambow 2000; LangkildeGeary 2002; Chambers and Allen 2004; Nakatsu and White 2006). The seminal work of Langkilde and Knight (1998) in this area showed that high quality paraphrases can be generated from an underspeciﬁed representation of meaning, by ﬁrst applying a very underconstrained, rule-based overgeneration phase, whose outputs are then ranked by an SLM scoring phase. The SLM scoring gives a low score (rank) to any ungrammatical output produced by the rule-based generator. We will refer to this as the overgenerate and scoring (OS) approach. In a novel twist, Isard, Brockmann, and Oberlander (2006)"
J11-3002,W02-1022,0,0.0123521,"Missing"
J11-3002,W04-2302,0,0.058638,"eration. 1.1 Previous Statistical Language Generation Methods Previous work on SNLG has focused on three main approaches: (a) learning statistical language models (SLMs) from corpora in order to rerank a set of pre-generated 457 Computational Linguistics Volume 37, Number 3 utterances; (b) learning utterance reranking models from user feedback rather than corpora; and (c) learning generation parameters directly from data. The ﬁrst approach has used SLMs to rerank a large set of candidate utterances, and focused on grammaticality and naturalness (Bangalore and Rambow 2000; LangkildeGeary 2002; Chambers and Allen 2004; Nakatsu and White 2006). The seminal work of Langkilde and Knight (1998) in this area showed that high quality paraphrases can be generated from an underspeciﬁed representation of meaning, by ﬁrst applying a very underconstrained, rule-based overgeneration phase, whose outputs are then ranked by an SLM scoring phase. The SLM scoring gives a low score (rank) to any ungrammatical output produced by the rule-based generator. We will refer to this as the overgenerate and scoring (OS) approach. In a novel twist, Isard, Brockmann, and Oberlander (2006) applied this method to the generation of dial"
J11-3002,W04-3205,0,0.028583,"Missing"
J11-3002,W06-1405,0,0.0614464,"Missing"
J11-3002,P98-1116,0,0.785178,"work on SNLG has focused on three main approaches: (a) learning statistical language models (SLMs) from corpora in order to rerank a set of pre-generated 457 Computational Linguistics Volume 37, Number 3 utterances; (b) learning utterance reranking models from user feedback rather than corpora; and (c) learning generation parameters directly from data. The ﬁrst approach has used SLMs to rerank a large set of candidate utterances, and focused on grammaticality and naturalness (Bangalore and Rambow 2000; LangkildeGeary 2002; Chambers and Allen 2004; Nakatsu and White 2006). The seminal work of Langkilde and Knight (1998) in this area showed that high quality paraphrases can be generated from an underspeciﬁed representation of meaning, by ﬁrst applying a very underconstrained, rule-based overgeneration phase, whose outputs are then ranked by an SLM scoring phase. The SLM scoring gives a low score (rank) to any ungrammatical output produced by the rule-based generator. We will refer to this as the overgenerate and scoring (OS) approach. In a novel twist, Isard, Brockmann, and Oberlander (2006) applied this method to the generation of dialogues in which conversational agents with different personalities discuss"
J11-3002,W02-2103,0,0.0289086,"Missing"
J11-3002,A97-1039,0,0.298786,"syntactic templates for expressing individual propositions, and aggregates them to produce the utterance’s 460 Mairesse and Walker Trainable Generation of Personality Traits Figure 1 The architecture of the P ERSONAGE base generator. full syntactic structure. The pragmatic marker insertion component then modiﬁes the syntactic structure locally to produce various pragmatic effects, depending on the markers’ insertion constraints. The lexical choice component selects the most appropriate lexeme for each content word, given the lexical selection parameters. Finally, the RealPro surface realizer (Lavoie and Rambow 1997) converts the ﬁnal syntactic structure into a string by applying surface grammatical rules, such as morphological inﬂection and function word insertion. When integrated into a dialogue system, the output of the realizer is annotated for prosodic information by the prosody assigner before being sent to the text-to-speech engine to be converted into an acoustic signal. P ERSONAGE does not currently express personality through prosody, although there are studies that could be used to develop such parameters (Scherer 1979; Furnham 1990). Figure 1 also indicates the modules in which P ERSONAGE intr"
J11-3002,P04-1045,0,0.00829188,"tic variation into account, developing algorithms to modify the system’s linguistic style based on either the user’s linguistic style, or other factors such as the user’s emotional state, her personality, or considerations of politeness strategies (Walker, Cahn, and Whittaker 1997; Lester, Towns, and Fitzgerald 1999; Lester, Stone, and Stelling 1999; Andr´e et al. 2000; Cassell and Bickmore 2003; Piwek 2003). There is growing evidence that dialogue systems such as intelligent tutoring systems are more effective if they can generate a range of different types of stylistic linguistic variation (Litman and Forbes-Riley 2004, 2006; Porayska-Pomsta and Mellish 2004; Wang et al. 2005; McQuiggan, Mott, and Lester 2008; Tapus and Mataric 2008). Most of this work uses either templates or handcrafted rules to generate utterances. This guarantees high quality, natural outputs, which is useful for demonstrating the utility of stylistic variation. Handcrafted approaches mean that utterances have to be constructed by hand for each new application, however, leading to problems of portability and scalability (Rambow, Rogati, and Walker 2001). Statistical natural language generation (SNLG) has the potential to address such sc"
J11-3002,P10-1157,1,0.541835,"Missing"
J11-3002,P07-1063,1,0.762827,"work, we argue that the Big Five model of personality provides a useful framework for modeling some types of stylistic linguistic variation. This model of human personality has become widely accepted in psychology over the last 50 years (Funder 1997). Table 1 tabulates each Big Five trait along with some of the important trait adjectives associated with the extremes of each trait. We believe that these trait adjectives provide an intuitive, meaningful deﬁnition of linguistic style. In previous work we describe a rule-based version of P ERSONAGE, which here we will refer to as P ERSONAGE -RB (Mairesse and Walker 2007; Mairesse 2008). In P ERSONAGE -RB, generation parameters are implemented, and their values are set based on correlations between linguistic cues and the Big Five traits that have been systematically documented in the psychology literature (Scherer 1979; Furnham 1990; Pennebaker and King 1999; Mehl, Gosling, and Pennebaker 2006). For example, parameters for the extraversion 456 Mairesse and Walker Trainable Generation of Personality Traits Table 1 Example adjectives associated with the extremes of all Big Five traits. High Low Extraversion warm, gregarious, assertive, sociable, excitement see"
J11-3002,P06-1140,0,0.3415,"tistical Language Generation Methods Previous work on SNLG has focused on three main approaches: (a) learning statistical language models (SLMs) from corpora in order to rerank a set of pre-generated 457 Computational Linguistics Volume 37, Number 3 utterances; (b) learning utterance reranking models from user feedback rather than corpora; and (c) learning generation parameters directly from data. The ﬁrst approach has used SLMs to rerank a large set of candidate utterances, and focused on grammaticality and naturalness (Bangalore and Rambow 2000; LangkildeGeary 2002; Chambers and Allen 2004; Nakatsu and White 2006). The seminal work of Langkilde and Knight (1998) in this area showed that high quality paraphrases can be generated from an underspeciﬁed representation of meaning, by ﬁrst applying a very underconstrained, rule-based overgeneration phase, whose outputs are then ranked by an SLM scoring phase. The SLM scoring gives a low score (rank) to any ungrammatical output produced by the rule-based generator. We will refer to this as the overgenerate and scoring (OS) approach. In a novel twist, Isard, Brockmann, and Oberlander (2006) applied this method to the generation of dialogues in which conversati"
J11-3002,P05-1008,0,0.551795,"am SLMs can only model local linguistic phenomena, Belz showed that a context-free grammar (PCFG) can successfully model individual differences in the production of weather reports (Belz 2005, 2008). This method provides a principled way to produce utterances matching the linguistic style of a speciﬁc corpus (e.g., of an individual author) without any overgeneration phase. However, standard PCFG generation methods require a treebank-annotated corpus, and they cannot model context-dependent generation decisions, such as the control of sentence length or the generation of referring expressions. Paiva and Evans (2005) adopt a more general framework by learning a regression model mapping generation decisions to stylistic dimensions extracted from a corpus, independently of the language generation mechanism. Factors are identiﬁed by applying factor analysis to a corpus exhibiting stylistic variation, and expressed as a linear combination of linguistic features (Biber 1988). Textual outputs are generated with a rule-based generator in the target domain that is allowed to randomly vary the generation parameters, while logging the parameter settings corresponding to each output. Then the same factors found in t"
J11-3002,E03-1019,0,0.120813,"1996; Bouayad-Agha, Scott, and Power 2000; Power, Scott, and Bouayad-Agha 2003; Inkpen and Hirst 2004). Recent research in language generation for dialogue applications has also begun to take linguistic variation into account, developing algorithms to modify the system’s linguistic style based on either the user’s linguistic style, or other factors such as the user’s emotional state, her personality, or considerations of politeness strategies (Walker, Cahn, and Whittaker 1997; Lester, Towns, and Fitzgerald 1999; Lester, Stone, and Stelling 1999; Andr´e et al. 2000; Cassell and Bickmore 2003; Piwek 2003). There is growing evidence that dialogue systems such as intelligent tutoring systems are more effective if they can generate a range of different types of stylistic linguistic variation (Litman and Forbes-Riley 2004, 2006; Porayska-Pomsta and Mellish 2004; Wang et al. 2005; McQuiggan, Mott, and Lester 2008; Tapus and Mataric 2008). Most of this work uses either templates or handcrafted rules to generate utterances. This guarantees high quality, natural outputs, which is useful for demonstrating the utility of stylistic variation. Handcrafted approaches mean that utterances have to be constru"
J11-3002,P01-1056,1,0.857651,"Missing"
J11-3002,P04-1011,1,0.324828,"Missing"
J11-3002,W98-1419,0,0.058235,"Missing"
J11-3002,P98-2219,1,0.188434,"Missing"
J11-3002,2007.mtsummit-ucnlg.4,0,0.138603,"Missing"
J11-3002,C98-1112,0,\N,Missing
J11-3002,E03-1062,0,\N,Missing
J11-3002,C98-2214,1,\N,Missing
J11-3002,P07-1033,0,\N,Missing
J94-2003,P87-1022,0,0.968992,"Missing"
J94-2003,C90-2047,0,0.153633,"Missing"
J94-2003,J86-3001,0,0.139382,"Missing"
J94-2003,P83-1007,0,0.405783,"Missing"
J94-2003,C82-1017,0,0.0304402,"roximation of discourse salience. This in turn is the main determinant of discourse interpretation processes such as the resolution of zeros in Japanese. A crucial question then is what discourse factors must be considered to determine the ordering of the forward centers, Cf, in Japanese discourse. Being a subject has been shown to be an important factor for English; this is reflected in a Cf ordering by grammatical function (Prince 1981b; Brennan, Friedman, and Pollard 1987; Hudson-D&apos;Zmura 1988; Brennan submitted). Aspects of surface order may also affect the interpretation (Di Eugenio 1990; Hajicova and Vrbova 1982). An interpretation algorithm can also use pronominalization as an indicator of what the speaker believes is salient (Grosz, Joshi, and Weinstein unpublished). Furthermore, zeros in Japanese are not realized syntactically so that there must be a way to distinguish zeros from other entities inferred to be part of a discourse situation. Consider: Example 6 Taroo ga 0 aimasita. Taroo SUBJ OBJ2 met Taroo met (0). This sentence is not felicitous unless the addressee has already been given some information about the person that Taroo met, either in the current discourse or in previous discourses. In"
J94-2003,P85-1008,0,0.291544,"sylvania, Linguistics Department, Philadelphia PA 19104. E-mail: cote@linc.cis.upenn.edu. @ 1994 Association for Computational Linguistics Computational Linguistics Volume 20, Number 2 in Japanese so that it can be used in computational systems, and (3) to provide some insights on the effect of syntactic factors in Japanese on discourse interpretation. In the computational literature, there are two foci for research on the interpretation of anaphoric elements such as pronouns. The first viewpoint focuses on an inferential process driven by the underlying semantics and relations in the domain (Hobbs 1985a; Hobbs et al. 1987; Hobbs and Martin 1987). A polar focus is to concentrate on the role of syntactic information such as what was previously the topic or subject (Hobbs 1976b; Kameyama 1985; Yoshimoto 1988). We will argue for an intermediate position with respect to the interpretation of ZEROS, unexpressed arguments of the verb, in Japanese. Our position is that the interpretation of zeros is an inferential process, but that syntactic information provides constraints on this inferential process (Joshi and Kuhn 1979; Joshi and Weinstein 1981). We will argue that syntactic cues and semantic in"
J94-2003,P86-1031,0,0.922687,"Missing"
J94-2003,C92-1051,0,0.017806,"that these syntactic cues do indeed affect the interpretation of ZEROS,but that having previously been the TOPIC and being realized as a ZERO also contributes to the salience of a discourse entity. We propose a discourse rule of ZERO TOPIC ASSIGNMENT,and show that CENTERINGprovides constraints on when a ZEROcan be interpreted as the ZEROTOPIC. 1. Introduction 1.1 Centering in Japanese Discourse Recently there has been an increasing amount of work in computational linguistics involving the interpretation of anaphoric elements in Japanese (Yoshimoto 1988; Kuno 1989; Walker, Iida, and Cote 1990; Nakagawa 1992). These accounts are intended as components of computational systems for machine translation between Japanese and English or for natural language processing in Japanese alone. This paper has three aims: (1) to generalize a computational account of the discourse process called CENTERING (Sidner 1979; Joshi and Weinstein 1981; Grosz, Joshi, and Weinstein 1983; Grosz, Joshi, and Weinstein unpublished), (2) to apply this account to discourse processing * University of Pennsylvania, Computer Science Department, Philadelphia PA 19104. E-mail: lyn@linc.cis.upenn.edu. t CSLI, Stanford University, Stan"
J94-2003,J81-4001,0,0.0845521,"Missing"
J94-2003,P89-1031,1,0.953664,"coherence d e p e n d s on h o w each segm e n t relates to the overall p u r p o s e of the discourse; local coherence d e p e n d s on aspects such as the syntactic structure of the utterances in that segment, the choice of referring expressions, a n d the use of ellipses. CENTERING m o d e l s local coherence a n d is formalized as a s y s t e m of constraints a n d rules. O u r analysis uses an a d a p t a t i o n of the Centering algorithm that w a s d e v e l o p e d b y Brennan, Friedman, a n d Pollard, b a s e d on these constraints a n d rules (Brennan, Friedman, a n d Pollard 1987; Walker 1989). The p u r p o s e of centering as p a r t of a c o m p u t a t i o n a l m o d e l of discourse interpretation is to m o d e l ATTENTIONALSTATE in discourse in order to control inference (Joshi a n d K u h n 1979; Joshi a n d Weinstein 1981). 3 O u r a p p r o a c h to m o d e l i n g attentional state is to explore aspects of the correlation b e t w e e n syntax a n d discourse function. This a s s u m e s that there are l a n g u a g e conventions a b o u t discourse salience a n d that conversants a t t e m p t to m a i n t a i n a sense of shared context. 2 While native speakers understa"
J94-2003,C92-1054,1,0.546769,"ic value for one parameter of the theory, i.e., Cf ranking (as in Section 2). This parameter is language-dependent because different languages offer different means of expressing discourse function. We conjecture that ZTA may apply in any free-word order language with zeros. Future work must examine the interaction between centering and discourse segmentation in both monologue and dialogue (Whittaker and Stenton 1988; Walker and Whittaker 1990; Walker 1993b), and the role of deictics, lexical semantics, one anaphora, and propositional discourse entities in centering (Webber 1978; Sidner 1979; Walker 1992, 1993a; Cote 1995). It is also important to examine the interaction of zeros with overt pronouns and with deictics and the interaction of pronominalization with accenting (Terken 1995). In addition, the semantic theory underlying centering must be further developed (Roberts 1995). Finally, centering transitions are currently defined by an equality relation between discourse entities, but POSET relations and functional dependencies often link entities in discourse (Prince 1978b, 1981a; Ward 1985; Grosz, Joshi, and Weinstein unpublished). The predictions made here should also be tested on a lar"
J94-2003,P90-1010,1,0.744804,"We proposed that the centering component of a theory of discourse interpretation can be constructed in a language-independent fashion, up to the declaration of a language-specific value for one parameter of the theory, i.e., Cf ranking (as in Section 2). This parameter is language-dependent because different languages offer different means of expressing discourse function. We conjecture that ZTA may apply in any free-word order language with zeros. Future work must examine the interaction between centering and discourse segmentation in both monologue and dialogue (Whittaker and Stenton 1988; Walker and Whittaker 1990; Walker 1993b), and the role of deictics, lexical semantics, one anaphora, and propositional discourse entities in centering (Webber 1978; Sidner 1979; Walker 1992, 1993a; Cote 1995). It is also important to examine the interaction of zeros with overt pronouns and with deictics and the interaction of pronominalization with accenting (Terken 1995). In addition, the semantic theory underlying centering must be further developed (Roberts 1995). Finally, centering transitions are currently defined by an equality relation between discourse entities, but POSET relations and functional dependencies"
J94-2003,P88-1015,0,0.0219174,"; Hofhnan 1995; Turan 1995). We proposed that the centering component of a theory of discourse interpretation can be constructed in a language-independent fashion, up to the declaration of a language-specific value for one parameter of the theory, i.e., Cf ranking (as in Section 2). This parameter is language-dependent because different languages offer different means of expressing discourse function. We conjecture that ZTA may apply in any free-word order language with zeros. Future work must examine the interaction between centering and discourse segmentation in both monologue and dialogue (Whittaker and Stenton 1988; Walker and Whittaker 1990; Walker 1993b), and the role of deictics, lexical semantics, one anaphora, and propositional discourse entities in centering (Webber 1978; Sidner 1979; Walker 1992, 1993a; Cote 1995). It is also important to examine the interaction of zeros with overt pronouns and with deictics and the interaction of pronominalization with accenting (Terken 1995). In addition, the semantic theory underlying centering must be further developed (Roberts 1995). Finally, centering transitions are currently defined by an equality relation between discourse entities, but POSET relations a"
J94-2003,C88-2159,0,0.452644,"irs, varied by one of the above factors. We demonstrate that these syntactic cues do indeed affect the interpretation of ZEROS,but that having previously been the TOPIC and being realized as a ZERO also contributes to the salience of a discourse entity. We propose a discourse rule of ZERO TOPIC ASSIGNMENT,and show that CENTERINGprovides constraints on when a ZEROcan be interpreted as the ZEROTOPIC. 1. Introduction 1.1 Centering in Japanese Discourse Recently there has been an increasing amount of work in computational linguistics involving the interpretation of anaphoric elements in Japanese (Yoshimoto 1988; Kuno 1989; Walker, Iida, and Cote 1990; Nakagawa 1992). These accounts are intended as components of computational systems for machine translation between Japanese and English or for natural language processing in Japanese alone. This paper has three aims: (1) to generalize a computational account of the discourse process called CENTERING (Sidner 1979; Joshi and Weinstein 1981; Grosz, Joshi, and Weinstein 1983; Grosz, Joshi, and Weinstein unpublished), (2) to apply this account to discourse processing * University of Pennsylvania, Computer Science Department, Philadelphia PA 19104. E-mail: l"
J94-2003,P88-1014,0,\N,Missing
J96-2005,P88-1023,0,0.0140126,"val cues. Next, consider the differences in status of the entities in completed discourse segments. In the stack model, focus spaces for segments that have been closed are popped from the stack and entities in those focus spaces are not accessible. In the cache model, ""popping"" only occurs via displacement. Thus, even when a segment is clearly closed, if a new topic has not been initiated, the popped entities should still be available. Some support for the cache model predictions about popped entities is that (1) rules proposed for deaccenting noun phrases treat popped entities as accessible (Davis and Hirschberg 1988); and (2) rules for referring expressions in argumentative texts treat the conclusions of p o ppe d sisters as salient (Huang 1994). Stronger evidence would be the reaction times to the mention of entities in a closed segment, after it is clear that a new segment has been initiated, but before the topic of that new segment has initiated a retrieval to, and hence displacement from, the cache. It should also be possible to test whether entities that are in the focus spaces on the stack, according to the stack model, are more accessible than entities that have been popped off the stack. In the ca"
J96-2005,C90-2047,0,0.0322147,"Missing"
J96-2005,J86-3001,0,0.949417,"ecifiers for a pronoun. The limited attention constraint has been defined by some researchers by linear recency: a representation of an utterance A is linearly recent for a representation of an utterance B if A is linearly adjacent to B. Using linear recency as a model of the limited attention constraint would mean that an antecedent for an anaphor is determined by a linear backward search of the text, or of a discourse model representation of the text (Clark and Sengul 1979, inter alia). In contrast, other work has formulated the limited attention constraint in terms of hierarchical recency (Grosz and Sidner 1986; Hobbs 1985; Mann and Thompson 1987, inter alia). A representation of an utterance A is hierarchically recent for a representation of an utterance B if A is adjacent to B in the tree structure of the discourse. Of all theories based on hierarchical recency, only Grosz and Sidner's theory of discourse structure provides an operationalization of hierarchical recency in terms of their stack model of attentional state (Sidner 1979; Grosz 1977; Grosz and Sidner 1986). Thus, below, the relationship between limited attention and hierarchical recency will be discussed in terms of their stack model, b"
J96-2005,W94-0317,0,0.00995838,"that have been closed are popped from the stack and entities in those focus spaces are not accessible. In the cache model, ""popping"" only occurs via displacement. Thus, even when a segment is clearly closed, if a new topic has not been initiated, the popped entities should still be available. Some support for the cache model predictions about popped entities is that (1) rules proposed for deaccenting noun phrases treat popped entities as accessible (Davis and Hirschberg 1988); and (2) rules for referring expressions in argumentative texts treat the conclusions of p o ppe d sisters as salient (Huang 1994). Stronger evidence would be the reaction times to the mention of entities in a closed segment, after it is clear that a new segment has been initiated, but before the topic of that new segment has initiated a retrieval to, and hence displacement from, the cache. It should also be possible to test whether entities that are in the focus spaces on the stack, according to the stack model, are more accessible than entities that have been popped off the stack. In the cache model, the entities in these focus spaces would not have a privileged attentional status, unless of course they had been refres"
J96-2005,C92-1054,1,\N,Missing
J97-1001,P96-1009,0,0.0207517,"model of dialogue (Power 1979; Houghton and Isard 1985), the interaction of risk-taking dialogue strategies and corresponding repair strategies (Carletta 1992), the relationship between resource bounds, task complexity, and dialogue strategies (Walker 1996; Jordan and Walker 1996), the role of belief revision in tasks in which agents negotiate a problem solution (Logan, Reece, and Sparck-Jones 1994), and the relationship between mixed initiative and knowledge distribution (Guinn 1994). Empirical research in testbeds for human-computer dialogue interfaces is very recent (Hirschman et al. 1990; Allen et al. 1996). This method supports studies of the interaction of various components; for example Danieli and Gerbino (1995) propose an implicit recovery metric for evaluating how the dialogue manager overcomes limitations in the speech recognizer. Other research parameterizes the dialogue manager to select different behaviors in different contexts, such as expert vs. novice discourse strategies (Kamm 1995), different repair strategies (Hirschman and Pao 1993), or different degrees of initiative (Potjer et al. 1996; Smith and Gordon, this volume). These dialogue interfaces also provide an opportunity for t"
J97-1001,P95-1017,0,0.0972862,"sifiers functions as a causal model, which can then be examined and further tested. For example, this method has been used to identify features for predicting accent assignment in text-tospeech (Hirschberg 1993), for repairing disfluencies (Nakatani and Hirschberg 1993), cue vs. noncue uses of discourse cue words (Litman 1996; Siegel and McKeown 1994), discourse segment boundaries (Grosz and Hirschberg 1992; Passonneau and Litman, this volume), intonational phrase boundaries (Wang and Hirschberg 1992), and the Computational Linguistics Volume 23, Number 1 most likely antecedents for anaphors (Aone and Bennet 1995; Connolly, Burger, and Day 1994). Discourse tagging is also instrumental in stage 3 of our empirical method, by producing a test set that can be used for comparison to a program&apos;s output. This method is used by most of the articles in this volume. A common application is testing coreference resolution algorithms; the tags indicate the preferred interpretation of a potentially ambiguous utterance containing anaphoric noun phrases (Walker 1989; Suri and McCoy 1994; Chinchor and Sundheim 1995). Coreference algorithms are then tested on their ability to select the right equivalence class for an a"
J97-1001,E89-1039,0,0.0548772,"Missing"
J97-1001,P92-1002,0,0.0215406,"st set that can be used for comparison to a program&apos;s output. This method is used by most of the articles in this volume. A common application is testing coreference resolution algorithms; the tags indicate the preferred interpretation of a potentially ambiguous utterance containing anaphoric noun phrases (Walker 1989; Suri and McCoy 1994; Chinchor and Sundheim 1995). Coreference algorithms are then tested on their ability to select the right equivalence class for an anaphoric noun phrase. The same method has been applied to empirically testing an algorithm for resolving verb phrase ellipsis (Hardt 1992). In each case, we can generalize on the basis of specific features from studies of specific algorithms operating on specific corpora, whenever the corpora represent a general task for the algorithm. The more varied the test data is, the more generalizable we expect the results to be. For example, the claim that a model is general can be supported by test corpora representing different genres (Fox 1987; Kroch and Hindle 1982), or different language families (Strube and Hahn 1996; Iida 1997; Di Eugenio 1997; Hoffman 1997). Human performance can also be compared to algorithm output through the u"
J97-1001,H93-1004,0,0.0378651,"Missing"
J97-1001,H90-1023,0,0.191522,"nning as an underlying model of dialogue (Power 1979; Houghton and Isard 1985), the interaction of risk-taking dialogue strategies and corresponding repair strategies (Carletta 1992), the relationship between resource bounds, task complexity, and dialogue strategies (Walker 1996; Jordan and Walker 1996), the role of belief revision in tasks in which agents negotiate a problem solution (Logan, Reece, and Sparck-Jones 1994), and the relationship between mixed initiative and knowledge distribution (Guinn 1994). Empirical research in testbeds for human-computer dialogue interfaces is very recent (Hirschman et al. 1990; Allen et al. 1996). This method supports studies of the interaction of various components; for example Danieli and Gerbino (1995) propose an implicit recovery metric for evaluating how the dialogue manager overcomes limitations in the speech recognizer. Other research parameterizes the dialogue manager to select different behaviors in different contexts, such as expert vs. novice discourse strategies (Kamm 1995), different repair strategies (Hirschman and Pao 1993), or different degrees of initiative (Potjer et al. 1996; Smith and Gordon, this volume). These dialogue interfaces also provide"
J97-1001,J94-4002,0,0.02252,"prove the performance of a speech recognizer with tag-specific language models (Taylor et al. 1996), and whether an induced discourse model based on the tagging can predict the next dialogue act in the dialogue, and thus affect how the system translates the next utterance (Reithinger and Maier 1995; Nagata 1992). Tagging is also critical for ablation studies, where algorithm features are selectively turned off, and performance examined. Tagging can characterize the input in terms of features the algorithm uses for producing the target behavior or characterize the target behavior. For example, Lappin and Leass (1994) report an ablation study of an anaphora resolution algorithm, operating on computer manuals, in which various factors that were hypothesized to determine the most likely antecedent were selectively turned off. (See also Dagan et al. [1995]). Sample results include the finding that there is no effect on performance, for this type of text, when an antecedent&apos;s likelihood is increased for parallelism. Another use of discourse tagging is for algorithm induction using automatic classifiers, such as C4.5 or CART, that produce decision trees from data sets described by a set of features (Brieman et"
J97-1001,H91-1061,0,0.0426602,"Missing"
J97-1001,J92-4007,1,0.796955,"out input features that affect the target behavior are found in previous work (stage 1 of the methodology). In this case, the tagging contributes to developing a causal model. The tagged corpus is used to test whether the features predict the target behavior. For example, researchers have devised algorithms for generating the surface form of explanations and instructions from underlying intention-based representations by tagging naturally occurring discourses for surface form features, informational relations, and intentional relations (Vander Linden and Di Eugenio 1996; Moser and Moore 1995; Moore and Pollack 1992; Paris and Scott 1994). Another promising area is speech act (dialogue move) tagging, where, for example, researchers have tested whether an automatic tagger trained on the tagged corpus can improve the performance of a speech recognizer with tag-specific language models (Taylor et al. 1996), and whether an induced discourse model based on the tagging can predict the next dialogue act in the dialogue, and thus affect how the system translates the next utterance (Reithinger and Maier 1995; Nagata 1992). Tagging is also critical for ablation studies, where algorithm features are selectively tur"
J97-1001,J91-1002,0,0.0107243,"planation) or a response (e.g., reply, acknowledgment). The paper discusses issues with determining the reliability and generality of tagging sets, and with refining tag sets on the basis of reliability data (Carletta 1996; Condon and Cech 1995). The Discourse Working Group is also applying this tagging scheme to other dialogue types to evaluate its generality (Hirschman et al. 1996; Luperfoy 1996). 3.2 Hearst The target behavior that Hearst is concerned with is subtopic identification in expository texts. She tests the hypothesis that term repetition is a primary feature of textual cohesion (Morris and Hirst 1991) by using it as the basis for two different algorithms for identifying multiparagraph subtopical units. The algorithms are evaluated by comparing the units they propose against a baseline of randomly generated topic boundaries, and against a corpus tagged by human judges. Precision, recall, and K (Carletta 1996; Krippendorf 1980) are used as evaluation metrics to assess the performance of the two algorithms. In order to generalize, Hearst tests the algorithm&apos;s performance on a new task: that of distinguishing boundaries between sets of concatenated news articles. Hearst&apos;s algorithm performs co"
J97-1001,P95-1018,1,0.844334,"rizable) dialogue models implemented in human-computer dialogue interfaces. H o w are these methods used and how do they contribute to the development of general theories? Discourse tagging classifies discourse units in naturally occurring texts or dialogues into one of a set of categories. Discourse units range from referring expressions and syntactic constructions (Fox 1987; Kroch and Hindle 1982; Prince 1985), to words or phrases (Heeman and Allen 1994; Hirschberg and Litman 1993; Novick and Sutton 1994), to utterances and relationships among them (Dahlback 1991; Reithinger and Maier 1995; Moser and Moore 1995; Nagata 1992; Rose et al. 1995), to multiutterance units identified by a range of criteria such as speaker intention or initiative (Flammia and Zue 1995a; Hirschberg and Nakatani 1996; Whittaker and Stenton 1988). The article by Carletta et al. (this volume) presents a tagging scheme for three levels of discourse structure. Discourse tags categorize either features of the input (independent variables) or features of the output (dependent variables). Often hypotheses about input features that affect the target behavior are found in previous work (stage 1 of the methodology). In this case, the"
J97-1001,P93-1007,0,0.0256028,"trees from data sets described by a set of features (Brieman et al. 1984; Quinlan 1993). This approach uses automatic methods for stages 2 and 3 of our empirical research strategy. Since discourse tagging associates sets of features with discourse phenomena, tagged data is used as input to these automatic classifiers. The decision tree produced by the classifiers functions as a causal model, which can then be examined and further tested. For example, this method has been used to identify features for predicting accent assignment in text-tospeech (Hirschberg 1993), for repairing disfluencies (Nakatani and Hirschberg 1993), cue vs. noncue uses of discourse cue words (Litman 1996; Siegel and McKeown 1994), discourse segment boundaries (Grosz and Hirschberg 1992; Passonneau and Litman, this volume), intonational phrase boundaries (Wang and Hirschberg 1992), and the Computational Linguistics Volume 23, Number 1 most likely antecedents for anaphors (Aone and Bennet 1995; Connolly, Burger, and Day 1994). Discourse tagging is also instrumental in stage 3 of our empirical method, by producing a test set that can be used for comparison to a program&apos;s output. This method is used by most of the articles in this volume. A"
J97-1001,P94-1014,0,0.0614787,"mpirical Studies in Discourse models using computer-computer dialogue simulation; and (9) Testbeds of (parameterizable) dialogue models implemented in human-computer dialogue interfaces. H o w are these methods used and how do they contribute to the development of general theories? Discourse tagging classifies discourse units in naturally occurring texts or dialogues into one of a set of categories. Discourse units range from referring expressions and syntactic constructions (Fox 1987; Kroch and Hindle 1982; Prince 1985), to words or phrases (Heeman and Allen 1994; Hirschberg and Litman 1993; Novick and Sutton 1994), to utterances and relationships among them (Dahlback 1991; Reithinger and Maier 1995; Moser and Moore 1995; Nagata 1992; Rose et al. 1995), to multiutterance units identified by a range of criteria such as speaker intention or initiative (Flammia and Zue 1995a; Hirschberg and Nakatani 1996; Whittaker and Stenton 1988). The article by Carletta et al. (this volume) presents a tagging scheme for three levels of discourse structure. Discourse tags categorize either features of the input (independent variables) or features of the output (dependent variables). Often hypotheses about input features"
J97-1001,P89-1016,0,0.0280877,"dialogue systems. Because testing dialogue systems requires a fully implemented natural language system, there are two empirical methods for testing hypotheses about discourse models that are independent of the current state of the art in speech or language processing. The first method is Wizard-of-Oz simulation, and the second is computational testbeds for dialogue simulation. In the Wizard-of-Oz (WOZ) approach, a human wizard simulates the behavior of a program interacting with a human to carry out a particular task in a particular domain (Dahlb~ick and Jonsson 1989; Hirschman et al. 1993; Oviatt and Cohen 1989; Whittaker and Stenton 1989). The WOZ method can, in principle, be used to test, refine, or generalize any behavior implementable in a program and thus is appropriate at several stages of our methodology. For example, the wizard may follow a proto4 Walker and Moore Empirical Studies in Discourse col that includes particular system limitations or error-handling strategies, to explore potential problems before implementation, e.g. determining how the program&apos;s level of interactivity affects the complexity of the instructions it is given (Oviatt and Cohen 1989). In addition, WOZ is often used to"
J97-1001,P95-1005,0,0.06507,"Missing"
J97-1001,C96-1059,0,0.0244856,"Missing"
J97-1001,P89-1031,1,0.803546,"tional phrase boundaries (Wang and Hirschberg 1992), and the Computational Linguistics Volume 23, Number 1 most likely antecedents for anaphors (Aone and Bennet 1995; Connolly, Burger, and Day 1994). Discourse tagging is also instrumental in stage 3 of our empirical method, by producing a test set that can be used for comparison to a program&apos;s output. This method is used by most of the articles in this volume. A common application is testing coreference resolution algorithms; the tags indicate the preferred interpretation of a potentially ambiguous utterance containing anaphoric noun phrases (Walker 1989; Suri and McCoy 1994; Chinchor and Sundheim 1995). Coreference algorithms are then tested on their ability to select the right equivalence class for an anaphoric noun phrase. The same method has been applied to empirically testing an algorithm for resolving verb phrase ellipsis (Hardt 1992). In each case, we can generalize on the basis of specific features from studies of specific algorithms operating on specific corpora, whenever the corpora represent a general task for the algorithm. The more varied the test data is, the more generalizable we expect the results to be. For example, the claim"
J97-1001,P90-1010,1,0.782305,"but volunteers relevant facts. Dialogue structure is tagged via a model that segments circuit repair dialogues into five phases: introduction, assessment, diagnosis, repair and test. Then Smith and Gordon examine how the subdialogue length varies depending on initiative mode. Their results exemplify the empirical generalization strategy by showing that a subdialogue model based on WOZ simulations can be generalized to human-computer dialogues. They also show that claims about the effect of initiative on dialogue structure in human-human dialogues in other domains (Whittaker and Stenton 1988; Walker and Whittaker 1990) generalize to human-computer dialogues in the circuit repair domain. Further generalizations could result from determining whether the subdialogue model can be used in other types of human-human or human-computer problemsolving dialogues. 3.6 Yeh and Mellish The target behavior that Yeh and Mellish model is the generation of anaphoric noun phrases in Chinese texts. The algorithm must select from among zero pronouns, overt pronouns, and full noun phrases; in addition, for full noun phrases, appropriate content must be determined. Their training set is a corpus of Chinese texts tagged for anaph"
J97-1001,P96-1036,0,0.0128781,"aphoric noun phrase. The same method has been applied to empirically testing an algorithm for resolving verb phrase ellipsis (Hardt 1992). In each case, we can generalize on the basis of specific features from studies of specific algorithms operating on specific corpora, whenever the corpora represent a general task for the algorithm. The more varied the test data is, the more generalizable we expect the results to be. For example, the claim that a model is general can be supported by test corpora representing different genres (Fox 1987; Kroch and Hindle 1982), or different language families (Strube and Hahn 1996; Iida 1997; Di Eugenio 1997; Hoffman 1997). Human performance can also be compared to algorithm output through the use of reaction time or comprehension or production experiments (Brennan 1995; Gordon, Grosz, and Gilliom 1993; Hudson-D&apos;Zmura 1988). These methods allow researchers fine-grained control of the phenomena studied, and avoid problems with sparse data that can arise with corpus analyses. Reaction time studies also provide researchers with an indirect measure of how humans process a particular phenomenon; processing times can then be compared with the predictions of a model. The meth"
J97-1001,P88-1015,0,0.0985833,"urse units in naturally occurring texts or dialogues into one of a set of categories. Discourse units range from referring expressions and syntactic constructions (Fox 1987; Kroch and Hindle 1982; Prince 1985), to words or phrases (Heeman and Allen 1994; Hirschberg and Litman 1993; Novick and Sutton 1994), to utterances and relationships among them (Dahlback 1991; Reithinger and Maier 1995; Moser and Moore 1995; Nagata 1992; Rose et al. 1995), to multiutterance units identified by a range of criteria such as speaker intention or initiative (Flammia and Zue 1995a; Hirschberg and Nakatani 1996; Whittaker and Stenton 1988). The article by Carletta et al. (this volume) presents a tagging scheme for three levels of discourse structure. Discourse tags categorize either features of the input (independent variables) or features of the output (dependent variables). Often hypotheses about input features that affect the target behavior are found in previous work (stage 1 of the methodology). In this case, the tagging contributes to developing a causal model. The tagged corpus is used to test whether the features predict the target behavior. For example, researchers have devised algorithms for generating the surface for"
J97-1001,E89-1016,0,0.0146249,"use testing dialogue systems requires a fully implemented natural language system, there are two empirical methods for testing hypotheses about discourse models that are independent of the current state of the art in speech or language processing. The first method is Wizard-of-Oz simulation, and the second is computational testbeds for dialogue simulation. In the Wizard-of-Oz (WOZ) approach, a human wizard simulates the behavior of a program interacting with a human to carry out a particular task in a particular domain (Dahlb~ick and Jonsson 1989; Hirschman et al. 1993; Oviatt and Cohen 1989; Whittaker and Stenton 1989). The WOZ method can, in principle, be used to test, refine, or generalize any behavior implementable in a program and thus is appropriate at several stages of our methodology. For example, the wizard may follow a proto4 Walker and Moore Empirical Studies in Discourse col that includes particular system limitations or error-handling strategies, to explore potential problems before implementation, e.g. determining how the program&apos;s level of interactivity affects the complexity of the instructions it is given (Oviatt and Cohen 1989). In addition, WOZ is often used to collect sample dialogues tha"
J97-1001,J93-3003,0,\N,Missing
J97-1001,E95-1034,0,\N,Missing
J97-1001,H91-1062,0,\N,Missing
J97-1001,C90-2047,0,\N,Missing
J97-1001,P95-1016,0,\N,Missing
J97-1001,P94-1041,0,\N,Missing
J97-1001,J94-2006,0,\N,Missing
J97-1001,P96-1038,0,\N,Missing
J97-1001,J93-3001,0,\N,Missing
J97-4008,J80-3003,0,0.215485,"llowing Goffman (1970), and Brown and Levinson (1987), Clark claims that the production of each utterancelevel signaling event is governed by a set of social constraints that derive from the social situation in which the conversation is carried out and the social relationship that holds between the conversants. These theories claim that it is primarily the orientation to social constraints that leads to many indirect forms of communicative acts. The use of planning representations in the interpretation of these indirect speech acts has been the focus of much work in computational linguistics (Perrault and Allen 1980; Litman 1985; McRoy and Hirst 1995), but these theories have had little impact on models of language production used in computational linguistics (with the exception of models reported by Hovy [1990] and Walker, Cahn, and Whittaker [1997]). Thus, Clark provides a view of language use that integrates a number of perspectives, many of which have individually already been influential in computational linguistics. The integrative model that Clark presents has many complexities, but the book is accessible to readers with little or no background. The claims are nicely illustrated with excerpts from"
J97-4008,P88-1015,0,0.0095329,"larger joint activity (goal or plan) at the discourse level (Bruce 1975; Power 1974; Allen and Perrault 1980; Litman 1985). At the discourse level, each joint signaling event consists of individual segments, or sections in Clark's terminology. A transition between two sections s and t depends on a set of relations that can hold between sections, such as t being subsequent to s, t being a part of s, or t being a digression from s (Reichman 1985; Litman 1985; Grosz and Sidner 1986). Each section requires the conversants to coordinate on the entry into the section and the exit from the section (Whittaker and Stenton 1988). One basis for coordination is the marking of transitions between sections in different ways, e.g., by discourse markers (Hirschberg and Litman 1993). Computational linguists who read this book will notice that these core ideas are consistent in many ways with commonly assumed planning models of dialogue in computational linguistics. While Clark does not always make clear the relationship between his proposals and work in computational linguistics, many researchers in computational linguistics have used these ideas within computational frameworks that are more precise and testable. Clark argu"
L16-1163,elson-2012-dramabank,0,0.510539,"H or SIG . The story topics include stories about romance, travel, sports, holidays, watching wildlife, and weather. The stories have also been annotated for overall positive and negative tone. The SIG representation provides a propositional representation of the story timeline, the goals and motivations of the story characters, and the affective impacts of story events on characters. Our approach builds on the corpus and tools associated with the DramaBank language resource, a collection of Aesop’s Fables and other classic stories that utilize the SIG representation (Elson and McKeown, 2010; Elson, 2012a; Elson, 2010). This work is the first to apply this formalism to informal personal narratives, such as the story about The Startled Squirrel shown in Figure 1. This is one of those times I wish I had a digital camera. We keep a large stainless steel bowl of water outside on the back deck for Benjamin to drink out of when he’s playing outside. His bowl has become a very popular site. Throughout the day, many birds drink out of it and bathe in it. The birds literally line up on the railing and wait their turn. Squirrels also come to drink out of it. The craziest squirrel just came by- he was l"
L16-1163,D10-1008,0,0.373446,"Missing"
L16-1163,L16-1550,1,0.843052,"applications related to storytelling, game playing and narrative generation. (Harmon and Jhala, 2015) draw parallels between the narrative representation of SIGs and Skald, a narrative generator. This combination allows for narrative generation while keeping the affordances of the SIG representation. SIGs are also hugely beneficial as a content planner. (Antoun et al., 2015) has used the SIG as an intermediate representation of meaning by transforming a play trace of the PromWeek game into a representation which can be used to generate natural language recaps of the game. Our companion paper (Hu et al., 2016) released the Story Dialogue with Gestures (SDG) corpus which contains 50 personal narratives rendered as dialogues between two agents with complete gesture and placement annotations. Their first approach for generating dialogues manually split personal narratives. Their second approach uses SIGs of personal narratives as a way to get story content from the WYSIWYM realization and automatically produce dialogues from this telling, using the EST. The Expressive-Story Translator (EST) explores the use of personal narratives in storytelling by utilizing the rich representation of SIGs. Storytelle"
L16-1163,W15-4627,1,0.777032,"language generation engine (Mairesse and Walker, 2007). This syntactic representation of a story enables the retelling of any story that is represented as a SIG. Table 4 shows an excerpt from Story 57, the corresponding WYSIWMY realization, and the generated output from the EST and the syntactic structure that allows the EST to generate variations. PersonaBank allows for the exploration of storytelling by retelling personal narratives by implementing a variety of narrative and sentence parameters including contingency discourse relations into the generation of stories (Lukin and Walker, 2015; Lukin et al., 2015). Our current work examines many possible combinations of parameters and generates many versions of a sentence according to different framing goals. For example, Table 5 shows how the EST generates surface strings from the syntactic template from Table 4 and how we can create many variations by utilizing generation parameters. Variations 1 thru 6 are in the first person perspective, and 7 thru 12 are in the third person voice and reference the narrator as Anne. All the sentences vary the sentence construction in some way. Sentences 1, 2, Original excerpt: I went in last wednesday to take what"
L16-1163,P07-1063,1,0.593182,"the same incident (Mateas, 2004). For example, storytellers tell richer stories to highly interactive and responsive addressees (Thorne, 1987), and stories told by young adults “play” to the audience, repeatedly telling a story to get a desired effect and communicate effectively with the audience (Thorne and McLean, 2003). (Rishes et al., 2013) use SIGs of Aesop’s Fables from the DramaBank for retelling these stories in different ways by creating a mapping between the content representation of the SIG and the syntactic representation used by the PER SONAGE natural language generation engine (Mairesse and Walker, 2007). This syntactic representation of a story enables the retelling of any story that is represented as a SIG. Table 4 shows an excerpt from Story 57, the corresponding WYSIWMY realization, and the generated output from the EST and the syntactic structure that allows the EST to generate variations. PersonaBank allows for the exploration of storytelling by retelling personal narratives by implementing a variety of narrative and sentence parameters including contingency discourse relations into the generation of stories (Lukin and Walker, 2015; Lukin et al., 2015). Our current work examines many po"
L16-1504,L16-1550,1,0.882416,"Missing"
L16-1504,N06-2031,0,0.296049,"w), there are many potential analyses that may be undertaken in the future. For example, researchers may test efficiency based on whether descriptions were holistic or piecemeal, abstract or concrete, co-occuring with directional information or not. Researchers may also explore whether repetition of words across conversational participants enhanced or hindered efficiency (Brennan & Clark 1996). There are also a number of automatic analyses of entrainment or alignment phenomena in terms of structural priming or referring expression content selection that could be carried out using this corpus (Reitter et al, 2006, Reitter & Moore, 2014, Gupta & Stent, 2005, Jordan & Walker, 2005, Guhe & Bard 2008). Example of Attempting to Use Expertise on Santa Cruz and Common Ground (Dyad 16) F: There’s a roller coaster here, but it's got like, yeah it looks like a prison cell because of the way the shot’s like drawn... but it's not that it's not as small as you described it D: How tall is it? or how big is it? F: U:h across, I’m gonna say like 5 feet, 6 feet D: Maybe it’s- okay maybe it's bigger than what I’m describing it to be, but yeah it kinda looks like train tracks, and it kinda looks like a ladder like tilte"
L16-1504,L16-1553,1,0.867573,"Missing"
L16-1550,P09-4003,0,0.0330198,"story ranges from 174 words to 410 words. A sample story about pets is shown in Figure 1 and another story about being present at a protest is shown in Figure 5. Topic Camping Holiday Gardening Party Pet Sports Travel Weather Conditions Other Number of Stories 3 5 7 10 3 4 7 3 13 Table 1: Distribution of story topics in the SDG corpus. Although we are not making using of the STORY INTENTION GRAPHS ( SIGs) yet to automatically produce dialogues from their monologic representations, we have annotated each of the 50 stories with their SIG using the freely available annotation tool Scheherazade (Elson and McKeown, 2009). More description of the Scheherazade annotation and the resulting SIG representation is provided in our companion paper (Lukin et al., 2016). Our approach builds on the DramaBank language resource, a collection classic stories that also utilize the SIG representation (Elson, 2012a; Elson and McKeown, 2010; Elson, 2012b), but the SIG formalism has not previously been used for the purpose of automatically generating animated dialogues, and this is one of the first uses of the SIG on personal narratives (Lukin and Walker, 2015; Lukin et al., 2015). DramaBank provides a symbolic annotation tool"
L16-1550,elson-2012-dramabank,0,0.492306,"she runs (top speed) to get them and bring them back. Again, she will do this until she is out of breath. If only I could work out that hard... I’d probably be thinner. Figure 1: An example story on the topic of Pet. man annotators, gesture annotations on human generated dialogues, videos of story dialogues generated from this representation that vary the introversion and extraversion of the animated agents, video clips of each gesture used in the gesture annotations, and annotations of the original personal narratives with a deep representation of story called a STORY INTENTION GRAPH or SIG (Elson, 2012a; Elson 3447 A1: B1: A2: B2: A3: B3: A4: B4: A5: B5: A6: B6: A7: B7: Pet Story Dialogue I have always felt like I was a dog person but our two cats are great. They are much more low maintenance than dogs are. Yeah, I’m really glad we got our first one at a no-kill shelter. I had wanted a little kitty, but the only baby kitten they had scratched the crap out of me the minute I picked it up so that was a big “NO”. Well, the no-kill shelter also had what they called “teenagers”, which were cats around four to six months old... A bit bigger than the little kitties. Oh yeah, I saw those “teenagers"
L16-1550,W15-4627,1,0.917796,"es, learn social behaviors, and entertain (Ryokai et al., 2003; Pennebaker and Seagal, 1999; Gratch et al., 2013). Previous research has also shown that conveying information in the form of a dialogue is more engaging, effective and persuasive compared to a monologue (Lee et al., 1998; Craig et al., 2000; Suzuki and Yamada, 2004; Andr´e et al., 2000). Thus our long term goal is the automatic generation of story co-tellings as animated dialogues. Given a deep representation of the story, many different versions of the story can be generated, both dialogic and monologic (Lukin and Walker, 2015; Lukin et al., 2015). This paper presents a new corpus, the S TORY D IALOGUE WITH G ESTURES (SDG) corpus, consisting of 50 personal narratives regenerated as dialogues by human annotators, complete with annotations of gesture placement and accompanying gesture forms. These annotations can be supplemented programmatically to produce changes in the nonverbal dialogic behavior in gesture rate, expanse and speed. We have thus used these to generate tellings that vary the personality of the teller (introverted vs. extroverted) (Hu et al., 2015). An example original monologic personal narrative about having cats as pet"
L16-1550,L16-1163,1,0.84288,"wn in Figure 5. Topic Camping Holiday Gardening Party Pet Sports Travel Weather Conditions Other Number of Stories 3 5 7 10 3 4 7 3 13 Table 1: Distribution of story topics in the SDG corpus. Although we are not making using of the STORY INTENTION GRAPHS ( SIGs) yet to automatically produce dialogues from their monologic representations, we have annotated each of the 50 stories with their SIG using the freely available annotation tool Scheherazade (Elson and McKeown, 2009). More description of the Scheherazade annotation and the resulting SIG representation is provided in our companion paper (Lukin et al., 2016). Our approach builds on the DramaBank language resource, a collection classic stories that also utilize the SIG representation (Elson, 2012a; Elson and McKeown, 2010; Elson, 2012b), but the SIG formalism has not previously been used for the purpose of automatically generating animated dialogues, and this is one of the first uses of the SIG on personal narratives (Lukin and Walker, 2015; Lukin et al., 2015). DramaBank provides a symbolic annotation tool for stories called Scheherazade that automatically produces the SIG as a result of the annotation. Every annotation involves: (1) identifying"
L16-1550,L16-1552,1,0.870158,"searchers interested in natural language generation, intelligent virtual agents, generation of nonverbal behavior, and story and narrative representations. Keywords: storytelling, personal narrative, dialogue, virtual agents, gesture generation, personality 1. Introduction Sharing experiences by story-telling is a fundamental and prevalent aspect of human social behavior (Bruner, 1991; Bohanek et al., 2006; Labov and Waletzky, 1997; Nelson, 2003). In the wild, stories are told conversationally in social settings, often as a dialogue and with accompanying gestures and other nonverbal behavior (Tolins et al., 2016a). Storytelling in the wild serves many different social functions: e.g. stories are used to persuade, share troubles, establish shared values, learn social behaviors, and entertain (Ryokai et al., 2003; Pennebaker and Seagal, 1999; Gratch et al., 2013). Previous research has also shown that conveying information in the form of a dialogue is more engaging, effective and persuasive compared to a monologue (Lee et al., 1998; Craig et al., 2000; Suzuki and Yamada, 2004; Andr´e et al., 2000). Thus our long term goal is the automatic generation of story co-tellings as animated dialogues. Given a d"
L16-1550,L16-1553,1,0.901644,"searchers interested in natural language generation, intelligent virtual agents, generation of nonverbal behavior, and story and narrative representations. Keywords: storytelling, personal narrative, dialogue, virtual agents, gesture generation, personality 1. Introduction Sharing experiences by story-telling is a fundamental and prevalent aspect of human social behavior (Bruner, 1991; Bohanek et al., 2006; Labov and Waletzky, 1997; Nelson, 2003). In the wild, stories are told conversationally in social settings, often as a dialogue and with accompanying gestures and other nonverbal behavior (Tolins et al., 2016a). Storytelling in the wild serves many different social functions: e.g. stories are used to persuade, share troubles, establish shared values, learn social behaviors, and entertain (Ryokai et al., 2003; Pennebaker and Seagal, 1999; Gratch et al., 2013). Previous research has also shown that conveying information in the form of a dialogue is more engaging, effective and persuasive compared to a monologue (Lee et al., 1998; Craig et al., 2000; Suzuki and Yamada, 2004; Andr´e et al., 2000). Thus our long term goal is the automatic generation of story co-tellings as animated dialogues. Given a d"
L16-1552,brugman-russel-2004-annotating,0,0.134437,"Missing"
L16-1552,W09-0612,0,0.0329585,"resent a personality that matches their own (Bailenson & Yee, 2005; Liu et al., 2013, in press). However, people do seem to be forgiving of machines in ways that some researchers have found they are not of each other (Aronson & Linder, 1965). Computers’ changes to be unlike humans did not result in people disliking the computer (Nass & Moon, 2000). Investigation in human-agent interaction has explored both how humans adapt to agents (Branigan, et al., 2003; Heyselaar et al., 2014), and how agents can be designed to be perceptive towards users’ communicative behaviors and adaptive in response (Buschmeier et al., 2009; De Jong, et al., 2008; Walker, et al., 2007, Mairesse & Walker 2010, 2011). While studies have considered how humans adapt to computers linguistically, through the repetition of syntactic constructions or register, few studies have considered the adaptation of nonverbal expressive behavior in relation to embodied conversational agents (for an exception see Kramer et al., 2007). Similarly, while researchers have attempted to implement systems that imbue agents with automatic alignment between the perception and production of gestures, less work has been done on user gestures directed towards"
L16-1552,J11-3002,1,0.799348,"Missing"
L16-1552,L16-1163,1,0.547747,"Missing"
L16-1552,L16-1553,1,0.535385,"Missing"
L16-1553,W07-1906,0,0.389148,"ypically been found in tasks involving giving and receiving directions, or taking turns describing objects, with a focus on the form of the gesture. We investigated gesture produced in an open, spontaneous dialogue on an abstract subject, and measured partner-specific adaptation in terms of stylistic dimensions of expressive behavior. We note that personality is only one variable that may affect the production of nonverbal behavior, including how nonverbal behavior changes over time. rapport established across conversational partners as well as individual emotion and mood (André et al. 2000;; Cassell et al. 2007; Zhao et al, 2014; Gratch et al. 2007). These factors may be involved both in shaping the production of gestures as well as influencing the degree to which an individual adapts to their conversational partner. Figure 3: Screen capture of speech and gesture transcription in ANVIL, demonstrating the speech and gesture phase and phrase transcription structure. There are likely many reasons why one person adapts to another and another person does not. As with entrainment, other factors might include power dynamics between the interactants, prior shared history, and what interactants think of them"
L16-1553,L16-1550,1,0.867163,"lation can be made, the existing data set is useful for researchers seeking detailed information about how some people with particular personality profiles on the extreme of the extraversion-introversion scale behave across a set of dialogues over the course of one recording session. This corpus could be useful for researchers investigating how personality affects dyadic interaction on verbal and bodily levels. Close investigations could include looking at how gestures that are co-produced with particular words, syntactic structures, or emotional content vary based on personality or time. See Hu et al. (2016) for a demonstration of a possible implementation based on such a paradigm. For example, a gesture used to identify a referent could be quickly taken up by an addressee of a matched personality, but slowly taken up by an addressee of a mismatched personality. As another example, the ratio of beat, iconic, metaphoric, and deictic gestures may vary for different personality types, or as people interact with each other over time. People may also display different postural changes based on their personality or time. As was observed with gestural adaptation in Tolins et al. (2013), postural adaptat"
L16-1553,L16-1552,1,0.535145,"Missing"
L16-1704,W11-0702,1,0.836466,"Quote/Response Pairs from 4forums.com with Mechanical Turk annotations for topic, stance, agreement, hostility, argument type (emotional appeal or fact based), and sarcasm. The agreement/hostility/etc. are mean annotator judgments on a [-5,+5] scale while sarcasm is the percentage of annotators who select Yes of Yes/No/Unsure options. ˇ Boltuzic and Snajder, 2014; Conrad et al., 2012; Habernal and Gurevych, 2015; Habernal and Gurevych, 2016). In our own work to date, we have used the corpus for studies on distinguishing agreement and disagreement (Sridhar et al., 2014; Misra and Walker, 2015; Abbott et al., 2011) and to classify posts by stance-side (Walker et al., 2012c; Sridhar et al., 2014; Walker et al., 2012a; Anand et al., 2011). We have put together a corpus of summaries of the dialogic threads of arguments on Gay Marriage (Misra et al., 2015), and done studies using these summaries as indicators of the importance of arguments and the facets of particular arguments that recur frequenly across the corpus. We have also used this corpus to develop methods for extracting highly specific well-formed arguments on particular topics (Swanson et al., 2015). Other to date has used IAC 1.0 to recognize sa"
L16-1704,P12-1042,0,0.033333,"Missing"
L16-1704,W14-2107,0,0.123215,"Missing"
L16-1704,W12-3810,0,0.0444128,"Missing"
L16-1704,D15-1255,0,0.0873417,"Missing"
L16-1704,W13-3514,0,0.210861,"Really? Well, when I have a kid, I’ll be sure to just leave it in the woods, since it can apparently care for itself (R4 in Fig. 1, see also see Q2 and R2). Insults are common: Here come the Christians, thinking they can know everything by guessing, and commiting the genetic fallacy left and right (R5 in Fig. 1). Much of the corpus is also labelled for STANCE, so that it is useful for studies on stance classification, i.e. whether the speaker is PRO or CON on an issue under discussion (Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009; Hassan et al., 2012; Murakami and Raymond, 2010; Hasan and Ng, 2013). Stance also interacts with agreement and disagreement classification (Yin et al., 2012; Rosenthal and McKeown, 2015), and argumenting mining, where it is useful to know the side of an issue that a particular argument supports (Misra et al., 2015; Hasan and Ng, 2014; 4445 Topic Evolution Evolution Gay Marriage Abortion Existence of God Quote Q, Response R Q1: How can you say such things? The Bible says that God CREATED over and OVER and OVER again! And you reject that and say that everything came about by evolution? If you reject the literal account of the Creation in Genesis, you are saying"
L16-1704,D14-1083,0,0.109647,"Missing"
L16-1704,D10-1121,0,0.0267268,"1. Introduction Large scale corpora have benefited many areas of research in natural language processing, but until recently, resources for dialogue have lagged behind. This is changing as more and more researchers work with social media sites structured in the form of dialogues, such as 4Forums, Create Debate and Reddit as well as sites such as Twitter that provide dialogic affordances such as synchronized community Tweet-Ups and the use of replies to other tweets (AbuJbara et al., 2012; Biran and Rambow, 2011; Somasundaran and Wiebe, 2009; Rosenthal and McKeown, 2015; Sridhar et al., 2015; Hassan et al., 2010; Cook et al., 2014; Cook et al., 2013; Bamman and Smith, 2015). In previous work, we released the I NTERNET A RGUMENT C ORPUS, one of the first larger scale resources available for opinion sharing dialogue (Walker et al., 2012b). Dataset 4forums ConvinceMe CreateDebate Authors 3.5K 5.5K 709 Discussions 11K 5.4K 61 Posts 414K 65K 3K Tokens 57M 6.5M 275K Table 1: The size of each dataset included. This paper describes the I NTERNET A RGUMENT C OR PUS 2.0 (IAC 2.0).1 We have developed a larger scale dialogic corpus by adding conversations from additional sites and structuring them into a novel d"
L16-1704,D12-1006,0,0.0667286,"Missing"
L16-1704,D15-1239,0,0.0287772,"s when posting and must respond with a support, clarify, or dispute tag. Unlike ConvinceMe, responses on CreateDebate appear inline under their parents, creating a more natural discourse. It is also possible for a user to dispute the post of another user even if they self-label the same stance, which creates the opportunity to analyze how debaters supporting the same stance may disagree on certain sub-issues (Sridhar et al., 2014). Like ConvinceMe, CreateDebate allows users to vote on other posts, which the dataset also includes. These votes have been used to analyze persuasion effectiveness (Jaech et al., 2015). There are other releases of subsets of CreateDebate (Hasan and Ng, 2013; Rosenthal and McKeown, 2015). We believe that the IAC 2.0 schema applied to CreateDebate provides a more complete representation of CreateDebate’s extensive affordances and post-response system. It is possible to import other CreatDebate subsets into the IAC 2.0 schema. Other Datasets. We have used this schema successfully with the ConVote corpus as well as with data from Twitter and Reddit. We provide code to import ConVote with all its 4447 Figure 4: The schema’s core elements. This and other schema diagrams are inclu"
L16-1704,P15-2124,0,0.00983004,"l., 2015). Other to date has used IAC 1.0 to recognize sarcasm and nastiness in dialogue (Lukin and Walker, 2013; Justo et al., 2014; Swanson et al., 2014), and to distinguish factual from emotional argumentation (Oraby et al., 2015). Subcorpora of IAC 2.0 useful for working on these topics are also available for download https://nlds.soe.ucsc.edu. Oraby et al.’s bootstrapped corpus of factual vs. feeling arguments (Oraby et al., 2015) and subsets of IAC labelled for disagreement and sarcasm have also been used by other researchers (Pavlick and Tetreault, 2016; Schl¨oder and Fern´andez, 2014; Joshi et al., 2015). 2. Internet Argument Corpus 2.0 Data The IAC 2.0 provides an expanded dataset consisting of dialogues from 4forums.com, CreateDebate.com, and Convinceme.Net. 4forums. 4forums.com is an online forum for political debate and discussion. Its sub-forums cover a broad range of topics relevant to the US political landscape. Users may initiate discussion threads and respond to other posts. The ability to quote other posts in whole or in part is a com4446 monly invoked mechanism which provides precise context. IAC 1.0 contained only discussions from 4forums. The 4forums section of IAC 2.0 is based o"
L16-1704,W13-1104,1,0.576289,"side (Walker et al., 2012c; Sridhar et al., 2014; Walker et al., 2012a; Anand et al., 2011). We have put together a corpus of summaries of the dialogic threads of arguments on Gay Marriage (Misra et al., 2015), and done studies using these summaries as indicators of the importance of arguments and the facets of particular arguments that recur frequenly across the corpus. We have also used this corpus to develop methods for extracting highly specific well-formed arguments on particular topics (Swanson et al., 2015). Other to date has used IAC 1.0 to recognize sarcasm and nastiness in dialogue (Lukin and Walker, 2013; Justo et al., 2014; Swanson et al., 2014), and to distinguish factual from emotional argumentation (Oraby et al., 2015). Subcorpora of IAC 2.0 useful for working on these topics are also available for download https://nlds.soe.ucsc.edu. Oraby et al.’s bootstrapped corpus of factual vs. feeling arguments (Oraby et al., 2015) and subsets of IAC labelled for disagreement and sarcasm have also been used by other researchers (Pavlick and Tetreault, 2016; Schl¨oder and Fern´andez, 2014; Joshi et al., 2015). 2. Internet Argument Corpus 2.0 Data The IAC 2.0 provides an expanded dataset consisting of"
L16-1704,P14-5010,0,0.00131034,"ess of its source. Quotes are an important affordance of the IAC. While quotes typically come verbatim from previous posts, they are ultimately a form of markup, and users often alter quoted material or quote from posts elsewhere in a discussion or on the site as well as external sources (e.g., Wikipedia). We store quote information as standoff annotation in the quotes table. If a quote’s original source post can be identified, it is referenced by identifiers and a text offset. We also mark differences between the quote and its source. Parses. The schema provides support for Stanford CoreNLP (Manning et al., 2014) annotations including tokenization, part of speech tags, parse trees, dependencies, named entities, coreference, and sentence level sentiment. Scripts are provided for calling CoreNLP to generate xml output and storing the parses in the database. We store the constituency parses in a nested set data structure which supports queries over parse structures. See Fig. 6. Annotations. There are also a large number of annotations for the corpus with additional annotations being added all the time. See Fig. 7. SELECT SUBSTR( t e x t , MIN( s t a r t ) + 1 , MAX( end)−MIN( s t a r t ) ) AS n p s t r i"
L16-1704,N15-1046,1,0.239371,"scale while sarcasm is the percentage of annotators who select Yes of Yes/No/Unsure options. ˇ Boltuzic and Snajder, 2014; Conrad et al., 2012; Habernal and Gurevych, 2015; Habernal and Gurevych, 2016). In our own work to date, we have used the corpus for studies on distinguishing agreement and disagreement (Sridhar et al., 2014; Misra and Walker, 2015; Abbott et al., 2011) and to classify posts by stance-side (Walker et al., 2012c; Sridhar et al., 2014; Walker et al., 2012a; Anand et al., 2011). We have put together a corpus of summaries of the dialogic threads of arguments on Gay Marriage (Misra et al., 2015), and done studies using these summaries as indicators of the importance of arguments and the facets of particular arguments that recur frequenly across the corpus. We have also used this corpus to develop methods for extracting highly specific well-formed arguments on particular topics (Swanson et al., 2015). Other to date has used IAC 1.0 to recognize sarcasm and nastiness in dialogue (Lukin and Walker, 2013; Justo et al., 2014; Swanson et al., 2014), and to distinguish factual from emotional argumentation (Oraby et al., 2015). Subcorpora of IAC 2.0 useful for working on these topics are als"
L16-1704,C10-2100,0,0.0185657,"Missing"
L16-1704,W15-0515,1,0.217206,"f summaries of the dialogic threads of arguments on Gay Marriage (Misra et al., 2015), and done studies using these summaries as indicators of the importance of arguments and the facets of particular arguments that recur frequenly across the corpus. We have also used this corpus to develop methods for extracting highly specific well-formed arguments on particular topics (Swanson et al., 2015). Other to date has used IAC 1.0 to recognize sarcasm and nastiness in dialogue (Lukin and Walker, 2013; Justo et al., 2014; Swanson et al., 2014), and to distinguish factual from emotional argumentation (Oraby et al., 2015). Subcorpora of IAC 2.0 useful for working on these topics are also available for download https://nlds.soe.ucsc.edu. Oraby et al.’s bootstrapped corpus of factual vs. feeling arguments (Oraby et al., 2015) and subsets of IAC labelled for disagreement and sarcasm have also been used by other researchers (Pavlick and Tetreault, 2016; Schl¨oder and Fern´andez, 2014; Joshi et al., 2015). 2. Internet Argument Corpus 2.0 Data The IAC 2.0 provides an expanded dataset consisting of dialogues from 4forums.com, CreateDebate.com, and Convinceme.Net. 4forums. 4forums.com is an online forum for political"
L16-1704,Q16-1005,0,0.00790694,"ific well-formed arguments on particular topics (Swanson et al., 2015). Other to date has used IAC 1.0 to recognize sarcasm and nastiness in dialogue (Lukin and Walker, 2013; Justo et al., 2014; Swanson et al., 2014), and to distinguish factual from emotional argumentation (Oraby et al., 2015). Subcorpora of IAC 2.0 useful for working on these topics are also available for download https://nlds.soe.ucsc.edu. Oraby et al.’s bootstrapped corpus of factual vs. feeling arguments (Oraby et al., 2015) and subsets of IAC labelled for disagreement and sarcasm have also been used by other researchers (Pavlick and Tetreault, 2016; Schl¨oder and Fern´andez, 2014; Joshi et al., 2015). 2. Internet Argument Corpus 2.0 Data The IAC 2.0 provides an expanded dataset consisting of dialogues from 4forums.com, CreateDebate.com, and Convinceme.Net. 4forums. 4forums.com is an online forum for political debate and discussion. Its sub-forums cover a broad range of topics relevant to the US political landscape. Users may initiate discussion threads and respond to other posts. The ability to quote other posts in whole or in part is a com4446 monly invoked mechanism which provides precise context. IAC 1.0 contained only discussions fr"
L16-1704,prasad-etal-2008-penn,0,0.099147,"Missing"
L16-1704,W15-4625,0,0.146872,"nt, stance, data integration, online forums, debate 1. Introduction Large scale corpora have benefited many areas of research in natural language processing, but until recently, resources for dialogue have lagged behind. This is changing as more and more researchers work with social media sites structured in the form of dialogues, such as 4Forums, Create Debate and Reddit as well as sites such as Twitter that provide dialogic affordances such as synchronized community Tweet-Ups and the use of replies to other tweets (AbuJbara et al., 2012; Biran and Rambow, 2011; Somasundaran and Wiebe, 2009; Rosenthal and McKeown, 2015; Sridhar et al., 2015; Hassan et al., 2010; Cook et al., 2014; Cook et al., 2013; Bamman and Smith, 2015). In previous work, we released the I NTERNET A RGUMENT C ORPUS, one of the first larger scale resources available for opinion sharing dialogue (Walker et al., 2012b). Dataset 4forums ConvinceMe CreateDebate Authors 3.5K 5.5K 709 Discussions 11K 5.4K 61 Posts 414K 65K 3K Tokens 57M 6.5M 275K Table 1: The size of each dataset included. This paper describes the I NTERNET A RGUMENT C OR PUS 2.0 (IAC 2.0).1 We have developed a larger scale dialogic corpus by adding conversations from additiona"
L16-1704,W14-4321,0,0.0351496,"Missing"
L16-1704,P09-1026,0,0.0606215,"ogue, argument mining, sentiment, stance, data integration, online forums, debate 1. Introduction Large scale corpora have benefited many areas of research in natural language processing, but until recently, resources for dialogue have lagged behind. This is changing as more and more researchers work with social media sites structured in the form of dialogues, such as 4Forums, Create Debate and Reddit as well as sites such as Twitter that provide dialogic affordances such as synchronized community Tweet-Ups and the use of replies to other tweets (AbuJbara et al., 2012; Biran and Rambow, 2011; Somasundaran and Wiebe, 2009; Rosenthal and McKeown, 2015; Sridhar et al., 2015; Hassan et al., 2010; Cook et al., 2014; Cook et al., 2013; Bamman and Smith, 2015). In previous work, we released the I NTERNET A RGUMENT C ORPUS, one of the first larger scale resources available for opinion sharing dialogue (Walker et al., 2012b). Dataset 4forums ConvinceMe CreateDebate Authors 3.5K 5.5K 709 Discussions 11K 5.4K 61 Posts 414K 65K 3K Tokens 57M 6.5M 275K Table 1: The size of each dataset included. This paper describes the I NTERNET A RGUMENT C OR PUS 2.0 (IAC 2.0).1 We have developed a larger scale dialogic corpus by adding"
L16-1704,W10-0214,0,0.0404753,"nand et al., 2011) and (Walker et al., 2012d). The dataset consists of 65,368 posts in 5413 debates by 5783 authors. Users may initiate a debate by specifying the topic and sides. Other users are then forced to self-label stance when commenting by posting on the side they support or by using a rebuttal mechanism, which forces their post to the opposite side. By choosing which side to post on authors self-label for stance. We annotated discussions for topic and mapped the discussion stance to a broader topic stance. Noting the lack of a human topline for stance classification in previous work (Somasundaran and Wiebe, 2010; Thomas et al., 2006), we collected human topline stance annotations for this corpus (Anand et al., 2011). The annotation task presented the annotators with the topic, sides, and a sample post from each side, and then asked them to decide which side of the debate a post belonged to. Context, such as a parent post, was not provided, because this most nearly approximated the conditions under which automatic algorithms for stance classification operated at that time. These annotations are included in the release. CreateDebate. We also introduce a gun control specific subset of CreateDebate.com,"
L16-1704,W14-2715,1,0.892535,"PRO CON PRO CON PRO PRO CON Figure 1: Sample Quote/Response Pairs from 4forums.com with Mechanical Turk annotations for topic, stance, agreement, hostility, argument type (emotional appeal or fact based), and sarcasm. The agreement/hostility/etc. are mean annotator judgments on a [-5,+5] scale while sarcasm is the percentage of annotators who select Yes of Yes/No/Unsure options. ˇ Boltuzic and Snajder, 2014; Conrad et al., 2012; Habernal and Gurevych, 2015; Habernal and Gurevych, 2016). In our own work to date, we have used the corpus for studies on distinguishing agreement and disagreement (Sridhar et al., 2014; Misra and Walker, 2015; Abbott et al., 2011) and to classify posts by stance-side (Walker et al., 2012c; Sridhar et al., 2014; Walker et al., 2012a; Anand et al., 2011). We have put together a corpus of summaries of the dialogic threads of arguments on Gay Marriage (Misra et al., 2015), and done studies using these summaries as indicators of the importance of arguments and the facets of particular arguments that recur frequenly across the corpus. We have also used this corpus to develop methods for extracting highly specific well-formed arguments on particular topics (Swanson et al., 2015)."
L16-1704,P15-1012,1,0.783024,"online forums, debate 1. Introduction Large scale corpora have benefited many areas of research in natural language processing, but until recently, resources for dialogue have lagged behind. This is changing as more and more researchers work with social media sites structured in the form of dialogues, such as 4Forums, Create Debate and Reddit as well as sites such as Twitter that provide dialogic affordances such as synchronized community Tweet-Ups and the use of replies to other tweets (AbuJbara et al., 2012; Biran and Rambow, 2011; Somasundaran and Wiebe, 2009; Rosenthal and McKeown, 2015; Sridhar et al., 2015; Hassan et al., 2010; Cook et al., 2014; Cook et al., 2013; Bamman and Smith, 2015). In previous work, we released the I NTERNET A RGUMENT C ORPUS, one of the first larger scale resources available for opinion sharing dialogue (Walker et al., 2012b). Dataset 4forums ConvinceMe CreateDebate Authors 3.5K 5.5K 709 Discussions 11K 5.4K 61 Posts 414K 65K 3K Tokens 57M 6.5M 275K Table 1: The size of each dataset included. This paper describes the I NTERNET A RGUMENT C OR PUS 2.0 (IAC 2.0).1 We have developed a larger scale dialogic corpus by adding conversations from additional sites and structurin"
L16-1704,swanson-etal-2014-getting,1,0.846323,"2014; Walker et al., 2012a; Anand et al., 2011). We have put together a corpus of summaries of the dialogic threads of arguments on Gay Marriage (Misra et al., 2015), and done studies using these summaries as indicators of the importance of arguments and the facets of particular arguments that recur frequenly across the corpus. We have also used this corpus to develop methods for extracting highly specific well-formed arguments on particular topics (Swanson et al., 2015). Other to date has used IAC 1.0 to recognize sarcasm and nastiness in dialogue (Lukin and Walker, 2013; Justo et al., 2014; Swanson et al., 2014), and to distinguish factual from emotional argumentation (Oraby et al., 2015). Subcorpora of IAC 2.0 useful for working on these topics are also available for download https://nlds.soe.ucsc.edu. Oraby et al.’s bootstrapped corpus of factual vs. feeling arguments (Oraby et al., 2015) and subsets of IAC labelled for disagreement and sarcasm have also been used by other researchers (Pavlick and Tetreault, 2016; Schl¨oder and Fern´andez, 2014; Joshi et al., 2015). 2. Internet Argument Corpus 2.0 Data The IAC 2.0 provides an expanded dataset consisting of dialogues from 4forums.com, CreateDebate.c"
L16-1704,W15-4631,1,0.205202,"t (Sridhar et al., 2014; Misra and Walker, 2015; Abbott et al., 2011) and to classify posts by stance-side (Walker et al., 2012c; Sridhar et al., 2014; Walker et al., 2012a; Anand et al., 2011). We have put together a corpus of summaries of the dialogic threads of arguments on Gay Marriage (Misra et al., 2015), and done studies using these summaries as indicators of the importance of arguments and the facets of particular arguments that recur frequenly across the corpus. We have also used this corpus to develop methods for extracting highly specific well-formed arguments on particular topics (Swanson et al., 2015). Other to date has used IAC 1.0 to recognize sarcasm and nastiness in dialogue (Lukin and Walker, 2013; Justo et al., 2014; Swanson et al., 2014), and to distinguish factual from emotional argumentation (Oraby et al., 2015). Subcorpora of IAC 2.0 useful for working on these topics are also available for download https://nlds.soe.ucsc.edu. Oraby et al.’s bootstrapped corpus of factual vs. feeling arguments (Oraby et al., 2015) and subsets of IAC labelled for disagreement and sarcasm have also been used by other researchers (Pavlick and Tetreault, 2016; Schl¨oder and Fern´andez, 2014; Joshi et"
L16-1704,W06-1639,0,0.713377,"developed a larger scale dialogic corpus by adding conversations from additional sites and structuring them into a novel data schema in SQL. See Table 1. The IAC 2.0 schema provides support for forum posts within distinct dialogues, quotations, markup (bold, italic, etc), and various human and machine developed annotations, including Stanford CoreNLP annotations, such as POS tags, parses, and named entities. We provide Python code that facilitates querying and combining data from different sources. We also demonstrate the generalizability of the schema with code to import the ConVote corpus (Thomas et al., 2006). 1 IAC 2.0 is available at https://nlds.soe.ucsc. edu/iac2 The IAC 2.0 corpus can support research on many different aspects of social language and dialogue structure. The language of dialogue, and particularly of conversations in online forums on social and political topics, is very different from newspaper articles or broadcast news. Subjective genres in traditional media tend to be both monologic and formal, while online debates are strongly dialogic, interpersonal, and colloquial, often containing emotional and colorful language, as exemplified by the excerpts in Fig. 1. Fig. 1 illustrate"
L16-1704,walker-etal-2012-corpus,1,0.776518,"ocial media sites structured in the form of dialogues, such as 4Forums, Create Debate and Reddit as well as sites such as Twitter that provide dialogic affordances such as synchronized community Tweet-Ups and the use of replies to other tweets (AbuJbara et al., 2012; Biran and Rambow, 2011; Somasundaran and Wiebe, 2009; Rosenthal and McKeown, 2015; Sridhar et al., 2015; Hassan et al., 2010; Cook et al., 2014; Cook et al., 2013; Bamman and Smith, 2015). In previous work, we released the I NTERNET A RGUMENT C ORPUS, one of the first larger scale resources available for opinion sharing dialogue (Walker et al., 2012b). Dataset 4forums ConvinceMe CreateDebate Authors 3.5K 5.5K 709 Discussions 11K 5.4K 61 Posts 414K 65K 3K Tokens 57M 6.5M 275K Table 1: The size of each dataset included. This paper describes the I NTERNET A RGUMENT C OR PUS 2.0 (IAC 2.0).1 We have developed a larger scale dialogic corpus by adding conversations from additional sites and structuring them into a novel data schema in SQL. See Table 1. The IAC 2.0 schema provides support for forum posts within distinct dialogues, quotations, markup (bold, italic, etc), and various human and machine developed annotations, including Stanford Core"
L16-1704,N12-1072,1,0.147744,"ocial media sites structured in the form of dialogues, such as 4Forums, Create Debate and Reddit as well as sites such as Twitter that provide dialogic affordances such as synchronized community Tweet-Ups and the use of replies to other tweets (AbuJbara et al., 2012; Biran and Rambow, 2011; Somasundaran and Wiebe, 2009; Rosenthal and McKeown, 2015; Sridhar et al., 2015; Hassan et al., 2010; Cook et al., 2014; Cook et al., 2013; Bamman and Smith, 2015). In previous work, we released the I NTERNET A RGUMENT C ORPUS, one of the first larger scale resources available for opinion sharing dialogue (Walker et al., 2012b). Dataset 4forums ConvinceMe CreateDebate Authors 3.5K 5.5K 709 Discussions 11K 5.4K 61 Posts 414K 65K 3K Tokens 57M 6.5M 275K Table 1: The size of each dataset included. This paper describes the I NTERNET A RGUMENT C OR PUS 2.0 (IAC 2.0).1 We have developed a larger scale dialogic corpus by adding conversations from additional sites and structuring them into a novel data schema in SQL. See Table 1. The IAC 2.0 schema provides support for forum posts within distinct dialogues, quotations, markup (bold, italic, etc), and various human and machine developed annotations, including Stanford Core"
L16-1704,W12-3710,0,0.041556,"Missing"
L16-1704,J17-1004,0,\N,Missing
L18-1628,P16-1231,0,0.0134953,"ting dialogue about hotels revolve either around search or around using a structured dialogue flow. Neither of these methods on their own support fully natural dialogue, and there is not yet an architecture for conversational agents that flexibly combines unstructured information, such as that found in the InfoBox or in reviews or other textual forms, and structured information such as that in Figure 2. Search methods could focus on the content in the current InfoBox, and carry out short (1-2 turn) conversations by applying compression techniques on sentences to make them more conversational (Andor et al., 2016; Krause et al., 2017b). For example, when asked “Tell me about Bass Lake Taverne”, Google Home currently produces an utterance providing its location and how far it is from the user’s location. When asked about hotels in a location, Google Home reads out parts of the information in the Infobox, but it does not engage in further dialogue that explores individual content items. Moreover, the well-known differences between written and oral language (Biber, 1991) means that selected spans from written descriptions may not sound natural when spoken in conversation, and techniques may be needed to"
L18-1628,bonneau-maynard-etal-2006-results,0,0.0605567,"Missing"
L18-1628,W00-1407,0,0.162348,"content to be exchanged, and there are no corpora available with example dialogue flows and generated utterances. To build a simulation for such complex, rich content, we first need a model for how the dialogue manager (DM) should (1) order the content across turns, and (2) select and group the content in each individual turn. Our assumption is that the most important information should be presented earlier in the dialogue, so one way to do this is to apply methods for inducing a ranking on the content attributes. Previous work has developed a model of user preferences to solve this problem (Carenini and Moore, 2000), and shown that users prefer systems whose dialogue behaviors are based on such customized content selection and presentation (Stent et al., 2002; Polifroni et al., 2003; Walker et al., 2007). These preferences (ranking on attributes) can be acquired directly from the user, or can be inferred from their past behavior. Here we try two other methods. First, in Section 3., we ask Turkers to select the most important sentence from the InfoBox descriptions. We then tabulate which attributes are in the selected sentences, and use this to induce a ranking. After using this tabulation to collect addi"
L18-1628,devillers-etal-2004-french,0,0.0311692,"great deal according to the type of hotel: for specialized hotels it includes highly distinctive low-frequency attributes for look-and-feel such 1 The publicly available Yelp dataset2 has around 8,000 entries for US hotels, providing around 80 unique attributes. Figure 1: InfoBox Hotel Description for Bass Lake Taverne as “feels swanky” “historical rooms” or amenities such as “direct access to beach”, “has hot tubs”, or “ski-in, ski-out”. Research on dialogue systems for hotel information has existed for many years, in some cases producing shared dialogue corpora that include hotel bookings (Devillers et al., 2004; Walker et al., 2002; Rudnicky et al., 1999; Villaneau and Antoine, 2009; Bonneau-Maynard et al., 2006; Hastie et al., 2002; Lemon et al., 2006). Historically, these systems have greatly simplified the richness of the domain, and supported highly restricted versions of the hotel booking task, by limiting the information that the system can talk about to a small number of attributes, such as location, number of stars, room type, and price. Data collection involved users being given specific tasks where they simply had to find a hotel in a particular location, rather than satisfy the complex pr"
L18-1628,hastie-etal-2002-automatic,1,0.250234,"look-and-feel such 1 The publicly available Yelp dataset2 has around 8,000 entries for US hotels, providing around 80 unique attributes. Figure 1: InfoBox Hotel Description for Bass Lake Taverne as “feels swanky” “historical rooms” or amenities such as “direct access to beach”, “has hot tubs”, or “ski-in, ski-out”. Research on dialogue systems for hotel information has existed for many years, in some cases producing shared dialogue corpora that include hotel bookings (Devillers et al., 2004; Walker et al., 2002; Rudnicky et al., 1999; Villaneau and Antoine, 2009; Bonneau-Maynard et al., 2006; Hastie et al., 2002; Lemon et al., 2006). Historically, these systems have greatly simplified the richness of the domain, and supported highly restricted versions of the hotel booking task, by limiting the information that the system can talk about to a small number of attributes, such as location, number of stars, room type, and price. Data collection involved users being given specific tasks where they simply had to find a hotel in a particular location, rather than satisfy the complex preferences that users may have booking hotels. This reduction in content simplifies the decisions 3981 Figure 2: Sample of Ho"
L18-1628,W17-5515,0,0.130456,"hotels revolve either around search or around using a structured dialogue flow. Neither of these methods on their own support fully natural dialogue, and there is not yet an architecture for conversational agents that flexibly combines unstructured information, such as that found in the InfoBox or in reviews or other textual forms, and structured information such as that in Figure 2. Search methods could focus on the content in the current InfoBox, and carry out short (1-2 turn) conversations by applying compression techniques on sentences to make them more conversational (Andor et al., 2016; Krause et al., 2017b). For example, when asked “Tell me about Bass Lake Taverne”, Google Home currently produces an utterance providing its location and how far it is from the user’s location. When asked about hotels in a location, Google Home reads out parts of the information in the Infobox, but it does not engage in further dialogue that explores individual content items. Moreover, the well-known differences between written and oral language (Biber, 1991) means that selected spans from written descriptions may not sound natural when spoken in conversation, and techniques may be needed to adapt the utterance t"
L18-1628,A97-1039,0,0.39598,"Missing"
L18-1628,A92-1006,0,0.350729,"lt for the typed query “Tell me about Bass Lake Taverne”. These descriptions are written by human writers within Google Content Studio and cover more than 200 thousand hotels worldwide. The descriptions are designed to provide travelers with quick, reliable and accurate information that they may need when making booking decisions, namely a hotel’s amenities, property, and location. The writers implement many of the decisions that a dialogue system would have to make: they make decisions about content selection, content structuring, attribute groupings and the final realization of the content (Rambow and Korelsky, 1992). They access multiple sources of information, such as user reviews and the hotels’ own web pages. The descriptions cannot be longer than 650 characters and are optimized for visual scanning. There is currently no method for delivering this content to users via a conversation other than reading the whole InfoBox aloud, or reading individual sections of it. Structured data is also available for each hotel, which includes information about the setting of a hotel and its grounds, the feel of the hotel and its rooms, points of interest nearby, room features, and amenities such as restaurants and s"
L18-1628,W09-0507,0,0.0170069,"t includes highly distinctive low-frequency attributes for look-and-feel such 1 The publicly available Yelp dataset2 has around 8,000 entries for US hotels, providing around 80 unique attributes. Figure 1: InfoBox Hotel Description for Bass Lake Taverne as “feels swanky” “historical rooms” or amenities such as “direct access to beach”, “has hot tubs”, or “ski-in, ski-out”. Research on dialogue systems for hotel information has existed for many years, in some cases producing shared dialogue corpora that include hotel bookings (Devillers et al., 2004; Walker et al., 2002; Rudnicky et al., 1999; Villaneau and Antoine, 2009; Bonneau-Maynard et al., 2006; Hastie et al., 2002; Lemon et al., 2006). Historically, these systems have greatly simplified the richness of the domain, and supported highly restricted versions of the hotel booking task, by limiting the information that the system can talk about to a small number of attributes, such as location, number of stars, room type, and price. Data collection involved users being given specific tasks where they simply had to find a hotel in a particular location, rather than satisfy the complex preferences that users may have booking hotels. This reduction in content s"
L18-1707,P87-1022,0,0.718044,"nal discourse state representation. In our example, two entities are extracted, mapping star wars to entity type MovieSeries, and revenge of the sith to Star Wars: Episode III - Revenge Of The Sith with entity type Movie. 3.3.3. Entity Linking Named Entity Linking is primarily encapsulated in two phases, Web Source Linking and Discourse Linking. With Web Source Linking we are interested in linking a known entity to existing resources on the web while discourse linking is focused on linking each mention of the entity within the input to the same discourse entity in our internal representation (Brennan et al., 1987; Walker et al., 1997). As mentioned in Section 3.1., the Google Knowledge Graph query returns a Wikipedia article associated with the entity. We can further increase our web based linking by utilizing the fact that a large number of popular websites use the Schema.org MicroData, allowing us to easily target relevant sources for information extraction. Finally, through empirical examination, we note that pairing the entity type with the precise entity name as provided in the query will allow for easy subsequent queries to large databases such as YAGO (Rebele et al., 2016) or DBpedia (Auer et a"
L18-1707,W99-0613,0,0.549191,"Missing"
L18-1707,N09-1037,0,0.0608372,"Missing"
L18-1707,P14-5010,0,0.0119581,"ctuator, which are based on over 10,000 real user conversations with the system. We perform an extensive analysis of our system and the corpora to identify important areas of future work. NER and NEL have been actively researched topics for decades (Finkel and Manning, 2009; Ratinov and Roth, 2009; Ritter et al., 2011; Derczynski et al., 2015; Nitish Gupta and Roth, 2017). However, the resulting entity classification is often coarse and does not encode an ontology. For example, Stanford NER features only a small number of abstract entity types such as PERSON, LOCATION, ORGANIZATION, and MISC (Manning et al., 2014; Finkel and Manning, 2009); these categories don’t provide enough information for dialogue interpretation and generalization. Although other resources such as that from Ratinov and Roth (2009) utilize additional external knowledge by extracting 30 gazetteers from both the web and Wikipedia, the entity types are still not as varied as we need, and the framework lacks a clear ontology. Furthermore, the alignment of classes between systems can be inconsistent as there is no universally shared taxonomy between them and the various data streams necessary to support open domain conversation(Bowden"
L18-1707,D17-1284,0,0.0663608,"Missing"
L18-1707,W09-1119,0,0.266845,"Missing"
L18-1707,D11-1141,0,0.126124,"Missing"
N01-1003,W98-1411,0,0.0507918,"Missing"
N01-1003,P95-1018,0,0.0119121,"ining data. In general there may be no corpus available for a new application area, or if there is a corpus available, it is a transcript of human-human dialogs. Human-human dialogs, however, may not provide a very good model of sentence planning strategies for a computational system because the sentence planner must plan communicative goals such as implicit confirmation which are needed to prevent and correct errors in automatic speech recognition but which are rare in human-human dialog. Other related work deals with discourse-related aspects of sentence planning such as cue word placement (Moser and Moore, 1995), clearly a crucial task whose integration into our approach we leave to future work. Mellish et al. (1998) investigate the problem of determining a discourse tree for a set of elementary speech acts which are partially constrained by rhetorical relations. Using hand-crafted evaluation metrics, they show that a genetic algorithm achieves good results in finding discourse trees. However, they do not address clausecombining, and we do not use hand-crafted metrics. 7 Discussion We have presented SPoT, a trainable sentence planner. SPoT re-conceptualizes the sentence planning task as consisting of"
N01-1003,W00-0306,0,0.114168,"expected interactions (Hovy and Wanner, 1996), and it is difficult to develop new applications quickly. Presumably, this is the reason why dialog systems to date have not used this kind of sentence planning. Most dialog systems today use template-based generation. The template outputs are typically concatenated to produce a turn realizing all the communicative goals. It is hard to achieve high quality output by concatenating the template-based output for individual communicative goals, and templates are difficult to develop and maintain for a mixed-initiative dialog system. For these reasons, Oh and Rudnicky (2000) use e -gram models and Ratnaparkhi (2000), maximum entropy to choose templates, using hand-written rules to score different candidates. But syntactically simplistic approaches may have quality problems, and more importantly, these approaches only deal with inform speech acts. And crucially, these approaches suffer from the need for training data. In general there may be no corpus available for a new application area, or if there is a corpus available, it is a transcript of human-human dialogs. Human-human dialogs, however, may not provide a very good model of sentence planning strategies for"
N01-1003,A92-1006,1,0.740532,"need only to be in a relation of synonymy or hyperonymy (rather than being identical). S OFT- MERGE - GENERAL. Same as M ERGE - GENERAL, except that the verbs need only to be in a relation of synonymy or hyperonymy. C ONJUNCTION. This is standard conjunction with conjunction reduction. R ELATIVE - CLAUSE. This includes participial adjuncts to nouns. A DJECTIVE. This transforms a predicative use of an adjective into an adnominal construction. P ERIOD. Joins two complete clauses with a period. These operations are not domain-specific and are similar to those of previous aggregation components (Rambow and Korelsky, 1992; Shaw, 1998; Danlos, 2000), although the various M ERGE operations are, to our knowledge, novel in this form. The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts Rule M ERGE M ERGE - GENERAL S OFT- MERGE S OFT- MERGE - Sample first argument You are leaving from Newark. What time would you like to leave? You are leaving from Newark Sample second argument You are leaving at 5 You are leaving from Newark. You are going to Dallas C ONJUNCTION What time would you like to leave? You are le"
N01-1003,A00-2026,0,0.0714695,"and it is difficult to develop new applications quickly. Presumably, this is the reason why dialog systems to date have not used this kind of sentence planning. Most dialog systems today use template-based generation. The template outputs are typically concatenated to produce a turn realizing all the communicative goals. It is hard to achieve high quality output by concatenating the template-based output for individual communicative goals, and templates are difficult to develop and maintain for a mixed-initiative dialog system. For these reasons, Oh and Rudnicky (2000) use e -gram models and Ratnaparkhi (2000), maximum entropy to choose templates, using hand-written rules to score different candidates. But syntactically simplistic approaches may have quality problems, and more importantly, these approaches only deal with inform speech acts. And crucially, these approaches suffer from the need for training data. In general there may be no corpus available for a new application area, or if there is a corpus available, it is a transcript of human-human dialogs. Human-human dialogs, however, may not provide a very good model of sentence planning strategies for a computational system because the sentenc"
N01-1003,W98-1415,0,0.092665,"ion of synonymy or hyperonymy (rather than being identical). S OFT- MERGE - GENERAL. Same as M ERGE - GENERAL, except that the verbs need only to be in a relation of synonymy or hyperonymy. C ONJUNCTION. This is standard conjunction with conjunction reduction. R ELATIVE - CLAUSE. This includes participial adjuncts to nouns. A DJECTIVE. This transforms a predicative use of an adjective into an adnominal construction. P ERIOD. Joins two complete clauses with a period. These operations are not domain-specific and are similar to those of previous aggregation components (Rambow and Korelsky, 1992; Shaw, 1998; Danlos, 2000), although the various M ERGE operations are, to our knowledge, novel in this form. The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts Rule M ERGE M ERGE - GENERAL S OFT- MERGE S OFT- MERGE - Sample first argument You are leaving from Newark. What time would you like to leave? You are leaving from Newark Sample second argument You are leaving at 5 You are leaving from Newark. You are going to Dallas C ONJUNCTION What time would you like to leave? You are leaving from N"
N01-1003,P97-1026,0,0.0247795,"Missing"
N01-1003,A97-1039,0,\N,Missing
N01-1003,C98-1114,1,\N,Missing
N01-1003,P98-1118,1,\N,Missing
N06-2022,P90-1010,1,0.686839,"Missing"
N12-1072,C08-2004,0,0.0746146,"e in doubt by both lay and acedamia, then organic chemistry, physics and gravity are in doubt. Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution? Or is it because you are ignorant? Angelic Falling anyone? I’m obviously ignorant. Look how many times i’ve been given the title. “Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are a"
N12-1072,P11-1151,0,0.0891379,"and gravity are in doubt. Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution? Or is it because you are ignorant? Angelic Falling anyone? I’m obviously ignorant. Look how many times i’ve been given the title. “Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are also personal, giving a strong sense of the indi592 2012 Conference of t"
N12-1072,C10-2100,0,0.059224,"giving it the same treatment you do to evolution? Or is it because you are ignorant? Angelic Falling anyone? I’m obviously ignorant. Look how many times i’ve been given the title. “Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are also personal, giving a strong sense of the indi592 2012 Conference of the North American Chapter of the Association for Computational Ling"
N12-1072,P09-1026,0,0.166411,"nyone? I’m obviously ignorant. Look how many times i’ve been given the title. “Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are also personal, giving a strong sense of the indi592 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 592–596, c Montr´eal, Canada, June 3-8, 2012. 2012 Associati"
N12-1072,W10-0214,0,0.511799,"Look how many times i’ve been given the title. “Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are also personal, giving a strong sense of the indi592 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 592–596, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguisti"
N12-1072,W06-1639,0,0.0530364,"hysics and gravity are in doubt by both lay and acedamia, then organic chemistry, physics and gravity are in doubt. Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution? Or is it because you are ignorant? Angelic Falling anyone? I’m obviously ignorant. Look how many times i’ve been given the title. “Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points"
N12-1072,walker-etal-2012-corpus,1,0.542065,", and the argument was framed as “Yes we should keep it” vs. “No we should not”. posts per author. The first 5 columns of Table 2 shows the variation in these dimensions by topic. In this paper we show that information about dialogic relations between authors (SOURCE factors) improves performance for STANCE classification, when compared to models that only have access to properties of the ARGUMENT. We model SOURCE relations with a graph, and add this information to classifiers operating on the text of a post. Sec. 2 describes the corpus and our approach. Our corpus is publicly available, see (Walker et al., 2012). We show in Sec. 3 that modeling source properties improves performance when the debates are highly dialogic. We leave a more detailed comparison to previous work to Sec. 3 so that we can contrast previous work with our approach. 2 Experimental Method and Approach Our corpus consists of two-sided debates from Convinceme.net for 14 topics that range from playful debates such as Superman vs. Batman (Fig. 2 to more heated political topics such as the Death Penalty (Fig. 3. In total the corpus consists of 2902 two-sided debates (36,307 posts), totaling 3,080,874 words; the topic labelled debates"
N12-1072,N10-1097,0,0.126318,"Missing"
N12-1072,D10-1102,0,0.137351,"y and acedamia, then organic chemistry, physics and gravity are in doubt. Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution? Or is it because you are ignorant? Angelic Falling anyone? I’m obviously ignorant. Look how many times i’ve been given the title. “Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are also personal, giving a str"
N15-1046,W11-0702,1,0.816669,"ed in processing the monologic, informational language characteristic of newswire text. But an increasing share of the text data on the web is unlike newswire in a variety of ways: it is dialogic, opinionated, argumentative. And while some of these dialogs may be a little more than flame wars, a significant portion involve contentful, reasoned disputes on important social and political topics, as exemplified by the forum snippets in Figs. 1 and 3. Studying data like this will undoubtedly help us to understand dialogic and informal argumentative language in general. And, indeed, previous work (Abbott et al., 2011; Somasundaran and Wiebe, 2010) has examined the structure of these discussions – e.g., the argumentative discourse relation a post bears to its parent (agreeing or disagreeing), or the stance that a person takes on an issue. Our goal here is to develop techniques to recognize the specific arguments and counterarguments people tend to advance, and group them across discussions into the FACETS on which that issue is ar430 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 430–440, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association fo"
N15-1046,S12-1051,0,0.049945,"r central propositions. See Fig. 2b. We propose that the CENTRAL PROPOSITIONS of a dialog are exactly those arguments that people find most salient, which is naturally reflected by their summarization behavior. We then apply the Pyramid method, by which the CENTRAL PROPO SITIONS bubble up to the highest tiers of the pyramid, thereby allowing us to identify them. With the central propositions in hand, we proceed to build the argument facet inducer. We introduce a new task of ARGUMENT FACET SIMILARITY (AFS). We discuss how AFS is similar to, but different than SEMANTIC TEXTUAL SIMILARITY (STS) (Agirre et al., 2012; Jurgens et al., 2014; Agirre et al., 2013; Beltagy et al., 2014; Han et al., 2013). Sec. 2 provides a more detailed overview and description of our method, and the data that it produces. Sec. 3 describes our experimental setup for the AFS task and then presents our results. We describe a learning approach that achieves correlations of .54 on the AFS task, as compared to a baseline correlation of .45 using off-the-shelf modules that are competitive in STS tasks. We delay a detailed discussion of related work to Sec. 4 when we can compare it to our own approach. Sec. 5 summarizes the paper and"
N15-1046,P14-1114,0,0.0148034,"PROPOSITIONS of a dialog are exactly those arguments that people find most salient, which is naturally reflected by their summarization behavior. We then apply the Pyramid method, by which the CENTRAL PROPO SITIONS bubble up to the highest tiers of the pyramid, thereby allowing us to identify them. With the central propositions in hand, we proceed to build the argument facet inducer. We introduce a new task of ARGUMENT FACET SIMILARITY (AFS). We discuss how AFS is similar to, but different than SEMANTIC TEXTUAL SIMILARITY (STS) (Agirre et al., 2012; Jurgens et al., 2014; Agirre et al., 2013; Beltagy et al., 2014; Han et al., 2013). Sec. 2 provides a more detailed overview and description of our method, and the data that it produces. Sec. 3 describes our experimental setup for the AFS task and then presents our results. We describe a learning approach that achieves correlations of .54 on the AFS task, as compared to a baseline correlation of .45 using off-the-shelf modules that are competitive in STS tasks. We delay a detailed discussion of related work to Sec. 4 when we can compare it to our own approach. Sec. 5 summarizes the paper and discusses future work. 2 Experimental Method Fig. 2 summarizes o"
N15-1046,W14-2107,0,0.082279,"S. Our scale and MT task for AFS was inspired by the STS task and definition. In addition, as a baseline we apply an off-the-shelf system that calculates STS (UMBC) and compare it with our own system (Han et al., 2013). In order to avoid asking for judgements for many unrelated arguments (CEN TRAL PROPOSITIONS ), and to make the AFS task more doable for Turkers, we also use UMBC as a filter on pairs of CENTRAL PROPOSITIONS as part of making our HIT. This biases the distribution of the training set to having a much larger set of more similar pairs, which has been a problem for previous ˇ work (Boltuzic and Snajder, 2014), where the vast majority of pairs that were labelled were unrelated. However the AFS task is clearly different than STS, partly because the data is dialogic and partly because it is argumentative. Our results show that we can improve on STS systems for the AFS task. Dialog Summarization. Much previous work on dialog summarization focused on extracting phenomena specific to meetings, such as action items or decisions (Murray et al., 2006; Hsueh and Moore, 2008; Whittaker et al., 2012; Janin et al., 2004; Carletta, 2007). Other approaches, like our work, use semantic similarity metrics to ident"
N15-1046,N10-1122,0,0.0231122,"our facets are deliberately designed to unify across stance disagreement. Finally, all other approaches in argument mining work from the source text itself. We instead (to our knowledge, for the first time) work from human summaries of dialogs because it is an open question whether the CENTRAL PROPOSITIONS for a dialog are really identifiable as continuous spans of text in the dialog itself. (Indeed, our corpus will allow us to determine how true that assumption is.) Semantic Textual Similarity. There appears to be similarity between FACET induction and aspect learning in sentiment analysis (Brody and Elhadad, 2010), but FACETS are propositional abstract objects, while aspects can usually be described as nouns or properties. Facet induction is more similar to work on STS (Mihalcea et al., 2006; Yeh et al., 2009; Agirre et al., 2012; Han et al., 2013; Jurgens et al., 2014). Calculating similarity is a central aspect of AFS. Our scale and MT task for AFS was inspired by the STS task and definition. In addition, as a baseline we apply an off-the-shelf system that calculates STS (UMBC) and compare it with our own system (Han et al., 2013). In order to avoid asking for judgements for many unrelated arguments"
N15-1046,W12-3810,0,0.0915768,"Missing"
N15-1046,P11-1099,0,0.0377233,"elated work: (1) argument mining; (2) semantic textual similarity; and (3) dialog summarization, which we discuss and compare with our work below. Argument Mining. The study of the structure of arguments has a long tradition in logic, rhetoric and psychology (Walton et al., 2008; Reed and Rowe, 2004; Walton, 2009; Gilbert, 1997; Jackson and Jacobs, 1980; Madnani et al., 2012). Much of this work has been on formal (legal or political) argumentation, and the small computational literature that has applied the rhetorical categories of this research has likewise focused on formal, monologic text (Feng and Hirst, 2011; Palau and Moens, 2009; Goudas et al., 2014). More recent work (Ghosh et al., 2014) has attempted to apply these theories to dialogic text in online forums. Ghosh et al. label spans in conversations with attacking moves (CALL OUTS ) and their corresponding argumentative TAR GETS in another speaker’s utterance, and they attempt to learn these callout-target pairs in a supervised framework. Other work attempts to identify general categories of speech-acts such as disagreements or justifications (Misra and Walker, 2015; Biran and Rambow, 2011). What unites all of the above approaches is an inter"
N15-1046,W14-2106,0,0.0585207,"marization, which we discuss and compare with our work below. Argument Mining. The study of the structure of arguments has a long tradition in logic, rhetoric and psychology (Walton et al., 2008; Reed and Rowe, 2004; Walton, 2009; Gilbert, 1997; Jackson and Jacobs, 1980; Madnani et al., 2012). Much of this work has been on formal (legal or political) argumentation, and the small computational literature that has applied the rhetorical categories of this research has likewise focused on formal, monologic text (Feng and Hirst, 2011; Palau and Moens, 2009; Goudas et al., 2014). More recent work (Ghosh et al., 2014) has attempted to apply these theories to dialogic text in online forums. Ghosh et al. label spans in conversations with attacking moves (CALL OUTS ) and their corresponding argumentative TAR GETS in another speaker’s utterance, and they attempt to learn these callout-target pairs in a supervised framework. Other work attempts to identify general categories of speech-acts such as disagreements or justifications (Misra and Walker, 2015; Biran and Rambow, 2011). What unites all of the above approaches is an interest in understanding the detailed rheotrical structure of a particular linguistic in"
N15-1046,C04-1110,0,0.253373,"ted. However the AFS task is clearly different than STS, partly because the data is dialogic and partly because it is argumentative. Our results show that we can improve on STS systems for the AFS task. Dialog Summarization. Much previous work on dialog summarization focused on extracting phenomena specific to meetings, such as action items or decisions (Murray et al., 2006; Hsueh and Moore, 2008; Whittaker et al., 2012; Janin et al., 2004; Carletta, 2007). Other approaches, like our work, use semantic similarity metrics to identify the most central or important utterances of a spoken dialog (Gurevych and Strube, 2004), but do not attempt to find the FACETS of a set of arguments across multiple dialogs. Another parallel may exist between work on nuclearity in RST and its use in summarization (Marcu, 1999). However our notion of a CENTRAL PROPOSITION is different than nuclearity in RST, since FACETS are derived from CEN TRAL PROPOSITIONS that rise to the top of the pyramid across summarizers, and then (via AFS) across many dialogs on a topic, while RST nuclearity is only defined for a span of text by a single speaker. Other work examines how social phenomena affect summarization, such as a study of how the p"
N15-1046,S13-1005,0,0.106917,"log are exactly those arguments that people find most salient, which is naturally reflected by their summarization behavior. We then apply the Pyramid method, by which the CENTRAL PROPO SITIONS bubble up to the highest tiers of the pyramid, thereby allowing us to identify them. With the central propositions in hand, we proceed to build the argument facet inducer. We introduce a new task of ARGUMENT FACET SIMILARITY (AFS). We discuss how AFS is similar to, but different than SEMANTIC TEXTUAL SIMILARITY (STS) (Agirre et al., 2012; Jurgens et al., 2014; Agirre et al., 2013; Beltagy et al., 2014; Han et al., 2013). Sec. 2 provides a more detailed overview and description of our method, and the data that it produces. Sec. 3 describes our experimental setup for the AFS task and then presents our results. We describe a learning approach that achieves correlations of .54 on the AFS task, as compared to a baseline correlation of .45 using off-the-shelf modules that are competitive in STS tasks. We delay a detailed discussion of related work to Sec. 4 when we can compare it to our own approach. Sec. 5 summarizes the paper and discusses future work. 2 Experimental Method Fig. 2 summarizes our overall method f"
N15-1046,D14-1083,0,0.0179626,"gories of speech-acts such as disagreements or justifications (Misra and Walker, 2015; Biran and Rambow, 2011). What unites all of the above approaches is an interest in understanding the detailed rheotrical structure of a particular linguistic interaction (monologic or dialogic). Our present work is focused instead on inducing the recurring FACETS in a particular topic domain via weakly supervised learning over several dialogic interactions. Several different threads of recent research on argument mining have strong parallels with this goal (Conrad et al., 2012; Boltuzic and ˇ Snajder, 2014; Hasan and Ng, 2014). Conrad & Wiebe construct an argument mining system on monologic weblog and news data about universal healthcare. One component of their system identifies ARGUING SEGMENTS and the second component labels the segments with the relevant stance-specific ARGUMENT TAGS. They show that distributional similarity features help identify arguments that belong to the same tag set (notably, we did not find distributional similarity helpful for AFS.) Boltuzic & Snajder pursue argument mining on comment streams. Instead of hand-generating argument tags like Conrad & Wiebe, they select short sentential summ"
N15-1046,W04-1013,0,0.0283692,"our dialog corpus. From this data, we extracted the LIWC categories most frequent nouns, verbs and adjectives. For the verbs category, we excluded the verbs present in the NLTK stop word list. We retained only semantically rich categories such as Biological Processes, Causation, Cognitive Processes, Humans, Negative Emotion, Positive Emotion, Religion, Sexual, and Social Processes. The score for this set was the LIWC category overlap count across pairs for each category. ROUGE Scores. ROUGE is a family of metrics to determine the quality of a summary by comparing it to other ideal summaries (Lin, 2004). It is based on a number of overlapping units such as n-gram, word sequences, and word pairs. This feature includes all of the rouge f-scores available via the package at https://pypi.python.org/pypi/pyrouge/0.1.0. 3.2 Row 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Feature Set NGRAM (N) UMBC (U) LIWC (L) DISCO (D) ROUGE (R) N-U N-L N-R N-D U-R U-L U-D N-L-R U-L-R N-L-R-D N-L-R-U N-L-R-D-U R 0.39 0.46 0.32 0.33 0.34 0.47 0.45 0.42 0.41 0.48 0.51 0.45 0.48 0.53 0.50 0.54 0.54 MAE 0.90 0.86 0.92 0.93 0.91 0.85 0.86 0.88 0.89 0.84 0.83 0.86 0.84 0.81 0.83 0.80 0.80 RMS 1.09 1.06 1.13 1.12 1.12 1.0"
N15-1046,I11-1068,0,0.0422335,". In future work, we aim to expand on this work in several ways. First, we hope to expand summaries, similarity judgments, and systems to several topics beyond gay marriage. We believe, for example, that the features and the system we have trained for AFS will apply to other domains without retraining, since none of the features are topic specific, but we have not shown that. In addition, we aim to develop additional features and improve on the results reported here. For example, we believe that it is possible that other off-the-shelf systems, such as for example one for sentence specificity (Louis and Nenkova, 2011; Louis and Nenkova, 2012), might possibly help with aspects of this task. In addition, in future, we aim to automatically identify CENTRAL PROPO SITIONS without the mediation of human summarizers and evaluators. Given the summaries that we have collected for each dialog, we plan to examine the relationship between the contributors to the related pyramid and the original source text, to determine whether indeed there are surface features of the source that would allow us to treat CENTRAL PROPOSITION detection as an extractive task. Acknowledgments This work was funded by NSF GRANT IIS-1302668,"
N15-1046,louis-nenkova-2012-corpus,0,0.0149937,"to expand on this work in several ways. First, we hope to expand summaries, similarity judgments, and systems to several topics beyond gay marriage. We believe, for example, that the features and the system we have trained for AFS will apply to other domains without retraining, since none of the features are topic specific, but we have not shown that. In addition, we aim to develop additional features and improve on the results reported here. For example, we believe that it is possible that other off-the-shelf systems, such as for example one for sentence specificity (Louis and Nenkova, 2011; Louis and Nenkova, 2012), might possibly help with aspects of this task. In addition, in future, we aim to automatically identify CENTRAL PROPO SITIONS without the mediation of human summarizers and evaluators. Given the summaries that we have collected for each dialog, we plan to examine the relationship between the contributors to the related pyramid and the original source text, to determine whether indeed there are surface features of the source that would allow us to treat CENTRAL PROPOSITION detection as an extractive task. Acknowledgments This work was funded by NSF GRANT IIS-1302668, Grant NPS-BAA-03, and an"
N15-1046,N12-1003,0,0.0234168,"n and holocau are somehow related. NGRAM overlap does the best in Row 13 despite the fact that the phrase No one argues the point that does not participate in the NGRAM overlap. 4 Related Work Our approach draws on three different strands of related work: (1) argument mining; (2) semantic textual similarity; and (3) dialog summarization, which we discuss and compare with our work below. Argument Mining. The study of the structure of arguments has a long tradition in logic, rhetoric and psychology (Walton et al., 2008; Reed and Rowe, 2004; Walton, 2009; Gilbert, 1997; Jackson and Jacobs, 1980; Madnani et al., 2012). Much of this work has been on formal (legal or political) argumentation, and the small computational literature that has applied the rhetorical categories of this research has likewise focused on formal, monologic text (Feng and Hirst, 2011; Palau and Moens, 2009; Goudas et al., 2014). More recent work (Ghosh et al., 2014) has attempted to apply these theories to dialogic text in online forums. Ghosh et al. label spans in conversations with attacking moves (CALL OUTS ) and their corresponding argumentative TAR GETS in another speaker’s utterance, and they attempt to learn these callout-targe"
N15-1046,N06-1047,0,0.0527974,"HIT. This biases the distribution of the training set to having a much larger set of more similar pairs, which has been a problem for previous ˇ work (Boltuzic and Snajder, 2014), where the vast majority of pairs that were labelled were unrelated. However the AFS task is clearly different than STS, partly because the data is dialogic and partly because it is argumentative. Our results show that we can improve on STS systems for the AFS task. Dialog Summarization. Much previous work on dialog summarization focused on extracting phenomena specific to meetings, such as action items or decisions (Murray et al., 2006; Hsueh and Moore, 2008; Whittaker et al., 2012; Janin et al., 2004; Carletta, 2007). Other approaches, like our work, use semantic similarity metrics to identify the most central or important utterances of a spoken dialog (Gurevych and Strube, 2004), but do not attempt to find the FACETS of a set of arguments across multiple dialogs. Another parallel may exist between work on nuclearity in RST and its use in summarization (Marcu, 1999). However our notion of a CENTRAL PROPOSITION is different than nuclearity in RST, since FACETS are derived from CEN TRAL PROPOSITIONS that rise to the top of t"
N15-1046,N04-1019,0,0.138139,"ing summarizing a sample dialog. Workers were instructed to summarize according to dialog length: dialogs under 750 words in 125 words, and those above 750 in 175 words. We use 45 dialogs in this study and save the other 40 for future work. We collect 5 summaries for each dialog resulting in a dataset of 225 summaries. Fig. 4 provides 2 of the 5 summaries collected for the dialog in Fig. 3. S3: Pyramid Annotation. We trained three undergraduates to annotate summaries to produce pyramids. We hypothesize that we can use the Pyramid method to induce the FACETS of a topic across a set of dialogs (Nenkova and Passonneau, 2004). The annotation of Pyramids seeks to uncover the common elements, or summary content units (SCUs), across several summaries (in our case, 5). Each SCU identifies a set of spans that are semantically equivalent. Each SCU also has a unique annotatorgenerated label that reflects the semantic meaning of the contributions. Because our aim here is to focus on argument propositional content, the annotators were instructed to keep only the main proposition in the SCU as the label, ignoring any attributions or other types of content. See Table 1. Once annotation is complete, the SCUs are ranked based"
N15-1046,W10-0214,0,0.0236854,"monologic, informational language characteristic of newswire text. But an increasing share of the text data on the web is unlike newswire in a variety of ways: it is dialogic, opinionated, argumentative. And while some of these dialogs may be a little more than flame wars, a significant portion involve contentful, reasoned disputes on important social and political topics, as exemplified by the forum snippets in Figs. 1 and 3. Studying data like this will undoubtedly help us to understand dialogic and informal argumentative language in general. And, indeed, previous work (Abbott et al., 2011; Somasundaran and Wiebe, 2010) has examined the structure of these discussions – e.g., the argumentative discourse relation a post bears to its parent (agreeing or disagreeing), or the stance that a person takes on an issue. Our goal here is to develop techniques to recognize the specific arguments and counterarguments people tend to advance, and group them across discussions into the FACETS on which that issue is ar430 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 430–440, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics (a)"
N15-1046,P05-1037,0,0.042505,"ETS are derived from CEN TRAL PROPOSITIONS that rise to the top of the pyramid across summarizers, and then (via AFS) across many dialogs on a topic, while RST nuclearity is only defined for a span of text by a single speaker. Other work examines how social phenomena affect summarization, such as a study of how the politeness level in computer-generated dialogs impacted summaries (Roman et al., 2006). Emotion naturally occurs in the IAC, and summarizers’ orientation to emotion is intriguing. Emotional information has been observed even in summaries of professional chats discussing technology (Zhou and Hovy, 2005). However the instructions to our Pyramid annotators were to not include information of this type in the pyramids. We are currently collecting an additional summary corpus using a method that we expect to result in more evaluative and emotional assessments in summaries. 5 Conclusion This paper presents a method and results for extracting FACETS of a topic, across multiple informal arguments on the same topic. We first use human summarization of dialogs as a probe to determine the CENTRAL PROPOSITIONS of each dialog. Then we use clustering in combination with measures of SEMANTIC SIMILARITY to"
N15-1046,W09-3206,0,\N,Missing
N15-1046,S14-2003,0,\N,Missing
N15-1046,S13-1004,0,\N,Missing
N15-1046,W13-4006,1,\N,Missing
N16-1146,P14-2050,0,0.0637387,"Missing"
N16-1146,D14-1215,0,0.623745,"even human annotators. We also present an analysis of the similes in our data set with respect to their interpretive diversity (intuitively, a measure of how many plausible interpretations a simile has). We show that our method performs best on similes with low diversity, as one would expect since their implicit properties are most clear to humans. 2 Problem Description and Data A simile typically consists of four key components: the topic or tenor (subject of the comparison), the vehicle (object of the comparison), the event (act or state), and a comparator (usually “as”, “like”, or “than”) (Niculae and Danescu-Niculescu-Mizil, 2014). For the simile “the room feels like Antarctica”, “room” is the tenor, “feels” is the event, and “Antarctica” is the vehicle. A property (shared attribute) can optionally be included to explicitly state how the tenor is being compared with the vehicle, (e.g., “the room is as cold as Antarctica”). Table 1 shows examples of open similes from our Twitter data set, along with several properties inferred by our human annotators (our data set will be described in Section 2.1). We represent each simile using just the head noun of the tenor and vehicle, and the lemma of the event. Veale and Hao (2007"
N16-1146,P13-3013,0,0.151977,"is given. Hanks (2005) manually categorized vehicle nouns of similes into semantic categories. Automatic approaches that use computational models for similes are relatively rare. Veale and Hao (2007) extracted salient properties of vehicles from the web using “as ADJ as a/an NOUN” extraction pattern to acquire knowledge for concept categories. Veale (2012) built a knowledge-base of affective stereotypes by characterizing simile vehicles with salient properties. Li et al. (2012) used explicit property extraction patterns to determine the sentiment that properties convey toward simile vehicles. Niculae and Yaneva (2013) and Niculae (2013) used constituency and dependency parsing-based techniques to identify similes in text. Qadir et al. (2015) classified similes into positive and negative affective polarities using supervised classification, with 1231 features derived from simile components. Niculae and Danescu-Niculescu-Mizil (2014) designed a classifier with domain specific, domain agnostic, and metaphor inspired features to determine when comparisons are figurative. Computational approaches to work on figurative language also include figurative language identification using word sense disambiguation (Rent"
N16-1146,W13-3829,0,0.332402,"y categorized vehicle nouns of similes into semantic categories. Automatic approaches that use computational models for similes are relatively rare. Veale and Hao (2007) extracted salient properties of vehicles from the web using “as ADJ as a/an NOUN” extraction pattern to acquire knowledge for concept categories. Veale (2012) built a knowledge-base of affective stereotypes by characterizing simile vehicles with salient properties. Li et al. (2012) used explicit property extraction patterns to determine the sentiment that properties convey toward simile vehicles. Niculae and Yaneva (2013) and Niculae (2013) used constituency and dependency parsing-based techniques to identify similes in text. Qadir et al. (2015) classified similes into positive and negative affective polarities using supervised classification, with 1231 features derived from simile components. Niculae and Danescu-Niculescu-Mizil (2014) designed a classifier with domain specific, domain agnostic, and metaphor inspired features to determine when comparisons are figurative. Computational approaches to work on figurative language also include figurative language identification using word sense disambiguation (Rentoumi et al., 2009),"
N16-1146,N13-1039,0,0.0174992,"Missing"
N16-1146,D15-1019,1,0.742686,"tional models for similes are relatively rare. Veale and Hao (2007) extracted salient properties of vehicles from the web using “as ADJ as a/an NOUN” extraction pattern to acquire knowledge for concept categories. Veale (2012) built a knowledge-base of affective stereotypes by characterizing simile vehicles with salient properties. Li et al. (2012) used explicit property extraction patterns to determine the sentiment that properties convey toward simile vehicles. Niculae and Yaneva (2013) and Niculae (2013) used constituency and dependency parsing-based techniques to identify similes in text. Qadir et al. (2015) classified similes into positive and negative affective polarities using supervised classification, with 1231 features derived from simile components. Niculae and Danescu-Niculescu-Mizil (2014) designed a classifier with domain specific, domain agnostic, and metaphor inspired features to determine when comparisons are figurative. Computational approaches to work on figurative language also include figurative language identification using word sense disambiguation (Rentoumi et al., 2009), harvesting metaphors by using noun and verb clustering-based techniques (Shutova et al., 2010), interpreti"
N16-1146,R09-1067,0,0.0204717,"013) and Niculae (2013) used constituency and dependency parsing-based techniques to identify similes in text. Qadir et al. (2015) classified similes into positive and negative affective polarities using supervised classification, with 1231 features derived from simile components. Niculae and Danescu-Niculescu-Mizil (2014) designed a classifier with domain specific, domain agnostic, and metaphor inspired features to determine when comparisons are figurative. Computational approaches to work on figurative language also include figurative language identification using word sense disambiguation (Rentoumi et al., 2009), harvesting metaphors by using noun and verb clustering-based techniques (Shutova et al., 2010), interpreting metaphors by generating literal paraphrases (Shutova, 2010), etc. Although previous research has extensively used explicit property extraction patterns for various tasks, none has explored the impact of multiple simile components for inferring properties. To our knowledge, we are the first to introduce the task of automatically inferring the implicit properties in open similes, which is fundamental to automatic understanding of similes. 6 Conclusion In this work, we addressed the prob"
N16-1146,C10-1113,0,0.0891833,"Missing"
N16-1146,P12-2015,0,0.194683,"and property salience have been compared by Gagn´e (2002). Fishelov (2007) experimented with affective connotation and degrees of difficulty associated with understanding a simile when a simile property is conventional or unconventional, or no property is given. Hanks (2005) manually categorized vehicle nouns of similes into semantic categories. Automatic approaches that use computational models for similes are relatively rare. Veale and Hao (2007) extracted salient properties of vehicles from the web using “as ADJ as a/an NOUN” extraction pattern to acquire knowledge for concept categories. Veale (2012) built a knowledge-base of affective stereotypes by characterizing simile vehicles with salient properties. Li et al. (2012) used explicit property extraction patterns to determine the sentiment that properties convey toward simile vehicles. Niculae and Yaneva (2013) and Niculae (2013) used constituency and dependency parsing-based techniques to identify similes in text. Qadir et al. (2015) classified similes into positive and negative affective polarities using supervised classification, with 1231 features derived from simile components. Niculae and Danescu-Niculescu-Mizil (2014) designed a c"
N16-1146,N10-1147,0,\N,Missing
N18-1014,W17-5519,0,0.0253268,"State-of-the-art NLG systems build thus on deep neural sequenceto-sequence models (Sutskever et al., 2014) with an encoder-decoder architecture (Cho et al., 2014) equipped with an attention mechanism (Bahdanau et al., 2015). They typically also rely on slot delexicalization (Mairesse et al., 2010; Henderson et al., 2014), which allows the model to better generalize to unseen inputs, as exemplified by TGen (Duˇsek and Jurˇc´ıcˇ ek, 2016). However, Nayak et al. (2017) point out that there are frequent scenarios where delexicalization behaves inadequately (see Section 5.1 for more details), and Agarwal and Dymetman (2017) show that a character-level approach to NLG may avoid the need for delexicalization, at the potential cost of making more semantic omission errors. The end-to-end approach to NLG typically requires a mechanism for aligning slots on the output utterances: this allows the model to generate 1 E2E 3 Datasets We evaluated the models on three datasets from different domains. The primary one is the recently released E2E restaurant dataset (Novikova et al., 2017b) with 48K samples. For benchmarking we use the TV dataset and the Laptop dataset (Wen et al., 2016) with 7K and 13K samples, respectively."
N18-1014,D16-1230,0,0.0183459,"tively. 6.2 LSTM Experiments on the E2E Dataset We start by evaluating our system on the E2E dataset. Since the reference utterances in the test set were kept secret for the E2E NLG Challenge, we carried out the metric evaluation using the validation set. This was necessary to narrow down the models that perform well compared to the baseline. The final model selection was done based on a human evaluation of the models’ outputs on the test set. 6.2.2 Human Evaluation It is known that automatic metrics function only as a general and vague indication of the quality of an utterance in a dialogue (Liu et al., 2016; Novikova et al., 2017a). Systems which score similarly according to these metrics could produce utterances that are significantly different because automatic 6.2.1 Automatic Metric Evaluation In the first experiment, we assess what effect the augmenting of the training set via utterance splitting has on the performance of different models. The results in Table 6 show that both the LSTM and the CNN models clearly benefit from additional pseudo-samples in the training set. This can likely be attributed to the model having access to 3 The scores here correspond to the model submitted to the E2E"
N18-1014,D17-1151,0,0.0674643,"Missing"
N18-1014,P10-1157,0,0.623753,"Missing"
N18-1014,J14-4003,0,0.0591507,"mplex ones, such as an inform DA containing multiple slots with various types of values (see example in Table 1). Earlier work on statistical NLG approaches were typically hybrids of a handcrafted component and a statistical training method (Langkilde and Knight, 1998; Stent et al., 2004; Rieser and Lemon, 2010). The handcrafted aspects, however, lead to decreased portability and potentially limit the variability of the outputs. New corpusbased approaches emerged that used semantically aligned data to train language models that output utterances directly from their MRs (Mairesse et al., 2010; Mairesse and Young, 2014). The alignment provides valuable information during training, but the semantic annotation is costly. The most recent methods do not require aligned data and use an end-to-end approach to training, performing sentence planning and surface realization simultaneously (Konstas and Lapata, 2013). The most successful systems trained on unaligned data use recurrent neural networks (RNNs) paired with an encoder-decoder system design (Mei et al., 152 Proceedings of NAACL-HLT 2018, pages 152–162 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2016; Duˇsek and"
N18-1014,P14-5010,0,0.00336533,"ltering the training data to contain only the most natural sounding and structurally complex utterances for each MR. For instance, we prefer having an elegant, singlesentence utterance with an apposition as the reference for an MR, rather than an utterance composed of three simple sentences, two of which begin with “it” (see the examples in Table 5). We assess the complexity and naturalness of each utterance by the use of discourse phenomena, such as contrastive cues, subordinate clauses, or aggregation. We identify these in the utterance’s parse-tree produced by the Stanford CoreNLP toolkit (Manning et al., 2014) by defining a set of rules for extracting the discourse phenomena. Furthermore, we consider the number of sentences used to convey all the information in the corresponding MR, as longer sentences tend to exhibit more advanced discourse phenomena. Penalizing utterances for too many sentences contributes to reducing the proportion of generic reference utter157 ances, such as the “simple” example in the above table, in the filtered training set. 6 BLEU NIST METEOR ROUGE Evaluation Researchers in NLG have generally used both automatic and human evaluation. Our results report the standard automati"
N18-1014,P16-2008,0,0.335065,"Missing"
N18-1014,N16-1086,0,0.188901,"Missing"
N18-1014,D17-1238,0,0.461648,"Missing"
N18-1014,W16-6644,0,0.303128,"Missing"
N18-1014,P02-1040,0,0.101722,"for extracting the discourse phenomena. Furthermore, we consider the number of sentences used to convey all the information in the corresponding MR, as longer sentences tend to exhibit more advanced discourse phenomena. Penalizing utterances for too many sentences contributes to reducing the proportion of generic reference utter157 ances, such as the “simple” example in the above table, in the filtered training set. 6 BLEU NIST METEOR ROUGE Evaluation Researchers in NLG have generally used both automatic and human evaluation. Our results report the standard automatic evaluation metrics: BLEU (Papineni et al., 2002), NIST (Przybocki et al., 2009), METEOR (Lavie and Agarwal, 2007), and ROUGE-L (Lin, 2004). For the E2E dataset experiments, we additionally report the results of the human evaluation carried out on the CrowdFlower platform as a part of the E2E NLG Challenge. 6.1 s 0.6664 8.0150 0.4420 s 0.6930‡ 8.4198 0.4379 0.7062 0.7099 CNN s 0.6599 7.8520 0.4333 s 0.6760† 8.0440 0.4448 0.7018 0.7055 Table 6: Automatic metric scores of different models tested on the E2E dataset, both unmodified (s) and augmented (s) through the utterance splitting. The symbols † and ‡ indicate statistically significant impr"
N18-1014,C16-1105,0,0.11306,"antic annotation is costly. The most recent methods do not require aligned data and use an end-to-end approach to training, performing sentence planning and surface realization simultaneously (Konstas and Lapata, 2013). The most successful systems trained on unaligned data use recurrent neural networks (RNNs) paired with an encoder-decoder system design (Mei et al., 152 Proceedings of NAACL-HLT 2018, pages 152–162 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2016; Duˇsek and Jurˇc´ıcˇ ek, 2016), but also other concepts, such as imitation learning (Lampouras and Vlachos, 2016). These NLG models, however, typically require greater amount of data for training due to the lack of semantic alignment, and they still have problems producing syntactically and semantically correct output, as well as being limited in naturalness (Nayak et al., 2017). Here we present a neural ensemble natural language generator, which we train and test on three large unaligned datasets in the restaurant, television, and laptop domains. We explore novel ways to represent the MR inputs, including novel methods for delexicalizing slots and their values, automatic slot alignment, as well as the u"
N18-1014,P98-1116,0,0.109696,"hich need to be conveyed to the human user during the dialogue. Each piece of information is represented by a slotvalue pair, where the slot identifies the type of information and the value is the corresponding content. Dialogue act (DA) types vary depending on the dialogue manager, ranging from simple ones, such as a goodbye DA with no slots at all, to complex ones, such as an inform DA containing multiple slots with various types of values (see example in Table 1). Earlier work on statistical NLG approaches were typically hybrids of a handcrafted component and a statistical training method (Langkilde and Knight, 1998; Stent et al., 2004; Rieser and Lemon, 2010). The handcrafted aspects, however, lead to decreased portability and potentially limit the variability of the outputs. New corpusbased approaches emerged that used semantically aligned data to train language models that output utterances directly from their MRs (Mairesse et al., 2010; Mairesse and Young, 2014). The alignment provides valuable information during training, but the semantic annotation is costly. The most recent methods do not require aligned data and use an end-to-end approach to training, performing sentence planning and surface real"
N18-1014,W07-0734,0,0.0617952,"r the number of sentences used to convey all the information in the corresponding MR, as longer sentences tend to exhibit more advanced discourse phenomena. Penalizing utterances for too many sentences contributes to reducing the proportion of generic reference utter157 ances, such as the “simple” example in the above table, in the filtered training set. 6 BLEU NIST METEOR ROUGE Evaluation Researchers in NLG have generally used both automatic and human evaluation. Our results report the standard automatic evaluation metrics: BLEU (Papineni et al., 2002), NIST (Przybocki et al., 2009), METEOR (Lavie and Agarwal, 2007), and ROUGE-L (Lin, 2004). For the E2E dataset experiments, we additionally report the results of the human evaluation carried out on the CrowdFlower platform as a part of the E2E NLG Challenge. 6.1 s 0.6664 8.0150 0.4420 s 0.6930‡ 8.4198 0.4379 0.7062 0.7099 CNN s 0.6599 7.8520 0.4333 s 0.6760† 8.0440 0.4448 0.7018 0.7055 Table 6: Automatic metric scores of different models tested on the E2E dataset, both unmodified (s) and augmented (s) through the utterance splitting. The symbols † and ‡ indicate statistically significant improvement over the s counterpart with p &lt; 0.05 and p &lt; 0.01, respec"
N18-1014,W14-3301,0,0.0715868,"Missing"
N18-1014,P04-1011,1,0.781506,"the human user during the dialogue. Each piece of information is represented by a slotvalue pair, where the slot identifies the type of information and the value is the corresponding content. Dialogue act (DA) types vary depending on the dialogue manager, ranging from simple ones, such as a goodbye DA with no slots at all, to complex ones, such as an inform DA containing multiple slots with various types of values (see example in Table 1). Earlier work on statistical NLG approaches were typically hybrids of a handcrafted component and a statistical training method (Langkilde and Knight, 1998; Stent et al., 2004; Rieser and Lemon, 2010). The handcrafted aspects, however, lead to decreased portability and potentially limit the variability of the outputs. New corpusbased approaches emerged that used semantically aligned data to train language models that output utterances directly from their MRs (Mairesse et al., 2010; Mairesse and Young, 2014). The alignment provides valuable information during training, but the semantic annotation is costly. The most recent methods do not require aligned data and use an end-to-end approach to training, performing sentence planning and surface realization simultaneous"
N18-1014,W15-4639,0,0.290826,"Missing"
N18-1014,N16-1015,0,0.380458,"Missing"
N18-1014,D15-1199,0,0.56038,"Missing"
N18-1014,E09-1078,0,\N,Missing
N18-1014,C98-1112,0,\N,Missing
P00-1024,W98-1411,0,\N,Missing
P00-1024,J97-1007,0,\N,Missing
P00-1024,A00-2003,0,\N,Missing
P00-1024,P95-1018,0,\N,Missing
P00-1024,J86-3001,0,\N,Missing
P00-1024,P98-1052,1,\N,Missing
P00-1024,C98-1051,1,\N,Missing
P00-1024,poesio-2000-annotating,0,\N,Missing
P00-1024,J98-3006,0,\N,Missing
P00-1024,J96-2005,1,\N,Missing
P01-1056,P98-1116,0,0.383749,"eering studies of template development and maintenance, this claim is supported by abundant anecdotal evidence. independent. However, the quality of the output for a particular domain, or a particular situation in a dialog, may be inferior to that of a templatebased system without considerable investment in domain-specific rules or domain-tuning of general rules. Furthermore, since rule-based systems use sophisticated linguistic representations, this handcrafting requires linguistic knowledge. Recently, several approaches for automatically training modules of an NLG system have been proposed (Langkilde and Knight, 1998; Mellish et al., 1998; Walker, 2000). These hold the promise that the complex step of customizing NLG systems by hand can be automated, while avoiding the need for tedious hand-crafting of templates. While the engineering benefits of trainable approaches appear obvious, it is unclear whether the utterance quality is high enough. In (Walker et al., 2001) we propose a new model of sentence planning called SP OT. In SP OT, the sentence planner is automatically trained, using feedback from two human judges, to choose the best from among different options for realizing a set of communicative goals"
P01-1056,A97-1039,0,0.530528,"Missing"
P01-1056,J97-1004,0,0.0268803,"ve rule-based sentence planners, and performs as well as the hand-crafted TEMPLATE system, but is more easily and quickly tuned to a new domain: the training materials for the SP OT sentence planner can be collected from subjective judgements from a small number of judges with little or no linguistic knowledge. Previous work on evaluation of natural language generation has utilized three different approaches to evaluation (Mellish and Dale, 1998). The first approach is a subjective evaluation methodology such as we use here, where human subjects rate NLG outputs produced by different sources (Lester and Porter, 1997). Other work has evaluated template-based spoken dialog generation with a task-based approach, i.e. the generator is evaluated with a metric such as task completion or user satisfaction after dialog completion (Walker, 2000). This approach can work well when the task only involves one or two exchanges, when the choices have large effects over the whole dialog, or the choices vary the content of the utterance. Because sentence planning choices realize the same content and only affect the current utterance, we believed it important to get local feedback. A final approach focuses on subproblems o"
P01-1056,W98-1411,0,0.169868,"Missing"
P01-1056,A92-1006,1,0.840926,"e little or no linguistic training is needed to write templates, it is a tedious and time-consuming task: one or more templates must be written for each combination of goals and discourse contexts, and linguistic issues such as subject-verb agreement and determiner-noun agreement must be repeatedly encoded for each template. Furthermore, maintenance of the collection of templates becomes a software engineering problem as the complexity of the dialog system increases.1 The second approach is natural language generation (NLG), which customarily divides the generation process into three modules (Rambow and Korelsky, 1992): (1) Text Planning, (2) Sentence Planning, and (3) Surface Realization. In this paper, we discuss only sentence planning; the role of the sentence planner is to choose abstract lexico-structural resources for a text plan, where a text plan encodes the communicative goals for an utterance (and, sometimes, their rhetorical structure). In general, NLG promises portability across application domains and dialog situations by focusing on the development of rules for each generation module that are general and domain1 Although we are not aware of any software engineering studies of template developm"
P01-1056,N01-1003,1,0.927935,"es. Furthermore, since rule-based systems use sophisticated linguistic representations, this handcrafting requires linguistic knowledge. Recently, several approaches for automatically training modules of an NLG system have been proposed (Langkilde and Knight, 1998; Mellish et al., 1998; Walker, 2000). These hold the promise that the complex step of customizing NLG systems by hand can be automated, while avoiding the need for tedious hand-crafting of templates. While the engineering benefits of trainable approaches appear obvious, it is unclear whether the utterance quality is high enough. In (Walker et al., 2001) we propose a new model of sentence planning called SP OT. In SP OT, the sentence planner is automatically trained, using feedback from two human judges, to choose the best from among different options for realizing a set of communicative goals. In (Walker et al., 2001), we evaluate the performance of the learning component of SP OT, and show that SP OT learns to select sentence plans that are highly rated by the two human judges. While this evaluation shows that SP OT has indeed learned from the human judges, it does not show that using only two human judgments is sufficient to produce more b"
P01-1056,J97-1007,0,0.0160307,"completion (Walker, 2000). This approach can work well when the task only involves one or two exchanges, when the choices have large effects over the whole dialog, or the choices vary the content of the utterance. Because sentence planning choices realize the same content and only affect the current utterance, we believed it important to get local feedback. A final approach focuses on subproblems of natural language generation such as the generation of referring expressions. For this type of problem it is possible to evaluate the generator by the degree to which it matches human performance (Yeh and Mellish, 1997). When evaluating sentence planning, this approach doesn’t make sense because many different realizations may be equally good. However, this experiment did not show that trainable sentence planners produce, in general, better-quality output than template-based or rulebased sentence planners. That would be impossible: given the nature of template and rulebased systems, any quality standard for the output can be met given sufficient person-hours, elapsed time, and software engineering acumen. Our principal goal, rather, is to show that the quality of the TEMPLATE output, for a currently operatio"
P01-1056,C98-1112,0,\N,Missing
P01-1056,W98-1415,0,\N,Missing
P01-1056,P97-1026,0,\N,Missing
P01-1066,H92-1006,0,0.0575335,"Missing"
P01-1066,H01-1015,1,0.596513,"ask an utterance contributes to separately from what speech act function it serves. Thus, a central aspect of DATE is that it makes distinctions within three orthogonal dimensions of utterance classification: (1) a SPEECH - ACT dimension; (2) a TASK - SUBTASK dimension; and (3) a CONVERSATIONAL - DOMAIN dimension. We believe that these distinctions are important for using such a scheme for evaluation. Figure 1 shows a COMMUNICATOR dialogue with each system utterance classified on these three dimensions. The tagset for each dimension are briefly described in the remainder of this section. See (Walker and Passonneau, 2001) for more detail. 3.1 Speech Acts In DATE, the SPEECH - ACT dimension has ten categories. We use familiar speech-act labels, such as OFFER, REQUEST- INFO, PRESENT- INFO, AC KNOWLEDGE, and introduce new ones designed to help us capture generalizations about communicative behavior in this domain, on this task, given the range of system and human behavior we see in the data. One new one, for example, is STATUS - REPORT. Examples of each speech-act type are in Figure 2. Speech-Act REQUEST- INFO PRESENT- INFO OFFER ACKNOWLEDGE STATUS - REPORT EXPLICITCONFIRM IMPLICITCONFIRM INSTRUCTION APOLOGY OPEN"
P01-1066,C92-1054,1,0.577698,"n utterance is about. Each speech act can occur in any of three domains of discourse described below. The ABOUT- TASK domain is necessary for evaluating a dialogue system’s ability to collaborate with a speaker on achieving the task goal of making reservations for a specific trip. It supports metrics such as the amount of time/effort the system takes to complete a particular phase of making an airline reservation, and any ancillary hotel/car reservations. The ABOUT- COMMUNICATION domain reflects the system goal of managing the verbal channel and providing evidence of what has been understood (Walker, 1992; Clark and Schaefer, 1989). Utterances of this type are frequent in human-computer dialogue, where they are motivated by the need to avoid potentially costly errors arising from imperfect speech recognition. All implicit and explicit confirmations are about communication; See Figure 1 for examples. The SITUATION - FRAME domain pertains to the goal of managing the culturally relevant framing expectations (Goffman, 1974). The utterances in this domain are particularly relevant in humancomputer dialogues because the users’ expectations need to be defined during the course of the conversation. Ab"
P01-1066,P95-1016,0,\N,Missing
P01-1066,P98-1052,0,\N,Missing
P01-1066,C98-1051,0,\N,Missing
P02-1048,C00-1054,1,0.503361,"n the user has hit the click-to-speak button, when a speech result arrives, and whether or not the user is inking on the display. When a speech lattice arrives, if inking is in progress MMFST waits for the ink meaning lattice, otherwise it applies a short timeout (1 sec.) and treats the speech as unimodal. When an ink meaning lattice arrives, if the user has tapped click-to-speak MMFST waits for the speech lattice to arrive, otherwise it applies a short timeout (1 sec.) and treats the ink as unimodal. MMFST uses the finite-state approach to multimodal integration and understanding proposed by Johnston and Bangalore (2000). Possibilities for multimodal integration and understanding are captured in a three tape device in which the first tape represents the speech stream (words), the second the ink stream (gesture symbols) and the third their combined meaning (meaning symbols). In essence, this device takes the speech and ink meaning lattices as inputs, consumes them using the first two tapes, and writes out a multimodal meaning lattice using the third tape. The three tape finite-state device is simulated using two transducers: G:W which is used to align speech and ink and G W:M which takes a composite alphabet o"
P02-1048,C00-1053,1,0.772153,"alues such as area, point, line, arrow. MEANING indicates the meaning of that form; for example an area can be either a loc(ation) or a sel(ection). NUMBER and TYPE indicate the number of entities in a selection (1,2,3, many) and their type (rest(aurant), theatre). SEM is a place holder for the specific content of the gesture, such as the points that make up an area or the identifiers of objects in a selection. When multiple selection gestures are present an aggregation technique (Johnston and Bangalore, 2001) is employed to overcome the problems with deictic plurals and numerals described in Johnston (2000). Aggregation augments the ink meaning lattice with aggregate gestures that result from combining adjacent selection gestures. This allows a deictic expression like these three restaurants to combine with two area gestures, one which selects one restaurant and the other two, as long as their sum is three. For example, if the user makes two area gestures, one around a single restaurant and the other around two restaurants (Figure 3), the resulting ink meaning lattice will be as in Figure 8. The first gesture (node numbers 0-7) is either a reference to a location (loc.) (0-3,7) or a reference to"
P02-1048,1997.iwpt-1.19,0,0.0197684,"e the corresponding I symbol is the specific interpretation. After multimodal integration a projection G:M is taken from the result G W:M machine and composed with the original I:G in order to reincorporate the specific contents that were left out of the finite-state process (I:G o G:M = I:M). The multimodal finite-state transducers used at runtime are compiled from a declarative multimodal context-free grammar which captures the structure Figure 8: Ink Meaning Lattice and interpretation of multimodal and unimodal commands, approximated where necessary using standard approximation techniques (Nederhof, 1997). This grammar captures not just multimodal integration patterns but also the parsing of speech and gesture, and the assignment of meaning. In Figure 9 we present a small simplified fragment capable of handling MATCH commands such as phone numbers for these three restaurants. A multimodal CFG differs from a normal CFG in that the terminals are triples: W:G:M, where W is the speech stream (words), G the ink stream (gesture symbols) and M the meaning stream (meaning symbols). An XML representation for meaning is used to facilate parsing and logging by other system components. The meaning tape sy"
P02-1048,P99-1024,1,0.628273,"ir specific contents in I:G (I:G o G:M = I:M). The meaning read off I:M is <cmd> <phone> <restaurant> [id1,id2,id3] </restaurant> </phone> </cmd>. This is passed to the multimodal dialog manager (MDM) and from there to the Multimodal UI resulting in a display like Figure 4 with coordinated TTS output. Since the speech input is a lattice and there is also potential for ambiguity in the multimodal grammar, the output from MMFST to MDM is an N-best list of potential multimodal interpretations. Multimodal Dialog Manager (MDM) The MDM is based on previous work on speech-act based models of dialog (Stent et al., 1999; Rich and Sidner, 1998). It uses a Java-based toolkit for writing dialog managers that is similar in philosophy to TrindiKit (Larsson et al., 1999). It includes several rule-based S CMD DEICTICNP DDETPL RESTPL NUM !! ! !!! < > < > eps:eps: cmd CMD eps:eps: /cmd phone:eps: phone numbers:eps:eps for:eps:eps DEICTICNP eps:eps: /phone DDETPL eps:area:eps eps:selection:eps NUM RESTPL eps:eps: restaurant eps:SEM:SEM eps:eps: /restaurant these:G:eps restaurants:restaurant:eps three:3:eps < < > > < < > > Figure 9: Multimodal grammar fragment processes that operate on a shared state. The state include"
P02-1048,W02-2110,0,0.226817,"e restaurants in Figure 3 and writes phone, the system responds with a graphical callout on the display, synchronized with a text-to-speech (TTS) prompt of the phone number, for each restaurant in turn (Figure 4). Figure 3: Two area gestures Figure 4: Phone query callouts The system also provides subway directions. If the user says How do I get to this place? and circles one Figure 5: Multimodal subway route User-tailored generation MATCH can also provide a user-tailored summary, comparison, or recommendation for an arbitrary set of restaurants, using a quantitative model of user preferences (Walker et al., 2002). The system will only discuss restaurants that rank highly according to the user’s dining preferences, and will only describe attributes of those restaurants the user considers important. This permits concise, targeted system responses. For example, the user could say compare these restaurants and circle a large set of restaurants (Figure 6). If the user considers inexpensiveness and food quality to be the most important attributes of a restaurant, the system response might be: Compare-A: Among the selected restaurants, the following offer exceptional overall value. Uguale’s price is 33 dolla"
P02-1048,P00-1020,0,\N,Missing
P02-1049,H01-1028,0,0.0216091,"Missing"
P02-1049,P98-1122,0,0.0638596,"Missing"
P02-1049,P99-1040,1,0.891002,"Missing"
P02-1049,W02-0221,1,0.829319,"us labelling was done because we found that systems had augmented their inventory of named entities and utterance patterns from 2000 to 2001, and these were not accounted for by the 2000 tagger database. For the extension, we collected a fresh set of vocabulary lists from the sites and augmented the pattern database with additional 800 labelled utterance patterns. We also implemented a contextual rule-based postprocessor that takes any remaining unlabelled utterances and attempts to label them by looking at their surrounding DATE labels. More details about the extended tagger can be found in (Prasad and Walker, 2002). On the 2001 corpus, we were able to label 98.4 of the data. A hand evaluation of 10 randomly selected dialogues from each system shows that we achieved a classification accuracy of 96 at the utterance level. For User Satisfaction Prediction, we found that the distribution of DATE acts were better captured by using the frequency normalized over the total number of dialogue acts. In addition to these unigram proportions, the bigram frequencies of the DATE dialogue acts were also calculated. In the following two sections, we discuss which DATE labels are discriminatory for predicting Task Compl"
P02-1049,P01-1066,1,0.851463,"ialogues that provide training data for further system development. As a spoken dialogue system is developed, it is first tested as a prototype, then fielded in a limited setting, possibly running with human supervision (Gorin et al., 1997), and finally deployed. At each stage from research prototype to deployed commercial application, the system is constantly undergoing further development. When a system is prototyped in house or first tested in the field, human subjects are often paid to use the system and give detailed feedback on task completion and user satisfaction (Baggia et al., 1998; Walker et al., 2001). Even when a system is deployed, it often keeps evolving, either because customers want to do different things with it, or because new tasks arise out of developments in the underlying application. However, real customers of a deployed system may not be willing to give detailed feedback. Thus, the widespread use of these systems has created a data management and analysis problem. System designers need to constantly track system performance, identify problems, and fix them. System modules such as automatic speech recognition (ASR), natural language understanding (NLU) and dialogue management m"
P02-1049,C98-1117,0,\N,Missing
P04-1011,A00-2023,0,0.0311426,"es portability across domains and dialog contexts by using general rules for each generation module. However, the quality of the output for a particular domain, or a particular dialog context, may be inferior to that of a templatebased system unless domain-specific rules are developed or general rules are tuned for the particular domain. Furthermore, full NLG may be too slow for use in dialog systems. A third, more recent, approach is trainable generation: techniques for automatically training NLG modules, or hybrid techniques that adapt NLG modules to particular domains or user groups, e.g. (Langkilde, 2000; Mellish, 1998; Walker, Rambow and Rogati, 2002). Open questions about the trainable approach include (1) whether the output quality is high enough, and (2) whether the techniques work well across domains. For example, the training method used in SPoT (Sentence Planner Trainable), as described in (Walker, Rambow and Rogati, 2002), was only shown to work in the travel domain, for the information gathering phase of the dialog, and with simple content plans involving no rhetorical relations. This paper describes trainable sentence planning for information presentation in the MATCH (Multimodal Ac"
P04-1011,P02-1048,1,0.318096,"d Rogati, 2002). Open questions about the trainable approach include (1) whether the output quality is high enough, and (2) whether the techniques work well across domains. For example, the training method used in SPoT (Sentence Planner Trainable), as described in (Walker, Rambow and Rogati, 2002), was only shown to work in the travel domain, for the information gathering phase of the dialog, and with simple content plans involving no rhetorical relations. This paper describes trainable sentence planning for information presentation in the MATCH (Multimodal Access To City Help) dialog system (Johnston et al., 2002). We provide evidence that the trainable approach is feasible by showing (1) that the training technique used for SPoT can be extended to a new domain (restaurant information); (2) that this technique, previously used for informationgathering utterances, can be used for information presentations, namely recommendations and comparisons; and (3) that the quality of the output is comparable to that of a template-based generator previously developed and experimentally evaluated with MATCH users (Walker et al., 2002; Stent et al., 2002). Section 2 describes SPaRKy (Sentence Planning with Rhetorical"
P04-1011,A97-1039,0,0.0665551,"ts a set of text plan trees (tp-trees), consisting of a set of speech acts to be communicated and the rhetorical relations that hold between them. For example, the two tp-trees in Figure 6 are generated for the content plan in Figure 2. Sentence plans such as alternative 25 in Figure 4 are avoided; it is clearly worse than alternatives 12, 13 and 20 since it neither combines information based on a restaurant entity (e.g Babbo) nor on an attribute (e.g. decor). The top ranked sentence plan output by the SPR is input to the RealPro surface realizer which produces a surface linguistic utterance (Lavoie and Rambow, 1997). A prosody assignment module uses the prior levels of linguistic representation to determine the appropriate prosody for the utterance, and passes a markedup string to the text-to-speech module. Sentence Plan Generation As in SPoT, the basis of the SPG is a set of clause-combining operations that operate on tptrees and incrementally transform the elementary predicate-argument lexico-structural representations (called DSyntS (Melcuk, 1988)) associated with the speech-acts on the leaves of the tree. The operations are applied in a bottom-up left-to-right fashion and the resulting representation"
P04-1011,W98-1411,0,0.0360157,"Missing"
P04-1011,A92-1006,0,0.089344,"tructuring phase, called infer, which holds for combinations of speech acts for which there is no rhetorical relation expressed in the content plan, as in (Marcu, 1997). By explicitly representing the discourse structure of the information presentation, we can generate information presentations with considerably more internal complexity than those generated in (Walker, Rambow and Rogati, 2002) and eliminate those that violate certain coherence principles, as described in Section 2. The clause-combining operations are general operations similar to aggregation operations used in other research (Rambow and Korelsky, 1992; Danlos, 2000). The operations and the 1 Although the probability distribution here is handcrafted based on assumed preferences for operations such as merge, relative-clause and with-reduction, it might also be possible to learn this probability distribution from the data by training in two phases. elaboration infer nucleus:&lt;1&gt;assert-com-list_exceptional contrast contrast nucleus:&lt;2&gt;assert-com-decor nucleus:&lt;4&gt;assert-com-service nucleus:&lt;3&gt;assert-com-decor contrast nucleus:&lt;6&gt;assert-com-cuisine nucleus:&lt;5&gt;assert-com-service nucleus:&lt;7&gt;assert-com-cuisine elaboration nucleus:&lt;1&gt;assert-com-list_"
P04-1011,P01-1056,1,0.92581,"Missing"
P04-1011,W02-2110,0,0.123126,"mation presentation in the MATCH (Multimodal Access To City Help) dialog system (Johnston et al., 2002). We provide evidence that the trainable approach is feasible by showing (1) that the training technique used for SPoT can be extended to a new domain (restaurant information); (2) that this technique, previously used for informationgathering utterances, can be used for information presentations, namely recommendations and comparisons; and (3) that the quality of the output is comparable to that of a template-based generator previously developed and experimentally evaluated with MATCH users (Walker et al., 2002; Stent et al., 2002). Section 2 describes SPaRKy (Sentence Planning with Rhetorical Knowledge), an extension of SPoT that uses rhetorical relations. SPaRKy consists of a randomized sentence plan generator (SPG) and a trainable sentence plan ranker (SPR); these are described in Sections 3 strategy:recommend items: Chanpen Thai relations:justify(nuc:1;sat:2); justify(nuc:1;sat:3); justify(nuc:1;sat:4) content: 1. assert(best(Chanpen Thai)) 2. assert(has-att(Chanpen Thai, decor(decent))) 3. assert(has-att(Chanpen Thai, service(good)) 4. assert(has-att(Chanpen Thai, cuisine(Thai))) Figure 1: A co"
P04-1011,P87-1022,0,\N,Missing
P06-1034,W02-1022,0,0.0412888,"Missing"
P06-1034,W02-2103,0,\N,Missing
P06-1034,W98-1428,0,\N,Missing
P06-1034,J02-4007,0,\N,Missing
P06-1034,H05-1042,0,\N,Missing
P06-1034,W00-0306,0,\N,Missing
P06-1034,N06-1046,0,\N,Missing
P06-1034,C02-1138,1,\N,Missing
P06-1034,H05-2017,0,\N,Missing
P06-1034,H05-1043,0,\N,Missing
P06-1034,H92-1022,0,\N,Missing
P06-1034,A92-1021,0,\N,Missing
P06-1034,P01-1008,0,\N,Missing
P06-1034,P06-2059,0,\N,Missing
P06-1034,P07-1063,1,\N,Missing
P06-1034,N03-1003,0,\N,Missing
P06-1034,P01-1056,1,\N,Missing
P06-1034,P04-1011,1,\N,Missing
P06-1034,J02-3001,0,\N,Missing
P06-1034,P02-1053,0,\N,Missing
P06-1034,P05-1015,0,\N,Missing
P06-1034,P87-1023,0,\N,Missing
P06-1034,W04-2302,0,\N,Missing
P06-1034,N04-1041,0,\N,Missing
P06-1034,H01-1047,0,\N,Missing
P06-1034,A97-1039,0,\N,Missing
P06-1034,W06-1650,0,\N,Missing
P07-1063,W00-1407,0,0.0219229,"cy to put information into perspective, the CONCESSION POLARITY parameter controls whether the positive or the negative content is concessed, i.e. marked as the satellite of the CON CESS relation. The last sentence of Alt-3 in Table 1 illustrates a positive concession, in which the good food quality is put before the high price. Content ordering: Although extraverts use more positive language (Pennebaker and King, 1999; Thorne, 1987), it is unclear how they position the positive content within their utterances. Additionally, the position of the claim affects the persuasiveness of an argument (Carenini and Moore, 2000): starting with the claim facilitates the hearer’s understanding, while finishing with the claim is more effective if the hearer disagrees. The POSITIVE CON TENT FIRST parameter therefore controls whether positive content items – including the claim – appear first or last, and the order in which the content items are aggregated. However, some operations can still impose a specific ordering (e.g. BECAUSE cue word 499 to realize the JUSTIFY relation, see Section 4.2). 4 Sentence Planning Sentence planning chooses the linguistic resources from the lexicon and the syntactic and discourse structure"
P07-1063,W06-1405,0,0.39197,"” model of personality traits has become a standard in psychology (extraversion, neuroticism, agreeableness, conscientiousness, and openness to experience), and research has systematically documented correlations between a wide range of linguistic variables and the Big Five traits (Mehl et al., 2006; Norman, 1963; Oberlander and Gill, 2006; Pennebaker and King, 1999). A distinct line of research has explored methods for automatically generating language that varies along personality dimensions, targeting applications such as computer gaming and educational virtual worlds (Andr´e et al., 2000; Isard et al., 2006; Loyall and Bates, 1997; Piwek, 2003; Walker et al., 1997) inter 496 Marilyn Walker Department of Computer Science University of Sheffield Sheffield, S1 4DP, United Kingdom M.A.Walker@sheffield.ac.uk alia. Other work suggests a clear utility for generating language manifesting personality (Reeves and Nass, 1996). However, to date, (1) research in generation has not systematically exploited the psycholinguistic findings; and (2) there has been little evaluation showing that automatic generators can produce language with recognizable personality variation. Alt Realization 5 Err... it seems to m"
P07-1063,A97-1039,0,0.0514074,"ct and structure content; 2. Sentence planning; choose linguistic resources (lexicon, syntax) to achieve goals; 3. Realization: use grammar (syntax, morphology) to generate surface utterances. Given the NLG architecture, speech-act types, and domain, the first step then is to summarise psychological findings on extraversion and map them to this architecture. The column NLG modules of Table 2 gives the proposed mapping. The first row specifies findings for the content planning module and the other rows are aspects of sentence planning. Realization is achieved with the RealPro surface realizer (Lavoie and Rambow, 1997). An examination of the introvert and extravert findings in Table 2 highlights the challenges above, i.e. exploiting these findings in a systematic way within a parameterizable NLG system. The column Parameter in Table 2 proposes parameters (explained in Sections 3 and 4) that are manipulated within each module to realize the findings in the other columns. Each parameter varies continuously from 0 to 1, where end points are meant to produce extreme but plausible output. Given the challenges above, it is important to note that these parameters represent hypotheses about how a finding can be map"
P07-1063,P05-1008,0,0.036361,"Missing"
P07-1063,E03-1019,0,0.403487,"tandard in psychology (extraversion, neuroticism, agreeableness, conscientiousness, and openness to experience), and research has systematically documented correlations between a wide range of linguistic variables and the Big Five traits (Mehl et al., 2006; Norman, 1963; Oberlander and Gill, 2006; Pennebaker and King, 1999). A distinct line of research has explored methods for automatically generating language that varies along personality dimensions, targeting applications such as computer gaming and educational virtual worlds (Andr´e et al., 2000; Isard et al., 2006; Loyall and Bates, 1997; Piwek, 2003; Walker et al., 1997) inter 496 Marilyn Walker Department of Computer Science University of Sheffield Sheffield, S1 4DP, United Kingdom M.A.Walker@sheffield.ac.uk alia. Other work suggests a clear utility for generating language manifesting personality (Reeves and Nass, 1996). However, to date, (1) research in generation has not systematically exploited the psycholinguistic findings; and (2) there has been little evaluation showing that automatic generators can produce language with recognizable personality variation. Alt Realization 5 Err... it seems to me that Le Marais isn’t as bad as the"
P07-1063,P04-1011,1,0.153858,"the claim of Alt-2 in Table 1, i.e. I am sure you would like Le Marais, will be rated higher than Le Marais isn’t as bad as the others in Alt-5. Polarity: While polarity can be expressed by content selection and structure, it can also be directly associated with the DSyntS. The CLAIM POLARITY parameter determines the DSyntS selected to realize the claim. DSyntS are manually annotated for polarity. For example, Alt-4’s claim in Table 1, i.e. Le Marais is the only restaurant that is any good, has a lower polarity than Alt-2. 4.2 Aggregation operations SPaRKy aggregation operations are used (See Stent et al. (2004)), with additional operations for concessions and restatements. See Table 2. The probability of the operations biases the production of complex clauses, periods and formal cue words for introverts, to express their preference for complex syntactic constructions, long pauses and rich vocabulary (Furnham, 1990). Thus, the introvert parameters favor operations such as RELATIVE CLAUSE for the INFER relation, PERIOD HOWEVER CUE WORD for CONTRAST , and ALTHOUGH ADVERBIAL CLAUSE for CONCESS, that we hypothesize to result in more formal language. Extravert aggregation produces longer sentences with si"
P07-1063,E03-1062,0,\N,Missing
P08-1020,C00-1007,0,0.0379453,"cal natural language generation (SNLG), showing that high quality paraphrases can be generated from an underspecified representation of meaning, by first applying a very underconstrained, rule-based overgeneration phase, whose outputs are then ranked by an SLM scoring phase. Since then, research in SNLG has explored a range of models for both dialogue and text generation. One line of work has primarily focused on grammaticality and naturalness, scoring the overgeneration phase with a SLM, and evaluating against a gold-standard corpus, using string or tree-match metrics (Langkilde-Geary, 2002; Bangalore and Rambow, 2000; Chambers and Allen, 2004; Belz, 2005; Isard et al., 2006). Another thread investigates SNLG scoring models trained using higher-level linguistic features to replicate human judgments of utterance quality (Rambow et al., 2001; Nakatsu and White, 2006; Stent and Guo, 2005). The error of these scoring models approaches the gold-standard human ranking with a relatively small training set. A third SNLG approach eliminates the overgeneration phase (Paiva and Evans, 2005). It applies factor analysis to a corpus exhibiting stylistic variation, and then learns which generation parameters to manipulat"
P08-1020,P05-1008,0,0.19336,"phase with a SLM, and evaluating against a gold-standard corpus, using string or tree-match metrics (Langkilde-Geary, 2002; Bangalore and Rambow, 2000; Chambers and Allen, 2004; Belz, 2005; Isard et al., 2006). Another thread investigates SNLG scoring models trained using higher-level linguistic features to replicate human judgments of utterance quality (Rambow et al., 2001; Nakatsu and White, 2006; Stent and Guo, 2005). The error of these scoring models approaches the gold-standard human ranking with a relatively small training set. A third SNLG approach eliminates the overgeneration phase (Paiva and Evans, 2005). It applies factor analysis to a corpus exhibiting stylistic variation, and then learns which generation parameters to manipulate to correlate with factor measurements. The generator was shown to reproduce intended factor levels across several factors, thus modelling the stylistic variation as measured in the original corpus. Our goal is a generation technique that can target multiple stylistic effects simultaneously and over a continuous scale, controlling stylistic dimensions that are commonly understood and thus meaningful to users and application developers. Our intended applications are"
P08-1020,W04-2302,0,0.072549,"ion (SNLG), showing that high quality paraphrases can be generated from an underspecified representation of meaning, by first applying a very underconstrained, rule-based overgeneration phase, whose outputs are then ranked by an SLM scoring phase. Since then, research in SNLG has explored a range of models for both dialogue and text generation. One line of work has primarily focused on grammaticality and naturalness, scoring the overgeneration phase with a SLM, and evaluating against a gold-standard corpus, using string or tree-match metrics (Langkilde-Geary, 2002; Bangalore and Rambow, 2000; Chambers and Allen, 2004; Belz, 2005; Isard et al., 2006). Another thread investigates SNLG scoring models trained using higher-level linguistic features to replicate human judgments of utterance quality (Rambow et al., 2001; Nakatsu and White, 2006; Stent and Guo, 2005). The error of these scoring models approaches the gold-standard human ranking with a relatively small training set. A third SNLG approach eliminates the overgeneration phase (Paiva and Evans, 2005). It applies factor analysis to a corpus exhibiting stylistic variation, and then learns which generation parameters to manipulate to correlate with factor"
P08-1020,W06-1405,0,0.169295,"paraphrases can be generated from an underspecified representation of meaning, by first applying a very underconstrained, rule-based overgeneration phase, whose outputs are then ranked by an SLM scoring phase. Since then, research in SNLG has explored a range of models for both dialogue and text generation. One line of work has primarily focused on grammaticality and naturalness, scoring the overgeneration phase with a SLM, and evaluating against a gold-standard corpus, using string or tree-match metrics (Langkilde-Geary, 2002; Bangalore and Rambow, 2000; Chambers and Allen, 2004; Belz, 2005; Isard et al., 2006). Another thread investigates SNLG scoring models trained using higher-level linguistic features to replicate human judgments of utterance quality (Rambow et al., 2001; Nakatsu and White, 2006; Stent and Guo, 2005). The error of these scoring models approaches the gold-standard human ranking with a relatively small training set. A third SNLG approach eliminates the overgeneration phase (Paiva and Evans, 2005). It applies factor analysis to a corpus exhibiting stylistic variation, and then learns which generation parameters to manipulate to correlate with factor measurements. The generator was"
P08-1020,P98-1116,0,0.142079,"recognisable variation along a meaningful stylistic dimension— personality—without the computational cost incurred by overgeneration techniques. We present the first evaluation of a data-driven generation method that projects multiple personality traits simultaneously and on a continuous scale. We compare our performance to a rule-based generator in the same domain. 1 Introduction Over the last 20 years, statistical language models (SLMs) have been used successfully in many tasks in natural language processing, and the data available for modeling has steadily grown (Lapata and Keller, 2005). Langkilde and Knight (1998) first applied SLMs to statistical natural language generation (SNLG), showing that high quality paraphrases can be generated from an underspecified representation of meaning, by first applying a very underconstrained, rule-based overgeneration phase, whose outputs are then ranked by an SLM scoring phase. Since then, research in SNLG has explored a range of models for both dialogue and text generation. One line of work has primarily focused on grammaticality and naturalness, scoring the overgeneration phase with a SLM, and evaluating against a gold-standard corpus, using string or tree-match m"
P08-1020,W02-2103,0,0.0163455,"pplied SLMs to statistical natural language generation (SNLG), showing that high quality paraphrases can be generated from an underspecified representation of meaning, by first applying a very underconstrained, rule-based overgeneration phase, whose outputs are then ranked by an SLM scoring phase. Since then, research in SNLG has explored a range of models for both dialogue and text generation. One line of work has primarily focused on grammaticality and naturalness, scoring the overgeneration phase with a SLM, and evaluating against a gold-standard corpus, using string or tree-match metrics (Langkilde-Geary, 2002; Bangalore and Rambow, 2000; Chambers and Allen, 2004; Belz, 2005; Isard et al., 2006). Another thread investigates SNLG scoring models trained using higher-level linguistic features to replicate human judgments of utterance quality (Rambow et al., 2001; Nakatsu and White, 2006; Stent and Guo, 2005). The error of these scoring models approaches the gold-standard human ranking with a relatively small training set. A third SNLG approach eliminates the overgeneration phase (Paiva and Evans, 2005). It applies factor analysis to a corpus exhibiting stylistic variation, and then learns which genera"
P08-1020,P07-1063,1,0.283559,"cts simultaneously and over a continuous scale, controlling stylistic dimensions that are commonly understood and thus meaningful to users and application developers. Our intended applications are output utterances for intelligent training or intervention systems, video game characters, or virtual environment avatars. In previous work, we presented P ERSON AGE , a psychologically-informed rule-based generator based on the Big Five personality model, and we showed that P ERSONAGE can project extreme personality on the extraversion scale, i.e. both introverted and extraverted personality types (Mairesse and Walker, 2007). We used the Big Five model to develop P ERSONAGE for several reasons. First, the Big Five has been shown in psychology to ex165 Proceedings of ACL-08: HLT, pages 165–173, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Trait Extraversion Emotional stability Agreeableness High warm, assertive, sociable, excitement seeking, active, spontaneous, optimistic, talkative calm, even-tempered, reliable, peaceful, confident trustworthy, considerate, friendly, generous, helpful Conscientiousness Openness to experience competent, disciplined, dutiful, achievement strivin"
P08-1020,E03-1019,0,0.0185287,"ns differ significantly at the p < .05 level (two-tailed independent sample t-test). 4 Conclusion We present a new method for generating linguistic variation projecting multiple personality traits continuously, by combining and extending previous research in statistical natural language generation (Paiva and Evans, 2005; Rambow et al., 2001; Isard et al., 2006; Mairesse and Walker, 2007). While handcrafted rule-based approaches are limited to variation along a small number of discrete points (Hovy, 1988; Walker et al., 1997; Lester et al., 1997; Power et al., 2003; Cassell and Bickmore, 2003; Piwek, 2003; Mairesse and Walker, 2007; Rehm and Andr´e, in press), we learn models that predict parameter values for any arbitrary value on the variation dimension scales. Additionally, our data-driven approach can be applied to any dimension that is meaningful to human judges, and it provides an elegant way to project multiple dimensions simultaneously, by including the relevant dimensions as features of the parameter models’ training data. Isard et al. (2006) and Mairesse and Walker (2007) also propose a personality generation method, in which a data-driven personality model selects the best utterance"
P08-1020,P01-1056,1,0.919099,"s are then ranked by an SLM scoring phase. Since then, research in SNLG has explored a range of models for both dialogue and text generation. One line of work has primarily focused on grammaticality and naturalness, scoring the overgeneration phase with a SLM, and evaluating against a gold-standard corpus, using string or tree-match metrics (Langkilde-Geary, 2002; Bangalore and Rambow, 2000; Chambers and Allen, 2004; Belz, 2005; Isard et al., 2006). Another thread investigates SNLG scoring models trained using higher-level linguistic features to replicate human judgments of utterance quality (Rambow et al., 2001; Nakatsu and White, 2006; Stent and Guo, 2005). The error of these scoring models approaches the gold-standard human ranking with a relatively small training set. A third SNLG approach eliminates the overgeneration phase (Paiva and Evans, 2005). It applies factor analysis to a corpus exhibiting stylistic variation, and then learns which generation parameters to manipulate to correlate with factor measurements. The generator was shown to reproduce intended factor levels across several factors, thus modelling the stylistic variation as measured in the original corpus. Our goal is a generation t"
P08-1020,C98-1112,0,\N,Missing
P08-1020,E03-1062,0,\N,Missing
P08-1020,P06-2081,0,\N,Missing
P08-1020,P06-1140,0,\N,Missing
P08-1055,P06-4015,0,0.0354687,"Missing"
P08-1055,W00-1407,0,0.42472,"ensional summaries in cruiser (Cooperative Responses Using Intensional Summaries of Entities and Relations), a DS for in-car or mobile users to access restaurant information (Becker et al.2006; Weng et al.2005; Weng et al.2006). Figure 1 contrasts our proposed intensional summary strategy with the system initiative strategy used in many dialogue systems (Walker et al., 2002; VXML, 2007). Previous research on cooperative responses has noted that summary strategies should vary according to the context (Sparck Jones, 1993), and the interests and preferences of the user (Gaasterland et al., 1992; Carenini and Moore, 2000; Demberg and Moore, 2006). A number of proposals have emphasized the importance of making generalizations (Kaplan, 1984; Kalita et al., 1986; Joshi et al., 1986). In this paper we explore different methods for constructing intensional summaries and investigate their effectiveness. We present fully automated algorithms for constructing intensional summaries using knowledge discovery techniques (Acar, 2005; Lesh and Mitzenmacher, 2004; Han et al., 1996), and decisiontheoretic user models (Carenini and Moore, 2000). We first explain in Sec. 2 our fully automated, domain-independent algorithm for"
P08-1055,H89-1037,0,0.402463,"Missing"
P08-1055,E06-1009,0,0.582204,"eneration has had a long history and produced many useful algorithms and observations (Mays 1980; Pollack et al.1982; Joshi et al., 1986; Kalita et al., 1986) inter alia. However, it has had little impact on the recent commercialization of dialogue technologies, particularly within the spoken dialogue community. We believe that this lack of interest in cooperative response generation arises from two limitations of previous work: (1) There has been relatively little empirical research showing that cooperative responses lead to more natural, effective, or efficient dialogues (Litman et al.1998; Demberg and Moore, 2006); and (2) Previous work has hand-crafted such responses, or hand-annotated the database to support them (Kaplan, 1984; Kalita et al., 1986; Cholvy, 1990; Polifroni et al., 2003; Benamara, 2004), which has made it difficult to port and scale these algorithms. Moreover, we believe that there is an even greater need today for cooperative response generation. Larger and more complex datasets are daily being created on the Web, as information 479 Proceedings of ACL-08: HLT, pages 479–487, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics is integrated across multiple"
P08-1055,H86-1017,0,0.571676,"Missing"
P08-1055,P84-1088,0,0.383766,"Missing"
P08-1055,P98-2129,1,0.779067,"Missing"
P08-1055,C98-2124,1,\N,Missing
P15-1012,W11-0702,1,0.971956,"UMS and C REATE D EBATE . Also in the online debate setting, Hasan and Ng (2014) show the benefits of joint modeling to classify post-level stance and the authors’ reasons for their stances. In contrast, in this work we focus on the dependencies between stance and polarity of replies. polarity for improvements in stance classification (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Burfoot et al., 2011). However, these methods rely heavily on features specific to the congressional setting in order to predict link polarity, and make little use of textual features. In contrast, Abbott et al. (2011) use a range of linguistic features from the text of posts and their parents to classify agreement or disagreement between posts on the online debate website 4F ORUMS . COM, without the goal of classifying stance. In this work, we study datasets from two online debate websites: 4F ORUMS . COM, from the Internet Argument Corpus (Walker et al., 2012b), and C REATE D EBATE . COM (Hasan and Ng, 2013). Table 1 shows statistics about these datasets including the average number of users per discussion topic and average number of posts authored. The best stance classification accuracy to date for onli"
P15-1012,P13-2144,0,0.0842621,"Missing"
P15-1012,C08-2004,0,0.0181248,"Missing"
P15-1012,W14-2107,0,0.127515,"Missing"
P15-1012,P11-1151,0,0.76138,"tive approaches that jointly predict user stance seem promising (Walker et al., 2012c; Hasan and Ng, 2013), the rich structure of online debate forums necessitates many modeling choices. For example, users publish opinions and reply and respond to each others’ posts. In so doing, they may agree or disagree with either all or a portion of another user’s post, suggesting that collective classifiers for stance may benefit from text-based disagreement modeling. Furthermore, one can model stance either at the author level— assuming that an author’s stance is based on all of their posts on a topic (Burfoot et al., 2011)—or at Introduction Understanding stance and opinion in dialogues can provide critical insight into the theoretical underpinnings of discourse, argumentation, and sentiment. Systems for predicting the stances of individuals can potentially have positive social impact and are of practical interest to non-profits, governmental organizations, and companies. For exam116 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 116–125, c Beijing, China, July 26-31, 2015. 2015 Association"
P15-1012,W14-2106,0,0.0523685,"Missing"
P15-1012,D14-1083,0,0.581271,"Missing"
P15-1012,C10-2100,0,0.43516,"Missing"
P15-1012,P09-1026,0,0.3332,"l debates. Posts are short and informal, there is limited external information about authors, and debate topics admit many modes of argumentation ranging from serious, to tangential, to sarcastic. The reply graph in online debates also has substantially different semantics to networks in other debate settings, such as the graph of speaker mentions in congressional debates. To illustrate this setting, Fig. 1 shows an example dialogue between two users who are debating their opinions on the topic of gun control. In the context of online debate forums, stance classification (Thomas et al., 2006; Somasundaran and Wiebe, 2009) is the task of assigning stance labels with respect to a discussion topic, either at the level of the user or the level of the post. Stance is typically treated as a binary classification problem, with labels PRO and ANTI. In Fig. 1, both users’ stances toward gun control are ANTI. Previous work on stance in online debates has shown that contextual information given by reply links is important for stance classification (Walker et al., 2012a), and that collective classification often outperforms methods which treat each post independently. Hasan and Ng (2013) use conditional random fields (CRF"
P15-1012,W10-0214,0,0.750525,"t 18 and use handguns and military weapons, but you cant purchase a handgun until 21. ANTI Figure 1: Example of a debate dialogue turn between two users on the gun control topic, from 4F ORUMS . COM. ple, stance predictions may be used to target public awareness and advocacy campaigns, direct political fundraising and get-out-the vote efforts, and improve personalized recommendations. Online debate websites are a particularly rich source of argumentative dialogic data (Fig. 1). On these websites, users debate and share their opinions on a variety of social and political issues. Previous work (Somasundaran and Wiebe, 2010; Walker et al., 2012c) has shown that stance classification in online debates is a challenging problem. While collective approaches that jointly predict user stance seem promising (Walker et al., 2012c; Hasan and Ng, 2013), the rich structure of online debate forums necessitates many modeling choices. For example, users publish opinions and reply and respond to each others’ posts. In so doing, they may agree or disagree with either all or a portion of another user’s post, suggesting that collective classifiers for stance may benefit from text-based disagreement modeling. Furthermore, one can"
P15-1012,W14-2715,1,0.800694,"em, with labels PRO and ANTI. In Fig. 1, both users’ stances toward gun control are ANTI. Previous work on stance in online debates has shown that contextual information given by reply links is important for stance classification (Walker et al., 2012a), and that collective classification often outperforms methods which treat each post independently. Hasan and Ng (2013) use conditional random fields (CRFs) to encourage opposite stances between sequences of posts, and Walker et al. (2012c) use MaxCut over explicitly given rebuttal links between posts to separate them into PRO and ANTI clusters. Sridhar et al. (2014) use hinge-loss Markov random fields (HL-MRFs) to encourage consistency between post level stance labels and observed post-level textual agreements and disagreements. Online Debate Forums Online debate forums represent richly structured argumentative dialogues. On these forums, users debate with each other in discussion threads on a While the first two approaches leverage rebuttal or reply links, they model reply links as being indicative of opposite stances. However, as shown in Fig. 1, responses—even rebuttals—can occur be1 PSL is an open-source Java toolkit, available here: http://psl.cs.um"
P15-1012,W06-1639,0,0.647419,"such as congressional debates. Posts are short and informal, there is limited external information about authors, and debate topics admit many modes of argumentation ranging from serious, to tangential, to sarcastic. The reply graph in online debates also has substantially different semantics to networks in other debate settings, such as the graph of speaker mentions in congressional debates. To illustrate this setting, Fig. 1 shows an example dialogue between two users who are debating their opinions on the topic of gun control. In the context of online debate forums, stance classification (Thomas et al., 2006; Somasundaran and Wiebe, 2009) is the task of assigning stance labels with respect to a discussion topic, either at the level of the user or the level of the post. Stance is typically treated as a binary classification problem, with labels PRO and ANTI. In Fig. 1, both users’ stances toward gun control are ANTI. Previous work on stance in online debates has shown that contextual information given by reply links is important for stance classification (Walker et al., 2012a), and that collective classification often outperforms methods which treat each post independently. Hasan and Ng (2013) use"
P15-1012,walker-etal-2012-corpus,1,0.912762,"Missing"
P15-1012,N12-1072,1,0.48263,"d Stance in Online Debate Dhanya Sridhar,1 James Foulds,1 Bert Huang,2 Lise Getoor,1 Marilyn Walker1 1 Department of Computer Science, University of California Santa Cruz {dsridhar, jfoulds, getoor, mawalker}@ucsc.edu 2 Department of Computer Science, Virginia Tech bhuang@vt.edu Abstract Online debate forums present a valuable opportunity for the understanding and modeling of dialogue. To understand these debates, a key challenge is inferring the stances of the participants, all of which are interrelated and dependent. While collectively modeling users’ stances has been shown to be effective (Walker et al., 2012c; Hasan and Ng, 2013), there are many modeling decisions whose ramifications are not well understood. To investigate these choices and their effects, we introduce a scalable unified probabilistic modeling framework for stance classification models that 1) are collective, 2) reason about disagreement, and 3) can model stance at either the author level or at the post level. We comprehensively evaluate the possible modeling choices on eight topics across two online debate corpora, finding accuracy improvements of up to 11.5 percentage points over a local classifier. Our results highlight the imp"
P15-1012,P14-2113,0,0.0765528,"Missing"
P15-1012,I13-1191,0,\N,Missing
P17-2022,S13-2053,0,0.0344175,"est parameters on the dev set for positive is θf =18, θp =0.85 and θn =1 and for negative is θf =1, θp =0.5 and θn =1. We specify that if the sentence is in both classes we rename it as neutral. We will refer to this classifier as the AutoSlog classifier. Baseline First-Person Sentence Classifiers. Our goal is to see whether the knowledge we learn using AutoSlog-TS complements existing sentiment classifiers. We thus experiment with a number of baseline classifiers: the default SVM classifier from Weka with unigram features (Hall et al., 2005), a version of the NRC-Canada sentiment classifier (Mohammad et al., 2013), provided to us by Qadir and Riloff (2014), and the Stanford Sentiment classifier (Socher et al., 2013). Experimental Setup Our experimental setup involves first creating a corpus of training and test sentences, then applying AutoSlog-TS a second time to learn linguistic patterns. We then set up methods for cascading classifiers to explore whether ensemble classifiers improve our results. Training Set: From the bootstrapped set of stories, we create a corpus of sentences. A critical simplifying assumption of our method is that a multi-sentence story can be labelled as a whole as positive or n"
P17-2022,D14-1125,0,0.508975,"as general expressions for readability, but the actual patterns must match the syntactic constraints associated with the pattern template. 142 of the verbal predicate, which means that these patterns are proxies for one column of verbal function tables like those in Table 1. However, they can also include verb-particle constructions, such as cheated on, or verb-head-of-preposition constructions. In each case though, because these patterns are localized to a verb and only one element, they allow us to learn highly specific patterns that could be incorporated into a dictionary such as +Effect (Choi and Wiebe, 2014). AutoSlog simultaneously harvests both (syntactically constrained) MWE patterns and more compositionally regular verb-argument groups at the same time. AutoSlog-TS computes statistics on the strength of association of each pattern with each class, i.e. P(POSITIVE |p) and P(NEGATIVE |p), along with the pattern’s overall frequency. We define three parameters for each class: θf , the frequency with which a pattern occurs, θp , the probability with which a pattern is associated with the given class and θn , the number of patterns that must occur in the text for it to be labeled. These parameters"
P17-2022,P13-2022,0,0.162276,"their triples, whereas we have found that verbs relating to private states such as need, want and realize are important indicators of first-person affect. Balahur et al. (2012) use the narratives produced by the ISEAR questionnaire (Scherer et al., 1986) for first-person examples of particular emotions and extract sequences of subject-verb-object triples, which they annotate for basic emotions. Recent work has built on this idea, and developed methods to automatically expand Anand & Reschke’s verb classes to create completely new lexical resources (Balahur et al., 2012; Choi and Wiebe, 2014; Deng et al., 2013; Deng and Wiebe, 2014; Ruppenhofer and Brandes, 2015). Choi & Wiebe’s work comes closest to ours in trying to induce (not annotate) lexical functions, but we attempt to infer these from stories directly, whereas they use a structured lexical resource. yield patterns that correspond to the A&R classes, thus validating our suspicion that first-person sentences furnish a simplifying test ground for discovering functional patterns in the wild. However, many patterns are not covered by A&R’s general classes, see Table 6. Looking first at verbs, one major correlation is between positive classes and"
P17-2022,W15-0515,1,0.873274,"(syntactically constrained) MWE patterns and more compositionally regular verb-argument groups at the same time. AutoSlog-TS computes statistics on the strength of association of each pattern with each class, i.e. P(POSITIVE |p) and P(NEGATIVE |p), along with the pattern’s overall frequency. We define three parameters for each class: θf , the frequency with which a pattern occurs, θp , the probability with which a pattern is associated with the given class and θn , the number of patterns that must occur in the text for it to be labeled. These parameters are tuned on the dev set (Riloff, 1996; Oraby et al., 2015; Riloff and Wiebe, 2003). To bootstrap a larger corpus, we want settings that have lower recall but very high precision. We select θp = 0.7, θf = 10 and θn = 3 for the positive class and θp = 0.85, θf = 10 and θn = 4 for the negative class for bootstrapping. 3 these out for annotation by 5 Turkers, who label each instance as positive, negative, or neutral. To ensure the high quality of the test set, we select sentences that were labelled consistently positive or negative by 4 or 5 Turkers. We collected 1,266 positive and 1,440 negative sentences. Dev Set: We created the dev set using the same"
P17-2022,E14-1040,0,0.119471,"reas we have found that verbs relating to private states such as need, want and realize are important indicators of first-person affect. Balahur et al. (2012) use the narratives produced by the ISEAR questionnaire (Scherer et al., 1986) for first-person examples of particular emotions and extract sequences of subject-verb-object triples, which they annotate for basic emotions. Recent work has built on this idea, and developed methods to automatically expand Anand & Reschke’s verb classes to create completely new lexical resources (Balahur et al., 2012; Choi and Wiebe, 2014; Deng et al., 2013; Deng and Wiebe, 2014; Ruppenhofer and Brandes, 2015). Choi & Wiebe’s work comes closest to ours in trying to induce (not annotate) lexical functions, but we attempt to infer these from stories directly, whereas they use a structured lexical resource. yield patterns that correspond to the A&R classes, thus validating our suspicion that first-person sentences furnish a simplifying test ground for discovering functional patterns in the wild. However, many patterns are not covered by A&R’s general classes, see Table 6. Looking first at verbs, one major correlation is between positive classes and public events and neg"
P17-2022,D14-1127,0,0.026817,"is θf =18, θp =0.85 and θn =1 and for negative is θf =1, θp =0.5 and θn =1. We specify that if the sentence is in both classes we rename it as neutral. We will refer to this classifier as the AutoSlog classifier. Baseline First-Person Sentence Classifiers. Our goal is to see whether the knowledge we learn using AutoSlog-TS complements existing sentiment classifiers. We thus experiment with a number of baseline classifiers: the default SVM classifier from Weka with unigram features (Hall et al., 2005), a version of the NRC-Canada sentiment classifier (Mohammad et al., 2013), provided to us by Qadir and Riloff (2014), and the Stanford Sentiment classifier (Socher et al., 2013). Experimental Setup Our experimental setup involves first creating a corpus of training and test sentences, then applying AutoSlog-TS a second time to learn linguistic patterns. We then set up methods for cascading classifiers to explore whether ensemble classifiers improve our results. Training Set: From the bootstrapped set of stories, we create a corpus of sentences. A critical simplifying assumption of our method is that a multi-sentence story can be labelled as a whole as positive or negative, and that each of its sentences inh"
P17-2022,P16-1030,0,0.13918,"mputational Linguistics (Short Papers), pages 141–147 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2022 pant. Table 1 contains A&R’s functions for verbs of possession: a state in which X has Y or X lacks Y does not convey a clear affect unless we know what the speaker thinks of both X and Y . If the speaker has positive affect toward both X and Y (Row 1), then we infer that her attitude toward the event is positive, but if either is negative, then we infer that the speaker is negative toward the event. Similarly, Rashkin et al. (2016) represent the typical affect communicated by particular predicates via connotation frames. Here we are finding the internal sentiment of the speaker, or, as Rashkin et al. refer to it, the ”mental state” of the speaker. Inspired by A&R’s framework, our work learns lexico-functional patterns (patterns involving lexical items or pairs of lexical items in specific grammatical relations that we show to capture functorargument relations in A&R’s sense), about the effects of combining particular arguments with particular verbs (event types) from first-person narratives. Our novel observation is tha"
P17-2022,D10-1008,0,0.303492,"Missing"
P17-2022,S13-2085,0,0.0179052,"ying I am happy. Large-scale sentiment dictionaries focus on compiling lexical items that bear a consistent affect all on their own (Wilson et al., 2005). But people tend to describe situations, such as My friend bought me flowers, or I got a parking ticket, from which other humans can readily infer their implicit affective reactions. One approach to this problem aims to directly learn units larger than a lexical item that reliably bear some marker of polarity or emotion (Vu et al., 2014; Li et al., 2014; Ding and Riloff, 2016; Goyal et al., 2010; Russo et al., 2015; Kiritchenko et al., 2014; Reckman et al., 2013). Informal first-person narratives are a unique resource for computational models of everyday events and people’s affective reactions to them. People blogging about their day tend not to explicitly say I am happy. Instead they describe situations from which other humans can readily infer their affective reactions. However current sentiment dictionaries are missing much of the information needed to make similar inferences. We build on recent work that models affect in terms of lexical predicate functions and affect on the predicate’s arguments. We present a method to learn proxies for these fun"
P17-2022,W03-1014,0,0.639659,"that I did care for &lt;him&gt;. ActVP Prep &lt;np&gt; neg I didn’t think anything of it until I thought about when he cheated on &lt;me&gt;. InfVP Prep &lt;np&gt; pos ...my friend from college who was so generous to offer his place to &lt;us&gt;... Table 3: AutoSlog-TS Templates and Example Instantiations We hand-annotate a set of 477 positive and 440 negative stories, and use these to bootstrap a larger set of 1,420 negative and 2,288 positive stories. To bootstrap, we apply AutoSlog-TS, a weakly supervised pattern learner that only requires training sets os stories labeled broadly as POSITIVE or NEGATIVE (Riloff, 1996; Riloff and Wiebe, 2003). AutoSlog uses a set of syntactic templates to define different types of linguistic expressions. The left-hand side of Table 3 lists examples of AutoSlog patterns and the right-hand side illustrates a specific lexical-syntactic pattern that corresponds to each general pattern template, as instantiated in first-person stories.1 When bootstrapping a larger positive and negative story corpus, we use the whole story, not just the first person sentences. The left-hand-side of Table 3 shows that the learned patterns can involve syntactic arguments Positive Sentences We had a marvelous visit and dra"
P17-2022,W15-2910,0,0.283906,"ade comes Classifier 1 2 3 4 5 6 7 8 9 SVM NRC Stanford AutoSlog (ASlog) Retrained Stanford NRC, ASlog Stanford, ASlog NRC, ASlog, Stanford NRC, ASlog, SVM Pos F1 0.66 0.58 0.54 0.11 0.53 0.60 0.55 0.64 0.70 Neg F1 0.60 0.69 0.73 0.68 0.73 0.78 0.76 0.79 0.78 Macro F 0.64 0.64 0.67 0.53 0.67 0.71 0.70 0.74 0.75 Table 4: Test Set Results Analysis and Discussion. Here we discuss how the patterns we learned from AutoSlog can supplement the knowledge encoded in current sentiment classifiers, and in newly evolving sentiment resources (Goyal et al., 2010; Choi and Wiebe, 2014; Balahur et al., 2012; Ruppenhofer and Brandes, 2015). POS PATTERNS HAVE FUN HAVE PARTY HEADED FOR NEG PATTERNS HAVE CANCER LOST NOT COME HOME NOT GOING KILL Basic Entailment property possession location Basic Entailment property possession location existence Table 5: Highly predictable AutoSlog extracted case frames and functional description Tables 5 and 6 illustrate several learned lexicofunctional patterns for positive events used in the AutoSlog classifier. The patterns shown in Table 5 are predicted by A&R’s framework, some functions of which can be seen in Table 1. For example, we find a range of basic state descriptions (have party, have"
P17-2022,S15-2077,0,0.390683,"o explicitly flag their affective state, by saying I am happy. Large-scale sentiment dictionaries focus on compiling lexical items that bear a consistent affect all on their own (Wilson et al., 2005). But people tend to describe situations, such as My friend bought me flowers, or I got a parking ticket, from which other humans can readily infer their implicit affective reactions. One approach to this problem aims to directly learn units larger than a lexical item that reliably bear some marker of polarity or emotion (Vu et al., 2014; Li et al., 2014; Ding and Riloff, 2016; Goyal et al., 2010; Russo et al., 2015; Kiritchenko et al., 2014; Reckman et al., 2013). Informal first-person narratives are a unique resource for computational models of everyday events and people’s affective reactions to them. People blogging about their day tend not to explicitly say I am happy. Instead they describe situations from which other humans can readily infer their affective reactions. However current sentiment dictionaries are missing much of the information needed to make similar inferences. We build on recent work that models affect in terms of lexical predicate functions and affect on the predicate’s arguments. W"
P17-2022,D13-1170,0,0.0451157,"es. The left-hand-side of Table 3 shows that the learned patterns can involve syntactic arguments Positive Sentences We had a marvelous visit and drank coffee and ate homemade chocolate chip cookies. Now, I could swim both froggy and free style swimming!! Negative Sentences But last week, he said that he doesn’t know if he has the same feelings for me anymore. I didn’t want to lose him. Table 2: Sentences from the training data In addition, we demonstrate that these lexicofunctional patterns improve the performance of several off-the-shelf sentiment analyzers. We show that Stanford sentiment (Socher et al., 2013) has a best performance of 0.67 macro F on our test set. We then supplement it with our learned patterns and demonstrate significant improvements. Our final ensemble achieves 0.75 F on the test set. 1 The examples are shown as general expressions for readability, but the actual patterns must match the syntactic constraints associated with the pattern template. 142 of the verbal predicate, which means that these patterns are proxies for one column of verbal function tables like those in Table 1. However, they can also include verb-particle constructions, such as cheated on, or verb-head-of-prep"
P17-2022,E14-4025,0,0.147215,"central obstacle to reliable affect prediction is that that people tend not to explicitly flag their affective state, by saying I am happy. Large-scale sentiment dictionaries focus on compiling lexical items that bear a consistent affect all on their own (Wilson et al., 2005). But people tend to describe situations, such as My friend bought me flowers, or I got a parking ticket, from which other humans can readily infer their implicit affective reactions. One approach to this problem aims to directly learn units larger than a lexical item that reliably bear some marker of polarity or emotion (Vu et al., 2014; Li et al., 2014; Ding and Riloff, 2016; Goyal et al., 2010; Russo et al., 2015; Kiritchenko et al., 2014; Reckman et al., 2013). Informal first-person narratives are a unique resource for computational models of everyday events and people’s affective reactions to them. People blogging about their day tend not to explicitly say I am happy. Instead they describe situations from which other humans can readily infer their affective reactions. However current sentiment dictionaries are missing much of the information needed to make similar inferences. We build on recent work that models affect in"
P17-2022,J04-3002,0,0.0451441,"positional functions is greatly simplified in the case of first-person affect. People bear positive affect to themselves, so sentences with first-person elements, e.g. I/we/me, reduce the problem for an approach like A&R’s to learning the polarity that results from composing the verb with only one of its arguments, i.e. only Rows 1, 2 in Table 1 need to be learned for first person subjects. Firstperson narratives are full of such sentences. See Table 2. We show that the learned patterns are often consonant with A&R’s predictions, but are richer, including e.g. many private state descriptions (Wiebe et al., 2004; Wiebe, 1990). We discuss related work in more detail in Sec. 5. 2 Bootstrapping a First-Person Sentiment Corpus We start with a set of first-person narratives (weblogs) drawn from the Spinn3r corpus, that cover a wide range of topics (Burton et al., 2009; Gordon and Swanson, 2009). To reduce noise, we restrict the blogs to those from well-known blogging sites (Ding and Riloff, 2016), and select 15,466 stories whose length ranges from 225 to 375 words. Pattern Template Class Example Instantiations &lt;subj&gt; ActVP neg &lt;I&gt; cry at the thought of it and I’m crying now. &lt;subj&gt; ActInfVP pos &lt;I&gt; got to"
P17-2022,C90-2069,0,0.25053,"is greatly simplified in the case of first-person affect. People bear positive affect to themselves, so sentences with first-person elements, e.g. I/we/me, reduce the problem for an approach like A&R’s to learning the polarity that results from composing the verb with only one of its arguments, i.e. only Rows 1, 2 in Table 1 need to be learned for first person subjects. Firstperson narratives are full of such sentences. See Table 2. We show that the learned patterns are often consonant with A&R’s predictions, but are richer, including e.g. many private state descriptions (Wiebe et al., 2004; Wiebe, 1990). We discuss related work in more detail in Sec. 5. 2 Bootstrapping a First-Person Sentiment Corpus We start with a set of first-person narratives (weblogs) drawn from the Spinn3r corpus, that cover a wide range of topics (Burton et al., 2009; Gordon and Swanson, 2009). To reduce noise, we restrict the blogs to those from well-known blogging sites (Ding and Riloff, 2016), and select 15,466 stories whose length ranges from 225 to 375 words. Pattern Template Class Example Instantiations &lt;subj&gt; ActVP neg &lt;I&gt; cry at the thought of it and I’m crying now. &lt;subj&gt; ActInfVP pos &lt;I&gt; got to swim from the"
P17-2022,H05-2018,0,0.0799208,"r se. This is sometimes referred to as internal sentiment or self reflective sentiment. While in many situations that is overlaid with the author’s opinions, in first-personal narratives, because the author is the protagonist, the two perspectives align. Here, we use the term affect to reference this protagonist-centered notion of opinion. A central obstacle to reliable affect prediction is that that people tend not to explicitly flag their affective state, by saying I am happy. Large-scale sentiment dictionaries focus on compiling lexical items that bear a consistent affect all on their own (Wilson et al., 2005). But people tend to describe situations, such as My friend bought me flowers, or I got a parking ticket, from which other humans can readily infer their implicit affective reactions. One approach to this problem aims to directly learn units larger than a lexical item that reliably bear some marker of polarity or emotion (Vu et al., 2014; Li et al., 2014; Ding and Riloff, 2016; Goyal et al., 2010; Russo et al., 2015; Kiritchenko et al., 2014; Reckman et al., 2013). Informal first-person narratives are a unique resource for computational models of everyday events and people’s affective reaction"
P19-1065,W14-4320,0,0.0176801,"er et al., 2008; Lin et al., 2009) or rule-based systems (Wellner et al., 2006). Recent work has proposed neural network based models with attention or advanced representations, such as CNN (Qin et al., 2016), attention on neural tensor network (Guo et al., 2018), and memory networks (Jia et al., 2018). Advanced representations may help to achieve higher performance (Bai and Zhao, 2018). Some methods also consider context paragraphs and inter-paragraph dependency (Dai and Huang, 2018). To utilize machine learning models for this task, larger datasets would provide a bigger optimization space (Li and Nenkova, 2014). Marcu and Echihabi (2002) is the first work to generate artificial samples to extend the dataset by using rules to 3 EU FP6 contract No. 33549, http://www. ist-luna.eu/ 4 The Edina dataset is publicly available at https://github.com/jfainberg/self_ dialogue_corpus 2 https://github.com/derekmma/ dialogue-discourse-relation 667 then have humans annotate a small sample of the data in order to validate the automated pipeline. Our pipeline targets the four level-1 discourse relations, i.e., “Comparison”, “Expansion”, “Contingency” and “Temporal”. We obtained this initial connectives pool accordin"
P19-1065,D09-1036,0,0.0822546,"Missing"
P19-1065,L18-1707,1,0.809923,"nnectives appear in the middle of an utterance, and another pattern for when connectives link two arguments in adjacent utterances across separate turns. Finally, 5 The list of connectives for each relation in detail can be found in (Pitler et al., 2008). 668 man annotations, which proves the reliability of our proposed extraction method. 4 model is trained on over 120,000 utterances and labeled across 22 topics. This includes commonly discussed topics such as POLITICS, FASH ION , SPORTS , SCIENCE AND TECHNOLOGY , and MUSIC . Core Entities Types: We use SlugNERDS to detect our named entities (Bowden et al., 2018b, 2017). SlugNERDS is specialized for opendomain dialogue interactions. It can sift through noisy user data and it uses the constantly updated Google Knowledge Graph6 to remain aware of even the latest named entities. Both of these points are vital for understanding social chit-chat. We only consider the entity types of the entities as feature rather than entities themselves. We use standard schema.org types and there are totally 614 types. For example, if SlugNERDS detects “Cam Newton”, which is an entity with type PERSON, then PERSON is used as feature. Model We propose the novel approach o"
P19-1065,C14-1160,0,0.105115,"ct a discourse relation pair dataset from a large corpus of open-domain dialogue, which to our knowledge is the first of its kind. Second, we investigated a feature-based model with different dialogue feature combinations and enhanced a deep learning model by incorporating dialogue features that utilize aspects unique to dialogue. The dataset and related code are publicly available.2 2 convert explicit discourse relation pairs into implicit pairs by dropping the connectives. This work is further extended by methods for selecting high-quality samples (Rutherford and Xue, 2015; Xu et al., 2018; Braud and Denis, 2014; Wang et al., 2012). Most of the existing work discussed so far is based on the PDTB dataset, which targets formal texts like news, making it less suitable for our task which is centered around informal dialogue. Related work on discourse relation annotation in a dialogue corpus is limited (Stent, 2000; Tonelli et al., 2010). For example Tonelli et al. (2010) annotated the Luna corpus,3 which does not include English annotations. To our knowledge there is no English dialogue-based corpus with implicit discourse relation labels, as such research specifically targeting a discourse relation iden"
P19-1065,P14-5010,0,0.0128232,"gument pairs into the NLU pipeline and get dialogue features which are then fed as one-hot vectors to a logistic regression classifier. A full dialogue feature vector contains 448 features. The dialogue features include: Dialogue Act: The act of a dialogue utterance is obtained using the NPS dialogue act classifier (Forsyth and Martell, 2007). There are 15 different dialogue acts, including G REET, C LARIFY, and S TATEMENT. The full list of dialogue acts is described in (Forsyth and Martell, 2007). Sentiment: The sentiment of a dialogue utterance is obtained from the Stanford CoreNLP Toolkit (Manning et al., 2014) and there are five possible sentiment values: VERY POSITIVE, POSITIVE, NEUTRAL , NEGATIVE, and VERY NEGATIVE . Intent: An utterance intent ontology consisting of 33 discrete intents is developed and recognized using heuristics and a trained model. It is designed to obtain utterance intent without conversational context, so only the input utterance is considered for intent detection. Some sample intents are REQUEST OPINION, REQUEST SERVICE, RE QUEST CHANGE TOPIC . It is trained using a subset of Common Alexa Prize Chats (CAPC) dataset with roughly 50K utterances and the model ensembles both a"
P19-1065,N18-1202,0,0.0106726,"(Khatri et al., 2018), which is a Deep Average network BiLSTM model. The 4.2 Deep Learning Model with Dialogue Features To investigate the adaptability of existing discourse relation identification models on dialogue data and our proposed features, we build on the Deep Enhanced Representation (DER) model of Bai and Zhao (2018)7 , which demonstrated its efficiency by achieving the current state-of-the-art performance on the PDTB dataset. It utilized different grained text representations including character, sub-word, word, sentence, and sentence pair levels, with embeddings obtained by ELMo (Peters et al., 2018). The model first generates representations for the argument pairs using an encoder and bi-attention module; these are then sent to the classifier, consisting of multiple layer perceptrons with softmax, to predict the discourse relation. We take the DER design and architecture and train on Edina-DR dataset to evaluate the adaptability of existing model in dialogue environment. Then we explore a variation of this model by connecting dialogue feature vectors to the argument pairs representation vector to extend the representation. We use the same method to encode all dialogue features as the fea"
P19-1065,C18-1046,0,0.0351757,"Missing"
P19-1065,prasad-etal-2008-penn,0,0.660652,"further improve the identification of such relations by performing feature ablation and incorporating dialogue features to enhance the state-of-the-art model. 1 Introduction Discourse analysis considering relations between clauses has received increasing attention from the field, and implicit discourse relation identification is one of the most challenging problems in discourse parsing since it is purely based on textual features. Previous work has defined four widely accepted major classes of discourse relation “Comparison”, “Expansion”, “Contingency” and “Temporal” (Miltsakaki et al., 2008; Prasad et al., 2008). These four relations can either be explicitly or implicitly realized. When explicitly realized, 1 More details about Penn Discourse Treebank can be found at https://www.seas.upenn.edu/˜pdtb/ 666 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 666–672 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics There are many fundamental challenges with identifying and utilizing discourse relations in an open-domain dialogue system. All existing datasets for discourse relation identification are based on monologic t"
P19-1065,P18-2070,0,0.0247821,"erances. We Related Work The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) makes research on machine learning based implicit discourse relation recognition possible. Most previous work is based on linguistic and semantic features such as word pairs and brown cluster pair representation (Pitler et al., 2008; Lin et al., 2009) or rule-based systems (Wellner et al., 2006). Recent work has proposed neural network based models with attention or advanced representations, such as CNN (Qin et al., 2016), attention on neural tensor network (Guo et al., 2018), and memory networks (Jia et al., 2018). Advanced representations may help to achieve higher performance (Bai and Zhao, 2018). Some methods also consider context paragraphs and inter-paragraph dependency (Dai and Huang, 2018). To utilize machine learning models for this task, larger datasets would provide a bigger optimization space (Li and Nenkova, 2014). Marcu and Echihabi (2002) is the first work to generate artificial samples to extend the dataset by using rules to 3 EU FP6 contract No. 33549, http://www. ist-luna.eu/ 4 The Edina dataset is publicly available at https://github.com/jfainberg/self_ dialogue_corpus 2 https://githu"
P19-1065,D16-1246,0,0.0206354,"e automatically extracts argument pairs and assign discourse relation labels to each of the utterances. We Related Work The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) makes research on machine learning based implicit discourse relation recognition possible. Most previous work is based on linguistic and semantic features such as word pairs and brown cluster pair representation (Pitler et al., 2008; Lin et al., 2009) or rule-based systems (Wellner et al., 2006). Recent work has proposed neural network based models with attention or advanced representations, such as CNN (Qin et al., 2016), attention on neural tensor network (Guo et al., 2018), and memory networks (Jia et al., 2018). Advanced representations may help to achieve higher performance (Bai and Zhao, 2018). Some methods also consider context paragraphs and inter-paragraph dependency (Dai and Huang, 2018). To utilize machine learning models for this task, larger datasets would provide a bigger optimization space (Li and Nenkova, 2014). Marcu and Echihabi (2002) is the first work to generate artificial samples to extend the dataset by using rules to 3 EU FP6 contract No. 33549, http://www. ist-luna.eu/ 4 The Edina data"
P19-1065,N15-1081,0,0.0184535,", we carry out two steps. First, we construct a discourse relation pair dataset from a large corpus of open-domain dialogue, which to our knowledge is the first of its kind. Second, we investigated a feature-based model with different dialogue feature combinations and enhanced a deep learning model by incorporating dialogue features that utilize aspects unique to dialogue. The dataset and related code are publicly available.2 2 convert explicit discourse relation pairs into implicit pairs by dropping the connectives. This work is further extended by methods for selecting high-quality samples (Rutherford and Xue, 2015; Xu et al., 2018; Braud and Denis, 2014; Wang et al., 2012). Most of the existing work discussed so far is based on the PDTB dataset, which targets formal texts like news, making it less suitable for our task which is centered around informal dialogue. Related work on discourse relation annotation in a dialogue corpus is limited (Stent, 2000; Tonelli et al., 2010). For example Tonelli et al. (2010) annotated the Luna corpus,3 which does not include English annotations. To our knowledge there is no English dialogue-based corpus with implicit discourse relation labels, as such research specific"
P19-1065,D13-1170,0,0.00324768,"ger more balanced corpus. For single dialogue features, INTENT and ENTI provide the largest performance boost compared to other single dialogue features, and this demonstrates the effectiveness of using intent and types of entities for discourse relation identification. Other three features maintain the same level of performance, except a large drop in precision with respect to SENTIMENT. One possible explanation is that our sentiment classification results are obtained using the Sentiment Annotator from Stanford CoreNLP Toolkit, which is trained on movie reviews corpus (Manning et al., 2014; Socher et al., 2013). The nature of training data is not suitable for our dialogue corpus in this task. Using Table 2, we see that the best configuration includes all of our dialogue features except SEN TIMENT . TIES TYPES 5.2 F1 0.51 0.68 0.76 0.77 Table 3: Performance of Deep Learning Models (Dataset name is shown in parentheses) Feature-based Classifier and Dialogue Feature Selection Features Acc. 0.61 0.64 0.80 0.81 Deep Learning Models References In Table 3, we see the results of our experiments, where DER represents our baseline model. We use the default parameter for DER models. We also show the result of"
P19-1065,tonelli-etal-2010-annotation,0,0.0258847,"ogue. The dataset and related code are publicly available.2 2 convert explicit discourse relation pairs into implicit pairs by dropping the connectives. This work is further extended by methods for selecting high-quality samples (Rutherford and Xue, 2015; Xu et al., 2018; Braud and Denis, 2014; Wang et al., 2012). Most of the existing work discussed so far is based on the PDTB dataset, which targets formal texts like news, making it less suitable for our task which is centered around informal dialogue. Related work on discourse relation annotation in a dialogue corpus is limited (Stent, 2000; Tonelli et al., 2010). For example Tonelli et al. (2010) annotated the Luna corpus,3 which does not include English annotations. To our knowledge there is no English dialogue-based corpus with implicit discourse relation labels, as such research specifically targeting a discourse relation identification model for social open-domain dialogue remains unexplored. 3 Dataset Construction Previous work on discourse relation identification suggests that the most effective approach is supervised learning, but limited amounts of annotated data constrain the application of such algorithms. Previous work has additionally pro"
P19-1065,C12-1168,0,0.0442796,"Missing"
P19-1065,W06-1317,0,0.118803,"ct discourse relation argument pairs through utilizing the connective words which are known as clear relation indicators. The pipeline automatically extracts argument pairs and assign discourse relation labels to each of the utterances. We Related Work The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) makes research on machine learning based implicit discourse relation recognition possible. Most previous work is based on linguistic and semantic features such as word pairs and brown cluster pair representation (Pitler et al., 2008; Lin et al., 2009) or rule-based systems (Wellner et al., 2006). Recent work has proposed neural network based models with attention or advanced representations, such as CNN (Qin et al., 2016), attention on neural tensor network (Guo et al., 2018), and memory networks (Jia et al., 2018). Advanced representations may help to achieve higher performance (Bai and Zhao, 2018). Some methods also consider context paragraphs and inter-paragraph dependency (Dai and Huang, 2018). To utilize machine learning models for this task, larger datasets would provide a bigger optimization space (Li and Nenkova, 2014). Marcu and Echihabi (2002) is the first work to generate"
P19-1065,D18-1079,0,0.0133724,"First, we construct a discourse relation pair dataset from a large corpus of open-domain dialogue, which to our knowledge is the first of its kind. Second, we investigated a feature-based model with different dialogue feature combinations and enhanced a deep learning model by incorporating dialogue features that utilize aspects unique to dialogue. The dataset and related code are publicly available.2 2 convert explicit discourse relation pairs into implicit pairs by dropping the connectives. This work is further extended by methods for selecting high-quality samples (Rutherford and Xue, 2015; Xu et al., 2018; Braud and Denis, 2014; Wang et al., 2012). Most of the existing work discussed so far is based on the PDTB dataset, which targets formal texts like news, making it less suitable for our task which is centered around informal dialogue. Related work on discourse relation annotation in a dialogue corpus is limited (Stent, 2000; Tonelli et al., 2010). For example Tonelli et al. (2010) annotated the Luna corpus,3 which does not include English annotations. To our knowledge there is no English dialogue-based corpus with implicit discourse relation labels, as such research specifically targeting a"
P19-1065,W00-1433,0,\N,Missing
P19-1065,P02-1047,0,\N,Missing
P19-1065,C08-2022,0,\N,Missing
P19-1596,D14-1082,0,0.00727588,"nd “steak”. Beginning with the original set of over 4 million business reviews, we sentence-tokenize them and randomly sample a set of 500,000 sentences from restaurant reviews that mention of at least one of the meat items (spanning around 3k 2 https://www.yelp.com/dataset/ challenge unique restaurants, 170k users, and 340k reviews). We filter to select sentences that are between 4 and 30 words in length: restricting the length increases the likelihood of a successful parse and reduces noise in the process of automatic MR construction. We parse the sentences using Stanford dependency parser (Chen and Manning, 2014), removing any sentence that is tagged as a fragment. We show a sample sentence parse in Figure 1. We identify all nouns and search for them in the attribute lexicons, constructing (attribute, value) tuples if a noun is found in a lexicon, including the full noun compound if applicable, e.g. (food, chicken-chimichanga) in Figure 1.3 Next, for each (attribute, value) tuple, we extract all amod, nsubj, or compound relations between a noun value in the lexicons and an adjective using the dependency parse, resulting in (attribute, value, adjective) tuples. We add in “mention order” into the tuple"
P19-1596,dorr-etal-1998-thematic,0,0.380787,"and style information, and present a rigorous set of evaluations to quantify the effect of the style markup on the ability of the models to achieve multiple style goals. For future work, we plan on exploring other models for NLG, and on providing models with a more detailed input representation in order to help preserve more dependency information, as well as to encode more information on syntactic structures we want to realize in the output. We are also interested in including richer, more semanticallygrounded information in our MRs, for example using Abstract Meaning Representations (AMRs) (Dorr et al., 1998; Banarescu et al., 2013; Flanigan et al., 2014). Finally, we are interested in reproducing our corpus generation method on various other domains to allow for the creation of numerous useful datasets for the NLG community. 5946 References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473. Anusha Balakrishnan, Jinfeng Rao, Kartikeya Upasani, Michael White, and Rajen Subba. 2019. Constrained decoding for neural nlg from compositional representations in task-oriented dialogue. To appear in Proceedin"
P19-1596,W17-4912,0,0.0859232,"surdly overpriced at more than $50 a person for dinner. What do you get for that princely sum? Some cold crab legs (it’s NOT King Crab, either, despite what others are saying) Shrimp cocktail (several of which weren’t even deveined. GROSS. [...]) (5/5 star) One of my new fave buffets in Vegas! Very cute interior, and lots of yummy foods! [...] The delicious Fresh, delicious king grab legs!! [...]REALLY yummy desserts! [...] All were grrreat, but that tres leches was ridiculously delicious. Table 2: Yelp restaurant reviews for the same business. or formality (Fan et al., 2017; Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Herzig et al., 2017; Fu et al., 2018; Rao and Tetreault, 2018). However, human language actually involves a constellation of interacting aspects of style, and NNLG models should be able to jointly control these multiple interacting aspects. In this work, we tackle both bottlenecks simultaneously by leveraging masses of freely available, highly descriptive user review data, such as that shown in Table 2. These naturally-occurring examples show a highly positive and highly negative review for the same restaurant, with many examples of rich language and detailed descriptions,"
P19-1596,P14-1134,0,0.0128454,"us set of evaluations to quantify the effect of the style markup on the ability of the models to achieve multiple style goals. For future work, we plan on exploring other models for NLG, and on providing models with a more detailed input representation in order to help preserve more dependency information, as well as to encode more information on syntactic structures we want to realize in the output. We are also interested in including richer, more semanticallygrounded information in our MRs, for example using Abstract Meaning Representations (AMRs) (Dorr et al., 1998; Banarescu et al., 2013; Flanigan et al., 2014). Finally, we are interested in reproducing our corpus generation method on various other domains to allow for the creation of numerous useful datasets for the NLG community. 5946 References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473. Anusha Balakrishnan, Jinfeng Rao, Kartikeya Upasani, Michael White, and Rajen Subba. 2019. Constrained decoding for neural nlg from compositional representations in task-oriented dialogue. To appear in Proceedings of ACL 19. Laura Banarescu, Claire Bonial, Sh"
P19-1596,P17-1017,0,0.0794896,"ffer from two critical bottlenecks: (1) a data bottleneck, i.e. the lack of large parallel training data of MR to NL, and (2) a control bottleneck, i.e. the inability to systematically control important aspects of the generated output to allow for more stylistic variation. Recent efforts to address the data bottleneck with large corpora for training neural generators have relied almost entirely on high-effort, costly crowdsourcing, asking humans to write references given an input MR. Table 1 shows two recent efforts: the E 2 E NLG challenge (Novikova et al., 2017a) and the W EB NLG challenge (Gardent et al., 2017), both with an example of an MR, human reference, and system realization. The largest dataset, E2E, consists of 50k instances. Other datasets, such as the Laptop (13k) and TV (7k) product review datasets, are similar but smaller (Wen et al., 2015a,b). These datasets were created primarily to focus on the task of semantic fidelity, and thus it is very evident from comparing the human and system outputs from each system that the model realizations are less fluent, descriptive, and natural than the human reference. Also, the nature of the domains (restaurant description, Wikipedia infoboxes, and"
P19-1596,W17-3541,0,0.133379,"r dinner. What do you get for that princely sum? Some cold crab legs (it’s NOT King Crab, either, despite what others are saying) Shrimp cocktail (several of which weren’t even deveined. GROSS. [...]) (5/5 star) One of my new fave buffets in Vegas! Very cute interior, and lots of yummy foods! [...] The delicious Fresh, delicious king grab legs!! [...]REALLY yummy desserts! [...] All were grrreat, but that tres leches was ridiculously delicious. Table 2: Yelp restaurant reviews for the same business. or formality (Fan et al., 2017; Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Herzig et al., 2017; Fu et al., 2018; Rao and Tetreault, 2018). However, human language actually involves a constellation of interacting aspects of style, and NNLG models should be able to jointly control these multiple interacting aspects. In this work, we tackle both bottlenecks simultaneously by leveraging masses of freely available, highly descriptive user review data, such as that shown in Table 2. These naturally-occurring examples show a highly positive and highly negative review for the same restaurant, with many examples of rich language and detailed descriptions, such as “absurdly overpriced”, and “rid"
P19-1596,W08-0121,0,0.0144604,"uch as news and product reviews (Fu et al., 2018), movie reviews (Ficler and Goldberg, 2017; Hu et al., 2017), restaurant descriptions (Oraby et al., 2018), and customer care dialogs (Herzig et al., 2017). To our knowledge, our work is the very first to generate realizations that both express particular semantics and exhibit a particular descriptive or lexical style and sentiment. It is also the first work to our knowledge that controls lexical choice in neural generation, a long standing interest of the NLG community (Barzilay and Lee, 2002; Elhadad, 1992; Radev, 1998; Moser and Moore, 1995; Hirschberg, 2008). 6 Conclusions This paper presents the YelpNLG corpus, a set of 300,000 parallel sentences and MR pairs generated by sampling freely available review sentences that contain attributes of interest, and automatically constructing MRs for them. The dataset is unique in its huge range of stylistic variation and language richness, particularly compared to existing parallel corpora for NLG. We train different models with varying levels of information related to attributes, adjective dependencies, sentiment, and style information, and present a rigorous set of evaluations to quantify the effect of t"
P19-1596,W18-6554,1,0.812536,"smooth noise in the training data caused by parser or extraction errors.5 Row 3 shows an example of the value “chicken” mentioned 3 times, each with different adjectives (“bland”, “spicy”, and “seasoned”). Row 4 shows an example of 4 foods and very positive sentiment. 2.1 Comparison to Previous Datasets Table 4 compares Y ELP NLG to previous work in terms of data size, unique vocab and adjectives, entropy,6 average reference length (RefLen), and examples of stylistic and structural variation in terms of contrast (markers such as “but” and “although”), and aggregation (e.g. “both” and “also”) (Juraska and Walker, 2018), showing how our dataset is much larger and more varied than previous work. We note that the Laptop and E2E datasets (which allow multiple sentences per references) have longer references on average than YelpNLG (where references are always single sentences and have a maximum of 30 words). We are interested in experimenting with longer references, possibly with multiple sentences, in future work. Figure 2 shows the distribution of MR length, in terms of the number of attribute-value tuples. There is naturally a higher density of shorter MRs, with around 13k instances from the dataset containi"
P19-1596,W18-2706,0,0.0461684,"Missing"
P19-1596,P16-5005,0,0.0588337,"Missing"
P19-1596,C16-1105,0,0.0572397,"Missing"
P19-1596,D15-1166,0,0.128049,"Missing"
P19-1596,N19-1236,0,0.0239069,"oid having to crowdsource any data by working in reverse: we begin with naturally occurring user reviews, and automatically construct MRs from them. This allows us to create a novel dataset Y ELP NLG, the largest existing NLG dataset, with 300k parallel MR to sentence pairs with rich information on attribute, value, description, and mention order, in addition to a set of sentence-level style information, including sentiment, length, and pronouns. In terms of control mechanisms, very recent work in NNLG has begun to explore using an explicit sentence planning stage and hierarchical structures (Moryossef et al., 2019; Balakrishnan et al., 2019). In our own work, we show how we are able to control various aspects of style with simple supervision within the input MR, without requiring a dedicated sentence planner, and in line with the end-to-end neural generation paradigm. Previous work has primarily attempted to individually control aspects of content preservation and style attributes such as formality and verb tense, sentiment (2017), and personality in different domains such as news and product reviews (Fu et al., 2018), movie reviews (Ficler and Goldberg, 2017; Hu et al., 2017), restaurant descriptions"
P19-1596,P95-1018,0,0.230503,"in different domains such as news and product reviews (Fu et al., 2018), movie reviews (Ficler and Goldberg, 2017; Hu et al., 2017), restaurant descriptions (Oraby et al., 2018), and customer care dialogs (Herzig et al., 2017). To our knowledge, our work is the very first to generate realizations that both express particular semantics and exhibit a particular descriptive or lexical style and sentiment. It is also the first work to our knowledge that controls lexical choice in neural generation, a long standing interest of the NLG community (Barzilay and Lee, 2002; Elhadad, 1992; Radev, 1998; Moser and Moore, 1995; Hirschberg, 2008). 6 Conclusions This paper presents the YelpNLG corpus, a set of 300,000 parallel sentences and MR pairs generated by sampling freely available review sentences that contain attributes of interest, and automatically constructing MRs for them. The dataset is unique in its huge range of stylistic variation and language richness, particularly compared to existing parallel corpora for NLG. We train different models with varying levels of information related to attributes, adjective dependencies, sentiment, and style information, and present a rigorous set of evaluations to quant"
P19-1596,W18-6535,1,0.852238,"ch is not a constraint within the model; we include them for comparative purposes. From the table, we observe that across all metrics, we see a steady increase as more information is added. Overall, the + STYLE model has the highest scores for all metrics, i.e. + STYLE model outputs are most lexically similar to the references. Semantic Error Rate. The types of semantic errors the models make are more relevant than how well they conform to test references. We calculate average Semantic Error Rate (SER), which is a function of the number of semantic mistakes the model makes (Wen et al., 2015a; Reed et al., 2018). We find counts of two types of common mistakes: deletions, where the model fails to realize a value from the input MR, and repetitions, where the model repeats the same value more than once.9 Thus, we compute SER per MR as SER = D+R N , where D and R are the number of deletions and repetitions, and the N is the number of tuples in the MR, and average across the test outputs. 8 https://github.com/tuetschek/ e2e-metrics 9 We note that other types of errors include insertions and substitutions, but we evaluate these through our human evaluation in Sec 4.3 since our large vocabulary size makes i"
P19-1596,N16-1005,0,0.033229,"n is an additional feature, such as adjective or mention order. Specifically in the case of + STYLE MRs, the additional features may be sentence-level features, such as sentiment, length, or exclamation. In this case, we enforce additional constraints 5942 on the models for + ADJ , + SENT, and + STYLE, changing the conditional probability computation for w1:T given a source sentence x1:S to Q p(w1:T |x) = T1 p(wt |w1:t−1 , x, f ), where f is the set of new feature constraints to the model. We represent these additional features as a vector of additional supervision tokens or side constraints (Sennrich et al., 2016). Thus, we construct a vector for each set of features, and concatenate them to the end of each attributevalue pair, encoding the full sequence as for BASE above. Target decoding. At each time step of the decoding phase the decoder computes a new decoder hidden state based on the previously predicted word and an attentionally-weighted average of the encoder hidden states. The conditional nextword distribution p(wt |w1:t−1 , x, f ) depends on f, the stylistic feature constraints added as supervision. This is produced using the decoder hidden state to compute a distribution over the vocabulary o"
P19-1596,D13-1170,0,0.00442643,"l of length, we assign a length bin of short (≤ 10 words), medium (10-20 words), and long (≥ 20 words). We also include whether the sentence is in first person. For each sentence, we create 4 MR variations. The simplest variation, BASE, contains only attributes and their values. The + ADJ version adds adjectives, + SENT adds sentiment, and finally the richest MR, + STYLE, adds style information on 3 Including noun compounds allows us to identify new values that did not exist in our lexicons, thus automatically expanding them. 4 A pilot experiment comparing this method with Stanford sentiment (Socher et al., 2013) showed that copying down the original review ratings gives more reliable sentiment scores. 5940 1 The chicken chimichanga was tasty but the beef was even better! (attr=food, val=chicken chimichanga, adj=tasty, mention=1), (attr=food, val=beef, adj=no adj, mention=1) +[sentiment=positive, len=medium, first person=false, exclamation=true] 2 Food was pretty good ( i had a chicken wrap ) but service was crazy slow. (attr=food, val=chicken wrap, adj=no adj, mention=1), (attr=service, val=service, adj=slow, mention=1) +[sentiment=neutral, len=medium, first person=true, exclamation=false] 3 The chic"
P19-1596,W17-5525,0,0.197735,"Missing"
P19-1596,W17-4904,1,0.853584,"e, and parking availability for over 150k businesses, with around 4 million reviews in total. We note that this domain and dataset are particularly unique in how naturally descriptive the language used is, as exemplified in Table 2, especially compared to other datasets previously used for NLG in domains such as Wikipedia. For corpus creation, we must first sample sentences from reviews in such a way as to allow the automatic and reliable construction of MRs using fully automatic tools. To identify restaurant attributes, we use restaurant lexicons from our previous work on template-based NLG (Oraby et al., 2017). The lexicons include five attribute types prevalent in restaurant reviews: restaurant-type, cuisine, food, service, and staff collected from Wikipedia and DBpedia, including, for example, around 4k for foods (e.g. “sushi”), and around 40 for cuisines (e.g. “Italian”). We then expand these basic lexicons by adding in attributes for ambiance (e.g. “decoration”) and price (e.g. “cost”) using vocabulary items from the E2E generation challenge (Novikova et al., 2017b). To enforce some semantic constraints and “truth grounding” when selecting sentences without severely limiting variability, we onl"
P19-1596,W18-5019,1,0.852168,"Balakrishnan et al., 2019). In our own work, we show how we are able to control various aspects of style with simple supervision within the input MR, without requiring a dedicated sentence planner, and in line with the end-to-end neural generation paradigm. Previous work has primarily attempted to individually control aspects of content preservation and style attributes such as formality and verb tense, sentiment (2017), and personality in different domains such as news and product reviews (Fu et al., 2018), movie reviews (Ficler and Goldberg, 2017; Hu et al., 2017), restaurant descriptions (Oraby et al., 2018), and customer care dialogs (Herzig et al., 2017). To our knowledge, our work is the very first to generate realizations that both express particular semantics and exhibit a particular descriptive or lexical style and sentiment. It is also the first work to our knowledge that controls lexical choice in neural generation, a long standing interest of the NLG community (Barzilay and Lee, 2002; Elhadad, 1992; Radev, 1998; Moser and Moore, 1995; Hirschberg, 2008). 6 Conclusions This paper presents the YelpNLG corpus, a set of 300,000 parallel sentences and MR pairs generated by sampling freely avai"
P19-1596,D14-1162,0,0.0806803,"Missing"
P19-1596,P98-2176,0,0.104923,"d personality in different domains such as news and product reviews (Fu et al., 2018), movie reviews (Ficler and Goldberg, 2017; Hu et al., 2017), restaurant descriptions (Oraby et al., 2018), and customer care dialogs (Herzig et al., 2017). To our knowledge, our work is the very first to generate realizations that both express particular semantics and exhibit a particular descriptive or lexical style and sentiment. It is also the first work to our knowledge that controls lexical choice in neural generation, a long standing interest of the NLG community (Barzilay and Lee, 2002; Elhadad, 1992; Radev, 1998; Moser and Moore, 1995; Hirschberg, 2008). 6 Conclusions This paper presents the YelpNLG corpus, a set of 300,000 parallel sentences and MR pairs generated by sampling freely available review sentences that contain attributes of interest, and automatically constructing MRs for them. The dataset is unique in its huge range of stylistic variation and language richness, particularly compared to existing parallel corpora for NLG. We train different models with varying levels of information related to attributes, adjective dependencies, sentiment, and style information, and present a rigorous set"
P19-1596,N16-1015,0,0.116398,"Missing"
P19-1596,D15-1199,0,0.577129,"Missing"
P19-1596,C98-2171,0,\N,Missing
P19-1596,W02-1022,0,\N,Missing
P19-1596,W13-2322,0,\N,Missing
P19-1596,N18-1012,0,\N,Missing
P19-1596,D18-1547,0,\N,Missing
P19-1596,P19-1080,0,\N,Missing
P19-1596,N16-1086,0,\N,Missing
P89-1031,E89-1001,0,0.0168776,"Missing"
P89-1031,J86-3001,0,0.378133,"Missing"
P89-1031,P87-1023,0,0.0608238,"Missing"
P89-1031,P86-1032,0,0.0298353,"Missing"
P89-1031,J84-2002,0,\N,Missing
P89-1031,E89-1016,0,\N,Missing
P89-1031,E89-1039,0,\N,Missing
P89-1031,P86-1034,0,\N,Missing
P89-1031,C80-1027,0,\N,Missing
P89-1031,P87-1022,0,\N,Missing
P89-1031,P88-1015,0,\N,Missing
P89-1031,P83-1007,0,\N,Missing
P90-1010,J86-3001,0,0.551949,"Missing"
P90-1010,J84-2002,0,0.0848014,"Missing"
P90-1010,J87-1002,0,0.208702,"Missing"
P90-1010,P87-1023,0,0.0624661,"Missing"
P90-1010,P89-1016,0,0.044711,"Missing"
P90-1010,P88-1014,0,0.035025,"Missing"
P90-1010,P89-1031,1,\N,Missing
P90-1010,P87-1022,0,\N,Missing
P90-1010,P88-1015,1,\N,Missing
P97-1035,P84-1029,0,0.255803,"Missing"
P97-1035,P95-1019,0,0.0562158,"Missing"
P97-1035,J97-1005,1,0.46079,"Missing"
P97-1035,H92-1005,0,0.189592,"Missing"
P97-1035,P92-1032,0,0.00533955,"Missing"
P97-1035,J86-3001,0,0.175731,"Missing"
P97-1035,H92-1009,0,0.0215109,"Missing"
P97-1035,P96-1038,0,0.00615359,"Missing"
P97-1035,H90-1023,0,0.0327404,"Missing"
P97-1035,J97-1006,0,0.05548,"Missing"
P97-1035,C82-1066,0,\N,Missing
P97-1035,H91-1062,0,\N,Missing
P97-1035,P95-1018,0,\N,Missing
P97-1035,J96-2004,0,\N,Missing
P98-2129,P84-1029,0,0.333526,"n the 2 TOOT TOOT allows users to access online AMTRAK train schedules via a telephone dialogue, as in Figure 1.I (All examples are from the experiment in Section 3.) We have built two versions of TOOT: literal TOOT (LT) and cooperative TOOT (CT). LT and CT have equivalent functionality, but use different response strategies to present tabular results of web queries in a displayless environment) LT and CT incorporate many of the types of database responses in the focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation &apos;Our domain was chosen to afford future comparison with similar systems,"
P98-2129,J86-2002,0,0.122405,"Missing"
P98-2129,H92-1008,0,0.413772,"access online AMTRAK train schedules via a telephone dialogue, as in Figure 1.I (All examples are from the experiment in Section 3.) We have built two versions of TOOT: literal TOOT (LT) and cooperative TOOT (CT). LT and CT have equivalent functionality, but use different response strategies to present tabular results of web queries in a displayless environment) LT and CT incorporate many of the types of database responses in the focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation &apos;Our domain was chosen to afford future comparison with similar systems, e.g., (Danieli and Gerbino, 1995)."
P98-2129,H92-1005,0,0.061133,"ute a Mean Recognition score per dialogue. We also listened to the recordings to determine how many times the user interrupted the agent (Barge Ins). To measure dialogue efficiency., the number of System Turns and User Turns were extracted from the dialogue manager log, and the total Elapsed Time was determined from the recording. To measure user satisfaction 4, users responded to the web survey in Figure 4, which assessed their subjective evaluation of the agent&apos;s performance. Each question was designed to measure a partic4Questionnaire-based user satisfaction ratings (Shriberg et al., 1992; Polifroni et al., 1992) have been frequently used in the literature as an external indicator of agent usability. • Was the system easy to understand in this conversation? (TTS Performance) • In this conversation, did the system understand what you said? (ASR Performance) • In this conversation, was it easy to find the schedule you wanted? (Task Ease) • Was the pace of interaction with the system appropriate in this conversation? (Interaction Pace) • In this conversation, did you know what you could say at each point of the dialogue? (User Expertise) • How often was the system sluggish and slow to reply to you in thi"
P98-2129,H92-1009,0,0.212635,"ASR rejections, to compute a Mean Recognition score per dialogue. We also listened to the recordings to determine how many times the user interrupted the agent (Barge Ins). To measure dialogue efficiency., the number of System Turns and User Turns were extracted from the dialogue manager log, and the total Elapsed Time was determined from the recording. To measure user satisfaction 4, users responded to the web survey in Figure 4, which assessed their subjective evaluation of the agent&apos;s performance. Each question was designed to measure a partic4Questionnaire-based user satisfaction ratings (Shriberg et al., 1992; Polifroni et al., 1992) have been frequently used in the literature as an external indicator of agent usability. • Was the system easy to understand in this conversation? (TTS Performance) • In this conversation, did the system understand what you said? (ASR Performance) • In this conversation, was it easy to find the schedule you wanted? (Task Ease) • Was the pace of interaction with the system appropriate in this conversation? (Interaction Pace) • In this conversation, did you know what you could say at each point of the dialogue? (User Expertise) • How often was the system sluggish and sl"
P98-2129,P97-1035,1,0.841035,"aborate the conditions under which TOOT&apos; s cooperative rather than literal strategy contributes to greater performance. database queries in TOOT, a spoken dialogue agent for accessing online train schedules via a telephone conversation. We conduct an experiment in which 12 users carry out 4 tasks of varying difficulty with one of two versions of TOOT (literal and cooperative TOOT), resulting in a corpus of 48 dialogues. The values for a wide range of evaluation measures are then extracted from this corpus. We analyze our data using both traditional hypothesis testing methods and the PARADISE (Walker et al., 1997; Walker et al., 1998) methodology for estimating a performance function. Hypothesis testing shows that while differences among some evaluation measures depend on the response strategy (literal or cooperative), other differences are a function of application task and task/strategy interactions. A PARADISE assessment of the contribution of each evaluation measure to overall performance shows that strategy-dependent dialogue phenomena as well as phenomena associated with speech recognition significantly predict performance. Our results identify the conditions under which TOOT&apos; s cooperative resp"
P98-2219,P95-1019,0,0.0222605,"hat supports access to email by phone, with strategies for initiative, and for reading and sum1350 marizing messages. We show that ELVIS can learn that the System-Initiative strategy has higher utility than the Mixed-Initiative strategy, that Read-First is the best read strategy, and that Summarize-System is the best summary strategy. Here, our method was illustrated by evaluating strategies for managing initiative and for message presentation. However there are numerous dialogue strategies that an agent might use, e.g. to gather information, handle errors, or manage the dialogue interaction (Chu-Carroll and Carberry, 1995; Danieli and Gerbino, 1995; Hovy, 1993; McKeown, 1985; Moore and Paris, 1989). Previous work in natural language generation has proposed heuristics to determine an agent&apos;s choice of dialogue strategy, based on factors such as discourse focus, medium, style, and the content of previous explanations (McKeown, 1985; Moore and Paris, 1989; Maybury, 1991; Hovy, 1993). It should be possible to test experimentally whether an agent can automatically learn these heuristics since the methodology we propose is general, and could be applied to any dialogue strategy choice that an agent might make. Previo"
P98-2219,P89-1025,0,0.00993261,"and sum1350 marizing messages. We show that ELVIS can learn that the System-Initiative strategy has higher utility than the Mixed-Initiative strategy, that Read-First is the best read strategy, and that Summarize-System is the best summary strategy. Here, our method was illustrated by evaluating strategies for managing initiative and for message presentation. However there are numerous dialogue strategies that an agent might use, e.g. to gather information, handle errors, or manage the dialogue interaction (Chu-Carroll and Carberry, 1995; Danieli and Gerbino, 1995; Hovy, 1993; McKeown, 1985; Moore and Paris, 1989). Previous work in natural language generation has proposed heuristics to determine an agent&apos;s choice of dialogue strategy, based on factors such as discourse focus, medium, style, and the content of previous explanations (McKeown, 1985; Moore and Paris, 1989; Maybury, 1991; Hovy, 1993). It should be possible to test experimentally whether an agent can automatically learn these heuristics since the methodology we propose is general, and could be applied to any dialogue strategy choice that an agent might make. Previous work has also proposed that an agent&apos;s choice of dialogue strategy can be t"
P98-2219,P97-1035,1,0.718827,"agent can learn to choose an optimal dialogue strategy. While it is widely agreed that dialogue strategies should be formulated in terms of communicative intentions, there has been little work on automatically optimizing an agent&apos;s choices when there are multiple ways to realize a communicative intention. Our method is based on a combination of learning algorithms and empirical evaluation techniques. The learning component of our method is based on algorithms for reinforcement learning, such as dynamic programming and Q-learning. The empirical component uses the PARADISE evaluation framework (Walker et al., 1997) to identify the important performance factors and to provide the performance function needed by the learning algorithm. We illustrate our method with a dialogue agent named ELVIS (EmaiL Voice Interactive System), that supports access to email over the phone. We show how ELVIS can learn to choose among alternate strategies for agent initiative, for reading messages, and for summarizing email folders. 1 Introduction This paper describes a novel method by which a dialogue agent can learn to choose an optimal dialogue strategy. The main problem for dialogue agents is deciding what information to"
P99-1040,P98-1122,0,0.283049,"rformance of classifiers constructed from rather different feature sets (such as acoustic and lexical features) suggest that there is some redundancy between these feature sets (at least with respect to the task). Fourth, the fact 315 that the best estimated accuracy was achieved using all of the features suggests that even problems that seem inherently acoustic may best be solved by exploiting higher-level information. This work differs from previous work in focusing on behavior at the (sub)dialogue level, rather than on identifying single misrecognitions at the utterance level (Smith, 1998; Levow, 1998; van Zanten, 1998). The rationale is that a single misrecognition may not warrant a global change in dialogue strategy, whereas a user's repeated problems communicating with the system might warrant such a change. While we are not aware of any other work that has applied machine learning to detecting patterns suggesting that the user is having problems over the course of a dialogue, (Levow, 1998) has applied machine learning to identifying single misrecognitions. We are currently extending our feature set to include acoustic-prosodic features such as those used by Levow, in order to predict m"
P99-1040,P99-1024,0,0.0659675,"ports the use of ""set-valued"" features for representing text, and because if-then rules are often easier for people to understand than decision trees (Quinlan, 1993). Below we describe our corpus of dialogues, the assignment of classes to each dialogue, the extraction of features from each dialogue, and our learning experiments. Corpus: Our corpus consists of a set of 544 dialogues (over 40 hours of speech) between humans and one of three dialogue systems: ANNIE (Kamm et al., 1998), an agent for voice dialing and messaging; ELVIS (Walker et al., 1998b), an agent for accessing email; and TOOT (Litman and Pan, 1999), an agent for accessing online train schedules. Each agent was implemented using a generalpurpose platform for phone-based spoken dialogue systems (Kamm et al., 1997). The dialogues were obtained in controlled experiments designed to evaluate dialogue strategies for each agent. The exper~We also ran experimentsusing the machine learning program BOOSTEXTER(Schapire and Singer,To appear), with results similarto those presentedbelow. iments required users to complete a set of application tasks in conversations with a particular version of the agent. The experiments resulted in both a digitized r"
P99-1040,P98-2129,1,0.868864,"Missing"
P99-1040,H92-1009,0,0.353503,"Missing"
P99-1040,P98-2219,1,0.824553,"been appropriate in dialogue D1 from the Annie system (Kamm et al., 1998), shown in Figure 1. In order to improve performance through such adaptation, a system must first be able to identify, in real time, salient properties of an ongoing dialogue that call for some useful change in system strategy. In other words, adaptive systems should try to automatically identify actionable properties of ongoing dialogues. Previous work has shown that speech recognition performance is an important predictor of user satisfaction, and that changes in dialogue behavior impact speech recognition performance (Walker et al., 1998b; Litman et al., 1998; Kamm et al., 1998). Therefore, in this work, we focus on the task of automatically detecting poor speech recognition performance in several spoken dialogue systems developed at AT&T Labs. Rather than hand-crafting rules that classify speech recognition performance in an ongoing dialogue, we take a machine learning approach. We begin with a collection of system logs from actual dialogues that were labeled by humans as having had ""good"" or ""bad"" speech recognition (the training set). We then apply standard machine learning algorithms to this training set in the hope of di"
P99-1040,C98-1117,0,\N,Missing
P99-1040,C98-2214,1,\N,Missing
P99-1040,C98-2124,1,\N,Missing
polifroni-walker-2006-learning,W01-0813,0,\N,Missing
polifroni-walker-2006-learning,P03-1033,0,\N,Missing
rambow-etal-2002-dependency,J99-2004,0,\N,Missing
rambow-etal-2002-dependency,J93-2004,0,\N,Missing
rambow-etal-2002-dependency,P97-1003,0,\N,Missing
rambow-etal-2002-dependency,C00-1007,1,\N,Missing
rambow-etal-2002-dependency,H94-1020,0,\N,Missing
rambow-etal-2002-dependency,P95-1037,0,\N,Missing
S16-1068,W11-0702,1,0.784576,"The hashtags and boolean combinations of hashatgs selected for each topic were predicted to be stancebearing on their own. See Table 2. There has been considerable previous work on stance classification in online forums and in congressional debates (Thomas et al., 2006; Burfoot et al., 2011; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012c; Sridhar et al., 2015; Hasan and Ng, 2013; Boltuzic ˇ and Snajder, 2014; Hasan and Ng, 2014). A number of these studies show that collective classification approaches perform well, and that the context (Walker et al., 2012c; Abbott et al., 2011), and meta information such as author constraints are useful for stance classification (Hassan et al., 2012; Hasan and Ng, 2014). Collective classification is not possible in the current task because the only information provided is the text of each individual 421 tweet. Inspired by earlier work (Joshi and PensteinRos´e, 2009; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012b), we apply a framework for developing features for opinion-target pairs based on generalized structural dependency features, using the LIWC dictionary as the basis for generalization (Penneb"
S16-1068,L16-1704,1,0.869101,"nce of repeated characters by two characters. For example, convert “shooooooooot” to “shoot”. 422 We use a part-of-speech tagger for tweets to perform tokenization and POS labelling (Gimpel et al., 2011). We also use TweeboParser, a dependency parser specifically designed for tweets, to parse each tweet (Kong et al., 2014). Our data representation for the corpus keeps track of the original tweet, the normalization replacements, the POS tags, and the parses. Sec. 3 describes the features derived from these pre-processing steps that we also store in our corpus database, modelled after IAC 2.0. (Abbott et al., 2016). 3 Experimental Setup We explored a large number of machine learning algorithms and feature combinations, using the automatically harvested tweets as training and the training set provided for the task as our development data to fit the parameters for the final submitted NLDSUCSC system. Sec. 3.1 describes the feature sets created using the development set. To evaluate the effects of hashtags on the test set we explored two different ways to train the system. Table 4 presents the results on the test set with hashtags present in the dataset while Table 5 is the performance without hashtags. To"
S16-1068,W11-1701,1,0.966719,"tance using factual or emotional content, and due to their restricted length, tweets may not be well structured or coherent. As a result, NLP tools trained on wellstructured text do not work well in Twitter (Dey and Haque, 2008), and new tools are constantly being developed (Qadir and Riloff, 2014; Kong et al., 2014; Han and Baldwin, 2011; Zhu et al., 2014). Our approach to stance classification in tweets is primarily based on developing a suite of tools for processing Twitter that mirrors our previous work on stance classification in online forums (Walker et al., 2012c; Sridhar et al., 2015; Anand et al., 2011; Walker et al., 2012b; Misra and Walker, 2015). We develop generalized dependency features that capture expressed sentiment or attitude towards particular targets, using the Tweebo dependency parser (Kong et al., 2014). Given the small size of the official task dataset, we created our own topic-specific training corpus in a semi-supervised manner. We developed a set of high precision hashtags for each topic that were used to query the Twitter API in order to create a large training corpus without additional human labeling of tweets for stance. The hashtags and boolean combinations of hashatgs"
S16-1068,W14-2107,0,0.254627,"Missing"
S16-1068,P11-1151,0,0.0164785,"iven the small size of the official task dataset, we created our own topic-specific training corpus in a semi-supervised manner. We developed a set of high precision hashtags for each topic that were used to query the Twitter API in order to create a large training corpus without additional human labeling of tweets for stance. The hashtags and boolean combinations of hashatgs selected for each topic were predicted to be stancebearing on their own. See Table 2. There has been considerable previous work on stance classification in online forums and in congressional debates (Thomas et al., 2006; Burfoot et al., 2011; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012c; Sridhar et al., 2015; Hasan and Ng, 2013; Boltuzic ˇ and Snajder, 2014; Hasan and Ng, 2014). A number of these studies show that collective classification approaches perform well, and that the context (Walker et al., 2012c; Abbott et al., 2011), and meta information such as author constraints are useful for stance classification (Hassan et al., 2012; Hasan and Ng, 2014). Collective classification is not possible in the current task because the only information provided is the text of each individual 421 tweet."
S16-1068,I13-1053,0,0.0161595,"t polarity for a word, then the score is neutralized to zero. If a single dictionary lists the polarity word, but it is unlisted or neutral in the other dictionary, then the score is 1 in the direction of the polarity. If both dictionaries list a word with the same polarity, then the score is 2 in the direction of the polarity. After calculating the combined sentiment score, we check if either of the previous two words is listed as a negation by LIWC, 1 https://www.cs.uic.edu/ liub/FBS /sentiment-analysis.html 2 http://neuro.imm.dtu.dk/wiki/AFINN and invert the polarity if a negation is found(Cho et al., 2013; Hasan and Ng, 2012). Pointwise Mutual Information (PMI): For each topic, we calculate normalized pointwise mutual information over a combination of an extended version of IAC 2.0, a topic annotated database of posts from debate forums (Abbott et al., 2016; Walker et al., 2012a), and our own collected tweets for each topic. IAC 2.0 includes several topics that are in overlap with the topics in the current task. We then create a pool of top-N percent PMI unigrams, bigrams, and trigrams for each topic and use the count of words in each tweet that are also in this pool as a feature. We also use"
S16-1068,P11-2008,0,0.147732,"Missing"
S16-1068,P11-1038,0,0.0136822,"s nontrivial due to the challenges of the tweet genre. Tweets are often highly informal with language that is colorful and ungrammatical. They may also involve sarcasm, making opinion-mining tasks more challenging (Riloff et al., 2013; Reyes et al., 2012). Users may assert their stance using factual or emotional content, and due to their restricted length, tweets may not be well structured or coherent. As a result, NLP tools trained on wellstructured text do not work well in Twitter (Dey and Haque, 2008), and new tools are constantly being developed (Qadir and Riloff, 2014; Kong et al., 2014; Han and Baldwin, 2011; Zhu et al., 2014). Our approach to stance classification in tweets is primarily based on developing a suite of tools for processing Twitter that mirrors our previous work on stance classification in online forums (Walker et al., 2012c; Sridhar et al., 2015; Anand et al., 2011; Walker et al., 2012b; Misra and Walker, 2015). We develop generalized dependency features that capture expressed sentiment or attitude towards particular targets, using the Tweebo dependency parser (Kong et al., 2014). Given the small size of the official task dataset, we created our own topic-specific training corpus"
S16-1068,D12-1039,0,0.0273423,"Missing"
S16-1068,C12-2045,0,0.0184377,"ord, then the score is neutralized to zero. If a single dictionary lists the polarity word, but it is unlisted or neutral in the other dictionary, then the score is 1 in the direction of the polarity. If both dictionaries list a word with the same polarity, then the score is 2 in the direction of the polarity. After calculating the combined sentiment score, we check if either of the previous two words is listed as a negation by LIWC, 1 https://www.cs.uic.edu/ liub/FBS /sentiment-analysis.html 2 http://neuro.imm.dtu.dk/wiki/AFINN and invert the polarity if a negation is found(Cho et al., 2013; Hasan and Ng, 2012). Pointwise Mutual Information (PMI): For each topic, we calculate normalized pointwise mutual information over a combination of an extended version of IAC 2.0, a topic annotated database of posts from debate forums (Abbott et al., 2016; Walker et al., 2012a), and our own collected tweets for each topic. IAC 2.0 includes several topics that are in overlap with the topics in the current task. We then create a pool of top-N percent PMI unigrams, bigrams, and trigrams for each topic and use the count of words in each tweet that are also in this pool as a feature. We also use the highest PMI value"
S16-1068,I13-1191,0,0.0141795,"r. We developed a set of high precision hashtags for each topic that were used to query the Twitter API in order to create a large training corpus without additional human labeling of tweets for stance. The hashtags and boolean combinations of hashatgs selected for each topic were predicted to be stancebearing on their own. See Table 2. There has been considerable previous work on stance classification in online forums and in congressional debates (Thomas et al., 2006; Burfoot et al., 2011; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012c; Sridhar et al., 2015; Hasan and Ng, 2013; Boltuzic ˇ and Snajder, 2014; Hasan and Ng, 2014). A number of these studies show that collective classification approaches perform well, and that the context (Walker et al., 2012c; Abbott et al., 2011), and meta information such as author constraints are useful for stance classification (Hassan et al., 2012; Hasan and Ng, 2014). Collective classification is not possible in the current task because the only information provided is the text of each individual 421 tweet. Inspired by earlier work (Joshi and PensteinRos´e, 2009; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker"
S16-1068,D14-1083,0,0.012755,"or each topic that were used to query the Twitter API in order to create a large training corpus without additional human labeling of tweets for stance. The hashtags and boolean combinations of hashatgs selected for each topic were predicted to be stancebearing on their own. See Table 2. There has been considerable previous work on stance classification in online forums and in congressional debates (Thomas et al., 2006; Burfoot et al., 2011; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012c; Sridhar et al., 2015; Hasan and Ng, 2013; Boltuzic ˇ and Snajder, 2014; Hasan and Ng, 2014). A number of these studies show that collective classification approaches perform well, and that the context (Walker et al., 2012c; Abbott et al., 2011), and meta information such as author constraints are useful for stance classification (Hassan et al., 2012; Hasan and Ng, 2014). Collective classification is not possible in the current task because the only information provided is the text of each individual 421 tweet. Inspired by earlier work (Joshi and PensteinRos´e, 2009; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012b), we apply a framework for developing"
S16-1068,D12-1006,0,0.0151229,"g on their own. See Table 2. There has been considerable previous work on stance classification in online forums and in congressional debates (Thomas et al., 2006; Burfoot et al., 2011; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012c; Sridhar et al., 2015; Hasan and Ng, 2013; Boltuzic ˇ and Snajder, 2014; Hasan and Ng, 2014). A number of these studies show that collective classification approaches perform well, and that the context (Walker et al., 2012c; Abbott et al., 2011), and meta information such as author constraints are useful for stance classification (Hassan et al., 2012; Hasan and Ng, 2014). Collective classification is not possible in the current task because the only information provided is the text of each individual 421 tweet. Inspired by earlier work (Joshi and PensteinRos´e, 2009; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012b), we apply a framework for developing features for opinion-target pairs based on generalized structural dependency features, using the LIWC dictionary as the basis for generalization (Pennebaker et al., 2001). We also develop features to capture domain knowledge using PMI values for topic n-grams"
S16-1068,P09-2079,0,0.0813824,"Missing"
S16-1068,D14-1108,0,0.300541,"s issue. The task is nontrivial due to the challenges of the tweet genre. Tweets are often highly informal with language that is colorful and ungrammatical. They may also involve sarcasm, making opinion-mining tasks more challenging (Riloff et al., 2013; Reyes et al., 2012). Users may assert their stance using factual or emotional content, and due to their restricted length, tweets may not be well structured or coherent. As a result, NLP tools trained on wellstructured text do not work well in Twitter (Dey and Haque, 2008), and new tools are constantly being developed (Qadir and Riloff, 2014; Kong et al., 2014; Han and Baldwin, 2011; Zhu et al., 2014). Our approach to stance classification in tweets is primarily based on developing a suite of tools for processing Twitter that mirrors our previous work on stance classification in online forums (Walker et al., 2012c; Sridhar et al., 2015; Anand et al., 2011; Walker et al., 2012b; Misra and Walker, 2015). We develop generalized dependency features that capture expressed sentiment or attitude towards particular targets, using the Tweebo dependency parser (Kong et al., 2014). Given the small size of the official task dataset, we created our own topic-sp"
S16-1068,W13-1104,1,0.868494,"ture ablation results reveal that for the majority of the topics part-of-speech n-grams perform better than LIWC. This was surprising because LIWC was designed to capture emotional and psychological behavior in conversations, and because previous research on stance classification using debate forums shows LIWC categories can improve an ngram baseline (Anand et al., 2011). This may be due to sarcam and irony, a frequent phenomena in twitter not captured by LIWC, but which may to some extent be captured by part-of-speech n-grams that reflect the use of adjectives and adverbs in sarcastic posts (Lukin and Walker, 2013; Reyes et al., 2012). Generalizing Twitter-specific dependency structures using LIWC and sentiment lexicons does however prove useful. 4.1 We ran experiments using the SemEval training as our development data with NaiveBayesMultinomial, SVM, and J48 from WEKA. We tried a large number of feature combinations w/ and w/out stemmed ngrams. The best performing system ended up being different for each topic. Overall, NBM worked the best for all topics. Table 3 describes the system model submitted for each topic based on the results. The model that performed best on the dev set was used to report ac"
S16-1068,S16-1003,0,0.0267112,"logs, weblogs, and discussion forums are used by millions of users to express their opinions on almost everything from brands, celebrities, and events to important social and political issues. In recent years, the microblogging service Twitter has emerged as one of the most popular and useful sources of user content, and recent research has begun to develop tools and computational models for tweet-level opinion and sentiment analysis. Stance classification aims to identify, for a particular issue under discussion, whether the Detecting stance in tweets is a new task proposed for SemEval-2016 (Mohammad et al., 2016). The aim of the task is to determine user stance (FAVOR, AGAINST , or NONE ) in a dataset of tweets on the five selected topics of abortion, atheism, climate change, feminism and Hillary Clinton. Consider the tweets in Table 1, which express stance toward the target issue Climate Change is a Real Concern. It can be inferred that the author of tweet T1 is in favor of the target while the author of tweet T2 is clearly against the target. However due to the brevity of tweets, there is not always sufficient information about the target to determine stance: in the case of tweet T3, we are unsure w"
S16-1068,D14-1127,0,0.0235238,"tational Linguistics this issue. The task is nontrivial due to the challenges of the tweet genre. Tweets are often highly informal with language that is colorful and ungrammatical. They may also involve sarcasm, making opinion-mining tasks more challenging (Riloff et al., 2013; Reyes et al., 2012). Users may assert their stance using factual or emotional content, and due to their restricted length, tweets may not be well structured or coherent. As a result, NLP tools trained on wellstructured text do not work well in Twitter (Dey and Haque, 2008), and new tools are constantly being developed (Qadir and Riloff, 2014; Kong et al., 2014; Han and Baldwin, 2011; Zhu et al., 2014). Our approach to stance classification in tweets is primarily based on developing a suite of tools for processing Twitter that mirrors our previous work on stance classification in online forums (Walker et al., 2012c; Sridhar et al., 2015; Anand et al., 2011; Walker et al., 2012b; Misra and Walker, 2015). We develop generalized dependency features that capture expressed sentiment or attitude towards particular targets, using the Tweebo dependency parser (Kong et al., 2014). Given the small size of the official task dataset, we creat"
S16-1068,D13-1066,0,0.0636729,"Missing"
S16-1068,P09-1026,0,0.0330786,"the official task dataset, we created our own topic-specific training corpus in a semi-supervised manner. We developed a set of high precision hashtags for each topic that were used to query the Twitter API in order to create a large training corpus without additional human labeling of tweets for stance. The hashtags and boolean combinations of hashatgs selected for each topic were predicted to be stancebearing on their own. See Table 2. There has been considerable previous work on stance classification in online forums and in congressional debates (Thomas et al., 2006; Burfoot et al., 2011; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012c; Sridhar et al., 2015; Hasan and Ng, 2013; Boltuzic ˇ and Snajder, 2014; Hasan and Ng, 2014). A number of these studies show that collective classification approaches perform well, and that the context (Walker et al., 2012c; Abbott et al., 2011), and meta information such as author constraints are useful for stance classification (Hassan et al., 2012; Hasan and Ng, 2014). Collective classification is not possible in the current task because the only information provided is the text of each individual 421 tweet. Inspired by earlier work (Josh"
S16-1068,W10-0214,0,0.0883769,"created our own topic-specific training corpus in a semi-supervised manner. We developed a set of high precision hashtags for each topic that were used to query the Twitter API in order to create a large training corpus without additional human labeling of tweets for stance. The hashtags and boolean combinations of hashatgs selected for each topic were predicted to be stancebearing on their own. See Table 2. There has been considerable previous work on stance classification in online forums and in congressional debates (Thomas et al., 2006; Burfoot et al., 2011; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012c; Sridhar et al., 2015; Hasan and Ng, 2013; Boltuzic ˇ and Snajder, 2014; Hasan and Ng, 2014). A number of these studies show that collective classification approaches perform well, and that the context (Walker et al., 2012c; Abbott et al., 2011), and meta information such as author constraints are useful for stance classification (Hassan et al., 2012; Hasan and Ng, 2014). Collective classification is not possible in the current task because the only information provided is the text of each individual 421 tweet. Inspired by earlier work (Joshi and PensteinRos´e, 2009; Som"
S16-1068,P15-1012,1,0.835436,"ers may assert their stance using factual or emotional content, and due to their restricted length, tweets may not be well structured or coherent. As a result, NLP tools trained on wellstructured text do not work well in Twitter (Dey and Haque, 2008), and new tools are constantly being developed (Qadir and Riloff, 2014; Kong et al., 2014; Han and Baldwin, 2011; Zhu et al., 2014). Our approach to stance classification in tweets is primarily based on developing a suite of tools for processing Twitter that mirrors our previous work on stance classification in online forums (Walker et al., 2012c; Sridhar et al., 2015; Anand et al., 2011; Walker et al., 2012b; Misra and Walker, 2015). We develop generalized dependency features that capture expressed sentiment or attitude towards particular targets, using the Tweebo dependency parser (Kong et al., 2014). Given the small size of the official task dataset, we created our own topic-specific training corpus in a semi-supervised manner. We developed a set of high precision hashtags for each topic that were used to query the Twitter API in order to create a large training corpus without additional human labeling of tweets for stance. The hashtags and boolean comb"
S16-1068,W06-1639,0,0.0603411,"Kong et al., 2014). Given the small size of the official task dataset, we created our own topic-specific training corpus in a semi-supervised manner. We developed a set of high precision hashtags for each topic that were used to query the Twitter API in order to create a large training corpus without additional human labeling of tweets for stance. The hashtags and boolean combinations of hashatgs selected for each topic were predicted to be stancebearing on their own. See Table 2. There has been considerable previous work on stance classification in online forums and in congressional debates (Thomas et al., 2006; Burfoot et al., 2011; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012c; Sridhar et al., 2015; Hasan and Ng, 2013; Boltuzic ˇ and Snajder, 2014; Hasan and Ng, 2014). A number of these studies show that collective classification approaches perform well, and that the context (Walker et al., 2012c; Abbott et al., 2011), and meta information such as author constraints are useful for stance classification (Hassan et al., 2012; Hasan and Ng, 2014). Collective classification is not possible in the current task because the only information provided is the text of each"
S16-1068,walker-etal-2012-corpus,1,0.936813,"Missing"
S16-1068,N12-1072,1,0.941432,"eyes et al., 2012). Users may assert their stance using factual or emotional content, and due to their restricted length, tweets may not be well structured or coherent. As a result, NLP tools trained on wellstructured text do not work well in Twitter (Dey and Haque, 2008), and new tools are constantly being developed (Qadir and Riloff, 2014; Kong et al., 2014; Han and Baldwin, 2011; Zhu et al., 2014). Our approach to stance classification in tweets is primarily based on developing a suite of tools for processing Twitter that mirrors our previous work on stance classification in online forums (Walker et al., 2012c; Sridhar et al., 2015; Anand et al., 2011; Walker et al., 2012b; Misra and Walker, 2015). We develop generalized dependency features that capture expressed sentiment or attitude towards particular targets, using the Tweebo dependency parser (Kong et al., 2014). Given the small size of the official task dataset, we created our own topic-specific training corpus in a semi-supervised manner. We developed a set of high precision hashtags for each topic that were used to query the Twitter API in order to create a large training corpus without additional human labeling of tweets for stance. The ha"
S16-1068,S14-2077,0,0.0268555,"challenges of the tweet genre. Tweets are often highly informal with language that is colorful and ungrammatical. They may also involve sarcasm, making opinion-mining tasks more challenging (Riloff et al., 2013; Reyes et al., 2012). Users may assert their stance using factual or emotional content, and due to their restricted length, tweets may not be well structured or coherent. As a result, NLP tools trained on wellstructured text do not work well in Twitter (Dey and Haque, 2008), and new tools are constantly being developed (Qadir and Riloff, 2014; Kong et al., 2014; Han and Baldwin, 2011; Zhu et al., 2014). Our approach to stance classification in tweets is primarily based on developing a suite of tools for processing Twitter that mirrors our previous work on stance classification in online forums (Walker et al., 2012c; Sridhar et al., 2015; Anand et al., 2011; Walker et al., 2012b; Misra and Walker, 2015). We develop generalized dependency features that capture expressed sentiment or attitude towards particular targets, using the Tweebo dependency parser (Kong et al., 2014). Given the small size of the official task dataset, we created our own topic-specific training corpus in a semi-supervise"
S16-1068,W13-4006,1,\N,Missing
swanson-etal-2014-getting,walker-etal-2012-corpus,1,\N,Missing
swanson-etal-2014-getting,D08-1027,0,\N,Missing
swanson-etal-2014-getting,D09-1030,0,\N,Missing
swanson-etal-2014-getting,filatova-2012-irony,0,\N,Missing
swanson-etal-2014-getting,P11-2102,0,\N,Missing
swanson-etal-2014-getting,I11-1068,0,\N,Missing
swanson-etal-2014-getting,D13-1066,0,\N,Missing
W00-0304,C00-1073,1,0.868515,"Missing"
W02-0221,P98-1052,0,0.0660896,"Missing"
W02-0221,hastie-etal-2002-automatic,1,0.82665,"are available. 1 Introduction Recent research on dialogue is based on the assumption that dialogue acts provide a useful way of characterizing dialogue behaviors in both humanhuman (HH) and human-computer (HC) dialogue (Isard and Carletta, 1995; Shriberg et al., 2000; Di Eugenio et al., 1998; Cattoni et al., 2001). Previous research has used dialogue act tagging for tasks such as improving recognition performance (Shriberg et al., 2000), identifying important parts of a dialogue (Finke et al., 1998), evaluating and comparing spoken dialogue systems (Walker et al., 2001c; Cattoni et al., 2001; Hastie et al., 2002), as a constraint on nominal expression generation (Jordan, 2000), and for comparing HH to HC dialogues (Doran et al., 2001). Our work builds directly on the previous application of the DATE (Dialogue Act Tagging for Evaluation) tagging scheme to the evaluation and comparison of DARPA Communicator dialogues. The hypothesis underlying the use of dialogue act tagging in spoken dialogue evaluation is that a system’s dialogue behaviors have a strong effect on its usability. Because Communicator systems have unique dialogue strategies, and a unique way of representing and achieving particular commu"
W02-0221,P98-2188,0,0.0727287,"me domain with high accuracy; (2) A DATE tagger trained on data from an earlier version of the system only achieves moderate accuracy on a later version of the system without a small amount labelled training data from that later version; (3) Labelled training data from HC dialogues can improve the performance of a DATE tagger for HH dialogue when only a small amount of HH training data is available. Previous work has also reported results for dialogue act taggers, using similar features to those we use, with accuracies ranging from 62 to 75 (Reithinger and Klesen, 1997; Shriberg et al., 2000; Samuel et al., 1998). Our best accuracy for the HC data is 98 . The best performance for the HH corpus is 76 accuracy for the cross-validation study using only HH data. However, accuracies reported for previous work are not directly comparable to ours for several reasons. First, some of our results concern labelling the system side of utterances in HC dialogues for the purpose of automatic evaluation of system performance. It is much easier to develop a high accuracy tagger for HC dialogue than it is for HH dialogue. We also applied the DATE tagger to HH dialogue, and focused on the travel agent side of the dialo"
W02-0221,P97-1035,1,0.718575,"que way of representing and achieving particular communicative goals, DATE was developed to consistently label dialogue behaviors across systems so that the potential utility of dialogue act tagging could be explored. In previous work, Walker and Passonneau defined the DATE scheme, and labelled the system utterances in the June 2000 data collection of 663 dialogues from nine participating Communicator systems (Walker et al., 2001c; Walker et al., 2001a). They then derived dialogue act metrics from the DATE tags and showed that when these metrics were used in the PARADISE evaluation framework (Walker et al., 1997) that they improved models of user satisfaction by an absolute 5 , and that the new metrics could be used to understand which system’s dialogue strategies were most effective. A major part of evaluation effort using dialogue act tagging, however, is to actually label the dialogues with the dialogue act tags. In previous work (Walker et al., 2001c), the DATE labelling of the June-2000 corpus was done using a semi-automatic method that involved collection of a large number of utterance patterns from the different sites participating in the collection and subsequent hand labelling of these patter"
W02-0221,W01-1607,0,0.242704,"of characterizing dialogue behaviors in both humanhuman (HH) and human-computer (HC) dialogue (Isard and Carletta, 1995; Shriberg et al., 2000; Di Eugenio et al., 1998; Cattoni et al., 2001). Previous research has used dialogue act tagging for tasks such as improving recognition performance (Shriberg et al., 2000), identifying important parts of a dialogue (Finke et al., 1998), evaluating and comparing spoken dialogue systems (Walker et al., 2001c; Cattoni et al., 2001; Hastie et al., 2002), as a constraint on nominal expression generation (Jordan, 2000), and for comparing HH to HC dialogues (Doran et al., 2001). Our work builds directly on the previous application of the DATE (Dialogue Act Tagging for Evaluation) tagging scheme to the evaluation and comparison of DARPA Communicator dialogues. The hypothesis underlying the use of dialogue act tagging in spoken dialogue evaluation is that a system’s dialogue behaviors have a strong effect on its usability. Because Communicator systems have unique dialogue strategies, and a unique way of representing and achieving particular communicative goals, DATE was developed to consistently label dialogue behaviors across systems so that the potential utility of"
W02-0221,N01-1003,1,0.902299,"small amounts of human-human training data are available. 1 Introduction Recent research on dialogue is based on the assumption that dialogue acts provide a useful way of characterizing dialogue behaviors in both humanhuman (HH) and human-computer (HC) dialogue (Isard and Carletta, 1995; Shriberg et al., 2000; Di Eugenio et al., 1998; Cattoni et al., 2001). Previous research has used dialogue act tagging for tasks such as improving recognition performance (Shriberg et al., 2000), identifying important parts of a dialogue (Finke et al., 1998), evaluating and comparing spoken dialogue systems (Walker et al., 2001c; Cattoni et al., 2001; Hastie et al., 2002), as a constraint on nominal expression generation (Jordan, 2000), and for comparing HH to HC dialogues (Doran et al., 2001). Our work builds directly on the previous application of the DATE (Dialogue Act Tagging for Evaluation) tagging scheme to the evaluation and comparison of DARPA Communicator dialogues. The hypothesis underlying the use of dialogue act tagging in spoken dialogue evaluation is that a system’s dialogue behaviors have a strong effect on its usability. Because Communicator systems have unique dialogue strategies, and a unique way o"
W02-0221,P01-1066,1,0.911577,"Missing"
W02-0221,C98-2183,0,\N,Missing
W02-0221,C98-1051,0,\N,Missing
W07-2308,A97-1039,0,\N,Missing
W11-0702,C08-2004,0,0.10119,"Missing"
W11-0702,U08-1003,0,0.0938964,"Missing"
W11-0702,de-marneffe-etal-2006-generating,0,0.0501087,"Missing"
W11-0702,P97-1011,0,0.0145122,"Missing"
W11-0702,P04-1085,0,0.769037,"Missing"
W11-0702,J93-3003,0,0.21392,"y quoted, whether the quoted post is by the same author as the response (there were only an handful of these), whether the response mentions the quote author by name, and whether the response is longer than the quote. The forum software effectively does this annotation for us so there is no reason not to consider it as a clue in our quest to understand and interpret online dialogue. Discourse Markers. Previous work on dialogue analysis has repeatedly noted the discourse functions of particular discourse markers, and our corpus analysis above also suggests their use in this particular dataset (Hirschberg and Litman, 1993; Fox Tree, 2010; Schiffrin, 1987; Di Eugenio et al., 1997; Moser and Moore, 1995). However, because discourse markers can be stacked up Oh, so really we decided to represent this feature as post initial unigrams, bigrams and trigrams. Repeated Punctuation. Informal analyses of our data suggested that repeated sequential use of particular types of punctuation such as !! and ?? did not mean the same thing as simple counts or frequencies of punctuation across a whole post. Thus we developed distinct features for a subset of these repetitions. LIWC. We also derived features using the Linguistics"
W11-0702,P09-2079,0,0.121988,"Missing"
W11-0702,P03-1054,0,0.00716055,"ross a whole post. Thus we developed distinct features for a subset of these repetitions. LIWC. We also derived features using the Linguistics Inquiry Word Count tool (LIWC-2001) (Pennebaker et al., 2001). LIWC classifies words into 69 categories and counts how many words get classified into each category. Some LIWC features that we expect to be important are words per sentence (WPS), pronominal forms, and positive and negative emotion words. Dependency and Generalized Dependency. We used the Stanford parser to extract dependency features for each quote and response (De Marneffe et al., 2006; Klein and Manning, 2003). The dependency parse for a given sentence is a set of triples, composed of a grammatical relation and the pair of words for which the grammatical relation holds (reli , wj , wk ), where reli is the dependency relation among words wj and wk . The word wj is the HEAD of the dependency relation. Feature type Meta Initial n-gram Bigram Dependency Opinion Dependency Annotations Selected Features number-of-other-quotes, percent-quoted, author-quoteUSERNAME yes, so, I agree, well said, really?, I don’t know that you, ? -nil-, you have, evolution is dep-nsubj(agree, i), dep-nsubj(think, you), dep-pr"
W11-0702,P95-1018,0,0.0514645,"y an handful of these), whether the response mentions the quote author by name, and whether the response is longer than the quote. The forum software effectively does this annotation for us so there is no reason not to consider it as a clue in our quest to understand and interpret online dialogue. Discourse Markers. Previous work on dialogue analysis has repeatedly noted the discourse functions of particular discourse markers, and our corpus analysis above also suggests their use in this particular dataset (Hirschberg and Litman, 1993; Fox Tree, 2010; Schiffrin, 1987; Di Eugenio et al., 1997; Moser and Moore, 1995). However, because discourse markers can be stacked up Oh, so really we decided to represent this feature as post initial unigrams, bigrams and trigrams. Repeated Punctuation. Informal analyses of our data suggested that repeated sequential use of particular types of punctuation such as !! and ?? did not mean the same thing as simple counts or frequencies of punctuation across a whole post. Thus we developed distinct features for a subset of these repetitions. LIWC. We also derived features using the Linguistics Inquiry Word Count tool (LIWC-2001) (Pennebaker et al., 2001). LIWC classifies wor"
W11-0702,C10-2100,0,0.367968,"Missing"
W11-0702,P09-1026,0,0.552979,"uld reasonably be expected to be recognized independently of Agreement/Disagreement. 8 Figure 4: Sample model learned using JRip. The numbers represent (total instances covered by a rule / number incorrectly labeled). This particular model was built on development data. 4 Results Table 3 shows features which were selected for each of our feature categories using a χ2 test for feature selection. These results vindicate our interest in discourse markers as cues to argument structure, as well as the importance of the generalized dependency features and opinion target pairs (Wang and Ros´e, 2010; Somasundaran and Wiebe, 2009). Figure 4 shows a sample model learned using JRip. We limit our pair-wise comparisons between classifiers and feature sets to those corresponding to parFeats Uni,UniCue BOW Meta Response Local Quote Local Both Local Meta+Local All Just Annotations All+Annotations NB 0.578 0.598 0.579 0.600 0.531 0.601 0.603 0.603 0.765 0.603 JRipχ2 0.626 0.654 0.588 0.666 0.588 0.682 0.654 0.632 0.814 0.795 Table 4: Accuracies on a balanced test set (random baseline: 0.5). NB = NaiveBayes. JRipχ2 = Jripper with χ2 feature selection on the training set during cross validation. BOW = Unigrams, CueWords, Bigrams"
W11-0702,W10-0214,0,0.473961,"pendencies Description/Examples Non-lexical features. E.g. posterid, time between posts, etc. Word and Word Pair frequencies Initial unigram, bigram, and trigram Collapsed into one of the following: ??, !!, ?! LIWC measures and frequencies Dependencies derived from the Stanford Parser. Dependency features generalized with respect to POS of the head word and opinion polarity of both words. Table 2: Feature Sets, Descriptions, and Examples Unigrams, Bigrams, Trigrams. Results of previous work suggest that a unigram baseline can be difficult to beat for certain types of debates (Walker et al., ; Somasundaran and Wiebe, 2010). Thus we 7 derived both unigrams and bigrams as features. We captured the final token as a feature by padding with -nil- tokens when building the bigrams. See below for comments on initial uni/bi/tri-grams. MetaPost Info. Previous work suggested that non-lexical features like poster ids and the time between posts might contain indicators of disagreement. People on these forums get to know one another and often enjoy repeatedly arguing with the same person. In addition, we hypothesized that the “heat” of a particular conversation could be correlated with rapid-fire exchanges, as indicated by s"
W11-0702,W06-1639,0,0.558623,"Missing"
W11-0702,N10-1097,0,0.0829325,"Missing"
W11-1701,W11-0702,1,0.772117,"Missing"
W11-1701,de-marneffe-etal-2006-generating,0,0.0269903,"Missing"
W11-1701,N09-1057,0,0.122394,"ng and following the one you mention. We have no way to determine what of those contributed to a lower murder rate, if indeed there was one. You have to prove a cause and effect relationship and you have failed. Figure 1: Capital Punishment discussions with posts linked via rebuttal links. Introduction Recent work has highlighted the challenges of identifying the STANCE that a speaker holds towards a particular political, social or technical topic. Classifying stance involves identifying a holistic subjective disposition, beyond the word or sentence (Lin et al., 2006; Malouf and Mullen, 2008; Greene and Resnik, 2009; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). Our work is inspired by the large variety of such conversations now freely available online, and our observation that the contextual affordances of different debate and discussion This paper utilizes 1113 two-sided debates (4873 posts) from Convinceme.net for 14 different debate topics. See Table 1. On Convinceme, a person starts a debate by posting a topic or a question and providing sides such as for vs. against. Debate participants can then post arguments for one side or the other, essentially self-labelling their post for stanc"
W11-1701,P09-2079,0,0.0258253,"Missing"
W11-1701,P03-1054,0,0.0211557,"suggests the utility of dependency structure to determine the TARGET of an opinion word (Joshi and Penstein-Ros´e, 2009; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). The dependency parse for a given sentence is a set of triples, composed of a grammatical relation and the pair of words for which the grammatical relation holds (reli , wj , wk ), where reli is the dependency relation among words wj and wk . The word wj is the HEAD of the dependency relation. We use the Stanford parser to parse the utterances in the posts and extract dependency features (De Marneffe et al., 2006; Klein and Manning, 2003). Generalized Dependency. To create generalized dependencies, we “back off” the head word in each of the above features to its part-of-speech tag (Joshi and Penstein-Ros´e, 2009). Joshi & Rose’s results suggested that this approach would work better than either fully lexicalized or fully generalized dependency features. We call these POS generalized dependencies in the results below. Opinion Dependencies. Somasundaran & Wiebe (2009) introduced features that identify the TAR GET of opinion words. Inspired by this approach, we used the MPQA dictionary of opinion words to select the subset of dep"
W11-1701,W06-2915,0,0.0986309,"ngs were different than the periods preceding and following the one you mention. We have no way to determine what of those contributed to a lower murder rate, if indeed there was one. You have to prove a cause and effect relationship and you have failed. Figure 1: Capital Punishment discussions with posts linked via rebuttal links. Introduction Recent work has highlighted the challenges of identifying the STANCE that a speaker holds towards a particular political, social or technical topic. Classifying stance involves identifying a holistic subjective disposition, beyond the word or sentence (Lin et al., 2006; Malouf and Mullen, 2008; Greene and Resnik, 2009; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). Our work is inspired by the large variety of such conversations now freely available online, and our observation that the contextual affordances of different debate and discussion This paper utilizes 1113 two-sided debates (4873 posts) from Convinceme.net for 14 different debate topics. See Table 1. On Convinceme, a person starts a debate by posting a topic or a question and providing sides such as for vs. against. Debate participants can then post arguments for one side or the othe"
W11-1701,C10-2100,0,0.497098,"Missing"
W11-1701,P09-1026,0,0.688194,"you mention. We have no way to determine what of those contributed to a lower murder rate, if indeed there was one. You have to prove a cause and effect relationship and you have failed. Figure 1: Capital Punishment discussions with posts linked via rebuttal links. Introduction Recent work has highlighted the challenges of identifying the STANCE that a speaker holds towards a particular political, social or technical topic. Classifying stance involves identifying a holistic subjective disposition, beyond the word or sentence (Lin et al., 2006; Malouf and Mullen, 2008; Greene and Resnik, 2009; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). Our work is inspired by the large variety of such conversations now freely available online, and our observation that the contextual affordances of different debate and discussion This paper utilizes 1113 two-sided debates (4873 posts) from Convinceme.net for 14 different debate topics. See Table 1. On Convinceme, a person starts a debate by posting a topic or a question and providing sides such as for vs. against. Debate participants can then post arguments for one side or the other, essentially self-labelling their post for stance. These debates may be heated"
W11-1701,W10-0214,0,0.614766,"determine what of those contributed to a lower murder rate, if indeed there was one. You have to prove a cause and effect relationship and you have failed. Figure 1: Capital Punishment discussions with posts linked via rebuttal links. Introduction Recent work has highlighted the challenges of identifying the STANCE that a speaker holds towards a particular political, social or technical topic. Classifying stance involves identifying a holistic subjective disposition, beyond the word or sentence (Lin et al., 2006; Malouf and Mullen, 2008; Greene and Resnik, 2009; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). Our work is inspired by the large variety of such conversations now freely available online, and our observation that the contextual affordances of different debate and discussion This paper utilizes 1113 two-sided debates (4873 posts) from Convinceme.net for 14 different debate topics. See Table 1. On Convinceme, a person starts a debate by posting a topic or a question and providing sides such as for vs. against. Debate participants can then post arguments for one side or the other, essentially self-labelling their post for stance. These debates may be heated and emotional, discussing weig"
W11-1701,N10-1097,0,0.0618085,"Missing"
W13-1104,filatova-2012-irony,0,0.145484,"dev” (Table 1). Turkers were presented with utterances previously labeled sarcastic or nasty in IAC by 7 different Turkers, and were told “In a previous study, these responses were identified as being sarcastic by 3 out of 4 Turkers. For each quote/response pair, we will ask you to identify sarcastic or potentially sarcastic phrases in the response”. The Turkers then selected words or phrases from the response they believed could lead someone to believing the utterance was sarcastic or nasty. These utterances were not used again in further experiments. This crowdsourcing method is similar to (Filatova, 2012), but where their data is monologic, ours is dialogic. 4.1 Results from Indicator Cues Sarcasm is known to be highly variable in form, and to depend, in some cases, on context for its interpretation (Sperber and Wilson, 1981; Gibbs, 2000; Bryant and Fox Tree, 2002). We conducted an initial pilot on 100 of the 617 sarcastic utterances in χ2 like them too oh mean just make χ2 of the you mean yes, oh, you are like a I think χ2 to tell me would deny a like that? mean to tell sounds like a you mean to to deal with unigram MT idiot unfounded babbling lie selfish nonsense hurt bigram MT don’t expect"
W13-1104,W11-2606,0,0.019563,"Missing"
W13-1104,P11-2102,0,0.527603,"Missing"
W13-1104,W11-1715,0,0.0430215,"Missing"
W13-1104,W03-1014,0,0.12388,"Missing"
W13-1104,D08-1027,0,0.0844983,"Missing"
W13-1104,W02-1028,0,0.131801,"Missing"
W13-1104,walker-etal-2012-corpus,1,0.12054,"Missing"
W13-1104,P99-1032,0,0.125065,"Missing"
W13-1104,H05-2018,0,0.0739768,"ents. The left circle of Fig. 2 reflects the assumption that there are Sarcasm or Nasty Cues that can identify the category of interest with high precision (R&W call this the “Known Subjective Vocabulary”). The aim of first developing a high precision classifier, at the expense of recall, is to select utterances that are reliably of the category of interest from unannotated text. This is needed to ensure that the generalization step of “Extraction Pattern Learner” does not introduce too much noise. R&W did not need to develop a “Known Subjective Vocabulary” because previous work provided one (Wilson et al., 2005; Wiebe et al., 1999; Wiebe et al., 2003). Thus, our first question with applying R&W’s method to our data was whether or not it is possible to develop a reliable set of Sarcasm (Nastiness) Cues (O1 below). Two factors suggest that it might not be. First, R&W’s method assumes that the cues are in the utterance to be classified, but it has been claimed that sarcasm (1) is context dependent, and (2) requires world knowledge to recognize, 32 SARCASM MT exp dev HP train HP dev test PE eval All #sarc 617 1407 1614 1616 5254 #notsarc NA 1404 1614 1616 4635 total 617 2811 3228 3232 9889 NASTY MT exp"
W13-4006,P04-1085,0,0.896621,"PTANCE , i.e. (dis)agreement, independently of the topic. We assume that it will not always be possible to get annotated data for a particular topic, given the ever-burgeoning range of topics discussed online. We use the Evolution topic as our development set, and ask: given (dis)agreement annotations for only one topic, is it possible to develop features that perform well on another arbitrary topic? There is limited previous research on disagreement, thus it is an open issue what types of features might be useful. One line of previous work suggests that various pragmatic features might help (Galley et al., 2004). Another line suggests that disagreement is subtype of the COMPARISON (CONTRAST) discourse relation, in the Penn Discourse TreeBank taxonomy, suggesting that features for identifying COMPARISON, such as polarity and discourse cues might also be useful (Hahn et al., 2006; Prasad et al., 2010; Louis et al., 2010). We began by selecting and manually inspecting 460 agreements and 460 disagreements from the Evolution topic, and extracting their most frequent unigrams, bigrams and trigrams. This showed that features suggested by theoretical work on rejection were indeed highly frequent: our aim was"
W13-4006,W11-0702,1,0.860225,"Missing"
W13-4006,P12-1042,0,0.492525,"Missing"
W13-4006,N06-2014,0,0.458563,"n (dis)agreement annotations for only one topic, is it possible to develop features that perform well on another arbitrary topic? There is limited previous research on disagreement, thus it is an open issue what types of features might be useful. One line of previous work suggests that various pragmatic features might help (Galley et al., 2004). Another line suggests that disagreement is subtype of the COMPARISON (CONTRAST) discourse relation, in the Penn Discourse TreeBank taxonomy, suggesting that features for identifying COMPARISON, such as polarity and discourse cues might also be useful (Hahn et al., 2006; Prasad et al., 2010; Louis et al., 2010). We began by selecting and manually inspecting 460 agreements and 460 disagreements from the Evolution topic, and extracting their most frequent unigrams, bigrams and trigrams. This showed that features suggested by theoretical work on rejection were indeed highly frequent: our aim was to generalize what we observed in the Evolution dataset and then test whether the generalized features can distinguish agreements from disagreements. We first observed that very few unigrams Cue Words Description Ngrams indicative of accepting others claim. Cues as Ngra"
W13-4006,P88-1021,0,0.372765,"is logically consistent with the original assertion. Walker argues that the fact that an implicature can function as a rejection clearly indicates that inference rules about what gets added to the common ground must have the same logical status as implicatures, i.e. they must be default rules of inference that can be defeated by context. She then goes on to identify additional types of rejections in HGC that rely on detecting conflicts in the default inferences triggered by the epistemic inference rules used in speech act theory. Walker uses a compressed version of rules from (Perrault, 1990; Appelt and Konolige, 1988), assuming that conflicting defaults can arise between these inferences and implicature inferences (Hirschberg, 1985). The first rule is given in 1: ment on an 11 point scale [-5,5] implemented with a slider. The annotators were also able to signal uncertainty with a CAN ’ T TELL option. Each of the pairs was annotated by 5-7 annotators, in response to the annotation question Does the respondent agree or disagree with the prior post?. Annotators achieved high agreement on dis(agreement) annotation with an α of 0.62. We used thresholds of 1 and -1 on the mean agreement judgment to determine agr"
W13-4006,D10-1121,0,0.0308046,"Missing"
W13-4006,C08-2002,0,0.0287503,"om the Evolution topic. Pitler et al, (2009) also used ngrams consisting of the first and last three words for recognition of the PDTB COMPARISON relation. Other work on the PDTB also suggests that DENIAL can be indicated by contrast (Webber and Prasad, 2008). Cue Words. Both psychological research on discourse processes (Fox Tree and Schrock, 1999; Groen et al., 2010) and computational work on agreement and discourse markers (Galley et al., 2004; Louis et al., 2010) indicate that discourse markers are strongly associated with particular pragmatic functions such as stating a personal opinion (Asher et al., 2008; Webber and Prasad, 2008). Based on manual inspection of the Evolution devset we selected 18 items for the CUE WORDS feature set, as in Table 3. Examples are well in R2 and so and but in R5. Durational Features. Brown and Levinson’s theory of politeness would suggest that disagreements are dispreferred responses and thus that the length of the post could indicate disagreement; it predicts that people will elaborate more and provide reasons and justifications for disagreement (Brown and Levinson, 1987). Our durational features measure the length of the utterance in terms of characters, words a"
W13-4006,C08-2004,0,0.0693371,"Missing"
W13-4006,W10-4310,0,0.102547,"e topic, is it possible to develop features that perform well on another arbitrary topic? There is limited previous research on disagreement, thus it is an open issue what types of features might be useful. One line of previous work suggests that various pragmatic features might help (Galley et al., 2004). Another line suggests that disagreement is subtype of the COMPARISON (CONTRAST) discourse relation, in the Penn Discourse TreeBank taxonomy, suggesting that features for identifying COMPARISON, such as polarity and discourse cues might also be useful (Hahn et al., 2006; Prasad et al., 2010; Louis et al., 2010). We began by selecting and manually inspecting 460 agreements and 460 disagreements from the Evolution topic, and extracting their most frequent unigrams, bigrams and trigrams. This showed that features suggested by theoretical work on rejection were indeed highly frequent: our aim was to generalize what we observed in the Evolution dataset and then test whether the generalized features can distinguish agreements from disagreements. We first observed that very few unigrams Cue Words Description Ngrams indicative of accepting others claim. Cues as Ngrams and their LIWC CogMech generalizations"
W13-4006,P02-1047,0,0.0899759,"Missing"
W13-4006,U08-1003,0,0.194691,"Missing"
W13-4006,C10-2100,0,0.217283,"Missing"
W13-4006,W00-1407,0,0.110277,"Missing"
W13-4006,P09-1077,0,0.00797096,"suggested that agreements have few topic independent markers. Unigrams such as agree correct and right were also present in disagreements, and trigrams such as I agree but, You may be correct however I do not agree, I don’t agree were better indicators of disagreement. Our agreement markers are thus a small category where we check that the keywords agree, correct and right are not preceded by a negation marker and not followed by discourse markers such as but, yet, or however. However, the denial category at present has more than 300 ngrams extracted and generalized from the Evolution topic. Pitler et al, (2009) also used ngrams consisting of the first and last three words for recognition of the PDTB COMPARISON relation. Other work on the PDTB also suggests that DENIAL can be indicated by contrast (Webber and Prasad, 2008). Cue Words. Both psychological research on discourse processes (Fox Tree and Schrock, 1999; Groen et al., 2010) and computational work on agreement and discourse markers (Galley et al., 2004; Louis et al., 2010) indicate that discourse markers are strongly associated with particular pragmatic functions such as stating a personal opinion (Asher et al., 2008; Webber and Prasad, 2008)"
W13-4006,walker-etal-2012-corpus,1,0.818776,"Missing"
W13-4006,N12-1072,1,0.905039,"Introduction Research on the structure of dialogue has been hampered for years because large dialogue corpora have not been publicly available. This has impacted the dialogue research community’s ability to develop better theories, as well as good off-theshelf tools for dialogue processing that account for the richness of human dialogue. Happily, an increasing amount of information and opinion exchange occurs in natural dialogue in online forums, where people can express their opinion on a vast range of topics from Should there be more stringent gun laws? to Are school uniforms a good idea? (Walker et al., 2012a). For example, consider the dialogic exchange in Fig. 1. 41 Proceedings of the SIGDIAL 2013 Conference, pages 41–50, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics Type DENIAL LOGICAL CONTRADICTION IMPLICIT DENIAL REFUSAL IMPLICATURE REJECTION DENYING BELIEF TRANSFER INCONSISTENT PAST BELIEF CITING CONTRADICTORY AUTHORITY Context Pigs can fly. Kim and Lee have been partners since 1989. Julia’s daughter is a genius. Come and play ball with me. There’s a man in the garage. B: Well ah he uh ... he belongs to a money market fund now and uh they will do that for"
W13-4006,prasad-etal-2010-exploiting,0,0.0143161,"notations for only one topic, is it possible to develop features that perform well on another arbitrary topic? There is limited previous research on disagreement, thus it is an open issue what types of features might be useful. One line of previous work suggests that various pragmatic features might help (Galley et al., 2004). Another line suggests that disagreement is subtype of the COMPARISON (CONTRAST) discourse relation, in the Penn Discourse TreeBank taxonomy, suggesting that features for identifying COMPARISON, such as polarity and discourse cues might also be useful (Hahn et al., 2006; Prasad et al., 2010; Louis et al., 2010). We began by selecting and manually inspecting 460 agreements and 460 disagreements from the Evolution topic, and extracting their most frequent unigrams, bigrams and trigrams. This showed that features suggested by theoretical work on rejection were indeed highly frequent: our aim was to generalize what we observed in the Evolution dataset and then test whether the generalized features can distinguish agreements from disagreements. We first observed that very few unigrams Cue Words Description Ngrams indicative of accepting others claim. Cues as Ngrams and their LIWC Cog"
W13-4006,W03-1014,0,0.0609446,"Missing"
W13-4006,P11-2065,0,0.386829,"Missing"
W13-4006,P09-1026,0,0.317256,"Missing"
W13-4006,W10-0214,0,0.246306,"ms. seems to me, my view, actually, my opinion, essentially, somewhat, my perspective, rather, although, really, I suppose, perhaps Duration Sentence, word and post lengths Polarity Means of positive and negative polarity terms. Punctuation Counts of question marks and exclamation points. Table 3: Feature Sets, Descriptions, and Examples. The unigrams features are our baseline case; these features are not theoretically motivated. Unigrams. Results of previous work on stance identification in argumentative discourse suggest that a unigram baseline can be difficult to beat (Thomas et al., 2006; Somasundaran and Wiebe, 2010). Thus we test our theoretically motivated features against unfiltered unigrams and un1 Since participants are not generally making plans together in these dialogues, we leave aside Walker’s classification of rejections of proposals. 45 feature for distinguishing (dis)agreement, yielding the hedge features in Table 3. Polarity. Work on discourse relations in the PDTB also suggests that differences in polarity across adjacent utterances might be an indicator of the COMPARISON relation. In addition, Horn’s classes of REJECTIONS shown in Fig. 2 all include markers of negation. Thus to capture the"
W13-4006,H05-2018,0,0.0120974,"nce participants are not generally making plans together in these dialogues, we leave aside Walker’s classification of rejections of proposals. 45 feature for distinguishing (dis)agreement, yielding the hedge features in Table 3. Polarity. Work on discourse relations in the PDTB also suggests that differences in polarity across adjacent utterances might be an indicator of the COMPARISON relation. In addition, Horn’s classes of REJECTIONS shown in Fig. 2 all include markers of negation. Thus to capture the overall sentiment of the post we used the MPQA subjectivity lexicon (Wiebe et al., 2003; Wilson et al., 2005). Each word is POS tagged and then categorized as strongly or weakly subjective. The positive polarity feature is the sum of the strongly subjective words of positive polarity, and the negative polarity feature represents the sum of strongly subjective words of negative polarity. Punctuation. Another indication of DENYING BELIEF TRANSFER rejections are the question marks and exclamation marks that conversants frequently use to express their disbelief and doubt about another conversant’s claim. For example, R1 and R5 in Fig. 3 have a high frequency of question marks. igrams+bigrams as baselines"
W13-4006,H05-1116,0,0.0250396,"Missing"
W13-4006,W12-3710,0,0.580443,"Missing"
W13-4006,W02-1028,0,0.0160036,"Missing"
W13-4006,W00-1408,0,0.0664729,"Missing"
W13-4006,W06-1639,0,0.490389,"sibly, anyway, it terms. seems to me, my view, actually, my opinion, essentially, somewhat, my perspective, rather, although, really, I suppose, perhaps Duration Sentence, word and post lengths Polarity Means of positive and negative polarity terms. Punctuation Counts of question marks and exclamation points. Table 3: Feature Sets, Descriptions, and Examples. The unigrams features are our baseline case; these features are not theoretically motivated. Unigrams. Results of previous work on stance identification in argumentative discourse suggest that a unigram baseline can be difficult to beat (Thomas et al., 2006; Somasundaran and Wiebe, 2010). Thus we test our theoretically motivated features against unfiltered unigrams and un1 Since participants are not generally making plans together in these dialogues, we leave aside Walker’s classification of rejections of proposals. 45 feature for distinguishing (dis)agreement, yielding the hedge features in Table 3. Polarity. Work on discourse relations in the PDTB also suggests that differences in polarity across adjacent utterances might be an indicator of the COMPARISON relation. In addition, Horn’s classes of REJECTIONS shown in Fig. 2 all include markers o"
W14-2715,P12-1042,0,0.0906828,"Missing"
W14-2715,W13-4006,1,0.813931,"articipate in multiple discussions in the same topic, and may discuss multiple topics. For example consider the sample posts from the online discussion forum 4forums.com shown in Fig. 1. Here, we see discussion topics, together with sample quotes and responses, where the response is a direct reply to the quote text. The annotations for stance were gathered using Amazon’s Mechanical Turk service with an interface that allowed annotators to see complete discussions. Quotes provide additional context that were used by human annotators in a separate task for annotating agreement and disagreement (Misra and Walker, 2013). Responses can be labeled as either PRO or CON toward the topic. For the example shown in Fig. 1, Online debate sites are a large source of informal and opinion-sharing dialogue on current socio-political issues. Inferring users’ stance (PRO or CON) towards discussion topics in domains such as politics or news is an important problem, and is of utility to researchers, government organizations, and companies. Predicting users’ stance supports identification of social and political groups, building of better recommender systems, and personalization of users’ information preferences to their ide"
W14-2715,C10-2100,0,0.382092,"e 10 train/test splits. For the PSL model, the measures are computed for joint inference over all topics in the test sets. For the per-topic linear SVMs (LOCAL model), we compute the measures individually for the predictions of each topic in the test sets and take a weighted average over the 5 Related Work Over the last ten years, there has been significant progress on modeling stance. Previous work covers three different debate settings: (1) congressional debates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Walker et al., 2012c; Anand et al., 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites because the posts are 114 Classifier LOCAL PSL F1 Score 0.66 ± 0.015 0.74 ± 0.04 AUC-PR negative class 0.44 ± 0.04 0.511 ± 0.04 AUROC 0.54 ± 0.02 0.59 ± 0.05 Table 3: Averages and standard deviations for F1 score for the positive class, area under PR curve for the"
W14-2715,C08-2004,0,0.289481,"j . Using the tree structure and posts that have annotations for agreement or disagreement, we conWe believe that models of post stance in online debate should capture both the content and the context of author posts. By jointly reasoning over both the content of the post and its relationships with other posts in the discussion, we perform collective classification, as we further define in Section 3 (Sen et al., 2008). Previous work has shown that collective classification models often perform better than content-only approaches. (Burfoot et al., 2011; Hasan and Ng, 2013; Thomas et al., 2006; Bansal et al., 2008; Walker et al., 2012c). Here, we develop a collective classification approach for stance prediction which leverages the sentiment conveyed in a post through its language, and the reply links consisting of agreements or rebuttals between posts in a discussion. We implement our approach using Probabilistic Soft Logic (PSL) (Bach et al., 2013), a recently introduced tool for collective inference in relational data. We evaluate our model on data from the 4F ORUMS online debate site (Walker et al., 2012b). Section 2 first presents an overview of our approach and then in Section 3.1 we describe the"
W14-2715,P09-1026,0,0.354282,"cs in the test sets. For the per-topic linear SVMs (LOCAL model), we compute the measures individually for the predictions of each topic in the test sets and take a weighted average over the 5 Related Work Over the last ten years, there has been significant progress on modeling stance. Previous work covers three different debate settings: (1) congressional debates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Walker et al., 2012c; Anand et al., 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites because the posts are 114 Classifier LOCAL PSL F1 Score 0.66 ± 0.015 0.74 ± 0.04 AUC-PR negative class 0.44 ± 0.04 0.511 ± 0.04 AUROC 0.54 ± 0.02 0.59 ± 0.05 Table 3: Averages and standard deviations for F1 score for the positive class, area under PR curve for the negative class, and area under ROC curve for post stance over 10 train/test splits. isProPost(P, T) ∧ wr"
W14-2715,P11-1151,0,0.0460266,"ates in topic ti if there exist any posts pj ∈ di with author aj . Using the tree structure and posts that have annotations for agreement or disagreement, we conWe believe that models of post stance in online debate should capture both the content and the context of author posts. By jointly reasoning over both the content of the post and its relationships with other posts in the discussion, we perform collective classification, as we further define in Section 3 (Sen et al., 2008). Previous work has shown that collective classification models often perform better than content-only approaches. (Burfoot et al., 2011; Hasan and Ng, 2013; Thomas et al., 2006; Bansal et al., 2008; Walker et al., 2012c). Here, we develop a collective classification approach for stance prediction which leverages the sentiment conveyed in a post through its language, and the reply links consisting of agreements or rebuttals between posts in a discussion. We implement our approach using Probabilistic Soft Logic (PSL) (Bach et al., 2013), a recently introduced tool for collective inference in relational data. We evaluate our model on data from the 4F ORUMS online debate site (Walker et al., 2012b). Section 2 first presents an ov"
W14-2715,W10-0214,0,0.396396,"er-topic linear SVMs (LOCAL model), we compute the measures individually for the predictions of each topic in the test sets and take a weighted average over the 5 Related Work Over the last ten years, there has been significant progress on modeling stance. Previous work covers three different debate settings: (1) congressional debates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Walker et al., 2012c; Anand et al., 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites because the posts are 114 Classifier LOCAL PSL F1 Score 0.66 ± 0.015 0.74 ± 0.04 AUC-PR negative class 0.44 ± 0.04 0.511 ± 0.04 AUROC 0.54 ± 0.02 0.59 ± 0.05 Table 3: Averages and standard deviations for F1 score for the positive class, area under PR curve for the negative class, and area under ROC curve for post stance over 10 train/test splits. isProPost(P, T) ∧ writesPost(A, P) ¬ isProPost(P,"
W14-2715,U08-1003,0,0.0228764,".ucsc.edu Abstract organizations alike. Predicting a user’s stance towards a given issue can support the identification of social or political groups (Gawron et al., 2012; Abu-Jbara et al., 2012; Anand et al., 2011; Qiu et al., 2013; Hasan and Ng, 2013), help develop better recommendation systems, or tailor users’ information preferences to their ideologies and beliefs. Stance classification problems consist of a collection of debate-style discussions by authors on different controversial, political topics. While these may be spoken as in the Congressional Debates corpus (Thomas et al., 2006; Burfoot, 2008), we focus on forum posts on social media debate sites. Users on debate sites share their opinions freely, using informal and social language, providing a rich and much more challenging domain for stance prediction. Social media debate sites contain online discussions with posts from various authors, where each post is either a response to another post or the root of the discussion (Anand et al., 2011; Walker et al., 2012a). Posts are linked to one another by either rebuttal or agreement links and are labelled for stance, either PRO or CON, depending on the framing of the issue under discussio"
W14-2715,W06-1639,0,0.882863,"e.ucsc.edu getoor@soe.ucsc.edu Abstract organizations alike. Predicting a user’s stance towards a given issue can support the identification of social or political groups (Gawron et al., 2012; Abu-Jbara et al., 2012; Anand et al., 2011; Qiu et al., 2013; Hasan and Ng, 2013), help develop better recommendation systems, or tailor users’ information preferences to their ideologies and beliefs. Stance classification problems consist of a collection of debate-style discussions by authors on different controversial, political topics. While these may be spoken as in the Congressional Debates corpus (Thomas et al., 2006; Burfoot, 2008), we focus on forum posts on social media debate sites. Users on debate sites share their opinions freely, using informal and social language, providing a rich and much more challenging domain for stance prediction. Social media debate sites contain online discussions with posts from various authors, where each post is either a response to another post or the root of the discussion (Anand et al., 2011; Walker et al., 2012a). Posts are linked to one another by either rebuttal or agreement links and are labelled for stance, either PRO or CON, depending on the framing of the issue"
W14-2715,walker-etal-2012-corpus,1,0.893355,"Missing"
W14-2715,N12-1072,1,0.321031,"lection of debate-style discussions by authors on different controversial, political topics. While these may be spoken as in the Congressional Debates corpus (Thomas et al., 2006; Burfoot, 2008), we focus on forum posts on social media debate sites. Users on debate sites share their opinions freely, using informal and social language, providing a rich and much more challenging domain for stance prediction. Social media debate sites contain online discussions with posts from various authors, where each post is either a response to another post or the root of the discussion (Anand et al., 2011; Walker et al., 2012a). Posts are linked to one another by either rebuttal or agreement links and are labelled for stance, either PRO or CON, depending on the framing of the issue under discussion. Each post reflects the stance and sentiment of its author. Authors may participate in multiple discussions in the same topic, and may discuss multiple topics. For example consider the sample posts from the online discussion forum 4forums.com shown in Fig. 1. Here, we see discussion topics, together with sample quotes and responses, where the response is a direct reply to the quote text. The annotations for stance were"
W14-2715,I13-1191,0,0.678087,"ere exist any posts pj ∈ di with author aj . Using the tree structure and posts that have annotations for agreement or disagreement, we conWe believe that models of post stance in online debate should capture both the content and the context of author posts. By jointly reasoning over both the content of the post and its relationships with other posts in the discussion, we perform collective classification, as we further define in Section 3 (Sen et al., 2008). Previous work has shown that collective classification models often perform better than content-only approaches. (Burfoot et al., 2011; Hasan and Ng, 2013; Thomas et al., 2006; Bansal et al., 2008; Walker et al., 2012c). Here, we develop a collective classification approach for stance prediction which leverages the sentiment conveyed in a post through its language, and the reply links consisting of agreements or rebuttals between posts in a discussion. We implement our approach using Probabilistic Soft Logic (PSL) (Bach et al., 2013), a recently introduced tool for collective inference in relational data. We evaluate our model on data from the 4F ORUMS online debate site (Walker et al., 2012b). Section 2 first presents an overview of our approa"
W14-2715,N10-1097,0,0.0617879,"Missing"
W14-2715,D10-1102,0,0.236367,"Missing"
W14-4323,elson-mckeown-2010-building,0,0.0808902,"d charmed every one around. I had prepared my “gift” as they wished. Decision witheld, they decided that I neednt to bother, they liked me too much. I should go free. I even managed to meet famous Raus, the big chief. He was too happy to let me go when he realized I was no one. But then, a Major at his side noticed my Visa was expired. Damn! My current Visa is being renewed in my other passport at Immigration’s. Fuck. In custody, for real. Figure 1: An excerpt from an example story from our corpus annotated with the L&W categories. and annotation scheme, such as the one employed by DramaBank (Elson and McKeown, 2010; Elson, 2012) that extends theories of narrative structure and plot units (Stein et al., 2000; Lehnert, 1981), offers many advantages. However, acquiring this level of analysis on user generated content, such as blog stories, is resource intensive. Research on computational models of narrative structure typically focus on inferring the causal and temporal relationships between events (Goyal et al., 2010; Chambers and Jurafsky, 2009; Riaz and Girju, 2010; Beamer and Girju, 2009; Do et al., 2011; Manshadi et al., 2008; Gordon et al., 2011; Hu et al., 2013). Yet L&W point out that stories are no"
W14-4323,P09-1068,0,0.178114,", for real. Figure 1: An excerpt from an example story from our corpus annotated with the L&W categories. and annotation scheme, such as the one employed by DramaBank (Elson and McKeown, 2010; Elson, 2012) that extends theories of narrative structure and plot units (Stein et al., 2000; Lehnert, 1981), offers many advantages. However, acquiring this level of analysis on user generated content, such as blog stories, is resource intensive. Research on computational models of narrative structure typically focus on inferring the causal and temporal relationships between events (Goyal et al., 2010; Chambers and Jurafsky, 2009; Riaz and Girju, 2010; Beamer and Girju, 2009; Do et al., 2011; Manshadi et al., 2008; Gordon et al., 2011; Hu et al., 2013). Yet L&W point out that stories are not just about the events that occur. In fact, L&W say that stories that are only about events are boring. Current methods for inferring narrative structure, including our own (Hu et al., 2013), do not distinguish event clauses from other narrative clause types. But note that actions only constitute about one third of the clauses in the narratives in Fig. 1 and Fig. 4. Sec. 2 provides more detail about L&W’s theory. Sec. 3 describes t"
W14-4323,D11-1027,0,0.0604043,"tated with the L&W categories. and annotation scheme, such as the one employed by DramaBank (Elson and McKeown, 2010; Elson, 2012) that extends theories of narrative structure and plot units (Stein et al., 2000; Lehnert, 1981), offers many advantages. However, acquiring this level of analysis on user generated content, such as blog stories, is resource intensive. Research on computational models of narrative structure typically focus on inferring the causal and temporal relationships between events (Goyal et al., 2010; Chambers and Jurafsky, 2009; Riaz and Girju, 2010; Beamer and Girju, 2009; Do et al., 2011; Manshadi et al., 2008; Gordon et al., 2011; Hu et al., 2013). Yet L&W point out that stories are not just about the events that occur. In fact, L&W say that stories that are only about events are boring. Current methods for inferring narrative structure, including our own (Hu et al., 2013), do not distinguish event clauses from other narrative clause types. But note that actions only constitute about one third of the clauses in the narratives in Fig. 1 and Fig. 4. Sec. 2 provides more detail about L&W’s theory. Sec. 3 describes the annotation experiments and efforts to improve annotation rel"
W14-4323,D10-1008,0,0.29918,"Missing"
W14-4323,D13-1036,1,0.926561,"told online, and are widely available in social media sources such as weblogs. A personal narrative about an arrest is shown in Fig. 1, and one about a protest is in Fig. 4. Narratives such as these provide a valuable resource for learning a wealth of commonsense knowledge about people, the types of activities they engage in, and the attitudes they hold. They are also well suited to learning about causal and temporal relationships between events because narrative interpretation explicitly depends on the coherence of these relationships (Graesser et al., 1994; Elson, 2012; Gordon et al., 2011; Hu et al., 2013). This paper applies and tests a narrative clause labeling scheme to personal narratives. Our scheme is derived from Labov & Waletzky’s (henceforth L&W) theory of oral narrative (Labov, 1997; Labov and Waletzky, 1967). L&W’s theory distinguishes (1) clauses that indicate causal relationships (AC TION ), from (2) clauses that provide traits or properties of the setting or characters (ORIENTATION), from (3) clauses describing the story characters’ emotional reactions to the events (EVALUATION). We adopt L&W’s theory for three reasons. First, we believe that the narrative structure of personal na"
W14-4323,I11-1068,0,0.0125118,"rs. tion metrics may hide information about the ability of the classifiers to learn from our feature set. For example, the best performing classifier (NB) incorrectly labeled 127 clauses out of 430 possible in the test set. However, 44 of these errors agreed with at least one annotator, but were counted as entirely incorrect in the previous evaluations. To address these concerns we also evaluated the performance of the the best performing classifier based on the level of agreement of each instance using two different approaches. See Table 6. The first approach was inspired by the approach in (Louis and Nenkova, 2011) where the clauses in the test set are binned based on the number of annotators who agreed with the gold standard label. The performance is then calculated for each bin. The first three rows of Table 6 show the performance for the different levels of agreement in the dataset. There were only 15 clauses in the test set where there was no agreement at all. It is unsurprising that when the annotators could not agree on a label the system performed near chance levels. However, when all three annotators agreed on the gold standard label the F-score improved to 0.767. As a comparison, the F-score of"
W14-4323,W10-4310,0,0.0206454,"rked not part of the story. The types of errors described above are not mutually exclusive and in some cases are causally related. For example, the purpose of a clause may be ambiguous because it contains conflicting lexical indicators. Similarly, a clause containing multiple categories will likely have strong lexical indicators from each of these categories so that the classification algorithms cannot disambiguate among possible labels. This might be improved by more data, more sophisticated semantic features, or possibly an analysis focused on discourse relations, such as those in the PDTB (Louis et al., 2010; Prasad et al., 2008), or Elson’s STORY INTENTION GRAPH (Rishes et al., 2013; Elson and McKeown, 2010; Elson, 2012). 6 Discussion This paper describes work on categorization of narrative clauses based on Labov & Waletzky’s theory of oral narrative, applied to personal narratives written by ordinary people. We show that we can automatically classify narrative clauses with these categories achieving an overall F-score of 0.689, which is substantially higher than a random (0.250) or majority class (0.437) baseline, which increases to an F-score of .767 on the cases where all three annotators agr"
W14-4323,prasad-etal-2008-penn,0,0.098389,"story. The types of errors described above are not mutually exclusive and in some cases are causally related. For example, the purpose of a clause may be ambiguous because it contains conflicting lexical indicators. Similarly, a clause containing multiple categories will likely have strong lexical indicators from each of these categories so that the classification algorithms cannot disambiguate among possible labels. This might be improved by more data, more sophisticated semantic features, or possibly an analysis focused on discourse relations, such as those in the PDTB (Louis et al., 2010; Prasad et al., 2008), or Elson’s STORY INTENTION GRAPH (Rishes et al., 2013; Elson and McKeown, 2010; Elson, 2012). 6 Discussion This paper describes work on categorization of narrative clauses based on Labov & Waletzky’s theory of oral narrative, applied to personal narratives written by ordinary people. We show that we can automatically classify narrative clauses with these categories achieving an overall F-score of 0.689, which is substantially higher than a random (0.250) or majority class (0.437) baseline, which increases to an F-score of .767 on the cases where all three annotators agreed. The learning curv"
W15-0515,U08-1003,0,0.0130446,"lity FACT Phrases with “OF” and FEEL Phrases with “FOR” FACT “OF” Phrases RESULT OF ORIGIN OF THEORY OF EVIDENCE OF PARTS OF EVOLUTION OF PERCENT OF THOUSANDS OF EXAMPLE OF LAW OF 5 FEEL “FOR” Phrases MARRIAGE FOR STANDING FOR SAME FOR TREATMENT FOR DEMAND FOR ATTENTION FOR ADVOCATE FOR NO EVIDENCE FOR JUSTIFICATION FOR EXCUSE FOR Related Work Related research on argumentation has primarily worked with different genres of argument than found in IAC, such as news articles, weblogs, legal briefs, supreme court summaries, and congressional debates (Marwell and Schmitt, 1967; Thomas et al., 2006; Burfoot, 2008; Cialdini, 2000; McAlister et al., 2014; Reed and Rowe, 2004). The examples from IAC in Figure 1 illustrate that natural informal dialogues such as those found in online forums exhibit a much broader range of argumentative styles. Other work has on models of natural informal arguments have focused on stance classification (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012), argument summarization (Misra et al., 2015), sarcasm detection (Justo et al., 2014), and identifying the structure of arguments such as main claims and their justifications (Biran and Rambow,"
W15-0515,D10-1121,0,0.0456432,"Missing"
W15-0515,C04-1200,0,0.255455,"Missing"
W15-0515,W13-1104,1,0.300424,"Missing"
W15-0515,N15-1046,1,0.139075,"A Fact: Distinguishing Factual and Emotional Argumentation in Online Dialogue Shereen Oraby∗ , Lena Reed∗ , Ryan Compton∗ , Ellen Riloff † , Marilyn Walker∗ and Steve Whittaker∗ ∗ University of California Santa Cruz {soraby,lireed,rcompton,mawalker,swhittak}@ucsc.edu † University of Utah riloff@cs.utah.edu Abstract Schmitt, 1967; Cialdini, 2000; McAlister et al., 2014; Reed and Rowe, 2004). Recent work has begun to model different aspects of these natural informal arguments, with tasks including stance classification (Somasundaran and Wiebe, 2010; Walker et al., 2012), argument summarization (Misra et al., 2015), sarcasm detection (Justo et al., 2014), and work on the detailed structure of arguments (Biran and Rambow, 2011; Purpura et al., 2008; Yang and Cardie, 2013). Successful models of these tasks have many possible applications in sentiment detection, automatic summarization, argumentative agents (Zuckerman et al., 2015), and in systems that support human argumentative behavior (Rosenfeld and Kraus, 2015). We investigate the characteristics of factual and emotional argumentation styles observed in online debates. Using an annotated set of FACTUAL and FEELING debate forum posts, we extract patter"
W15-0515,S12-1033,0,0.0604829,"Missing"
W15-0515,N12-1071,0,0.0458703,"Missing"
W15-0515,W02-1011,0,0.017406,"Missing"
W15-0515,W14-2105,0,0.0115292,"responses may try to bolster their argument by providing statistics related to a position, giving historical or scientific background, or presenting specific examples or data. There is clearly a relationship between a proposition being FACTUAL versus OBJECTIVE or VERIDICAL, although each of these different labelling tasks may elicit differences from annotators (Wiebe and Riloff, 2005; Riloff and 116 Proceedings of the 2nd Workshop on Argumentation Mining, pages 116–126, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics Wiebe, 2003; Saur´ı and Pustejovsky, 2009; Park and Cardie, 2014). Class FACT FACT FEEL FEEL Debate Forum Dialogue Quote: Even though our planet is getting warmer, it is still a lot cooler than it was 4000 years ago. Response: The average global temperature follows a sinusoidal pattern, the general consensus is we are supposed to be approaching a peak. Projections show that instead of peaking, there will be continue to be an increase in average global temperature. Quote: “When you go to war against your enemies...suppose you see a beautiful woman whom you desire...you shall take her..and she shall marry you.” - Deut. 21:10 Response: Read to the very end of"
W15-0515,W13-1602,1,0.0897573,"Missing"
W15-0515,W03-1014,1,0.711763,"Missing"
W15-0515,roberts-etal-2012-empatweet,0,0.0752252,"Missing"
W15-0515,D08-1027,0,0.0567413,"Missing"
W15-0515,P09-1026,0,0.247158,"Missing"
W15-0515,W10-0214,0,0.0611393,"Missing"
W15-0515,W06-1639,0,0.052753,"Table 4: High-Probability FACT Phrases with “OF” and FEEL Phrases with “FOR” FACT “OF” Phrases RESULT OF ORIGIN OF THEORY OF EVIDENCE OF PARTS OF EVOLUTION OF PERCENT OF THOUSANDS OF EXAMPLE OF LAW OF 5 FEEL “FOR” Phrases MARRIAGE FOR STANDING FOR SAME FOR TREATMENT FOR DEMAND FOR ATTENTION FOR ADVOCATE FOR NO EVIDENCE FOR JUSTIFICATION FOR EXCUSE FOR Related Work Related research on argumentation has primarily worked with different genres of argument than found in IAC, such as news articles, weblogs, legal briefs, supreme court summaries, and congressional debates (Marwell and Schmitt, 1967; Thomas et al., 2006; Burfoot, 2008; Cialdini, 2000; McAlister et al., 2014; Reed and Rowe, 2004). The examples from IAC in Figure 1 illustrate that natural informal dialogues such as those found in online forums exhibit a much broader range of argumentative styles. Other work has on models of natural informal arguments have focused on stance classification (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012), argument summarization (Misra et al., 2015), sarcasm detection (Justo et al., 2014), and identifying the structure of arguments such as main claims and their justifications (Bir"
W15-0515,walker-etal-2012-corpus,1,0.308204,"Missing"
W15-0515,J04-3002,0,0.0812307,"Missing"
W15-0515,H05-1044,0,0.129561,"Missing"
W15-0515,P13-1161,0,0.0214478,"and Steve Whittaker∗ ∗ University of California Santa Cruz {soraby,lireed,rcompton,mawalker,swhittak}@ucsc.edu † University of Utah riloff@cs.utah.edu Abstract Schmitt, 1967; Cialdini, 2000; McAlister et al., 2014; Reed and Rowe, 2004). Recent work has begun to model different aspects of these natural informal arguments, with tasks including stance classification (Somasundaran and Wiebe, 2010; Walker et al., 2012), argument summarization (Misra et al., 2015), sarcasm detection (Justo et al., 2014), and work on the detailed structure of arguments (Biran and Rambow, 2011; Purpura et al., 2008; Yang and Cardie, 2013). Successful models of these tasks have many possible applications in sentiment detection, automatic summarization, argumentative agents (Zuckerman et al., 2015), and in systems that support human argumentative behavior (Rosenfeld and Kraus, 2015). We investigate the characteristics of factual and emotional argumentation styles observed in online debates. Using an annotated set of FACTUAL and FEELING debate forum posts, we extract patterns that are highly correlated with factual and emotional arguments, and then apply a bootstrapping methodology to find new patterns in a larger pool of unannot"
W15-0515,P14-1031,0,0.0340034,"Missing"
W15-0515,W03-1017,0,0.201515,"Missing"
W15-4627,P01-1015,0,0.0797034,"Missing"
W15-4627,E14-1074,0,0.0300524,"Missing"
W15-4627,P09-4003,0,0.491828,"ylistic models for character voices. Previous work can generate narratological variations, but is domain dependent (Callaway and Lester, 2002; Montfort, 2007). Sec. 2 describes our corpus of stories and the architecture of our story generation framework, EST 2.0.1 Sec. 3 describes experiments testing the coverage and correctness of EST 2.0. Sec. 4 describes experiments testing user perceptions of different linguistic variations in storytelling. Our contributions are: • We produce SIG representations of 100 personal narratives from a weblog corpus, using the story annotation tool Scheherezade (Elson and McKeown, 2009; Elson, 2012); • We compare EST 2.0 to EST and show how we have not only made improvements to the translation algorithm, but can extend and compare to personal narratives. • We implement a parameterized variation of linguistic style in order to introduce discourse structure into our generated narratives. • We carry out experiments to gather user perceptions of different sentence planning choices that can be made with complex sentences in stories. We sum up and discuss future work in Sec. 5. 2 Story Generation Framework Figure 1: NLG pipeline method of the ES Translator. Fig. 1 illustrates our"
W15-4627,E03-1019,0,0.0569635,"s are created daily in the thousands and cover any topic imaginable. They are natural and personal, and may be funny, sad, heart-warming or serious. There are many potential applications: virtual companions, educational storytelling, or to share troubles in therapeutic settings (Bickmore, 2003; Pennebaker and Seagal, 1999; Gratch et al., 2012). Previous research on NLG of linguistic style shows that dialogue systems are more effective if they can generate stylistic linguistic variations based on the user’s emotional state, personality, style, confidence, or other factors (Andr´e et al., 2000; Piwek, 2003; McQuiggan et al., 2008; Porayska-Pomsta and Mellish, 2004; Forbes-Riley and Litman, 2011; Wang et al., 2005; Dethlefs et al., 2014). Other work focuses on variation in journalistic writing or instruction manuals, where stylistic variations as well as journalistic slant or connotations have been explored (Hovy, 1988; Green and DiMarco, 1993; Paris and Scott, 1994; Power et al., 2003; Inkpen and Hirst, 2004). Previous iterations of the EST simply presented a sequence of events (Rishes et al., 2013). This work implements parameterized variation of linguistic style in the context of weblogs in o"
W15-4627,prasad-etal-2008-penn,0,0.060233,"e can change the sentence to be realized in the first person. For example, to produce the variations in Table 4, we use both first person, and direct speech, as well as linguistic styles from PER SONAGE: a neutral voice for the narrator, a shy voice for the crow, and a laid-back voice for the fox (Lukin and Walker, 2015). We fully utilize this variation when we retell personal narratives in EST 2.0. This paper and introduces support for new discourse relations, such as aggregating clauses related by the contingency discourse relation (one of many listed in the Penn Discourse Tree Bank (PDTB) (Prasad et al., 2008)). In SIG encoding, contingency clauses are always expressed with the “in order to” relation (Table 6, 1). To support linguistic variation, we introduce “de-aggregation” onto these aggregating clauses in order to have the flexibility to rephrase, restructure, or ignore clauses as indicated by our parameterized sentence planner. We identify candidate story points in the SIG that contain a contingency relation (annotated in the Timeline layer) and deliberately break apart 3 Personal Narrative Evaluation After annotating our 100 stories with the SCHEHERAZADE annotation tool, we ran them through t"
W15-4627,W09-3941,0,0.0179488,"n tool can be applied to informal narratives such as personal narratives from weblogs, and the resulting SIG representations work with existing tools for translating from the SIG to a retelling of a story. We present a parameterized sentence planner for story generation, that provides aggregation operations and variations in point of view. The technical aspects of de-aggregation and aggregation builds on previous work in NLG and our earlier work on SPaRKy (Cahill et al., 2001; Scott and de Souza, 1990; Paris and Scott, 1994; Nakatsu and White, 2010; Howcroft et al., 2013; Walker et al., 2007; Stent and Molina, 2009). However we are not aware of previous NLG applications needing to first de-aggregate the content, before applying aggregation operations. Our experiments show that, as expected, readers almost always prefer the original sentence over automatically produced variations, but that the soSN variant is preferred. We examine two specific stories where preferences vary from the overall trend: these stories suggest future possible experiments where we might vary more aspects of the story context and audience. We also compare our best variation to what SCHEHERAZADE produces. Despite the fact that the S"
W15-4627,P88-1020,0,0.336184,"1999; Gratch et al., 2012). Previous research on NLG of linguistic style shows that dialogue systems are more effective if they can generate stylistic linguistic variations based on the user’s emotional state, personality, style, confidence, or other factors (Andr´e et al., 2000; Piwek, 2003; McQuiggan et al., 2008; Porayska-Pomsta and Mellish, 2004; Forbes-Riley and Litman, 2011; Wang et al., 2005; Dethlefs et al., 2014). Other work focuses on variation in journalistic writing or instruction manuals, where stylistic variations as well as journalistic slant or connotations have been explored (Hovy, 1988; Green and DiMarco, 1993; Paris and Scott, 1994; Power et al., 2003; Inkpen and Hirst, 2004). Previous iterations of the EST simply presented a sequence of events (Rishes et al., 2013). This work implements parameterized variation of linguistic style in the context of weblogs in order to introduce discourse structure into our generated stories. Our approach differs from previous work on NLG for narrative because we emphasize (1) domain-independent methods; and (2) generating a large range of variation, both narratological and stylistic. (Lukin and Walker, 2015)’s work on the EST is the first"
W15-4627,W13-2104,0,0.0290262,"e first time that the SCHEHEREZADE annotation tool can be applied to informal narratives such as personal narratives from weblogs, and the resulting SIG representations work with existing tools for translating from the SIG to a retelling of a story. We present a parameterized sentence planner for story generation, that provides aggregation operations and variations in point of view. The technical aspects of de-aggregation and aggregation builds on previous work in NLG and our earlier work on SPaRKy (Cahill et al., 2001; Scott and de Souza, 1990; Paris and Scott, 1994; Nakatsu and White, 2010; Howcroft et al., 2013; Walker et al., 2007; Stent and Molina, 2009). However we are not aware of previous NLG applications needing to first de-aggregate the content, before applying aggregation operations. Our experiments show that, as expected, readers almost always prefer the original sentence over automatically produced variations, but that the soSN variant is preferred. We examine two specific stories where preferences vary from the overall trend: these stories suggest future possible experiments where we might vary more aspects of the story context and audience. We also compare our best variation to what SCHE"
W15-4627,kipper-etal-2006-extending,0,0.00993926,"(3) model the annotator’s understanding of the overarching goals, plans and beliefs of the story’s agents. SCHEHERAZADE allows users to annotate a story along several dimensions, starting with the surface form of the story (first column in Table 2) and then proceeding to deeper representations. The first dimension (second column in Table 2) is called the “timeline layer”, in which the story is encoded as predicate-argument structures (propositions) that are temporally ordered on a timeline. SCHEHERAZADE adapts information about predicate-argument structures from the VerbNet lexical database (Kipper et al., 2006) and uses WordNet (Fellbaum, 1998) as its noun and adjectives taxonomy. The arcs of the story graph are labeled with discourse relations, such as attempts to cause, or temporal order (see Chapter 4 of (Elson, 2012).) The EST applies a model of syntax to the SIG which translates from the semantic representation of the SIG to the syntactic formalism of Deep Syntactic Structures (DSYNTS) required by the PER SONAGE generator (Lavoie and Rambow, 1997; Melˇcuk, 1988; Mairesse and Walker, 2011). Fig. 1 provides a high level view of the architecture of EST . The full translation methodology is describ"
W15-4627,A97-1039,0,0.244852,"itions) that are temporally ordered on a timeline. SCHEHERAZADE adapts information about predicate-argument structures from the VerbNet lexical database (Kipper et al., 2006) and uses WordNet (Fellbaum, 1998) as its noun and adjectives taxonomy. The arcs of the story graph are labeled with discourse relations, such as attempts to cause, or temporal order (see Chapter 4 of (Elson, 2012).) The EST applies a model of syntax to the SIG which translates from the semantic representation of the SIG to the syntactic formalism of Deep Syntactic Structures (DSYNTS) required by the PER SONAGE generator (Lavoie and Rambow, 1997; Melˇcuk, 1988; Mairesse and Walker, 2011). Fig. 1 provides a high level view of the architecture of EST . The full translation methodology is described in (Rishes et al., 2013). DSYNTS are a flexible dependency tree representation of an utterance that gives us access to the underlying linguistic structure of a sentence that goes beyond surface string manipulation. The nodes of the DSYNTS syntactic trees are labeled with lexemes and the arcs of the tree are labeled with syntactic relations. The DSYNTS formalism distinguishes between arguments and modifiers and between different types of argum"
W15-4627,W05-1610,0,0.0718372,"semantic components are lost in the translation process. 5 Acknowledgements. This research was supported by Nuance Foundation Grant SC-14-74, NSF Grants IIS-HCC-1115742 and IIS-1002921. Appendix. Table 10 provides additional examples of the output of the EST 2.0 system, illustrating particular user preferences and system strengths and weaknesses. Discussion and Conclusions To our knowledge, this is the first time that sentence planning variations for story telling have been implemented in a framework where the discourse (telling) is completely independent of the fabula (content) of the story (Lonneker, 2005). We also show for the first time that the SCHEHEREZADE annotation tool can be applied to informal narratives such as personal narratives from weblogs, and the resulting SIG representations work with existing tools for translating from the SIG to a retelling of a story. We present a parameterized sentence planner for story generation, that provides aggregation operations and variations in point of view. The technical aspects of de-aggregation and aggregation builds on previous work in NLG and our earlier work on SPaRKy (Cahill et al., 2001; Scott and de Souza, 1990; Paris and Scott, 1994; Naka"
W15-4627,J11-3002,1,0.933054,"ntence planner for story generation that provides aggregation operations, variations in discourse and in point of view. Finally, we present a user evaluation of different personal narrative retellings. 1 Table 1: The Startled Squirrel Weblog Story limitation greatly restricts the range of applications; it also means that it is impossible to take advantage of recent work in expressive and statistical language generation that can dynamically and automatically produce a large number of variations of given content (Rieser and Lemon, 2011; Paiva and Evans, 2004; Langkilde, 1998; Rowe et al., 2008; Mairesse and Walker, 2011). Such variations are important for expressive purposes, we well as for user adaptation and personalization (Zukerman and Litman, 2001; Wang et al., 2005; McQuiggan et al., 2008). We propose that a solution to this problem lies in new methods for developing language generation resources. First we describe the ES - TRANSLATOR (or EST ), a computational language generator that has previously been applied only to fables, e.g. the fable in Table 3 (Rishes et al., 2013). We quantitatively evaluate the domain independence of the EST by applying it to social media narratives, such as the Startled Squ"
W15-4627,A00-2023,0,\N,Missing
W15-4627,E03-1062,0,\N,Missing
W15-4631,I11-1068,0,0.184153,"ally contain words with a high information content. Several studies show that word length is a surprisingly good indicator that outperforms more complex measures, such as rarity (Piantadosi et al., 2011). Thus we include features based on word length, including the min, max, mean and median. We also create a feature whose value is the count of words of lengths 1 to 20 (or longer). Speciteller (SPTL): We add a single aggregate feature from the result of Speciteller, a tool that assesses the specificity of a sentence in the range of 0 (least specific) to 1 (most specific) (Li and Nenkova, 2015; Louis and Nenkova, 2011). High specificity should correlate with argument quality. Kullback-Leibler Divergence (KLDiv): We expect that sentences on one topic domain will have different content than sentences outside the domain. We built two trigram language models using the Berkeley LM toolkit (Pauls and Klein, 2011). One (P) built from all the sentences in the IAC within the domain, excluding all sentences from the annotated dataset, and one (Q) built from all sentences in IAC outside the domain. The KL Divergence is then computed using the discrete n-gram probabilities in the sentence from each model as in equation"
W15-4631,N15-1046,1,0.5097,"their choices about the surface realization of their arguments. We examine a number of theoretically motivated cues for extraction, that we expect to be domain-independent. We describe how we use these cues to sample from the corpus in a way that lets us test the impact of the hypothesized cues. Both the argument extraction and facet similarity tasks have strong similarities to other work in natural language processing. Argument extraction resembles the sentence extraction phase of multi-document summarization. Facet similarity resembles semantic textual similarity and paraphrase recognition (Misra et al., 2015; Boltuzic ˇ and Snajder, 2014; Conrad et al., 2012; Han et al., 2013; Agirre et al., 2012). Work on multidocument summarization also uses a similar module to merge redundant content from extracted candidate sentences (Barzilay, 2003; Gurevych and Strube, 2004; Misra et al., 2015). Sec. 2 describes our corpus of arguments, and describes the hypothesized markers of highquality argument segments. We sample from the corpus using these markers, and then annotate the extracted argument segments for ARGUMENT QUALITY . Sec. 3.2 describes experiments to test whether: (1) we can predict argument qualit"
W15-4631,S12-1051,0,0.0139313,"eoretically motivated cues for extraction, that we expect to be domain-independent. We describe how we use these cues to sample from the corpus in a way that lets us test the impact of the hypothesized cues. Both the argument extraction and facet similarity tasks have strong similarities to other work in natural language processing. Argument extraction resembles the sentence extraction phase of multi-document summarization. Facet similarity resembles semantic textual similarity and paraphrase recognition (Misra et al., 2015; Boltuzic ˇ and Snajder, 2014; Conrad et al., 2012; Han et al., 2013; Agirre et al., 2012). Work on multidocument summarization also uses a similar module to merge redundant content from extracted candidate sentences (Barzilay, 2003; Gurevych and Strube, 2004; Misra et al., 2015). Sec. 2 describes our corpus of arguments, and describes the hypothesized markers of highquality argument segments. We sample from the corpus using these markers, and then annotate the extracted argument segments for ARGUMENT QUALITY . Sec. 3.2 describes experiments to test whether: (1) we can predict argument quality; (2) our hypothesized cues are good indicators of argument quality; and (3) an argument q"
W15-4631,P11-1027,0,0.0148869,"also create a feature whose value is the count of words of lengths 1 to 20 (or longer). Speciteller (SPTL): We add a single aggregate feature from the result of Speciteller, a tool that assesses the specificity of a sentence in the range of 0 (least specific) to 1 (most specific) (Li and Nenkova, 2015; Louis and Nenkova, 2011). High specificity should correlate with argument quality. Kullback-Leibler Divergence (KLDiv): We expect that sentences on one topic domain will have different content than sentences outside the domain. We built two trigram language models using the Berkeley LM toolkit (Pauls and Klein, 2011). One (P) built from all the sentences in the IAC within the domain, excluding all sentences from the annotated dataset, and one (Q) built from all sentences in IAC outside the domain. The KL Divergence is then computed using the discrete n-gram probabilities in the sentence from each model as in equation (1). Experiments 3.1 Implicit Markup Hypothesis Validation We can now briefly validate some of the IMPLICIT MARKUP hypothesis using an ANOVA testing the effect of a connective and its position in post on argument quality. Across all sentences in all topics, the presence of a connective is sig"
W15-4631,W14-2107,0,0.211454,"Missing"
W15-4631,W15-0514,0,0.0296986,"o take a supervised approach, developing techniques for argument mining on online forums about technical topics and applying a theory of argument structure that is based on identifying TARGETS and CALL OUTS , where the callout attacks a target proposition in another speaker’s utterance (Ghosh et al., 2014b). However, their work does not attempt to discover high quality callouts and targets that can be understood out of context like we do. More recent work also attempts to do some aspects of argument mining in an unsupervised way (Boltuzic ˇ and Snajder, 2015; Sobhani et al., 2015). Howˇ ever (Boltuzic and Snajder, 2015) focus on the argument facet similarity task, using as input a corpus where the arguments have already been extracted. (Sobhani et al., 2015) present an architecture where arguments are first topic-labelled in a semi-supervised way, and then used for stance classification, however this approach treats the whole comment as the extracted argument, rather than attempting to pull out specific focused argument segments as we do here. Discussion and Conclusions This paper addresses the Argument Extraction task in a framework whose long-term aim is to first extract arguments from online dialogues, an"
W15-4631,prasad-etal-2008-penn,0,0.117902,"Missing"
W15-4631,W12-3810,0,0.0530216,"ir arguments. We examine a number of theoretically motivated cues for extraction, that we expect to be domain-independent. We describe how we use these cues to sample from the corpus in a way that lets us test the impact of the hypothesized cues. Both the argument extraction and facet similarity tasks have strong similarities to other work in natural language processing. Argument extraction resembles the sentence extraction phase of multi-document summarization. Facet similarity resembles semantic textual similarity and paraphrase recognition (Misra et al., 2015; Boltuzic ˇ and Snajder, 2014; Conrad et al., 2012; Han et al., 2013; Agirre et al., 2012). Work on multidocument summarization also uses a similar module to merge redundant content from extracted candidate sentences (Barzilay, 2003; Gurevych and Strube, 2004; Misra et al., 2015). Sec. 2 describes our corpus of arguments, and describes the hypothesized markers of highquality argument segments. We sample from the corpus using these markers, and then annotate the extracted argument segments for ARGUMENT QUALITY . Sec. 3.2 describes experiments to test whether: (1) we can predict argument quality; (2) our hypothesized cues are good indicators of"
W15-4631,W15-0509,0,0.00860235,"ˇ Snajder, 2014). Ghosh et al. also take a supervised approach, developing techniques for argument mining on online forums about technical topics and applying a theory of argument structure that is based on identifying TARGETS and CALL OUTS , where the callout attacks a target proposition in another speaker’s utterance (Ghosh et al., 2014b). However, their work does not attempt to discover high quality callouts and targets that can be understood out of context like we do. More recent work also attempts to do some aspects of argument mining in an unsupervised way (Boltuzic ˇ and Snajder, 2015; Sobhani et al., 2015). Howˇ ever (Boltuzic and Snajder, 2015) focus on the argument facet similarity task, using as input a corpus where the arguments have already been extracted. (Sobhani et al., 2015) present an architecture where arguments are first topic-labelled in a semi-supervised way, and then used for stance classification, however this approach treats the whole comment as the extracted argument, rather than attempting to pull out specific focused argument segments as we do here. Discussion and Conclusions This paper addresses the Argument Extraction task in a framework whose long-term aim is to first ext"
W15-4631,P07-1033,0,0.0215964,"Missing"
W15-4631,W14-2106,0,0.282402,"ntative structure. The Discourse Relation hypothesis suggests that the Arg1 and Arg2 of explicit SPECIFICA TION , CONTRAST, CONCESSION and CONTIN GENCY markers are more likely to contain good argumentative segments (Prasad et al., 2008). In the case of explicit connectives, Arg2 is the argument to which the connective is syntactically bound, and Arg1 is the other argument. For example, a C ONTINGENCY relation is frequently marked by the lexical anchor If, as in R1 in Fig. 1. A C ONTRAST relation may mark a challenge to an opponent’s claim, what Ghosh et al. call callout-target argument pairs (Ghosh et al., 2014b; Maynard, 1985). The C ONTRAST relation is frequently marked by But, as in R3 and R4 in Fig. 1. A S PECIFICATION relation may indicate a focused detailed argument, as marked by First in R2 in Fig. 1 (Li and Nenkova, 2015). We decided to extract only the Arg2, where the discourse argument is syntactically bound to the connective, since Arg1’s are more difficult to locate, especially in dialogue. We began by extracting the Arg2’s for the connectives most strongly associated with these discourse relations over the whole corpus, and then once we saw what the most frequent connectives were in our"
W15-4631,C04-1110,0,0.0301331,"s test the impact of the hypothesized cues. Both the argument extraction and facet similarity tasks have strong similarities to other work in natural language processing. Argument extraction resembles the sentence extraction phase of multi-document summarization. Facet similarity resembles semantic textual similarity and paraphrase recognition (Misra et al., 2015; Boltuzic ˇ and Snajder, 2014; Conrad et al., 2012; Han et al., 2013; Agirre et al., 2012). Work on multidocument summarization also uses a similar module to merge redundant content from extracted candidate sentences (Barzilay, 2003; Gurevych and Strube, 2004; Misra et al., 2015). Sec. 2 describes our corpus of arguments, and describes the hypothesized markers of highquality argument segments. We sample from the corpus using these markers, and then annotate the extracted argument segments for ARGUMENT QUALITY . Sec. 3.2 describes experiments to test whether: (1) we can predict argument quality; (2) our hypothesized cues are good indicators of argument quality; and (3) an argument quality predictor trained on one topic or a set of topics can be used on unseen topics. The results in Sec. 4 show that we can predict argument quality with RRSE values a"
W15-4631,S13-1005,0,0.0161389,"ine a number of theoretically motivated cues for extraction, that we expect to be domain-independent. We describe how we use these cues to sample from the corpus in a way that lets us test the impact of the hypothesized cues. Both the argument extraction and facet similarity tasks have strong similarities to other work in natural language processing. Argument extraction resembles the sentence extraction phase of multi-document summarization. Facet similarity resembles semantic textual similarity and paraphrase recognition (Misra et al., 2015; Boltuzic ˇ and Snajder, 2014; Conrad et al., 2012; Han et al., 2013; Agirre et al., 2012). Work on multidocument summarization also uses a similar module to merge redundant content from extracted candidate sentences (Barzilay, 2003; Gurevych and Strube, 2004; Misra et al., 2015). Sec. 2 describes our corpus of arguments, and describes the hypothesized markers of highquality argument segments. We sample from the corpus using these markers, and then annotate the extracted argument segments for ARGUMENT QUALITY . Sec. 3.2 describes experiments to test whether: (1) we can predict argument quality; (2) our hypothesized cues are good indicators of argument quality;"
W15-4631,walker-etal-2012-corpus,1,\N,Missing
W16-3604,P15-2124,0,0.360908,"Missing"
W16-3604,L16-1704,1,0.12245,"Missing"
W16-3604,N16-1016,0,0.0298906,"Missing"
W16-3604,P15-2122,0,0.0528141,"stic Cues Rhetorical Questions. There is no previous work on distinguishing sarcastic from non-sarcastic uses of rhetorical questions (RQs). RQs are syntactically formulated as a question, but function as an indirect assertion (Frank, 1990). The polarity of the question implies an assertion of the opposite polarity, e.g. Can you read? implies You can’t read. RQs are prevalent in persuasive discourse, and are frequently used ironically (Schaffer, 2005; Ilie, 1994; Gibbs, 2000). Previous work focuses on their formal semantic properties (Han, 1997), or distinguishing RQs from standard questions (Bhattasali et al., 2015). We hypothesized that we could find RQs in abundance by searching for questions in the middle of a post, that are followed by a statement, using the assumption that questions followed by a statement are unlikely to be standard informationTable 3: Annotation Counts for a Subset of Cues Cue annotation experiments. After running a large number of retrieval experiments with our regex pattern matcher, we select batches of the resulting posts that mix different cue classes to put out for annotation, in such a way as to not allow the annotators to determine what regex cues were used. We then success"
W16-3604,N16-1082,0,0.010216,"7: Supervised Learning Results for Generic (Gen: 3,260 posts per class), Rhetorical Questions (RQ: 851 posts per class) and Hyperbole (Hyp: 582 posts per class) Supervised Learning. We restrict our supervised experiments to a default linear SVM learner with Stochastic Gradient Descent (SGD) training and L2 regularization, available in the SciKit-Learn toolkit (Pedregosa et al., 2011). We use 10-fold cross-validation, and only two types of features: n-grams and Word2Vec word embeddings. We expect Word2Vec to be able to capture semantic generalizations that n-grams do not (Socher et al., 2013; Li et al., 2016). The n-gram features include unigrams, bigrams, and trigrams, including sequences of punctuation (for example, ellipses or “!!!”), and emoticons. We use GoogleNews Word2Vec features (Mikolov et al., 2013).4 Table 7 summarizes the results of our supervised learning experiments on our datasets using 10-fold cross validation. The data is balanced evenly between the SARCASTIC and NOTSARCASTIC classes, and the best F-Measures for each class are shown in bold. The default W2V model, (trained on Google News), gives the best overall F-measure of 0.74 on the Gen corpus for the SARCASTIC class, while n"
W16-3604,W13-1104,1,0.878413,"Missing"
W16-3604,P14-5010,0,0.00267574,"r three subcorpora with AutoSlog-TS parameters, aimed at optimizing precision 4 Linguistic Analysis Dataset # Sarc Patterns # NotSarc Patterns Generic (Gen) 1,316 3,556 Rhetorical Questions (RQ) 671 1,000 Hyperbole (Hyp) 411 527 Table 10: Total number of patterns passing threshold of Freq ≥ 2, Prob ≥ 0.75 Here we aim to provide a linguistic characterization of the differences between the sarcastic and the not-sarcastic classes. We use the AutoSlog-TS pattern learner to generate patterns automatically, and the Stanford dependency parser to examine relationships between arguments (Riloff, 1996; Manning et al., 2014). Table 10 shows the number of sarcastic patterns we extract with AutoSlog-TS, with a frequency of at least 2 and a probability of at least 0.75 for each corpus. We learn many novel lexico-syntactic cue patterns that are not the regex that we search for. We discuss specific novel learned patterns for each class below. Rhetorical Questions. We notice that while the NOT- SARCASTIC patterns generated for RQs are similar to the topic-specific NOT- SARCASTIC patterns we find in the general dataset, there are some interesting features of the SARCASTIC patterns that are more unique to the RQs. Many o"
W16-3604,filatova-2012-irony,0,0.121062,"Missing"
W16-3604,W15-0515,1,0.41263,"Missing"
W16-3604,P11-2102,0,0.396638,"Missing"
W16-3604,W03-1014,1,0.652095,"Missing"
W16-3604,D13-1066,1,0.901418,"Missing"
W16-3604,D13-1170,0,0.00263744,"the corpus. Hyp Table 7: Supervised Learning Results for Generic (Gen: 3,260 posts per class), Rhetorical Questions (RQ: 851 posts per class) and Hyperbole (Hyp: 582 posts per class) Supervised Learning. We restrict our supervised experiments to a default linear SVM learner with Stochastic Gradient Descent (SGD) training and L2 regularization, available in the SciKit-Learn toolkit (Pedregosa et al., 2011). We use 10-fold cross-validation, and only two types of features: n-grams and Word2Vec word embeddings. We expect Word2Vec to be able to capture semantic generalizations that n-grams do not (Socher et al., 2013; Li et al., 2016). The n-gram features include unigrams, bigrams, and trigrams, including sequences of punctuation (for example, ellipses or “!!!”), and emoticons. We use GoogleNews Word2Vec features (Mikolov et al., 2013).4 Table 7 summarizes the results of our supervised learning experiments on our datasets using 10-fold cross validation. The data is balanced evenly between the SARCASTIC and NOTSARCASTIC classes, and the best F-Measures for each class are shown in bold. The default W2V model, (trained on Google News), gives the best overall F-measure of 0.74 on the Gen corpus for the SARCAS"
W16-3604,swanson-etal-2014-getting,1,0.912284,"Missing"
W16-3604,W02-1028,1,0.37196,"Missing"
W16-3604,walker-etal-2012-corpus,1,0.39942,"Missing"
W16-3604,P14-2084,0,0.13863,"Missing"
W16-3636,L16-1704,1,0.918953,"im to induce and identify facets of an argument across multiple conversations, and produce summaries of all the different facets. However our aim is to do this automatically, and over time. In order to simplify the problem, we focus on SENTENTIAL ARGU MENTS , single sentences that clearly express Introduction When people converse about social or political topics, similar arguments are often paraphrased by different speakers, across many different conversations. For example, consider the dialog excerpts in Fig. 1 from the 89K sentences about gun control in the IAC 2.0 corpus of online dialogs (Abbott et al., 2016). Each of the sentences S1 to S6 provide 1 See http://debatepedia.idebate.org/en/ index.php/Debate: Gun control, 2 See http://gun-control.procon.org/ 276 Proceedings of the SIGDIAL 2016 Conference, pages 276–287, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics Pro Arguments A1: The only function of a gun is to kill. A2: The legal ownership of guns by ordinary citizens inevitably leads to many accidental deaths. A3: Sports shooting desensitizes people to the lethal nature of firearms. A4: Gun ownership increases the risk of suicide. Con Arguments A5: Gun"
W16-3636,D15-1255,0,0.240066,"scale used for STS, we first define what a facet is, and then define the values of the AFS scale as shown in Fig. 10 in the appendix (repeated from Misra et al. (2015) for convenience). We distinguish AFS from STS because: (1) our data are so different: STS data consists of descriptive sentences whereas our sentences are argumentative excerpts from dialogs; and (2) our definition of facet allows for sentences that express opposite stance to be realizations of the same facet (AFS = 3) in Fig. 10. Related work has primarily used entailment or semantic equivalence to define argument similarity (Habernal and Gurevych, 2015; Boltuzic and ˇ ˇ Snajder, 2015; Boltuzic and Snajder, 2015; Habernal et al., 2014). We believe the definition of AFS given in Fig. 10 will be more useful in the long run than semantic equivalence or entailment, because two arguments can only be contradictory if they are about the same facet. For example, consider that sentential argument S7 in Fig. 5 is anti gun-control, while sentences S8 and S9 are pro gun-control. Our annotation guidelines label them with the same facet, in a similar way to how the We then calculated the probability that the sentences in each bin were high quality argumen"
W16-3636,S12-1051,0,0.0344694,"m overlap feature. For each argument, we extract the unigrams, bigrams and trigrams, and then calculate the cosine similarity between two texts represented as vectors of their ngram counts. Rouge. Rouge is a family of metrics for comparing the similarity of two summaries (Lin, 2004), which measures overlapping units such as continuous and skip ngrams, common subsequences, and word pairs. We use all the rouge f-scores from the pyrouge package. Our analysis shows that rouge s* f score correlates most highly with AFS.6 UMBC STS. We consider STS, a measure of the semantic similarity of two texts (Agirre et al., 2012), as another baseline, using the UMBC STS tool. Fig. 6 illustrates that in general, STS is rough approximation of AFS. It is possible that our selection of data for pairs for annotation using UMBC STS either improves or reduces its performance. Google Word2Vec. Word embeddings from word2vec (Mikolov et al., 2013) are popular for expressing semantic relationships between words, but using word embeddings to express entire sentences often requires some compromises. In particular, averaging word2vec embeddings for each word may lose too much information in long sentences. Previous work on argument"
W16-3636,S13-1005,0,0.020614,"Missing"
W16-3636,W14-2107,0,0.0604984,"automatically identified. Our previous work on Argument Extraction achieved good results, (Swanson et al., 2015), and is extended here (Sec. 2). Task2 takes pairs of sentences from Task1 as input and then learns a regressor that can predict Argument Facet Similarity (henceforth AFS). Related work on argument mining (discussed in more detail in Sec. 4) defines a finite set of facets for each topic, similar to those from Idebate in Fig. 2.3 Previous work then labels posts or sentences using these facets, and trains a classifier to return a facet label (Conrad et al., 2012; Hasan ˇ and Ng, 2014; Boltuzic and Snajder, 2014; Naderi and Hirst, 2015), inter alia. However, this simplification may not work in the long term, both because the sentential realizations of argument facets are propositional, and hence graded, and because 3 2 Corpora and Problem Definition Many existing websites summarize the frequent, and repeated, facets of arguments about current topics, that are linguistically realized in different ways, across many different social media and debate forums. For example, Fig. 2 illustrates the eight facets for gun control on IDebate. Fig. 3 illustrates a different type of summary, for the death penalty t"
W16-3636,W13-3514,0,0.0259045,". We also create our own 300-dimensional embeddings for our dialogic doˇ uˇrek and Somain using the Gensim library (Reh˚ jka, 2010), with default settings, and a very large corpus of user-generated dialogic content. This includes the corpus described in Sec. 2 (929, 206 forum posts), an internal corpus of 1, 688, 639 tweets on various topics, and a corpus of 53, 851, 542 posts from Reddit.8 LIWC category and Dependency Overlap. Both dependency structures and the Linguistics Inquiry Word Count (LIWC) tool have been useful in previous work (Pennebaker et al., 2001; Somasundaran and Wiebe, 2009; Hasan and Ng, 2013). We develop a novel feature set that combines LIWC category and dependency overlap, aiming to capture a generalized notion of concept overlap between two arguments, i.e. to capture the hypothesis that classes of content words such as affective processes or emotion types are indicative of a shared facet across pairs of arguments. two dependencies may share many generalizations or only a few. Here, the tuples with dependent tokens fear and punishment are more closely related because their shared generalization include both Negative Emotion and Affective Processes, but the tuples with dependent"
W16-3636,W15-0514,0,0.600045,"n define the values of the AFS scale as shown in Fig. 10 in the appendix (repeated from Misra et al. (2015) for convenience). We distinguish AFS from STS because: (1) our data are so different: STS data consists of descriptive sentences whereas our sentences are argumentative excerpts from dialogs; and (2) our definition of facet allows for sentences that express opposite stance to be realizations of the same facet (AFS = 3) in Fig. 10. Related work has primarily used entailment or semantic equivalence to define argument similarity (Habernal and Gurevych, 2015; Boltuzic and ˇ ˇ Snajder, 2015; Boltuzic and Snajder, 2015; Habernal et al., 2014). We believe the definition of AFS given in Fig. 10 will be more useful in the long run than semantic equivalence or entailment, because two arguments can only be contradictory if they are about the same facet. For example, consider that sentential argument S7 in Fig. 5 is anti gun-control, while sentences S8 and S9 are pro gun-control. Our annotation guidelines label them with the same facet, in a similar way to how the We then calculated the probability that the sentences in each bin were high quality arguments using the resulting AQ gold standard labels, and found th"
W16-3636,D14-1083,0,0.0451921,"informal, and do not necessarily follow logical rules or schemas of argumentation (Stab and Gurevych, 2014; Peldszus and Stede, 2013; Ghosh et al., 2014; Habernal et al., 2014; Goudas et al., 2014; Cabrio and Villata, 2012). Moreover, in social media, segments of text that are argumentative must first be identified, as in our Task1. Habernal and Gurevych (2016) train a classifier to recognize text segments that are argumentative, but much previous work does Task1 manually. Goudas et al. (2014) annotate 16,000 sentences from social media documents and consider 760 of them to be argumentative. Hasan and Ng (2014) also manually identify argumentaˇ tive sentences, while Boltuzic and Snajder (2014) treat the whole post as argumentative, after manually removing “spam” posts. Biran and Rambow (2011) automatically identify justifications as a structural component of an argument. Other work groups semantically-similar classes of reasons or frames that underlie a particular speaker’s stance, what we call ARGUMENT FACETS . One approach categorizes sentences or posts using topic-specific argument labels, which are functionally similar to our facets as discussed above (Conrad et al., 2012; Hasan and Ng, 2014; ˇ"
W16-3636,P12-2041,0,0.0181551,"ity of views on a topic, a quality not always guaranteed by summarization techniques, human or machine. There are many theories of argumentation that might be applicable for our task (Jackson and Jacobs, 1980; Reed and Rowe, 2004; Walton et al., 2008; Gilbert, 1997; Toulmin, 1958; Dung, 1995), but one definition of argument structure may not work for every NLP task. Social media arguments are often informal, and do not necessarily follow logical rules or schemas of argumentation (Stab and Gurevych, 2014; Peldszus and Stede, 2013; Ghosh et al., 2014; Habernal et al., 2014; Goudas et al., 2014; Cabrio and Villata, 2012). Moreover, in social media, segments of text that are argumentative must first be identified, as in our Task1. Habernal and Gurevych (2016) train a classifier to recognize text segments that are argumentative, but much previous work does Task1 manually. Goudas et al. (2014) annotate 16,000 sentences from social media documents and consider 760 of them to be argumentative. Hasan and Ng (2014) also manually identify argumentaˇ tive sentences, while Boltuzic and Snajder (2014) treat the whole post as argumentative, after manually removing “spam” posts. Biran and Rambow (2011) automatically ident"
W16-3636,W12-3810,0,0.0879937,"Missing"
W16-3636,P09-2079,0,0.0801739,"Missing"
W16-3636,I05-5002,0,0.0850073,"ifier that included detailed instructions and sample annotations. A score of 3 was mapped to a yes and scores of 1 or 2 mapped to a no. We simplified the task slightly in the HIT for gun control, where five annotators were instructed to select a yes label if the sentence clearly expressed an argument (score 3), or a no label otherwise (score 1 or 2). Figure 5: Paraphrases of the Gun ownership does not lead to higher crime facet of the Gun Control topic across different conversations. Our approach to Task2 draws strongly on recent work on semantic textual similarity (STS) (Agirre et al., 2013; Dolan and Brockett, 2005; Mihalcea et al., 2006). STS measures the degree of semantic similarity between a pair of sentences with values that range from 0 to 5. Inspired by the scale used for STS, we first define what a facet is, and then define the values of the AFS scale as shown in Fig. 10 in the appendix (repeated from Misra et al. (2015) for convenience). We distinguish AFS from STS because: (1) our data are so different: STS data consists of descriptive sentences whereas our sentences are argumentative excerpts from dialogs; and (2) our definition of facet allows for sentences that express opposite stance to be"
W16-3636,W14-2106,0,0.0242685,"or level information, so that our summaries represent the diversity of views on a topic, a quality not always guaranteed by summarization techniques, human or machine. There are many theories of argumentation that might be applicable for our task (Jackson and Jacobs, 1980; Reed and Rowe, 2004; Walton et al., 2008; Gilbert, 1997; Toulmin, 1958; Dung, 1995), but one definition of argument structure may not work for every NLP task. Social media arguments are often informal, and do not necessarily follow logical rules or schemas of argumentation (Stab and Gurevych, 2014; Peldszus and Stede, 2013; Ghosh et al., 2014; Habernal et al., 2014; Goudas et al., 2014; Cabrio and Villata, 2012). Moreover, in social media, segments of text that are argumentative must first be identified, as in our Task1. Habernal and Gurevych (2016) train a classifier to recognize text segments that are argumentative, but much previous work does Task1 manually. Goudas et al. (2014) annotate 16,000 sentences from social media documents and consider 760 of them to be argumentative. Hasan and Ng (2014) also manually identify argumentaˇ tive sentences, while Boltuzic and Snajder (2014) treat the whole post as argumentative, after manu"
W16-3636,W04-1013,0,0.0184552,"h facet. See Fig. 3. Given the data collected above, we defined a supervised machine learning experiment with AFS as our dependent variable. We developed a number of baselines using off the shelf tools. Features are grouped into sets and discussed in detail below. 3.1 Feature Sets NGRAM cosine. Our primary baseline is an ngram overlap feature. For each argument, we extract the unigrams, bigrams and trigrams, and then calculate the cosine similarity between two texts represented as vectors of their ngram counts. Rouge. Rouge is a family of metrics for comparing the similarity of two summaries (Lin, 2004), which measures overlapping units such as continuous and skip ngrams, common subsequences, and word pairs. We use all the rouge f-scores from the pyrouge package. Our analysis shows that rouge s* f score correlates most highly with AFS.6 UMBC STS. We consider STS, a measure of the semantic similarity of two texts (Agirre et al., 2012), as another baseline, using the UMBC STS tool. Fig. 6 illustrates that in general, STS is rough approximation of AFS. It is possible that our selection of data for pairs for annotation using UMBC STS either improves or reduces its performance. Google Word2Vec. W"
W16-3636,D14-1162,0,0.0802278,"ments beyond simply being the same topic, however the speakers are on opposite stance sides. Both of the arguments in row GM5 (MT AFS of 3.3) reference the same facet of the financial and legal benefits available to married couples, but Arg2 is more specific. Both Word2vec and our trained AFS model can recognize the similarity in the concepts in the two arguments and make good predictions. nation for learning similarity with vector representations works much better than the common practice of reducing a pair of vectors to a single score using cosine similarity. Previous work (Li et al., 2015; Pennington et al., 2014) also shows that all dimensions are not equally useful predictors for a specific task. For sentiment classification, Li et al. (2015) find that “too large a dimensionality leads many dimensions to be non-functional ... causing two sentences of opposite sentiment to differ only in a few dimensions.” This may also be the situation for the 300-dimensional embeddings used for AFS. Hence, when using concatenation, single dimensions can be weighted to adjust for non-functional dimensions, but using cosine makes this per-dimension weighting impossible. This might explain why our custom word2vec model"
W16-3636,N12-1019,0,0.0243786,"ing the training data with more highly similar pairs. 283 4 Related Work Table 4, suggests that treating facet discovery as a similarity problem is productive, i.e. examination of particular pairs suggests facets about legal and financial benefits for same-sex couples, the claim that the death penalty does not actually affect murder rates, and an assertion that “they”, implying “congress”, do not have the express, enumerated power to pass legislation restricting guns. Previous work shows that metrics used for evaluating machine translation quality perform well on paraphrase recognition tasks (Madnani et al., 2012). In our experiments, ROUGE performed very well, suggesting that other machine translation metrics such as Terp and Meteor may be useful (Snover et al., 2009; Lavie and Denkowski, 2009). We will explore this in future work. In future, we will use our AFS regressor to cluster and group similar arguments and produce argument facet summaries as a final output of our pipeline. Habernal and Gurevych (2015) apply clustering in argument mining by averaging word embeddings from posts and sentences from debate portals, clustering the resulting averaged vectors, and then computing distance measures from"
W16-3636,P14-5010,0,0.00341839,"performs better than RR, we use SVR only. Significance is calculated using paired t-tests between the RMSE values across folds. We paired Ngrams separately with LIWC and ROUGE to evaluate if the combination is significant. Ngram+Rouge (Row 1) is significantly better than Ngram for Gun Control and Death Penalty (p &lt; .01), and Gay Marriage (p = .03). Figure 7: LIWC Generalized Dep. tuples We create partially generalized LIWC dependency features and count overlap normalized by sentence length across pairs, building on previous work (Joshi and Penstein-Ros´e, 2009). Stanford dependency features (Manning et al., 2014) are generalized by leaving one dependency element lexicalized, replacing the other word in the dependency relation with its LIWC category and by removing the actual dependency type (nsubj, dobj, etc.) from the triple. This creates a tuple of (“governor token”, LIWC category of dependent token). We call these simplified LIWC dependencies. Fig. 7 illustrates the generalization process for three LIWC simplified dependencies, (”deter”, ”fear), (”deter”, ”punishment”), and (”deter”, ”love”). Because LIWC is a hierarchical lexicon, 8 One month sample https://www.reddit.com/ r/datasets/comments/3bxl"
W16-3636,W09-0441,0,0.020376,"i.e. examination of particular pairs suggests facets about legal and financial benefits for same-sex couples, the claim that the death penalty does not actually affect murder rates, and an assertion that “they”, implying “congress”, do not have the express, enumerated power to pass legislation restricting guns. Previous work shows that metrics used for evaluating machine translation quality perform well on paraphrase recognition tasks (Madnani et al., 2012). In our experiments, ROUGE performed very well, suggesting that other machine translation metrics such as Terp and Meteor may be useful (Snover et al., 2009; Lavie and Denkowski, 2009). We will explore this in future work. In future, we will use our AFS regressor to cluster and group similar arguments and produce argument facet summaries as a final output of our pipeline. Habernal and Gurevych (2015) apply clustering in argument mining by averaging word embeddings from posts and sentences from debate portals, clustering the resulting averaged vectors, and then computing distance measures from clusters to unseen sentences (“classification units”) as features. Cosine similarity between weighted and summed vector representations is also a comˇ mon a"
W16-3636,P09-1026,0,0.0340738,"e differences. Custom Word2Vec. We also create our own 300-dimensional embeddings for our dialogic doˇ uˇrek and Somain using the Gensim library (Reh˚ jka, 2010), with default settings, and a very large corpus of user-generated dialogic content. This includes the corpus described in Sec. 2 (929, 206 forum posts), an internal corpus of 1, 688, 639 tweets on various topics, and a corpus of 53, 851, 542 posts from Reddit.8 LIWC category and Dependency Overlap. Both dependency structures and the Linguistics Inquiry Word Count (LIWC) tool have been useful in previous work (Pennebaker et al., 2001; Somasundaran and Wiebe, 2009; Hasan and Ng, 2013). We develop a novel feature set that combines LIWC category and dependency overlap, aiming to capture a generalized notion of concept overlap between two arguments, i.e. to capture the hypothesis that classes of content words such as affective processes or emotion types are indicative of a shared facet across pairs of arguments. two dependencies may share many generalizations or only a few. Here, the tuples with dependent tokens fear and punishment are more closely related because their shared generalization include both Negative Emotion and Affective Processes, but the t"
W16-3636,C14-1142,0,0.0402574,"nt extraction, our AFS model, stance, post and author level information, so that our summaries represent the diversity of views on a topic, a quality not always guaranteed by summarization techniques, human or machine. There are many theories of argumentation that might be applicable for our task (Jackson and Jacobs, 1980; Reed and Rowe, 2004; Walton et al., 2008; Gilbert, 1997; Toulmin, 1958; Dung, 1995), but one definition of argument structure may not work for every NLP task. Social media arguments are often informal, and do not necessarily follow logical rules or schemas of argumentation (Stab and Gurevych, 2014; Peldszus and Stede, 2013; Ghosh et al., 2014; Habernal et al., 2014; Goudas et al., 2014; Cabrio and Villata, 2012). Moreover, in social media, segments of text that are argumentative must first be identified, as in our Task1. Habernal and Gurevych (2016) train a classifier to recognize text segments that are argumentative, but much previous work does Task1 manually. Goudas et al. (2014) annotate 16,000 sentences from social media documents and consider 760 of them to be argumentative. Hasan and Ng (2014) also manually identify argumentaˇ tive sentences, while Boltuzic and Snajder (2014) tre"
W16-3636,N15-1046,1,0.911578,"nature of firearms. A4: Gun ownership increases the risk of suicide. Con Arguments A5: Gun ownership is an integral facet of the right to self defense. A6: Gun ownership increases national security within democratic states. A7: Sports shooting is a safe activity. A8: Effective gun control is not achievable in democratic states with a tradition of civilian gun owership. facets evolve over time, and hence cannot be represented by a finite list. In our previous work on AFS, we developed an AFS regressor for predicting the similarity of human-generated labels for summaries of dialogic arguments (Misra et al., 2015). We collected 5 human summaries of each dialog, and then used the Pyramid tool and scheme to annotate sentences from these summaries as contributors to (paraphrases of) a particular facet (Nenkova and Passonneau, 2004). The Pyramid tool requires the annotator to provide a human readable label for a collection of contributors that realize the same propositional content. The AFS regressor operated on pairs of human-generated labels from Pyramid summaries of different dialogs about the same topic. In this case, facet identification is done by the human summarizers, and collections of similar lab"
W16-3636,W15-4631,1,0.91588,"of similar labels represent an argument facet. We believe this is a much easier task than the one we attempt here of training an AFS regressor on automatically extracted raw sentences from social media dialogs. The contributions of this paper are: Figure 2: The eight facets for Gun Control on IDebate, a curated debate site. a particular argument facet in dialog. We aim to use SENTENTIAL ARGUMENTS to produce extractive summaries of online dialogs about current social and political topics. This paper extends our previous work which frames our goal as consisting of two tasks (Misra et al., 2015; Swanson et al., 2015). • Task1: Argument Extraction: How can we extract sentences from dialog that clearly express a particular argument facet? • Task2: Argument Facet Similarity: How can we recognize that two sentential arguments are semantically similar, i.e. that they are different linguistic realizations of the same facet of the argument? • We develop a new corpus of sentential arguments with gold-standard labels for AFS. • We analyze and improve our argument extractor, by testing it on a much larger dataset. We develop a larger gold standard corpus for ARGUMENT QUALITY (AQ). • We develop a regressor that can"
W16-3636,N04-1019,0,0.0825,"ocratic states. A7: Sports shooting is a safe activity. A8: Effective gun control is not achievable in democratic states with a tradition of civilian gun owership. facets evolve over time, and hence cannot be represented by a finite list. In our previous work on AFS, we developed an AFS regressor for predicting the similarity of human-generated labels for summaries of dialogic arguments (Misra et al., 2015). We collected 5 human summaries of each dialog, and then used the Pyramid tool and scheme to annotate sentences from these summaries as contributors to (paraphrases of) a particular facet (Nenkova and Passonneau, 2004). The Pyramid tool requires the annotator to provide a human readable label for a collection of contributors that realize the same propositional content. The AFS regressor operated on pairs of human-generated labels from Pyramid summaries of different dialogs about the same topic. In this case, facet identification is done by the human summarizers, and collections of similar labels represent an argument facet. We believe this is a much easier task than the one we attempt here of training an AFS regressor on automatically extracted raw sentences from social media dialogs. The contributions of t"
W16-3636,W13-2324,0,\N,Missing
W16-3636,J17-1004,0,\N,Missing
W16-3644,P09-1068,0,0.194671,"and Abelson, 1977; Mooney and DeJong, 1985) This idea has motivated previous work exploring whether commonsense knowledge about events can be learned from text, however, only a few learn from data other than newswire (Hu et al., 2013; Manshadi et al., 2008; Beamer and Girju, 2009). News articles (obviously) cover newsworthy topics such 350 Proceedings of the SIGDIAL 2016 Conference, pages 350–359, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics grained knowledge we learn is simply not found in publicly available narrative and event schema collections (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013). Personal stories provide both advantages and disadvantages for learning common-sense knowledge about events. An advantage is that they tend to be told in chronological order (Swanson and Gordon, 2009), and temporal order between events is a strong cue to contingency (Prasad et al., 2008; Beamer and Girju, 2009). However, their structure is more similar to oral narrative than to newswire (Rahimtoroghi et al., 2014; Swanson et al., 2014). Only about a third of the sentences in a personal narrative describe actions,1 so novel methods are needed to find useful rela"
W16-3644,W04-3205,0,0.198946,"contingency (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Manshadi et al., 2008; Nguyen et al., 2015; Balasubramanian et al., 2013; Pichotta and Mooney, 2014). Historically, work on scripts explicitly modeled causality (Lehnert, 1981; Mooney and DeJong, 1985) inter alia. Our work is motivated by Penn Discourse Treebank (PDTB) definition of CONTINGENCY that has two types: CAUSE and CONDITION, and is more similar to approaches that learn specific event relations such as contingency or causality (Hu et al., 2013; Do et al., 2011; Girju, 2003; Riaz and Girju, 2010; Rink et al., 2010; Chklovski and Pantel, 2004). Our contributions are as follows: stories. We apply Causal Potential (Beamer and Girju, 2009) to model the contingency relation between two events. We directly compare our method to several other approaches as baselines (Sec. 3). We also identify topicindicative contingent event pairs from our topic-specific corpus that can be used as building blocks for generating coherent event chains and narrative schema for a particular theme (Sec. 4.3); • We conduct several experiments to evaluate the quality of the event knowledge learned in our work that indicate our results are contingent and topic-r"
W16-3644,D11-1027,0,0.121917,"ents that tend to co-occur”. Thus what is learned is not evaluated for contingency (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Manshadi et al., 2008; Nguyen et al., 2015; Balasubramanian et al., 2013; Pichotta and Mooney, 2014). Historically, work on scripts explicitly modeled causality (Lehnert, 1981; Mooney and DeJong, 1985) inter alia. Our work is motivated by Penn Discourse Treebank (PDTB) definition of CONTINGENCY that has two types: CAUSE and CONDITION, and is more similar to approaches that learn specific event relations such as contingency or causality (Hu et al., 2013; Do et al., 2011; Girju, 2003; Riaz and Girju, 2010; Rink et al., 2010; Chklovski and Pantel, 2004). Our contributions are as follows: stories. We apply Causal Potential (Beamer and Girju, 2009) to model the contingency relation between two events. We directly compare our method to several other approaches as baselines (Sec. 3). We also identify topicindicative contingent event pairs from our topic-specific corpus that can be used as building blocks for generating coherent event chains and narrative schema for a particular theme (Sec. 4.3); • We conduct several experiments to evaluate the quality of the event"
W16-3644,W03-1210,0,0.088369,"co-occur”. Thus what is learned is not evaluated for contingency (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Manshadi et al., 2008; Nguyen et al., 2015; Balasubramanian et al., 2013; Pichotta and Mooney, 2014). Historically, work on scripts explicitly modeled causality (Lehnert, 1981; Mooney and DeJong, 1985) inter alia. Our work is motivated by Penn Discourse Treebank (PDTB) definition of CONTINGENCY that has two types: CAUSE and CONDITION, and is more similar to approaches that learn specific event relations such as contingency or causality (Hu et al., 2013; Do et al., 2011; Girju, 2003; Riaz and Girju, 2010; Rink et al., 2010; Chklovski and Pantel, 2004). Our contributions are as follows: stories. We apply Causal Potential (Beamer and Girju, 2009) to model the contingency relation between two events. We directly compare our method to several other approaches as baselines (Sec. 3). We also identify topicindicative contingent event pairs from our topic-specific corpus that can be used as building blocks for generating coherent event chains and narrative schema for a particular theme (Sec. 4.3); • We conduct several experiments to evaluate the quality of the event knowledge le"
W16-3644,D13-1036,1,0.946361,"e knowledge about contingent relations between everyday events from such stories. We show that the fineIntroduction The original idea behind scripts as introduced by Schank was to capture knowledge about the finegrained events of everyday experience, such as opening a fridge enabling preparing food, or the event of getting out of bed being triggered by an alarm going off (Schank and Abelson, 1977; Mooney and DeJong, 1985) This idea has motivated previous work exploring whether commonsense knowledge about events can be learned from text, however, only a few learn from data other than newswire (Hu et al., 2013; Manshadi et al., 2008; Beamer and Girju, 2009). News articles (obviously) cover newsworthy topics such 350 Proceedings of the SIGDIAL 2016 Conference, pages 350–359, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics grained knowledge we learn is simply not found in publicly available narrative and event schema collections (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013). Personal stories provide both advantages and disadvantages for learning common-sense knowledge about events. An advantage is that they tend to be told in chronological order"
W16-3644,D13-1178,0,0.247052,"d DeJong, 1985) This idea has motivated previous work exploring whether commonsense knowledge about events can be learned from text, however, only a few learn from data other than newswire (Hu et al., 2013; Manshadi et al., 2008; Beamer and Girju, 2009). News articles (obviously) cover newsworthy topics such 350 Proceedings of the SIGDIAL 2016 Conference, pages 350–359, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics grained knowledge we learn is simply not found in publicly available narrative and event schema collections (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013). Personal stories provide both advantages and disadvantages for learning common-sense knowledge about events. An advantage is that they tend to be told in chronological order (Swanson and Gordon, 2009), and temporal order between events is a strong cue to contingency (Prasad et al., 2008; Beamer and Girju, 2009). However, their structure is more similar to oral narrative than to newswire (Rahimtoroghi et al., 2014; Swanson et al., 2014). Only about a third of the sentences in a personal narrative describe actions,1 so novel methods are needed to find useful relationships between events. Anoth"
W16-3644,P14-5010,0,0.00339718,"ject Lemma, prt:Particle) Table 3 shows example sentences describing an event from the Camping topic along with their event structure. The examples show how including the arguments often change the meaning of an event. In Row 1 the direct object and particle are required to completely understand the event in this sentence. Row 2 shows another example where the verb have cannot implicate what event is happening and the direct object oatmeal is needed to understand what has occurred in the story. We parse each sentence and extract every verb lemma with its arguments using Stanford dependencies (Manning et al., 2014). For each verb, we extract the nsubj, dobj, and prt dependency relations if they exist, and use their lemma in the event representation. To generalize the event representations, we use the types identified by Stanford’s Named Entity Recognizer and map each argument to its named entity type if available, e.g., in Row 3 of Table 3, the Lost Valley River Campground is represented by its type LOCATION. We use abstract types for named entities such as PERSON, ORGANIZATION , TIME and DATE. We also represent each pronoun by the abstract type PERSON, e.g. Row 5 in Table 3. 353 Event-Bigram model desc"
W16-3644,P08-1090,0,0.817203,"contingency (Prasad et al., 2008; Beamer and Girju, 2009). However, their structure is more similar to oral narrative than to newswire (Rahimtoroghi et al., 2014; Swanson et al., 2014). Only about a third of the sentences in a personal narrative describe actions,1 so novel methods are needed to find useful relationships between events. Another difference between our work and prior research is that much of the work on narrative schemas, scripts, or event schemas characterize what is learned as “collections of events that tend to co-occur”. Thus what is learned is not evaluated for contingency (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Manshadi et al., 2008; Nguyen et al., 2015; Balasubramanian et al., 2013; Pichotta and Mooney, 2014). Historically, work on scripts explicitly modeled causality (Lehnert, 1981; Mooney and DeJong, 1985) inter alia. Our work is motivated by Penn Discourse Treebank (PDTB) definition of CONTINGENCY that has two types: CAUSE and CONDITION, and is more similar to approaches that learn specific event relations such as contingency or causality (Hu et al., 2013; Do et al., 2011; Girju, 2003; Riaz and Girju, 2010; Rink et al., 2010; Chklovski and Pantel, 2004). Our contrib"
W16-3644,1985.tmi-1.17,0,0.589926,"ng up a tent and placing a tarp. The Storm story contains implicit knowledge about events such as the hurricane made landfall, the wind blew, a tree fell. Our aim is to learn fine-grained common-sense knowledge about contingent relations between everyday events from such stories. We show that the fineIntroduction The original idea behind scripts as introduced by Schank was to capture knowledge about the finegrained events of everyday experience, such as opening a fridge enabling preparing food, or the event of getting out of bed being triggered by an alarm going off (Schank and Abelson, 1977; Mooney and DeJong, 1985) This idea has motivated previous work exploring whether commonsense knowledge about events can be learned from text, however, only a few learn from data other than newswire (Hu et al., 2013; Manshadi et al., 2008; Beamer and Girju, 2009). News articles (obviously) cover newsworthy topics such 350 Proceedings of the SIGDIAL 2016 Conference, pages 350–359, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics grained knowledge we learn is simply not found in publicly available narrative and event schema collections (Chambers and Jurafsky, 2009; Balasubramanian"
W16-3644,P15-1019,0,0.0224447,"Missing"
W16-3644,E14-1024,0,0.587408,"re (Rahimtoroghi et al., 2014; Swanson et al., 2014). Only about a third of the sentences in a personal narrative describe actions,1 so novel methods are needed to find useful relationships between events. Another difference between our work and prior research is that much of the work on narrative schemas, scripts, or event schemas characterize what is learned as “collections of events that tend to co-occur”. Thus what is learned is not evaluated for contingency (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Manshadi et al., 2008; Nguyen et al., 2015; Balasubramanian et al., 2013; Pichotta and Mooney, 2014). Historically, work on scripts explicitly modeled causality (Lehnert, 1981; Mooney and DeJong, 1985) inter alia. Our work is motivated by Penn Discourse Treebank (PDTB) definition of CONTINGENCY that has two types: CAUSE and CONDITION, and is more similar to approaches that learn specific event relations such as contingency or causality (Hu et al., 2013; Do et al., 2011; Girju, 2003; Riaz and Girju, 2010; Rink et al., 2010; Chklovski and Pantel, 2004). Our contributions are as follows: stories. We apply Causal Potential (Beamer and Girju, 2009) to model the contingency relation between two ev"
W16-3644,prasad-etal-2008-penn,0,0.0976426,"opics such 350 Proceedings of the SIGDIAL 2016 Conference, pages 350–359, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics grained knowledge we learn is simply not found in publicly available narrative and event schema collections (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013). Personal stories provide both advantages and disadvantages for learning common-sense knowledge about events. An advantage is that they tend to be told in chronological order (Swanson and Gordon, 2009), and temporal order between events is a strong cue to contingency (Prasad et al., 2008; Beamer and Girju, 2009). However, their structure is more similar to oral narrative than to newswire (Rahimtoroghi et al., 2014; Swanson et al., 2014). Only about a third of the sentences in a personal narrative describe actions,1 so novel methods are needed to find useful relationships between events. Another difference between our work and prior research is that much of the work on narrative schemas, scripts, or event schemas characterize what is learned as “collections of events that tend to co-occur”. Thus what is learned is not evaluated for contingency (Chambers and Jurafsky, 2008; Cha"
W16-3644,W14-4323,1,0.865315,"Linguistics grained knowledge we learn is simply not found in publicly available narrative and event schema collections (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013). Personal stories provide both advantages and disadvantages for learning common-sense knowledge about events. An advantage is that they tend to be told in chronological order (Swanson and Gordon, 2009), and temporal order between events is a strong cue to contingency (Prasad et al., 2008; Beamer and Girju, 2009). However, their structure is more similar to oral narrative than to newswire (Rahimtoroghi et al., 2014; Swanson et al., 2014). Only about a third of the sentences in a personal narrative describe actions,1 so novel methods are needed to find useful relationships between events. Another difference between our work and prior research is that much of the work on narrative schemas, scripts, or event schemas characterize what is learned as “collections of events that tend to co-occur”. Thus what is learned is not evaluated for contingency (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Manshadi et al., 2008; Nguyen et al., 2015; Balasubramanian et al., 2013; Pichotta and Mooney, 2014). Historically, work on sc"
W16-3644,C90-2069,0,0.0562671,"topic-specific dataset. We used a 2-skip bigram model which considers two events to be adjacent if the second event occurs within two or less events after the first one. We use skip-2 bigram in order to capture the fact that two related events may often be separated by a non-essential event, because of the oralnarrative nature of our data (Rahimtoroghi et al., 2014). In contrast to the verbs that describe an event (e.g., hike, climb, evacuate, drive), some verbs describe private states such as as belong, depend, feel, know. We filter out clauses that tend to be associated with private states (Wiebe, 1990). A pilot evaluation showed that this improves the results. Equation 1 shows the formula for calculating Causal Potential of a pair consisting of two events: (e1 , e2 ). Here P denotes probability and P (e1 → e2 ) is the probability of e2 occurring after e1 in the adjacency window which is equal to 3 due to the skip-2 bigram model. P (e2 |e1 ) is the conditional probability of e2 given that e1 has been seen in the adjacency window. This is equivalent to the Verb Lemma (subj:Subject Lemma, dobj:Direct Object Lemma, prt:Particle) Table 3 shows example sentences describing an event from the Campi"
W17-2708,D13-1178,0,0.0174776,"(verbs) from newswire, with the HAPPENS - BEFORE relation defined as “indicating that the two verbs refer to two temporally disjoint intervals or instances”. WordNet’s cause relation, between a causative and a resultative verb (as in buy::own) is tagged as an instance of HAPPENS - BEFORE in VerbOcean, consistent with the heuristic that temporal ordering is a major component of causality. Other examples of the HAPPENS - BEFORE relation in the VerbOcean knowledge base include marry::divorce, detain::prosecute, enroll::graduate, schedule::reschedule, and tie::untie (Chklovski and Pantel, 2004). Balasubramanian et al. (2013) generate pairs of event relational tuples, called Rel-grams. The Rel-grams are publicly available through an online search interface1 . Rel-gram tuples are extracted using a co-occurrence statistical metric, Symmetric Conditional Probability (SCP), which combines Bigram probability in both directions as follows: SCP (e1 , e2 ) = P (e2 |e1 ) × P (e1 |e2 ) Background and Related Work (1) Their evaluation experiments directly compared the knowledge learned in Rel-grams to the previous work on narrative schemas (Chambers and Jurafsky, 2008, 2009), showing that they achieve better results, thus ou"
W17-2708,D13-1036,1,0.89085,"narrative structure because many types of natural language texts are narratively structured, e.g. news, reviews, film scripts, conversations, and personal blogs (Polanyi, 1989; Jurafsky et al., 2014; Bell, 2005; Gordon et al., 2011a). Human understanding of narrative is driven by reasoning about causal relations between the events and states in the story (Ger52 Proceedings of the Events and Stories in the News Workshop, pages 52–58, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics are also action-rich and told in fine-grained detail (Beamer and Girju, 2009; Hu et al., 2013). Moreover, both of these genres typically report events in temporal order, which is a primary cue to causality. In this position paper, we claim that knowledge about fine-grained causal relations between everyday events is often not available in news, and can be better learned from other narrative genres. For example, Figure 1 shows a part of a personal narrative written in a blog about a camping trip (Burton et al., 2009). The major event in this story is camping, which is contingent upon several finer-grained events, such as packing things the night before, waking up in the morning, packing"
W17-2708,P08-1090,0,0.0188164,"eschedule, and tie::untie (Chklovski and Pantel, 2004). Balasubramanian et al. (2013) generate pairs of event relational tuples, called Rel-grams. The Rel-grams are publicly available through an online search interface1 . Rel-gram tuples are extracted using a co-occurrence statistical metric, Symmetric Conditional Probability (SCP), which combines Bigram probability in both directions as follows: SCP (e1 , e2 ) = P (e2 |e1 ) × P (e1 |e2 ) Background and Related Work (1) Their evaluation experiments directly compared the knowledge learned in Rel-grams to the previous work on narrative schemas (Chambers and Jurafsky, 2008, 2009), showing that they achieve better results, thus our work compares directly to the tuples available in Rel-grams. Other work focuses more directly on learning causal or contingency relations between events. Cognitive theories of narrative understanding define narrative coherence in terms of four different sources of causal inferences between events A and B (Trabasso and van den Broek, 1985; Warren et al., 1979; Trabasso et al., 1989; Van den Broek, 1990). (1) Physical: A physically causes event B. (2) Motivational: A happens with B as a motivation. (3) Psychological: A brings about emot"
W17-2708,P09-1068,0,0.0684794,"Missing"
W17-2708,W04-3205,0,0.0732605,"relations between event types (verbs) from newswire, with the HAPPENS - BEFORE relation defined as “indicating that the two verbs refer to two temporally disjoint intervals or instances”. WordNet’s cause relation, between a causative and a resultative verb (as in buy::own) is tagged as an instance of HAPPENS - BEFORE in VerbOcean, consistent with the heuristic that temporal ordering is a major component of causality. Other examples of the HAPPENS - BEFORE relation in the VerbOcean knowledge base include marry::divorce, detain::prosecute, enroll::graduate, schedule::reschedule, and tie::untie (Chklovski and Pantel, 2004). Balasubramanian et al. (2013) generate pairs of event relational tuples, called Rel-grams. The Rel-grams are publicly available through an online search interface1 . Rel-gram tuples are extracted using a co-occurrence statistical metric, Symmetric Conditional Probability (SCP), which combines Bigram probability in both directions as follows: SCP (e1 , e2 ) = P (e2 |e1 ) × P (e1 |e2 ) Background and Related Work (1) Their evaluation experiments directly compared the knowledge learned in Rel-grams to the previous work on narrative schemas (Chambers and Jurafsky, 2008, 2009), showing that they"
W17-2708,E14-1024,0,0.0420681,"Missing"
W17-2708,D11-1027,0,0.726866,"Missing"
W17-2708,W16-3644,1,0.691989,"rs, selected from the high, middle and low CP ranges in their learned causal pairs. We compare their 30 highest CP events with causal event pairs that we learn from film. Riaz and Girju (2010) apply a similar measure to topic-sorted news stories about Hurricane Katrina and the Iraq War and present ranked causality relations between events for these topics, suggesting that topic-sorted corpora can produce better causal knowledge. Other work has also used CP to measure the contingency relation between two events, reporting better results than achieved with PMI or bigrams alone (Hu et al., 2013; Rahimtoroghi et al., 2016). 3 Corpus 3.1 Datasets Topical coherence and similarity of events within the corpus used for learning event relations can be as important as the size of the corpus (Riaz and Girju, 2010; Rahimtoroghi et al., 2016). We use two datasets for learning causal event pairs: firstperson narratives from blogs (Burton et al., 2009; Rahimtoroghi et al., 2016), and film scene descriptions (excluding dialogs because dialogs are not as action-rich) (Walker et al., 2012; Hu et al., 2013). Our experiment on blogs learns causal relations from a topic-sorted corpus of ∼1000 camping stories. We also posit that"
W17-2708,W13-4004,0,0.696443,"Missing"
W17-2708,W14-4323,1,0.800576,"genre overlaps with many other genres. We thus compare two narrow film genres of Fantasy and Mystery with the Drama genre from an existing corpus (Walker et al., 2012; Hu et al., 2013). The raw numbers for each subcorpus are shown in Table 1. Note that Camping corpus consists of blog posts which are much shorter compared to movie scripts. Thus their word count is much smaller compared to films corpus despite the larger number of documents. 3.2 Methods In the blogs, related event pairs are more frequently separated by utterances that provide state descriptions or affective reactions to events (Swanson et al., 2014). As a result, we use Causal Potential (CP) measure to assess the causal relation between events and apply skip-2 bigram method for modeling event pairs. But in film scenes, events are very densely distributed, thus related event pairs are often adjacent to one another and therefore nearby events are more likely to be causal. So, for event pairs extracted from Methods and Evaluations Our primary goal is simply to show that finegrained causal relations can be learned from film scripts and blogs, and that these are not found in causal knowledge bases learned from newswire. In this section we des"
W17-2708,D10-1008,0,0.0486303,"Missing"
W17-2708,walker-etal-2012-annotated,1,0.833632,"sure the contingency relation between two events, reporting better results than achieved with PMI or bigrams alone (Hu et al., 2013; Rahimtoroghi et al., 2016). 3 Corpus 3.1 Datasets Topical coherence and similarity of events within the corpus used for learning event relations can be as important as the size of the corpus (Riaz and Girju, 2010; Rahimtoroghi et al., 2016). We use two datasets for learning causal event pairs: firstperson narratives from blogs (Burton et al., 2009; Rahimtoroghi et al., 2016), and film scene descriptions (excluding dialogs because dialogs are not as action-rich) (Walker et al., 2012; Hu et al., 2013). Our experiment on blogs learns causal relations from a topic-sorted corpus of ∼1000 camping stories. We also posit that the genre of a film may select for similar types of events. However genres can be defined broadly or narrowly, e.g. the Drama genre overlaps with many other genres. We thus compare two narrow film genres of Fantasy and Mystery with the Drama genre from an existing corpus (Walker et al., 2012; Hu et al., 2013). The raw numbers for each subcorpus are shown in Table 1. Note that Camping corpus consists of blog posts which are much shorter compared to movie sc"
W17-4904,C00-1007,0,0.141564,"ses express extremely valenced reactions to restaurants, their menu items, and related attributes, using figurative language. The creativity exhibited in these user-generated restaurant reviews can be contrasted with natural language generation (NLG) for the restaurant domain. Methods for NLG typically begin with a structured meaning representation (MR), as shown in Table 2, and map these meaning representations into surface language forms, using a range of different methods, including template-based generation, statistically trained linguistically-informed NLG engines, and neural approaches (Bangalore and Rambow, 2000; Walker and Rambow, 2002). These approaches vary in the degree to which they can generate syntactically and semantically correct utterances, but in most cases the stylistic variation they can generate is extremely limited. Table 2 illustrates sample restaurant domain utterances produced by recent statistical/neural natural language generators (Higashinaka et al., 2007a; Mairesse and Walker, 2007; Wen et al., 2015; Novikova et al., 2016; Dusek and Jurc´ıcek, 2016). 1. Collect a large number of strongly positive and strongly negative reviews in the restaurant domain; One of the most prominent c"
W17-4904,W08-0119,0,0.136383,"recommendations (Higashinaka et al., 2007b; Mairesse and Walker, 2010; Dethlefs et al., 2014). Given this, it is surprising that previous work has not especially noted that restaurant reviews are a fertile source of creative and figurative language. For example, consider the elaborate descriptions in the restaurant reviews in Table 11 , e.g. phrases such as worst thing that Introduction The restaurant domain has been one of the most common applications for spoken dialogue systems for at least 25 years (Polifroni et al., 1992; Whittaker et al., 2002; Stent et al., 2004; Devillers et al., 2004; Gasic et al., 2008). There has been a tremendous amount of previous work on natural language generation of recommendations and descriptions for restaurants (Howcroft et al., 2013; Wen et al., 2015; Novikova et al., 2016), some of 1 Reviews from the Yelp 2016 dataset challenge: https://www.yelp.com/dataset_challenge 28 Proceedings of the Workshop on Stylistic Variation, pages 28–36 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics was simply the most incredible dining experience in my entire life.” Such frames can be seen in the reviews in Table 1, but we also see many ot"
W17-4904,W13-2104,0,0.216853,"ally noted that restaurant reviews are a fertile source of creative and figurative language. For example, consider the elaborate descriptions in the restaurant reviews in Table 11 , e.g. phrases such as worst thing that Introduction The restaurant domain has been one of the most common applications for spoken dialogue systems for at least 25 years (Polifroni et al., 1992; Whittaker et al., 2002; Stent et al., 2004; Devillers et al., 2004; Gasic et al., 2008). There has been a tremendous amount of previous work on natural language generation of recommendations and descriptions for restaurants (Howcroft et al., 2013; Wen et al., 2015; Novikova et al., 2016), some of 1 Reviews from the Yelp 2016 dataset challenge: https://www.yelp.com/dataset_challenge 28 Proceedings of the Workshop on Stylistic Variation, pages 28–36 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics was simply the most incredible dining experience in my entire life.” Such frames can be seen in the reviews in Table 1, but we also see many other idiomatic hyperbolic expressions such as out of this world (Cano Mora, 2009). Our goal is to develop a natural language generator for the restaurant domain"
W17-4904,W15-0515,1,0.876284,"Missing"
W17-4904,H92-1005,0,0.174996,"taset Challenge Corpus which has even focused on generating stylistically varied restaurant recommendations (Higashinaka et al., 2007b; Mairesse and Walker, 2010; Dethlefs et al., 2014). Given this, it is surprising that previous work has not especially noted that restaurant reviews are a fertile source of creative and figurative language. For example, consider the elaborate descriptions in the restaurant reviews in Table 11 , e.g. phrases such as worst thing that Introduction The restaurant domain has been one of the most common applications for spoken dialogue systems for at least 25 years (Polifroni et al., 1992; Whittaker et al., 2002; Stent et al., 2004; Devillers et al., 2004; Gasic et al., 2008). There has been a tremendous amount of previous work on natural language generation of recommendations and descriptions for restaurants (Howcroft et al., 2013; Wen et al., 2015; Novikova et al., 2016), some of 1 Reviews from the Yelp 2016 dataset challenge: https://www.yelp.com/dataset_challenge 28 Proceedings of the Workshop on Stylistic Variation, pages 28–36 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics was simply the most incredible dining experience in my"
W17-4904,P98-1116,0,0.223772,"al intensifiers like totally, absolutely, and incredibly can shift the strength of the assertion to extreme negative or positive. 2. Use a linguistic pattern learner to identify linguistic frames that use hyperbole; 3. Create generation templates from the identified linguistic patterns and infer their contexts of use; 4. Learn to rank the generation templates for convincingness and quality. We see Steps 1 to 3 as the overgeneration phase, aimed at vastly expanding the types of stylistic variation possible, while Step 4 is the ranking phase, in a classic overgenerate and rank NLG architecture (Langkilde and Knight, 1998; Rambow et al., 2001). We focus in this paper on Steps 1 to 3, expecting to improve these steps before we move on to Step 4. Thus, in this paper, we conducted an evaluation experiment to compare three different types of NLG templates: pre-defined BASIC templates similar to those used in current NLG engines for the restaurant domain (Walker et al., 2007; Wen et al., 2015), the basic templates stylized with Similarly, Kreuz and Roberts (1995) describe a standard frame for hyperbole in English where an adverb modifies an extreme, positive adjective, e.g. “That was absolutely amazing!” or “That 2"
W17-4904,P01-1056,1,0.631597,", absolutely, and incredibly can shift the strength of the assertion to extreme negative or positive. 2. Use a linguistic pattern learner to identify linguistic frames that use hyperbole; 3. Create generation templates from the identified linguistic patterns and infer their contexts of use; 4. Learn to rank the generation templates for convincingness and quality. We see Steps 1 to 3 as the overgeneration phase, aimed at vastly expanding the types of stylistic variation possible, while Step 4 is the ranking phase, in a classic overgenerate and rank NLG architecture (Langkilde and Knight, 1998; Rambow et al., 2001). We focus in this paper on Steps 1 to 3, expecting to improve these steps before we move on to Step 4. Thus, in this paper, we conducted an evaluation experiment to compare three different types of NLG templates: pre-defined BASIC templates similar to those used in current NLG engines for the restaurant domain (Walker et al., 2007; Wen et al., 2015), the basic templates stylized with Similarly, Kreuz and Roberts (1995) describe a standard frame for hyperbole in English where an adverb modifies an extreme, positive adjective, e.g. “That was absolutely amazing!” or “That 29 MR NLG System name[E"
W17-4904,D16-1230,0,0.073929,"Missing"
W17-4904,P07-1063,1,0.925969,"representations into surface language forms, using a range of different methods, including template-based generation, statistically trained linguistically-informed NLG engines, and neural approaches (Bangalore and Rambow, 2000; Walker and Rambow, 2002). These approaches vary in the degree to which they can generate syntactically and semantically correct utterances, but in most cases the stylistic variation they can generate is extremely limited. Table 2 illustrates sample restaurant domain utterances produced by recent statistical/neural natural language generators (Higashinaka et al., 2007a; Mairesse and Walker, 2007; Wen et al., 2015; Novikova et al., 2016; Dusek and Jurc´ıcek, 2016). 1. Collect a large number of strongly positive and strongly negative reviews in the restaurant domain; One of the most prominent characteristics of restaurant reviews in the Yelp corpus is the prevalent use of hyperbolic language, such as the phrase “incredibly remarkably terrible” in Table 1. Hyperbole is often found in persuasive language, and is classified as a form of figurative language (McCarthy and Carter, 2004; Cano Mora, 2009). Colston and O’Brien describe how an event or situation evokes a scale, and how hyperbole"
W17-4904,D15-1199,0,0.498462,"ant reviews are a fertile source of creative and figurative language. For example, consider the elaborate descriptions in the restaurant reviews in Table 11 , e.g. phrases such as worst thing that Introduction The restaurant domain has been one of the most common applications for spoken dialogue systems for at least 25 years (Polifroni et al., 1992; Whittaker et al., 2002; Stent et al., 2004; Devillers et al., 2004; Gasic et al., 2008). There has been a tremendous amount of previous work on natural language generation of recommendations and descriptions for restaurants (Howcroft et al., 2013; Wen et al., 2015; Novikova et al., 2016), some of 1 Reviews from the Yelp 2016 dataset challenge: https://www.yelp.com/dataset_challenge 28 Proceedings of the Workshop on Stylistic Variation, pages 28–36 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics was simply the most incredible dining experience in my entire life.” Such frames can be seen in the reviews in Table 1, but we also see many other idiomatic hyperbolic expressions such as out of this world (Cano Mora, 2009). Our goal is to develop a natural language generator for the restaurant domain that can harvest"
W17-4904,W16-6644,0,0.41068,"fertile source of creative and figurative language. For example, consider the elaborate descriptions in the restaurant reviews in Table 11 , e.g. phrases such as worst thing that Introduction The restaurant domain has been one of the most common applications for spoken dialogue systems for at least 25 years (Polifroni et al., 1992; Whittaker et al., 2002; Stent et al., 2004; Devillers et al., 2004; Gasic et al., 2008). There has been a tremendous amount of previous work on natural language generation of recommendations and descriptions for restaurants (Howcroft et al., 2013; Wen et al., 2015; Novikova et al., 2016), some of 1 Reviews from the Yelp 2016 dataset challenge: https://www.yelp.com/dataset_challenge 28 Proceedings of the Workshop on Stylistic Variation, pages 28–36 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics was simply the most incredible dining experience in my entire life.” Such frames can be seen in the reviews in Table 1, but we also see many other idiomatic hyperbolic expressions such as out of this world (Cano Mora, 2009). Our goal is to develop a natural language generator for the restaurant domain that can harvest and make use of these ty"
W17-4904,whittaker-etal-2002-fish,1,0.716804,"hich has even focused on generating stylistically varied restaurant recommendations (Higashinaka et al., 2007b; Mairesse and Walker, 2010; Dethlefs et al., 2014). Given this, it is surprising that previous work has not especially noted that restaurant reviews are a fertile source of creative and figurative language. For example, consider the elaborate descriptions in the restaurant reviews in Table 11 , e.g. phrases such as worst thing that Introduction The restaurant domain has been one of the most common applications for spoken dialogue systems for at least 25 years (Polifroni et al., 1992; Whittaker et al., 2002; Stent et al., 2004; Devillers et al., 2004; Gasic et al., 2008). There has been a tremendous amount of previous work on natural language generation of recommendations and descriptions for restaurants (Howcroft et al., 2013; Wen et al., 2015; Novikova et al., 2016), some of 1 Reviews from the Yelp 2016 dataset challenge: https://www.yelp.com/dataset_challenge 28 Proceedings of the Workshop on Stylistic Variation, pages 28–36 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics was simply the most incredible dining experience in my entire life.” Such fram"
W17-4904,W16-3604,1,0.898957,"Missing"
W17-4904,P06-1034,1,\N,Missing
W17-4904,C98-1112,0,\N,Missing
W17-4904,P04-1011,1,\N,Missing
W17-4911,P07-1063,1,0.831089,"Missing"
W17-4911,P09-4003,0,0.0208911,"cters and Penny’s social skills. Two additional female characters, both scientists, were introduced as love interests to two main male characters, and have since became main characters themselves. 4 P Y P ER (Bowden et al., 2016) is a spin-off implementation of P ERSONAGE (Mairesse and Walker, 2007) in Python that provides new controls for expressive NLG. It is currently part of the M2D Monolog-to-Dialogue generation (Bowden et al., 2016) framework, which we briefly describe the architecture below (Figure 1). The EST framework (Rishes et al., 2013) produces a story annotated by S CHEHERAZADE (Elson and McKeown, 2009) as a list of sentences represented as Deep Syntactic Structures (DsyntS). DSyntS is therefore a dependency-tree structure with nodes containing lexical information about words. This is the input format for the surface realizer RealPro (). M2D converts the story (list of DsyntS) into two-speaker dialogue by accepting input parameters that control the allocation of content, pragmatic markers, etc. DsyntS M2D: Monolog to Dialog Altered DsyntS Monolog Input Stylistic Features Extraction After extracting dialogic utterances from transcripts, we extract features reflecting particular linguistic beh"
W17-4911,A97-1039,0,0.0665541,"edges-per-sent words:[on the other hand, however, despite, though, also, even though, but], ingroup words, LIWC–{Conjunctions, Third Person Plural, See}, hedges per sentence persuasive words, emotional words, conceptual words, words:[even though, yet, while], Dialogue Act–emphasis, LIWC–{Personal Pronouns, Second person, Auxiliary Verbs, Function Words, Past Tense} LIWC–{Quantifiers, UniqueWords, FutureTense, Causation}, RID Emotion words, Dialogue Act - Continuer, opinion words, words:[though, but] and manipulates DSyntS to add expressive elements, and 4) send “expressive” DSyntS to RealPro (Lavoie and Rambow, 1997) (a sentence realizer) for generation. We focus on operation 3 where we use our learned character stylistic models to add expressive elements to generic sentences. 5.1 easily added. As an example, a partial mapping for LIWC categories are shown in Table 3. For multiple features mapped to the same P Y P ER parameter, we calculate a weighted average of the features. 5.2 Narrative Content Our narrative content comes from fables and stories: 1 fable (The Fox and the Crow) and 6 blog stories about garden, protest, squirrel, bug, employer, and storm (Gordon et al., 2007). We use The Fox and the Crow"
W17-5211,P14-1145,0,0.0280528,"Missing"
W17-5211,D14-1125,0,0.0286288,"Missing"
W17-5211,P13-2022,0,0.0315474,"king the user reactions directly to theories of well-being as exemplified in Table 1. Influential accounts such as Appraisal Theory (Scherer et al., 2001, 1986; Ortony et al., 1990) A related line of work builds lexico-semantic resources for sentiment analysis with a focus on how the participants of an event are affected by it. Goyal and Riloff (2013) bootstrap a set of patientpolarity verbs from narratives and Ding and Riloff (2016) extract event-triples from blogs that reliably indicate positive or negative affect on one of the event participants. Reed et al. (2017) take a similar approach. Deng et al. (2013) annotate how participants of an event are affected, and Deng & Wiebe (2014) show that this assists inference about the author’s sentiment towards entities or events. Balahur et al. (2012) use the narratives produced by the ISEAR questionnaire (Scherer et al., 1986) for first-person examples of particular emotions (“I felt angry when X and then Y happened”) and extract sequences of subject-verbobject triples, which they then annotate for seven 82 Row # Source Subtype Affect 1 Goals Achieved POS Thwarted NEG Autonomy POS 4 5 Lack-Autonomy Competence NEG POS 6 7 Incompetence Connection NEG POS L"
W17-5211,E14-1040,0,0.0130344,"n Table 1. Influential accounts such as Appraisal Theory (Scherer et al., 2001, 1986; Ortony et al., 1990) A related line of work builds lexico-semantic resources for sentiment analysis with a focus on how the participants of an event are affected by it. Goyal and Riloff (2013) bootstrap a set of patientpolarity verbs from narratives and Ding and Riloff (2016) extract event-triples from blogs that reliably indicate positive or negative affect on one of the event participants. Reed et al. (2017) take a similar approach. Deng et al. (2013) annotate how participants of an event are affected, and Deng & Wiebe (2014) show that this assists inference about the author’s sentiment towards entities or events. Balahur et al. (2012) use the narratives produced by the ISEAR questionnaire (Scherer et al., 1986) for first-person examples of particular emotions (“I felt angry when X and then Y happened”) and extract sequences of subject-verbobject triples, which they then annotate for seven 82 Row # Source Subtype Affect 1 Goals Achieved POS Thwarted NEG Autonomy POS 4 5 Lack-Autonomy Competence NEG POS 6 7 Incompetence Connection NEG POS Lack or Neg-Connection NEG Savouring Savouring POS NEG 2 3 Eudaimonics 8 9 He"
W17-5211,P13-1174,0,0.0430403,"Missing"
W17-5211,S15-2077,0,0.0399085,"Missing"
W17-5211,W15-0515,1,0.89465,"Missing"
W17-5211,P17-2022,1,0.79111,"ories of events mentioned in ECHO posts, linking the user reactions directly to theories of well-being as exemplified in Table 1. Influential accounts such as Appraisal Theory (Scherer et al., 2001, 1986; Ortony et al., 1990) A related line of work builds lexico-semantic resources for sentiment analysis with a focus on how the participants of an event are affected by it. Goyal and Riloff (2013) bootstrap a set of patientpolarity verbs from narratives and Ding and Riloff (2016) extract event-triples from blogs that reliably indicate positive or negative affect on one of the event participants. Reed et al. (2017) take a similar approach. Deng et al. (2013) annotate how participants of an event are affected, and Deng & Wiebe (2014) show that this assists inference about the author’s sentiment towards entities or events. Balahur et al. (2012) use the narratives produced by the ISEAR questionnaire (Scherer et al., 1986) for first-person examples of particular emotions (“I felt angry when X and then Y happened”) and extract sequences of subject-verbobject triples, which they then annotate for seven 82 Row # Source Subtype Affect 1 Goals Achieved POS Thwarted NEG Autonomy POS 4 5 Lack-Autonomy Competence N"
W17-5211,W11-0145,1,0.766151,"Missing"
W17-5211,W15-2910,0,0.0263205,"Missing"
W17-5211,baccianella-etal-2010-sentiwordnet,0,\N,Missing
W17-5211,J14-1002,0,\N,Missing
W17-5211,W14-3001,0,\N,Missing
W17-5537,P15-2122,0,0.054115,"Missing"
W17-5537,W10-2914,0,0.235582,"Missing"
W17-5537,filatova-2012-irony,0,0.028364,"only computational work utilizing that data is by Battasali et al. (2015), who used n-gram language models with pre- and post-context to distinguish RQs from regular questions in SWBDDAMSL. Using context improved their results to 0.83 F1 on a balanced dataset of 958 instances, demonstrating that context information could be very useful for this task. Although it has been observed in the literature that RQs are often used sarcastically (Gibbs, 2000; Ilie, 1994), previous work on sarcasm classification has not focused on RQs (Bamman and Smith, 2015; Riloff et al., 2013; Liebrecht et al., 2013; Filatova, 2012; Gonz´alez-Ib´an˜ ez et al., 2011; Davi311 S ARCASTIC dov et al., 2010; Tsur et al., 2010). Riloff et al. (2013) investigated the utility of sequential features in tweets, emphasizing a subtype of sarcasm that consists of an expression of positive emotion contrasted with a negative situation, and showed that sequential features performed much better than features that did not capture sequential information. More recent work on sarcasm has focused specifically on sarcasm identification on Twitter using neural network approaches (Poria et al, 2016; Ghosh and Veale, 2016; Zhang et al., 2016; Ami"
W17-5537,W16-0425,0,0.0291747,", 2013; Liebrecht et al., 2013; Filatova, 2012; Gonz´alez-Ib´an˜ ez et al., 2011; Davi311 S ARCASTIC dov et al., 2010; Tsur et al., 2010). Riloff et al. (2013) investigated the utility of sequential features in tweets, emphasizing a subtype of sarcasm that consists of an expression of positive emotion contrasted with a negative situation, and showed that sequential features performed much better than features that did not capture sequential information. More recent work on sarcasm has focused specifically on sarcasm identification on Twitter using neural network approaches (Poria et al, 2016; Ghosh and Veale, 2016; Zhang et al., 2016; Amir et al., 2016). Other work emphasizes features of semantic incongruity in recognizing sarcasm (Joshi et al., 2015; Reyes et al., 2012). Sarcastic RQs clearly feature semantic incongruity, in some cases by expressing the certainty of particular facts in the frame of a question, and in other cases by asking questions like “Can you read?” (Row 2 in Table 1), a competence which a speaker must have, prima facie, to participate in online discussion. To our knowledge, our previous work is the first to consider the task of distinguishing sarcastic vs. not-sarcastic RQs, where"
W17-5537,W15-4322,0,0.0560455,"Missing"
W17-5537,P11-2102,0,0.0564532,"g that data is by Battasali et al. (2015), who used n-gram language models with pre- and post-context to distinguish RQs from regular questions in SWBDDAMSL. Using context improved their results to 0.83 F1 on a balanced dataset of 958 instances, demonstrating that context information could be very useful for this task. Although it has been observed in the literature that RQs are often used sarcastically (Gibbs, 2000; Ilie, 1994), previous work on sarcasm classification has not focused on RQs (Bamman and Smith, 2015; Riloff et al., 2013; Liebrecht et al., 2013; Filatova, 2012; Gonz´alez-Ib´an˜ ez et al., 2011; Davi311 S ARCASTIC dov et al., 2010; Tsur et al., 2010). Riloff et al. (2013) investigated the utility of sequential features in tweets, emphasizing a subtype of sarcasm that consists of an expression of positive emotion contrasted with a negative situation, and showed that sequential features performed much better than features that did not capture sequential information. More recent work on sarcasm has focused specifically on sarcasm identification on Twitter using neural network approaches (Poria et al, 2016; Ghosh and Veale, 2016; Zhang et al., 2016; Amir et al., 2016). Other work emphas"
W17-5537,L16-1704,1,0.851195,"ds for both domains below. Corpus Creation Sarcasm is a prevalent discourse function of RQs. In previous work, we observe both sarcastic and not-sarcastic uses of RQs in forums, and collect a set of sarcastic and not-sarcastic RQs in debate by using a heuristic stating that an RQ is a question that occurs in the middle of a turn, and which is answered immediately by the speaker themselves (Oraby et al., 2016). RQs are thus defined intentionally: the speaker indicates that their intention is not to elicit an answer by not ceding the turn.3 Debate Forums: The Internet Argument Corpus (IAC 2.0) (Abbott et al., 2016) contains a large number of discussions about politics and social issues, making it a good source of RQs. Following our previous work (2016), we first extract RQs in 3 We acknowledge that this method may miss RQs that do not follow this heuristic, but opt to use this conservative pattern for expanding the data to avoid introducing extra noise. 312 FACTUAL /I NFO -S EEKING Q UESTIONS posts whose length varies from 10-150 words, and collect five annotations for each of the RQs paired with the context of their following statements. We ask Turkers to specify whether or not the RQ-response pair is"
W17-5537,D15-1019,1,0.82697,"off et al. (2013) investigated the utility of sequential features in tweets, emphasizing a subtype of sarcasm that consists of an expression of positive emotion contrasted with a negative situation, and showed that sequential features performed much better than features that did not capture sequential information. More recent work on sarcasm has focused specifically on sarcasm identification on Twitter using neural network approaches (Poria et al, 2016; Ghosh and Veale, 2016; Zhang et al., 2016; Amir et al., 2016). Other work emphasizes features of semantic incongruity in recognizing sarcasm (Joshi et al., 2015; Reyes et al., 2012). Sarcastic RQs clearly feature semantic incongruity, in some cases by expressing the certainty of particular facts in the frame of a question, and in other cases by asking questions like “Can you read?” (Row 2 in Table 1), a competence which a speaker must have, prima facie, to participate in online discussion. To our knowledge, our previous work is the first to consider the task of distinguishing sarcastic vs. not-sarcastic RQs, where we construct a corpus of sarcasm in three types: generic, RQ, and hyperbole, and provide simple baseline experiments using ngrams (0.70 F1"
W17-5537,J81-4005,0,0.672732,"Missing"
W17-5537,W13-1605,0,0.145752,"Missing"
W17-5537,D13-1066,1,0.934664,"Missing"
W17-5537,W16-3604,1,0.9192,"learly feature semantic incongruity, in some cases by expressing the certainty of particular facts in the frame of a question, and in other cases by asking questions like “Can you read?” (Row 2 in Table 1), a competence which a speaker must have, prima facie, to participate in online discussion. To our knowledge, our previous work is the first to consider the task of distinguishing sarcastic vs. not-sarcastic RQs, where we construct a corpus of sarcasm in three types: generic, RQ, and hyperbole, and provide simple baseline experiments using ngrams (0.70 F1 for SARC and 0.71 F1 for NOT- SARC) (Oraby et al., 2016). Here, we adopt the same heuristic for gathering RQs and expand the corpus in debate forums, also collecting a novel Twitter corpus. We show that we can distinguish between SARCASTIC and OTHER uses of RQs that we observe, such as argumentation and persuasion in forums and Twitter, respectively. We show that linguistic features aid in the classification task, and explore the effects of context, using traditional and neural models. 3 1 2 Do you even read what anyone posts? Try it, you might learn something.......maybe not....... If they haven’t been discovered yet, HOW THE BLOODY HELL DO YOU KN"
W17-5537,swanson-etal-2014-getting,1,0.901893,"Missing"
W17-5537,W15-0515,1,0.879,"Missing"
W17-5537,P15-2124,0,\N,Missing
W17-5537,N16-1146,1,\N,Missing
W17-5537,C16-1151,0,\N,Missing
W17-5537,C16-1231,0,\N,Missing
W17-5540,W12-3019,0,0.0386935,"Missing"
W17-5540,D13-1178,0,0.0435221,"Missing"
W17-5540,P08-1090,0,0.436813,"CHOLOGICAL: Event A brings about emotions (expressed in event B) • ENABLING: Event A creates a state or condition for B to happen. A enables B. Introduction Telling and understanding stories is a central part of human experience, and many types of human communication involve narrative structures. Theories of narrative posit that NARRATIVE CAUSAL ITY underlies human understanding of a narrative (Warren et al., 1979; Trabasso et al., 1989; Van den Broek, 1990). However previous computational work on narrative schemas, scripts or event schemas learn “collections of events that tend to co-occur” (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Pichotta and Mooney, 2014), rather than causal relations between events (Rahimtoroghi et al., 2016). Another limitation of previous work is that it has mostly been applied to newswire, limiting what is learned to relations between newsworthy events, rather than everyday Previous work on learning causal relations has primarily focused on physical causality (Riaz and Girju, 2010; Beamer and Girju, 2009), while our aim is to learn event pairs manifesting all types of narrative causality, and test their generality as a source of causal knowledge. We posit that film"
W17-5540,P09-1068,0,0.0523663,"ad et al., 2008). They present a detailed formula for calculating contingency/causality that takes into account several different kinds of argument overlap between adjacent events. However they do not provide any evidence that all the components of this formula actually contribute to their results. Gordon et al. (2011) used event ngrams and discourse cues to learn causal relations from first person stories posted on weblogs and evaluated them with respect to the COPA SEM-EVAL task. Other related work learns likely sequences of temporally ordered events but does not explicitly model CAUSALITY (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Manshadi et al., 2008). Work on VerbOcean (Chklovski and Pantel, 2004) use lexical patterns to learn semantic verb relations of similarity, strength, antonymy, enablement and happens-before relations. Balasubramanian et al. (2013) use symmetric probability to learn semantically typed relational triples (actor, relation, actor), which they call Rel-grams (relational n-grams), and show that their schemas outperform previous work (Chambers and Jurafsky, 2009). We thus compared our event pairs with Rel-grams, showing that humans are more likely to perceive narrative"
W17-5540,W04-3205,0,0.0838489,"o account several different kinds of argument overlap between adjacent events. However they do not provide any evidence that all the components of this formula actually contribute to their results. Gordon et al. (2011) used event ngrams and discourse cues to learn causal relations from first person stories posted on weblogs and evaluated them with respect to the COPA SEM-EVAL task. Other related work learns likely sequences of temporally ordered events but does not explicitly model CAUSALITY (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Manshadi et al., 2008). Work on VerbOcean (Chklovski and Pantel, 2004) use lexical patterns to learn semantic verb relations of similarity, strength, antonymy, enablement and happens-before relations. Balasubramanian et al. (2013) use symmetric probability to learn semantically typed relational triples (actor, relation, actor), which they call Rel-grams (relational n-grams), and show that their schemas outperform previous work (Chambers and Jurafsky, 2009). We thus compared our event pairs with Rel-grams, showing that humans are more likely to perceive narrative causality in our event pairs. 5 Discussion and Future Work We present an unsupervised model based on"
W17-5540,D11-1027,0,0.674292,"come part of the genres of Action, Adventure, and Fantasy. Each film’s scene descriptions ranges from 2000 to 35000 words. Table 1 enumerates the sizes of each genre, illustrating the potential tradeoff between getting good probability estimates for event co-occurrence when the same events are repeated within a genre, vs. across the whole corpus. We use Stanford CoreNLP 3.5.2 to tokenize, lemmatize, POS tag, dependency parse and label named entities (Manning et al., 2014). 2.2 Compute Event Representations. An event is defined as a verb lemma, as in previous work (Chambers and Jurafsky, 2008; Do et al., 2011; Riaz and Girju, 2010; Manshadi et al., 2008). 2 343 From https://nlds.soe.ucsc.edu/fc2 Genre Action Adventure Comedy Crime Drama Fantasy Horror Mystery Romance Sci-Fi Thriller # Films 290 166 347 201 579 113 149 107 192 155 373 Word Count 3,758,387 2,115,247 3,434,612 2,342,324 6,680,749 1,186,587 1,789,667 1,346,496 2,022,305 1,964,856 4,548,043 Example The Avengers Indiana Jones and the Temple of Doom All About Steve The Italian Job American Beauty Lord of the Rings: Fellowship of the Ring Scream Black Swan Last Tango in Paris I, Robot Ghost Rider Table 1: Distribution of Films By Genre. W"
W17-5540,J86-3001,0,0.478744,"ju, 2010). We obtain the frequency of every event and event pair for each genre. Unseen event pairs are smoothed with frequency equal to 1. In this paper, the notion of window size indicates how many events after the current event are paired with the current event. We use window sizes 1, 2 and 3, and calculate narrative causality for each window size. In film scenes, events are very densely distributed, (see Figure 1), thus related event pairs are often adjacent to one another, but the discourse structure of film scenes, not surprisingly, also contain related events separated by other events (Grosz and Sidner, 1986; Mann and Thompson, 1987). For example, in Scene 3 of Figure 1, Bilbo pulling out the ring enables him to slide it off his palm later (pull out slide off ). Moreover, while related events are less 344 In this task, we will present you with two pairs of events (upper case verbs) that were automatically extracted from film scripts, and ask you to tell us which event pair is more likely to have a narrative causality relation. According to the theories of narrative, in a pair of events [A -> B], the narrative causality relation consists of 4 possible types of event relations, given below with def"
W17-5540,D13-1036,1,0.931587,"at is learned to relations between newsworthy events, rather than everyday Previous work on learning causal relations has primarily focused on physical causality (Riaz and Girju, 2010; Beamer and Girju, 2009), while our aim is to learn event pairs manifesting all types of narrative causality, and test their generality as a source of causal knowledge. We posit that film scene descriptions are a good resource for learning narrative causality because they are: (1) action rich; (2) about everyday events; and (3) told in temporal order, providing a primary cue to causality (Beamer and Girju, 2009; Hu et al., 2013). Film scenes contain many descriptions encoding PHYSICAL CAUSALITY, e.g. in Fig. 1, Scene 1, Frodo grabs Pippin’s sleeve, causing Pippin to spill his beer (grab - spill). Pippin then pushes Frodo away, causing Frodo to stumble backwards and fall to the floor (push - stumble, stumble - fall, and push - fall). But they also contain all other types of narrative causality: in Scene 2, Gandalf has to stoop, because he wants to avoid hitting his head on the low ceiling (stoop - avoid: MOTIVA TIONAL ). He then looks around, and enjoys the result of looking: the familiarity of Bag End (look enjoy: PS"
W17-5540,P14-5010,0,0.00365523,"per genre range from 107 to 579. Films can belong to multiple genres, e.g. the scenes from The Fellowship of the Ring shown in Figure 1 would become part of the genres of Action, Adventure, and Fantasy. Each film’s scene descriptions ranges from 2000 to 35000 words. Table 1 enumerates the sizes of each genre, illustrating the potential tradeoff between getting good probability estimates for event co-occurrence when the same events are repeated within a genre, vs. across the whole corpus. We use Stanford CoreNLP 3.5.2 to tokenize, lemmatize, POS tag, dependency parse and label named entities (Manning et al., 2014). 2.2 Compute Event Representations. An event is defined as a verb lemma, as in previous work (Chambers and Jurafsky, 2008; Do et al., 2011; Riaz and Girju, 2010; Manshadi et al., 2008). 2 343 From https://nlds.soe.ucsc.edu/fc2 Genre Action Adventure Comedy Crime Drama Fantasy Horror Mystery Romance Sci-Fi Thriller # Films 290 166 347 201 579 113 149 107 192 155 373 Word Count 3,758,387 2,115,247 3,434,612 2,342,324 6,680,749 1,186,587 1,789,667 1,346,496 2,022,305 1,964,856 4,548,043 Example The Avengers Indiana Jones and the Temple of Doom All About Steve The Italian Job American Beauty Lord"
W17-5540,E14-1024,0,0.0151261,"nt B) • ENABLING: Event A creates a state or condition for B to happen. A enables B. Introduction Telling and understanding stories is a central part of human experience, and many types of human communication involve narrative structures. Theories of narrative posit that NARRATIVE CAUSAL ITY underlies human understanding of a narrative (Warren et al., 1979; Trabasso et al., 1989; Van den Broek, 1990). However previous computational work on narrative schemas, scripts or event schemas learn “collections of events that tend to co-occur” (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Pichotta and Mooney, 2014), rather than causal relations between events (Rahimtoroghi et al., 2016). Another limitation of previous work is that it has mostly been applied to newswire, limiting what is learned to relations between newsworthy events, rather than everyday Previous work on learning causal relations has primarily focused on physical causality (Riaz and Girju, 2010; Beamer and Girju, 2009), while our aim is to learn event pairs manifesting all types of narrative causality, and test their generality as a source of causal knowledge. We posit that film scene descriptions are a good resource for learning narrat"
W17-5540,prasad-etal-2008-penn,0,0.0571317,"oghi et al. (2016) also used a modified version of the the CP measure, adjusted to account for the discourse structure of personal narratives in blogs. Here we use a much larger set of films and apply different techniques and a detailed evaluation. Our learned causal pairs and supporting film data are available for download 6 . Do et al. (2011) used a minimally supervised approach, based on focused distributional similarity methods and discourse connectives, to identify 6 https://nlds.soe.ucsc.edu/narrativecausality causality relations between events in PDTB in context (both verbs and nouns) (Prasad et al., 2008). They present a detailed formula for calculating contingency/causality that takes into account several different kinds of argument overlap between adjacent events. However they do not provide any evidence that all the components of this formula actually contribute to their results. Gordon et al. (2011) used event ngrams and discourse cues to learn causal relations from first person stories posted on weblogs and evaluated them with respect to the COPA SEM-EVAL task. Other related work learns likely sequences of temporally ordered events but does not explicitly model CAUSALITY (Chambers and Jur"
W17-5540,W16-3644,1,0.391968,"enables B. Introduction Telling and understanding stories is a central part of human experience, and many types of human communication involve narrative structures. Theories of narrative posit that NARRATIVE CAUSAL ITY underlies human understanding of a narrative (Warren et al., 1979; Trabasso et al., 1989; Van den Broek, 1990). However previous computational work on narrative schemas, scripts or event schemas learn “collections of events that tend to co-occur” (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Pichotta and Mooney, 2014), rather than causal relations between events (Rahimtoroghi et al., 2016). Another limitation of previous work is that it has mostly been applied to newswire, limiting what is learned to relations between newsworthy events, rather than everyday Previous work on learning causal relations has primarily focused on physical causality (Riaz and Girju, 2010; Beamer and Girju, 2009), while our aim is to learn event pairs manifesting all types of narrative causality, and test their generality as a source of causal knowledge. We posit that film scene descriptions are a good resource for learning narrative causality because they are: (1) action rich; (2) about everyday event"
W17-5540,walker-etal-2012-annotated,1,0.836789,"t is learned when we train on genre specific texts vs. the whole collection. Our results show that: • human judges can distinguish between strong and weakly causal event pairs induced using our method (Section 3.1); 1 Gandalf did not turn in order to knock, which would have been MOTIVATIONAL. Nor was it entailed that turning would cause knocking, which would have been PHYSICAL, because he clearly could have missed hitting his head if he had been more careful. Experimental Method Film Scenes & Pre-Processing. We chose 11 genres with more than 100 films from a corpus of film scene descriptions (Walker et al., 2012; Hu et al., 2013),2 resulting in 955 unique films. Film scripts were scraped from the IMSDb website, film dialogs and scene descriptions were then automatically separated. Films per genre range from 107 to 579. Films can belong to multiple genres, e.g. the scenes from The Fellowship of the Ring shown in Figure 1 would become part of the genres of Action, Adventure, and Fantasy. Each film’s scene descriptions ranges from 2000 to 35000 words. Table 1 enumerates the sizes of each genre, illustrating the potential tradeoff between getting good probability estimates for event co-occurrence when th"
W17-5543,P11-2117,0,0.0152518,"ds on seminal work on a computational model of Lehnert’s plot units, that applied modern NLP tools to tracking narrative affect states in Aesop’s Fables (Goyal et al., 2010; Lehnert, 1981; Goyal and Riloff, 2013). Our framing of the problem is also inspired by recent work that identifies three forms of desire expressions in short narratives from MCTest and SimpleWiki and develops models to predict whether desires are fulfilled or unfulfilled (Chaturvedi et al., 2016). However DesireDB’s narrative and sentence structure is more complex than either MCTest or SimpleWiki (Richardson et al., 2013; Coster and Kauchak, 2011). We propose new features (Sec 4.1), as well as testing features used in previous work, and apply different classifiers to model desire fulfillment in our corpus. We also directly compare to results on MCTest and SimpleWiki (Sec 4.4). We apply LSTM models that distinguish between prior and post context and capture the flow of the narrative. Our best system, a Skip-Thought RNN model, achieves an F-measure of 0.70, while a logistic regression system achieves 0.66. Our models and features outperform Chaturvedi et al. (2016) on MCTest and SimpleWiki, while providing new results for a new corpus fo"
W17-5543,P13-1174,0,0.0288792,"es and goals, using different features and models, including LSTM architectures that can encode the sequential structure of the narratives. We first describe our features and models. Then, we present our feature analysis study to examine their importance in modeling fulfillment. Finally we provide results of direct comparison to previous work on the existing corpora. 364 for success or failure. One set (Discourse Features) looks for overt discourse relation markers that signal violation of expectation (e.g., ‘but’, ‘however’) or its opposite (e.g., ‘so’). Another uses the Connotation Lexicon (Feng et al., 2013) to model whether the context provides a positive or negative event. All of these features are inspired by Chaturvedi et al. (2016). Finally, motivated by the AESOP modeling of affect states for identifying plot units (Goyal and Riloff, 2013), one set of features (Sentiment-Flow-Features) indexes whether there has been a change in sentiment in the surrounding context (which might be the mention of a thwarted effort or a hard won victory). Figure 4 provides an example of this. Fulfilled Unfulfilled Unknown None Total 1,366 953 380 70 2,780 Table 2: Simple-DesireDB dataset Connotation-Features."
W17-5543,P98-1013,0,0.653226,"Missing"
W17-5543,D14-1181,0,0.00837078,"Missing"
W17-5543,D13-1170,0,0.00371391,"-Disgree-i is defined similarly. Sentiment-Flow-Features. To model affect states, we compute a sentiment score for the desire expression sentence as well as each sentence in the context. Then for each sentence of the context, the booleans Sentiment-Agree-i and SentimentDisagree-i mark whether that sentence and the desire expression sentence have the same sentiment polarity (see Figure 4). While there is evidence suggesting that models of implicit sentiment (e.g., (Goyal et al., 2010; Reed et al., 2017)) could do much better at tracking affect states, here we use the Stanford Sentiment system (Socher et al., 2013). In addition to a BOW (Bag of Words) baseline, we extracted the four types of features mentioned above. For features that examine the context around the desire expression, our experiments used the pre-context, the post-context, or both, as discussed below; context features are computed per sentence i of the context. We also tested various ablations of these features described below as well. We now describe the full set of features in more detail. Desire-Features. From a desire expression of the form ‘X Ved S’, we extract the lexical feature Desire-Verb, the lemma for V. We also extract a list"
W17-5543,W14-4323,1,0.817605,"maintaining that representation as the narrative evolves, as a vehicle for explaining the protagonist’s actions and tracking narrative outcomes (Elson, 2012; Rapp and Gerrig, 2006; Trabasso 1 https://nlds.soe.ucsc.edu/DesireDB 360 Proceedings of the SIGDIAL 2017 Conference, pages 360–369, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics post context of the desire expressions, since theories of narrative structure suggest that the evaluation points of a narrative can precede the expression of the events, goals and desires of the narrator (Labov, 1972; Swanson et al., 2014). Our approach builds on seminal work on a computational model of Lehnert’s plot units, that applied modern NLP tools to tracking narrative affect states in Aesop’s Fables (Goyal et al., 2010; Lehnert, 1981; Goyal and Riloff, 2013). Our framing of the problem is also inspired by recent work that identifies three forms of desire expressions in short narratives from MCTest and SimpleWiki and develops models to predict whether desires are fulfilled or unfulfilled (Chaturvedi et al., 2016). However DesireDB’s narrative and sentence structure is more complex than either MCTest or SimpleWiki (Richar"
W17-5543,W17-0906,0,0.0542412,"Missing"
W17-5543,D15-1257,0,0.0401994,"use if I was wrong that would make standing there the whole rest of the concert too awkward... (3) Afterward, I wandered through the city making stops at several bars and clubs, met some new people, some old people (4) As in people I knew - I actually didn’t met any old people, unless you count the tourist family whose dad asked me about my t-shirt. (5) And when I thought the night was over (and the doorman of the club did insist it was over) I met this great guy going into the subway. Figure 2: A desire expression with its surrounding context extracted from a personal narrative et al., 2014; Ouyang and McKeown, 2015, 2014). However there has been limited work on computational models for recognizing the expression of the protagonist’s goals and desires in narrative genres. Our approach builds on work by Goyal and Riloff (2013) that applied modern NLP tools to track narrative affect states in Aesop’s Fables (Goyal et al., 2010). They present a system called AESOP that uses a number of existing resources to identify affect states of the characters as part of deriving plot units. The motivation of modeling plot units is the idea that emotional reactions are central to the notion of a narrative and the main p"
W17-5543,ouyang-mckeown-2014-towards,0,0.108199,"Missing"
W17-5543,W16-3644,1,0.849165,"r study shows that many affect states arise from events where a character is acted upon in positive or negative ways, not explicit expression of emotions. They also show that most of the affect states emerge by the expression of goals and plans and goal completion. Some of our features are motivated by the idea that implicit sentiment polarity can represent success or failure of goals and can be used to better model desire and goal Related Work There has recently been an upsurge in interest in computational models of narrative structure (Lehnert, 1981; Wilensky, 1982) and story understanding (Rahimtoroghi et al., 2016; Swanson 361 3.1 fulfillment in a narrative (Reed et al., 2017), although we cannot directly compare our findings to theirs because their annotations are not publicly available. Chaturvedi et al. (2016) exploit two deliberately simplified datasets in order to model desire and its fulfillment: MCTest which contains 660 stories limited to content understandable by 7-year old children, and, SimpleWiki created from a dump of the Simple English Wikipedia discarding all the lists, tables and titles. They use desire statements matching a list of three verb phrases, wanted to, hoped to, and wished to"
W17-5543,P17-2022,1,0.884075,"er is acted upon in positive or negative ways, not explicit expression of emotions. They also show that most of the affect states emerge by the expression of goals and plans and goal completion. Some of our features are motivated by the idea that implicit sentiment polarity can represent success or failure of goals and can be used to better model desire and goal Related Work There has recently been an upsurge in interest in computational models of narrative structure (Lehnert, 1981; Wilensky, 1982) and story understanding (Rahimtoroghi et al., 2016; Swanson 361 3.1 fulfillment in a narrative (Reed et al., 2017), although we cannot directly compare our findings to theirs because their annotations are not publicly available. Chaturvedi et al. (2016) exploit two deliberately simplified datasets in order to model desire and its fulfillment: MCTest which contains 660 stories limited to content understandable by 7-year old children, and, SimpleWiki created from a dump of the Simple English Wikipedia discarding all the lists, tables and titles. They use desire statements matching a list of three verb phrases, wanted to, hoped to, and wished to. Their context representation consists of five or fewer sentenc"
W17-5543,D13-1020,0,0.0606603,"2014). Our approach builds on seminal work on a computational model of Lehnert’s plot units, that applied modern NLP tools to tracking narrative affect states in Aesop’s Fables (Goyal et al., 2010; Lehnert, 1981; Goyal and Riloff, 2013). Our framing of the problem is also inspired by recent work that identifies three forms of desire expressions in short narratives from MCTest and SimpleWiki and develops models to predict whether desires are fulfilled or unfulfilled (Chaturvedi et al., 2016). However DesireDB’s narrative and sentence structure is more complex than either MCTest or SimpleWiki (Richardson et al., 2013; Coster and Kauchak, 2011). We propose new features (Sec 4.1), as well as testing features used in previous work, and apply different classifiers to model desire fulfillment in our corpus. We also directly compare to results on MCTest and SimpleWiki (Sec 4.4). We apply LSTM models that distinguish between prior and post context and capture the flow of the narrative. Our best system, a Skip-Thought RNN model, achieves an F-measure of 0.70, while a logistic regression system achieves 0.66. Our models and features outperform Chaturvedi et al. (2016) on MCTest and SimpleWiki, while providing new"
W18-5003,H05-1042,0,0.0386246,"Missing"
W18-5003,W16-3622,0,0.0502973,"Missing"
W18-5003,W17-4912,0,0.0155998,"r, 2017). DAS scores can be used as generation decision probabilities. A DAS score of 0.48 for the LIWC feature set indicates that the probability of adapting to LIWC features in discourse context (prime) is 0.48. By mapping DAS scores to generation parameters, the generator could be directly controlled to exhibit the correct amount of adaptation for any feature set. Neural NLG. Recent work in Neural NLG (NNLG) explores controlling stylistic variation in outputs using a vector to encode style parameters, possibly in combination with the use of a context vector to represent the dialog context (Ficler and Goldberg, 2017; Oraby et al., 2018). The vector based probabilities that are represented in the DAS adaptation model could be encoded into the context vector in NNLG. No other known adaptation measures could be used in this way. We hypothesize that different conversational contexts may lead to more or less adaptive behavior, so we apply DAS on four human-human dialog corpora: two task-oriented dialog corpora that were designed to elicit adaptation (ArtWalk and Walking Around), one topic-centric spontaneous dialog corpus (Switchboard), and the MapTask Corpus used in much previous work. We obtain linguistic f"
W18-5003,W08-0127,0,0.0958005,"Missing"
W18-5003,W12-1508,0,0.0280855,"al patterns of behavior. Theories of personality aim to explain these patterns in terms of personality traits, e.g. the Big Five traits of extraversion or agreeableness. Previous work has shown: (1) the language that people generate includes linguistic features that express these personality traits; (2) it is possible to train models to automatically recognize a person’s personality from his language; and (3) it is possible to automatically train models for natural language generation that express personality traits (Pennebaker and King, 1999; Mairesse et al., 2007; Mairesse and Walker, 2011; Gill et al., 2012). A distinct line of work has shown that people adapt to one another’s conversational behaviors and that conversants reliably re-use or mimic many We posit that it is crucial to enable adaptation in computer agents in order to make them more human-like. However, we need models to control the amount of adaptation in natural language generation. A primary challenge is that dialogs exhibit many different types of linguistic features, any or all of which, in principle, could be adapted. Previous work has often focused on individual features when measuring adaptation, and referring expressions have"
W18-5003,W09-0612,0,0.0282493,"e calculation. Repetition decay measures observe the decay rate of repetition probability of linguistic features. Previous work has fit the probability of linguistic feature repetition decrease with the distance between prime and target in logarithmic decay models (Reitter et al., 2006a,b; Reitter, 2008), linear decay models (Ward and Litman, 2007), and exponential decay models (Pietsch et al., 2012). Previous work on linguistic adaptation in natural language generation has also attempted to use adaptation models learned from human conversations. The alignment-capable microplanner SPUD prime (Buschmeier et al., 2009, 2010) uses the repetition decay model from Reitter (2008) as part of the activation functions for linguistic structures. However, the parameters are not learned from real 7 Discussion and Future Work To obtain models of linguistic adaptation, most measures could only measure an individual feature at a time, and need the whole dialog to calculate the measure (Church, 2000; Stenchikova and Stent, 2007; Danescu-Niculescu-Mizil et al., 2012; Pietsch et al., 2012; Reitter et al., 2006b; Ward and Litman, 2007). This paper proposes the Dialog Adaptation Score (DAS) measure, which can be applied to"
W18-5003,W06-1405,0,0.0513572,"logs stay relatively stable. Figure 6 in Appendix shows plots of average DAS scores on different window sizes for original and randomized dialogs. Plots of the AWC and WAC show similar trends. Experiments with larger window sizes show that the original and random scores meet at window size 6 - 7 (with different versions of randomized dialogs). In MapTask, the original and random scores meet at window size 3 - 4. In SWBD, original and random scores meet at window size 2. 6 data. Repetition decay models do well in statistical parameterized NLG, but is hard to apply to overgenerate and rank NLG. Isard et al. (2006) apply a pre-trained n-grams adaptation model to generate conversations. Hu et al. (2014) explore the effects of adaptation to various features by human evaluations, but their generator is not capable of deciding which features to adapt based on input context. Duˇsek and Jurˇc´ıcˇ ek (2016) use a seq2seq model to generate responses adapting to previous context. They utilize an n-gram match ranker that promotes outputs with phrase overlap with context. Our learned adaptation models could serve as a ranker. In addition to n-grams, DAS could produce models with any combinations of feature sets, p"
W18-5003,C00-1027,0,0.536142,"as a ranker. In addition to n-grams, DAS could produce models with any combinations of feature sets, providing more versatile adaptation behavior. Related Work Recent measures of linguistic adaptation fall into three categories: probabilistic measures, repetition decay measures, and document similarity measures (Xu and Reitter, 2015). Probabilistic measures compute the probability of a single linguistic feature appearing in the target after its appearance in the prime. Some measures in this category focus more on comparing adaptation amongst features and do not handle turn by turn adaptation (Church, 2000; Stenchikova and Stent, 2007). Moreover, these measures produce scores for individual features, which need aggregation to reflect overall adaptivity (Danescu-Niculescu-Mizil et al., 2011, 2012). Document similarity measures calculate the similarity between prime and target by measuring the number of features that appear in both prime and target, normalized by the size of the two text sets (Wang et al., 2014). Both probabilistic measures and document similarity measures require the whole dialog to be complete before calculation. Repetition decay measures observe the decay rate of repetition pr"
W18-5003,W17-4911,1,0.746536,"vectors and learned DAS adaptation model. The response with the smallest distance is the response with the best amount of adaptation. We can also emphasize specific feature sets by giving weights to different dimensions of the vector and calculating weighted distance. For instance, in order to adapt more to personality and avoid too much lexical mimicry, one could prioritize related LIWC features, and adapt by using words from the same LIWC categories. Statistical Parameterized NLG. Some NLG engines provide a list of parameters that can be controlled at generation time (Paiva and Evans, 2004; Lin and Walker, 2017). DAS scores can be used as generation decision probabilities. A DAS score of 0.48 for the LIWC feature set indicates that the probability of adapting to LIWC features in discourse context (prime) is 0.48. By mapping DAS scores to generation parameters, the generator could be directly controlled to exhibit the correct amount of adaptation for any feature set. Neural NLG. Recent work in Neural NLG (NNLG) explores controlling stylistic variation in outputs using a vector to encode style parameters, possibly in combination with the use of a context vector to represent the dialog context (Ficler a"
W18-5003,L16-1504,1,0.852672,"logs where the orders of the turns have been randomized. We then show how DAS varies as a function of the feature sets used and the dialog corpora. We also show how DAS can be used for fine-grained adaptation by applying DAS to individual dialog segments, and individual speakers, and illustrating the differences in adaptation as a function of these variables. Finally, we show how DAS scores decrease as the adaptation window size increases. 3 Corpora We develop models of adaptation using DAS on the following four corpora. ArtWalk Corpus (AWC).1 Figure 1 provides a sample of the Artwalk Corpus (Liu et al., 2016), a collection of mobile-to-Skype conversations between friend and stranger dyads performing a real world-situated task that was designed to elicit adaptation behaviors. Every dialog involves a stationary director on campus, and a follower downtown. The director provided directions to help the follower find 10 public art pieces such as sculptures, mosaics, or murals in downtown Santa Cruz. The director had access to Google Earth views of the follower’s route and a map with locations and pictures of art pieces. The corpus consists of transcripts of 24 friend and 24 stranger dyads (48 dialogs)."
W18-5003,J11-3002,1,0.750306,"yet they often share general patterns of behavior. Theories of personality aim to explain these patterns in terms of personality traits, e.g. the Big Five traits of extraversion or agreeableness. Previous work has shown: (1) the language that people generate includes linguistic features that express these personality traits; (2) it is possible to train models to automatically recognize a person’s personality from his language; and (3) it is possible to automatically train models for natural language generation that express personality traits (Pennebaker and King, 1999; Mairesse et al., 2007; Mairesse and Walker, 2011; Gill et al., 2012). A distinct line of work has shown that people adapt to one another’s conversational behaviors and that conversants reliably re-use or mimic many We posit that it is crucial to enable adaptation in computer agents in order to make them more human-like. However, we need models to control the amount of adaptation in natural language generation. A primary challenge is that dialogs exhibit many different types of linguistic features, any or all of which, in principle, could be adapted. Previous work has often focused on individual features when measuring adaptation, and referr"
W18-5003,N06-2031,0,0.359593,"y, and that adaptation varies according to corpora and task, speaker, and the set of features used to model it. We also produce fine-grained models according to the dialog segmentation or the speaker, and demonstrate the decaying trend of adaptation. 1 Figure 1: Dialog excerpt from the ArtWalk Corpus. different aspects of their partner’s verbal and nonverbal behaviors, including lexical and syntactical traits, accent, speech rate, pause length, etc. (Coupland et al., 1988; Willemyns et al., 1997; Brennan and Clark, 1996; Branigan et al., 2010; Coupland et al., 1988; Parent and Eskenazi, 2010; Reitter et al., 2006a; Chartrand and Bargh, 1999; Hu et al., 2014). Previous work primarily focuses on developing methods on measuring adaptation in dialog, and studies have shown that adaptation measures are correlated with task success (Reitter and Moore, 2007), and that social variables such as power affect adaptation (Danescu-Niculescu-Mizil et al., 2012). Introduction Every person is unique, yet they often share general patterns of behavior. Theories of personality aim to explain these patterns in terms of personality traits, e.g. the Big Five traits of extraversion or agreeableness. Previous work has shown:"
W18-5003,P14-5010,0,0.00451699,"sed on lexical and syntactical features, which are included as baselines. Referring expressions and discourse markers are key features that are commonly studied for adaptation behaviors in task-oriented dialogs, which are often hand annotated. Here we automatically extract these features by rules. To model adaptation on the personality level, we draw features that correlate significantly with personality ratings from LIWC features. We hypothesize that our feature sets will demonstrate different adaptation models. We lemmatize, POS tag and derive constituency structures using Stanford CoreNLP (Manning et al., 2014). We then extract the following linguistic features from annotations and raw text. The following example features are based on D137 in Figure 2. Unigram Lemma/POS. We use lemma combined with POS tags to distinguish word senses. E.g., lemmapos building/NN and lemmapos brick/NNS in D137. Bigram Lemma. E.g., bigram the-brick and bigram side-of in D137. Syntactic Structure. Following Reitter et al. (2006b), we take all the subtrees from a constituency parse tree (excluding the leaf nodes that contain words) as features. E.g., syntax VP->VBP+PP and syntax ADJP-> DT+JJ in D137. The difference is tha"
W18-5003,P07-1102,0,0.0369277,"adaptation. 1 Figure 1: Dialog excerpt from the ArtWalk Corpus. different aspects of their partner’s verbal and nonverbal behaviors, including lexical and syntactical traits, accent, speech rate, pause length, etc. (Coupland et al., 1988; Willemyns et al., 1997; Brennan and Clark, 1996; Branigan et al., 2010; Coupland et al., 1988; Parent and Eskenazi, 2010; Reitter et al., 2006a; Chartrand and Bargh, 1999; Hu et al., 2014). Previous work primarily focuses on developing methods on measuring adaptation in dialog, and studies have shown that adaptation measures are correlated with task success (Reitter and Moore, 2007), and that social variables such as power affect adaptation (Danescu-Niculescu-Mizil et al., 2012). Introduction Every person is unique, yet they often share general patterns of behavior. Theories of personality aim to explain these patterns in terms of personality traits, e.g. the Big Five traits of extraversion or agreeableness. Previous work has shown: (1) the language that people generate includes linguistic features that express these personality traits; (2) it is possible to train models to automatically recognize a person’s personality from his language; and (3) it is possible to automa"
W18-5003,W18-5019,1,0.842026,"used as generation decision probabilities. A DAS score of 0.48 for the LIWC feature set indicates that the probability of adapting to LIWC features in discourse context (prime) is 0.48. By mapping DAS scores to generation parameters, the generator could be directly controlled to exhibit the correct amount of adaptation for any feature set. Neural NLG. Recent work in Neural NLG (NNLG) explores controlling stylistic variation in outputs using a vector to encode style parameters, possibly in combination with the use of a context vector to represent the dialog context (Ficler and Goldberg, 2017; Oraby et al., 2018). The vector based probabilities that are represented in the DAS adaptation model could be encoded into the context vector in NNLG. No other known adaptation measures could be used in this way. We hypothesize that different conversational contexts may lead to more or less adaptive behavior, so we apply DAS on four human-human dialog corpora: two task-oriented dialog corpora that were designed to elicit adaptation (ArtWalk and Walking Around), one topic-centric spontaneous dialog corpus (Switchboard), and the MapTask Corpus used in much previous work. We obtain linguistic features using fully a"
W18-5003,2007.sigdial-1.29,0,0.171458,"for dialogs that integrates the predictions of both personality theories and adaptation theories. NLGs need to operate as a dialog unfolds on a turnby-turn basis, thus the requirements for a model of adaptation for NLG are different than simply measuring adaptation. Method and Overview Our goal is an algorithm for adaptive natural language generation (NLG) that controls the system output at each step of the dialog. Our first aim therefore is a measure of dialog adaptation that can be applied on a turn by turn basis as a dialog unfolds. For this purpose, previous measures of dialog adaptation (Stenchikova and Stent, 2007; Danescu-Niculescu-Mizil et al., 2011) have two limitations: (1) their calculation require the complete dialog, and (2) they focus on single features and do not provide a model to control the interaction of multiple parameters in a single output, while our method measures adaptation with respect to any set of features. We further compare our method to existing measures in Section 6. Measures of adaptation focus on prime-target pairs: (p, t), in which the prime contains linguistic features that the target may adapt to. While linguistic adaptation occur beyond the next turn, we simplify the cal"
W18-5003,W15-1107,0,0.0192478,"which features to adapt based on input context. Duˇsek and Jurˇc´ıcˇ ek (2016) use a seq2seq model to generate responses adapting to previous context. They utilize an n-gram match ranker that promotes outputs with phrase overlap with context. Our learned adaptation models could serve as a ranker. In addition to n-grams, DAS could produce models with any combinations of feature sets, providing more versatile adaptation behavior. Related Work Recent measures of linguistic adaptation fall into three categories: probabilistic measures, repetition decay measures, and document similarity measures (Xu and Reitter, 2015). Probabilistic measures compute the probability of a single linguistic feature appearing in the target after its appearance in the prime. Some measures in this category focus more on comparing adaptation amongst features and do not handle turn by turn adaptation (Church, 2000; Stenchikova and Stent, 2007). Moreover, these measures produce scores for individual features, which need aggregation to reflect overall adaptivity (Danescu-Niculescu-Mizil et al., 2011, 2012). Document similarity measures calculate the similarity between prime and target by measuring the number of features that appear"
W18-5019,W17-4912,0,0.0662933,"It is kid friendly. It is a pub. It is in riverside. 3 DISAG+ CONSC 3.81 0.84 lack of parallel corpora, the difficulty of evaluating content preservation (semantic fidelity), and the challenges with measuring whether the outputs realize a particular style. Previous experiments attempt to control the sentiment and verb tense of generated movie review sentences (Hu et al., 2017), the content preservation and style transfer of news headlines and product review sentences (Fu et al., 2018), multiple automatically extracted style attributes along with sentiment and sentence theme for movie reviews (Ficler and Goldberg, 2017), sentiment, fluency and semantic equivalence (Shen et al., 2017), utterance length and topic (Fan et al., 2017), and the personality of customer care utterances in dialogue (Herzig et al., 2017). However, to our knowledge, no previous work evaluates simultaneous achievement of multiple targets as we do. Recent work introduces a large parallel corpus that varies on the formality dimension, and introduces several novel evaluation metrics, including a custom trained model for measuring semantic fidelity (Rao and Tetreault). Browns Cambridge is damn moderately priced, also it’s in city centre. It"
W18-5019,W08-0119,0,0.0689804,"Missing"
W18-5019,W00-1401,0,0.358364,"the encoder of the seq2seq architecture (Sutskever et al., 2014). The attention (Bahdanau et al., 2014) is computed over all of the encoder states and the hidden state of the fully connected network. Again, we set the learning rate, alpha decay, and maximum training epochs (up to 20) based on loss monitoring on the validation set. 4 Quantitative Results Here, we present results on controlling stylistic variation while maintaining semantic fidelity. 4.1 Evaluating Semantic Quality It is widely agreed that new evaluation metrics are needed for NLG (Langkilde-Geary, 2002; Belz and Reiter, 2006; Bangalore et al., 2000; Novikova et al., 2017a). We first present automated metrics used in NLG to measure how well model outputs compare to P ERSONAGE input, then introduce novel metrics designed to fill the gap left by current evaluation metrics. Automatic Metrics. The automatic evaluation uses the E2E generation challenge script.8 Table 4 summarizes the results for BLEU (n-gram precision), NIST (weighted n-gram precision), METEOR (n-grams with synonym recall), and ROUGE (n-gram recall). Although the differences in metrics are small, M ODEL CONTEXT shows a slight improvement across all of the metrics. Figure 3: N"
W18-5019,E06-1040,0,0.15828,"itectures. time step of the encoder of the seq2seq architecture (Sutskever et al., 2014). The attention (Bahdanau et al., 2014) is computed over all of the encoder states and the hidden state of the fully connected network. Again, we set the learning rate, alpha decay, and maximum training epochs (up to 20) based on loss monitoring on the validation set. 4 Quantitative Results Here, we present results on controlling stylistic variation while maintaining semantic fidelity. 4.1 Evaluating Semantic Quality It is widely agreed that new evaluation metrics are needed for NLG (Langkilde-Geary, 2002; Belz and Reiter, 2006; Bangalore et al., 2000; Novikova et al., 2017a). We first present automated metrics used in NLG to measure how well model outputs compare to P ERSONAGE input, then introduce novel metrics designed to fill the gap left by current evaluation metrics. Automatic Metrics. The automatic evaluation uses the E2E generation challenge script.8 Table 4 summarizes the results for BLEU (n-gram precision), NIST (weighted n-gram precision), METEOR (n-grams with synonym recall), and ROUGE (n-gram recall). Although the differences in metrics are small, M ODEL CONTEXT shows a slight improvement across all of"
W18-5019,W17-3541,0,0.0877355,"asuring whether the outputs realize a particular style. Previous experiments attempt to control the sentiment and verb tense of generated movie review sentences (Hu et al., 2017), the content preservation and style transfer of news headlines and product review sentences (Fu et al., 2018), multiple automatically extracted style attributes along with sentiment and sentence theme for movie reviews (Ficler and Goldberg, 2017), sentiment, fluency and semantic equivalence (Shen et al., 2017), utterance length and topic (Fan et al., 2017), and the personality of customer care utterances in dialogue (Herzig et al., 2017). However, to our knowledge, no previous work evaluates simultaneous achievement of multiple targets as we do. Recent work introduces a large parallel corpus that varies on the formality dimension, and introduces several novel evaluation metrics, including a custom trained model for measuring semantic fidelity (Rao and Tetreault). Browns Cambridge is damn moderately priced, also it’s in city centre. It is a pub. It is an italian place. It is near Adriatic. It is damn family friendly. Table 10: Multiple-Personality Generation Output based on D ISAGREEABLE ple of each personality on its own. The"
W18-5019,W16-3622,0,0.115032,"Missing"
W18-5019,P16-2008,0,0.265921,"Missing"
W18-5019,W13-2104,0,0.0689719,"f the prior dialogue into response generation. Sordoni et al. (2015) propose a basic approach where they incorporate previous utterances as a bag of words model and use a feed-forward neural network to inject a fixed sized context vector into the LSTM cell of the encoder. Ghosh et al. (2016) proposed a modified LSTM cell with an additional gate that incorporates the previous context as input during encoding. Our context representation encodes stylistic parameters. Related Work and Conclusion The restaurant domain has long been a testbed for conversational agents with much earlier work on NLG (Howcroft et al., 2013; Stent et al., 2004; Devillers et al., 2004; Gaˇsic et al., 2008; Mairesse et al., 2010; Higashinaka et al., 2007), so it is not surprising that recent work using neural generation methods has also focused on the restaurant domain (Wen et al., 2015; Mei et al., 2015; Dusek and Jurc´ıcek, 2016b; Lampouras and Vlachos, 2016; Juraska et al., 2018). The restaurant domain is ideal for testing generation models because sentences can range from extremely simple to more complex forms that exhibit discourse relations such as justification or contrast (Stent et al., 2004). Most recent work focuses on a"
W18-5019,E14-1074,0,0.0520686,"Missing"
W18-5019,devillers-etal-2004-french,0,0.0121817,"on. Sordoni et al. (2015) propose a basic approach where they incorporate previous utterances as a bag of words model and use a feed-forward neural network to inject a fixed sized context vector into the LSTM cell of the encoder. Ghosh et al. (2016) proposed a modified LSTM cell with an additional gate that incorporates the previous context as input during encoding. Our context representation encodes stylistic parameters. Related Work and Conclusion The restaurant domain has long been a testbed for conversational agents with much earlier work on NLG (Howcroft et al., 2013; Stent et al., 2004; Devillers et al., 2004; Gaˇsic et al., 2008; Mairesse et al., 2010; Higashinaka et al., 2007), so it is not surprising that recent work using neural generation methods has also focused on the restaurant domain (Wen et al., 2015; Mei et al., 2015; Dusek and Jurc´ıcek, 2016b; Lampouras and Vlachos, 2016; Juraska et al., 2018). The restaurant domain is ideal for testing generation models because sentences can range from extremely simple to more complex forms that exhibit discourse relations such as justification or contrast (Stent et al., 2004). Most recent work focuses on achieving semantic fidelity for simpler synta"
W18-5019,Q17-1024,0,0.0917333,"Missing"
W18-5019,N09-1072,0,0.0859157,"Missing"
W18-5019,N18-1014,1,0.843937,"that incorporates the previous context as input during encoding. Our context representation encodes stylistic parameters. Related Work and Conclusion The restaurant domain has long been a testbed for conversational agents with much earlier work on NLG (Howcroft et al., 2013; Stent et al., 2004; Devillers et al., 2004; Gaˇsic et al., 2008; Mairesse et al., 2010; Higashinaka et al., 2007), so it is not surprising that recent work using neural generation methods has also focused on the restaurant domain (Wen et al., 2015; Mei et al., 2015; Dusek and Jurc´ıcek, 2016b; Lampouras and Vlachos, 2016; Juraska et al., 2018). The restaurant domain is ideal for testing generation models because sentences can range from extremely simple to more complex forms that exhibit discourse relations such as justification or contrast (Stent et al., 2004). Most recent work focuses on achieving semantic fidelity for simpler syntactic structures, although there has also been a focus on crowdsourcing or harvesting training data that exhibits more stylistic variation (Novikova et al., 2017; Nayak et al., 2017; Oraby et al., 2017). Most previous work on neural stylistic generation has been carried out in the framework of “style tr"
W18-5019,D17-1238,0,0.0919803,"Missing"
W18-5019,C16-1105,0,0.296547,"Missing"
W18-5019,W17-5525,0,0.331317,"Missing"
W18-5019,W02-2103,0,0.157459,"e 3 summarizes the architectures. time step of the encoder of the seq2seq architecture (Sutskever et al., 2014). The attention (Bahdanau et al., 2014) is computed over all of the encoder states and the hidden state of the fully connected network. Again, we set the learning rate, alpha decay, and maximum training epochs (up to 20) based on loss monitoring on the validation set. 4 Quantitative Results Here, we present results on controlling stylistic variation while maintaining semantic fidelity. 4.1 Evaluating Semantic Quality It is widely agreed that new evaluation metrics are needed for NLG (Langkilde-Geary, 2002; Belz and Reiter, 2006; Bangalore et al., 2000; Novikova et al., 2017a). We first present automated metrics used in NLG to measure how well model outputs compare to P ERSONAGE input, then introduce novel metrics designed to fill the gap left by current evaluation metrics. Automatic Metrics. The automatic evaluation uses the E2E generation challenge script.8 Table 4 summarizes the results for BLEU (n-gram precision), NIST (weighted n-gram precision), METEOR (n-grams with synonym recall), and ROUGE (n-gram recall). Although the differences in metrics are small, M ODEL CONTEXT shows a slight imp"
W18-5019,W16-6644,0,0.182843,"Missing"
W18-5019,A97-1039,0,0.515134,"Missing"
W18-5019,P16-1094,0,0.0696115,"Missing"
W18-5019,W17-4904,1,0.864807,"omain (Wen et al., 2015; Mei et al., 2015; Dusek and Jurc´ıcek, 2016b; Lampouras and Vlachos, 2016; Juraska et al., 2018). The restaurant domain is ideal for testing generation models because sentences can range from extremely simple to more complex forms that exhibit discourse relations such as justification or contrast (Stent et al., 2004). Most recent work focuses on achieving semantic fidelity for simpler syntactic structures, although there has also been a focus on crowdsourcing or harvesting training data that exhibits more stylistic variation (Novikova et al., 2017; Nayak et al., 2017; Oraby et al., 2017). Most previous work on neural stylistic generation has been carried out in the framework of “style transfer”: this work is hampered by the This paper evaluates the ability of different neural architectures to faithfully render the semantic content of an utterance while simultaneously exhibiting stylistic variations characteristic of Big Five personalities. We created a novel parallel training corpus of over 88,000 meaning representations in the restaurant domain, and matched reference outputs by using an existing statistical natural language generator, P ERSONAGE (Mairesse and Walker, 2010)."
W18-5019,J11-3002,1,0.918371,"Missing"
W18-5019,H92-1005,0,0.404662,"Missing"
W18-5019,P10-1157,0,0.140446,"Missing"
W18-5019,N18-1012,0,0.0968649,"Missing"
W18-5019,P08-1020,1,0.688329,"Missing"
W18-5019,N16-1086,0,0.127541,"Missing"
W18-5019,N15-1020,0,0.0989047,"Missing"
W18-5019,D15-1199,0,0.152039,"Missing"
W18-5019,whittaker-etal-2002-fish,1,0.508528,"Missing"
W18-5019,P04-1011,1,\N,Missing
W18-5019,W18-2706,0,\N,Missing
W18-6535,N01-1002,0,0.134014,"Missing"
W18-6535,C00-1007,0,0.374076,"he framework of statistical NLG , where each module was assumed to require training data that matched its representational requirements. Methods focused on training individual modules for content selection and linearization (Marcu, 1997; Lapata, 2003; Barzilay and Lapata, 2005), and trainable sentence planning for discourse structure and aggregation operations (Stent and Molina, 2009; Walker et al., 2007; Paiva and Evans, 2004; Sauper and Barzilay, 2009; H. Cheng and Mellish, 2001). Previous work also explored statistical and hybrid methods for surface realization (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2002). and text-to-speech realizations (Hitzeman et al., 1998; Bulyko and Ostendorf, 2001; Hirschberg, 1993). Other work on NNLG also uses token supervision and modifications of the architecture in order to control stylistic aspects of the output in the context of text-to-text or paraphrase generation. Some types of stylistic variation correspond to sentence planning operations, e.g. to express a particular personality type (Oraby et al., 2018b; Mairesse and Walker, 2011; Oraby et al., 2018a), or to control sentiment and sentence theme (Ficler and Goldberg, 2017). Herzig et"
W18-6535,W17-3541,0,0.0799254,"mbow, 2000; Oh and Rudnicky, 2002). and text-to-speech realizations (Hitzeman et al., 1998; Bulyko and Ostendorf, 2001; Hirschberg, 1993). Other work on NNLG also uses token supervision and modifications of the architecture in order to control stylistic aspects of the output in the context of text-to-text or paraphrase generation. Some types of stylistic variation correspond to sentence planning operations, e.g. to express a particular personality type (Oraby et al., 2018b; Mairesse and Walker, 2011; Oraby et al., 2018a), or to control sentiment and sentence theme (Ficler and Goldberg, 2017). Herzig et al. (2017) automatically label the personality of customer care agents and then control the personality during generation. Rao and Tetreault (2018) train a model to paraphrase from formal to informal style and Niu and Bansal (2018) use a high precision classifier and a blended language model to control utterance politness. Previous work on contrast has explored how the user model determines which values should be contrasted, since people may have differing opinions about whether an attribute value is positive or negative (e.g. family friendly) (Carenini and Moore, 1993; Walker et al., 2002a; White et al"
W18-6535,H05-1042,0,0.0306813,"control neural sentence planning. The results show that the models benefit from extra latent variable supervision, which improves the semantic accuracy of the NNLG, provides the capability to control variation in the output, and enables generalizing to unseen value combinations. Much of the previous work focused on sentence planning was done in the framework of statistical NLG , where each module was assumed to require training data that matched its representational requirements. Methods focused on training individual modules for content selection and linearization (Marcu, 1997; Lapata, 2003; Barzilay and Lapata, 2005), and trainable sentence planning for discourse structure and aggregation operations (Stent and Molina, 2009; Walker et al., 2007; Paiva and Evans, 2004; Sauper and Barzilay, 2009; H. Cheng and Mellish, 2001). Previous work also explored statistical and hybrid methods for surface realization (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2002). and text-to-speech realizations (Hitzeman et al., 1998; Bulyko and Ostendorf, 2001; Hirschberg, 1993). Other work on NNLG also uses token supervision and modifications of the architecture in order to control stylistic aspects"
W18-6535,N06-1046,0,0.0161014,"stematically varied sentence planning corpus.5 Evaluation metrics. It is well known that evaluation metrics used for translation such as BLEU are not well suited to evaluating generation outputs (Belz and Reiter, 2006; Liu et al., 2016; Novikova et al., 2017): they penalize stylistic variation, and don’t account for the fact that different dialogue responses can be equally good, and can vary due to contextual factors (Jordan, 2000; Krahmer et al., 2002). We also note that previous work on sentence planning has always assumed that sentence planning operations improve the quality of the output (Barzilay and Lapata, 2006; Shaw, 1998), while our primary focus here is to determine whether an NNLG can be trained to perform such operations while maintaining semantic fidelity. Moreover, due to the large size of our controlled training sets, we observe few problems with output quality and fluency. Thus we leave an evaluation of fluency and naturalness to future work, and focus here on evaluating the multiple targets of semantic accuracy and sentence planning accuracy. Because the MR is clearly defined, we define scripts (information extraction patterns) to measure the occurrence of the MR attributes and their value"
W18-6535,E06-1040,0,0.098298,"ns of them. The E2E dataset consists of pairs of reference utterances and their meaning representations (MRs), where each utterance contains up to 8 unique attributes, and each MR has multiple references. We populate PERSONAGE with the syntax/meaning mappings that it needs to produce output for the E2E meaning representations, and then automatically produce a very large (204,955 utterance/MR pairs) systematically varied sentence planning corpus.5 Evaluation metrics. It is well known that evaluation metrics used for translation such as BLEU are not well suited to evaluating generation outputs (Belz and Reiter, 2006; Liu et al., 2016; Novikova et al., 2017): they penalize stylistic variation, and don’t account for the fact that different dialogue responses can be equally good, and can vary due to contextual factors (Jordan, 2000; Krahmer et al., 2002). We also note that previous work on sentence planning has always assumed that sentence planning operations improve the quality of the output (Barzilay and Lapata, 2006; Shaw, 1998), while our primary focus here is to determine whether an NNLG can be trained to perform such operations while maintaining semantic fidelity. Moreover, due to the large size of ou"
W18-6535,C16-1105,0,0.134971,"Missing"
W18-6535,W16-6626,0,0.0624638,"how the user model determines which values should be contrasted, since people may have differing opinions about whether an attribute value is positive or negative (e.g. family friendly) (Carenini and Moore, 1993; Walker et al., 2002a; White et al., 2010). To our knowledge, no-one has yet trained an NNLG to use a model of user preferences for content selection. Here, values are treated as inherently good or bad, e.g. service is ranked from great to terrible. 7 In future work we plan to test these methods in different domains, e.g. the WebNLG challenge or WikiBio dataset (Wiseman et al., 2018; Colin et al., 2016). We also plan to experiment with more complex sentence planning operations and test whether an NNLG system can be endowed with fine-tuned control, e.g. controlling multiple aggregation operations. Another possibility is that hierarchical input representations representing the sentence plan might improve performance or allow finer-grained control (Moore et al., 2004; Su and Chen, 2018; Bangalore and Rambow, 2000). It may be desirable to control which attributes are aggregated together, distributed or contrasted, and to allow more than two values to be contrasted. Here, our main goal was to tes"
W18-6535,P98-1116,0,0.254807,"tence planning was done in the framework of statistical NLG , where each module was assumed to require training data that matched its representational requirements. Methods focused on training individual modules for content selection and linearization (Marcu, 1997; Lapata, 2003; Barzilay and Lapata, 2005), and trainable sentence planning for discourse structure and aggregation operations (Stent and Molina, 2009; Walker et al., 2007; Paiva and Evans, 2004; Sauper and Barzilay, 2009; H. Cheng and Mellish, 2001). Previous work also explored statistical and hybrid methods for surface realization (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2002). and text-to-speech realizations (Hitzeman et al., 1998; Bulyko and Ostendorf, 2001; Hirschberg, 1993). Other work on NNLG also uses token supervision and modifications of the architecture in order to control stylistic aspects of the output in the context of text-to-text or paraphrase generation. Some types of stylistic variation correspond to sentence planning operations, e.g. to express a particular personality type (Oraby et al., 2018b; Mairesse and Walker, 2011; Oraby et al., 2018a), or to control sentiment and sentence theme (Ficler and"
W18-6535,W16-3622,0,0.0421437,"Missing"
W18-6535,P03-1069,0,0.116868,"s required to control neural sentence planning. The results show that the models benefit from extra latent variable supervision, which improves the semantic accuracy of the NNLG, provides the capability to control variation in the output, and enables generalizing to unseen value combinations. Much of the previous work focused on sentence planning was done in the framework of statistical NLG , where each module was assumed to require training data that matched its representational requirements. Methods focused on training individual modules for content selection and linearization (Marcu, 1997; Lapata, 2003; Barzilay and Lapata, 2005), and trainable sentence planning for discourse structure and aggregation operations (Stent and Molina, 2009; Walker et al., 2007; Paiva and Evans, 2004; Sauper and Barzilay, 2009; H. Cheng and Mellish, 2001). Previous work also explored statistical and hybrid methods for surface realization (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2002). and text-to-speech realizations (Hitzeman et al., 1998; Bulyko and Ostendorf, 2001; Hirschberg, 1993). Other work on NNLG also uses token supervision and modifications of the architecture in order t"
W18-6535,P16-2008,0,0.133023,"Missing"
W18-6535,D16-1230,0,0.0378721,"aset consists of pairs of reference utterances and their meaning representations (MRs), where each utterance contains up to 8 unique attributes, and each MR has multiple references. We populate PERSONAGE with the syntax/meaning mappings that it needs to produce output for the E2E meaning representations, and then automatically produce a very large (204,955 utterance/MR pairs) systematically varied sentence planning corpus.5 Evaluation metrics. It is well known that evaluation metrics used for translation such as BLEU are not well suited to evaluating generation outputs (Belz and Reiter, 2006; Liu et al., 2016; Novikova et al., 2017): they penalize stylistic variation, and don’t account for the fact that different dialogue responses can be equally good, and can vary due to contextual factors (Jordan, 2000; Krahmer et al., 2002). We also note that previous work on sentence planning has always assumed that sentence planning operations improve the quality of the output (Barzilay and Lapata, 2006; Shaw, 1998), while our primary focus here is to determine whether an NNLG can be trained to perform such operations while maintaining semantic fidelity. Moreover, due to the large size of our controlled train"
W18-6535,W17-4912,0,0.0230337,"ight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2002). and text-to-speech realizations (Hitzeman et al., 1998; Bulyko and Ostendorf, 2001; Hirschberg, 1993). Other work on NNLG also uses token supervision and modifications of the architecture in order to control stylistic aspects of the output in the context of text-to-text or paraphrase generation. Some types of stylistic variation correspond to sentence planning operations, e.g. to express a particular personality type (Oraby et al., 2018b; Mairesse and Walker, 2011; Oraby et al., 2018a), or to control sentiment and sentence theme (Ficler and Goldberg, 2017). Herzig et al. (2017) automatically label the personality of customer care agents and then control the personality during generation. Rao and Tetreault (2018) train a model to paraphrase from formal to informal style and Niu and Bansal (2018) use a high precision classifier and a blended language model to control utterance politness. Previous work on contrast has explored how the user model determines which values should be contrasted, since people may have differing opinions about whether an attribute value is positive or negative (e.g. family friendly) (Carenini and Moore, 1993; Walker et a"
W18-6535,J11-3002,1,0.935471,"7). Here, we systematically perform a set of controlled experiments to test whether an NNLG can learn to do sentence planning operations. Section 2 describes our experimental setup and the NNLG architecture that allows us, during training, to vary the amount of supervision provided as to Table 3: Justify & Contrast Discourse Relations which sentence planning operations appear in the outputs. To ensure that the training data contains enough examples of particular phenomena, we experiment with supplementing crowdsourced data with automatically generated stylistically-varied data from PERSONAGE (Mairesse and Walker, 2011). To achieve sufficient control for some experiments, we exclusively use Personage training data where we can specify exactly which sentence planning operations will be used and in what frequency. It is not possible to do this with crowdsourced data. While our expectation was that an NNLG can reproduce any sentence planning operation that appears frequently enough in the training data, the results in Sections 3, 4 and 5 show that explicit supervision improves the semantic accuracy of the NNLG, provides the capability to control variation in the output, and enables generalizing to unseen value"
W18-6535,P01-1056,1,0.53533,"friendly, also it’s a pub, and it is an English place near Avalon. 2 3 Sents Moderately priced Zizzi isn’t kid friendly, it’s in riverside and it is near Avalon. It is a pub. It is an English place. 3 5 Sents Zizzi is moderately priced near Avalon. It is a pub. It’s in riverside. It isn’t family friendly. It is an English place. Table 1: Sentence Scoping: a sentence planning operation that decides what content to place in each sentence of an utterance. In contrast, earlier models of statistical natural language generation (SNLG) for dialogue were based around the NLG architecture in Figure 1 (Rambow et al., 2001; Stent, 2002; Stent and Molina, 2009). Figure 1: Statistical NLG Dialogue Architecture Introduction Neural natural language generation (NNLG) promises to simplify the process of producing high Here the dialogue manager sends one or more dialogue acts and their arguments to the NLG en284 Proceedings of The 11th International Natural Language Generation Conference, pages 284–295, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics # Type Example NAME [T HE M ILL ], EAT T YPE [ COFFEE SHOP ], FOOD [I TALIAN ], PRICE R ANGE [ LOW ], CUSTOMER R ATING [ HI"
W18-6535,N16-1086,0,0.0450606,". Discourse structuring is often critical in persuasive settings (Scott and de Souza, 1990; Moore and Paris, 1993), in order to express discourse relations that hold between content items. Table 3 shows how RECOMMEND dialogue acts can be included in the MR, and how content can be related with JUSTIFY and CONTRAST discourse relations (Stent et al., 2002). Recent work in NNLG explicitly claims that training models end-to-end allows them to do both sentence planning and surface realization without the need for intermediate representations (Dusek and Jurc´ıcek, 2016b; Lampouras and Vlachos, 2016; Mei et al., 2016; Wen et al., 2015; Nayak et al., 2017). To date, however, no-one has actually shown that an NNLG can faithfully produce outputs that exhibit the sentence planning and discourse operations in Tables 1, 2 and 3. Instead, NNLG evaluations focus on measuring the semantic correctness of the outputs and their fluency (Novikova et al., 2017; Nayak et al., 2017). Here, we systematically perform a set of controlled experiments to test whether an NNLG can learn to do sentence planning operations. Section 2 describes our experimental setup and the NNLG architecture that allows us, during training, to va"
W18-6535,N18-1012,0,0.0258694,"993). Other work on NNLG also uses token supervision and modifications of the architecture in order to control stylistic aspects of the output in the context of text-to-text or paraphrase generation. Some types of stylistic variation correspond to sentence planning operations, e.g. to express a particular personality type (Oraby et al., 2018b; Mairesse and Walker, 2011; Oraby et al., 2018a), or to control sentiment and sentence theme (Ficler and Goldberg, 2017). Herzig et al. (2017) automatically label the personality of customer care agents and then control the personality during generation. Rao and Tetreault (2018) train a model to paraphrase from formal to informal style and Niu and Bansal (2018) use a high precision classifier and a blended language model to control utterance politness. Previous work on contrast has explored how the user model determines which values should be contrasted, since people may have differing opinions about whether an attribute value is positive or negative (e.g. family friendly) (Carenini and Moore, 1993; Walker et al., 2002a; White et al., 2010). To our knowledge, no-one has yet trained an NNLG to use a model of user preferences for content selection. Here, values are tre"
W18-6535,J93-4004,0,0.808652,"food with excellent VICE ] quality and it is inexpensive. However the service is poor. It is near the Sorrento in the West Village. Sentence scoping (Table 1) affects the complexity of the sentences that compose an utterance, allowing the NLG to produce simpler sentences when desired that might be easier for particular users to understand. Aggregation reduces redundancy, composing multiple content items into single sentences. Table 2 shows common aggregation operations (Cahill et al., 2001; Shaw, 1998). Discourse structuring is often critical in persuasive settings (Scott and de Souza, 1990; Moore and Paris, 1993), in order to express discourse relations that hold between content items. Table 3 shows how RECOMMEND dialogue acts can be included in the MR, and how content can be related with JUSTIFY and CONTRAST discourse relations (Stent et al., 2002). Recent work in NNLG explicitly claims that training models end-to-end allows them to do both sentence planning and surface realization without the need for intermediate representations (Dusek and Jurc´ıcek, 2016b; Lampouras and Vlachos, 2016; Mei et al., 2016; Wen et al., 2015; Nayak et al., 2017). To date, however, no-one has actually shown that an NNLG"
W18-6535,P09-1024,0,0.0199244,"pability to control variation in the output, and enables generalizing to unseen value combinations. Much of the previous work focused on sentence planning was done in the framework of statistical NLG , where each module was assumed to require training data that matched its representational requirements. Methods focused on training individual modules for content selection and linearization (Marcu, 1997; Lapata, 2003; Barzilay and Lapata, 2005), and trainable sentence planning for discourse structure and aggregation operations (Stent and Molina, 2009; Walker et al., 2007; Paiva and Evans, 2004; Sauper and Barzilay, 2009; H. Cheng and Mellish, 2001). Previous work also explored statistical and hybrid methods for surface realization (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2002). and text-to-speech realizations (Hitzeman et al., 1998; Bulyko and Ostendorf, 2001; Hirschberg, 1993). Other work on NNLG also uses token supervision and modifications of the architecture in order to control stylistic aspects of the output in the context of text-to-text or paraphrase generation. Some types of stylistic variation correspond to sentence planning operations, e.g. to express a particular p"
W18-6535,W08-0111,0,0.0388522,"or 0 (no contrast) to test if that would achieve better control of contrast. Table 9: Overview of the training sets for contrast experiments on HIGH, where most of the mistakes on HIGH include repeating the attribute high rating and rating, including examples where it does not distribute at all, e.g. high rating and high rating. We plan to explore alternative semantic encodings in future work. 5 Discourse Contrast Experiment Persuasive settings such as recommending restaurants, hotels or travel options often have a critical discourse structure (Scott and de Souza, 1990; Moore and Paris, 1993; Nakatsu, 2008). For example a recommendation may consist of an explicitly evaluative utterance e.g. Chanpen Thai is the best option, along with content related by the justify discourse relation, e.g. It has great food and service, as in Table 3. Our experiments focus on DISCOURSE CONTRAST. We developed a script to find contrastive sentences in the 40K E2E training set by searching for any instance of a contrast cue word, such as but, although, and even if. This identified 3,540 instances. While this data size is comparable to the 3-4K instances used in prior work (Wen et al., 2015; Nayak et al., 2017), we a"
W18-6535,W98-1415,0,0.474779,"orrento in the West Village. 8 CONTRAST I would suggest Babbo because it [ PRICE , SER - serves Italian food with excellent VICE ] quality and it is inexpensive. However the service is poor. It is near the Sorrento in the West Village. Sentence scoping (Table 1) affects the complexity of the sentences that compose an utterance, allowing the NLG to produce simpler sentences when desired that might be easier for particular users to understand. Aggregation reduces redundancy, composing multiple content items into single sentences. Table 2 shows common aggregation operations (Cahill et al., 2001; Shaw, 1998). Discourse structuring is often critical in persuasive settings (Scott and de Souza, 1990; Moore and Paris, 1993), in order to express discourse relations that hold between content items. Table 3 shows how RECOMMEND dialogue acts can be included in the MR, and how content can be related with JUSTIFY and CONTRAST discourse relations (Stent et al., 2002). Recent work in NNLG explicitly claims that training models end-to-end allows them to do both sentence planning and surface realization without the need for intermediate representations (Dusek and Jurc´ıcek, 2016b; Lampouras and Vlachos, 2016;"
W18-6535,Q18-1027,0,0.0212499,"ure in order to control stylistic aspects of the output in the context of text-to-text or paraphrase generation. Some types of stylistic variation correspond to sentence planning operations, e.g. to express a particular personality type (Oraby et al., 2018b; Mairesse and Walker, 2011; Oraby et al., 2018a), or to control sentiment and sentence theme (Ficler and Goldberg, 2017). Herzig et al. (2017) automatically label the personality of customer care agents and then control the personality during generation. Rao and Tetreault (2018) train a model to paraphrase from formal to informal style and Niu and Bansal (2018) use a high precision classifier and a blended language model to control utterance politness. Previous work on contrast has explored how the user model determines which values should be contrasted, since people may have differing opinions about whether an attribute value is positive or negative (e.g. family friendly) (Carenini and Moore, 1993; Walker et al., 2002a; White et al., 2010). To our knowledge, no-one has yet trained an NNLG to use a model of user preferences for content selection. Here, values are treated as inherently good or bad, e.g. service is ranked from great to terrible. 7 In"
W18-6535,C04-1129,0,0.158945,"Missing"
W18-6535,D17-1238,0,0.1234,"ons (Stent et al., 2002). Recent work in NNLG explicitly claims that training models end-to-end allows them to do both sentence planning and surface realization without the need for intermediate representations (Dusek and Jurc´ıcek, 2016b; Lampouras and Vlachos, 2016; Mei et al., 2016; Wen et al., 2015; Nayak et al., 2017). To date, however, no-one has actually shown that an NNLG can faithfully produce outputs that exhibit the sentence planning and discourse operations in Tables 1, 2 and 3. Instead, NNLG evaluations focus on measuring the semantic correctness of the outputs and their fluency (Novikova et al., 2017; Nayak et al., 2017). Here, we systematically perform a set of controlled experiments to test whether an NNLG can learn to do sentence planning operations. Section 2 describes our experimental setup and the NNLG architecture that allows us, during training, to vary the amount of supervision provided as to Table 3: Justify & Contrast Discourse Relations which sentence planning operations appear in the outputs. To ensure that the training data contains enough examples of particular phenomena, we experiment with supplementing crowdsourced data with automatically generated stylistically-varied da"
W18-6535,W09-3941,0,0.10575,"is an English place near Avalon. 2 3 Sents Moderately priced Zizzi isn’t kid friendly, it’s in riverside and it is near Avalon. It is a pub. It is an English place. 3 5 Sents Zizzi is moderately priced near Avalon. It is a pub. It’s in riverside. It isn’t family friendly. It is an English place. Table 1: Sentence Scoping: a sentence planning operation that decides what content to place in each sentence of an utterance. In contrast, earlier models of statistical natural language generation (SNLG) for dialogue were based around the NLG architecture in Figure 1 (Rambow et al., 2001; Stent, 2002; Stent and Molina, 2009). Figure 1: Statistical NLG Dialogue Architecture Introduction Neural natural language generation (NNLG) promises to simplify the process of producing high Here the dialogue manager sends one or more dialogue acts and their arguments to the NLG en284 Proceedings of The 11th International Natural Language Generation Conference, pages 284–295, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics # Type Example NAME [T HE M ILL ], EAT T YPE [ COFFEE SHOP ], FOOD [I TALIAN ], PRICE R ANGE [ LOW ], CUSTOMER R ATING [ HIGH ], NEAR [T HE S ORRENTO ] 4 With, A"
W18-6535,W18-5019,1,0.925955,"he correct type of sentence planning operation; (3) models with semantic supervision, tested only on distributive aggregation, where the input vector is supplemented with specific instructions of which attribute value to distribute over, e.g. low, average or high, in the DIS TRIBUTE token. We describe the specific model variations for each experiment below. Data Sets. One challenge is that NNLG models are highly sensitive to the distribution of phenomena in training data, and our previous work has shown that the outputs of NNLG models exhibit less stylistic variation than their training data (Oraby et al., 2018b). Moreover, even large corpora, such as the 50K E2E Generation Challenge corpus, may not contain particular stylistic variations. For example, out of 50K crowdsourced examples in the E2E corpus, there are 1,956 examples of contrast with the operator “but”. There is only 1 instance of distributive aggregation because attribute values are rarely lexicalized identically in E2E. To ensure that the training data contains enough examples of particular phenomena, our experiments combine crowdsourced E2E data3 with automatically generated data from PERSON AGE (Mairesse and Walker, 2011).4 This allow"
W18-6535,W02-2110,0,0.390038,"gregation operation over attributes price range and rating. • Semantic Supervision: we add a supervision token, DISTRIBUTE, containing a string that is either none if there is no aggregation over price range and rating in the corresponding reference text, or a value of LOW, AVERAGE , or HIGH for aggregation. single sentences or phrases. We focus here on distributive aggregation as defined in Figure 2 and illustrated in Row 6 of Table 2. In an SNLG setting, the generator achieves this type of aggregation by operating on syntactic trees (Shaw, 1998; Scott and de Souza, 1990; Stent et al., 2004; Walker et al., 2002b). In an NNLG setting, we hope the model will induce the syntactic structure and the mathematical operation underlying it, automatically, without explicit training supervision. To prepare the training data, we limit the values for PRICE and RATING attributes to LOW, AVERAGE , and HIGH . We reserve the combination {PRICE = HIGH , RATING = HIGH} for test, leaving two combinations of values where distribution is possible ({PRICE = LOW, RATING = LOW} and {PRICE = AVERAGE , RATING = AVERAGE}). We then use all three values in MRs where the price and rating are not the same {PRICE = LOW, RATING = HI"
W18-6535,D15-1199,0,0.185151,"Missing"
W18-6535,J10-2001,0,0.479063,"Missing"
W18-6535,D18-1356,0,0.069887,"Missing"
W18-6535,C98-1112,0,\N,Missing
W18-6535,P04-1011,1,\N,Missing
W18-6535,P01-1015,0,\N,Missing
W18-6536,W11-1401,0,0.0500768,"Missing"
W18-6536,N18-1014,1,0.829627,"(document, question, answer) triples with questions and answers either being collected from the web or created by crowd workers. Automatic Question Generation methods can be used to cheaply supplement resources available to QA models, further assisting in advancing QA capabilities. Indeed, Sachan and Xing (2018) have recently shown that a joint QA-QG model is able to achieve state-of-the art results on a variety of different QA related tasks. Sequence-to-sequence Neural Network models have been shown to be effective at a variety of other NLP problems (Bahdanau et al., 2014; Rush et al., 2015; Juraska et al., 2018), and recent work has also applied them to QG (Du et al., 2017; Zhou et al., 2017; Yuan et al., 2017). As in other recent work on QG, we use an attentional Recurrent Neural Network encoder–decoder model that is similar to the model of Bahdanau et al. (2014). In this approach, the QG task is cast as a sequence-tosequence language modeling task. The input senNatural 4.10 4.09 4.36 Table 9: Human Evaluation Results 4 Sentences and Examples she publicly endorsed same sex marriage on march 26, 2013, after the supreme court debate on california ’s proposition 8. what did beyonce publicly support? wh"
W18-6536,C16-1107,0,0.0376987,"Missing"
W18-6536,P17-4012,0,0.0374659,"allows the answer-specific sentence embedding to influence decoding decisions during each time step of the decoding process. We experiment with pre-training the sentence encoder. The pre-training process is carried out in two steps. First, to facilitate training of the sentence encoder by itself, we need some ground truth sentence representation from which to measure similarity. Since this sentence encoder is used 299 # Model S3 Q10 Copy Q11 No Copy S4 Q12 Copy Q13 No Copy S5 Q14 Copy Q15 No Copy S6 Q16 Copy Q17 No Copy mented using PyTorch1 and OpenNMT-py2 which is a PyTorch port of OpenNMT(Klein et al., 2017). The encoder, decoder, and sentence encoder are multi-layer RNNs, each with two layers. We use bi-directional LSTM cells with 640 units. The model is trained using Dropout (Srivastava et al., 2014) of 0.3 between RNN layers. Word embeddings are initialized using Glove 300 dimensional word vectors (Pennington et al., 2014) that are not updated during training. The sentence encoder is initialized using the pre-training process described in Section 2.2. All other model parameters are initialized using Glorot initialization (Glorot and Bengio, 2010). The model parameters are optimized using Stoch"
W18-6536,W05-0909,0,0.0863532,"a word case feature, and a special answer signaling feature. The answer signaling feature allows our model to generate multiple questions for each sentence, illustrated with Q5, Q6 and Q7 in Table 1. We also introduce a coreference resolution model and supplement the sentence input representation with resolved coreferences, as well as a copying mechanism. Section 3 presents an evaluation of the final model on the benchmark SQuAD testset using automatic evaluation metrics and shows that it achieves state of the art results of 19.98 BLEU 4, 22.26 METEOR, and 48.23 ROUGE (Papineni et al., 2002; Banerjee and Lavie, 2005; Lin, 2004). To our knowledge this model outperforms all previously published results by a significant margin. A human evaluation also shows that the introduced features and answer-specific sentence embedding improve the quality of the generated questions. We delay a more detailed review of previous work to Section 4 and conclude in Section 5. 2 Model Our QG model follows a standard RNN Encoder– Decoder model (Sutskever et al., 2014) that maps a source sequence (declarative sentence) to a target sequence (question). The architecture of the baseline model is as follows: the encoder is a multi-"
W18-6536,P15-1086,0,0.101968,"Examples she publicly endorsed same sex marriage on march 26, 2013, after the supreme court debate on california ’s proposition 8. what did beyonce publicly support? what did madonna publicly endorsed on march 26, 2013? west is one of the best-selling artists of all time, having sold more than 32 million albums and 100 million digital downloads worldwide how many grammy awards did he win? how many grammy awards has madonna won? Related Work Much of the work on automatic question generation has been motivated by helping teachers in test creation (Mitkov and Ha, 2003; Heilman and Smith, 2010a; Labutov et al., 2015; Araki et al., 303 of hand crafted rules that operate on declarative sentences, transforming them into questions. In order to control quality, the output questions are filtered through a logistic regression model that ranks the questions on acceptability. tence, represented as a series of words, is mapped to an output series of words representing a question. Sequence-to-sequence models have several advantages over previous rule-based approaches to QG. First, they eliminate the need for large handcrafted rule sets – the model automatically learns how to perform the subtasks of Question Represe"
W18-6536,J15-1001,0,0.020581,"interpret and allow developers greater control over model behavior. Furthermore, they typically require less data to develop than a complex Neural Network might need to achieve a similar level of performance. However, rule-based systems have some weaknesses as well. They tend to be laborious to develop, or domain specific. For example the system developed by Mostow and Chen (2009) relies on the presence of one of a set of 239 modal verbs in a sentence, and Olney et al. (2012) use 3000 keywords provided by the glossary of a Biology text book and a test-prep study guide. The system described in Chali and Hasan (2015) uses roughly 350 hand-crafted rules. Furthermore, these systems rely heavily on syntactic parsers, and may struggle to recover from parser inaccuracies. Question Generation is the task of automatically creating questions from textual input. In this work we present a new Attentional Encoder–Decoder Recurrent Neural Network model for automatic question generation. Our model incorporates linguistic features and an additional sentence embedding to capture meaning at both sentence and word levels. The linguistic features are designed to capture information related to named entity recognition, word"
W18-6536,W17-5038,0,0.0298054,"Missing"
W18-6536,D15-1166,0,0.0740696,"he introduced features and answer-specific sentence embedding improve the quality of the generated questions. We delay a more detailed review of previous work to Section 4 and conclude in Section 5. 2 Model Our QG model follows a standard RNN Encoder– Decoder model (Sutskever et al., 2014) that maps a source sequence (declarative sentence) to a target sequence (question). The architecture of the baseline model is as follows: the encoder is a multi-layer bidirectional LSTM (Hochreiter and Schmidhuber, 1997) and the decoder is a unidirectional LSTM that uses global attention with input-feeding (Luong et al., 2015). This baseline model yields one question per sentence (Q4 in Table 1). Figure 1: Diagram of our answer focus model. We create our model by enhancing the baseline model in the following three ways: • We add 4 different token level supervision features to the input. See Section 2.1. • We add a sentence encoder that creates a question specific sentence embedding. • We use a copy mechanism (See et al., 2017) to copy words from the sentence directly into the question. 2.1 Feature Supervision A feature-rich encoding is constructed by concatenating several token level features onto the token’s word-"
W18-6536,P17-1123,0,0.337276,"son has the most spoken dialogue in the game? who provided the basis for midna’s voice? what country does akiko komoto come from? Baseline what is her ? Our Model: FocusCR what character has the most voice acting in english? what is the name of the japanese voice actress? what is the nationality of akiko komoto? Table 1: Sentence and associated questions generated from the baseline and our best model. Among many different approaches to question generation, our work is most similar to recent work applying neural network models to the task of generating short answer factoid questions for SQUAD (Du et al., 2017; Yuan et al., 2017; Sachan and Xing, 2018). However these previous models have several limitations. As illustrated in Table 1, the SQUAD corpus (Rajpurkar et al., 2016) provides multiple gold standard references for each sentence (Q1, Q2, and Q3), but previous work to date can only generate one question for each sentence as represented by the baseline model (Q4), whereas our model can generate multiple questions as shown in Table 1. In Section 2, we present our novel model that introduces additional token supervision representing features of the text as well as an additional lower dimensional"
W18-6536,P14-5010,0,0.00597276,"Missing"
W18-6536,P05-1045,0,0.0159582,"Missing"
W18-6536,W03-0203,0,0.0966319,"ble 9: Human Evaluation Results 4 Sentences and Examples she publicly endorsed same sex marriage on march 26, 2013, after the supreme court debate on california ’s proposition 8. what did beyonce publicly support? what did madonna publicly endorsed on march 26, 2013? west is one of the best-selling artists of all time, having sold more than 32 million albums and 100 million digital downloads worldwide how many grammy awards did he win? how many grammy awards has madonna won? Related Work Much of the work on automatic question generation has been motivated by helping teachers in test creation (Mitkov and Ha, 2003; Heilman and Smith, 2010a; Labutov et al., 2015; Araki et al., 303 of hand crafted rules that operate on declarative sentences, transforming them into questions. In order to control quality, the output questions are filtered through a logistic regression model that ranks the questions on acceptability. tence, represented as a series of words, is mapped to an output series of words representing a question. Sequence-to-sequence models have several advantages over previous rule-based approaches to QG. First, they eliminate the need for large handcrafted rule sets – the model automatically learns"
W18-6536,N10-1086,0,0.47181,"the Hylian Shield. What was released on november 19, 2006? What was released on september 17, 2006? At the age of 10, West moved with his mother to Nanjing, China, where she was teaching at Nanjing University as part of an exchange program. at what age did west move with his mother to nanjing? at what age did von neumann teach at nanjing university? 3.1 Table 4: Question Generation with and without the Copy mechanism. Automatic Evaluation We compare our system’s results to that of several other QG systems. The rows of Table 5 with labels H&S, Yuan, Du, and S&X refer to the models presented in Heilman and Smith (2010a); Yuan et al. (2017); Du et al. (2017), and Sachan and Xing (2018), respectfully. Please refer to Section 4 Related Work for further details on each of these systems. The results of the H&S system are reported in this work for the sake of comparison. The actual experiments were performed by Du et al. (2017) who describe the specific configuration of H&S in greater detail. Results. We use BLEU score (Papineni et al., 2002) as an automatic evaluation metric and compare directly to other work. BLEU measures the similarity between a generated text called a candidate and a set of human written te"
W18-6536,K16-1028,0,0.0660613,"Missing"
W18-6536,W10-0705,0,0.388214,"the Hylian Shield. What was released on november 19, 2006? What was released on september 17, 2006? At the age of 10, West moved with his mother to Nanjing, China, where she was teaching at Nanjing University as part of an exchange program. at what age did west move with his mother to nanjing? at what age did von neumann teach at nanjing university? 3.1 Table 4: Question Generation with and without the Copy mechanism. Automatic Evaluation We compare our system’s results to that of several other QG systems. The rows of Table 5 with labels H&S, Yuan, Du, and S&X refer to the models presented in Heilman and Smith (2010a); Yuan et al. (2017); Du et al. (2017), and Sachan and Xing (2018), respectfully. Please refer to Section 4 Related Work for further details on each of these systems. The results of the H&S system are reported in this work for the sake of comparison. The actual experiments were performed by Du et al. (2017) who describe the specific configuration of H&S in greater detail. Results. We use BLEU score (Papineni et al., 2002) as an automatic evaluation metric and compare directly to other work. BLEU measures the similarity between a generated text called a candidate and a set of human written te"
W18-6536,P02-1040,0,0.104925,"ognition (NER) feature, a word case feature, and a special answer signaling feature. The answer signaling feature allows our model to generate multiple questions for each sentence, illustrated with Q5, Q6 and Q7 in Table 1. We also introduce a coreference resolution model and supplement the sentence input representation with resolved coreferences, as well as a copying mechanism. Section 3 presents an evaluation of the final model on the benchmark SQuAD testset using automatic evaluation metrics and shows that it achieves state of the art results of 19.98 BLEU 4, 22.26 METEOR, and 48.23 ROUGE (Papineni et al., 2002; Banerjee and Lavie, 2005; Lin, 2004). To our knowledge this model outperforms all previously published results by a significant margin. A human evaluation also shows that the introduced features and answer-specific sentence embedding improve the quality of the generated questions. We delay a more detailed review of previous work to Section 4 and conclude in Section 5. 2 Model Our QG model follows a standard RNN Encoder– Decoder model (Sutskever et al., 2014) that maps a source sequence (declarative sentence) to a target sequence (question). The architecture of the baseline model is as follow"
W18-6536,D14-1162,0,0.0825323,"truth sentence representation from which to measure similarity. Since this sentence encoder is used 299 # Model S3 Q10 Copy Q11 No Copy S4 Q12 Copy Q13 No Copy S5 Q14 Copy Q15 No Copy S6 Q16 Copy Q17 No Copy mented using PyTorch1 and OpenNMT-py2 which is a PyTorch port of OpenNMT(Klein et al., 2017). The encoder, decoder, and sentence encoder are multi-layer RNNs, each with two layers. We use bi-directional LSTM cells with 640 units. The model is trained using Dropout (Srivastava et al., 2014) of 0.3 between RNN layers. Word embeddings are initialized using Glove 300 dimensional word vectors (Pennington et al., 2014) that are not updated during training. The sentence encoder is initialized using the pre-training process described in Section 2.2. All other model parameters are initialized using Glorot initialization (Glorot and Bengio, 2010). The model parameters are optimized using Stochastic Gradient Descent with mini-batches of size 64. Beam search with five beams is used during inference and OOV words are replaced using the token of highest attention weight in the source sentence. We tune our model with the development dataset and select the model of lowest Perplexity to evaluate on the test dataset. S"
W18-6536,D16-1264,0,0.61201,"l: FocusCR what character has the most voice acting in english? what is the name of the japanese voice actress? what is the nationality of akiko komoto? Table 1: Sentence and associated questions generated from the baseline and our best model. Among many different approaches to question generation, our work is most similar to recent work applying neural network models to the task of generating short answer factoid questions for SQUAD (Du et al., 2017; Yuan et al., 2017; Sachan and Xing, 2018). However these previous models have several limitations. As illustrated in Table 1, the SQUAD corpus (Rajpurkar et al., 2016) provides multiple gold standard references for each sentence (Q1, Q2, and Q3), but previous work to date can only generate one question for each sentence as represented by the baseline model (Q4), whereas our model can generate multiple questions as shown in Table 1. In Section 2, we present our novel model that introduces additional token supervision representing features of the text as well as an additional lower dimensional word embedding. The features include a Named Entity Recognition (NER) feature, a word case feature, and a special answer signaling feature. The answer signaling feature"
W18-6536,W10-4234,0,0.0471823,"ty recognition, word case, and entity coreference resolution. In addition our model uses a copying mechanism and a special answer signal that enables generation of numerous diverse questions on a given sentence. Our model achieves state of the art results of 19.98 Bleu 4 on a benchmark Question Generation dataset, outperforming all previously published results by a significant margin. A human evaluation also shows that the added features improve the quality of the generated questions. 1 Introduction Question Generation (QG) is the task of automatically generating questions from textual input (Rus et al., 2010). There are a wide variety of question types and forms, e.g., short answer, open ended, multiple choice, and gap questions, each require a different approach to generate. One distinguishing aspect of a QG system is the type of questions that it produces. This paper focuses on the generation of factoid short answer questions, i.e., questions that can be answered by a single short phrase, usually appearing directly in the input text. The work of a QG system typically consists of three conceptual subtasks: Target Selection, Ques296 Proceedings of The 11th International Natural Language Generation"
W18-6536,D15-1044,0,0.0442975,"of human authored (document, question, answer) triples with questions and answers either being collected from the web or created by crowd workers. Automatic Question Generation methods can be used to cheaply supplement resources available to QA models, further assisting in advancing QA capabilities. Indeed, Sachan and Xing (2018) have recently shown that a joint QA-QG model is able to achieve state-of-the art results on a variety of different QA related tasks. Sequence-to-sequence Neural Network models have been shown to be effective at a variety of other NLP problems (Bahdanau et al., 2014; Rush et al., 2015; Juraska et al., 2018), and recent work has also applied them to QG (Du et al., 2017; Zhou et al., 2017; Yuan et al., 2017). As in other recent work on QG, we use an attentional Recurrent Neural Network encoder–decoder model that is similar to the model of Bahdanau et al. (2014). In this approach, the QG task is cast as a sequence-tosequence language modeling task. The input senNatural 4.10 4.09 4.36 Table 9: Human Evaluation Results 4 Sentences and Examples she publicly endorsed same sex marriage on march 26, 2013, after the supreme court debate on california ’s proposition 8. what did beyon"
W18-6536,N18-1058,0,0.245589,"the game? who provided the basis for midna’s voice? what country does akiko komoto come from? Baseline what is her ? Our Model: FocusCR what character has the most voice acting in english? what is the name of the japanese voice actress? what is the nationality of akiko komoto? Table 1: Sentence and associated questions generated from the baseline and our best model. Among many different approaches to question generation, our work is most similar to recent work applying neural network models to the task of generating short answer factoid questions for SQUAD (Du et al., 2017; Yuan et al., 2017; Sachan and Xing, 2018). However these previous models have several limitations. As illustrated in Table 1, the SQUAD corpus (Rajpurkar et al., 2016) provides multiple gold standard references for each sentence (Q1, Q2, and Q3), but previous work to date can only generate one question for each sentence as represented by the baseline model (Q4), whereas our model can generate multiple questions as shown in Table 1. In Section 2, we present our novel model that introduces additional token supervision representing features of the text as well as an additional lower dimensional word embedding. The features include a Nam"
W18-6536,P17-1099,0,0.0504301,"del is as follows: the encoder is a multi-layer bidirectional LSTM (Hochreiter and Schmidhuber, 1997) and the decoder is a unidirectional LSTM that uses global attention with input-feeding (Luong et al., 2015). This baseline model yields one question per sentence (Q4 in Table 1). Figure 1: Diagram of our answer focus model. We create our model by enhancing the baseline model in the following three ways: • We add 4 different token level supervision features to the input. See Section 2.1. • We add a sentence encoder that creates a question specific sentence embedding. • We use a copy mechanism (See et al., 2017) to copy words from the sentence directly into the question. 2.1 Feature Supervision A feature-rich encoding is constructed by concatenating several token level features onto the token’s word-based embedding using a method similar to that described by Nallapati et al. (2016) for abstractive text summarization. 297 ate questions Q5, Q6, and Q7 the model was fed the same sentence three separate times, each time with only one of a1, a2 or a3 activated. Case Feature. The case feature is a simple binary feature that represents whether or not the input token contains an upper case letter. NER Featur"
W18-6536,W17-2603,0,0.0409005,"Missing"
W18-6554,W17-4912,0,0.0285916,"large corpus as a simple way of producing stylistic variation. cent efforts to collect training data for NLG with emphasis on stylistic variation (Nayak et al., 2017; Novikova et al., 2017a; Oraby et al., 2017). While there is previous work on stylistic variation in NLG (Paiva and Evans, 2004; Mairesse and Walker, 2007), this work did not use crowdsourced utterances for training. More recent work in neural NLG that explores stylistic control has not needed to control semantic correctness, or examined the interaction between semantic correctness and stylistic variation (Sennrich et al., 2016; Ficler and Goldberg, 2017). Also related is the work of Niu and Carpuat (2017) that analyzes how dense word embeddings capture style variations, Kabbara and Cheung (2016) who explore the ability of neural NLG systems to transfer style without the need for parallel corpora, which are difficult to collect (Rao and Tetreault, 2018), while Li et al. (2018) use a simple delete-and-retrieve method also without alignment to outperform adversarial methods in style transfer. Finally, Oraby et al. (2018) propose two different methods that give neural generators control over the language style, corresponding to the Big Five perso"
W18-6554,W13-2104,0,0.118984,"s almost 10 times more data than the San Francisco restaurant dataset (Wen et al., 2015), which had frequently been used for NLG benchmarks. This significant increase in size allows successful training of neural models on smaller subsets of the dataset. Careful selection of the training subset can be used to influence the style of the utterances produced by the model, as we show in this paper. A portion of the human reference utterances The restaurant domain has always been the domain of choice for NLG tasks in dialogue systems (Stent et al., 2004; Gaˇsi´c et al., 2008; Mairesse et al., 2010; Howcroft et al., 2013), as it offers a good combination of structured information availability, expression complexity, and ease of incorporation into conversation. Hence, even the more recent neural models for NLG continue to be tested primarily on data in this domain (Wen et al., 2015; Duˇsek and Jurˇc´ıcˇ ek, 2016; Nayak et al., 2017). These tend to focus solely on syntactic and semantic correctness of the generated utterances, nevertheless, there have also been re442 Samples Unique MRs Training Validation Test 42,061 4,672 630 4,862 547 630 Total 47,363 6,039 Domain Utterance Table 2: Number of samples vs. uniqu"
W18-6554,N18-1014,1,0.918846,"sting relation can be drowned among the thousands of other samples in the E2E dataset, meaning that it is difficult for the learned model to produce them. Hence, we augment the input given to the model with the information about which slots should be put into a contrastive relation. We hypothesize that this explicit indication will help the model to learn to apply contrasting significantly more easily despite the small proportion of training samples exhibiting the property. In order to extract the information as exactly as possible from the training utterance, we use a heuristic slot aligner (Juraska et al., 2018) to identify two slots that are in a contrastive relation. For the relation we only consider the two scalar slots (price range and customer rating), plus the boolean slot family friendly. Whenever a contrastive relation appears to the aligner to involve a slot other than the above three, we discard it as an undesirable utterance formulation. Depending on the values of the two identified slots, we assign the sample either of the following labels: For an illustration, let us assume there are eight different reference utterances for an MR. All of them will be scored based on the discourse markers"
W18-6554,W16-6010,0,0.0154477,"n (Nayak et al., 2017; Novikova et al., 2017a; Oraby et al., 2017). While there is previous work on stylistic variation in NLG (Paiva and Evans, 2004; Mairesse and Walker, 2007), this work did not use crowdsourced utterances for training. More recent work in neural NLG that explores stylistic control has not needed to control semantic correctness, or examined the interaction between semantic correctness and stylistic variation (Sennrich et al., 2016; Ficler and Goldberg, 2017). Also related is the work of Niu and Carpuat (2017) that analyzes how dense word embeddings capture style variations, Kabbara and Cheung (2016) who explore the ability of neural NLG systems to transfer style without the need for parallel corpora, which are difficult to collect (Rao and Tetreault, 2018), while Li et al. (2018) use a simple delete-and-retrieve method also without alignment to outperform adversarial methods in style transfer. Finally, Oraby et al. (2018) propose two different methods that give neural generators control over the language style, corresponding to the Big Five personalities, while maintaining semantic fidelity of the generated utterances. To our knowledge, there is no previous work exploring the use of and"
W18-6554,N18-1169,0,0.0676069,"Missing"
W18-6554,P10-1157,0,0.246005,"Missing"
W18-6554,D14-1179,0,0.0107699,"was not the case for a model trained on the full training set. Therefore, our next step was to verify whether our model is capable of learning all the concepts of the discourse phenomena individually and apply them in generated utterances. To that end, we repeatedly trained the model on subsets of the E2E dataset, each containing only samples with a specific group of discourse markers, as listed in the second column of Table 6.2 We then evaluated the outputs on the correspondingly reduced Evaluation Experimental Setup For our sequence-to-sequence NLG model we use the standard encoder-decoder (Cho et al., 2014) architecture equipped with an attention mechanism as defined in Bahdanau et al. (2015). The samples are delexicalized before being fed into the model as input, so as to enhance the ability of the model to generalize the learned concepts to unseen MRs. We only delexicalize categorical slots whose values always propagate verbatim from the MR to the utterance. The corresponding values in the input MR get thus replaced with placeholder tokens for which the values from the original MR are eventually substituted in the output utterance as a part of post-processing. We use a 4-layer bidirectional LS"
W18-6554,P07-1063,1,0.827706,"e propose a method of labeling the style variants during training, and show that we can modify the style of the output using our stylistic labels. We contrast these methods, showing how they vary in terms of semantic quality and stylistic control. These methods promise to be usable with any sufficiently large corpus as a simple way of producing stylistic variation. cent efforts to collect training data for NLG with emphasis on stylistic variation (Nayak et al., 2017; Novikova et al., 2017a; Oraby et al., 2017). While there is previous work on stylistic variation in NLG (Paiva and Evans, 2004; Mairesse and Walker, 2007), this work did not use crowdsourced utterances for training. More recent work in neural NLG that explores stylistic control has not needed to control semantic correctness, or examined the interaction between semantic correctness and stylistic variation (Sennrich et al., 2016; Ficler and Goldberg, 2017). Also related is the work of Niu and Carpuat (2017) that analyzes how dense word embeddings capture style variations, Kabbara and Cheung (2016) who explore the ability of neural NLG systems to transfer style without the need for parallel corpora, which are difficult to collect (Rao and Tetreaul"
W18-6554,P16-2008,0,0.0767548,"Missing"
W18-6554,W17-4903,0,0.030628,"iation. cent efforts to collect training data for NLG with emphasis on stylistic variation (Nayak et al., 2017; Novikova et al., 2017a; Oraby et al., 2017). While there is previous work on stylistic variation in NLG (Paiva and Evans, 2004; Mairesse and Walker, 2007), this work did not use crowdsourced utterances for training. More recent work in neural NLG that explores stylistic control has not needed to control semantic correctness, or examined the interaction between semantic correctness and stylistic variation (Sennrich et al., 2016; Ficler and Goldberg, 2017). Also related is the work of Niu and Carpuat (2017) that analyzes how dense word embeddings capture style variations, Kabbara and Cheung (2016) who explore the ability of neural NLG systems to transfer style without the need for parallel corpora, which are difficult to collect (Rao and Tetreault, 2018), while Li et al. (2018) use a simple delete-and-retrieve method also without alignment to outperform adversarial methods in style transfer. Finally, Oraby et al. (2018) propose two different methods that give neural generators control over the language style, corresponding to the Big Five personalities, while maintaining semantic fidelity of the"
W18-6554,D15-1199,0,0.0846054,"Missing"
W18-6554,W17-5525,0,0.201466,"Missing"
W18-6554,W16-6644,0,0.0582673,"nsumption. (Wen et al., 2016) Laptop For the price of 449 dollars, you could purchase the Satellite Hypnos 38 laptop. (Wen et al., 2016) People Born in the London Borough of Havering, Alex Day started performing in 2006. (Gardent et al., 2017) Food Sago is the main ingredient in binignit, but sweet potatoes are also used in it. (Gardent et al., 2017) Table 4: Examples of utterances in different datasets/domains, also exhibiting interesting discourse phenomena. was collected using pictures as the source of information, which was shown to inspire more natural utterances compared to textual MRs (Novikova et al., 2016). The reference utterances in the E2E dataset exhibit superior lexical richness and syntactic variation, including more complex discourse phenomena. It aims to provide higherquality training data for end-to-end NLG systems to learn to produce better phrased and more naturally sounding utterances. 4 Stylistic Selection We note that the E2E dataset is significantly larger than what is needed for a neural model to learn to produce correct utterances in this domain. Thus, we seek a way to help the model learn more than just to be correct. We strive to achieve higher stylistic diversity of the utte"
W18-6554,W17-4904,1,0.861264,"ic partitions and quantify the effect of smaller, but more stylistically controlled training data; (2) we propose a method of labeling the style variants during training, and show that we can modify the style of the output using our stylistic labels. We contrast these methods, showing how they vary in terms of semantic quality and stylistic control. These methods promise to be usable with any sufficiently large corpus as a simple way of producing stylistic variation. cent efforts to collect training data for NLG with emphasis on stylistic variation (Nayak et al., 2017; Novikova et al., 2017a; Oraby et al., 2017). While there is previous work on stylistic variation in NLG (Paiva and Evans, 2004; Mairesse and Walker, 2007), this work did not use crowdsourced utterances for training. More recent work in neural NLG that explores stylistic control has not needed to control semantic correctness, or examined the interaction between semantic correctness and stylistic variation (Sennrich et al., 2016; Ficler and Goldberg, 2017). Also related is the work of Niu and Carpuat (2017) that analyzes how dense word embeddings capture style variations, Kabbara and Cheung (2016) who explore the ability of neural NLG sy"
W18-6554,W18-5019,1,0.802703,"c correctness, or examined the interaction between semantic correctness and stylistic variation (Sennrich et al., 2016; Ficler and Goldberg, 2017). Also related is the work of Niu and Carpuat (2017) that analyzes how dense word embeddings capture style variations, Kabbara and Cheung (2016) who explore the ability of neural NLG systems to transfer style without the need for parallel corpora, which are difficult to collect (Rao and Tetreault, 2018), while Li et al. (2018) use a simple delete-and-retrieve method also without alignment to outperform adversarial methods in style transfer. Finally, Oraby et al. (2018) propose two different methods that give neural generators control over the language style, corresponding to the Big Five personalities, while maintaining semantic fidelity of the generated utterances. To our knowledge, there is no previous work exploring the use of and utility of stylistic selection for controlling stylistic variation in NLG from structured MRs. This may be either because there have not been sufficiently large corpora in a particular domain, or because it is surprising, as we show, that relatively small corpora (2000 samples) whose style is controlled can be used to train a n"
W18-6554,N18-1012,0,0.0271017,"nd Walker, 2007), this work did not use crowdsourced utterances for training. More recent work in neural NLG that explores stylistic control has not needed to control semantic correctness, or examined the interaction between semantic correctness and stylistic variation (Sennrich et al., 2016; Ficler and Goldberg, 2017). Also related is the work of Niu and Carpuat (2017) that analyzes how dense word embeddings capture style variations, Kabbara and Cheung (2016) who explore the ability of neural NLG systems to transfer style without the need for parallel corpora, which are difficult to collect (Rao and Tetreault, 2018), while Li et al. (2018) use a simple delete-and-retrieve method also without alignment to outperform adversarial methods in style transfer. Finally, Oraby et al. (2018) propose two different methods that give neural generators control over the language style, corresponding to the Big Five personalities, while maintaining semantic fidelity of the generated utterances. To our knowledge, there is no previous work exploring the use of and utility of stylistic selection for controlling stylistic variation in NLG from structured MRs. This may be either because there have not been sufficiently large"
W18-6554,N16-1005,0,0.0559368,"e with any sufficiently large corpus as a simple way of producing stylistic variation. cent efforts to collect training data for NLG with emphasis on stylistic variation (Nayak et al., 2017; Novikova et al., 2017a; Oraby et al., 2017). While there is previous work on stylistic variation in NLG (Paiva and Evans, 2004; Mairesse and Walker, 2007), this work did not use crowdsourced utterances for training. More recent work in neural NLG that explores stylistic control has not needed to control semantic correctness, or examined the interaction between semantic correctness and stylistic variation (Sennrich et al., 2016; Ficler and Goldberg, 2017). Also related is the work of Niu and Carpuat (2017) that analyzes how dense word embeddings capture style variations, Kabbara and Cheung (2016) who explore the ability of neural NLG systems to transfer style without the need for parallel corpora, which are difficult to collect (Rao and Tetreault, 2018), while Li et al. (2018) use a simple delete-and-retrieve method also without alignment to outperform adversarial methods in style transfer. Finally, Oraby et al. (2018) propose two different methods that give neural generators control over the language style, corresp"
W18-6554,P04-1011,1,0.608189,"k-oriented language generation in the restaurant domain. It offers almost 10 times more data than the San Francisco restaurant dataset (Wen et al., 2015), which had frequently been used for NLG benchmarks. This significant increase in size allows successful training of neural models on smaller subsets of the dataset. Careful selection of the training subset can be used to influence the style of the utterances produced by the model, as we show in this paper. A portion of the human reference utterances The restaurant domain has always been the domain of choice for NLG tasks in dialogue systems (Stent et al., 2004; Gaˇsi´c et al., 2008; Mairesse et al., 2010; Howcroft et al., 2013), as it offers a good combination of structured information availability, expression complexity, and ease of incorporation into conversation. Hence, even the more recent neural models for NLG continue to be tested primarily on data in this domain (Wen et al., 2015; Duˇsek and Jurˇc´ıcˇ ek, 2016; Nayak et al., 2017). These tend to focus solely on syntactic and semantic correctness of the generated utterances, nevertheless, there have also been re442 Samples Unique MRs Training Validation Test 42,061 4,672 630 4,862 547 630 Tot"
W18-6554,N16-1015,0,0.147386,"Missing"
W19-8101,J11-3002,1,\N,Missing
W19-8101,P14-5010,0,\N,Missing
W19-8101,P06-1140,0,\N,Missing
W19-8101,W13-2104,0,\N,Missing
W19-8101,P16-1094,0,\N,Missing
W19-8101,N16-1005,0,\N,Missing
W19-8101,P17-4012,0,\N,Missing
W19-8101,W17-3541,0,\N,Missing
W19-8101,W18-6535,1,\N,Missing
W19-8623,P04-1011,1,0.61249,"ese large datasets, and instead they learn to produce the same generic type of sentences as with considerably less training data (Deriu and Cieliebak, 2018; Juraska and Walker, 2018; Duˇsek et al., 2019). 1 The ViGGO corpus is available for download at: https://nlds.soe.ucsc.edu/viggo 164 Proceedings of The 12th International Conference on Natural Language Generation, pages 164–172, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics responding values to talk about. While they certainly can be a topic of a casual conversation, the existing restaurant datasets (Stent et al., 2004; Gaˇsi´c et al., 2008; Mairesse et al., 2010; Howcroft et al., 2013; Wen et al., 2015a; Nayak et al., 2017) are geared more toward a task-oriented dialogue where a system tries to narrow down a restaurant based on the user’s preferences and ultimately give a recommendation. Our new video game dataset is designed to be more conversational, and to thus enable neural models to produce utterances more suitable for an open-domain dialogue system. Even the most recent addition to the publicly available restaurant datasets for data-to-text NLG, the E2E dataset (Novikova et al., 2017), suffers from t"
W19-8623,W18-6554,1,0.844203,"cally trained on one of the few parallel corpora publicly available, in particular the E2E (Novikova et al., 2017) and the WebNLG (Gardent et al., 2017) datasets. Crowdsourcing large NLG datasets tends to be a costly and time-consuming process, making it impractical outside of task-oriented dialogue systems. At the same time, current neural NLG models struggle to replicate the high language diversity of the training sentences present in these large datasets, and instead they learn to produce the same generic type of sentences as with considerably less training data (Deriu and Cieliebak, 2018; Juraska and Walker, 2018; Duˇsek et al., 2019). 1 The ViGGO corpus is available for download at: https://nlds.soe.ucsc.edu/viggo 164 Proceedings of The 12th International Conference on Natural Language Generation, pages 164–172, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics responding values to talk about. While they certainly can be a topic of a casual conversation, the existing restaurant datasets (Stent et al., 2004; Gaˇsi´c et al., 2008; Mairesse et al., 2010; Howcroft et al., 2013; Wen et al., 2015a; Nayak et al., 2017) are geared more toward a task-oriented dialogue where"
W19-8623,N16-1015,0,0.0317523,"Missing"
W19-8623,W15-4639,0,\N,Missing
W19-8623,D15-1199,0,\N,Missing
W19-8623,P02-1040,0,\N,Missing
W19-8623,W07-0734,0,\N,Missing
W19-8623,W04-1013,0,\N,Missing
W19-8623,P10-1157,0,\N,Missing
W19-8623,W13-2104,0,\N,Missing
W19-8623,D18-1255,0,\N,Missing
W19-8623,W18-6503,0,\N,Missing
W19-8623,W08-0119,0,\N,Missing
W93-0238,J86-3001,0,0.0244216,"Missing"
W93-0238,C92-1054,1,0.639717,"Missing"
W93-0238,P90-1010,1,0.858777,"Missing"
W94-0320,J86-3001,0,0.0493933,"ance in memory, communication cost, and retrieval cost. The value for a given triple indicates whether or not information stored at this distance is accessible. T h e values in the table are determined using the simulation environment. Presumably, this process weakly corresponds to the acquisition of proper text planning strategies by h u m a n agents. While an obvious candidate for the model of H&apos;s attentional state is the AWM model itself, certain aspects of this model do not exactly m a t c h some widely believed and intuitively motivated observations aboUt hierarchical discourse structure [10, 18]. However, hierarchical structure interacts with attentional state in ways t h a t have not been fully explored in the literature to date. In particular, if a discourse segment consisting of a nucleus with a hierarchically complex satellite that is extremely long, then a further satellite to the same nucleus m a y well require repetition of the nucleus [35]. Neither R S T nor the model of [10] accounts for such effects. We conclude that it is not a priori obvious t h a t hierarchical structure contradicts our model. We will investigate this issue further. T h r o u g h o u t this paper, we hav"
W94-0320,P88-1020,0,0.0382266,"on of discourses in a situation in which we can vary the cognitive parameters of the hearer. Our results show that modeling cognitive state can lead to more effective discourses (measured with respect to a simple task). 1 Introduction Text planning is the task for a speaker (S) of deciding what information to :communicate to a hearer (H) and how and when to communicate it. Over the last few years a consensus has emerged that the text planning task should be formulated in terms of communicative goals or intentions [19, 25, 23, 16]. Consider, for example, the RST-based planners developed at ISI [13, 21, 14]. These planners use the discourse relations proposed by Rhetorical S&apos;tructure Theory (RST) [18] as plan operators, by interpreting the requirements on the related segments as preconditions, and the resultant effect of the discourse relation as a postcondition in a traditional AI planning architecture. Two types (at least) of discourse relations have been identified in the literature. A SUBJECT-MATTER relation [18] or SEMANTIC relation [14] simply reflects a relation that exists independently in the world, such as causation. Each subject-matter relation can be seen as a rhetorical strategy for"
W94-0320,P89-1025,0,0.0260836,"on of discourses in a situation in which we can vary the cognitive parameters of the hearer. Our results show that modeling cognitive state can lead to more effective discourses (measured with respect to a simple task). 1 Introduction Text planning is the task for a speaker (S) of deciding what information to :communicate to a hearer (H) and how and when to communicate it. Over the last few years a consensus has emerged that the text planning task should be formulated in terms of communicative goals or intentions [19, 25, 23, 16]. Consider, for example, the RST-based planners developed at ISI [13, 21, 14]. These planners use the discourse relations proposed by Rhetorical S&apos;tructure Theory (RST) [18] as plan operators, by interpreting the requirements on the related segments as preconditions, and the resultant effect of the discourse relation as a postcondition in a traditional AI planning architecture. Two types (at least) of discourse relations have been identified in the literature. A SUBJECT-MATTER relation [18] or SEMANTIC relation [14] simply reflects a relation that exists independently in the world, such as causation. Each subject-matter relation can be seen as a rhetorical strategy for"
W94-0320,J93-4004,0,0.19184,"rse relations have been identified in the literature. A SUBJECT-MATTER relation [18] or SEMANTIC relation [14] simply reflects a relation that exists independently in the world, such as causation. Each subject-matter relation can be seen as a rhetorical strategy for the linguistic realization of a *Walker was partially funded by ARO grant DAAL0389-C0031PRI and DARPA grant N00014-90-J-1863 at the University of Pennsylvania and by Hewlett Packard, U.K. t Rambow was supported by NATO on a NATO/NSF postdoctoral fellowship in France. rainbow©linguist, j ussieu, fr range of communicative intentions [22]. A PRESENTATIONAL RELATION[18] or INTERPERSONALrelation [14] holds between two discourse segments such that the juxtaposition increases H&apos;s STRENGTH of belief, desire, or intention. Each presentational relation maps directly to a communicative intention. Examples of presentational relations include the MOTIVATION relation, which increases H&apos;s desire to perform an action, hopefully persuading H to form an intention to do the action. Both subject-matter and presentational relations relate two clauses: (1) the NUCLEUS which realizes the main point; and (2) the SATELLITE which is auxiliary option"
W94-0320,J88-3006,0,0.0872994,") an analysis of naturally occurring dialogue, and (2) by simulating the generation of discourses in a situation in which we can vary the cognitive parameters of the hearer. Our results show that modeling cognitive state can lead to more effective discourses (measured with respect to a simple task). 1 Introduction Text planning is the task for a speaker (S) of deciding what information to :communicate to a hearer (H) and how and when to communicate it. Over the last few years a consensus has emerged that the text planning task should be formulated in terms of communicative goals or intentions [19, 25, 23, 16]. Consider, for example, the RST-based planners developed at ISI [13, 21, 14]. These planners use the discourse relations proposed by Rhetorical S&apos;tructure Theory (RST) [18] as plan operators, by interpreting the requirements on the related segments as preconditions, and the resultant effect of the discourse relation as a postcondition in a traditional AI planning architecture. Two types (at least) of discourse relations have been identified in the literature. A SUBJECT-MATTER relation [18] or SEMANTIC relation [14] simply reflects a relation that exists independently in the world, such as cau"
W94-0320,C92-1054,1,0.793135,"er option. For example, in (4-4) Kim rejects the proposal in (4-3), for pursuing option-45, and proposes option-56 instead. The form of the rejection as a counter-proposal is based on observations about how rejection is communicated in naturally-occurring dialogue as codified in the GOLLABOR.ATIVE PLANNING PRINCIPLES [37]. Proposals i and 2 are inferred to be implicitly ACCEPTED because they are not rejected [37]. If a proposal is ACCEPTED, either implicitly or explicitly, then the option that was the content of the proposal becomes a mutual intention that contributes to the final design plan [27, 34, 30]. The model of AWM discussed above plays a critical role in determining agents&apos; performance. Remember that only salient beliefs can be used in means-end reasoning and deliberation, so that if the warrant for a proposal is not salient, the agent cannot properly evaluate a proposal. 4.2 Varying Discourse Strategies Agents are parametrized for different discourse strategies by placing different expansions of discourse plans in their plan libraries. In Design-World the only discourse plans required are plans for PROPOSAL, REJECTION, ACCEPTANCE, CLAR.IFICATION,OPENING and CLOSING. The only variatio"
W94-0320,P90-1010,1,0.844786,"the proposal she will attempt to retrieve the score proposition stored earlier in memory. Thus the propositions about the scores of furniture items are VCARJ1.ANTS for supporting deliberation. Agents REJECT a proposal if deliberation leads them to believe that they know of a better option. For example, in (4-4) Kim rejects the proposal in (4-3), for pursuing option-45, and proposes option-56 instead. The form of the rejection as a counter-proposal is based on observations about how rejection is communicated in naturally-occurring dialogue as codified in the GOLLABOR.ATIVE PLANNING PRINCIPLES [37]. Proposals i and 2 are inferred to be implicitly ACCEPTED because they are not rejected [37]. If a proposal is ACCEPTED, either implicitly or explicitly, then the option that was the content of the proposal becomes a mutual intention that contributes to the final design plan [27, 34, 30]. The model of AWM discussed above plays a critical role in determining agents&apos; performance. Remember that only salient beliefs can be used in means-end reasoning and deliberation, so that if the warrant for a proposal is not salient, the agent cannot properly evaluate a proposal. 4.2 Varying Discourse Strateg"
W94-0320,J92-4007,0,0.0313658,") an analysis of naturally occurring dialogue, and (2) by simulating the generation of discourses in a situation in which we can vary the cognitive parameters of the hearer. Our results show that modeling cognitive state can lead to more effective discourses (measured with respect to a simple task). 1 Introduction Text planning is the task for a speaker (S) of deciding what information to :communicate to a hearer (H) and how and when to communicate it. Over the last few years a consensus has emerged that the text planning task should be formulated in terms of communicative goals or intentions [19, 25, 23, 16]. Consider, for example, the RST-based planners developed at ISI [13, 21, 14]. These planners use the discourse relations proposed by Rhetorical S&apos;tructure Theory (RST) [18] as plan operators, by interpreting the requirements on the related segments as preconditions, and the resultant effect of the discourse relation as a postcondition in a traditional AI planning architecture. Two types (at least) of discourse relations have been identified in the literature. A SUBJECT-MATTER relation [18] or SEMANTIC relation [14] simply reflects a relation that exists independently in the world, such as cau"
W97-0601,J86-3001,0,0.0138314,"trategies that operate at the level of dialogue subtasks (subdialogues). Consider the longer versions of Dialogues 1 and 2 in Figures 2 and 3. Each utterance in Figures 2 and 3 has been tagged using one or more of the attribute abbreviations in Table 1, according to the subtask(s) the utterance contributes to. As a convention of this type of tagging, utterances that contribute to the success of the whole dialogue, such as greetings, are tagged with all the attributes. Thus the goal of the tagging is to show how the structure of the dialogue reflects the structure of the task (Carbelrry, 1989; Grosz and Sidner, 1986; Litman and Allen, 1990). Tagging by AVM attributes is required to calculate costs over subdialogues, since for any subdialogue, task attributes define the subdialogue. For example, the subdialogue about the attribute arrival-city (SA) consists of utterances A6 and U6, its cost Cl(SA) is 2. Tagging by AVM attributes is also required to calculate the cost of some of the qualitative measures, such as number of repair utterances. (Note that to calculate such costs, each utterance in the corpus of dialogues must also be tagged with respect to the qualitative phenomenon in question, e.g. whether t"
W97-0601,P96-1038,0,0.016709,"For example, let c2 be the number of repair utterances. The repair utterances in Figure 2 are A3 through U6, thus c2(D1) is 10 utterances and c2(SA) is 2 utterances. The repair utterance in Figure 3 is U2, but note that according to the AVM task tagging, U2 simultaneously addresses the information goals for arrival-city and depart-range. In 7This tagging can be hand generated, or system generated and hand corrected. Preliminary studies indicate that reliability for human tagging is higher for AVM attribute tagging than for other types of discourse segment tagging (Passonneau and Litman, 1997; Hirschberg and Nakatani, 1996). 8Previous work has shown that this can be done with high reliability (Hirschman and Pao, 1993). general, if an utterance U contributes to the information goals of N different attributes, each attribute accounts for 1/N of any costs derivable from U. Thus, c2(D2) is .5. Given a set of ci, it is necessary to combine the different cost measures in order to determine their relative contribution to performance. The next section explains how to combine ~ with a set of ci to yield an overall performance measure. 2.4 Estimating a Performance Function Given the definition of success and costs above a"
W97-0601,H93-1004,0,0.0250551,", 1992; Price et al., 1992; Simpson and Fraser, 1993). Another problem is that dialog evaluation is not reducible to transcript evaluation, or to comparison with a wizard's reference answers (Bates and Ayuso, 1993; Polifroni et al., 1992; Price et al., 1992), because the set of potentially acceptable dialogs can be very large. Current proposals for dialog evaluation metrics are both objective and subjective. The objective metrics that have been used to evaluate a dialog as a whole include (Abella, Brown, and Buntschuh, 1996; Ciaremella, 1993; Danieli and Gerbino, 1995; Hirschman et al., 1990; Hirschman et al., 1993; Polifroni et al., 1992; Price et al., 1992; Smith and Hipp, 1994; Smith and Gordon, 1997; Walker, 1996): • percentage of ""non-trivial"" (more than one word) utterances. • mean length of ""non-trivial"" utterances Objective metrics can be calculated without recourse to human judgement, and in many cases, can be calculated automatically by the spoken dialogue system. One possible exception is task-based success measures, such as transaction success, task completion or quality of solution metrics, which can be either an objective or a subjective measure depending on whether the users' goals are we"
W97-0601,H90-1023,0,0.032847,"spect to a set of reference answers • transaction success, task completion, or quality of solution • number of turns or utterances; • dialogue time or task completion time • mean user response time • mean system response time • frequency of diagnostic error messages I Introduction Interactive spoken dialogue systems are based on many component technologies: speech recognition, text-tospeech, natural language understanding, natural language generation, and database query languages. While evaluation metrics for these components are well understand (Sparck-Jones and Galliers, 1996; Walker, 1989; Hirschman et al., 1990), it has been difficult to develop standard metrics for complete systems that integrate all these technologies. One problem is that there are so many potential metrics that can be used to evaluate a dialog system. For example, a dialog system can be evaluated by measuring the system's ability to help users achieve their goals, the system's robustness in detecting and recovering from errors of speech recognition or of understanding, and the overall quality of the system's interactions with users (Danieli and Gerbino, 1995; Hirschman and Pao, 1993; Polifroni et al., 1992; Price et al., 1992; Sim"
W97-0601,J97-1005,1,0.819888,"he utterance is a repair. 8) For example, let c2 be the number of repair utterances. The repair utterances in Figure 2 are A3 through U6, thus c2(D1) is 10 utterances and c2(SA) is 2 utterances. The repair utterance in Figure 3 is U2, but note that according to the AVM task tagging, U2 simultaneously addresses the information goals for arrival-city and depart-range. In 7This tagging can be hand generated, or system generated and hand corrected. Preliminary studies indicate that reliability for human tagging is higher for AVM attribute tagging than for other types of discourse segment tagging (Passonneau and Litman, 1997; Hirschberg and Nakatani, 1996). 8Previous work has shown that this can be done with high reliability (Hirschman and Pao, 1993). general, if an utterance U contributes to the information goals of N different attributes, each attribute accounts for 1/N of any costs derivable from U. Thus, c2(D2) is .5. Given a set of ci, it is necessary to combine the different cost measures in order to determine their relative contribution to performance. The next section explains how to combine ~ with a set of ci to yield an overall performance measure. 2.4 Estimating a Performance Function Given the definit"
W97-0601,H92-1005,0,0.1602,"rs, 1996; Walker, 1989; Hirschman et al., 1990), it has been difficult to develop standard metrics for complete systems that integrate all these technologies. One problem is that there are so many potential metrics that can be used to evaluate a dialog system. For example, a dialog system can be evaluated by measuring the system's ability to help users achieve their goals, the system's robustness in detecting and recovering from errors of speech recognition or of understanding, and the overall quality of the system's interactions with users (Danieli and Gerbino, 1995; Hirschman and Pao, 1993; Polifroni et al., 1992; Price et al., 1992; Simpson and Fraser, 1993). Another problem is that dialog evaluation is not reducible to transcript evaluation, or to comparison with a wizard's reference answers (Bates and Ayuso, 1993; Polifroni et al., 1992; Price et al., 1992), because the set of potentially acceptable dialogs can be very large. Current proposals for dialog evaluation metrics are both objective and subjective. The objective metrics that have been used to evaluate a dialog as a whole include (Abella, Brown, and Buntschuh, 1996; Ciaremella, 1993; Danieli and Gerbino, 1995; Hirschman et al., 1990; Hirsch"
W97-0601,H92-1006,0,0.0185075,"Hirschman et al., 1990), it has been difficult to develop standard metrics for complete systems that integrate all these technologies. One problem is that there are so many potential metrics that can be used to evaluate a dialog system. For example, a dialog system can be evaluated by measuring the system's ability to help users achieve their goals, the system's robustness in detecting and recovering from errors of speech recognition or of understanding, and the overall quality of the system's interactions with users (Danieli and Gerbino, 1995; Hirschman and Pao, 1993; Polifroni et al., 1992; Price et al., 1992; Simpson and Fraser, 1993). Another problem is that dialog evaluation is not reducible to transcript evaluation, or to comparison with a wizard's reference answers (Bates and Ayuso, 1993; Polifroni et al., 1992; Price et al., 1992), because the set of potentially acceptable dialogs can be very large. Current proposals for dialog evaluation metrics are both objective and subjective. The objective metrics that have been used to evaluate a dialog as a whole include (Abella, Brown, and Buntschuh, 1996; Ciaremella, 1993; Danieli and Gerbino, 1995; Hirschman et al., 1990; Hirschman et al., 1993; Po"
W97-0601,H92-1009,0,0.0322943,"le performance evaluation function. The use of decision theory requires a specification of both the objectives of the decision problem and a set of measures (known as attributes in decision theory) for operationalizing the objectives. The PARADISE model is based on the structure of objectives (rectangles) shown in Figure 1. At the top level, this model posits that performance can be correlated with a meaningful external criterion such as usability, and thus that the overall goal of a spoken dialogue agent is to maximize an objective related to usability. User satisfaction ratings (Kamm, 1995; Shriberg, Wade, and Price, 1992; Polifroni et al., 1992) are the most widely used external indicator of the usability of a dialogue agent. The model further posits that two types of factors are potential relevant contributors to user satisfaction, namely task success and dialogue costs. PARADISE uses linear regression to quantify the relative contribution of the success and cost factors to user satisfaction. The task success measure builds on previous measures of transaction success and task completion (Danieli and Gerbino, 1995; Polifroni et al., 1992), but makes use of the Kappa coefficient (Carletta, 1996; Siegel and Cas"
W97-0601,J97-1006,0,0.013058,"uation is not reducible to transcript evaluation, or to comparison with a wizard's reference answers (Bates and Ayuso, 1993; Polifroni et al., 1992; Price et al., 1992), because the set of potentially acceptable dialogs can be very large. Current proposals for dialog evaluation metrics are both objective and subjective. The objective metrics that have been used to evaluate a dialog as a whole include (Abella, Brown, and Buntschuh, 1996; Ciaremella, 1993; Danieli and Gerbino, 1995; Hirschman et al., 1990; Hirschman et al., 1993; Polifroni et al., 1992; Price et al., 1992; Smith and Hipp, 1994; Smith and Gordon, 1997; Walker, 1996): • percentage of ""non-trivial"" (more than one word) utterances. • mean length of ""non-trivial"" utterances Objective metrics can be calculated without recourse to human judgement, and in many cases, can be calculated automatically by the spoken dialogue system. One possible exception is task-based success measures, such as transaction success, task completion or quality of solution metrics, which can be either an objective or a subjective measure depending on whether the users' goals are well-defined at the beginning of the dialogue. This is the case in controlled experiments, b"
W97-0601,P89-1031,1,0.805634,"nswers with respect to a set of reference answers • transaction success, task completion, or quality of solution • number of turns or utterances; • dialogue time or task completion time • mean user response time • mean system response time • frequency of diagnostic error messages I Introduction Interactive spoken dialogue systems are based on many component technologies: speech recognition, text-tospeech, natural language understanding, natural language generation, and database query languages. While evaluation metrics for these components are well understand (Sparck-Jones and Galliers, 1996; Walker, 1989; Hirschman et al., 1990), it has been difficult to develop standard metrics for complete systems that integrate all these technologies. One problem is that there are so many potential metrics that can be used to evaluate a dialog system. For example, a dialog system can be evaluated by measuring the system's ability to help users achieve their goals, the system's robustness in detecting and recovering from errors of speech recognition or of understanding, and the overall quality of the system's interactions with users (Danieli and Gerbino, 1995; Hirschman and Pao, 1993; Polifroni et al., 1992"
W97-0601,P97-1035,1,0.711917,"across systems performing different tasks (Cohen, 1995; Sparck-Jones and Galliers, 1996). It would be useful to know how users' perceptions of performance depend on the strategy used, and on tradeoffs among factors such as efficiency, speed, and accuracy. In addition to agent factors such as the differences in dialogue strategy seen in Dialogues 1 and 2, task factors such as database size and environmental factors such as background noise may also be relevant predictors of performance. In the remainder of this paper, we discuss the PARADISE framework (PARAdigm for Dialogue System Evaluation) (Walker et al., 1997), and that it addresses these limitations, as well as others. We will show that PARADISE provides a useful methodology for evaluating dialog systems that integrates and enhances previous work. 2 Integrating Previous Approaches to Evaluation in the PARADISE Framework MAXIMIZEUSERSATISFACTION MAXIMIZETASK SUCCESS MINIMIZECOSTS ~ MF~SOIIES Figure 1: PARADISE's structure of objectives for spoken dialogue performance The PARADISE framework for spoken dialogue evaluation is based on methods from decision theory (Keeney and Raiffa, 1976; Doyle, 1992), which supports combining the disparate set of per"
W97-0601,H91-1062,0,\N,Missing
walker-2004-talk,C00-1073,1,\N,Missing
walker-etal-2000-developing,walker-etal-2000-evaluation,1,\N,Missing
walker-etal-2000-developing,H92-1009,0,\N,Missing
walker-etal-2000-developing,H93-1004,0,\N,Missing
walker-etal-2000-developing,H90-1023,0,\N,Missing
walker-etal-2000-developing,P99-1040,1,\N,Missing
walker-etal-2000-developing,P98-2219,1,\N,Missing
walker-etal-2000-developing,C98-2214,1,\N,Missing
walker-etal-2000-developing,P98-2129,1,\N,Missing
walker-etal-2000-developing,C98-2124,1,\N,Missing
walker-etal-2000-evaluation,H92-1009,0,\N,Missing
walker-etal-2000-evaluation,H92-1006,1,\N,Missing
walker-etal-2000-evaluation,H93-1004,0,\N,Missing
walker-etal-2000-evaluation,walker-etal-2000-developing,1,\N,Missing
walker-etal-2000-evaluation,polifroni-seneff-2000-galaxy,0,\N,Missing
walker-etal-2012-annotated,J11-3002,1,\N,Missing
walker-etal-2012-annotated,E03-1019,0,\N,Missing
walker-etal-2012-annotated,E03-1062,0,\N,Missing
walker-etal-2012-corpus,C10-2100,0,\N,Missing
walker-etal-2012-corpus,W06-1639,0,\N,Missing
walker-etal-2012-corpus,D08-1027,0,\N,Missing
walker-etal-2012-corpus,N09-1057,0,\N,Missing
walker-etal-2012-corpus,W10-0214,0,\N,Missing
walker-etal-2012-corpus,P04-1085,0,\N,Missing
walker-etal-2012-corpus,W06-2915,0,\N,Missing
walker-etal-2012-corpus,P09-1026,0,\N,Missing
walker-etal-2012-corpus,W11-1701,1,\N,Missing
walker-etal-2012-corpus,C08-2004,0,\N,Missing
whittaker-etal-2002-fish,P00-1020,1,\N,Missing
whittaker-etal-2002-fish,P01-1066,1,\N,Missing
