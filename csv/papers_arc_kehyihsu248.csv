2021.rocling-1.3,Nested Named Entity Recognition for {C}hinese Electronic Health Records with {QA}-based Sequence Labeling,2021,-1,-1,4,0,2293,yulun chiang,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"This study presents a novel QA-based sequence labeling (QASL) approach to naturally tackle both flat and nested Named Entity Recogntion (NER) tasks on a Chinese Electronic Health Records (CEHRs) dataset. This proposed QASL approach parallelly asks a corresponding natural language question for each specific named entity type, and then identifies those associated NEs of the same specified type with the BIO tagging scheme. The associated nested NEs are then formed by overlapping the results of various types. In comparison with those pure sequence-labeling (SL) approaches, since the given question includes significant prior knowledge about the specified entity type and the capability of extracting NEs with different types, the performance for nested NER task is thus improved, obtaining 90.70{\%} of F1-score. Besides, in comparison with the pure QA-based approach, our proposed approach retains the SL features, which could extract multiple NEs with the same types without knowing the exact number of NEs in the same passage in advance. Eventually, experiments on our CEHR dataset demonstrate that QASL-based models greatly outperform the SL-based models by 6.12{\%} to 7.14{\%} of F1-score."
2021.rocling-1.5,A Flexible and Extensible Framework for Multiple Answer Modes Question Answering,2021,-1,-1,15,0,2300,chengchung fan,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"This paper presents a framework to answer the questions that require various kinds of inference mechanisms (such as Extraction, Entailment-Judgement, and Summarization). Most of the previous approaches adopt a rigid framework which handles only one inference mechanism. Only a few of them adopt several answer generation modules for providing different mechanisms; however, they either lack an aggregation mechanism to merge the answers from various modules, or are too complicated to be implemented with neural networks. To alleviate the problems mentioned above, we propose a divide-and-conquer framework, which consists of a set of various answer generation modules, a dispatch module, and an aggregation module. The answer generation modules are designed to provide different inference mechanisms, the dispatch module is used to select a few appropriate answer generation modules to generate answer candidates, and the aggregation module is employed to select the final answer. We test our framework on the 2020 Formosa Grand Challenge Contest dataset. Experiments show that the proposed framework outperforms the state-of-the-art Roberta-large model by about 11.4{\%}."
2021.rocling-1.15,Mining Commonsense and Domain Knowledge from Math Word Problems,2021,-1,-1,4,0,2344,shihhung tsai,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"Current neural math solvers learn to incorporate commonsense or domain knowledge by utilizing pre-specified constants or formulas. However, as these constants and formulas are mainly human-specified, the generalizability of the solvers is limited. In this paper, we propose to explicitly retrieve the required knowledge from math problemdatasets. In this way, we can determinedly characterize the required knowledge andimprove the explainability of solvers. Our two algorithms take the problem text andthe solution equations as input. Then, they try to deduce the required commonsense and domain knowledge by integrating information from both parts. We construct two math datasets and show the effectiveness of our algorithms that they can retrieve the required knowledge for problem-solving."
2021.eacl-main.51,How Fast can {BERT} Learn Simple Natural Language Inference?,2021,-1,-1,2,1,10585,yichung lin,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"This paper empirically studies whether BERT can really learn to conduct natural language inference (NLI) without utilizing hidden dataset bias; and how efficiently it can learn if it could. This is done via creating a simple entailment judgment case which involves only binary predicates in plain English. The results show that the learning process of BERT is very slow. However, the efficiency of learning can be greatly improved (data reduction by a factor of 1,500) if task-related features are added. This suggests that domain knowledge greatly helps when conducting NLI with neural networks."
2021.acl-short.121,Sequence to General Tree: Knowledge-Guided Geometry Word Problem Solving,2021,-1,-1,4,0,2344,shihhung tsai,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"With the recent advancements in deep learning, neural solvers have gained promising results in solving math word problems. However, these SOTA solvers only generate binary expression trees that contain basic arithmetic operators and do not explicitly use the math formulas. As a result, the expression trees they produce are lengthy and uninterpretable because they need to use multiple operators and constants to represent one single formula. In this paper, we propose sequence-to-general tree (S2G) that learns to generate interpretable and executable operation trees where the nodes can be formulas with an arbitrary number of arguments. With nodes now allowed to be formulas, S2G can learn to incorporate mathematical domain knowledge into problem-solving, making the results more interpretable. Experiments show that S2G can achieve a better performance against strong baselines on problems that require domain knowledge."
2020.acl-main.92,A Diverse Corpus for Evaluating and Developing {E}nglish Math Word Problem Solvers,2020,-1,-1,3,0,22606,shenyun miao,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms of both language patterns and problem types) English math word problem (MWP) corpus for evaluating the capability of various MWP solvers. Existing MWP corpora for studying AI progress remain limited either in language usage patterns or in problem types. We thus present a new English MWP corpus with 2,305 MWPs that cover more text patterns and most problem types taught in elementary school. Each MWP is annotated with its problem type and grade level (for indicating the level of difficulty). Furthermore, we propose a metric to measure the lexicon usage diversity of a given MWP corpus, and demonstrate that ASDiv is more diverse than existing corpora. Experiments show that our proposed corpus reflects the true capability of MWP solvers more faithfully."
O18-1006,æ¯éé¡ä¹æ¯æè­ææª¢ç´¢ (Supporting Evidence Retrieval for Answering Yes/No Questions) [In {C}hinese],2018,0,0,3,0,2306,mengtse wu,Proceedings of the 30th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2018),0,None
N18-1060,A Meaning-Based Statistical {E}nglish Math Word Problem Solver,2018,0,2,4,1,2310,chaochun liang,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"We introduce MeSys, a meaning-based approach, for solving English math word problems (MWPs) via understanding and reasoning in this paper. It first analyzes the text, transforms both body and question parts into their corresponding logic forms, and then performs inference on them. The associated context of each quantity is represented with proposed role-tags (e.g., nsubj, verb, etc.), which provides the flexibility for annotating an extracted math quantity with its associated context information (i.e., the physical meaning of this quantity). Statistical models are proposed to select the operator and operands. A noisy dataset is designed to assess if a solver solves MWPs mainly via understanding or mechanical pattern matching. Experimental results show that our approach outperforms existing systems on both benchmark datasets and the noisy dataset, which demonstrates that the proposed approach understands the meaning of each quantity in the text more."
C18-1035,Adopting the Word-Pair-Dependency-Triplets with Individual Comparison for Natural Language Inference,2018,0,1,3,0,30757,qianlong du,Proceedings of the 27th International Conference on Computational Linguistics,0,"This paper proposes to perform natural language inference with Word-Pair-Dependency-Triplets. Most previous DNN-based approaches either ignore syntactic dependency among words, or directly use tree-LSTM to generate sentence representation with irrelevant information. To overcome the problems mentioned above, we adopt Word-Pair-Dependency-Triplets to improve alignment and inference judgment. To be specific, instead of comparing each triplet from one passage with the merged information of another passage, we first propose to perform comparison directly between the triplets of the given passage-pair to make the judgement more interpretable. Experimental results show that the performance of our approach is better than most of the approaches that use tree structures, and is comparable to other state-of-the-art approaches."
2018.ijclclp-2.4,Supporting Evidence Retrieval for Answering Yes/No Questions,2018,0,0,3,0,2306,mengtse wu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 23, Number 2, December 2018",0,None
O16-1031,æ§å»ºä¸åä¸­æåå°æ¸å­¸æå­åé¡èªæåº«(Building a Corpus for Developing the {C}hinese Elementary School Math Word Problem Solver)[In {C}hinese],2016,-1,-1,4,0,22606,shenyun miao,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),0,None
N16-3014,"A Tag-based {E}nglish Math Word Problem Solver with Understanding, Reasoning and Explanation",2016,13,4,6,1,2310,chaochun liang,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,None
L16-1466,Building A Case-based Semantic {E}nglish-{C}hinese Parallel Treebank,2016,0,0,3,0,35192,huaxing shi,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We construct a case-based English-to-Chinese semantic constituent parallel Treebank for a Statistical Machine Translation (SMT) task by labelling each node of the Deep Syntactic Tree (DST) with our refined semantic cases. Since subtree span-crossing is harmful in tree-based SMT, DST is adopted to alleviate this problem. At the same time, we tailor an existing case set to represent bilingual shallow semantic relations more precisely. This Treebank is a part of a semantic corpus building project, which aims to build a semantic bilingual corpus annotated with syntactic, semantic cases and word senses. Data in our Treebank is from the news domain of Datum corpus. 4,000 sentence pairs are selected to cover various lexicons and part-of-speech (POS) n-gram patterns as much as possible. This paper presents the construction of this case Treebank. Also, we have tested the effect of adopting DST structure in alleviating subtree span-crossing. Our preliminary analysis shows that the compatibility between Chinese and English trees can be significantly increased by transforming the parse-tree into the DST. Furthermore, the human agreement rate in annotation is found to be acceptable (90{\%} in English nodes, 75{\%} in Chinese nodes)."
C16-2032,"A Meaning-based {E}nglish Math Word Problem Solver with Understanding, Reasoning and Explanation",2016,7,3,5,1,2310,chaochun liang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"This paper presents a meaning-based statistical math word problem (MWP) solver with understanding, reasoning and explanation. It comprises a web user interface and pipelined modules for analysing the text, transforming both body and question parts into their logic forms, and then performing inference on them. The associated context of each quantity is represented with proposed role-tags (e.g., nsubj, verb, etc.), which provides the flexibility for annotating the extracted math quantity with its associated syntactic and semantic information (which specifies the physical meaning of that quantity). Those role-tags are then used to identify the desired operands and filter out irrelevant quantities (so that the answer can be obtained precisely). Since the physical meaning of each quantity is explicitly represented with those role-tags and used in the inference process, the proposed approach could explain how the answer is obtained in a human comprehensible way."
O15-3001,Designing a Tag-Based Statistical Math Word Problem Solver with Reasoning and Explanation,2015,45,4,9,1,10585,yichung lin,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 20, Number 2, {D}ecember 2015 - Special Issue on Selected Papers from {ROCLING} {XXVII}",0,"This paper proposes a tag-based statistical framework to solve math word problems with understanding and reasoning. It analyzes the body and question texts into their associated tag-based logic forms, and then performs inference on them. Comparing to those rule-based approaches, the proposed statistical approach alleviates rules coverage and ambiguity resolution problems, and our tag-based approach also provides the flexibility of handling various kinds of related questions with the same body logic form. On the other hand, comparing to those purely statistical approaches, the proposed approach is more robust to the irrelevant information and could more accurately provide the answer. The major contributions of our work are: (1) proposing a tag-based logic representation such that the system is less sensitive to the irrelevant information and could provide answer more precisely; (2) proposing a unified statistical framework for performing reasoning from the given text."
O15-3002,Explanation Generation for a Math Word Problem Solver,2015,15,6,3,1,34634,chientsung huang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 20, Number 2, {D}ecember 2015 - Special Issue on Selected Papers from {ROCLING} {XXVII}",0,"This paper proposes a math operation (e.g., Summation, Addition, Subtraction, Multiplication, Division, etc.) oriented approach to explain how the answers are obtained for math word problems. Based on the reasoning chain given by the inference engine, we search each math operator involved. For each math operator, we generate one sentence. Since explaining math operation does not require complicated syntax, we adopt a specific template to generate the text for each kind of math operator. To the best of our knowledge, this is the first explanation generation that is specifically tailored to solving the math word problem."
O15-1006,Designing a Tag-Based Statistical Math Word Problem Solver with Reasoning and Explanation,2015,45,4,9,1,34634,chientsung huang,Proceedings of the 27th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2015),0,"This paper proposes a tag-based statistical framework to solve math word problems with understanding and reasoning. It analyzes the body and question texts into their associated tag-based logic forms, and then performs inference on them. Comparing to those rule-based approaches, the proposed statistical approach alleviates rules coverage and ambiguity resolution problems, and our tag-based approach also provides the flexibility of handling various kinds of related questions with the same body logic form. On the other hand, comparing to those purely statistical approaches, the proposed approach is more robust to the irrelevant information and could more accurately provide the answer. The major contributions of our work are: (1) proposing a tag-based logic representation such that the system is less sensitive to the irrelevant information and could provide answer more precisely; (2) proposing a unified statistical framework for performing reasoning from the given text."
O15-1007,Explanation Generation for a Math Word Problem Solver,2015,15,6,3,1,34634,chientsung huang,Proceedings of the 27th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2015),0,"This paper proposes a math operation (e.g., Summation, Addition, Subtraction, Multiplication, Division, etc.) oriented approach to explain how the answers are obtained for math word problems. Based on the reasoning chain given by the inference engine, we search each math operator involved. For each math operator, we generate one sentence. Since explaining math operation does not require complicated syntax, we adopt a specific template to generate the text for each kind of math operator. To the best of our knowledge, this is the first explanation generation that is specifically tailored to solving the math word problem."
C14-1039,Dynamically Integrating Cross-Domain Translation Memory into Phrase-Based Machine Translation during Decoding,2014,25,4,3,1,7866,kun wang,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Our previous work focuses on combining translation memory (TM) and statistical machine translation (SMT) when the TM database and the SMT training set are the same. However, the TM database will deviate from the SMT training set in the real task when time goes by. In this work, we concentrate on the task when the TM database and the SMT training set are different and even from different domains. Firstly, we dynamically merge the matched TM phrase-pairs into the SMT phrase table to meet the real application. Secondly, we propose an improved integrated model to distinguish the original and the newly-added phrase-pairs. Thirdly, a simple but effective TM adaptation method is adopted to favor the consistent translations in cross-domain test. Our experiments have shown that merging the TM phrasepairs achieves significant improvements. Furthermore, the proposed approaches are significantly better than the TM, the SMT and previous integration works for both in-domain and cross-domain tests."
Y13-1010,A Study of the Effectiveness of Suffixes for {C}hinese Word Segmentation,2013,30,2,3,1,29664,xiaoqing li,"Proceedings of the 27th Pacific Asia Conference on Language, Information, and Computation ({PACLIC} 27)",0,"We investigate whether suffix related features can significantly improve the performance of character-based approaches for Chinese word segmentation (CWS). Since suffixes are quite productive in forming new words, and OOV is the main error source for CWS, many researchers expect that suffix information can further improve the performance. With this belief, we tried several suffix related features in both generative and discriminative approaches. However, our experiment results have shown that significant improvement can hardly be achieved by incorporating suffix related features into those widely adopted surface features, which is against the commonly believed supposition. Error analysis reveals that the main problem behind this surprising finding is the conflict between the degree of reliability and the coverage rate of suffix related features."
P13-1002,Integrating Translation Memory into Phrase-Based Machine Translation during Decoding,2013,25,15,3,1,7866,kun wang,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Since statistical machine translation (SMT) and translation memory (TM) complement each other in matched and unmatched regions, integrated models are proposed in this paper to incorporate TM information into phrase-based SMT. Unlike previous multi-stage pipeline approaches, which directly merge TM result into the final output, the proposed models refer to the corresponding TM information associated with each phrase at SMT decoding. On a Chinesexe2x80x93English TM database, our experiments show that the proposed integrated Model-III is significantly better than either the SMT or the TM systems when the fuzzy match score is above 0.4. Furthermore, integrated Model-III achieves overall 3.48 BLEU points improvement and 2.62 TER points reduction in comparison with the pure SMT system. Besides, the proposed models also outperform previous approaches significantly."
J13-2001,A Joint Model to Identify and Align Bilingual Named Entities,2013,66,18,3,1,6531,yufeng chen,Computational Linguistics,0,"In this article, an integrated model is derived that jointly identifies and aligns bilingual named entities NEs between Chinese and English. The model is motivated by the following observations: 1 whether an NE is translated semantically or phonetically depends greatly on its entity type, 2 entities within an aligned pair should share the same type, and 3 the initially detected NEs can act as anchors and provide further information while selecting NE candidates. Based on these observations, this article proposes a translation mode ratio feature defined as the proportion of NE internal tokens that are semantically translated, enforces an entity type consistency constraint, and utilizes additional new NE likelihoods based on the initially detected NE anchors.n n Experiments show that this novel method significantly outperforms the baseline. The type-insensitive F-score of identified NE pairs increases from 78.4% to 88.0% 12.2% relative improvement in our Chinese-English NE alignment task, and the type-sensitive F-score increases from 68.4% to 83.0% 21.3% relative improvement. Furthermore, the proposed model demonstrates its robustness when it is tested across different domains. Finally, when semi-supervised learning is conducted to train the adopted English NE recognition model, the proposed model also significantly boosts the English NE recognition type-sensitive F-score."
C12-1101,Integrating Surface and Abstract Features for Robust Cross-Domain {C}hinese Word Segmentation,2012,28,2,4,1,29664,xiaoqing li,Proceedings of {COLING} 2012,0,"Current character-based approaches are not robust for cross domain Chin ese word segmentation. In this paper, we alleviate this problem by deriving a novel enhanced ch aracter-based generative model with a new abstract aggregate candidate-feature, which indicates if th e given candidate prefers the corresponding position-tag of the longest dictionary matching wo rd. Since the distribution of the proposed feature is invariant across domains, our model thus possesses better generalization ability. Open tests on CIPS-SIGHAN-2010 show that the enhanced generative model achieves robust cross-domain performance for various OOV coverage rates and obtains the best performance on three out of four domains. The enhanced gen erative model is then further integrated with a discriminative model which also utilizes dictionary information . This integrated model is shown to be either superior or comparable to all other models repo rted in the literatur e on every domain of this task."
W10-4133,A Character-Based Joint Model for {CIPS}-{SIGHAN} Word Segmentation Bakeoff 2010,2010,10,3,3,1,7866,kun wang,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"This paper presents a Chinese Word Segmentation system for the closed track of CIPS-SIGHAN Word Segmentation Bakeoff 2010. This system adopts a character-based joint approach, which combines a character-based generative model and a character-based discriminative model. To further improve the crossdomain performance, we use an additional semi-supervised learning procedure to incorporate the unlabeled corpus. The final performance on the closed track for the simplified-character text shows that our system achieves comparable results with other state-of-the-art systems."
P10-1065,On Jointly Recognizing and Aligning Bilingual Named Entities,2010,12,16,3,1,6531,yufeng chen,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We observe that (1) how a given named entity (NE) is translated (i.e., either semantically or phonetically) depends greatly on its associated entity type, and (2) entities within an aligned pair should share the same type. Also, (3) those initially detected NEs are anchors, whose information should be used to give certainty scores when selecting candidates. From this basis, an integrated model is thus proposed in this paper to jointly identify and align bilingual named entities between Chinese and English. It adopts a new mapping type ratio feature (which is the proportion of NE internal tokens that are semantically translated), enforces an entity type consistency constraint, and utilizes additional monolingual candidate certainty factors (based on those NE anchors). The experiments show that this novel approach has substantially raised the type-sensitive F-score of identified NE-pairs from 68.4% to 81.7% (42.1% F-score imperfection reduction) in our Chinese-English NE alignment task."
C10-1132,A Character-Based Joint Model for {C}hinese Word Segmentation,2010,21,34,3,1,7866,kun wang,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"The character-based tagging approach is a dominant technique for Chinese word segmentation, and both discriminative and generative models can be adopted in that framework. However, generative and discriminative character-based approaches are significantly different and complement each other. A simple joint model combining the character-based generative model and the discriminative one is thus proposed in this paper to take advantage of both approaches. Experiments on the Second SIGHAN Bakeoff show that this joint approach achieves 21% relative error reduction over the discriminative model and 14% over the generative one. In addition, closed tests also show that the proposed joint model outperforms all the existing approaches reported in the literature and achieves the best F-score in four out of five corpora."
Y09-2047,"Which is More Suitable for {C}hinese Word Segmentation, the Generative Model or the Discriminative One?",2009,15,13,3,1,7866,kun wang,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"Since the traditional word-based n-gram model, a generative approach, cannot handle those out-of-vocabulary (OOV) words in the testing-set, the character-based discriminative approach has been widely adopted recently. However, this discriminative model, though is more robust to OOV words, fails to deliver satisfactory performance for those in-vocabulary (IV) words that have been observed before. Having analyzed the wordbased approach, its capability to handle the dependency between adjacent characters within a word, which is believed that the human adopts for doing segmentation, is found to account for its excellent performance for those IV words. To incorporate the intra-word characters dependency, a character-based approach with a generative model is thus proposed in this paper. The experiments conducted on the second SIGHAN Bakeoffs have shown that the proposed model not only achieves a good balance between those IV words and OOV words, but also outperforms the above-mentioned well-known approaches under the similar conditions."
C02-1009,A Robust Cross-Style Bilingual Sentences Alignment Model,2002,11,5,2,0,53606,tzliang kueng,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"Most current sentence alignment approaches adopt sentence length and cognate as the alignment features; and they are mostly trained and tested in the documents with the same style. Since the length distribution, alignment-type distribution (used by length-based approaches) and cognate frequency vary significantly across texts with different styles, the length-based approaches fail to achieve similar performance when tested in corpora of different styles. The experiments show that the performance in F-measure could drop from 98.2% to 85.6% when a length-based approach is trained by a technical manual and then tested on a general magazine.Since a large percentage of content words in the source text would be translated into the corresponding translation duals to preserve the meaning in the target text, transfer lexicons are usually regarded as more reliable cues for aligning sentences when the alignment task is performed by human. To enhance the robustness, a robust statistical model based on both transfer lexicons and sentence lengths are proposed in this paper. After integrating the transfer lexicons into the model, a 60% F-measure error reduction (from 14.4% to 5.8%) is observed."
O99-3002,A Level-Synchronous Approach to Ill-formed Sentence Parsing and Error Recovery,1999,25,1,2,1,10585,yichung lin,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 4, Number 1, {F}ebruary 1999",0,None
1999.mtsummit-1.29,"A customizable, self-learning parameterized {MT} system: the next generation",1999,8,1,1,1,2296,kehyih su,Proceedings of Machine Translation Summit VII,0,"In this paper, the major problems of the current machine translation systems are first outlined. A new direction, highlighting the system capability to be customizable and self-learnable, is then proposed for attacking the described problems, which are mainly resulted from the very complicated characteristics of natural languages. The proposed solution adopts an unsupervised two-way training mechanism and a parameterized architecture to acquire the required statistical knowledge, such that the system can be easily adapted to different domains and various preferences of individual users."
O98-1001,Error Recovery in Natural Language Parsing With a Level-Synchronous Approach,1998,0,0,2,1,10585,yichung lin,Proceedings of Research on Computational Linguistics Conference {XI},0,None
O97-4005,An Unsupervised Iterative Method for {C}hinese New Lexicon Extraction,1997,22,52,2,0.891082,42733,jingshin chang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 2, Number 2, August 1997",0,"An unsupervised iterative approach for extracting a new lexicon (or unknown words) from a Chinese text corpus is proposed in this paper. Instead of using a non-iterative segmentation-merging-filtering-and-disambiguation approach, the proposed method iteratively integrates the contextual constraints (among word candidates) and a joint character association metric to progressively improve the segmentation results of the input corpus (and thus the new word list.) An augmented dictionary, which includes potential unknown words (in addition to known words), is used to segment the input corpus, unlike traditional approaches which use only known words for segmentation. In the segmentation process, the augmented dictionary is used to impose contextual constraints over known words and potential unknown words within input sentences; an unsupervised Viterbi Training process is then applied to ensure that the selected potential unknown words (and known words) maximize the likelihood of the input corpus. On the other hand, the joint character association metric (which reflects the global character association characteristics across the corpus) is derived by integrating several commonly used word association metrics, such as mutual information and entropy, with a joint Gaussian mixture density function; such integration allows the filter to use multiple features simultaneously to evaluate character association, unlike traditional filters which apply multiple features independently. The proposed method then allows the contextual constraints and the joint character association metric to enhance each other; this is achieved by iteratively applying the joint association metric to truncate unlikely unknown words in the augmented dictionary and using the segmentation result to improve the estimation of the joint association metric. The refined augmented dictionary and improved estimation are then used in the next iteration to acquire better segmentation and carry out more reliable filtering. Experiments show that both the precision and recall rates are improved almost monotonically, in contrast to non-iterative segmentation-merging-filtering-and-disambiguation approaches, which often sacrifice precision for recall or vice versa. With a corpus of 311,591 sentences, the performance is 76% (bigram), 54% (trigram), and 70% (quadragram) in F-measure, which is significantly better than using the non-iterative approach with F-measures of 74% (bigram), 46% (trigram), and 58% (quadragram)."
O97-3001,Computational Tools and Resources for Linguistic Studies,1997,28,0,3,1,55639,yuling hsu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 2, Number 1, {F}ebruary 1997: Special Issue on Computational Resources for Research in {C}hinese Linguistics",0,"This paper presents several useful computational tools and available resources to facilitate linguistic studies. For each computational tool, we demonstrate why it is useful and how can it be used for research. In addition, linguistic examples are given for illustration. First, a very useful searching engine, Key Word in Context (KWIC), is introduced. This tool can automatically extract linguistically significant patterns from large corpora and help linguists discover syntagmatic generalizations. Second, Dynamic Clustering and Hierarchical Clustering are introduced for identifying natural clusters of words or phrases in distribution. Third, statistical measures which could be used to measure the degree of cohesion and correlation among linguistic units are presented. These tools can help linguists identify the boundaries of lexical units. Fourth, alignment tools for aligning parallel texts at the word, sentence and structure levels are presented for linguists who do comparative studies of different languages. Fifth, we introduce Sequential Forward Selection (SFS) and Classification and Regression Tree (CART) for automatic rule ordering. Finally, some available electronic Chinese resources are described to provide reference purposes for those who are interested."
O97-1007,A Level-synchronous Approach to Ill-formed Sentence Parsing,1997,0,1,2,1,10585,yichung lin,Proceedings of the 10th Research on Computational Linguistics International Conference,0,None
O97-1009,A Multivariate {G}aussian Mixture Model for Automatic Compound Word Extraction,1997,22,5,2,0.891082,42733,jingshin chang,Proceedings of the 10th Research on Computational Linguistics International Conference,0,None
1997.mtsummit-papers.14,Corpus-Based Statistics-Oriented ({CBSO}) Machine Translation Researches in {T}aiwan,1997,-1,-1,2,0.891082,42733,jingshin chang,Proceedings of Machine Translation Summit VI: Papers,0,"A brief introduction to the MT research projects in Taiwan is given in this paper. Special attention is given to the more and more popular corpus-based statistics-oriented (CBSO) approaches in MT researches. In particular, the parameterized two-way training philosophy in designing the second generation BehaviorTran, which is the first and the largest operational system in this area, is introduced in this paper."
W96-0110,Statistical Models for Deep-structure Disambiguation,1996,-1,-1,2,1,55899,tunghui chiang,Fourth Workshop on Very Large Corpora,0,None
O96-2004,An Overview of Corpus-Based Statistics-Oriented ({CBSO}) Techniques for Natural Language Processing,1996,73,10,1,1,2296,kehyih su,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 1, Number 1, August 1996",0,"A Corpus-Based Statistics-Oriented (CBSO) methodology, which is an attempt to avoid the drawbacks of traditional rule-based approaches and purely statistical approaches, is introduced in this paper. Rule-based approaches, with rules induced by human experts, had been the dominant paradigm in the natural language processing community. Such approaches, however, suffer from serious difficulties in knowledge acquisition in terms of cost and consistency. Therefore, it is very difficult for such systems to be scaled-up. Statistical methods, with the capability of automatically acquiring knowledge from corpora, are becoming more and more popular, in part, to amend the shortcomings of rule-based approaches. However, most simple statistical models, which adopt almost nothing from existing linguistic knowledge, often result in a large parameter space and, thus, require an unaffordably large training corpus for even well-justified linguistic phenomena. The corpus-based statistics-oriented (CBSO) approach is a compromise between the two extremes of the spectrum for knowledge acquisition. CBSO approach emphasizes use of well-justified linguistic knowledge in developing the underlying language model and application of statistical optimization techniques on top of high level constructs, such as annotated syntax trees, rather than on surface strings, so that only a training corpus of reasonable size is needed for training and long distance dependency between constituents could be handled. In this paper, corpus-based statistics-oriented techniques are reviewed. General techniques applicable to CBSO approaches are introduced. In particular, we shall address the following important issues: (1) general tasks in developing an NLP system; (2) why CBSO is the preferred choice among different strategies; (3) how to achieve good performance systematically using a CBSO approach, and (4) frequently used CBSO techniques. Several examples are also reviewed."
W95-0109,Automatic Construction of a {C}hinese Electronic Dictionary,1995,10,16,3,1,42733,jingshin chang,Third Workshop on Very Large Corpora,0,"In this paper, an unsupervised approach for constructing a large-scale Chinese electronic dictionary is surveyed. The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from a large untagged text corpus with the aid of the information in a small tagged seed corpus. The basic model is based on a Viterbi reestimation technique. During the dictionary construction process, it tries to optimize the automatic segmentation and tagging process by repeatedly refining the set of parameters of the underlying language model. The refined parameters are then used to furtherget a better tagging result. In addition, a two-class classifier, which is capable of classifying an n-gram either as a word or a non-word, is used in combination with the Viterbi training module to improve the system performance. Two different system configurations had been developed to construct the dictionary. The configurations include (1) a Viterbi word identification module followed by a Viterbi POS tagging module and (2) a two-class classification module as the postfilter for the above Viterbi word identification module. With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences, the performance for bigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier is applied to the word list suggested by the Viterbi word identification module. The Viterbi part of speech tag reestimation stage gives the figures of 71.16% and 71.81% weighted precision rates and 73.42% and 73.83% weighted recall rates for the 2 different configurations when using a seed corpus of 9676 sentences."
O95-1003,The New Generation {B}ehavior{T}ran: Design Philosophy And System Architecture,1995,0,1,2,1,55639,yuling hsu,Proceedings of Rocling {VIII} Computational Linguistics Conference {VIII},0,None
J95-3002,"Robust Learning, Smoothing, and Parameter Tying on Syntactic Ambiguity Resolution",1995,28,15,3,1,55899,tunghui chiang,Computational Linguistics,0,"Statistical approaches to natural language processing generally obtain the parameters by using the maximum likelihood estimation (MLE) method. The MLE approaches, however, may fail to achieve good performance in difficult tasks, because the discrimination and robustness issues are not taken into consideration in the estimation processes. Motivated by that concern, a discrimination-and robustness-oriented learning algorithm is proposed in this paper for minimizing the error rate. In evaluating the robust learning procedure on a corpus of 1,000 sentences, 64.3% of the sentences are assigned their correct syntactic structures, while only 53.1% accuracy rate is obtained with the MLE approach.In addition, parameters are usually estimated poorly when the training data is sparse. Smoothing the parameters is thus important in the estimation process. Accordingly, we use a hybrid approach combining the robust learning procedure with the smoothing method. The accuracy rate of 69.8% is attained by using this approach. Finally, a parameter tying scheme is proposed to tie those highly correlated but unreliably estimated parameters together so that the parameters can be better trained in the learning process. With this tying scheme, the number of parameters is reduced by a factor of 2,000 (from 8.7 x 108 to 4.2 x 105), and the accuracy rate for parse tree selection is improved up to 70.3% when the robust learning procedure is applied on the tied parameters."
1995.tmi-1.27,"A Corpus-based Two-Way Design for Parameterized {MT} Systems: Rationale, Architecture and Training Issues",1995,-1,-1,1,1,2296,kehyih su,Proceedings of the Sixth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
P94-1033,A Corpus-based Approach to Automatic Compound Extraction,1994,10,41,1,1,2296,kehyih su,32nd Annual Meeting of the Association for Computational Linguistics,1,"An automatic compound retrieval method is proposed to extract compounds within a text message. It uses n-gram mutual information, relative frequency count and parts of speech as the features for compound extraction. The problem is modeled as a two-class classification problem based on the distributional characteristics of n-gram tokens in the compound and the non-compound clusters. The recall and precision using the proposed approach are 96.2% and 48.2% for bigram compounds and 96.6% and 39.6% for trigram compounds for a testing corpus of 49,314 words. A significant cutdown in processing time has been observed."
P94-1034,An Automatic Treebank Conversion Algorithm for Corpus Sharing,1994,3,16,3,0,56283,jongnae wang,32nd Annual Meeting of the Association for Computational Linguistics,1,"An automatic treebank conversion method is proposed in this paper to convert a treebank into another treebank. A new treebank associated with a different grammar can be generated automatically from the old one such that the information in the original treebank can be transformed to the new one and be shared among different research communities. The simple algorithm achieves conversion accuracy of 96.4% when tested on 8,867 sentences between two major grammar revisions of a large MT system."
C94-1023,{AUTOMATIC} {MODEL} {REFINEMENT} - with an application to tagging,1994,11,7,3,1,10585,yichung lin,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Statistical NLP models usually only consider coarse information and very restricted context to make the estimation of parameters feasible. To reduce the modeling error introduced by a simplified probabilistic model, the Classification and Regression Tree (CART) method was adopted in this paper to select more discriminative features for automatic model refinement. Because the features are adopted dependently during splitting the classification tree in CART, the number of training data in each terminal node is small, which makes the labeling process of terminal nodes not robust. This over-tuning phenomenon cannot be completely removed by cross - validation process (i.e., pruning process). A probabilistic classification model based on the selected discriminative features is thus proposed to use the training data more efficiently. In tagging the Brown Corpus, our probabilistic classification model reduces the error rate of the top 10 error dominant words from 5.71% to 4.35%, which shows 23.82% improvement over the unrefined model."
O93-1004,A Preliminary Study On Unknown Word Problem In {C}hinese Word Segmentation,1993,0,44,3,0,56733,mingyu lin,Proceedings of Rocling {VI} Computational Linguistics Conference {VI},0,None
O93-1006,Corpus-based Automatic Rule Selection in Designing a Grammar Checker,1993,0,2,3,0,56734,yuanling liu,Proceedings of Rocling {VI} Computational Linguistics Conference {VI},0,None
O93-1009,Corpus-based Automatic Compound Extraction with Mutual Information and Relative Frequency Count,1993,0,22,2,0,56282,mingwen wu,Proceedings of Rocling {VI} Computational Linguistics Conference {VI},0,None
1993.tmi-1.1,A Corpus-Based Statistics-Oriented Transfer and Generation Model for Machine Translation,1993,-1,-1,2,1,42733,jingshin chang,Proceedings of the Fifth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
P92-1023,{GPSM}: A Generalized Probabilistic Semantic Model for Ambiguity Resolution,1992,16,33,3,1,42733,jingshin chang,30th Annual Meeting of the Association for Computational Linguistics,1,"In natural language processing, ambiguity resolution is a central issue, and can be regarded as a preference assignment problem. In this paper, a Generalized Probabilistic Semantic Model (GPSM) is proposed for preference computation. An effective semantic tagging procedure is proposed for tagging semantic features. A semantic score function is derived based on a score function, which integrates lexical, syntactic and semantic preference under a uniform formulation. The semantic score measure shows substantial improvement in structural disambiguation over a syntax-based approach."
O92-1001,Discrimination Oriented Probabilistic Tagging,1992,0,19,3,1,10585,yichung lin,Proceedings of Rocling V Computational Linguistics Conference V,0,None
O92-1003,Statistical Models for Word Segmentation And Unknown Word Resolution,1992,4,54,4,1,55899,tunghui chiang,Proceedings of Rocling V Computational Linguistics Conference V,0,None
C92-2067,A New Quantitative Quality Measure for Machine Translation Systems,1992,16,54,1,1,2296,kehyih su,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In this paper, an objective quantitative quality measure is proposed to evaluate the performance of machine translation systems. The proposed method is to compare the raw translation output of an MT system with the final revised version for the customers, and then compute the editing efforts required to convert the raw translation to the final version. In contrast to the other proposals, the evaluation process can be done quickly and automatically. Hence, it can provide a quick response on any system change. A system designer can thus quickly find the advantages or faults of a particular performance dynamically. Application of such a measure to improve the system performance on-line on a parameterized and feedback-controlled system will be demonstrated. Furthermore, because the revised version is used directly as a reference, the performance measure can reflect the real quality gap between the system performance and customer expectation. A system designer can thus concentrate on practically important topics rather than on theoretically interesting issues."
C92-1055,Syntactic Ambiguity Resolution Using A Discrimination and Robustness Oriented Adaptive Learning Algorithm,1992,8,11,3,1,55899,tunghui chiang,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In this paper, a discrimination and robustness oriented adaptive learning procedure is proposed to deal with the task of syntactic ambiguity resolution. Owing to the problem of insufficient training data and approximation error introduced by the language model, traditional statistical approaches, which resolve ambiguities by indirectly and implicitly using maximum likelihood method, fail to achieve high performance in real applications. The proposed method remedies these problems by adjusting the parameters to maximize the accuracy rate directly. To make the proposed algorithm robust, the possible variations between the training corpus and the real tasks are also taken into consideration by enlarging the separation margin between the correct candidate and its competing members. Significant improvement has been observed in the test. The accuracy rate of syntactic disambiguation is raised from 46.0% to 60.62% by using this novel approach."
1992.tmi-1.22,Why corpus-based statistics-oriented machine translation,1992,-1,-1,1,1,2296,kehyih su,Proceedings of the Fourth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
O91-1009,Constructing A Phrase Structure Grammar By Incorporating Linguistic Knowledge And Statistical Log-Likelihood Ratio,1991,0,7,1,1,2296,kehyih su,Proceedings of Rocling {IV} Computational Linguistics Conference {IV},0,None
O90-1004,The Semantic Score Approach to the Disambiguation of {PP} Attachment Problem,1990,11,7,3,1,15624,chaolin liu,Proceedings of Rocling {III} Computational Linguistics Conference {III},0,None
W89-0210,A Sequential Truncation Parsing Algorithm Based on the Score Function,1989,0,7,1,1,2296,kehyih su,Proceedings of the First International Workshop on Parsing Technologies,0,"In a natural language processing system, a large amount of ambiguity and a large branching factor are hindering factors in obtaining the desired analysis for a given sentence in a short time. In this paper, we are proposing a sequential truncation parsing algorithm to reduce the searching space and thus lowering the parsing time. The algorithm is based on a score function which takes the advantages of probabilistic characteristics of syntactic information in the sentences. A preliminary test on this algorithm was conducted with a special version of our machine translation system, the ARCHTRAN, and an encouraging result was observed."
O89-1002,A Unification-based Approach to Lexicography for Machine Translation System,1989,0,0,3,1,57415,shuchuan chen,Proceedings of Rocling {II} Computational Linguistics Conference {II},0,None
O89-1007,A Quantitative Comparison Between an {LR} Parser and an {ATN} Interpreter,1989,0,1,2,1,15624,chaolin liu,Proceedings of Rocling {II} Computational Linguistics Conference {II},0,None
O89-1010,Smoothing Statistic Databases in a Machine Translation System,1989,0,2,1,1,2296,kehyih su,Proceedings of Rocling {II} Computational Linguistics Conference {II},0,None
O88-1001,Incremental Environment For Scored Machine Translation Systems,1988,0,1,2,0,42733,jingshin chang,Proceedings of Rocling {I} Computational Linguistics Conference {I},0,None
O88-1003,The Processing of {E}nglish Compound and Complex Words in an {E}nglish-{C}hinese Machine Translation System,1988,0,2,2,0,57415,shuchuan chen,Proceedings of Rocling {I} Computational Linguistics Conference {I},0,None
O88-1008,Criteria for the Classification of Lexical Categories in a Syntax-Oriented Parsing System,1988,0,0,2,0,57738,yuling shiu,Proceedings of Rocling {I} Computational Linguistics Conference {I},0,None
O88-1009,Descriptive Language as a Linguistic Tool,1988,-1,-1,2,0,57693,meihui su,Proceedings of Rocling {I} Computational Linguistics Conference {I},0,None
C88-2133,Semantic and Syntactic Aspects of Score Function,1988,3,16,1,1,2296,kehyih su,{C}oling {B}udapest 1988 Volume 2: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In a Machine Translation System (MTS), the number of possible analyses for a given sentence is largely due to the ambiguous characteristics of the source language. In this paper, a mechanism, called Score Function, is proposed for measuring the quality of the ambiguous syntax trees such that the one that best fits interpretation by human is selected. It is featured by incorporating the objectiveness of the probability theory and the subjective expertise of linguists. The underlying uncertainty that is fundamental to linguistic knowledge is also allowed to be incorporated into this system. This feature proposes an easy resolution to select the best syntax tree and provides some strategic advantages for scored parsing. The linguists can also be relieved of the necessity to describe the language in strictly correct linguistic rules, which, if not impossible, is a very hard task."
