2006.amta-papers.25,W05-0909,0,0.951185,"atthew Snover and Bonnie Dorr Institute for Advanced Computer Studies University of Maryland College Park, MD 20742 {snover,bonnie}@umiacs.umd.edu Richard Schwartz, Linnea Micciulla, and John Makhoul BBN Technologies 10 Moulton Street Cambridge, MA 02138 {schwartz,lmicciul,makhoul}@bbn.com Abstract most widely used of which is BLEU (Papineni et al., 2002), an evaluation metric that matches ngrams from multiple references. A variant of this metric, typically referred to as the “NIST” metric, was proposed by Doddington (Doddington, 2002). Other proposed methods for MT evaluation include METEOR (Banerjee and Lavie, 2005), which uses unigram matches on the words and their stems, and a linear combination of automatic MT evaluation methods along with meaning-based features for identifying paraphrases (Russo-Lassner et al., 2005).1 We define a new, more intuitive measure of “goodness” of MT output—specifically, the number of edits needed to fix the output so that it semantically matches a correct translation. In a less expensive variant, we attempt to avoid the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. We also seek to achieve higher correlations with"
2006.amta-papers.25,A94-1016,0,0.0221448,"s of our experiments, comparing TER and HTER to BLEU and METEOR (and their human-targeted variants, HBLEU and HMETEOR). We compare these measures against human judgments of the fluency and adequateness of the system output. Finally, we conclude with a summary of results and future work. 2 Related Work The first attempts at MT evaluation relied on purely subjective human judgments (King, 1996). Later work measured MT error by post editing MT output and counting the number of edits, typically measured in the number of keystrokes to convert the system output into a “canonical” human translation (Frederking and Nirenburg, 1994). Attempts have been made to improve MT performance by automatic post-editing techniques (Knight and Chander, 1994). Post editing measures have also been shown effective for text summarization evaluation (Mani et al., 2002) and natural language generation (Sripada et al., 2004). is the definitive MT measure. The authors make no such claim, and have adopted the name Translation Edit Rate for use in this paper and the wider community. When developing MT systems, a purely automatic measure of accuracy is preferred for rapid feedback and reliability. Purely human based evaluation metrics fail in t"
2006.amta-papers.25,niessen-etal-2000-evaluation,0,0.434114,"Missing"
2006.amta-papers.25,P02-1040,0,0.118133,"stems, a purely automatic measure of accuracy is preferred for rapid feedback and reliability. Purely human based evaluation metrics fail in this regard and have largely been replaced by purely automatic MT evaluations. Automatic MT evaluation has traditionally relied upon string comparisons between a set of reference translations and a translation hypothesis. The quality of such automatic measures can only be determined by comparisons to human judgments. One difficulty in using these automatic measures is that their output is not meaningful except to compare one system against another. BLEU (Papineni et al., 2002) calculates the score of a translation by measuring the number of ngrams, of varying length, of the system output that occur within the set of references. This measure has contributed to the recent improvement in MT systems by giving developers a reliable, cheap evaluation measure on which to compare their systems. However, BLEU is relatively unintuitive and relies upon a large number of references and a large number of sentences in order to correlate with human judgments. METEOR (Banerjee and Lavie, 2005) is an evaluation measure that counts the number of exact word matches between the system"
2006.amta-papers.25,2003.mtsummit-papers.51,0,0.150991,"mber of references and a large number of sentences in order to correlate with human judgments. METEOR (Banerjee and Lavie, 2005) is an evaluation measure that counts the number of exact word matches between the system output and reference. Unmatched words are then stemmed and matched. Additional penalities are assessed for reordering the words between the hypothesis and reference. This method has been shown to correlate very well with human judgments. An MT scoring measure that uses the notion of maximum matching string (MMS) has been demonstrated to yield high correlations with human judges (Turian et al., 2003). The MMS method is similar to the approach used by TER, in that it only allows a string to be matched once, and also permits string reordering. The MMS approach explicitly favors long contiguous matches, whereas TER attempts to minimize the number of edits between the reference and the hypothesis. TER assigns a lower cost to phrasal shifts than MMS, and does not explicitly favor longer matching strings. 224 3 Definition of Translation Edit Rate TER is defined as the minimum number of edits needed to change a hypothesis so that it exactly matches one of the references, normalized by the averag"
2014.iwslt-papers.11,J03-1002,0,0.00393638,"eker and “drives” the majority of conversations. These large-vocabulary conversations are spontaneous and freeform, with few restrictions. This collection consists of a variety of domains including force protection (e.g. checkpoint, reconnaissance, patrol), medical diagnosis and aid, maintenance and infrastructure, etc; each transcribed from spoken bilingual conversations and manually translated. The SMT parallel training corpus contains approximately 773K sentence pairs (7.3M English words). We used this corpus to extract translation phrase pairs from bidirectional IBM Model 4 word alignment [1] based on the heuristic approach of [2]. A 4-gram target LM was trained on all English transcriptions. Our phrase-based decoder is similar to Moses [3] and uses the phrase pairs and target LM to perform beam search stack decoding based on a standard log-linear model, the parameters of which were tuned with MERT [4] on a held-out development set (≈11,000 sentence pairs) using BLEU as the tuning metric. Finally, we evaluated translation performance on a separate, unseen test set (≈9,300 sentence pairs). Most of these conversations between bilingual speakers are mediated through a human interpret"
2014.iwslt-papers.11,N03-1017,0,0.0224379,"rsations. These large-vocabulary conversations are spontaneous and freeform, with few restrictions. This collection consists of a variety of domains including force protection (e.g. checkpoint, reconnaissance, patrol), medical diagnosis and aid, maintenance and infrastructure, etc; each transcribed from spoken bilingual conversations and manually translated. The SMT parallel training corpus contains approximately 773K sentence pairs (7.3M English words). We used this corpus to extract translation phrase pairs from bidirectional IBM Model 4 word alignment [1] based on the heuristic approach of [2]. A 4-gram target LM was trained on all English transcriptions. Our phrase-based decoder is similar to Moses [3] and uses the phrase pairs and target LM to perform beam search stack decoding based on a standard log-linear model, the parameters of which were tuned with MERT [4] on a held-out development set (≈11,000 sentence pairs) using BLEU as the tuning metric. Finally, we evaluated translation performance on a separate, unseen test set (≈9,300 sentence pairs). Most of these conversations between bilingual speakers are mediated through a human interpreter. Of the 773K training sentence pairs"
2014.iwslt-papers.11,P07-2045,0,0.0117528,"ion consists of a variety of domains including force protection (e.g. checkpoint, reconnaissance, patrol), medical diagnosis and aid, maintenance and infrastructure, etc; each transcribed from spoken bilingual conversations and manually translated. The SMT parallel training corpus contains approximately 773K sentence pairs (7.3M English words). We used this corpus to extract translation phrase pairs from bidirectional IBM Model 4 word alignment [1] based on the heuristic approach of [2]. A 4-gram target LM was trained on all English transcriptions. Our phrase-based decoder is similar to Moses [3] and uses the phrase pairs and target LM to perform beam search stack decoding based on a standard log-linear model, the parameters of which were tuned with MERT [4] on a held-out development set (≈11,000 sentence pairs) using BLEU as the tuning metric. Finally, we evaluated translation performance on a separate, unseen test set (≈9,300 sentence pairs). Most of these conversations between bilingual speakers are mediated through a human interpreter. Of the 773K training sentence pairs, about 267K originate in ≈3,000 marked-up bilingual conversations. We use this subset to construct an anticipat"
2014.iwslt-papers.11,P03-1021,0,0.00549053,"etc; each transcribed from spoken bilingual conversations and manually translated. The SMT parallel training corpus contains approximately 773K sentence pairs (7.3M English words). We used this corpus to extract translation phrase pairs from bidirectional IBM Model 4 word alignment [1] based on the heuristic approach of [2]. A 4-gram target LM was trained on all English transcriptions. Our phrase-based decoder is similar to Moses [3] and uses the phrase pairs and target LM to perform beam search stack decoding based on a standard log-linear model, the parameters of which were tuned with MERT [4] on a held-out development set (≈11,000 sentence pairs) using BLEU as the tuning metric. Finally, we evaluated translation performance on a separate, unseen test set (≈9,300 sentence pairs). Most of these conversations between bilingual speakers are mediated through a human interpreter. Of the 773K training sentence pairs, about 267K originate in ≈3,000 marked-up bilingual conversations. We use this subset to construct an anticipatory corpus for the adaptation experiments. These sentence pairs are assigned a unique utterance ID. All other sentence pairs are assigned to a default utterance ID,"
2014.iwslt-papers.11,W04-3250,0,0.0698034,"Missing"
2014.iwslt-papers.11,2001.mtsummit-papers.68,0,0.0643941,"Missing"
2014.iwslt-papers.11,2006.amta-papers.25,1,0.793889,"Missing"
2014.iwslt-papers.11,W05-0909,0,0.0242101,"Missing"
2014.iwslt-papers.11,W07-0717,0,0.0289487,"ation for incidental reasons. We also compared smoothed, sentence-level BLEU scores,2 and observed that the the n=100 adapted system scores higher than the baseline 884 times and lower than the baseline 763 times.3 We take this as further evidence that the retrieval-based adaptation leads to small but systematic improvements in translation quality. 5. Relation to Prior Work Online model adaptation for SMT has become an active area of research in recent years. The predominant approach is to divide the training data into discrete partitions representing either domains or genres to be adapted to [9, 10] or other linguistic phenomena of interest, such as whether the current utterance is a question [11]. At run-time, the domain, genre or other inferred properties of the current utterance are used to prefer phrase translation rules that originate in appropriate training data. By contrast, our approach makes no assumptions about the nature of the training data, and therefore requires no hard decisions about training set partitions and no labor-intensive manual annotation. Instead, we directly retrieve exemplars from the training set using lexical cues in order to guide the anticipatory inference"
2014.iwslt-papers.11,P12-2023,0,0.0182097,"ase translation rules that originate in appropriate training data. By contrast, our approach makes no assumptions about the nature of the training data, and therefore requires no hard decisions about training set partitions and no labor-intensive manual annotation. Instead, we directly retrieve exemplars from the training set using lexical cues in order to guide the anticipatory inference. To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis [12], ‘biTAM’ [13] or latent dirichlet allocation [14, 15, 16]. As it also avoids data set partitioning and explicit annotation, our work is in the same spirit as these, but we do not explicitly model topic distributions. In our previous work [16], we incrementally accumulated conversational history to compute a topic distribution vector. The phrasal translation rules were scored using the maximum similarity of the current conversational topic vector to all of the training conversation topic vectors from which that phrasal rule was drawn. This work is also incremental, but in contrast uses only the previous utterance of the conversational counterpart to"
2014.iwslt-papers.11,P13-2122,1,0.107286,"ase translation rules that originate in appropriate training data. By contrast, our approach makes no assumptions about the nature of the training data, and therefore requires no hard decisions about training set partitions and no labor-intensive manual annotation. Instead, we directly retrieve exemplars from the training set using lexical cues in order to guide the anticipatory inference. To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis [12], ‘biTAM’ [13] or latent dirichlet allocation [14, 15, 16]. As it also avoids data set partitioning and explicit annotation, our work is in the same spirit as these, but we do not explicitly model topic distributions. In our previous work [16], we incrementally accumulated conversational history to compute a topic distribution vector. The phrasal translation rules were scored using the maximum similarity of the current conversational topic vector to all of the training conversation topic vectors from which that phrasal rule was drawn. This work is also incremental, but in contrast uses only the previous utterance of the conversational counterpart to"
2014.iwslt-papers.11,P14-1129,1,0.840086,"Missing"
2014.iwslt-papers.11,D09-1074,0,0.0149101,"ation for incidental reasons. We also compared smoothed, sentence-level BLEU scores,2 and observed that the the n=100 adapted system scores higher than the baseline 884 times and lower than the baseline 763 times.3 We take this as further evidence that the retrieval-based adaptation leads to small but systematic improvements in translation quality. 5. Relation to Prior Work Online model adaptation for SMT has become an active area of research in recent years. The predominant approach is to divide the training data into discrete partitions representing either domains or genres to be adapted to [9, 10] or other linguistic phenomena of interest, such as whether the current utterance is a question [11]. At run-time, the domain, genre or other inferred properties of the current utterance are used to prefer phrase translation rules that originate in appropriate training data. By contrast, our approach makes no assumptions about the nature of the training data, and therefore requires no hard decisions about training set partitions and no labor-intensive manual annotation. Instead, we directly retrieve exemplars from the training set using lexical cues in order to guide the anticipatory inference"
2014.iwslt-papers.11,W08-0334,0,0.0433927,"the the n=100 adapted system scores higher than the baseline 884 times and lower than the baseline 763 times.3 We take this as further evidence that the retrieval-based adaptation leads to small but systematic improvements in translation quality. 5. Relation to Prior Work Online model adaptation for SMT has become an active area of research in recent years. The predominant approach is to divide the training data into discrete partitions representing either domains or genres to be adapted to [9, 10] or other linguistic phenomena of interest, such as whether the current utterance is a question [11]. At run-time, the domain, genre or other inferred properties of the current utterance are used to prefer phrase translation rules that originate in appropriate training data. By contrast, our approach makes no assumptions about the nature of the training data, and therefore requires no hard decisions about training set partitions and no labor-intensive manual annotation. Instead, we directly retrieve exemplars from the training set using lexical cues in order to guide the anticipatory inference. To avoid the need for hard decisions about domain membership, some have used topic modeling to imp"
2014.iwslt-papers.11,P06-2124,0,0.0313271,"ent utterance are used to prefer phrase translation rules that originate in appropriate training data. By contrast, our approach makes no assumptions about the nature of the training data, and therefore requires no hard decisions about training set partitions and no labor-intensive manual annotation. Instead, we directly retrieve exemplars from the training set using lexical cues in order to guide the anticipatory inference. To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis [12], ‘biTAM’ [13] or latent dirichlet allocation [14, 15, 16]. As it also avoids data set partitioning and explicit annotation, our work is in the same spirit as these, but we do not explicitly model topic distributions. In our previous work [16], we incrementally accumulated conversational history to compute a topic distribution vector. The phrasal translation rules were scored using the maximum similarity of the current conversational topic vector to all of the training conversation topic vectors from which that phrasal rule was drawn. This work is also incremental, but in contrast uses only the previous utt"
2020.clssts-1.7,J03-1002,0,0.0178101,"we train the learner is small, we did not manage to obtain results that generalized better. • The QST-transformed score scoreqst (q, d) w∈q Text Tune 482 449 460 Table 1: Size of various datasets (in terms of number of documents). • Original retrieval score(q, d) w∈q Train 338 316 291 4.2. The CLIR System We give a brief description of the CLIR system that is used to generate the original retrieval scores. A more detailed description appears in (Zbib et al., 2019). It uses a probabilistic bilingual dictionary, trained on a set of parallel sentences and lexicons that were aligned with GIZA++ (Och and Ney, 2003). For each language pair (Somali-English, Swahili-English and Tagalog-English) the bilingual dictionary provides a translation probability P (e|f ) between a source word f and a target word e. Queries consist of one or more words in the target language (English), and a document is deemed relevant to a query if it contains at least one occurrence of each of the terms of the query.2 αi · fi i=1 and new decisions decision(q, d) = 1[scoremodel (q, d) ≥ t∗ ]. During training, AQWV performance is also measured on a “tuning” set for early stopping. L2 regularization (which forces the trained weights"
2020.clssts-1.8,W19-6721,0,0.0609898,"Missing"
2020.clssts-1.8,kamholz-etal-2014-panlex,0,0.0601285,"Missing"
2020.clssts-1.8,P07-2045,0,0.00893272,"sed SMT: a phrase-based statistical MT system trained over the Lithuanian or Bulgarian bilingual data. • Post-Processing: Filter out any hyponyms that have a vector cosine distance relative to the original EXAMPLE OF phrase greater than 0.35. As above, we use the word embeddings from (Bojanowski et al., 2017). All MT models produce N-best (N=20) hypotheses as output for downstream summarization processing. We used the tensor2tensor toolkit (Vaswani et al., 2018) for the transformer implementation and the fairseq toolkit (Ott et al., 2019) for the dynamic convolution model. We also used Moses (Koehn et al., 2007) for training the phrase-based model. Our own tokenizer was used instead of the tokenizer from Moses to match the tokenization scheme used by other system components. Subword tokenization was done using the sentencepiece toolkit (Kudo and Richardson, 2018), an unsupervised text tokenization method that is independent of the language being processed. 5. For instance, the expansions for the query EXAMPLE OF(footwear) include: “baby shoe”, “bowling shoe”, “sneaker”, “wooden shoe”, “rubber boot”, “congress shoe”, “ghillie”, “combat boot”, “footgear”, “huarache”, etc. 5.2. Indexing We construct inv"
2020.clssts-1.8,D18-2012,0,0.0310714,"ve, we use the word embeddings from (Bojanowski et al., 2017). All MT models produce N-best (N=20) hypotheses as output for downstream summarization processing. We used the tensor2tensor toolkit (Vaswani et al., 2018) for the transformer implementation and the fairseq toolkit (Ott et al., 2019) for the dynamic convolution model. We also used Moses (Koehn et al., 2007) for training the phrase-based model. Our own tokenizer was used instead of the tokenizer from Moses to match the tokenization scheme used by other system components. Subword tokenization was done using the sentencepiece toolkit (Kudo and Richardson, 2018), an unsupervised text tokenization method that is independent of the language being processed. 5. For instance, the expansions for the query EXAMPLE OF(footwear) include: “baby shoe”, “bowling shoe”, “sneaker”, “wooden shoe”, “rubber boot”, “congress shoe”, “ghillie”, “combat boot”, “footgear”, “huarache”, etc. 5.2. Indexing We construct inverted indexes for both the source language and the target language. For text documents, we index words and n-grams. For speech documents, we index both the 1-best output (which is treated as regular text) and the confusion network, saving the ASR posterior"
2020.clssts-1.8,P11-1052,0,0.0715547,"Missing"
2020.clssts-1.8,Q17-1010,0,0.0435206,"tion (Section 6). Because there is no casing information in the ASR transcript, we augmented the MT training data with the lower-cased version of the source data with punctuation marks removed to mimic the condition of ASR output. The neural MT models were trained on both versions of the data together, in a single “multi-style” fashion, to handle both text and ASR transcript as input. This was however not done for the phrase-based model described below. 2 46 http://commoncrawl.org 4.2. MT Models are expanded using nearest-neighbor words of English pretrained Wikipedia-derived word embeddings (Bojanowski et al., 2017) (with a minimum cosine similarity cosmin , typically between 0.3-0.4). The weight of each expansion is an exponential function of the cosine similarity, as follows The machine translation component consists of two multilingual neural MT models and one phrase-based statistical MT (SMT) model: 1. Transformer NMT: a 6-layer transfomer-based model (Vaswani et al., 2017) jointly trained over Lithuanian, Bulgarian, Russian and Ukrainian data. We applied data oversampling and used 21k subword units in the vocabulary. We trained the model over the training data using 600k training steps with a batch"
2020.clssts-1.8,P19-3004,0,0.278746,"rier and to make domain information accessible to all users irrespective of language and region. The IARPA MATERIAL1 program presents us with the challenge of developing high-performance CLIR, machine translation, automatic speech recognition (ASR), and summarization for a new language in a few weeks, given limited training resources. In this paper, we describe our CLIR system entry to the MATERIAL evaluation of October, 2019. We were to process evaluation data for both Lithuanian and Bulgarian and to submit system output in 10 days. Our CLIR system achieves the same goal as the SARAL system (Boschee et al., 2019a). While both systems feature a neural network (NN) architecture, the main difference lies in the way an NN model is used. The SARAL system uses a neural network attention model (dot-product) to compute query-document relevance from a shared embedding space, while our system utilizes neural network (multilayer perceptron) as part of the Neural Network Lexical Translation Model (Zbib et al., 2019) to produce probability of translation needed by a probabilistic CLIR model. The rest of this paper is organized as follows: we introduce the task and data in section 2, including a high level overvie"
2020.clssts-1.8,N19-4009,0,0.0246244,"tree and record all hyponyms found during the process. 3. Moses Phrase-based SMT: a phrase-based statistical MT system trained over the Lithuanian or Bulgarian bilingual data. • Post-Processing: Filter out any hyponyms that have a vector cosine distance relative to the original EXAMPLE OF phrase greater than 0.35. As above, we use the word embeddings from (Bojanowski et al., 2017). All MT models produce N-best (N=20) hypotheses as output for downstream summarization processing. We used the tensor2tensor toolkit (Vaswani et al., 2018) for the transformer implementation and the fairseq toolkit (Ott et al., 2019) for the dynamic convolution model. We also used Moses (Koehn et al., 2007) for training the phrase-based model. Our own tokenizer was used instead of the tokenizer from Moses to match the tokenization scheme used by other system components. Subword tokenization was done using the sentencepiece toolkit (Kudo and Richardson, 2018), an unsupervised text tokenization method that is independent of the language being processed. 5. For instance, the expansions for the query EXAMPLE OF(footwear) include: “baby shoe”, “bowling shoe”, “sneaker”, “wooden shoe”, “rubber boot”, “congress shoe”, “ghillie”,"
2020.clssts-1.8,P16-1009,0,0.0911142,"Missing"
2020.clssts-1.8,tiedemann-2012-parallel,0,0.0605221,"Missing"
2020.clssts-1.8,W18-1819,0,0.0298316,"s. • WordNet Hyponym Traversal: For each Synset, recursively traverse its hyponym tree and record all hyponyms found during the process. 3. Moses Phrase-based SMT: a phrase-based statistical MT system trained over the Lithuanian or Bulgarian bilingual data. • Post-Processing: Filter out any hyponyms that have a vector cosine distance relative to the original EXAMPLE OF phrase greater than 0.35. As above, we use the word embeddings from (Bojanowski et al., 2017). All MT models produce N-best (N=20) hypotheses as output for downstream summarization processing. We used the tensor2tensor toolkit (Vaswani et al., 2018) for the transformer implementation and the fairseq toolkit (Ott et al., 2019) for the dynamic convolution model. We also used Moses (Koehn et al., 2007) for training the phrase-based model. Our own tokenizer was used instead of the tokenizer from Moses to match the tokenization scheme used by other system components. Subword tokenization was done using the sentencepiece toolkit (Kudo and Richardson, 2018), an unsupervised text tokenization method that is independent of the language being processed. 5. For instance, the expansions for the query EXAMPLE OF(footwear) include: “baby shoe”, “bowli"
H89-2033,H89-2027,1,0.875671,"Missing"
H89-2033,H89-2020,1,\N,Missing
H89-2033,H89-1016,0,\N,Missing
H89-2033,H89-1024,0,\N,Missing
H90-1022,H89-2019,1,\N,Missing
H91-1080,H90-1006,0,\N,Missing
H92-1012,H92-1003,0,0.0680289,"was minimal and most of the speech training data was read, with only a small amount of spontaneous training data. THE SESSION This session was devoted to presentations from the six sites that performed evaluations on the February 92 ATIS speech, natural language, and spoken language tests. These sites included AT&T, BBN, CMU, MIT, Paramax, and SRI. Since February 91, the ATIS database has been updated and, in an effort to quickly collect a larger amount of training and test data, a concerted effort has taken place in collecting data at five different sites (AT&T, BBN, CMU, MIT, and SRI). (See [1] for details.) About 10,000 spontaneous utterances were collected, of which about half were annotated (text transcriptions, reference answers, etc.) by December 20. Thus, for the first time since the decision to adopt ATIS as the common task for evaluation, the different sites had available to them sufficient amounts of The results show considerable performance improvements since a year ago. In speech recognition, much of the improvement in performance is attributable to the significant increase in the amount of appropriate t r a i n i n g data, w h i c h a l l o w e d the 65 development of be"
H92-1014,H91-1042,1,0.887996,"s optimal, and that the two-pass processing strategy was slightly better than either of the others. This was the configuration we used on the February '92 evaluation data. SPOKEN LANGUAGE UNDERSTANDING - HARC, BBN's spoken language system, utilizes BYBLOS as its speech recognition component, and DELPHI as its natural language understanding component. DELPHI uses a definite clause grammar formalism, augmented by the use of constraint nodes [9] and a labelled argument formalism [3]. The parsing algorithm uses a statistically trained agenda to produce the single best parse for an input utterance [1]. In Table 5 we show our Weighted Error on the February '92 evaluation data for Combined Class A+D, and Classes A and D separately, as calculated by NIST. During the test run, we had neglected to include the date information provided for individual scenarios. We include the results of a re-run with the same system as ran the February '92 test set, with We experimented with several conditions to optimize the connection of BYBLOS with DELPHI. The basic interface 76 Condition N WE Text FaUback on &quot; &quot; Fallback off &quot; &quot; (1) 1 5 20 1 5 20 47.9 64.6 58.0 60.1 64.2 56.9 59.0 5 56.6 Two Pass Finally, we"
H92-1014,H92-1061,1,0.872639,"Missing"
H92-1014,H92-1062,1,0.888034,"e October '91 dry-run corpus as development test in Table 4. The results of our experiments indicated that an N of 5 was optimal, and that the two-pass processing strategy was slightly better than either of the others. This was the configuration we used on the February '92 evaluation data. SPOKEN LANGUAGE UNDERSTANDING - HARC, BBN's spoken language system, utilizes BYBLOS as its speech recognition component, and DELPHI as its natural language understanding component. DELPHI uses a definite clause grammar formalism, augmented by the use of constraint nodes [9] and a labelled argument formalism [3]. The parsing algorithm uses a statistically trained agenda to produce the single best parse for an input utterance [1]. In Table 5 we show our Weighted Error on the February '92 evaluation data for Combined Class A+D, and Classes A and D separately, as calculated by NIST. During the test run, we had neglected to include the date information provided for individual scenarios. We include the results of a re-run with the same system as ran the February '92 test set, with We experimented with several conditions to optimize the connection of BYBLOS with DELPHI. The basic interface 76 Condition N W"
H92-1014,H89-2027,1,0.880499,"10411 spontaneous utterances from 286 subjects. The data originated from 5 collection sites using a variety of strategies for eliciting and capturing spontaneous queries from the subjects [7]. The training data was not balanced across the five sites, however. MIT was represented by 3-4 times as much data as any other site. Overall, MIT data accounted for nearly half of the ATIS2 subcorpus (4600 utterances). RECOGNITION The BYBLOS speech recognition system produces an ordered list of the N top-scoring hypotheses which is then reordered by several detailed knowledge sources. The Nbest strategy [4,8] permits the use of computationally prohibitive models by greatly reducing the search space to a few dozen word sequences. It has enabled us to use crossword-boundary triphone models and trigram language models with ease. The N-best list is also a robust interface between speech and natural language that provides a way to recover from speech errors in the top choice word sequence. The evaluation test data was drawn from this same pool of data so we decided to ignore the earlier batches of ATIS data that were collected under still different circumstances (most of it was read speech) and would n"
H92-1014,H91-1012,0,0.032865,"uage models with ease. The N-best list is also a robust interface between speech and natural language that provides a way to recover from speech errors in the top choice word sequence. The evaluation test data was drawn from this same pool of data so we decided to ignore the earlier batches of ATIS data that were collected under still different circumstances (most of it was read speech) and would not be represented in the new test (dialects were predominantly southern in the ATIS0 subcorpus). The overall system architecture for this evaluation is similar to that used in the February '91 tests [6]. Specifically, we use a 4-pass approach to produce the N-best lists for natural language processing. We filtered the training data for quality in several ways. All utterances that were marked as truncated in the SRO (speech recognition output) transcription were ignored. Similarly, we omitted from the training all utterances that contained a word fragment, We also ignored any utterances 1. A forward pass with a bigram grammar and discrete 72 that contained rare nonspeech events. Finally, our forwardbackward training program rejected any input that failed to align properly. These steps removed"
H92-1014,H92-1003,0,0.105162,"the N-best interface. Results are presented for speech recognition alone and for the overall spoken language system. A detailed discussion of DELPHI is presented in [2,3] elsewhere in these proceedings. 2. B Y B L O S - S P E E C H 2.1 Training and D e v e l o p m e n t Test Data We used speech data from the ATIS2 subcorpus exclusively to train the parameters of the acoustic model. This subcorpus consists of 10411 spontaneous utterances from 286 subjects. The data originated from 5 collection sites using a variety of strategies for eliciting and capturing spontaneous queries from the subjects [7]. The training data was not balanced across the five sites, however. MIT was represented by 3-4 times as much data as any other site. Overall, MIT data accounted for nearly half of the ATIS2 subcorpus (4600 utterances). RECOGNITION The BYBLOS speech recognition system produces an ordered list of the N top-scoring hypotheses which is then reordered by several detailed knowledge sources. The Nbest strategy [4,8] permits the use of computationally prohibitive models by greatly reducing the search space to a few dozen word sequences. It has enabled us to use crossword-boundary triphone models and"
H92-1014,H90-1003,1,0.903119,"10411 spontaneous utterances from 286 subjects. The data originated from 5 collection sites using a variety of strategies for eliciting and capturing spontaneous queries from the subjects [7]. The training data was not balanced across the five sites, however. MIT was represented by 3-4 times as much data as any other site. Overall, MIT data accounted for nearly half of the ATIS2 subcorpus (4600 utterances). RECOGNITION The BYBLOS speech recognition system produces an ordered list of the N top-scoring hypotheses which is then reordered by several detailed knowledge sources. The Nbest strategy [4,8] permits the use of computationally prohibitive models by greatly reducing the search space to a few dozen word sequences. It has enabled us to use crossword-boundary triphone models and trigram language models with ease. The N-best list is also a robust interface between speech and natural language that provides a way to recover from speech errors in the top choice word sequence. The evaluation test data was drawn from this same pool of data so we decided to ignore the earlier batches of ATIS data that were collected under still different circumstances (most of it was read speech) and would n"
H92-1014,H89-2006,1,0.902556,"s of one such experiment, utilizing the October '91 dry-run corpus as development test in Table 4. The results of our experiments indicated that an N of 5 was optimal, and that the two-pass processing strategy was slightly better than either of the others. This was the configuration we used on the February '92 evaluation data. SPOKEN LANGUAGE UNDERSTANDING - HARC, BBN's spoken language system, utilizes BYBLOS as its speech recognition component, and DELPHI as its natural language understanding component. DELPHI uses a definite clause grammar formalism, augmented by the use of constraint nodes [9] and a labelled argument formalism [3]. The parsing algorithm uses a statistically trained agenda to produce the single best parse for an input utterance [1]. In Table 5 we show our Weighted Error on the February '92 evaluation data for Combined Class A+D, and Classes A and D separately, as calculated by NIST. During the test run, we had neglected to include the date information provided for individual scenarios. We include the results of a re-run with the same system as ran the February '92 test set, with We experimented with several conditions to optimize the connection of BYBLOS with DELPHI"
H92-1049,H90-1016,1,0.886025,"ech Recognition Demonstrations Steve Austin, Rusty Bobrow, Dan Ellard, Robert Ingria, John Makhoul, Long Nguyen, Pat Peterson, Paul Placeway, Richard Schwartz BBN Systems and Technologies Cambridge MA 02138 Typically, real-time speech recognition - if achieved at all - is accomplished either by greatly simplifying the processing to be done, or by the use of special-purpose hardware. Each of these approaches has obvious problems. The former results in a substantial loss in accuracy, while the latter often results in obsolete hardware being developed at great expense and delay. Starting in 1990 [1] [2] we have taken a different approach based on modifying the algorithms to provide increased speed without loss in accuracy. Our goal has been to use commercially available off-the-shelf (COTS) hardware to perform speech recognition. Initially, this meant using workstations with powerful but standard signal processing boards acting as accelerators. However, even these signal processing boards have two significant disadvantages: 1. They often cost as much as the workstation they are plugged into. . The interface between each board and workstation is complicated, and always different for each"
H92-1049,H90-1003,1,0.810065,"Recognition Demonstrations Steve Austin, Rusty Bobrow, Dan Ellard, Robert Ingria, John Makhoul, Long Nguyen, Pat Peterson, Paul Placeway, Richard Schwartz BBN Systems and Technologies Cambridge MA 02138 Typically, real-time speech recognition - if achieved at all - is accomplished either by greatly simplifying the processing to be done, or by the use of special-purpose hardware. Each of these approaches has obvious problems. The former results in a substantial loss in accuracy, while the latter often results in obsolete hardware being developed at great expense and delay. Starting in 1990 [1] [2] we have taken a different approach based on modifying the algorithms to provide increased speed without loss in accuracy. Our goal has been to use commercially available off-the-shelf (COTS) hardware to perform speech recognition. Initially, this meant using workstations with powerful but standard signal processing boards acting as accelerators. However, even these signal processing boards have two significant disadvantages: 1. They often cost as much as the workstation they are plugged into. . The interface between each board and workstation is complicated, and always different for each comb"
H93-1015,H93-1003,0,0.0359805,"lgorithms that worked best on smaller vocabularies would not be the same ones that work best on larger vocabularies. We found that, while the required computation certainly increased, the programs that we had developed on the smaller problems still worked efficiently enough on the larger problems. However, while the BYBLOS system achieved the lowest word error rate obtained by any site for recognition of ATIS speech, the error rates for the WSJ tests were the second lowest of the six sites that tested their systems on this corpus. The reader will find more details on the evaluation results in [1]. In the sections that follow, we will describe the BBN BYBLOS system briefly. Then we enumerate several modifications to the BBN BYBLOS system. Following this we will describe four different experiments that we performed and the results obtained. 2. 3. Each of the N hypotheses is rescored with cross-wordboundary triphones and semi-continuous density HMMs. 4. The N-best list can be rescored with a trigram grammar (or any other language model). Each utterance is decoded with each gender-dependent model. For each utterance, the N-best list with the highest top-1 hypothesis score is chosen. The t"
H93-1015,H89-2027,1,0.863151,"ifications to the BBN BYBLOS system. Following this we will describe four different experiments that we performed and the results obtained. 2. 3. Each of the N hypotheses is rescored with cross-wordboundary triphones and semi-continuous density HMMs. 4. The N-best list can be rescored with a trigram grammar (or any other language model). Each utterance is decoded with each gender-dependent model. For each utterance, the N-best list with the highest top-1 hypothesis score is chosen. The top choice in the final list constitutes the speech recognition results reported below. This N-best strategy [3, 4] permits the use of otherwise computationally prohibitive models by greatly reducing the search space to a few (N=20-100) word sequences. It has enabled us to use cross-word-bonndary triphone models and trigram language models with ease. During most of the development of the system we used the 1000-Word RM cospus [8] for testing. More recently, the system has been used for recognizing spontaneous speech from the ATIS corpus, which contains many spontaneous speech effects, such as partial words, nonspeech sounds, extraneous.noises, false starts, etc. The vocabulary of the ATIS domain was about"
H93-1015,H93-1014,0,\N,Missing
H94-1065,H93-1015,1,0.796216,"age, a rather expensive approach for real applications, or by training on a large number of microphones in the hope that the system will obtain the necessary robustness. • Use robust signal processing algorithms. • • Develop a feature transformation that maps the alternate microphone data to training microphone data. • Use statistical methods in order to adapt the parameters of the acoustic models. In previous work we had discussed the use of Cepstmm Mean Subtraction and the RASTA algorithm as two simple signal processing algorithms to compensate the degradation caused by an alternate channel [7]. In this pape r, we present an approach towards feature mapping by modeling the difference between the test and the training microphone, prior to recotion. We have developed the Tied-Mixture Normalization Algorithm, a technique for adaptation to a new microphone based on modifying the continuous densities in a tied-mixture I-IMM system, using a relatively small amount of stereo training speech. This method is presented in detail in Section 2. In Section 3 we describe several experiments on a known microphone task and the effect of the adaptation method in the performance of the recognition sy"
H94-1086,P93-1008,0,0.0328063,"tters (bilets) are modeled as well as three letter (trilet) contexts. In a given set of sentences there may be many tfilets, up to the number of symbols cubed. However, in English only a subset of these are allowed. In the ATIS task there are 3639 different trilets in the training sentences. LF_~__~R~ognition__~ Mat Likely Speech _[ F u t u r e Input r I Extraction ] Vectea's I Search Senten~ Figure 1: BYBLOS speech system. 3. A I R L I N E T R A V E L I N F O R M A T I O N SERVICE: AN INITIAL 3050 WORD, 52 SYMBOL TASK In the initial system, the BBN BYBLOS Continuous Speech Recognition system [4, 5, 6] (see Figure I) was used without modification on an on-line cursive handwriting corpus created from prompts from the ARPA Airline Travel Information Service (ATIS) corpus [7]. These full sentence prompts (approximately 10 words per sentence) were written by a single subject. These sentences were then reviewed (verified) to make sure that the prompts were transcribed correctly. After verification, these sentences were separated into a set of 381 training sentences and a mutually exclusive set of 94 test sentences. The lexicon for this task consisted of 3050 words, where lowercase and capitalize"
H94-1086,H92-1003,0,0.0252218,"only a subset of these are allowed. In the ATIS task there are 3639 different trilets in the training sentences. LF_~__~R~ognition__~ Mat Likely Speech _[ F u t u r e Input r I Extraction ] Vectea's I Search Senten~ Figure 1: BYBLOS speech system. 3. A I R L I N E T R A V E L I N F O R M A T I O N SERVICE: AN INITIAL 3050 WORD, 52 SYMBOL TASK In the initial system, the BBN BYBLOS Continuous Speech Recognition system [4, 5, 6] (see Figure I) was used without modification on an on-line cursive handwriting corpus created from prompts from the ARPA Airline Travel Information Service (ATIS) corpus [7]. These full sentence prompts (approximately 10 words per sentence) were written by a single subject. These sentences were then reviewed (verified) to make sure that the prompts were transcribed correctly. After verification, these sentences were separated into a set of 381 training sentences and a mutually exclusive set of 94 test sentences. The lexicon for this task consisted of 3050 words, where lowercase and capitalized versions of a word are considered distinct. For this initial system there were 54 characters: 52 lower and upper case alphabetic, a space character, and a ""backspace"" chara"
H94-1086,H92-1073,0,0.0416167,"tion was suspended at this point since so few errors did not allow any further analysis of the problems in our methods. The above experiments demonstrated the potential utility of speech recognition methods, especially the use of HMMs and grammars, to the problem of on-line cursive handwriting recognition. Based on these good preliminary results, we embarked on a more ambitious task with a larger vocabulary and more writers. 4. WALL STREET JOURNAL: A 25,000 WORD, 86 SYMBOL TASK During the past year, we have collected cursive written data using text from the ARPA Wall Street Journal task (WSJ) [10], including numerals, punctuation, and other symbols, for a total of 88 symbols (62 alphanumeric, 24 punctuation and special symbols, space, and backspace). The prompts from the Wall Street Journal consist mainly of full sentences with scattered article headings and stock listings (all are referred to as sentences for convenience). We have thus far collected over 7000 sentences (175,000 words total or about 25 words/sentence) from 21 writers on two GRiD Convertible pentops. See Figure 5 for an example of the data collected. The writers were gathered from the Cambridge, Massachusetts area and w"
N12-1006,W05-0909,0,0.0349848,"Missing"
N12-1006,W10-0701,1,0.675469,"Missing"
N12-1006,E06-1047,0,0.780602,"Missing"
N12-1006,N09-1025,0,0.0340312,"tuning, and we only varied the corpora used for training, tuning and testing. The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al. (2008). We used GIZA++ (Och and Ney, 2003) to align sentences and extract hierarchical rules. The decoder used a log-linear model that combines the scores of multiple feature scores, including translation probabilities, smoothed lexical probabilities, a dependency tree language model, in addition to a trigram English language model. Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the Gigaword and from a web crawl. The feature weights were tuned to maximize the BLEU score on a tuning set using the ExpectedBLEU optimization procedure (Devlin, 2009). The Dialectal Arabic side of our corpus consisted of 1.5M words (1.1M Levantine and 380k Egyptian). Table 2 gives statistics about the various train/tune/test splits we used in our experiments. Since the Egyptian set was so small, we split it only to training/test sets, opting not to have a tuning set. The MSA training data we used consisted of ArabicEnglish cor"
N12-1006,P05-1071,0,0.186427,"e, 2004; Habash and Sadat, 2006) by decreasing the size of the source vocabulary, and improving the quality of word alignments. The morphological analyzers that underlie most segmenters were developed for MSA, but the different dialects of Arabic share many of the morphological affixes of MSA, and it is therefore not unreasonable to expect MSA segmentation to also improve Dialect Arabic to English MT. To test this, 3 We also computed TER (Snover et al., 2006) and METEOR scores, but omit them because they demonstrated similar trends. 53 we ran experiments using the MADA morphological analyzer (Habash and Rambow, 2005). Table 3 shows the effect of applying segmentation to the text, for both MSA and Dialectal Arabic. The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006). Morphological segmentation gives a smaller gain on dialectal input, which could be due to two factors: the segmentation accuracy likely decreases since we are using an unmodified MSA segmenter, and there is higher variability in the written form of dialect compared to MSA. Given the significant, albeit smaller gain on dialectal inpu"
N12-1006,P06-1086,0,0.552091,"Missing"
N12-1006,N06-2013,0,0.0454305,"achieved using the full set of dialectal data (which combines Levantine and Egyptian), since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the dialectal data results in marginally worse translations. score.3 In addition, we also report the OOV rate of the test set relative to the training corpus in each experimental setups. 4.1 Morphological Decomposition Arabic has a complex morphology compared to English. Preprocessing the Arabic source by morphological segmentation has been shown to improve the performance of Arabic MT (Lee, 2004; Habash and Sadat, 2006) by decreasing the size of the source vocabulary, and improving the quality of word alignments. The morphological analyzers that underlie most segmenters were developed for MSA, but the different dialects of Arabic share many of the morphological affixes of MSA, and it is therefore not unreasonable to expect MSA segmentation to also improve Dialect Arabic to English MT. To test this, 3 We also computed TER (Snover et al., 2006) and METEOR scores, but omit them because they demonstrated similar trends. 53 we ran experiments using the MADA morphological analyzer (Habash and Rambow, 2005). Table"
N12-1006,N04-4015,0,0.0397882,"scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian), since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the dialectal data results in marginally worse translations. score.3 In addition, we also report the OOV rate of the test set relative to the training corpus in each experimental setups. 4.1 Morphological Decomposition Arabic has a complex morphology compared to English. Preprocessing the Arabic source by morphological segmentation has been shown to improve the performance of Arabic MT (Lee, 2004; Habash and Sadat, 2006) by decreasing the size of the source vocabulary, and improving the quality of word alignments. The morphological analyzers that underlie most segmenters were developed for MSA, but the different dialects of Arabic share many of the morphological affixes of MSA, and it is therefore not unreasonable to expect MSA segmentation to also improve Dialect Arabic to English MT. To test this, 3 We also computed TER (Snover et al., 2006) and METEOR scores, but omit them because they demonstrated similar trends. 53 we ran experiments using the MADA morphological analyzer (Habash"
N12-1006,J03-1002,0,0.00335263,"tal cost was $44k, or $0.03/word – an order of magnitude cheaper than professional translation. 4 Experiments in Dialectal Arabic-English Machine Translation We performed a set of experiments to contrast systems trained using our dialectal parallel corpus with systems trained on a (much larger) MSA-English parallel corpus. All experiments use the same methods for training, decoding and parameter tuning, and we only varied the corpora used for training, tuning and testing. The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al. (2008). We used GIZA++ (Och and Ney, 2003) to align sentences and extract hierarchical rules. The decoder used a log-linear model that combines the scores of multiple feature scores, including translation probabilities, smoothed lexical probabilities, a dependency tree language model, in addition to a trigram English language model. Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the Gigaword and from a web crawl. The feature weights were tuned to maximize the BLEU score on a tuning set using the ExpectedBLEU opt"
N12-1006,P02-1040,0,0.104675,"Missing"
N12-1006,2006.amta-papers.21,0,0.538693,"Missing"
N12-1006,W11-2602,0,0.322886,"le 6). 4.5 Mapping from Dialectal Arabic to MSA Before Translating to English Given the large amount of linguistic resources that have been developed for MSA over the past years, and the extensive research that was conducted on machine translation from MSA to English and other languages, an obvious research question is whether Dialectal Arabic is best translated to English by first pivoting through MSA, rather than directly. The proximity of Dialectal Arabic to MSA makes the mapping in principle easier than general machine translation, and a number of researchers have explored this direction (Salloum and Habash, 2011). In this scenario, the dialectal source would first be automatically transformed to MSA, using either a rule-based or statistical mapping module. The Dialectal Arabic-English parallel corpus we created presents a unique opportunity to compare the MSA-pivoting approach against direct translation. First, we collected equivalent MSA data for the Levantine Web test and tuning sets, by asking Turkers to transform dialectal passages to valid and fluent MSA. Turkers were shown example transformations, and we encouraged fewer changes where applicable (e.g. morphological rather than lexical mapping),"
N12-1006,2010.amta-papers.5,0,0.238646,"Missing"
N12-1006,P08-1066,0,0.0165598,"creating our parallel corpus. The total cost was $44k, or $0.03/word – an order of magnitude cheaper than professional translation. 4 Experiments in Dialectal Arabic-English Machine Translation We performed a set of experiments to contrast systems trained using our dialectal parallel corpus with systems trained on a (much larger) MSA-English parallel corpus. All experiments use the same methods for training, decoding and parameter tuning, and we only varied the corpora used for training, tuning and testing. The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al. (2008). We used GIZA++ (Och and Ney, 2003) to align sentences and extract hierarchical rules. The decoder used a log-linear model that combines the scores of multiple feature scores, including translation probabilities, smoothed lexical probabilities, a dependency tree language model, in addition to a trigram English language model. Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the Gigaword and from a web crawl. The feature weights were tuned to maximize the BLEU score on a t"
N12-1006,2006.amta-papers.25,1,0.697297,"plex morphology compared to English. Preprocessing the Arabic source by morphological segmentation has been shown to improve the performance of Arabic MT (Lee, 2004; Habash and Sadat, 2006) by decreasing the size of the source vocabulary, and improving the quality of word alignments. The morphological analyzers that underlie most segmenters were developed for MSA, but the different dialects of Arabic share many of the morphological affixes of MSA, and it is therefore not unreasonable to expect MSA segmentation to also improve Dialect Arabic to English MT. To test this, 3 We also computed TER (Snover et al., 2006) and METEOR scores, but omit them because they demonstrated similar trends. 53 we ran experiments using the MADA morphological analyzer (Habash and Rambow, 2005). Table 3 shows the effect of applying segmentation to the text, for both MSA and Dialectal Arabic. The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006). Morphological segmentation gives a smaller gain on dialectal input, which could be due to two factors: the segmentation accuracy likely decreases since we are using an unmod"
N12-1006,P11-2007,1,0.80814,"nizing Arabic text with a focus on Dialectal Arabic. For example, MAGEAD (Habash and Rambow, 2006) is a morphological analyzer and generator that can analyze the surface form of MSA and dialect words into 50 their root/pattern and affixed morphemes, or generate the surface form in the opposite direction. Amazon’s Mechanical Turk (MTurk) is becoming an essential tool for creating annotated resources for computational linguistics. Callison-Burch and Dredze (2010) provide an overview of various tasks for which MTurk has been used, and offer a set of best practices for ensuring high-quality data. Zaidan and Callison-Burch (2011a) studied the quality of crowdsourced translations, by quantifying the quality of non-professional English translations of 2,000 Urdu sentences that were originally translated by the LDC. They demonstrated a variety of mechanisms that increase the translation quality of crowdsourced translations to near professional levels, with a total cost that is less than one tenth the cost of professional translation. Zaidan and Callison-Burch (2011b) created the Arabic Online Commentary (AOC) dataset, a 52Mword monolingual dataset rich in dialectal content. Over 100k sentences from the AOC were annotate"
N12-1006,P11-1122,1,0.553138,"Missing"
N13-1069,W10-0701,0,0.261316,"Missing"
N13-1069,N09-1025,0,0.0511387,"Missing"
N13-1069,N03-1017,0,0.0201457,"Missing"
N13-1069,W07-0716,0,0.0671596,"ced translations for training or parameter tuning. Zbib et al. (2012) trained a Dialectal Arabic to English MT system using Mechanical Turk translations. But the data they translated on MTurk does not have professional translations to conduct the systematic comparison we do in this paper. It is well known that scoring MT output against multiple references improves MT scores such as BLEU significantly, since it increases the chance of matching n-grams between the MT output and the references. Tuning system parameter with multiple references also improves machine translation for the same reason Madnani et al. (2007) and Madnani et al. (2008) showed that tuning on additional references obtained by automatic paraphrasing helps when only few tuning references are available. 3 Data Translation The data we used are Arabic-English parallel corpora released by the LDC for the DARPA BOLT Phase 1 program1 . The data was collected from Egyptian online discussion forums, and consists of separate discussion threads, each composed of an initial user posting and multiple reply postings. The data tends to be bimodal: the first posting in the thread is often formal and expressed in Modern Standard Arabic, while the subs"
N13-1069,2008.amta-papers.13,1,0.799125,"ing or parameter tuning. Zbib et al. (2012) trained a Dialectal Arabic to English MT system using Mechanical Turk translations. But the data they translated on MTurk does not have professional translations to conduct the systematic comparison we do in this paper. It is well known that scoring MT output against multiple references improves MT scores such as BLEU significantly, since it increases the chance of matching n-grams between the MT output and the references. Tuning system parameter with multiple references also improves machine translation for the same reason Madnani et al. (2007) and Madnani et al. (2008) showed that tuning on additional references obtained by automatic paraphrasing helps when only few tuning references are available. 3 Data Translation The data we used are Arabic-English parallel corpora released by the LDC for the DARPA BOLT Phase 1 program1 . The data was collected from Egyptian online discussion forums, and consists of separate discussion threads, each composed of an initial user posting and multiple reply postings. The data tends to be bimodal: the first posting in the thread is often formal and expressed in Modern Standard Arabic, while the subsequent threads use a less"
N13-1069,J03-1002,0,0.0109205,"Missing"
N13-1069,P02-1040,0,0.0993694,"Web-forum 0KW 100KW 200KW 400KW 22.82 24.05 24.85 25.19 22.82 23.79 24.20 24.51 22.82 24.26 25.19 25.38 22.82 24.31 25.16 Table 1: Comparison of the effect of web forum training data when using professional and MTurk reference translations. All results use professional references for the tuning and test sets. two versions of each set: one with the professional reference translations for the target, and the other with the same source data, but the MTurk translations. We defined two versions of the test and tuning sets similarly. We report translation results in terms of lower-case BLEU scores (Papineni et al., 2002). 4.1 Training Data References We first study the effect of training data references, varying the amount of training data and type of translations, while using the same professional translation references for tuning and scoring. The first set of baseline experiments were trained on web forum data only, using professional translations. The first line of Table 1 shows that doubling of the training data adds 2.5 then 2.3 BLEU points. We repeated the experiments, but with MTurk training references, and saw that the scores are lower by 1.32.5 BLEU points, depending on the size of training data, and"
N13-1069,P08-1066,0,0.0448893,"Missing"
N13-1069,P11-2007,0,0.0144577,"ference translations of four Arabic-English parallel corpora previously released by the Linguistic Data Consortium (LDC) • A second translation of the development set obtained via MTurk improves parameter tuning and output evaluation. 2 Previous Work There have been several publications on crowdsourcing data annotation for NLP. Callison-Burch and Dredze (2010) give an overview of the NAACL2010 Workshop on using Mechanical Turk for data annotation. They describe tasks for which MTurk can be used, and summarize a set of best practices. They also include references to the workshop contributions. Zaidan and Callison-Burch (2011a) created a monolingual Arabic data set rich in dialectal content from user commentaries on newspaper websites. They hired native Arabic speakers on MTurk 612 Proceedings of NAACL-HLT 2013, pages 612–616, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics to identify the dialect level and used the collected labels to train automatic dialect identification systems. They did not translate the collected data, however. Zaidan and Callison-Burch (2011b) obtained multiple translations of the NIST 2009 Urdu-English evaluation set using MTurk. They trained a statistica"
N13-1069,P11-1122,0,0.180064,"Missing"
N13-1069,N12-1006,1,0.834416,"Zaidan and Callison-Burch (2011b) obtained multiple translations of the NIST 2009 Urdu-English evaluation set using MTurk. They trained a statistical model on a set of features to select among the multiple translations. They showed that the MTurk translations selected by their model approached the range of quality of professional translations, and that the selected MTurk translations can be used reliably to score the outputs of different MT systems submitted to the NIST evaluation. Unlike our work, they did not investigate the use of crowdsourced translations for training or parameter tuning. Zbib et al. (2012) trained a Dialectal Arabic to English MT system using Mechanical Turk translations. But the data they translated on MTurk does not have professional translations to conduct the systematic comparison we do in this paper. It is well known that scoring MT output against multiple references improves MT scores such as BLEU significantly, since it increases the chance of matching n-grams between the MT output and the references. Tuning system parameter with multiple references also improves machine translation for the same reason Madnani et al. (2007) and Madnani et al. (2008) showed that tuning on"
P14-1129,D13-1106,0,0.772389,"Missing"
P14-1129,N09-1025,0,0.363403,"Missing"
P14-1129,J07-2003,0,0.0252724,"ior of our baseline features, as we show in the next section. 6.2 “Simple Hierarchical” NIST Results The baseline used in the last section is a highlyengineered research system, which uses a wide array of features that were refined over a number of years, and some of which require linguistic resources. Because of this, the baseline BLEU scores are much higher than a typical MT system – especially a real-time, production engine which must support many language pairs. Therefore, we also present results using a simpler version of our decoder which emulates Chiang’s original Hiero implementation (Chiang, 2007). Specifically, this means that we don’t use dependency-based rule extraction, and our decoder only contains the following MT features: (1) rule probabilities, (2) n-gram Kneser-Ney LM, (3) lexical smoothing, (4) target word count, (5) concat rule penalty. Results are shown in the third section of Table 3. The “Simple Hierarchical” Arabic-English system is -6.4 BLEU worse than our strong baseline, and would have ranked 10th place out of 11 systems in the evaluation. When the NNJM features are added to this system, we see an improvement of +6.3 BLEU, which would have ranked 1st place in the eva"
P14-1129,N12-1059,1,0.664043,"Missing"
P14-1129,N13-1044,0,0.0372017,"esc) 52.2 + T2S/L2R NNJM (Resc) 52.3 + T2S/R2L NNJM (Resc) 52.8 “Simple Hier.” Baseline 43.4 + S2T/L2R NNJM (Dec) 47.2 + S2T NNLTM (Dec) 48.5 + Other NNJMs (Resc) 49.7 We also perform 1000-best rescoring with the following features: • 5-gram Kneser-Ney LM • Recurrent neural network language model (RNNLM) (Mikolov et al., 2010) Although we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM. 5.2 Training and Optimization For Arabic word tokenization, we use the MADAARZ tokenizer (Habash et al., 2013) for the BOLT condition, and the Sakhr9 tokenizer for the NIST condition. For Chinese tokenization, we use a simple longest-match-first lexicon-based approach. For word alignment, we align all of the training data with both GIZA++ (Och and Ney, 2003) and NILE (Riesa et al., 2011), and concatenate the corpora together for rule extraction. For MT feature weight optimization, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010). 6 Experimental Results We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BO"
P14-1129,D13-1053,1,0.625933,"Missing"
P14-1129,D13-1176,0,0.682331,"Missing"
P14-1129,N12-1005,0,0.506559,"ls were primarily used to create n-gram neural network language models (NNLMs) for speech recognition and machine translation (Bengio et al., 2003; Schwenk, 2010). They have since been extended to translation modeling, parsing, and many other NLP tasks. In this paper we use a basic neural network architecture and a lexicalized probability model to create a powerful MT decoding feature. Specifically, we introduce a novel formulation for a neural network joint model (NNJM), which augments an n-gram target language model with an m-word source window. Unlike previous approaches to joint modeling (Le et al., 2012), our feature can be easily integrated into any statistical machine translation (SMT) decoder, which leads to substantially larger improvements than k-best rescoring only. Additionally, we present several variations of this model which provide significant additive BLEU gains. We also present a novel technique for training the neural network to be self-normalized, which avoids the costly step of posteriorizing over the entire vocabulary in decoding. When used in conjunction with a pre-computed hidden layer, these techniques speed up NNJM computation by a factor of 10,000x, with only a small red"
P14-1129,J06-4004,0,0.516283,"Missing"
P14-1129,N13-1090,0,0.0262738,"Missing"
P14-1129,J03-1002,0,0.0112043,"LM • Recurrent neural network language model (RNNLM) (Mikolov et al., 2010) Although we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM. 5.2 Training and Optimization For Arabic word tokenization, we use the MADAARZ tokenizer (Habash et al., 2013) for the BOLT condition, and the Sakhr9 tokenizer for the NIST condition. For Chinese tokenization, we use a simple longest-match-first lexicon-based approach. For word alignment, we align all of the training data with both GIZA++ (Och and Ney, 2003) and NILE (Riesa et al., 2011), and concatenate the corpora together for rule extraction. For MT feature weight optimization, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010). 6 Experimental Results We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BOLT conditions. We also present a set of auxiliary results in order to further analyze our features. 6.1 9 Table 3: Primary results on Arabic-English and Chinese-English NIST MT12 Test Set. The first section corresponds to the top and bottom ranked sy"
P14-1129,D11-1046,0,0.0133144,"k language model (RNNLM) (Mikolov et al., 2010) Although we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM. 5.2 Training and Optimization For Arabic word tokenization, we use the MADAARZ tokenizer (Habash et al., 2013) for the BOLT condition, and the Sakhr9 tokenizer for the NIST condition. For Chinese tokenization, we use a simple longest-match-first lexicon-based approach. For word alignment, we align all of the training data with both GIZA++ (Och and Ney, 2003) and NILE (Riesa et al., 2011), and concatenate the corpora together for rule extraction. For MT feature weight optimization, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010). 6 Experimental Results We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BOLT conditions. We also present a set of auxiliary results in order to further analyze our features. 6.1 9 Table 3: Primary results on Arabic-English and Chinese-English NIST MT12 Test Set. The first section corresponds to the top and bottom ranked systems from the evaluation, and"
P14-1129,W10-1748,1,0.480231,"the highest overlap with our NNJM. 5.2 Training and Optimization For Arabic word tokenization, we use the MADAARZ tokenizer (Habash et al., 2013) for the BOLT condition, and the Sakhr9 tokenizer for the NIST condition. For Chinese tokenization, we use a simple longest-match-first lexicon-based approach. For word alignment, we align all of the training data with both GIZA++ (Och and Ney, 2003) and NILE (Riesa et al., 2011), and concatenate the corpora together for rule extraction. For MT feature weight optimization, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010). 6 Experimental Results We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BOLT conditions. We also present a set of auxiliary results in order to further analyze our features. 6.1 9 Table 3: Primary results on Arabic-English and Chinese-English NIST MT12 Test Set. The first section corresponds to the top and bottom ranked systems from the evaluation, and are taken from the NIST website. The second section corresponds to results on top of our strongest baseline. The third section corresponds to results on top of a simpler baseline. Within each"
P14-1129,W10-3815,0,0.0589328,"T program (Approved for Public Release, Distribution Unlimited). The views, opinions, and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense. Introduction In recent years, neural network models have become increasingly popular in NLP. Initially, these models were primarily used to create n-gram neural network language models (NNLMs) for speech recognition and machine translation (Bengio et al., 2003; Schwenk, 2010). They have since been extended to translation modeling, parsing, and many other NLP tasks. In this paper we use a basic neural network architecture and a lexicalized probability model to create a powerful MT decoding feature. Specifically, we introduce a novel formulation for a neural network joint model (NNJM), which augments an n-gram target language model with an m-word source window. Unlike previous approaches to joint modeling (Le et al., 2012), our feature can be easily integrated into any statistical machine translation (SMT) decoder, which leads to substantially larger improvements th"
P14-1129,C12-2104,0,0.367884,"Missing"
P14-1129,J10-4005,0,0.0126491,"as a single concatenated token. Formally, the probability model is: |S| Πi=1 P (tsi |si , si−1 , si+1 , · · · ) This model is trained and evaluated like our NNJM. It is easy and computationally inexpensive to use this model in decoding, since only one neural network computation must be made for each source word. In rescoring, we also use a T2S NNLTM model computed over every target word: |T | Πi=1 P (sti |ti , ti−1 , ti+1 , · · · ) 5 MT System In this section, we describe the MT system used in our experiments. 5.1 MT Decoder We use a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). Our baseline decoder contains a large and powerful set of features, which include: T2S/L2R , ta0i , ta0i −1 , ta0i +1 , · · · ) |S| Πi=1 P (si |si−1 , si−2 , · · · T2S/R2L |S| Πi=1 P (si |si+1 , si+2 , · · · , ta0i , ta0i −1 , ta0i +1 , · · · ) where a0i is the target-to-source affiliation, defined analogously to ai . The T2S variations cannot be used in decoding due to the large target context required, and are thus only used in k-best rescoring. The S2T/R2L 1374 • • • • • • • • Forward and backward rule probabilities 4-gram Kneser-Ney LM Dependency LM (Shen et al., 2010) Contextual lexical"
P14-1129,D08-1090,1,0.567711,"Missing"
P14-1129,D13-1023,0,0.00907149,"e 2: Speed of the neural network computation on a single CPU thread. “lookups/sec” is the number of unique n-gram probabilities that can be computed per second. “sec/word” is the amortized cost of unique NNJM lookups in decoding, per source word. Table 2 shows the speed of self-normalization and pre-computation for the NNJM. The decoding cost is based on a measurement of ∼1200 unique NNJM lookups per source word for our ArabicEnglish system.8 By combining self-normalization and precomputation, we can achieve a speed of 1.4M lookups/second, which is on par with fast backoff LM implementations (Tanaka et al., 2013). We demonstrate in Section 6.6 that using the selfnormalized/pre-computed NNJM results in only a very small BLEU degradation compared to the standard NNJM. 3 Decoding with the NNJM Because our NNJM is fundamentally an n-gram NNLM with additional source context, it can easily be integrated into any SMT decoder. In this section, we describe the considerations that must be taken when integrating the NNJM into a hierarchical decoder. 6 tanh() is implemented using a lookup table. 3500 ≈ 5 × 512 + 2 × 513; 2.8M ≈ 2 × 2689 × 512 + 2 × 513; 35M ≈ 2 × 2689 × 512 + 2 × 513 × 32000. For the sake of a fa"
P14-1129,D13-1140,0,0.825958,"r training objective function: X  L = log(P (xi )) − α(log(Z(xi )) − 0)2 i X  = log(P (xi )) − α log2 (Z(xi )) i In this case, the output layer bias weights are initialized to log(1/|V |), so that the initial network is self-normalized. At decode time, we simply use Ur (x) as the feature score, rather than log(P (x)). For our NNJM architecture, selfnormalization increases the lookup speed during decoding by a factor of ∼15x. Table 1 shows the neural network training results with various values of the free parameter α. In all subsequent MT experiments, we use α = 10−1 . We should note that Vaswani et al. (2013) implements a method called Noise Contrastive Estimation (NCE) that is also used to train selfnormalized NNLMs. Although NCE results in faster training time, it has the downside that there 1372 α 0 10−2 10−1 1 Arabic BOLT Val log(P (x)) |log(Z(x))| −1.82 5.02 −1.81 1.35 −1.83 0.68 −1.91 0.28 Table 1: Comparison of neural network likelihood for various α values. log(P (x)) is the average log-likelihood on a held-out set. |log(Z(x)) |is the mean error in log-likelihood when using Ur (x) directly instead of the true softmax probability log(P (x)). Note that α = 0 is equivalent to the standard neu"
P14-1129,D13-1141,0,0.248696,"Missing"
P15-1004,D13-1106,0,0.147527,"ntly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features. 1 Introduction Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated ∗ * Research conducted when the author was at BBN. 31 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International J"
P15-1004,J93-2003,0,0.0288832,"0.8 31.5 31.3 31.5 31.9 31.8 32.0 Table 3: Experimental results to investigate the effects of the new features, DTN and MTL. The top part shows the BOLT results, while the bottom part shows the NIST results. The best results for each conditions and each language-pair are in bold. The baselines are in italics. . AR-EN mixed-case ZH-EN mixed-case Base. 53.7 51.8 36.6 34.4 Feat 55.4 53.1 37.8 35.5 Tensor 55.9 53.7 38.2 35.9 MTL 56.4 54.1 38.5 36.1 The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the sour"
P15-1004,D07-1007,0,0.0235059,"ST results. The best results for each conditions and each language-pair are in bold. The baselines are in italics. . AR-EN mixed-case ZH-EN mixed-case Base. 53.7 51.8 36.6 34.4 Feat 55.4 53.1 37.8 35.5 Tensor 55.9 53.7 38.2 35.9 MTL 56.4 54.1 38.5 36.1 The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name"
P15-1004,P14-1062,0,0.0563792,"ture and model learning. 38 task learning to integrate the SMT features more tightly. Empirically, all our proposals successfully produce an improvement over state-of-the-art machine translation system for Arabic-to-English and Chinese-to-English and for both BOLT web forum and NIST conditions. Building on the success of this paper, we plan to develop other neuralnetwork-based features, and to also relax the limiteation of current rule extraction heuristics by generating translations word-by-word. works, other network architectures that have been applied to SMT include convolutional networks (Kalchbrenner et al., 2014) and recursive networks (Socher et al., 2011). The simplicity of feedforward networks works to our advantage. More specifically, due to the absence of a feedback loop, the feedforward architecture allows us to treat individual decisions independently, which makes parallelization of the training easy and the querying the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like collocation, which has been long recognized as important information for many NLP tasks (e.g. word"
P15-1004,N09-1025,0,0.017756,"or the DARPA BOLT Web Forum and the NIST OpenMT12 conditions. 5.1 Baseline MT System We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). The baseline we use includes a set of powerful features as follow: • Forward and backward rule probabilities • Contextual lexical smoothing (Devlin, 2009) • 5-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model 1 In this and in the other parts of the paper, we add the normalization regularization term described in (Devlin et al., 2014) to the loss function to avoid computing the normalization constant at model query/decoding time. 35 around 0.9 BLEU point improvement in AR-EN and around 0.3 BLEU in ZH-EN compared to the baseline. The JMO consistently provides better improvement compared to a larger JM context (row S2 ), validating our hypothesis that using offset source context captures important non-local context. Rows"
P15-1004,N12-1005,0,0.150227,"roposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features. 1 Introduction Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated ∗ * Research conducted when the author was at BBN. 31 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference o"
P15-1004,N12-1059,1,0.860591,"xperiments We demonstrate the impact of our work with extensive MT experiments on Arabic-English and Chinese-English translation for the DARPA BOLT Web Forum and the NIST OpenMT12 conditions. 5.1 Baseline MT System We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). The baseline we use includes a set of powerful features as follow: • Forward and backward rule probabilities • Contextual lexical smoothing (Devlin, 2009) • 5-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model 1 In this and in the other parts of the paper, we add the normalization regularization term described in (Devlin et al., 2014) to the loss function to avoid computing the normalization constant at model query/decoding time. 35 around 0.9 BLEU point improvement in AR-EN and around 0.3 BLEU in ZH-EN compared to the baseline. The JMO consistently provides better improvement compared to a larger"
P15-1004,W02-1006,0,0.0792732,"(Socher et al., 2011). The simplicity of feedforward networks works to our advantage. More specifically, due to the absence of a feedback loop, the feedforward architecture allows us to treat individual decisions independently, which makes parallelization of the training easy and the querying the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like collocation, which has been long recognized as important information for many NLP tasks (e.g. word sense disambiguation (Lee and Ng, 2002)). The tensor formulation we use is similar to that of (Yu et al., 2012; Hutchinson et al., 2013). Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014). To our knowledge, our work is the first to use tensor networks in SMT. Our approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works, Finkel and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al. (2013) train models for"
P15-1004,P14-1129,1,0.74947,"Missing"
P15-1004,J03-1002,0,0.00979299,"Missing"
P15-1004,P14-1028,0,0.0536156,"allelization of the training easy and the querying the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like collocation, which has been long recognized as important information for many NLP tasks (e.g. word sense disambiguation (Lee and Ng, 2002)). The tensor formulation we use is similar to that of (Yu et al., 2012; Hutchinson et al., 2013). Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014). To our knowledge, our work is the first to use tensor networks in SMT. Our approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works, Finkel and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al. (2013) train models for coreference resolution, named entity recognition and relation extraction jointly. Both efforts are motivated by the minimization of cascading errors. Our work is most closely related to Collobert and Weston (2008; Collobert et al. (2011), who"
P15-1004,N09-1037,0,0.0705049,"Missing"
P15-1004,W10-1748,1,0.831052,"LEU points in ZH-EN. (NNLTM) (Devlin et al., 2014) As shown, our baseline system already includes neural network-based features. NNJM, NNLTM and use two hidden layers with 500 units and use embedding of size 200 for each input. We use the MADA-ARZ tokenizer (Habash et al., 2013) for Arabic word tokenization. For Chinese tokenization, we use a simple longest-matchfirst lexicon-based approach. We align the training data using GIZA++ (Och and Ney, 2003). For tuning the weights of MT features including the new features, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010), and decode the test sets after 5 tuning iteration. We report the lower-cased BLEU and TER scores. 5.2 BOLT Discussion Forum The bulk of our experiments is on the BOLT Web Discussion Forum domain, which uses data collected by the LDC. The parallel training data consists of all of the high-quality NIST training corpora, plus an additional 3 million words of translated forum data. The tuning and test sets consist of roughly 5000 segments each, with 2 independent references for Arabic and 3 for Chinese. 5.2.1 Effects of New Features We first look at the effects of the proposed features compared"
P15-1004,W10-3815,0,0.0299312,"53.7 51.8 36.6 34.4 Feat 55.4 53.1 37.8 35.5 Tensor 55.9 53.7 38.2 35.9 MTL 56.4 54.1 38.5 36.1 The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sun"
P15-1004,N13-1044,0,0.0222499,"features and the set of target-enumerating features produce around 1.1 to 1.2 points BLEU gain in AR-EN and 0.3 to 0.5 points BLEU gain in ZHEN. The combination of the two sets produces a complementary gain in addition to the gains of the individual models as Row (S10 ) shows. The combined gain improves to 1.5 BLEU points in AREN and 0.7 BLEU points in ZH-EN. (NNLTM) (Devlin et al., 2014) As shown, our baseline system already includes neural network-based features. NNJM, NNLTM and use two hidden layers with 500 units and use embedding of size 200 for each input. We use the MADA-ARZ tokenizer (Habash et al., 2013) for Arabic word tokenization. For Chinese tokenization, we use a simple longest-matchfirst lexicon-based approach. We align the training data using GIZA++ (Och and Ney, 2003). For tuning the weights of MT features including the new features, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010), and decode the test sets after 5 tuning iteration. We report the lower-cased BLEU and TER scores. 5.2 BOLT Discussion Forum The bulk of our experiments is on the BOLT Web Discussion Forum domain, which uses data collected by the LDC. The parallel training da"
P15-1004,C12-2104,0,0.234404,"esults in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features. 1 Introduction Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated ∗ * Research conducted when the author was at BBN. 31 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Langua"
P15-1004,D13-1053,1,0.854027,"extensive MT experiments on Arabic-English and Chinese-English translation for the DARPA BOLT Web Forum and the NIST OpenMT12 conditions. 5.1 Baseline MT System We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). The baseline we use includes a set of powerful features as follow: • Forward and backward rule probabilities • Contextual lexical smoothing (Devlin, 2009) • 5-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model 1 In this and in the other parts of the paper, we add the normalization regularization term described in (Devlin et al., 2014) to the loss function to avoid computing the normalization constant at model query/decoding time. 35 around 0.9 BLEU point improvement in AR-EN and around 0.3 BLEU in ZH-EN compared to the baseline. The JMO consistently provides better improvement compared to a larger JM context (row S2 ), validating our hypothesi"
P15-1004,P13-1124,1,0.852162,"=1 estimates P (e5 |e4 , e3 , Ca4 = {f8 , f9 , f10 }). . . . f5 z C7 = } |Ca5 { f6 f7 f8 . . . e3 e4 e5 e6 f9 e7 get word l(fj ) = ebj given a source context Cj , bj ∈ B is the source-to-target word affiliation as defined in (Devlin et al., 2014). When fj is translated to more than one word, we arbitrarily keep the left-most one. The target word vocabulary V is extended with a N U LL token to accommodate unaligned source words. Orientation model (ORI) describes the probability of orientation of the translation of phrases surrounding a source word fj relative to its own translation. We follow (Setiawan et al., 2013) in modeling the orientation of the left and right phrases of fj with maximal orientation span (the longest neighboring phrase consistent with alignment), which we denote by Lj and Rj respectively. Thus, o(fj ) = hoLj (fj ), oRj (fj )i, where oLj and oRj refer to the orientation of Lj and Rj respectively. For unaligned fj , we set o(fj ) = oLj (Rj ), the orientation of Rj with respect to Lj . Fertility model (FM) models the probability that a source word fj generates φ(fj ) words in the hypothesis. Our implemented model only distinguishes between aligned and unaligned source words (i.e., φ(fj"
P15-1004,D13-1176,0,0.0339894,"; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al. (2014) take the representation from a bidirectional LSTM recurrent neural network; and Kalchbrenner and Blunsom (2013) employ a convolutional sentence model. For target context, recent work has tried to look beyond the classical n-gram history. (Auli et al., 2013; Sundermeyer et al., 2014) consider an unbounded history, at the expense of making their model only applicable for N-best rescoring. Another recent line of research (Bahdanau et al., 2014; Sutskever et al., 2014) departs more radically from conventional feature-based SMT and implements the MT system as a single neural network. These models use a representation of the whole input sentence. We use a feedforward neural network in this work. Besides feed"
P15-1004,J10-4005,0,0.0232201,"the sum of log likelihoods since we assume that the features are independent. Fig. 3(c) illustrates MTL between M models sharing the input embedding layer and the first hidden layer (t = 1) compared to the separatelytrained conventional feedforward neural network and tensor neural network. 5 Experiments We demonstrate the impact of our work with extensive MT experiments on Arabic-English and Chinese-English translation for the DARPA BOLT Web Forum and the NIST OpenMT12 conditions. 5.1 Baseline MT System We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). The baseline we use includes a set of powerful features as follow: • Forward and backward rule probabilities • Contextual lexical smoothing (Devlin, 2009) • 5-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model 1 In this and in the other parts of the paper, we add the normalization regularization term"
P15-1004,D08-1090,1,0.790114,"Missing"
P15-1004,D14-1003,0,0.0876777,"g to estimate the neural network parameters jointly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features. 1 Introduction Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated ∗ * Research conducted when the author was at BBN. 31 Proceedings of the 53rd Annual Meeting of the Association for Computational"
P15-1004,N04-4026,0,0.122585,"Missing"
W10-1763,N07-2014,0,0.209499,"tional level of lexical ambiguity. Readers can usually guess the correct pronunciation of words in non-diacritized text from the sentence and discourse context. Grammatical case on nouns and adjectives are also marked using diacritics at the end of words. Arabic MT systems use undiacritized text, since most available Arabic data is undiacritized. There has been work done on using diacritics in Automatic Speech Recognition, e.g. (Vergyri and Kirchho, 2004). However, the only previous work on using diacritization for MT is (Diab et al., 2007), which used the diacritization system described in (Habash and Rambow, 2007). It investigated the eect of using full diacritization as well as partial diacritization on MT results. The authors found that using full diacritics deteriorates MT performance. They used partial diacritization schemes, such as diacritizing only passive verbs, keeping the case endings diacritics, or only gemination diacritics. They also saw no gain in most congurations. The authors argued that the deterioration in performance is caused by the increase in the size of the vocabulary, which in turn makes the translation model sparser; as well as by errors during the automatic diacritization pr"
W10-1763,N03-1017,0,0.00933618,"porate context more directly by using POS tags on the target side to model word context. They augmented the target words with POS tags of the word itself and its surrounding words, and used the augmented words in decoding and for language model rescoring. They reported gains on IraqiArabic-to-English translation. Finally, using word-to-word context-free lexical translation probabilities has been shown to improve the performance of machine translation systems, even those using much more sophisticated models. This feature, usually called lexical smoothing, has been used in phrase-based systems (Koehn et al., 2003). Och et al. (2004) also found that including 2.2 Decision Trees Decision trees have been used extensively in various areas of machine learning, typically as a way to cluster patterns in order to improve classication (Duda et al., 2000). They have, for instance, been long used successfully in speech recognition to cluster contextdependent phoneme model states (Young et al., 1994). Decision trees have also been used in machine translation, although to a lesser extent. In this respect, our work is most similar to (Brunning et al., 2009), where the authors extended word alignment models for IBM"
W10-1763,N03-1000,0,0.0391861,"Missing"
W10-1763,W05-0711,0,0.498154,"Missing"
W10-1763,J03-1002,0,0.00523203,"Missing"
W10-1763,N04-1021,0,0.0309876,"irectly by using POS tags on the target side to model word context. They augmented the target words with POS tags of the word itself and its surrounding words, and used the augmented words in decoding and for language model rescoring. They reported gains on IraqiArabic-to-English translation. Finally, using word-to-word context-free lexical translation probabilities has been shown to improve the performance of machine translation systems, even those using much more sophisticated models. This feature, usually called lexical smoothing, has been used in phrase-based systems (Koehn et al., 2003). Och et al. (2004) also found that including 2.2 Decision Trees Decision trees have been used extensively in various areas of machine learning, typically as a way to cluster patterns in order to improve classication (Duda et al., 2000). They have, for instance, been long used successfully in speech recognition to cluster contextdependent phoneme model states (Young et al., 1994). Decision trees have also been used in machine translation, although to a lesser extent. In this respect, our work is most similar to (Brunning et al., 2009), where the authors extended word alignment models for IBM Model 1 and Hidden"
W10-1763,P03-1021,0,0.0222495,"Missing"
W10-1763,P02-1040,0,0.0784444,"Missing"
W10-1763,P07-1000,0,0.309138,"Missing"
W10-1763,P08-1066,0,0.0163869,"trees to deal with the sparsity side-eect. The decision trees cluster attribute-dependent source words by reducing the entropy of the lexical translation probabilities. We also present another method where, instead of clustering the attribute-dependent source words, the decision trees are used to interpolate attributedependent lexical translation probability models, and use those probabilities to compute a feature in the decoder log-linear model. The experiments we present in this paper were conducted on the translation of Arabicto-English newswire data using a hierarchical system based on (Shen et al., 2008), and using Arabic diacritics (see section 2.3) and part-ofspeech (POS) as source word attributes. Previous work that attempts to use Arabic diacritics in machine translation runs against the sparsity problem, and appears to lose most of the useful information contained in the diacritics when using partial diacritization (Diab et al., 2007). Using the methods proposed in this paper, we manage to obtain consistent improvements from diacritics against a strong baseline. The methods we propose, though, are not restrictive to Arabic-to-English translation. The same techniques can also be used with"
W10-1763,2007.mtsummit-papers.20,0,0.945259,"nslation probability models, and use those probabilities to compute a feature in the decoder log-linear model. The experiments we present in this paper were conducted on the translation of Arabicto-English newswire data using a hierarchical system based on (Shen et al., 2008), and using Arabic diacritics (see section 2.3) and part-ofspeech (POS) as source word attributes. Previous work that attempts to use Arabic diacritics in machine translation runs against the sparsity problem, and appears to lose most of the useful information contained in the diacritics when using partial diacritization (Diab et al., 2007). Using the methods proposed in this paper, we manage to obtain consistent improvements from diacritics against a strong baseline. The methods we propose, though, are not restrictive to Arabic-to-English translation. The same techniques can also be used with other language pairs and arbitrary word attribute types. The attributes we use in the described experiments are local; but long distance features can also be used. In the next section, we review relevant previous work in three areas: Lexical smoothing and lexical disambiguation techniques in machine translation; using decision trees in nat"
W10-1763,2006.amta-papers.25,1,0.797346,"Missing"
W10-1763,2007.tmi-papers.28,0,0.0422183,"Missing"
W10-1763,W04-1612,0,0.539465,"Missing"
W10-1763,H05-1097,0,0.0230829,"s, as a lexical smoothing feature in the decoder model. We present experiments using Arabic-to-English newswire data, and using Arabic diacritics and part-ofspeech as source word attributes, and show that the proposed method improves on a state-of-the-art translation system. 1 Introduction Modern statistical machine translation (SMT) models, such as phrase-based SMT or hierarchical SMT, implicitly incorporate source language context. It has been shown, however, that such systems can still benet from the explicit addition of lexical, syntactic or other kinds of context-informed word features (Vickrey et al., 2005; Gimpel and Smith, 2008; Brunning et al., 2009; Devlin, 2009). But the benet obtained from the addition of attribute information is in general countered by the increase in the model complexity, which in turn results in a sparser translation model when estimated from the same corpus of data. The increase in model sparsity usually results in a deterioration of translation quality. 428 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 428–437, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics IBM Model 1 (Brown et al"
W10-1763,H94-1062,0,0.0589736,"lities has been shown to improve the performance of machine translation systems, even those using much more sophisticated models. This feature, usually called lexical smoothing, has been used in phrase-based systems (Koehn et al., 2003). Och et al. (2004) also found that including 2.2 Decision Trees Decision trees have been used extensively in various areas of machine learning, typically as a way to cluster patterns in order to improve classication (Duda et al., 2000). They have, for instance, been long used successfully in speech recognition to cluster contextdependent phoneme model states (Young et al., 1994). Decision trees have also been used in machine translation, although to a lesser extent. In this respect, our work is most similar to (Brunning et al., 2009), where the authors extended word alignment models for IBM Model 1 and Hidden Markov Model (HMM) alignments. They used decision trees to cluster the context-dependent source words. Contexts belonging to the same cluster were grouped together during Expectation Maximization (EM) training, thus providing a more robust probability estimate. While Brunning et al. (2009) used the source context clusters for word alignments, we use the attribut"
W10-1763,P06-1073,0,0.313466,"Missing"
W10-1763,W08-0302,0,\N,Missing
W10-1763,J93-2003,0,\N,Missing
W10-1763,N09-1013,0,\N,Missing
W10-1763,D07-1007,0,\N,Missing
W10-1763,P07-1005,0,\N,Missing
