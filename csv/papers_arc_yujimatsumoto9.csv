2021.starsem-1.20,Dependency Patterns of Complex Sentences and Semantic Disambiguation for {A}bstract {M}eaning {R}epresentation Parsing,2021,-1,-1,2,0,990,yuki yamamoto,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,0,"Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure. One of the challenges we find in AMR parsing is to capture the structure of complex sentences which expresses the relation between predicates. Knowing the core part of the sentence structure in advance may be beneficial in such a task. In this paper, we present a list of dependency patterns for English complex sentence constructions designed for AMR parsing. With a dedicated pattern matcher, all occurrences of complex sentence constructions are retrieved from an input sentence. While some of the subordinators have semantic ambiguities, we deal with this problem through training classification models on data derived from AMR and Wikipedia corpus, establishing a new baseline for future works. The developed complex sentence patterns and the corresponding AMR descriptions will be made public."
2021.mrl-1.2,Learning Contextualised Cross-lingual Word Embeddings and Alignments for Extremely Low-Resource Languages Using Parallel Corpora,2021,-1,-1,3,1,5200,takashi wada,Proceedings of the 1st Workshop on Multilingual Representation Learning,0,"We propose a new approach for learning contextualised cross-lingual word embeddings based on a small parallel corpus (e.g. a few hundred sentence pairs). Our method obtains word embeddings via an LSTM encoder-decoder model that simultaneously translates and reconstructs an input sentence. Through sharing model parameters among different languages, our model jointly trains the word embeddings in a common cross-lingual space. We also propose to combine word and subword embeddings to make use of orthographic similarities across different languages. We base our experiments on real-world data from endangered languages, namely Yongning Na, Shipibo-Konibo, and Griko. Our experiments on bilingual lexicon induction and word alignment tasks show that our model outperforms existing methods by a large margin for most language pairs. These results demonstrate that, contrary to common belief, an encoder-decoder translation model is beneficial for learning cross-lingual representations even in extremely low-resource conditions. Furthermore, our model also works well on high-resource conditions, achieving state-of-the-art performance on a German-English word-alignment task."
2021.findings-acl.164,Structured Refinement for Sequential Labeling,2021,-1,-1,3,1,7900,yiran wang,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.eacl-main.323,Removing Word-Level Spurious Alignment between Images and Pseudo-Captions in Unsupervised Image Captioning,2021,-1,-1,5,0,10998,ukyo honda,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Unsupervised image captioning is a challenging task that aims at generating captions without the supervision of image-sentence pairs, but only with images and sentences drawn from different sources and object labels detected from the images. In previous work, pseudo-captions, i.e., sentences that contain the detected object labels, were assigned to a given image. The focus of the previous work was on the alignment of input images and pseudo-captions at the sentence level. However, pseudo-captions contain many words that are irrelevant to a given image. In this work, we investigate the effect of removing mismatched words from image-sentence alignment to determine how they make this task difficult. We propose a simple gating mechanism that is trained to align image features with only the most reliable words in pseudo-captions: the detected object labels. The experimental results show that our proposed method outperforms the previous methods without introducing complex sentence-level learning objectives. Combined with the sentence-level alignment method of previous work, our method further improves its performance. These results confirm the importance of careful alignment in word-level details."
2021.eacl-demos.4,{C}ov{R}elex: A {COVID}-19 Retrieval System with Relation Extraction,2021,-1,-1,6,0,11006,vu tran,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,0,"This paper presents CovRelex, a scientific paper retrieval system targeting entities and relations via relation extraction on COVID-19 scientific papers. This work aims at building a system supporting users efficiently in acquiring knowledge across a huge number of COVID-19 scientific papers published rapidly. Our system can be accessed via https://www.jaist.ac.jp/is/labs/nguyen-lab/systems/covrelex/."
2021.acl-long.275,Nested Named Entity Recognition via Explicitly Excluding the Influence of the Best Path,2021,-1,-1,3,1,7900,yiran wang,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"This paper presents a novel method for nested named entity recognition. As a layered method, our method extends the prior second-best path recognition method by explicitly excluding the influence of the best path. Our method maintains a set of hidden states at each time step and selectively leverages them to build a different potential function for recognition at each level. In addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme. We provide extensive experimental results on ACE2004, ACE2005, and GENIA datasets to show the effectiveness and efficiency of our proposed method."
2020.emnlp-main.523,{LUKE}: Deep Contextualized Entity Representations with Entity-aware Self-attention,2020,-1,-1,5,0.729167,12644,ikuya yamada,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke."
2020.emnlp-demos.4,{W}ikipedia2{V}ec: An Efficient Toolkit for Learning and Visualizing the Embeddings of Words and Entities from {W}ikipedia,2020,-1,-1,7,0.729167,12644,ikuya yamada,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"The embeddings of entities in a large knowledge base (e.g., Wikipedia) are highly beneficial for solving various natural language tasks that involve real world knowledge. In this paper, we present Wikipedia2Vec, a Python-based open-source tool for learning the embeddings of words and entities from Wikipedia. The proposed tool enables users to learn the embeddings efficiently by issuing a single command with a Wikipedia dump file as an argument. We also introduce a web-based demonstration of our tool that allows users to visualize and explore the learned embeddings. In our experiments, our tool achieved a state-of-the-art result on the KORE entity relatedness dataset, and competitive results on various standard benchmark datasets. Furthermore, our tool has been used as a key component in various recent studies. We publicize the source code, demonstration, and the pretrained embeddings for 12 languages at https://wikipedia2vec.github.io/."
2020.coling-main.271,Coordination Boundary Identification without Labeled Data for Compound Terms Disambiguation,2020,-1,-1,8,0,21367,yuya sawada,Proceedings of the 28th International Conference on Computational Linguistics,0,"We propose a simple method for nominal coordination boundary identification. As the main strength of our method, it can identify the coordination boundaries without training on labeled data, and can be applied even if coordination structure annotations are not available. Our system employs pre-trained word embeddings to measure the similarities of words and detects the span of coordination, assuming that conjuncts share syntactic and semantic similarities. We demonstrate that our method yields good results in identifying coordinated noun phrases in the GENIA corpus and is comparable to a recent supervised method for the case when the coordinator conjoins simple noun phrases."
W19-2609,Playing by the Book: An Interactive Game Approach for Action Graph Extraction from Text,2019,0,5,4,0,10802,ronen tamari,Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications,0,"Understanding procedural text requires tracking entities, actions and effects as the narrative unfolds. We focus on the challenging real-world problem of action-graph extraction from materials science papers, where language is highly specialized and data annotation is expensive and scarce. We propose a novel approach, Text2Quest, where procedural text is interpreted as instructions for an interactive game. A learning agent completes the game by executing the procedure correctly in a text-based simulated lab environment. The framework can complement existing approaches and enables richer forms of learning compared to static texts. We discuss potential limitations and advantages of the approach, and release a prototype proof-of-concept, hoping to encourage research in this direction."
P19-1158,Stochastic Tokenization with a Language Model for Neural Text Classification,2019,0,1,3,0,7542,tatsuya hiraoka,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"For unsegmented languages such as Japanese and Chinese, tokenization of a sentence has a significant impact on the performance of text classification. Sentences are usually segmented with words or subwords by a morphological analyzer or byte pair encoding and then encoded with word (or subword) representations for neural networks. However, segmentation is potentially ambiguous, and it is unclear whether the segmented tokens achieve the best performance for the target task. In this paper, we propose a method to simultaneously learn tokenization and text classification to address these problems. Our model incorporates a language model for unsupervised tokenization into a text classifier and then trains both models simultaneously. To make the model robust against infrequent tokens, we sampled segmentation for each sentence stochastically during training, which resulted in improved performance of text classification. We conducted experiments on sentiment analysis as a text classification task and show that our method achieves better performance than previous methods."
P19-1300,Unsupervised Multilingual Word Embedding with Limited Resources using Neural Language Models,2019,0,4,3,1,5200,takashi wada,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Recently, a variety of unsupervised methods have been proposed that map pre-trained word embeddings of different languages into the same space without any parallel data. These methods aim to find a linear transformation based on the assumption that monolingual word embeddings are approximately isomorphic between languages. However, it has been demonstrated that this assumption holds true only on specific conditions, and with limited resources, the performance of these methods decreases drastically. To overcome this problem, we propose a new unsupervised multilingual embedding method that does not rely on such assumption and performs well under resource-poor scenarios, namely when only a small amount of monolingual data (i.e., 50k sentences) are available, or when the domains of monolingual data are different across languages. Our proposed model, which we call {`}Multilingual Neural Language Models{'}, shares some of the network parameters among multiple languages, and encodes sentences of multiple languages into the same space. The model jointly learns word embeddings of different languages in the same space, and generates multilingual embeddings without any parallel data or pre-training. Our experiments on word alignment tasks have demonstrated that, on the low-resource condition, our model substantially outperforms existing unsupervised and even supervised methods trained with 500 bilingual pairs of words. Our model also outperforms unsupervised methods given different-domain corpora across languages. Our code is publicly available."
N19-1286,Relation Classification Using Segment-Level Attention-based {CNN} and Dependency-based {RNN},2019,0,1,4,0,11007,vanhien tran,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Recently, relation classification has gained much success by exploiting deep neural networks. In this paper, we propose a new model effectively combining Segment-level Attention-based Convolutional Neural Networks (SACNNs) and Dependency-based Recurrent Neural Networks (DepRNNs). While SACNNs allow the model to selectively focus on the important information segment from the raw sequence, DepRNNs help to handle the long-distance relations from the shortest dependency path of relation entities. Experiments on the SemEval-2010 Task 8 dataset show that our model is comparable to the state-of-the-art without using any external lexical features."
N19-1343,Decomposed Local Models for Coordinate Structure Parsing,2019,0,0,3,0,21369,hiroki teranishi,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"We propose a simple and accurate model for coordination boundary identification. Our model decomposes the task into three sub-tasks during training; finding a coordinator, identifying inside boundaries of a pair of conjuncts, and selecting outside boundaries of it. For inference, we make use of probabilities of coordinators and conjuncts in the CKY parsing to find the optimal combination of coordinate structures. Experimental results demonstrate that our model achieves state-of-the-art results, ensuring that the global structure of coordinations is consistent."
Y18-1046,Automatic Error Correction on {J}apanese Functional Expressions Using Character-based Neural Machine Translation,2018,0,0,5,1,10636,jun liu,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
Y18-1052,Reduction of Parameter Redundancy in Biaffine Classifiers with Symmetric and Circulant Weight Matrices,2018,2,0,5,0,27520,tomoki matsuno,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,"Currently, the biaffine classifier has been attracting attention as a method to introduce an attention mechanism into the modeling of binary relations. For instance, in the field of dependency parsing, the Deep Biaffine Parser by Dozat and Manning has achieved state-of-the-art performance as a graph-based dependency parser on the English Penn Treebank and CoNLL 2017 shared task. On the other hand, it is reported that parameter redundancy in the weight matrix in biaffine classifiers, which has O(n^2) parameters, results in overfitting (n is the number of dimensions). In this paper, we attempted to reduce the parameter redundancy by assuming either symmetry or circularity of weight matrices. In our experiments on the CoNLL 2017 shared task dataset, our model achieved better or comparable accuracy on most of the treebanks with more than 16% parameter reduction."
W18-6009,Coordinate Structures in {U}niversal {D}ependencies for Head-final Languages,2018,0,0,7,0,1260,hiroshi kanayama,Proceedings of the Second Workshop on Universal Dependencies ({UDW} 2018),0,"This paper discusses the representation of coordinate structures in the Universal Dependencies framework for two head-final languages, Japanese and Korean. UD applies a strict principle that makes the head of coordination the left-most conjunct. However, the guideline may produce syntactic trees which are difficult to accept in head-final languages. This paper describes the status in the current Japanese and Korean corpora and proposes alternative designs suitable for these languages."
W18-4922,Cooperating Tools for {MWE} Lexicon Management and Corpus Annotation,2018,0,0,1,1,991,yuji matsumoto,"Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)",0,"We present tools for lexicon and corpus management that offer cooperating functionality in corpus annotation. The former, named Cradle, stores a set of words and expressions where multi-word expressions are defined with their own part-of-speech information and internal syntactic structures. The latter, named ChaKi, manages text corpora with part-of-speech (POS) and syntactic dependency structure annotations. Those two tools cooperate so that the words and multi-word expressions stored in Cradle are directly referred to by ChaKi in conducting corpus annotation, and the words and expressions annotated in ChaKi can be output as a list of lexical entities that are to be stored in Cradle."
P18-4010,Sentence Suggestion of {J}apanese Functional Expressions for {C}hinese-speaking Learners,2018,0,0,3,1,10636,jun liu,"Proceedings of {ACL} 2018, System Demonstrations",0,"We present a computer-assisted learning system, Jastudy, which is particularly designed for Chinese-speaking learners of Japanese as a second language (JSL) to learn Japanese functional expressions with suggestion of appropriate example sentences. The system automatically recognizes Japanese functional expressions using a free Japanese morphological analyzer MeCab, which is retrained on a new Conditional Random Fields (CRF) model. In order to select appropriate example sentences, we apply a pairwise-based machine learning tool, Support Vector Machine for Ranking (SVMrank) to estimate the complexity of the example sentences using Japanese{--}Chinese homographs as an important feature. In addition, we cluster the example sentences that contain Japanese functional expressions with two or more meanings and usages, based on part-of-speech, conjugation forms of verbs and semantic attributes, using the K-means clustering algorithm in Scikit-Learn. Experimental results demonstrate the effectiveness of our approach."
P18-2015,Ranking-Based Automatic Seed Selection and Noise Reduction for Weakly Supervised Relation Extraction,2018,0,3,4,1,26215,vanthuy phi,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper addresses the tasks of automatic seed selection for bootstrapping relation extraction, and noise reduction for distantly supervised relation extraction. We first point out that these tasks are related. Then, inspired by ranking relation instances and patterns computed by the HITS algorithm, and selecting cluster centroids using the K-means, LSA, or NMF method, we propose methods for selecting the initial seeds from an existing resource, or reducing the level of noise in the distantly labeled data. Experiments show that our proposed methods achieve a better performance than the baseline systems in both tasks."
L18-1147,A Parallel Corpus of {A}rabic-{J}apanese News Articles,2018,0,4,3,1,513,go inoue,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1175,{PDFA}nno: a Web-based Linguistic Annotation Tool for {PDF} Documents,2018,0,0,3,1,7901,hiroyuki shindo,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1287,{U}niversal {D}ependencies Version 2 for {J}apanese,2018,0,1,7,0,13282,masayuki asahara,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1355,{S}udachi: a {J}apanese Tokenizer for Business,2018,0,2,6,0,29895,kazuma takaoka,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1356,Chemical Compounds Knowledge Visualization with Natural Language Processing and Linked Data,2018,0,1,6,0,29899,kazunari tanaka,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1396,Construction of Large-scale {E}nglish Verbal Multiword Expression Annotated Corpus,2018,0,0,3,1,28108,akihiko kato,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1418,{EMTC}: Multilabel Corpus in Movie Domain for Emotion Analysis in Conversational Text,2018,0,1,2,0,29970,ducanh phan,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-1191,A Span Selection Model for Semantic Role Labeling,2018,41,0,3,1,9339,hiroki ouchi,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"We present a simple and accurate span-based model for semantic role labeling (SRL). Our model directly takes into account all possible argument spans and scores them for each label. At decoding time, we greedily select higher scoring labeled spans. One advantage of our model is to allow us to design and use span-level features, that are difficult to use in token-based BIO tagging approaches. Experimental results demonstrate that our ensemble model achieves the state-of-the-art results, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and 2012 datasets, respectively."
C18-1067,Dynamic Feature Selection with Attention in Incremental Parsing,2018,0,1,3,1,7960,ryosuke kohita,Proceedings of the 27th International Conference on Computational Linguistics,0,"One main challenge for incremental transition-based parsers, when future inputs are invisible, is to extract good features from a limited local context. In this work, we present a simple technique to maximally utilize the local features with an attention mechanism, which works as context- dependent dynamic feature selection. Our model learns, for example, which tokens should a parser focus on, to decide the next action. Our multilingual experiment shows its effectiveness across many languages. We also present an experiment with augmented test dataset and demon- strate it helps to understand the model{'}s behavior on locally ambiguous points."
Y17-1040,Sentence Complexity Estimation for {C}hinese-speaking Learners of {J}apanese,2017,0,0,2,1,10636,jun liu,"Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation",0,None
W17-6313,Effective Online Reordering with Arc-Eager Transitions,2017,13,0,3,1,7960,ryosuke kohita,Proceedings of the 15th International Conference on Parsing Technologies,0,"We present a new transition system with word reordering for unrestricted non-projective dependency parsing. Our system is based on decomposed arc-eager rather than arc-standard, which allows more flexible ambiguity resolution between a local projective and non-local crossing attachment. In our experiment on Universal Dependencies 2.0, we find our parser outperforms the ordinary swap-based parser particularly on languages with a large amount of non-projectivity."
P17-2068,{E}nglish Multiword Expression-aware Dependency Parsing Including Named Entities,2017,8,1,3,1,28108,akihiko kato,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Because syntactic structures and spans of multiword expressions (MWEs) are independently annotated in many English syntactic corpora, they are generally inconsistent with respect to one another, which is harmful to the implementation of an aggregate system. In this work, we construct a corpus that ensures consistency between dependency structures and MWEs, including named entities. Further, we explore models that predict both MWE-spans and an MWE-aware dependency structure. Experimental results show that our joint model using additional MWE-span features achieves an MWE recognition improvement of 1.35 points over a pipeline model."
P17-1026,{A}* {CCG} Parsing with a Supertag and Dependency Factored Model,2017,24,2,3,0.952381,25534,masashi yoshikawa,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose a new A* CCG parsing model in which the probability of a tree is decomposed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs. Our factored model allows the precomputation of all probabilities and runs very efficiently, while modeling sentence structures explicitly via dependencies. Our model achieves the state-of-the-art results on English and Japanese CCG parsing."
P17-1146,Neural Modeling of Multi-Predicate Interactions for {J}apanese Predicate Argument Structure Analysis,2017,19,3,3,1,9339,hiroki ouchi,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"The performance of Japanese predicate argument structure (PAS) analysis has improved in recent years thanks to the joint modeling of interactions between multiple predicates. However, this approach relies heavily on syntactic information predicted by parsers, and suffers from errorpropagation. To remedy this problem, we introduce a model that uses grid-type recurrent neural networks. The proposed model automatically induces features sensitive to multi-predicate interactions from the word sequence information of a sentence. Experiments on the NAIST Text Corpus demonstrate that without syntactic information, our model outperforms previous syntax-dependent models."
K17-3007,Adversarial Training for Cross-Domain {U}niversal {D}ependency Parsing,2017,2,4,4,0,25550,motoki sato,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"We describe our submission to the CoNLL 2017 shared task, which exploits the shared common knowledge of a language across different domains via a domain adaptation technique. Our approach is an extension to the recently proposed adversarial training technique for domain adaptation, which we apply on top of a graph-based neural dependency parsing model on bidirectional LSTMs. In our experiments, we find our baseline graph-based parser already outperforms the official baseline model (UDPipe) by a large margin. Further, by applying our technique to the treebanks of the same language with different domains, we observe an additional gain in the performance, in particular for the domains with less training data."
K17-1042,Joint Prediction of Morphosyntactic Categories for Fine-Grained {A}rabic Part-of-Speech Tagging Exploiting Tag Dictionary Information,2017,9,4,3,1,513,go inoue,Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017),0,"Part-of-speech (POS) tagging for morphologically rich languages such as Arabic is a challenging problem because of their enormous tag sets. One reason for this is that in the tagging scheme for such languages, a complete POS tag is formed by combining tags from multiple tag sets defined for each morphosyntactic category. Previous approaches in Arabic POS tagging applied one model for each morphosyntactic tagging task, without utilizing shared information between the tasks. In this paper, we propose an approach that utilizes this information by jointly modeling multiple morphosyntactic tagging tasks with a multi-task learning framework. We also propose a method of incorporating tag dictionary information into our neural models by combining word representations with representations of the sets of possible tags. Our experiments showed that the joint model with tag dictionary information results in an accuracy of 91.38{\%} on the Penn Arabic Treebank data set, with an absolute improvement of 2.11{\%} over the current state-of-the-art tagger."
I17-2017,Segment-Level Neural Conditional Random Fields for Named Entity Recognition,2017,12,5,4,0,25550,motoki sato,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"We present Segment-level Neural CRF, which combines neural networks with a linear chain CRF for segment-level sequence modeling tasks such as named entity recognition (NER) and syntactic chunking. Our segment-level CRF can consider higher-order label dependencies compared with conventional word-level CRF. Since it is difficult to consider all possible variable length segments, our method uses segment lattice constructed from the word-level tagging model to reduce the search space. Performing experiments on NER and chunking, we demonstrate that our method outperforms conventional word-level CRF with neural networks."
I17-2027,Can Discourse Relations be Identified Incrementally?,2017,0,0,3,1,11477,frances yung,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Humans process language word by word and construct partial linguistic structures on the fly before the end of the sentence is perceived. Inspired by this cognitive ability, incremental algorithms for natural language processing tasks have been proposed and demonstrated promising performance. For discourse relation (DR) parsing, however, it is not yet clear to what extent humans can recognize DRs incrementally, because the latent {`}nodes{'} of discourse structure can span clauses and sentences. To answer this question, this work investigates incrementality in discourse processing based on a corpus annotated with DR signals. We find that DRs are dominantly signaled at the boundary between the two constituent discourse units. The findings complement existing psycholinguistic theories on expectation in discourse processing and provide direction for incremental discourse parsing."
I17-2044,Improving Neural Text Normalization with Data Augmentation at Character- and Morphological Levels,2017,0,6,7,0,17965,itsumi saito,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"In this study, we investigated the effectiveness of augmented data for encoder-decoder-based neural normalization models. Attention based encoder-decoder models are greatly effective in generating many natural languages. {\%} such as machine translation or machine summarization. In general, we have to prepare for a large amount of training data to train an encoder-decoder model. Unlike machine translation, there are few training data for text-normalization tasks. In this paper, we propose two methods for generating augmented data. The experimental results with Japanese dialect normalization indicate that our methods are effective for an encoder-decoder model and achieve higher BLEU score than that of baselines. We also investigated the oracle performance and revealed that there is sufficient room for improving an encoder-decoder model."
I17-1003,Improving Sequence to Sequence Neural Machine Translation by Utilizing Syntactic Dependency Information,2017,19,5,4,0,32889,an le,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"Sequence to Sequence Neural Machine Translation has achieved significant performance in recent years. Yet, there are some existing issues that Neural Machine Translation still does not solve completely. Two of them are translation for long sentences and the {``}over-translation{''}. To address these two problems, we propose an approach that utilize more grammatical information such as syntactic dependencies, so that the output can be generated based on more abundant information. In our approach, syntactic dependencies is employed in decoding. In addition, the output of the model is presented not as a simple sequence of tokens but as a linearized tree construction. In order to assess the performance, we construct model based on an attention mechanism encoder-decoder model in which the source language is input to the encoder as a sequence and the decoder generates the target language as a linearized dependency tree structure. Experiments on the Europarl-v7 dataset of French-to-English translation demonstrate that our proposed method improves BLEU scores by 1.57 and 2.40 on datasets consisting of sentences with up to 50 and 80 tokens, respectively. Furthermore, the proposed method also solved the two existing problems, ineffective translation for long sentences and over-translation in Neural Machine Translation."
I17-1027,Coordination Boundary Identification with Similarity and Replaceability,2017,0,0,3,0,21369,hiroki teranishi,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,We propose a neural network model for coordination boundary detection. Our method relies on the two common properties - similarity and replaceability in conjuncts - in order to detect both similar pairs of conjuncts and dissimilar pairs of conjuncts. The model improves identification of clause-level coordination using bidirectional RNNs incorporating two properties as features. We show that our model outperforms the existing state-of-the-art methods on the coordination annotated Penn Treebank and Genia corpus without any syntactic information from parsers.
E17-2001,Multilingual Back-and-Forth Conversion between Content and Function Head for Easy Dependency Parsing,2017,0,2,3,1,7960,ryosuke kohita,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Universal Dependencies (UD) is becoming a standard annotation scheme cross-linguistically, but it is argued that this scheme centering on content words is harder to parse than the conventional one centering on function words. To improve the parsability of UD, we propose a back-and-forth conversion algorithm, in which we preprocess the training treebank to increase parsability, and reconvert the parser outputs to follow the UD scheme as a postprocess. We show that this technique consistently improves LAS across languages even with a state-of-the-art parser, in particular on core dependency arcs such as nominal modifier. We also provide an in-depth analysis to understand why our method increases parsability."
Y16-2004,A Generalized Framework for Hierarchical Word Sequence Language Model,2016,0,0,3,1,33286,xiaoyi wu,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers",0,None
Y16-2006,Multiple Emotions Detection in Conversation Transcripts,2016,-1,-1,3,0,29970,ducanh phan,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers",0,None
Y16-2015,Integrating Word Embedding Offsets into the Espresso System for Part-Whole Relation Extraction,2016,15,1,2,1,26215,vanthuy phi,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers",0,None
W16-5406,{BCCWJ}-{D}ep{P}ara: A Syntactic Annotation Treebank on the {`}{B}alanced {C}orpus of {C}ontemporary {W}ritten {J}apanese{'},2016,0,4,2,0,13282,masayuki asahara,Proceedings of the 12th Workshop on {A}sian Language Resources ({ALR}12),0,"Paratactic syntactic structures are difficult to represent in syntactic dependency tree structures. As such, we propose an annotation schema for syntactic dependency annotation of Japanese, in which coordinate structures are split from and overlaid on bunsetsu-based (base phrase unit) dependency. The schema represents nested coordinate structures, non-constituent conjuncts, and forward sharing as the set of regions. The annotation was performed on the core data of {`}Balanced Corpus of Contemporary Written Japanese{'}, which comprised about one million words and 1980 samples from six registers, such as newspapers, books, magazines, and web texts."
W16-4901,Simplification of Example Sentences for Learners of {J}apanese Functional Expressions,2016,0,1,2,1,10636,jun liu,Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA}2016),0,"Learning functional expressions is one of the difficulties for language learners, since functional expressions tend to have multiple meanings and complicated usages in various situations. In this paper, we report an experiment of simplifying example sentences of Japanese functional expressions especially for Chinese-speaking learners. For this purpose, we developed {``}Japanese Functional Expressions List{''} and {``}Simple Japanese Replacement List{''}. To evaluate the method, we conduct a small-scale experiment with Chinese-speaking learners on the effectiveness of the simplified example sentences. The experimental results indicate that simplified sentences are helpful in learning Japanese functional expressions."
W16-4912,{J}apanese Lexical Simplification for Non-Native Speakers,2016,12,3,2,0,33549,muhaimin hading,Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA}2016),0,"This paper introduces Japanese lexical simplification. Japanese lexical simplification is the task of replacing difficult words in a given sentence to produce a new sentence with simple words without changing the original meaning of the sentence. We purpose a method of supervised regression learning to estimate difficulty ordering of words with statistical features obtained from two types of Japanese corpora. For the similarity of words, we use a Japanese thesaurus and dependency-based word embeddings. Evaluation of the proposed method is performed by comparing the difficulty ordering of the words."
W16-4606,Global Pre-ordering for Improving Sublanguage Translation,2016,14,2,4,1,33573,masaru fuji,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"When translating formal documents, capturing the sentence structure specific to the sublanguage is extremely necessary to obtain high-quality translations. This paper proposes a novel global reordering method with particular focus on long-distance reordering for capturing the global sentence structure of a sublanguage. The proposed method learns global reordering models from a non-annotated parallel corpus and works in conjunction with conventional syntactic reordering. Experimental results on the patent abstract sublanguage show substantial gains of more than 25 points in the RIBES metric and comparable BLEU scores both for Japanese-to-English and English-to-Japanese translations."
W16-3918,{J}apanese Text Normalization with Encoder-Decoder Model,2016,15,5,3,0,33661,taishi ikeda,Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT}),0,"Text normalization is the task of transforming lexical variants to their canonical forms. We model the problem of text normalization as a character-level sequence to sequence learning problem and present a neural encoder-decoder model for solving it. To train the encoder-decoder model, many sentences pairs are generally required. However, Japanese non-standard canonical pairs are scarce in the form of parallel corpora. To address this issue, we propose a method of data augmentation to increase data size by converting existing resources into synthesized non-standard forms using handcrafted rules. We conducted an experiment to demonstrate that the synthesized corpus contributes to stably train an encoder-decoder model and improve the performance of Japanese text normalization."
W16-3813,Identification of Flexible Multiword Expressions with the Help of Dependency Structure Annotation,2016,-1,-1,5,0,33679,ayaka morimoto,Proceedings of the Workshop on Grammar and Lexicon: interactions and interfaces ({G}ram{L}ex),0,"This paper presents our ongoing work on compilation of English multi-word expression (MWE) lexicon. We are especially interested in collecting flexible MWEs, in which some other components can intervene the expression such as {``}a number of{''} vs {``}a large number of{''} where a modifier of {``}number{''} can be placed in the expression and inherit the original meaning. We fiest collect possible candidates of flexible English MWEs from the web, and annotate all of their occurrences in the Wall Street Journal portion of Ontonotes corpus. We make use of word dependency strcuture information of the sentences converted from the phrase structure annotation. This process enables semi-automatic annotation of MWEs in the corpus and simultanaously produces the internal and external dependency representation of flexible MWEs."
P16-2086,Modelling the Interpretation of Discourse Connectives by {B}ayesian Pragmatics,2016,23,1,4,1,11477,frances yung,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We propose a framework to model human comprehension of discourse connectives. Following the Bayesian pragmatic paradigm, we advocate that discourse connectives are interpreted based on a simulation of the production process by the speaker, who, in turn, considers the ease of interpretation for the listener when choosing connectives. Evaluation against the sense annotation of the Penn Discourse Treebank confirms the superiority of the model over literal comprehension. A further experiment demonstrates that the proposed model also improves automatic discourse parsing."
N16-1133,Discriminative Reranking for Grammatical Error Correction with Statistical Machine Translation,2016,24,9,2,1,22506,tomoya mizumoto,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Research on grammatical error correction has received considerable attention. For dealing with all types of errors, grammatical error correction methods that employ statistical machine translation (SMT) have been proposed in recent years. An SMT system generates candidates with scores for all candidates and selects the sentence with the highest score as the correction result. However, the 1-best result of an SMT system is not always the best result. Thus, we propose a reranking approach for grammatical error correction. The reranking approach is used to re-score N-best results of the SMT and reorder the results. Our experiments show that our reranking system using parts of speech and syntactic features improves performance and achieves state-of-theart quality, with an F0.5 score of 40.0."
L16-1261,{U}niversal {D}ependencies for {J}apanese,2016,2,2,7,0,29830,takaaki tanaka,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present an attempt to port the international syntactic annotation scheme, Universal Dependencies, to the Japanese language in this paper. Since the Japanese syntactic structure is usually annotated on the basis of unique chunk-based dependencies, we first introduce word-based dependencies by using a word unit called the Short Unit Word, which usually corresponds to an entry in the lexicon UniDic. Porting is done by mapping the part-of-speech tagset in UniDic to the universal part-of-speech tagset, and converting a constituent-based treebank to a typed dependency tree. The conversion is not straightforward, and we discuss the problems that arose in the conversion and the current solutions. A treebank consisting of 10,000 sentences was built by converting the existent resources and currently released to the public."
L16-1263,Construction of an {E}nglish Dependency Corpus incorporating Compound Function Words,2016,7,1,3,1,28108,akihiko kato,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The recognition of multiword expressions (MWEs) in a sentence is important for such linguistic analyses as syntactic and semantic parsing, because it is known that combining an MWE into a single token improves accuracy for various NLP tasks, such as dependency parsing and constituency parsing. However, MWEs are not annotated in Penn Treebank. Furthermore, when converting word-based dependency to MWE-aware dependency directly, one could combine nodes in an MWE into a single node. Nevertheless, this method often leads to the following problem: A node derived from an MWE could have multiple heads and the whole dependency structure including MWE might be cyclic. Therefore we converted a phrase structure to a dependency structure after establishing an MWE as a single subtree. This approach can avoid an occurrence of multiple heads and/or cycles. In this way, we constructed an English dependency corpus taking into account compound function words, which are one type of MWEs that serve as functional expressions. In addition, we report experimental results of dependency parsing using a constructed corpus."
K16-1030,Modelling the Usage of Discourse Connectives as Rational Speech Acts,2016,54,1,4,1,11477,frances yung,Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning,0,"Discourse relations can either be implicit or explicitly expressed by markers, such as xe2x80x99thereforexe2x80x99 and xe2x80x99butxe2x80x99. How a speaker makes this choice is a question that is not well understood. We propose a psycholinguistic model that predicts whether a speaker will produce an explicit marker given the discourse relation s/he wishes to express. Based on the framework of the Rational Speech Acts model, we quantify the utility of producing a marker based on the information-theoretic measure of surprisal, the cost of production, and a bias to maintain uniform information density throughout the utterance. Experiments based on the Penn Discourse Treebank show that our approach outperforms stateof-the-art approaches, while giving an explanatory account of the speakerxe2x80x99s choice."
D16-1109,Joint Transition-based Dependency Parsing and Disfluency Detection for Automatic Speech Recognition Texts,2016,5,10,3,0.952381,25534,masashi yoshikawa,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-2011,Demonstration of {C}ha{K}i.{NET} {--} beyond the corpus search system,2016,0,0,2,0,13282,masayuki asahara,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"ChaKi.NET is a corpus management system for dependency structure annotated corpora. After more than 10 years of continuous development, the system is now usable not only for corpus search, but also for visualization, annotation, labelling, and formatting for statistical analysis. This paper describes the various functions included in the current ChaKi.NET system."
2016.amta-researchers.11,Improving Neural Machine Translation on resource-limited pairs using auxiliary data of a third language,2016,-1,-1,2,0,29488,ander martinez,Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track,0,"In the recent years interest in Deep Neural Networks (DNN) has grown in the field of Natural Language Processing, as new training methods have been proposed. The usage of DNN has achieved state-of-the-art performance in various areas. Neural Machine Translation (NMT) described by Bahdanau et al. (2014) and its successive variations have shown promising results. DNN, however, tend to over-fit on small data-sets, which makes this method impracticable for resource-limited language pairs. This article combines three different ideas (splitting words into smaller units, using an extra dataset of a related language pair and using monolingual data) for improving the performance of NMT models on language pairs with limited data. Our experiments show that, in some cases, our proposed approach to subword-units performs better than BPE (Byte pair encoding) and that auxiliary language-pairs and monolingual data can help improve the performance of languages with limited resources."
Y15-2015,An Efficient Annotation for Phrasal Verbs using Dependency Information,2015,10,1,3,0,36270,masayuki komai,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation: Posters",0,"In this paper, we present an efficient semiautomatic method for annotating English phrasal verbs on the OntoNotes corpus. Our method first constructs a phrasal verb dictionary based on Wiktionary, then annotates each candidate example on the corpus as an either a phrasal verb usage or a literal one. For efficient annotation, we use the dependency structure of a sentence to filter out highly plausible positive and negative cases, resulting in a drastic reduction of annotation cost. We also show that a naive binary classification achieves better MWE identification performance than rule-based and sequence-labeling methods."
Y15-1051,An Improved Hierarchical Word Sequence Language Model Using Directional Information,2015,15,0,2,1,33286,xiaoyi wu,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"For relieving data sparsity problem, Hierarchical Word Sequence (abbreviated as HWS) language model, which uses word frequency information to convert raw sentences into special n-gram sequences, can be viewed as an effective alternative to normal n-gram method. In this paper, we use directional information to make HWS models more syntactically appropriate so that higher performance can be achieved. For evaluation, we perform intrinsic and extrinsic experiments, both verify the effectiveness of our improved model."
W15-5901,Keynote Lecture 1: Scientific Paper Analysis,2015,0,0,1,1,991,yuji matsumoto,Proceedings of the 12th International Conference on Natural Language Processing,0,None
W15-4404,Collocation Assistant for Learners of {J}apanese as a Second Language,2015,8,0,2,1,1228,lis pereira,Proceedings of the 2nd Workshop on Natural Language Processing Techniques for Educational Applications,0,None
W15-4412,Grammatical Error Correction Considering Multi-word Expressions,2015,19,0,3,1,22506,tomoya mizumoto,Proceedings of the 2nd Workshop on Natural Language Processing Techniques for Educational Applications,0,"Multi-word expressions (MWEs) have been recognized as important linguistic information and much research has been conducted especially on their extraction and interpretation. On the other hand, they have hardly been used in real application areas. While those who are learning English as a second language (ESL) use MWEs in their writings just like native speakers, MWEs havenxe2x80x99t been taken into consideration in grammatical error correction tasks. In this paper, we investigate the grammatical error correction method using MWEs. Our method proposes a straightforward application of MWEs to grammatical error correction, but experimental results show that MWEs have a beneficial effect on grammatical error correction."
W15-3101,Sequential Annotation and Chunking of {C}hinese Discourse Structure,2015,18,2,3,1,11477,frances yung,Proceedings of the Eighth {SIGHAN} Workshop on {C}hinese Language Processing,0,"We propose a linguistically driven approach to represent discourse relations in Chinese text as sequences. We observe that certain surface characteristics of Chinese texts, such as the order of clauses, are overt markers of discourse structures, yet existing annotation proposals adapted from formalism constructed for English do not fully incorporate these characteristics. We present an annotated resource consisting of 325 articles in the Chinese Treebank. In addition, using this annotation, we introduce a discourse chunker based on a cascade of classifiers and report 70% top-level discourse sense accuracy."
W15-2519,Crosslingual Annotation and Analysis of Implicit Discourse Connectives for Machine Translation,2015,29,2,3,1,11477,frances yung,Proceedings of the Second Workshop on Discourse in Machine Translation,0,"Usage of discourse connectives (DCs) differs across languages, thus addition and omission of connectives are common in translation. We investigate how implicit (omitted) DCs in the source text impacts various machine translation (MT) systems, and whether a discourse parser is needed as a preprocessor to explicitate implicit DCs. Based on the manual annotation and alignment of 7266 pairs of discourse relations in a Chinese-English translation corpus, we evaluate whether a preprocessing step that inserts explicit DCs at positions of implicit relations can improve MT. Results show that, without modifying the translation model, explicitating implicit relations in the input source text has limited effect on MT evaluation scores. In addition, translation spotting analysis shows that it is crucial to identify DCs that should be explicitly translated in order to improve implicit-to-explicit DC translation. On the other hand, further analysis reveals that the disambiguation as well as explicitation of implicit relations are subject to a certain level of optionality, suggesting the limitation to learn and evaluate this linguistic phenomenon using standard parallel corpora."
W15-2208,Coordination-Aware Dependency Parsing (Preliminary Report),2015,10,1,4,0,32890,akifumi yoshimoto,Proceedings of the 14th International Conference on Parsing Technologies,0,"Coordinate structures pose difficulties in dependency parsers. In this paper, we propose a set of parsing rules specifically designed to handle coordination, which are intended to be used in combination with Eisner and Sattaxe2x80x99s dependency rules. The new rules are compatible with existing similarity-based approaches to coordination structure analysis, and thus the syntactic and semantic similarity of conjuncts can be incorporated to the parse scoring function. Although we are yet to implement such a scoring function, we analyzed the time complexity of the proposed rules as well as their coverage of the Penn Treebank converted to the Stanford basic dependencies."
W15-2213,{CKY} Parsing with Independence Constraints,2015,13,0,2,1,36951,joseph irwin,Proceedings of the 14th International Conference on Parsing Technologies,0,"The CKY algorithm is an important component in many natural language parsers. We propose a novel type of constraint for context-free parsing called independence constraints. Based on the concept of independence between words, we show how these constraints can be used to reduce the work done in the CKY algorithm. We demonstrate a classifier which can be used to identify boundaries between independent words in a sentence using only surface features, and show that it can be used to speed up a CKY parser. We investigate the trade-off between speed and accuracy, and indicate directions for improvement."
P15-2043,Synthetic Word Parsing Improves {C}hinese Word Segmentation,2015,14,2,3,1,1612,fei cheng,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"We present a novel solution to improve the performance of Chinese word segmentation (CWS) using a synthetic word parser. The parser analyses the internal structure of words, and attempts to convert out-of-vocabulary words (OOVs) into in-vocabulary fine-grained sub-words. We propose a pipeline CWS system that first predicts this fine-grained segmentation, then chunks the output to reconstruct the original word segmentation standard. We achieve competitive results on the PKU and MSR datasets, with substantial improvements in OOV recall."
P15-2140,Semantic Structure Analysis of Noun Phrases using {A}bstract {M}eaning {R}epresentation,2015,18,5,3,0,37469,yuichiro sawai,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"We propose a method for semantic structure analysis of noun phrases using Abstract Meaning Representation (AMR). AMR is a graph representation for the meaning of a sentence, in which noun phrases (NPs) are manually annotated with internal structure and semantic relations. We extract NPs from the AMR corpus and construct a data set of NP semantic structures. We also propose a transition-based algorithm which jointly identifies both the nodes in a semantic structure tree and semantic relations between them. Compared to the baseline, our method improves the performance of NP semantic structure analysis by 2.7 points, while further incorporating external dictionary boosts the performance by 7.1 points."
P15-1093,Joint Case Argument Identification for {J}apanese Predicate Argument Structure Analysis,2015,11,2,4,1,9339,hiroki ouchi,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Existing methods for Japanese predicate argument structure (PAS) analysis identify case arguments of each predicate without considering interactions between the target PAS and others in a sentence. However, the argument structures of the predicates in a sentence are semantically related to each other. This paper proposes new methods for Japanese PAS analysis to jointly identify case arguments of all predicates in a sentence by (1) modeling multiple PAS interactions with a bipartite graph and (2) approximately searching optimal PAS combinations. Performing experiments on the NAIST Text Corpus, we demonstrate that our joint analysis methods substantially outperform a strong baseline and are comparable to previous work."
2015.mtsummit-papers.1,Patent claim translation based on sublanguage-specific sentence structure,2015,-1,-1,5,1,33573,masaru fuji,Proceedings of Machine Translation Summit XV: Papers,0,None
Y14-1056,A Hierarchical Word Sequence Language Model,2014,7,3,2,1,33286,xiaoyi wu,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,"Most language models used for natural language processing are continuous. However, the assumption of such kind of models is too simple to cope with data sparsity problem. Although many useful smoothing techniques are developed to estimate these unseen sequences, it is still important to make full use of contextual information in training data. In this paper, we propose a hierarchical word sequence language model to relieve the data sparsity problem. Experiments verified the effectiveness of our model."
W14-0819,Identifying collocations using cross-lingual association measures,2014,15,3,4,1,1228,lis pereira,Proceedings of the 10th Workshop on Multiword Expressions ({MWE}),0,"We introduce a simple and effective crosslingual approach to identifying collocations. This approach is based on the observation that true collocations, which cannot be translated word for word, will exhibit very different association scores before and after literal translation. Our experiments in Japanese demonstrate that our cross-lingual association measure can successfully exploit the combination of bilingual dictionary and large monolingual corpora, outperforming monolingual association measures."
pereira-etal-2014-collocation,Collocation or Free Combination? {---} Applying Machine Translation Techniques to identify collocations in {J}apanese,2014,9,1,3,1,1228,lis pereira,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This work presents an initial investigation on how to distinguish collocations from free combinations. The assumption is that, while free combinations can be literally translated, the overall meaning of collocations is different from the sum of the translation of its parts. Based on that, we verify whether a machine translation system can help us perform such distinction. Results show that it improves the precision compared with standard methods of collocation identification through statistical association measures."
cheng-etal-2014-parsing,Parsing {C}hinese Synthetic Words with a Character-based Dependency Model,2014,17,2,3,1,1612,fei cheng,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Synthetic word analysis is a potentially important but relatively unexplored problem in Chinese natural language processing. Two issues with the conventional pipeline methods involving word segmentation are (1) the lack of a common segmentation standard and (2) the poor segmentation performance on OOV words. These issues may be circumvented if we adopt the view of character-based parsing, providing both internal structures to synthetic words and global structure to sentences in a seamless fashion. However, the accuracy of synthetic word parsing is not yet satisfactory, due to the lack of research. In view of this, we propose and present experiments on several synthetic word parsers. Additionally, we demonstrate the usefulness of incorporating large unlabelled corpora and a dictionary for this task. Our parsers significantly outperform the baseline (a pipeline method)."
E14-4030,Improving Dependency Parsers with Supertags,2014,10,12,3,1,9339,hiroki ouchi,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"Transition-based dependency parsing systems can utilize rich feature representations. However, in practice, features are generally limited to combinations of lexical tokens and part-of-speech tags. In this paper, we investigate richer features based on supertags, which represent lexical templates extracted from dependency structure annotated corpus. First, we develop two types of supertags that encode information about head position and dependency relations in different levels of granularity. Then, we propose a transition-based dependency parser that incorporates the predictions from a CRF-based supertagger as new features. On standard English Penn Treebank corpus, we show that our supertag features achieve parsing improvements of 1.3% in unlabeled attachment, 2.07% root attachment, and 3.94% in complete tree accuracy."
E14-4037,Analysis and Prediction of Unalignable Words in Parallel Text,2014,10,0,3,1,11477,frances yung,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"Professional human translators usually do not employ the concept of word alignments, producing translations xe2x80x98sense-forsensexe2x80x99 instead of xe2x80x98word-for-wordxe2x80x99. This suggests that unalignable words may be prevalent in the parallel text used for machine translation (MT). We analyze this phenomenon in-depth for Chinese-English translation. We further propose a simple and effective method to improve automatic word alignment by pre-removing unalignable words, and show improvements on hierarchical MT systems in both translation directions. 1 Motivation It is generally acknowledged that absolute equivalence between two languages is impossible, since concept lexicalization varies across languages. Major translation theories thus argue that texts should be translated xe2x80x98sense-for-sensexe2x80x99 instead of xe2x80x98word-for-wordxe2x80x99 (Nida, 1964). This suggests that unalignable words may be an issue for the parallel text used to train current statistical machine translation (SMT) systems. Although existing automatic word alignment methods have some mechanism to handle the lack of exact word-for-word alignment (e.g. null probabilities, fertility in the IBM models (Brown et al., 1993)), they may be too coarse-grained to model the xe2x80x99sense-for-sensexe2x80x99 translations created by professional human translators."
Y13-1014,Towards Automatic Error Type Classification of {J}apanese Language Learners{'} Writings,2013,13,2,3,0,40460,hiromi oyama,"Proceedings of the 27th Pacific Asia Conference on Language, Information, and Computation ({PACLIC} 27)",0,"Learner corpora are receiving special attention as an invaluable source of educational feedback and are expected to improve teaching materials and methodology. However, they include various types of incorrect sentences. Error type classification is an important task in learner corpora which enables clarifying for learners why a certain sentence is classified as incorrect in order to help learners not to repeat errors. To address this issue, we defined a set of error type criteria and conducted automatic classification of errors into error types in the sentences from the NAIST Goyo Corpus and achieved an accuracy of 77.6%. We also tried inter-corpus evaluation of our system on the Lang-8 corpus of learner Japanese and achieved an accuracy of 42.3%. To know the accuracy, we also investigated the classification method by human judgement and compared the difference in classification between the machine and the human."
W13-4604,Towards High-Reliability Speech Translation in the Medical Domain,2013,28,4,5,0,834,graham neubig,The First Workshop on Natural Language Processing for Medical and Healthcare Fields,0,"In this paper, we describe the overall design for a speech translation system that aims to reduce the problems caused by language barriers in medical situations. As first steps to building a system according to this design, we describe a collection of a medical corpus, and some translation experiments performed on this corpus. As a result of the experiments, we find that the best of three modern translation systems is able to translate 33%-81% of the sentences in a way such that the main content is understandable."
W13-4409,A Hybrid {C}hinese Spelling Correction Using Language Model and Statistical Machine Translation with Reranking,2013,9,10,5,0,3500,xiaodong liu,Proceedings of the Seventh {SIGHAN} Workshop on {C}hinese Language Processing,0,"We describe the Nara Institute of Science and Technology (NAIST) spelling check system in the shared task. Our system contains three components: a word segmentation based language model to generate correction candidates; a statistical machine translation model to provide correction candidates and a Support Vector Machine (SVM) classifier to rerank the candidates provided by the previous two components. The experimental results show that the kbest language model and the statistical machine translation model could generate almost all the correction candidates, while the precision is very low. However, using the SVM classifier to rerank the candidates, we could obtain higher precision with a little recall dropping. To address the low resource problem of the Chinese spelling check, we generate 2 million artificial training data by simply replacing the character in the provided training sentence with the character in the confusion set."
W13-3604,{NAIST} at 2013 {C}o{NLL} Grammatical Error Correction Shared Task,2013,15,19,8,0,40836,ippei yoshimoto,Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,0,"This paper describes the Nara Institute of Science and Technology (NAIST) error correction system in the CoNLL 2013 Shared Task. We constructed three systems: a system based on the Treelet Language Model for verb form and subjectverb agreement errors; a classifier trained on both learner and native corpora for noun number errors; a statistical machine translation (SMT)-based model for preposition and determiner errors. As for subject-verb agreement errors, we show that the Treelet Language Model-based approach can correct errors in which the target verb is distant from its subject. Our system ranked fourth on the official run."
W13-3523,Topic Models + Word Alignment = A Flexible Framework for Extracting Bilingual Dictionary from Comparable Corpus,2013,28,17,3,0,3500,xiaodong liu,Proceedings of the Seventeenth Conference on Computational Natural Language Learning,0,"We propose a flexible and effective framework for extracting a bilingual dictionary from comparable corpora. Our approach is based on a novel combination of topic modeling and word alignment techniques. Intuitively, our approach works by converting a comparable document-aligned corpus into a parallel topic-aligned corpus, then learning word alignments using co-occurrence statistics. This topicaligned corpus is similar in structure to the sentence-aligned corpus frequently used in statistical machine translation, enabling us to exploit advances in word alignment research. Unlike many previous work, our framework does not require any languagespecific knowledge for initialization. Furthermore, our framework attempts to handle polysemy by allowing multiple translation probability models for each word. On a large-scale Wikipedia corpus, we demonstrate that our framework reliably extracts high-precision translation pairs on a wide variety of comparable data conditions."
W13-2263,Hidden {M}arkov Tree Model for Word Alignment,2013,23,5,3,0,21370,shuhei kondo,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We propose a novel unsupervised word alignment model based on the Hidden Markov Tree (HMT) model. Our model assumes that the alignment variables have a tree structure which is isomorphic to the target dependency tree and models the distortion probability based on the source dependency tree, thereby incorporating the syntactic structure from both sides of the parallel sentences. In English-Japanese word alignment experiments, our model outperformed an IBM Model 4 baseline by over 3 points alignment error rate. While our model was sensitive to posterior thresholds, it also showed a performance comparable to that of HMM alignment models."
W13-1717,{NAIST} at the {NLI} 2013 Shared Task,2013,3,6,5,1,22506,tomoya mizumoto,Proceedings of the Eighth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper describes the Nara Institute of Science and Technology (NAIST) native language identification (NLI) system in the NLI 2013 Shared Task. We apply feature selection using a measure based on frequency for the closed track and try Capping and Sampling data methods for the open tracks. Our system ranked ninth in the closed track, third in open track 1 and fourth in open track 2."
W13-1021,Construction of {E}nglish {MWE} Dictionary and its Application to {POS} Tagging,2013,11,7,9,0,17813,yutaro shigeto,Proceedings of the 9th Workshop on Multiword Expressions,0,"This paper reports our ongoing project for constructing an English multiword expression (MWE) dictionary and NLP tools based on the developed dictionary. We extracted functional MWEs from the English part of Wiktionary, annotated the Penn Treebank (PTB) with MWE information, and conducted POS tagging experiments. We report how the MWE annotation is done on PTB and the results of POS and MWE tagging experiments."
Q13-1012,Efficient Stacked Dependency Parsing by Forest Reranking,2013,31,13,3,1,12296,katsuhiko hayashi,Transactions of the Association for Computational Linguistics,0,"This paper proposes a discriminative forest reranking algorithm for dependency parsing that can be seen as a form of efficient stacked parsing. A dynamic programming shift-reduce parser produces a packed derivation forest which is then scored by a discriminative reranker, using the 1-best tree output by the shift-reduce parser as guide features in addition to third-order graph-based features. To improve efficiency and accuracy, this paper also proposes a novel shift-reduce parser that eliminates the spurious ambiguity of arc-standard transition systems. Testing on the English Penn Treebank data, forest reranking gave a state-of-the-art unlabeled dependency accuracy of 93.12."
P13-3008,Automated Collocation Suggestion for {J}apanese Second Language Learners,2013,19,3,3,1,1228,lis pereira,51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Student Research Workshop,0,"This study addresses issues of Japanese language learning concerning word combinations (collocations). Japanese learners may be able to construct grammatically correct sentences, however, these may sound xe2x80x9cunnaturalxe2x80x9d. In this work, we analyze correct word combinations using different collocation measures and word similarity methods. While other methods use well-formed text, our approach makes use of a large Japanese language learner corpus for generating collocation candidates, in order to build a system that is more sensitive to constructions that are difficult for learners. Our results show that we get better results compared to other methods that use only wellformed text."
P13-2124,A Learner Corpus-based Approach to Verb Suggestion for {ESL},2013,12,4,3,0,10404,yu sawai,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We propose a verb suggestion method which uses candidate sets and domain adaptation to incorporate error patterns produced by ESL learners. The candidate sets are constructed from a large scale learner corpus to cover various error patterns made by learners. Furthermore, the model is trained using both a native corpus and the learner corpus via a domain adaptation technique. Experiments on two learner corpora show that the candidate sets increase the coverage of error patterns and domain adaptation improves the performance for verb suggestion."
I13-1094,What Information is Helpful for Dependency Based Semantic Role Labeling,2013,17,0,3,0,40677,yanyan luo,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Semantic Role Labeling (SRL) is an important task since it benefits a wide range of natural language processing applications. Given a sentence, the task of SRL is to identify arguments for a predicate (target verb or noun) and assign semantically meaningful labels to them. Dependency parsing based methods have achieved much success in SRL. However, due to errors in dependency parsing, there remains a large performance gap between SRL based on oracle parses and SRL based on automatic parses in practice. In light of this, this paper investigates what additional information is necessary to close this gap. Is it worthwhile to introduce additional dependency information in the form of N-best parse features, or is it better to incorporate orthogonal nondependency information (base chunk constituents)? We compare the above features in a SRL system that achieves state-of-theart results on the CoNLL 2009 Chinese task corpus. Our findings suggest that orthogonal information in the form of constituents is much more helpful in improving dependency based SRL in practice."
D13-1014,Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks,2013,31,17,4,0,41783,masashi tsubaki,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each othersxe2x80x99 meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (xcfx81 = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011)."
Y12-1004,Things between Lexicon and Grammar,2012,0,0,1,1,991,yuji matsumoto,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,None
W12-2033,{NAIST} at the {HOO} 2012 Shared Task,2012,17,7,7,1,6885,keisuke sakaguchi,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"This paper describes the Nara Institute of Science and Technology (NAIST) error correction system in the Helping Our Own (HOO) 2012 Shared Task. Our system targets preposition and determiner errors with spelling correction as a pre-processing step. The result shows that spelling correction improves the Detection, Correction, and Recognition F-scores for preposition errors. With regard to preposition error correction, F-scores were not improved when using the training set with correction of all but preposition errors. As for determiner error correction, there was an improvement when the constituent parser was trained with a concatenation of treebank and modified treebank where all the articles appearing as the first word of an NP were removed. Our system ranked third in preposition and fourth in determiner error corrections."
P12-2039,Tense and Aspect Error Correction for {ESL} Learners Using Global Context,2012,10,48,3,0,42662,toshikazu tajiri,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"As the number of learners of English is constantly growing, automatic error correction of ESL learners' writing is an increasingly active area of research. However, most research has mainly focused on errors concerning articles and prepositions even though tense/aspect errors are also important. One of the main reasons why tense/aspect error correction is difficult is that the choice of tense/aspect is highly dependent on global context. Previous research on grammatical error correction typically uses pointwise prediction that performs classification on each word independently, and thus fails to capture the information of neighboring labels. In order to take global information into account, we regard the task as sequence labeling: each verb phrase in a document is labeled with tense/aspect depending on surrounding labels. Our experiments show that the global context makes a moderate contribution to tense/aspect error correction."
P12-1069,Head-driven Transition-based Parsing with Top-down Prediction,2012,28,6,4,1,12296,katsuhiko hayashi,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper presents a novel top-down head-driven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms."
ogiso-etal-2012-unidic,{U}ni{D}ic for Early Middle {J}apanese: a Dictionary for Morphological Analysis of Classical {J}apanese,2012,4,6,4,0,18343,toshinobu ogiso,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In order to construct an annotated diachronic corpus of Japanese, we propose to create a new dictionary for morphological analysis of Early Middle Japanese (Classical Japanese) based on UniDic, a dictionary for Contemporary Japanese. Differences between the Early Middle Japanese and Contemporary Japanese, which prevent a na{\""\i}ve adaptation of UniDic to Early Middle Japanese, are found at the levels of lexicon, morphology, grammar, orthography and pronunciation. In order to overcome these problems, we extended dictionary entries and created a training corpus of Early Middle Japanese to adapt UniDic for Contemporary Japanese to Early Middle Japanese. Experimental results show that the proposed UniDic-EMJ, a new dictionary for Early Middle Japanese, achieves as high accuracy (97{\%}) as needed for the linguistic research on lexicon and grammar in Japanese classical text analysis."
C12-2084,The Effect of Learner Corpus Size in Grammatical Error Correction of {ESL} Writings,2012,21,27,5,1,22506,tomoya mizumoto,Proceedings of {COLING} 2012: Posters,0,"English as a Second Language (ESL) learnersxe2x80x99 writings contain various grammatical errors. Previous research on automatic error correction for ESL learnersxe2x80x99 grammatical errors deals with restricted types of learnersxe2x80x99 errors. Some types of errors can be corrected by rules using heuristics, while others are difficult to correct without statistical models using native corpora and/or learner corpora. Since adding error annotation to learnersxe2x80x99 text is time-consuming, it was not until recently that large scale learner corpora became publicly available. However, little is known about the effect of learner corpus size in ESL grammatical error correction. Thus, in this paper, we investigate the effect of learner corpus size on various types of grammatical errors, using an error correction system based on phrase-based statistical machine translation (SMT) trained on a large scale errortagged learner corpus. We show that the phrase-based SMT approach is effective in correcting frequent errors that can be identified by local context, and that it is difficult for phrase-based SMT to correct errors that need long range contextual information."
C12-1066,Walk-based Computation of Contextual Word Similarity,2012,28,0,4,1,36950,kazuo hara,Proceedings of {COLING} 2012,0,"We propose a new measure of semantic similarity between words in context, which exploits the syntactic/semantic structure of the context surrounding each target word. For a given pair of target words and their sentential contexts, labeled directed graphs are made from the output of a semantic parser on these sentences. Nodes in these graphs represent words in the sentences, and labeled edges represent syntactic/semantic relations between them. The similarity between the target words is then computed as the sum of the similarity of walks starting from the target words (nodes) in the two graphs. The proposed measure is tested on word sense disambiguation and paraphrase ranking tasks, and the results are promising: The proposed measure outperforms existing methods which completely ignore or do not fully exploit syntactic/semantic structural co-occurrences between a target word and its neighbors."
C12-1144,Joint {E}nglish Spelling Error Correction and {POS} Tagging for Language Learners Writing,2012,31,2,4,1,6885,keisuke sakaguchi,Proceedings of {COLING} 2012,0,"We propose an approach to correcting spelling errors and assigning part-of-speech (POS) tags simultaneously for sentences written by learners of English as a second language (ESL). In ESL writing, there are several types of errors such as preposition, determiner, verb, noun, and spelling errors. Spelling errors often interfere with POS tagging and syntactic parsing, which makes other error detection and correction tasks very difficult. In studies of grammatical error detection and correction in ESL writing, spelling correction has been regarded as a preprocessing step in a pipeline. However, several types of spelling errors in ESL are difficult to correct in the preprocessing, for example, homophones (e.g. *hear/here), confusion (*quiet/quite), split (*now a day/nowadays), merge (*swimingpool/swimming pool), inflection (*please/pleased) and derivation (*badly/bad), where the incorrect word is actually in the vocabulary and grammatical information is needed to disambiguate. In order to correct these spelling errors, and also typical typographical errors (*begginning/beginning), we propose a joint analysis of POS tagging and spelling error correction with a CRF (Conditional Random Field)-based model. We present an approach that achieves significantly better accuracies for both POS tagging and spelling correction, compared to existing approaches using either individual or pipeline analysis. We also show that the joint model can deal with novel types of misspelling in ESL writing."
Y11-1036,Dependency-based Analysis for {T}agalog Sentences,2011,20,0,2,1,31115,erlyn manguilimotan,"Proceedings of the 25th Pacific Asia Conference on Language, Information and Computation",0,"Interest in dependency parsing increased because of its efficiency to represent languages with flexible word order. Many research have applied dependency-based syntactic analysis to different languages and results vary depending on the nature of the language. Languages with more flexible word order structure tend to have lower performances compared to more fixed word order languages. This work presents, for the first time, a dependencybased parsing for Tagalog language, a free word order language. The parser is tested using manually assigned POS and auto-assigned POS data. Experiment results show an average of about 78% accuracy on unlabeled attachment scores, and about 24% on complete dependency scores. As the position of the sentence head is not fixed, we look into sentence head accuracy as assigned by the parser. Results show that around 83% 84% of the sentence head are assigned correctly."
W11-3504,Different Input Systems for Different Devices,2011,5,0,4,0,44087,asad habib,Proceedings of the Workshop on Advances in Text Input Methods ({WTIM} 2011),0,"We live in the age of touch screen gadgets. The future trends also show promising growth for them. Currently available input systems developed for standard PCs have room for improvement in efficiency, visibility and usability etc. particularly for Perso-Arabic scripts e.g., Urdu. In addition, small touch screen devices expose users to health hazards. We put forth Ergonomics in prime focus to reduce potential health hazards. We proposed distinct touch-screen keypads for different devices that are practically applicable for fast, correct and easy composing. We computed the estimated input time and tapcounts using automated procedure to compare contemporary keypads with our proposed keypads. Our experiments on a considerably large Urdu corpus reveal results of ample significance. Our optimization technique for arrangement of alphabets and unique interface for data input is extendable and equally applicable to other natural languages."
W11-3506,Error Correcting Romaji-kana Conversion for {J}apanese Language Education,2011,8,3,4,0,44093,seiji kasahara,Proceedings of the Workshop on Advances in Text Input Methods ({WTIM} 2011),0,We present an approach to help editors of Japanese on a language learning SNS correct learnersxe2x80x99 sentences written in Roman characters by converting them into kana. Our system detects foreign words and converts only Japanese words even if they contain spelling errors. Experimental results show that our system achieves about 10 points higher conversion accuracy than traditional input method (IM). Error analysis reveals some tendencies of the errors specific to language learners.
W11-1913,Narrative Schema as World Knowledge for Coreference Resolution,2011,14,9,3,1,36951,joseph irwin,Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,0,"In this paper we describe the system with which we participated in the CoNLL-2011 Shared Task on modelling coreference. Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent research on narrative event schema (Chambers and Jurafsky, 2009). We demonstrate some improvements over the baseline when using schema information, although the effect varied between the metrics used. We also explore the impact of various features on our system's performance."
W11-0318,Using the Mutual k-Nearest Neighbor Graphs for Semi-supervised Classification on Natural Language Data,2011,28,49,4,0,44419,kohei ozaki,Proceedings of the Fifteenth Conference on Computational Natural Language Learning,0,"The first step in graph-based semi-supervised classification is to construct a graph from input data. While the k-nearest neighbor graphs have been the de facto standard method of graph construction, this paper advocates using the less well-known mutual k-nearest neighbor graphs for high-dimensional natural language data. To compare the performance of these two graph construction methods, we run semi-supervised classification methods on both graphs in word sense disambiguation and document classification tasks. The experimental results show that the mutual k-nearest neighbor graphs, if combined with maximum spanning trees, consistently outperform the k-nearest neighbor graphs. We attribute better performance of the mutual k-nearest neighbor graph to its being more resistive to making hub vertices. The mutual k-nearest neighbor graphs also perform equally well or even better in comparison to the state-of-the-art b-matching graph construction, despite their lower computational complexity."
W11-0123,Recognizing Confinement in Web Texts,2011,9,6,8,0,44459,megumi ohki,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,"In the Recognizing Textual Entailment (RTE) task, sentence pairs are classified into one of three semantic relations: Entailment, Contradiction or Unknown. While we find some sentence pairs hold full entailments or contradictions, there are a number of pairs that partially entail or contradict one another depending on a specific situation. These partial contradiction sentence pairs contain useful information for opinion mining and other such tasks, but it is difficult for Internet users to access this knowledge because current frameworks do not differentiate between full contradictions and partial contradictions. In this paper, under current approaches to semantic relation recognition, we define a new semantic relation known as Confinement in order to recognize this useful information. This information is classified as either Contradiction or Entailment. We provide a series of semantic templates to recognize Confinement relations in Web texts, and then implement a system for recognizing Confinement between sentence pairs. We show that our proposed system can obtains a F-score of 61% for recognizing Confinement in Japanese-language Web texts, and it outperforms a baseline which does not use a manually compiled list of lexico-syntactic patterns to instantiate the semantic templates."
P11-2006,{HITS}-based Seed Selection and Stop List Construction for Bootstrapping,2011,28,6,4,0,43833,tetsuo kiso,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"In bootstrapping (seed set expansion), selecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graph-based approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti's Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg's HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method."
I11-1017,Mining Revision Log of Language Learning {SNS} for Automated {J}apanese Error Correction of Second Language Learners,2011,12,78,4,1,22506,tomoya mizumoto,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We present an attempt to extract a largescale Japanese learnersxe2x80x99 corpus from the revision log of a language learning SNS. This corpus is easy to obtain in largescale, covers a wide variety of topics and styles, and can be a great source of knowledge for both language learners and instructors. We also demonstrate that the extracted learnersxe2x80x99 corpus of Japanese as a second language can be used as training data for learnersxe2x80x99 error correction using an SMT approach. We evaluate different granularities of tokenization to alleviate the problem of word segmentation errors caused by erroneous input from language learners. Experimental results show that the character-wise model outperforms the word-wise model."
I11-1023,{J}apanese Predicate Argument Structure Analysis Exploiting Argument Position and Type,2011,15,14,3,1,13659,yuta hayashibe,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We propose an approach to Japanese predicate argument structure analysis exploiting argument position and type. In particular, we propose the following two methods. First, in order to use information in the sentences in preceding context of the predicate more effectively, we propose an improved similarity measure between argument positions which is more robust than a previous co-reference-based measure. Second, we propose a flexible selection-and-classification approach which accounts for the minor types of arguments. Experimental results show that our proposed method achieves state-ofthe-art accuracy for Japanese predicate argument structure analysis."
I11-1033,Automatic Labeling of Voiced Consonants for Morphological Analysis of {M}odern {J}apanese Literature,2011,7,0,4,0,18339,teruaki oka,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Since the present-day Japanese use of voiced consonant mark had established in the Meiji Era, modern Japanese literary text written in the Meiji Era often lacks compulsory voiced consonant marks. This deteriorates the performance of morphological analyzers using ordinary dictionary. In this paper, we propose an approach for automatic labeling of voiced consonant marks for modern literary Japanese. We formulate the task into a binary classification problem. Our pointwise prediction method uses as its feature set only surface information about the surrounding character strings. As a consequence, training corpus is easy to obtain and maintain because we can exploit a partially annotated corpus for learning. We compared our proposed method as a preprocessing step for morphological analysis with a dictionary-based approach, and confirmed that pointwise prediction outperforms dictionary-based approach by a large margin."
I11-1126,Jointly Extracting {J}apanese Predicate-Argument Relation with {M}arkov {L}ogic,2011,17,10,3,0.833333,32762,katsumasa yoshikawa,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"This paper describes a new Markov Logic approach for Japanese Predicate-Argument (PA) relation extraction. Most previous work built separated classifiers corresponding to each case role and independently identified the PA relations, neglecting dependencies (constraints) between two or more PA relations. We propose a method which collectively extracts PA relations by optimizing all argument candidates in a sentence. Our method can jointly consider dependency between multiple PA relations and find the most probable combination of predicates and their arguments in a sentence. In addition, our model involves new constraints to avoid considering inappropriate candidates for arguments and identify correct PA relations effectively. Compared to the state-of-the-art, our method achieves competitive results without largescale data."
D11-1058,Multilayer Sequence Labeling,2011,13,0,2,0,41115,ai azuma,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we describe a novel approach to cascaded learning and inference on sequences. We propose a weakly joint learning model on cascaded inference on sequences, called multilayer sequence labeling. In this model, inference on sequences is modeled as cascaded decision. However, the decision on a sequence labeling sequel to other decisions utilizes the features on the preceding results as marginalized by the probabilistic models on them. It is not novel itself, but our idea central to this paper is that the probabilistic models on succeeding labeling are viewed as indirectly depending on the probabilistic models on preceding analyses. We also propose two types of efficient dynamic programming which are required in the gradient-based optimization of an objective function. One of the dynamic programming algorithms resembles back propagation algorithm for multilayer feed-forward neural networks. The other is a generalized version of the forward-backward algorithm. We also report experiments of cascaded part-of-speech tagging and chunking of English sentences and show effectiveness of the proposed method."
D11-1137,Third-order Variational Reranking on Packed-Shared Dependency Forests,2011,24,12,4,1,12296,katsuhiko hayashi,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"We propose a novel forest reranking algorithm for discriminative dependency parsing based on a variant of Eisner's generative model. In our framework, we define two kinds of generative model for reranking. One is learned from training data offline and the other from a forest generated by a baseline parser on the fly. The final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model. In order to efficiently train the model from and decode on a hypergraph data structure representing a forest, we apply extended inside/outside and Viterbi algorithms. Experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches."
W10-3904,Automatic Classification of Semantic Relations between Facts and Opinions,2010,19,7,9,1,15888,koji murakami,Proceedings of the Second Workshop on {NLP} Challenges in the Information Explosion Era ({NLPIX} 2010),0,"Classifying and identifying semantic relations between facts and opinions on the Web is of utmost importance for organizing information on the Web, however, this requires consideration of a broader set of semantic relations than are typically handled in Recognizing Textual Entailment (RTE), Cross-document Structure Theory (CST), and similar tasks. In this paper, we describe the construction and evaluation of a system that identifies and classifies semantic relations in Internet data. Our system targets a set of semantic relations that have been inspired by CST but that have been generalized and broadened to facilitate application to mixed fact and opinion data from the Internet. Our system identifies these semantic relations in Japanese Web texts using a combination of lexical, syntactic, and semantic information and evaluate our system against gold standard data that was manually constructed for this task. We will release all gold standard data used in training and evaluation of our system this summer."
P10-2018,A Structured Model for Joint Learning of Argument Roles and Predicate Senses,2010,14,22,3,1,5308,yotaro watanabe,Proceedings of the {ACL} 2010 Conference Short Papers,0,"In predicate-argument structure analysis, it is important to capture non-local dependencies among arguments and inter-dependencies between the sense of a predicate and the semantic roles of its arguments. However, no existing approach explicitly handles both non-local dependencies and semantic dependencies between predicates and arguments. In this paper we propose a structured model that overcomes the limitation of existing approaches; the model captures both types of dependencies simultaneously by introducing four types of factors including a global factor type capturing non-local dependencies among arguments and a pairwise factor type capturing local dependencies between a predicate and an argument. In experiments the proposed model achieved competitive results compared to the state-of-the-art systems without applying any feature selection procedure."
matsuyoshi-etal-2010-annotating,"Annotating Event Mentions in Text with Modality, Focus, and Source Information",2010,10,23,6,1,29950,suguru matsuyoshi,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Many natural language processing tasks, including information extraction, question answering and recognizing textual entailment, require analysis of the polarity, focus of polarity, tense, aspect, mood and source of the event mentions in a text in addition to its predicate-argument structure analysis. We refer to modality, polarity and other associated information as extended modality. In this paper, we propose a new annotation scheme for representing the extended modality of event mentions in a sentence. Our extended modality consists of the following seven components: Source, Time, Conditional, Primary modality type, Actuality, Evaluation and Focus. We reviewed the literature about extended modality in Linguistics and Natural Language Processing (NLP) and defined appropriate labels of each component. In the proposed annotation scheme, information of extended modality of an event mention is summarized at the core predicate of the event mention for immediate use in NLP applications. We also report on the current progress of our manual annotation of a Japanese corpus of about 50,000 event mentions, showing a reasonably high ratio of inter-annotator agreement."
Y09-2021,Interpolated {PLSI} for Learning Plausible Verb Arguments,2009,13,3,3,0,29747,hiram calvo,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"Learning Plausible Verb Arguments allows to automatically learn what kind of activities, where and how, are performed by classes of entities from sparse argument co%occurrences with a verb; this informati on it is useful for sentence reconstruction tasks. Calvo et al. (2009b) propose a non language%dependent model based on the Word Space Model for calculating the plausibility of candidate arguments given one verb and one argument, and compare with the single latent variable PLSI algorithm method, outperforming it. In this work we replicate their experiments with a different corpus, and explore variants to the PLSI method in order to explore further capabilities of this latter widely used technique. Particularly, we propose using an interpolated PLSI scheme that allows the combination of multiple latent semantic variables, and validate it in a task of identifying the real dependency%pair triple with re gard to an artificially created one, obtaining up to 83% recall."
Y09-2039,Factors Affecting Part-of-Speech Tagging for {T}agalog,2009,13,0,2,1,31115,erlyn manguilimotan,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"This paper investigates factors contributing to the performance of the POS Tagger for Tagalog language. Tagalog, a morphologically rich language, exhibits complex morphological structure, makes use of morphological information in determining parts of speech of the word, aspect and voice. As word feature information plays important role in efficient tagging, tag set definition capturing word information also contributes to the success or failure of the tagger. A refinement of tag set is defined to possibly improve tagging performance."
W09-3027,Annotating Semantic Relations Combining Facts and Opinions,2009,9,10,6,1,15888,koji murakami,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,"As part of the Statement Map project, we are constructing a Japanese corpus annotated with the semantic relations bridging facts and opinions that are necessary for online information credibility evaluation. In this paper, we identify the semantic relations essential to this task and discuss how to efficiently collect valid examples from Web documents by splitting complex sentences into fundamental units of meaning called statements and annotating relations at the statement level. We present a statement annotation scheme and examine its reliability by annotating around 1,500 pairs of statements. We are preparing the corpus for release this winter."
W09-1218,Multilingual Syntactic-Semantic Dependency Parsing with Three-Stage Approximate Max-Margin Linear Models,2009,21,1,3,1,5308,yotaro watanabe,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL} 2009): Shared Task,0,"This paper describes a system for syntactic-semantic dependency parsing for multiple languages. The system consists of three parts: a state-of-the-art higher-order projective dependency parser for syntactic dependency parsing, a predicate classifier, and an argument classifier for semantic dependency parsing. For semantic dependency parsing, we explore use of global features. All components are trained with an approximate max-margin learning algorithm.n n In the closed challenge of the CoNLL-2009 Shared Task (Hajic et al., 2009), our system achieved the 3rd best performances for English and Czech, and the 4th best performance for Japanese."
P09-2002,Bypassed alignment graph for learning coordination in {J}apanese sentences,2009,9,3,4,0,47169,hideharu okuma,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"Past work on English coordination has focused on coordination scope disambiguation. In Japanese, detecting whether coordination exists in a sentence is also a problem, and the state-of-the-art alignment-based method specialized for scope disambiguation does not perform well on Japanese sentences. To take the detection of coordination into account, this paper introduces a 'bypass' to the alignment graph used by this method, so as to explicitly represent the non-existence of coordinate structures in a sentence. We also present an effective feature decomposition scheme based on the distance between words in conjuncts."
P09-1046,Jointly Identifying Temporal Relations with {M}arkov {L}ogic,2009,14,103,4,0.833333,32762,katsumasa yoshikawa,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"Recent work on temporal relation identification has focused on three types of relations between events: temporal relations between an event and a time expression, between a pair of events and between an event and the document creation time. These types of relations have mostly been identified in isolation by event pairwise comparison. However, this approach neglects logical constraints between temporal relations of different types that we believe to be helpful. We therefore propose a Markov Logic model that jointly identifies relations of all three relation types simultaneously. By evaluating our model on the TempEval data we show that this approach leads to about 2% higher accuracy for all three types of relations ---and to the best results for the task when compared to those of other machine learning based systems."
P09-1073,Capturing Salience with a Trainable Cache Model for Zero-anaphora Resolution,2009,28,11,3,1,12930,ryu iida,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,This paper explores how to apply the notion of caching introduced by Walker (1996) to the task of zero-anaphora resolution. We propose a machine learning-based implementation of a cache model to reduce the computational cost of identifying an antecedent. Our empirical evaluation with Japanese newspaper articles shows that the number of candidate antecedents for each zero-pronoun can be dramatically reduced while preserving the accuracy of resolving it.
P09-1109,Coordinate Structure Analysis with Global Structural Constraints and Alignment-Based Local Features,2009,20,17,4,1,36950,kazuo hara,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"We propose a hybrid approach to coordinate structure analysis that combines a simple grammar to ensure consistent global structure of coordinations in a sentence, and features based on sequence alignment to capture local symmetry of conjuncts. The weight of the alignment-based features, which in turn determines the score of coordinate structures, is optimized by perceptron training on a given corpus. A bottom-up chart parsing algorithm efficiently finds the best scoring structure, taking both nested or non-overlapping flat coordinations into account. We demonstrate that our approach outperforms existing parsers in coordination scope detection on the Genia corpus."
W08-2132,A Pipeline Approach for Syntactic and Semantic Dependency Parsing,2008,13,3,4,1,5308,yotaro watanabe,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"This paper describes our system for syntactic and semantic dependency parsing to participate the shared task of CoNLL-2008. We use a pipeline approach, in which syntactic dependency parsing, word sense disambiguation, and semantic role labeling are performed separately: Syntactic dependency parsing is performed by a tournament model with a support vector machine; word sense disambiguation is performed by a nearest neighbour method in a compressed feature space by probabilistic latent semantic indexing; and semantic role labeling is performed by a an online passive-aggressive algorithm. The submitted result was 79.10 macro-average F1 for the joint task, 87.18% syntactic dependencies LAS, and 70.84 semantic dependencies F1. After the deadline, we constructed the other configuration, which achieved 80.89 F1 for the joint task, and 74.53 semantic dependencies F1. The result shows that the configuration of pipeline is a crucial issue in the task."
O08-4003,Constructing a Temporal Relation Tagged Corpus of {C}hinese Based on Dependency Structure Analysis,2008,-1,-1,3,1,38079,yuchang cheng,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 13, Number 2, June 2008",0,None
I08-4005,Use of Event Types for Temporal Relation Identification in {C}hinese Text,2008,6,3,3,1,38079,yuchang cheng,Proceedings of the Sixth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper investigates a machine learning approach for identification of temporal relation between events in Chinese text. We proposed a temporal relation annotation guideline (Cheng, 2007) and constructed temporal information annotated corpora. However, our previous criteria did not deal with various uses of Chinese verbs. For supplementing the previous version of our criteria, we introduce attributes of verbs that describe event types. We illustrate the attributes by the different examples of verb usages. We perform an experiment to evaluate the effect of our event type attributes in the temporal relation identification. As far as we know, this is the first work of temporal relation identification between verbs in Chinese texts. The result shows that the use of the attributes of verbs can improve the annotation accuracy."
I08-4008,Analyzing {C}hinese Synthetic Words with Tree-based Information and a Survey on {C}hinese Morphologically Derived Words,2008,7,5,3,0,48567,jia lu,Proceedings of the Sixth {SIGHAN} Workshop on {C}hinese Language Processing,0,"The lack of internal information of Chinese synthetic words has become a crucial problem for Chinese morphological analysis systems which will face various needs of segmentation standards for upper NLP applications in the future. In this paper, we first categorize Chinese synthetic words into several types according to their inside semantic and syntactic structure, and then propose a method to represent these inside information of word by applying a tree-based structure. Then we try to automatically identify the inner morphological structure of 3-character synthetic words by using a large corpus and try to add syntactic tags to their internal structure. We believe that this tree-based word internal information could be useful in specifying a Chinese synthetic word segmentation standard."
I08-1018,Generic Text Summarization Using Probabilistic Latent Semantic Indexing,2008,13,21,4,0,48664,harendra bhandari,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"This paper presents a strategy to generate generic summary of documents using Probabilistic Latent Semantic Indexing. Generally a document contains several topics rather than a single one. Summaries created by human beings tend to cover several topics to give the readers an overall idea about the original document. Hence we can expect that a summary containing sentences from better part of the topic spectrum should make a better summary. PLSI has proven to be an effective method in topic detection. In this paper we present a method for creating extractive summary of the document by using PLSI to analyze the features of document such as term frequency and graph structure. We also show our results, which was evaluated using ROUGE, and compare the results with other techniques, proposed in the past."
I08-1062,{J}apanese-{S}panish Thesaurus Construction Using {E}nglish as a Pivot,2008,7,6,3,0,48679,jessica ramirez,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"We present the results of research with the goal of automatically creating a multilingual thesaurus based on the freely available resources of Wikipedia and WordNet. Our goal is to increase resources for natural language processing tasks such as machine translation targeting the Japanese-Spanish language pair. Given the scarcity of resources, we use existing English resources as a pivot for creating a trilingual JapaneseSpanish-English thesaurus. Our approach consists of extracting the translation tuples from Wikipedia, disambiguating them by mapping them to WordNet word senses. We present results comparing two methods of disambiguation, the first using VSM on Wikipedia article texts and WordNet definitions, and the second using categorical information extracted from Wikipedia, We find that mixing the two methods produces favorable results. Using the proposed method, we have constructed a multilingual Spanish-Japanese-English thesaurus consisting of 25,375 entries. The same method can be applied to any pair of languages that are linked to English in Wikipedia."
I08-1065,Acquiring Event Relation Knowledge by Learning Cooccurrence Patterns and Fertilizing Cooccurrence Samples with Verbal Nouns,2008,13,8,3,0,48682,shuya abe,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"Aiming at acquiring semantic relations between events from a large corpus, this paper proposes several extensions to a state-of-theart method originally designed for entity relation extraction, reporting on the present results of our experiments on a Japanese Web corpus. The results show that (a) there are indeed specific cooccurrence patterns useful for event relation acquisition, (b) the use of cooccurrence samples involving verbal nouns has positive impacts on both recall and precision, and (c) over five thousand relation instances are acquired from a 500M-sentence Web corpus with a precision of about 66% for action-effect relations."
D08-1106,Graph-based Analysis of Semantic Drift in {E}spresso-like Bootstrapping Algorithms,2008,27,37,4,1,310,mamoru komachi,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"Bootstrapping has a tendency, called semantic drift, to select instances unrelated to the seed instances as the iteration proceeds. We demonstrate the semantic drift of bootstrapping has the same root as the topic drift of Kleinberg's HITS, using a simplified graph-based reformulation of bootstrapping. We confirm that two graph-based algorithms, the von Neumann kernels and the regularized Laplacian, can reduce semantic drift in the task of word sense disambiguation (WSD) on Senseval-3 English Lexical Sample Task. Proposed algorithms achieve superior performance to Espresso and previous graph-based WSD methods, even though the proposed algorithms have less parameters and are easy to calibrate."
C08-1001,Two-Phased Event Relation Acquisition: Coupling the Relation-Oriented and Argument-Oriented Approaches,2008,13,19,3,0,48682,shuya abe,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Addressing the task of acquiring semantic relations between events from a large corpus, we first argue the complementarity between the pattern-based relation-oriented approach and the anchor-based argument-oriented approach. We then propose a two-phased approach, which first uses lexico-syntactic patterns to acquire predicate pairs and then uses two types of anchors to identify shared arguments. The present results of our empirical evaluation on a large-scale Japanese Web corpus have shown that (a) the anchor-based filtering extensively improves the accuracy of predicate pair acquisition, (b) the two types of anchors are almost equally contributive and combining them improves recall without losing accuracy, and (c) the anchor-based method also achieves high accuracy in shared argument identification."
C08-1046,{J}apanese Dependency Parsing Using a Tournament Model,2008,14,3,3,0,44088,masakazu iwatate,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"In Japanese dependency parsing, Kudo's relative preference-based method (Kudo and Matsumoto, 2005) outperforms both deterministic and probabilistic CKY-based parsing methods. In Kudo's method, for each dependent word (or chunk) a log-linear model estimates relative preference of all other candidate words (or chunks) for being as its head. This cannot be considered in the deterministic parsing methods. We propose an algorithm based on a tournament model, in which the relative preferences are directly modeled by one-on-one games in a step-ladder tournament. In an evaluation experiment with Kyoto Text Corpus Version 4.0, the proposed method outperforms previous approaches, including the relative preference-based method."
C08-1111,Emotion Classification Using Massive Examples Extracted from the Web,2008,21,111,3,0,48744,ryoko tokuhisa,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"In this paper, we propose a data-oriented method for inferring the emotion of a speaker conversing with a dialog system from the semantic content of an utterance. We first fully automatically obtain a huge collection of emotion-provoking event instances from the Web. With Japanese chosen as a target language, about 1.3 million emotion provoking event instances are extracted using an emotion lexicon and lexical patterns. We then decompose the emotion classification task into two sub-steps: sentiment polarity classification (coarsegrained emotion classification), and emotion classification (fine-grained emotion classification). For each subtask, the collection of emotion-proviking event instances is used as labelled examples to train a classifier. The results of our experiments indicate that our method significantly outperforms the baseline method. We also find that compared with the single-step model, which applies the emotion classifier directly to inputs, our two-step model significantly reduces sentiment polarity errors, which are considered fatal errors in real dialog applications."
C08-1113,Training Conditional Random Fields Using Incomplete Annotations,2008,14,47,5,0.833333,30930,yuta tsuboi,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"We address corpus building situations, where complete annotations to the whole corpus is time consuming and unrealistic. Thus, annotation is done only on crucial part of sentences, or contains unresolved label ambiguities. We propose a parameter estimation method for Conditional Random Fields (CRFs), which enables us to use such incomplete annotations. We show promising results of our method as applied to two types of NLP tasks: a domain adaptation task of a Japanese word segmentation using partial annotations, and a part-of-speech tagging task using ambiguous tags in the Penn treebank corpus."
W07-1522,Annotating a {J}apanese Text Corpus with Predicate-Argument and Coreference Relations,2007,13,66,4,1,12930,ryu iida,Proceedings of the Linguistic Annotation Workshop,0,"In this paper, we discuss how to annotate coreference and predicate-argument relations in Japanese written text. There have been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations as are done in the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDA-Tagged Corpus (Hasida, 2005). However, there is still much room for refining their specifications. For this reason, we discuss issues in annotating these two types of relations, and propose a new specification for each. In accordance with the specification, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus1, which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006)."
S07-1052,{NAIST}.{J}apan: Temporal Relation Identification Using Dependency Parsed Tree,2007,8,26,3,1,38079,yuchang cheng,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"In this paper, we attempt to use a sequence labeling model with features from dependency parsed tree for temporal relation identification. In the sequence labeling model, the relations of contextual pairs can be used as features for relation identification of the current pair. Head-modifier relations between pairs of words within one sentence can be also used as the features. In our preliminary experiments, these features are effective for the temporal relation identification tasks."
D07-1068,A Graph-Based Approach to Named Entity Categorization in {W}ikipedia Using Conditional Random Fields,2007,22,45,3,1,5308,yotaro watanabe,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"This paper presents a method for categorizing named entities in Wikipedia. In Wikipedia, an anchor text is glossed in a linked HTML text. We formalize named entity categorization as a task of categorizing anchor texts with linked HTML texts which glosses a named entity. Using this representation, we introduce a graph structure in which anchor texts are regarded as nodes. In order to incorporate HTML structure on the graph, three types of cliques are defined based on the HTML tree structure. We propose a method with Conditional Random Fields (CRFs) to categorize the nodes on the graph. Since the defined graph may include cycles, the exact inference of CRFs is computationally expensive. We introduce an approximate inference method using Treebased Reparameterization (TRP) to reduce computational cost. In experiments, our proposed model obtained significant improvements compare to baseline models that use Support Vector Machines."
D07-1114,Extracting Aspect-Evaluation and Aspect-Of Relations in Opinion Mining,2007,21,133,3,0,40256,nozomi kobayashi,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"The technology of opinion extraction allows users to retrieve and analyze peoplexe2x80x99s opinions scattered over Web documents. We define an opinion unit as a quadruple consisting of the opinion holder, the subject being evaluated, the part or the attribute in which the subject is evaluated, and the value of the evaluation that expresses a positive or negative assessment. We use this definition as the basis for our opinion extraction task. We focus on two important subtasks of opinion extraction: (a) extracting aspect-evaluation relations, and (b) extracting aspect-of relations, and we approach each task using methods which combine contextual and statistical clues. Our experiments on Japanese weblog posts show that the use of contextual clues improve the performance for both tasks."
2007.tmi-papers.17,Combining resources for open source machine translation,2007,19,4,4,1,31491,eric nichols,Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,"In this paper, we present a Japanesexe2x86x92English machine translation system that combines rule-based and statistical translation. Our system is unique in that all of its components are freely available as open source software. We describe the development of the rule-based translation engine including transfer rule acquisition from an open bilingual dictionary. We also show how translations from both translation engines are combined through a simple ranking mechanism and compare their outputs."
Y06-1044,The Construction of a Dictionary for a Two-layer {C}hinese Morphological Analyzer,2006,11,2,5,1,44966,chooiling goh,"Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation",0,"We built a morphological analyzer, which can be freely used by anyone for research purpose. In order to build a pratical system, a dictionary with reasonable size is necessary. The initial dictionary is built from the Penn Chinese Treebank corpus v4.0 and contains only 33,438 entries. Since the initial dictionary is quite small, unknown word detection methods are applied to a huge raw text in order to extract new words to be added into the system dictionary. We have successfully constructed a dictionary with 120,769 entries. Finally, we propose a two-layer morphological analyzer to cater for two sets of outputs. The first layer produces the minimal segmentation units defined by us, and the second layer transforms the output of the first layer to the original segmentation units defined by Penn Chinese Treebank."
W06-2927,Multi-lingual Dependency Parsing at {NAIST},2006,14,16,3,1,38079,yuchang cheng,Proceedings of the Tenth Conference on Computational Natural Language Learning ({C}o{NLL}-X),0,"In this paper, we present a framework for multi-lingual dependency parsing. Our bottom-up deterministic parser adopts Nivre's algorithm (Nivre, 2004) with a preprocessor. Support Vector Machines (SVMs) are utilized to determine the word dependency attachments. Then, a maximum entropy method (MaxEnt) is used for determining the label of the dependency relation. To improve the performance of the parser, we construct a tagger based on SVMs to find neighboring attachment as a preprocessor. Experimental evaluation shows that the proposed extension improves the parsing accuracy of our base parser in 9 languages. (Hajic et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; Bohmova et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dzeroski et al., 2006; Civit and Marti, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003)."
P06-1079,Exploiting Syntactic Patterns as Clues in Zero-Anaphora Resolution,2006,26,40,3,1,12930,ryu iida,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zero-anaphora resolution. For the former problem, syntactic patterns of the appearance of zero-pronouns and their antecedents are useful clues. Taking Japanese as a target language, we empirically demonstrate that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora, which consequently improves the overall performance of zero-anaphora resolution."
P06-1089,Guessing Parts-of-Speech of Unknown Words Using Global Information,2006,21,11,2,1,27752,tetsuji nakagawa,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we present a method for guessing POS tags of unknown words using local and global information. Although many existing methods use only local information (i.e. limited window size or intra-sentential features), global information (extra-sentential features) provides valuable clues for predicting POS tags of unknown words. We propose a probabilistic model for POS guessing of unknown words using global information as well as local information, and estimate its parameters using Gibbs sampling. We also attempt to apply the model to semi-supervised learning, and conduct experiments on multiple corpora."
matsumoto-etal-2006-annotated,An Annotated Corpus Management Tool: {C}ha{K}i,2006,3,2,1,1,991,yuji matsumoto,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Large scale annotated corpora are very important not only inlinguistic research but also in practical natural language processingtasks since a number of practical tools such as Part-of-speech (POS) taggers and syntactic parsers are now corpus-based or machine learning-based systems which require some amount of accurately annotated corpora. This article presents an annotated corpus management tool that provides various functions that include flexible search, statistic calculation, and error correction for linguistically annotated corpora. The target of annotation covers POS tags, base phrase chunks and syntactic dependency structures. This tool aims at helping development of consistent construction of lexicon and annotated corpora to be used by researchers both in linguists and language processing communities."
inui-etal-2006-augmenting,Augmenting a Semantic Verb Lexicon with a Large Scale Collection of Example Sentences,2006,0,0,5,1,4154,kentaro inui,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"One of the crucial issues in semantic parsing is how to reduce costs of collecting a sufficiently large amount of labeled data. This paper presents a new approach to cost-saving annotation of example sentences with predicate-argument structure information, taking Japanese as a target language. In this scheme, a large collection of unlabeled examples are first clustered and selectively sampled, and for each sampled cluster, only one representative example is given a label by a human annotator. The advantages of this approach are empirically supported by the results of our preliminary experiments, where we use an existing similarity function and naive sampling strategy."
2006.iwslt-evaluation.11,Phrase reordering for statistical machine translation based on predicate-argument structure,2006,15,26,3,1,310,mamoru komachi,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper, we describe a novel phrase reordering model based on predicate-argument structure. Our phrase reordering method utilizes a general predicate-argument structure analyzer to reorder source language chunks based on predicate-argument structure. We explicitly model longdistance phrase alignments by reordering arguments and predicates. The reordering approach is applied as a preprocessing step in training phase of a phrase-based statistical MT system. We report experimental results in the evaluation campaign of IWSLT 2006."
O05-4005,{C}hinese Word Segmentation by Classification of Characters,2005,-1,-1,3,1,44966,chooiling goh,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 3, September 2005: Special Issue on Selected Papers from {ROCLING} {XVI}",0,None
I05-3003,{C}hinese Deterministic Dependency Analyzer: Examining Effects of Global Features and Root Node Finder,2005,0,2,3,1,38079,yuchang cheng,Proceedings of the Fourth {SIGHAN} Workshop on {C}hinese Language Processing,0,None
I05-3018,Combination of Machine Learning Methods for Optimum {C}hinese Word Segmentation,2005,10,21,6,1,13282,masayuki asahara,Proceedings of the Fourth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This article presents our recent work for participation in the Second International Chinese Word Segmentation Bakeoff. Our system performs two procedures: Out-ofvocabulary extraction and word segmentation. We compose three out-of-vocabulary extraction modules: Character-based tagging with different classifiers xe2x80x93 maximum entropy, support vector machines, and conditional random fields. We also compose three word segmentation modules xe2x80x93 character-based tagging by maximum entropy classifier, maximum entropy markov model, and conditional random fields. All modules are based on previously proposed methods. We submitted three systems which are different combination of the modules."
I05-2030,Opinion Extraction Using a Learning-Based Anaphora Resolution Technique,2005,9,35,4,0,40256,nozomi kobayashi,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,"This paper addresses the task of extracting opinions from a given document collection. Assuming that an opinion can be represented as a tuple xe3x80x88Subject, Attribute, Valuexe3x80x89, we propose a computational method to extract such tuples from texts. In this method, the main task is decomposed into (a) the process of extracting Attribute-Value pairs from a given text and (b) the process of judging whether an extracted pair expresses an opinion of the author. We apply machine-learning techniques to both subtasks. We also report on the results of our experiments and discuss future directions."
I05-1050,Automatic Extraction of Fixed Multiword Expressions,2005,0,0,3,0,51064,campbell hore,Second International Joint Conference on Natural Language Processing: Full Papers,0,None
I05-1059,Building a {J}apanese-{C}hinese Dictionary Using Kanji/Hanzi Conversion,2005,7,17,3,1,44966,chooiling goh,Second International Joint Conference on Natural Language Processing: Full Papers,0,"A new bilingual dictionary can be built using two existing bilingual dictionaries, such as Japanese-English and English-Chinese to build Japanese-Chinese dictionary. However, Japanese and Chinese are nearer languages than English, there should be a more direct way of doing this. Since a lot of Japanese words are composed of kanji, which are similar to hanzi in Chinese, we attempt to build a dictionary for kanji words by simple conversion from kanji to hanzi. Our survey shows that around 2/3 of the nouns and verbal nouns in Japanese are kanji words, and more than 1/3 of them can be translated into Chinese directly. The accuracy of conversion is 97%. Besides, we obtain translation candidates for 24% of the Japanese words using English as a pivot language with 77% accuracy. By adding the kanji/hanzi conversion method, we increase the candidates by 9%, to 33%, with better quality candidates."
I05-1079,Exploiting Lexical Conceptual Structure for Paraphrase Generation,2005,16,6,3,1,5049,atsushi fujita,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Lexical Conceptual Structure (LCS) represents verbs as semantic structures with a limited number of semantic predicates. This paper attempts to exploit how LCS can be used to explain the regularities underlying lexical and syntactic paraphrases, such as verb alternation, compound word decomposition, and lexical derivation. We propose a paraphrase generation model which transforms LCSs of verbs, and then conduct an empirical experiment taking the paraphrasing of Japanese light-verb constructions as an example. Experimental results justify that syntactic and semantic properties of verbs encoded in LCS are useful to semantically constrain the syntactic transformation in paraphrase generation."
Y04-1002,Machine Learning based {NLP} : Experiences and Supporting Tools,2004,0,0,1,1,991,yuji matsumoto,"Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation",0,None
Y04-1010,{J}apanese Subjects and Information Structure : A Constraint-based Approach,2004,9,0,2,0,31083,akira ohtani,"Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation",0,"This paper is concerned with how topic/focus articulation should be optimally integrated into Japanese grammar. Based on Engdahl and Vallduvs (1996) Information Structure, we pro- pose an analysis with the following characteristics: (i) information structure is an integral part of Japanese grammar and interacts in principled ways with both syntax and phonology, (ii) the representations of topic/focus in the information structure and its interactions with the par- ticles wa/ga show one-to-many relation, and (iii) the ordering of grammatical functions and its interactions with other grammatical parts play an important role in determining the focus domain."
Y04-1014,Pruning False Unknown Words to Improve {C}hinese Word Segmentation,2004,10,6,3,1,44966,chooiling goh,"Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation",0,"During the process of unknown word detection in Chinese word segmentation, many detected word candidates are invalid. These false unknown word candidates deteriorate the overall segmentation accuracy, as it will affect the segmentation accuracy of known words. Therefore, we propose to eliminate as many invalid word candidates as possible by a pruning process. Our experiments show that by cutting down the invalid unknown word candidates, we improve the segmentation accuracy of known words and hence that of the overall segmentation accuracy."
W04-3230,Applying Conditional Random Fields to {J}apanese Morphological Analysis,2004,18,467,3,1,29083,taku kudo,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents Japanese morphological analysis based on conditional random fields (CRFs). Previous work in CRFs assumed that observation sequence (word) boundaries were fixed. However, word boundaries are not clear in Japanese, and hence a straightforward application of CRFs is not possible. We show how CRFs can be applied to situations where word boundary ambiguity exists. CRFs offer a solution to the long-standing problems in corpus-based or statistical Japanese morphological analysis. First, flexible feature designs for hierarchical tagsets become possible. Second, influences of label and length bias are minimized. We experiment CRFs on the standard testbed corpus used for Japanese morphological analysis, and evaluate our results using the same experimental dataset as the HMMs and MEMMs previously reported in this task. Our results confirm that CRFs not only solve the long-standing problems but also improve the performance over HMMs and MEMMs."
W04-3239,A Boosting Algorithm for Classification of Semi-Structured Text,2004,0,0,2,1,29083,taku kudo,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,None
W04-2408,Modeling Category Structures with a Kernel Function,2004,8,5,2,1,4800,hiroya takamura,Proceedings of the Eighth Conference on Computational Natural Language Learning ({C}o{NLL}-2004) at {HLT}-{NAACL} 2004,0,"We propose one type of TOP (Tangent vector Of the Posterior log-odds) kernel and apply it to text categorization. In a number of categorization tasks including text categorization, negative examples are usually more common than positive examples and there may be several different types of negative examples. Therefore, we construct a TOP kernel, regarding the probabilistic model of negative examples as a mixture of several component models respectively corresponding to given categories. Since each component model of our mixture model is expressed using a one-dimensional Gaussian-type function, the proposed kernel has an advantage in computational time. We also show that the computational advantage is shared by a more general class of models. In our experiments, the proposed kernel used with Support Vector Machines outperformed the linear kernel and the Fisher kernel based on the Probabilistic Latent Semantic Indexing model."
W04-1109,{C}hinese Word Segmentation by Classification of Characters,2004,0,7,3,1,44966,chooiling goh,Proceedings of the Third {SIGHAN} Workshop on {C}hinese Language Processing,0,None
W04-0402,Paraphrasing of {J}apanese Light-verb Constructions Based on Lexical Conceptual Structure,2004,14,13,4,1,5049,atsushi fujita,Proceedings of the Workshop on Multiword Expressions: Integrating Processing,0,"Some particular classes of lexical paraphrases such as verb alteration and compound noun decomposition can be handled by a handful of general rules and lexical semantic knowledge. In this paper, we attempt to capture the regularity underlying these classes of paraphrases, focusing on the paraphrasing of Japanese light-verb constructions (LVCs). We propose a paraphrasing model for LVCs that is based on transforming the Lexical Conceptual Structures (LCSs) of verbal elements. We also propose a refinement of an existing LCS dictionary. Experimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases."
shimohata-etal-2004-building,Building a Paraphrase Corpus for Speech Translation,2004,7,9,3,1,51054,mitsuo shimohata,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"When a machine translation (MT) system receives input sentences of spoken language, the following two types of sentences are difficult to translate: (1) long sentences and (2) sentences having redundant expressions often seen in spoken language. To reduce these difficulties, we are developing methods to paraphrase input sentences into more translatable ones. In this paper, we report a preliminary Japanese paraphrase corpus. The corpus consists of original sentences derived from travel conversation and versions of them paraphrased by humans. We use three paraphrasing methods: plain, segment, and summary paraphrasing. Plain paraphrasing is applied to short sentences, where redundant expressions are replaced with plain ones. Segment and summary paraphrasing is applied to long sentences, where long sentences are converted into one or several short sentences. We also report a comparison of machine translation quality between the original sentences and the paraphrased sentences. We use two corpus-based machine translation systems in the experiment."
C04-1066,{J}apanese Unknown Word Identification by Character-based Chunking,2004,17,20,2,1,13282,masayuki asahara,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We introduce a character-based chunking for unknown word identification in Japanese text. A major advantage of our method is an ability to detect low frequency unknown words of unrestricted character type patterns. The method is built upon SVM-based chunking, by use of character n-gram and surrounding context of n-best word segmentation candidates from statistical morphological analysis as features. It is applied to newspapers and patent texts, achieving 95% precision and 55-70% recall for newspapers and more than 85% precision for patent texts."
C04-1130,Trajectory Based Word Sense Disambiguation,2004,15,9,2,1,6692,xiaojie wang,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Classifier combination is a promising way to improve performance of word sense disambiguation. We propose a new combinational method in this paper. We first construct a series of Naive Bayesian classifiers along a sequence of orderly varying sized windows of context, and perform sense selection for both training samples and test samples using these classifiers. We thus get a sense selection trajectory along the sequence of context windows for each sample. Then we make use of these trajectories to make final k-nearest-neighbors-based sense selection for test samples. This method aims to lower the uncertainty brought by classifiers using different context windows and make more robust utilization of context while perform well. Experiments show that our approach outperforms some other algorithms on both robustness and performance."
2004.tmi-1.12,Method for retrieving a similar sentence and its application to machine translation,2004,0,1,3,1,51054,mitsuo shimohata,Proceedings of the 10th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"xe8xa9xb1xe3x81x97xe8xa8x80xe8x91x89xe3x81xa7xe3x81x82xe3x82x8bxe7x99xbaxe8xa9xb1xe3x82x92xe5xafxbexe8xb1xa1xe3x81xa8xe3x81x97xe3x81xa6xe6xa9x9fxe6xa2xb0xe7xbfxbbxe8xa8xb3xe3x82x92xe8xa1x8cxe3x81xa3xe3x81x9fxe5xa0xb4xe5x90x88, xe8xa9xb1xe3x81x97xe8xa8x80xe8x91x89xe7x89xb9xe6x9cx89xe3x81xaexe6x80xa7xe8xb3xaaxe3x81x8cxe4xb8x80xe5x9bxa0xe3x81xa8xe3x81xaaxe3x81xa3xe3x81xa6xe9x81xa9xe5x88x87xe3x81xaaxe7xbfxbbxe8xa8xb3xe6x96x87xe3x81x8cxe5xbex97xe3x82x89xe3x82x8cxe3x81xaaxe3x81x84xe5xa0xb4xe5x90x88xe3x81x8cxe3x81x82xe3x82x8b.xe6x9cxacxe8xabx96xe6x96x87xe3x81xa7xe3x81xaf, xe9x81xa9xe5x88x87xe3x81xaaxe7xbfxbbxe8xa8xb3xe6x96x87xe3x81x8cxe5xbex97xe3x82x89xe3x82x8cxe3x81xaaxe3x81x8bxe3x81xa3xe3x81x9fxe5xa0xb4xe5x90x88xe3x81xabxe9xa1x9exe4xbcxbcxe6x96x87xe6xa4x9cxe7xb4xa2xe6x8ax80xe8xa1x93xe3x82x92xe7x94xa8xe3x81x84xe3x82x8bxe3x81x93xe3x81xa8xe3x81xa7xe9x81xa9xe5x88x87xe3x81xaaxe8xa8xb3xe6x96x87xe3x82x92xe5xbex97xe3x82x8bxe6x96xb9xe6xb3x95xe3x82x92xe6x8fx90xe6xa1x88xe3x81x99xe3x82x8b.xe4xb8x8exe3x81x88xe3x82x89xe3x82x8cxe3x81x9fxe5x85xa5xe5x8ax9bxe6x96x87xe3x81x8cxe9x81xa9xe5x88x87xe3x81xabxe7xbfxbbxe8xa8xb3xe3x81xa7xe3x81x8dxe3x81xaaxe3x81x84xe3x81xa8xe5x88xa4xe6x98x8exe3x81x97xe3x81x9fxe5xa0xb4xe5x90x88xe3x81xab, xe7xbfxbbxe8xa8xb3xe5x8fxafxe8x83xbdxe3x81xaaxe6x96x87xe3x82x92xe9x9bx86xe3x82x81xe3x81x9fxe3x82xb3xe3x83xbcxe3x83x91xe3x82xb9xe3x81x8bxe3x82x89xe5x85xa5xe5x8ax9bxe6x96x87xe3x81xaexe9xa1x9exe4xbcxbcxe6x96x87xe3x82x92xe6xa4x9cxe7xb4xa2xe3x81x99xe3x82x8b.xe6xa4x9cxe7xb4xa2xe3x81x95xe3x82x8cxe3x81x9fxe9xa1x9exe4xbcxbcxe6x96x87xe3x82x92xe6xa9x9fxe6xa2xb0xe7xbfxbbxe8xa8xb3xe3x81xabxe4xb8x8exe3x81x88xe3x81xa6xe7xbfxbbxe8xa8xb3xe6x96x87xe3x82x92xe5xbex97xe3x82x8bxe3x81x93xe3x81xa8xe3x81xa7, xe9x81xa9xe5x88x87xe3x81xaaxe7xbfxbbxe8xa8xb3xe6x96x87xe3x82x92xe5xbex97xe3x82x8bxe3x81x93xe3x81xa8xe3x81x8cxe3x81xa7xe3x81x8dxe3x82x8b.xe6xa4x9cxe7xb4xa2xe5xafxbexe8xb1xa1xe3x81xa8xe3x81xaaxe3x82x8bxe6x96x87 (xe5x80x99xe8xa3x9cxe6x96x87) xe3x81xa8xe5x85xa5xe5x8ax9bxe6x96x87xe3x81xaexe9x96x93xe3x81xaexe9xa1x9exe4xbcxbcxe5xbaxa6xe3x81xaf, xe5x80x99xe8xa3x9cxe6x96x87xe3x81xa8xe5x85xa5xe5x8ax9bxe6x96x87xe3x81xaexe9x96x93xe3x81xa7xe5x85xb1xe9x80x9axe3x81x99xe3x82x8bN-gramxe3x81xaexe6xafx94xe7x8ex87xe3x81xabxe5x9fxbaxe3x81xa5xe3x81x84xe3x81xa6xe7xaex97xe5x87xbaxe3x81x99xe3x82x8b.xe3x81x95xe3x82x89xe3x81xab, xe5x85xa5xe5x8ax9bxe6x96x87xe3x81xabxe3x81xaaxe3x81x84xe5x86x85xe5xaexb9xe8xaax9exe3x82x92xe5x90xabxe3x82x80xe5x80x99xe8xa3x9cxe6x96x87xe3x81xafxe5xafxbexe8xb1xa1xe5xa4x96xe3x81xa8xe3x81x99xe3x82x8bxe3x81x93xe3x81xa8xe3x82x84xe6xa9x9fxe8x83xbdxe8xaax9exe3x81xaexe9x87x8dxe3x81xbfxe3x82x92xe6xb8x9bxe3x82x89xe3x81x99xe3x81xa8xe3x81x84xe3x81xa3xe3x81x9fxe4xbbx98xe5x8axa0xe6x9dxa1xe4xbbxb6xe3x82x92xe5x8axa0xe3x81x88xe3x82x8bxe3x81x93xe3x81xa8xe3x81xa7xe7xb2xbexe5xbaxa6xe5x90x91xe4xb8x8axe3x82x92xe5x9bxb3xe3x81xa3xe3x81x9f.xe6x97xa5xe6x9cxacxe8xaax9exe3x81xabxe3x81x8axe3x81x91xe3x82x8bxe9xa1x9exe4xbcxbcxe6x96x87xe6xa4x9cxe7xb4xa2xe3x81xaexe5xaex9fxe9xa8x93xe3x81xa7xe3x81xaf, xe4xb8x8exe3x81x88xe3x81x9fxe5x85xa5xe5x8ax9bxe6x96x87xe3x81xaexe5x86x8587T2%xe3x81xabxe3x81xa4xe3x81x84xe3x81xa6xe6xa4x9cxe7xb4xa2xe6x96x87xe3x82x92xe5x87xbaxe5x8ax9bxe3x81x97, xe3x81x9dxe3x82x8cxe3x82x89xe3x81xaexe6xa4x9cxe7xb4xa2xe6x96x87xe3x81xaexe5x86x8560.4%xe3x81xafxe9x81xa9xe5x88x87xe3x81xaaxe9xa1x9exe4xbcxbcxe6x96x87xe3x81xa7xe3x81x82xe3x81xa3xe3x81x9f.xe6xa9x9fxe6xa2xb0xe7xbfxbbxe8xa8xb3xe3x81xa8xe7xb5x84xe3x81xbfxe5x90x88xe3x82x8fxe3x81x9bxe3x81x9fxe5xaex9fxe9xa8x93xe3x81xa7xe3x81xaf, xe7xbfxbbxe8xa8xb3xe4xb8x8dxe8x83xbdxe3x81xa8xe3x81xaaxe3x81xa3xe3x81x9fxe5x85xa5xe5x8ax9bxe6x96x87xe3x81xabxe3x81xa4xe3x81x84xe3x81xa6xe9xa1x9exe4xbcxbcxe6x96x87xe3x82x92xe6xa4x9cxe7xb4xa2xe3x81x95xe3x81x9b, xe3x81x9dxe3x82x8cxe3x82x89xe3x82x92xe6xa9x9fxe6xa2xb0xe7xbfxbbxe8xa8xb3xe3x81xabxe3x81x8bxe3x81x91xe3x81x9fxe3x81xa8xe3x81x93xe3x82x8d, xe7xbfxbbxe8xa8xb3xe4xb8x8dxe8x83xbdxe6x96x87xe3x81xaexe5x86x8525.9%xe3x81xabxe3x81xa4xe3x81x84xe3x81xa6xe9x81xa9xe5x88x87xe3x81xaaxe8xa8xb3xe6x96x87xe3x82x92xe5xbex97xe3x82x8bxe3x81x93xe3x81xa8xe3x81x8cxe3x81xa7xe3x81x8dxe3x81x9f."
W03-3023,Statistical Dependency Analysis with Support Vector Machines,2003,11,521,2,0,51406,hiroyasu yamada,Proceedings of the Eighth International Conference on Parsing Technologies,0,"In this paper, we propose a method for analyzing word-word dependencies using deterministic bottom-up manner using Support Vector machines. We experimented with dependency trees converted from Penn treebank data, and achieved over 90{\%} accuracy of word-word dependency. Though the result is little worse than the most up-to-date phrase structure based parsers, it looks satisfactorily accurate considering that our parser uses no information from phrase structures."
W03-2604,Incorporating Contextual Cues in Trainable Models for Coreference Resolution,2003,-1,-1,4,1,12930,ryu iida,Proceedings of the 2003 {EACL} Workshop on The Computational Treatment of Anaphora,0,None
W03-1720,Combining Segmenter and Chunker for {C}hinese Word Segmentation,2003,3,21,4,1,13282,masayuki asahara,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"Our proposed method is to use a Hidden Markov Model-based word segmenter and a Support Vector Machine-based chunker for Chinese word segmentation. Firstly, input sentences are analyzed by the Hidden Markov Model-based word segmenter. The word segmenter produces n-best word candidates together with some class information and confidence measures. Secondly, the extracted words are broken into character units and each character is annotated with the possible word class and the position in the word, which are then used as the features for the chunker. Finally, the Support Vector Machine-based chunker brings character units together into words so as to determine the word boundaries."
W03-1309,Protein Name Tagging for Biomedical Annotation in Text,2003,14,42,4,1,51335,kaoru yamamoto,Proceedings of the {ACL} 2003 Workshop on Natural Language Processing in Biomedicine,0,"We explore the use of morphological analysis as preprocessing for protein name tagging. Our method finds protein names by chunking based on a morpheme, the smallest unit determined by the morphological analysis. This helps to recognize the exact boundaries of protein names. Moreover, our morphological analyzer can deal with compounds. This offers a simple way to adapt name descriptions from biomedical resources for language processing. Using GENIA corpus 3.01, our method attains f-score of 70 points for protein molecule names, and 75 points for protein names including molecules, families and domains."
W03-1107,Feature Selection in Categorizing Procedural Expressions,2003,15,25,3,0,52705,mineki takechi,Proceedings of the Sixth International Workshop on Information Retrieval with {A}sian Languages,0,"Text categorization, as an essential component of applications for user navigation on the World Wide Web using Question-Answering in Japanese, requires more effective features for the categorization of documents and the efficient acquisition of knowledge. In the questions addressed by such navigation, we focus on those questions for procedures and intend to clarify specification of the answers."
W03-0311,Retrieving Meaning-equivalent Sentences for Example-based Rough Translation,2003,17,2,3,1,51054,mitsuo shimohata,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,0,"Example-based machine translation (EBMT) is a promising translation method for speech-to-speech translation because of its robustness. It retrieves example sentences similar to the input and adjusts their translations to obtain the output. However, it has problems in that the performance degrades when input sentences are long and when the style of inputs and that of the example corpus are different. This paper proposes a method for retrieving meaning-equivalent sentences to overcome these two problems. A meaning-equivalent sentence shares the main meaning with an input despite lacking some unimportant information. The translations of meaning-equivalent sentences correspond to rough translations. The retrieval is based on content words, modality, and tense."
W03-0314,Learning Sequence-to-Sequence Correspondences from Parallel Corpora via Sequential Pattern Mining,2003,13,9,4,1,51335,kaoru yamamoto,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,0,"We present an unsupervised extraction of sequence-to-sequence correspondences from parallel corpora by sequential pattern mining. The main characteristics of our method are two-fold. First, we propose a systematic way to enumerate all possible translation pair candidates of rigid and gapped sequences without falling into combinatorial explosion. Second, our method uses an efficient data structure and algorithm for calculating frequencies in a contingency table for each translation pair candidate. Our method is empirically evaluated using English-Japanese parallel corpora of 6 million words. Results indicate that it works well for multi-word translations, giving 56--84% accuracy at 19% token coverage and 11% type coverage."
P03-2039,{C}hinese Unknown Word Identification Using Character-based Tagging and Chunking,2003,3,43,3,0,46757,chooi goh,The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,0,"Since written Chinese has no space to delimit words, segmenting Chinese texts becomes an essential task. During this task, the problem of unknown word occurs. It is impossible to register all words in a dictionary as new words can always be created by combining characters. We propose a unified solution to detect unknown words in Chinese texts. First, a morphological analysis is done to obtain initial segmentation and POS tags and then a chunker is used to detect unknown words."
P03-1004,Fast Methods for Kernel-Based Text Analysis,2003,13,186,2,1,29083,taku kudo,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP). In NLP, although feature combinations are crucial to improving performance, they are heuristically selected. Kernel methods change this situation. The merit of the kernel methods is that effective feature combination is implicitly expanded without loss of generality and increasing the computational costs. Kernel-based text analysis shows an excellent performance in terms in accuracy; however, these methods are usually too slow to apply to large-scale text analysis. In this paper, we extend a Basket Mining algorithm to convert a kernel-based classifier into a simple and fast linear classifier. Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel-based classifiers."
P03-1057,Feedback Cleaning of Machine Translation Rules Using Automatic Evaluation,2003,11,51,3,0,324,kenji imamura,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"When rules of transfer-based machine translation (MT) are automatically acquired from bilingual corpora, incorrect/redundant rules are generated due to acquisition errors or translation variety in the corpora. As a new countermeasure to this problem, we propose a feedback cleaning method using automatic evaluation of MT quality, which removes incorrect/redundant rules as a way to increase the evaluation score. BLEU is utilized for the automatic evaluation. The hill-climbing algorithm, which involves features of this task, is applied to searching for the optimal combination of rules. Our experiments show that the MT quality improves by 10% in test sentences according to a subjective evaluation. This is considerable improvement over previous methods."
N03-1002,{J}apanese Named Entity Extraction with Redundant Morphological Analysis,2003,6,140,2,1,13282,masayuki asahara,Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Named Entity (NE) extraction is an important subtask of document processing such as information extraction and question answering. A typical method used for NE extraction of Japanese texts is a cascade of morphological analysis, POS tagging and chunking. However, there are some cases where segmentation granularity contradicts the results of morphological analysis and the building units of NEs, so that extraction of some NEs are inherently impossible in this setting. To cope with the unit problem, we propose a character-based chunking method. Firstly, the input sentence is analyzed redundantly by a statistical morphological analyzer to produce multiple (n-best) answers. Then, each character is annotated with its character types and its possible POS tags of the top n-best answers. Finally, a support vector machine-based chunker picks up some portions of the input sentence as NEs. This method introduces richer information to the chunker than previous methods that base on a single morphological analysis result. We apply our method to IREX NE extraction task. The cross validation result of the F-measure being 87.2 shows the superiority and effectiveness of the method."
E03-1029,Automatic Construction of Machine Translation Knowledge Using Translation Literalness,2003,10,14,3,0,324,kenji imamura,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"When machine translation (MT) knowledge is automatically constructed from bilingual corpora, redundant rules are acquired due to translation variety. These rules increase ambiguity or cause incorrect MT results. To overcome this problem, we constrain the sentences used for knowledge extraction to the appropriate bilingual sentences for the MT. In this paper, we propose a method using translation literalness to select appropriate sentences or phrases. The translation correspondence rate (TCR) is defined as the literalness measure.Based on the TCR, two automatic construction methods are tested. One is to filter the corpus before rule acquisition. The other is to split the acquisition process into two phases, where a bilingual sentence is divided into literal parts and the other parts before different generalizations are applied. The effects are evaluated by the MT quality, and about 4.9% of MT results were improved by the latter method."
2003.mtsummit-papers.47,Example-based rough translation for speech-to-speech translation,2003,-1,-1,3,1,51054,mitsuo shimohata,Proceedings of Machine Translation Summit IX: Papers,0,"Example-based machine translation (EBMT) is a promising translation method for speech-to-speech translation (S2ST) because of its robustness. However, it has two problems in that the performance degrades when input sentences are long and when the style of the input sentences and that of the example corpus are different. This paper proposes example-based rough translation to overcome these two problems. The rough translation method relies on {``}meaning-equivalent sentences,{''} which share the main meaning with an input sentence despite missing some unimportant information. This method facilitates retrieval of meaning-equivalent sentences for long input sentences. The retrieval of meaning-equivalent sentences is based on content words, modality, and tense. This method also provides robustness against the style differences between the input sentence and the example corpus."
W02-2016,{J}apanese Dependency Analysis using Cascaded Chunking,2002,6,405,2,1,29083,taku kudo,{COLING}-02: The 6th Conference on Natural Language Learning 2002 ({C}o{NLL}-2002),0,"In this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model. Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable. We propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side. Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency."
W02-2028,Two-dimensional Clustering for Text Categorization,2002,13,13,2,1,4800,hiroya takamura,{COLING}-02: The 6th Conference on Natural Language Learning 2002 ({C}o{NLL}-2002),0,"We propose a new method to improve the accuracy of Text Categorization using two-dimensional clustering. In a number of previous probabilistic approaches, texts in the same category are implicitly assumed to be generated from an identical distribution. We empirically show that this assumption is not accurate, and propose a new framework based on two-dimensional clustering to alleviate this problem. In our method, training texts are clustered so that the assumption is more likely to be true, and at the same time, features are also clustered in order to tackle the data sparseness problem. We conduct some experiments to validate the proposed two-dimensional clustering method."
P02-1059,Supervised Ranking in Open-Domain Text Summarization,2002,15,6,2,1,5972,tadashi nomoto,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"The paper proposes and empirically motivates an integration of supervised learning with unsupervised learning to deal with human biases in summarization. In particular, we explore the use of probabilistic decision tree within the clustering framework to account for the variation as well as regularity in human created summaries. The corpus of human created extracts is created from a newspaper corpus and used as a test set. We build probabilistic decision trees of different flavors and integrate each of them with the clustering framework. Experiments with the corpus demonstrate that the mixture of the two paradigms generally gives a significant boost in performance compared to cases where either of the two is considered alone."
P02-1063,Revision Learning and its Application to Part-of-Speech Tagging,2002,17,18,3,1,27752,tetsuji nakagawa,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents a revision learning method that achieves high performance with small computational cost by combining a model with high generalization capacity and a model with small computational cost. This method uses a high capacity model to revise the output of a small cost model. We apply this method to English part-of-speech tagging and Japanese morphological analysis, and show that the method performs well."
asahara-etal-2002-use,Use of {XML} and Relational Databases for Consistent Development and Maintenance of Lexicons and Annotated Corpora,2002,4,0,5,1,13282,masayuki asahara,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"Abstract In this paper, we present a use of XML and relational database for developing and maintaining Japanese linguistic resources. In languages that do not provide word delimitation in texts (e.g. Chinese and Japanese), consistent delimitation definition of words in a lexicon is a critical issue to build POS tagged corpora. When we change the definition of word delimitation in the lexicon, we need to modify the tagged corpora to make them consistent with the lexicon. We propose a use of relational database to perform these modifications in tandem. Hence, in the Japanese language, there are several standards for word delimitation definition. To accommodate more than one definition of word delimitation, we compose a compounding word lexicon in the database. The compounding word lexicon includes dependency structures of compounding words."
C02-1053,Extracting Important Sentences with Support Vector Machines,2002,16,62,4,0,3632,tsutomu hirao,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"Extracting sentences that contain important information from a document is a form of text summarization. The technique is the key to the automatic generation of summaries similar to those written by humans. To achieve such extraction, it is important to be able to integrate heterogeneous pieces of information. One approach, parameter tuning by machine learning, has been attracting a lot of attention. This paper proposes a method of sentence extraction based on Support Vector Machines (SVMs). To confirm the method's performance, we conduct experiments that compare our method to three existing methods. Results on the Text Summarization Challenge (TSC) corpus show that our method offers the highest accuracy. Moreover, we clarify the different features effective for extracting different document genres."
C02-1101,Detecting Errors in Corpora Using Support Vector Machines,2002,12,25,2,1,27752,tetsuji nakagawa,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"While the corpus-based research relies on human annotated corpora, it is often said that a non-negligible amount of errors remain even in frequently used corpora such as Penn Treebank. Detection of errors in annotated corpora is important for corpus-based natural language processing. In this paper, we propose a method to detect errors in corpora using support vector machines (SVMs). This method is based on the idea of extracting exceptional elements that violate consistency. We propose a method of using SVMs to assign a weight to each element and to find errors in a POS tagged corpus. We apply the method to English and Japanese POS-tagged corpora and achieve high precision in detecting errors."
Y01-1028,An {HPSG} Account of the Hierarchical Clause Formation in {J}apanese : {HPSG}-Based {J}apanese Grammar for Practical Parsing,2001,0,0,3,1,50739,takashi miyata,"Proceedings of the 15th Pacific Asia Conference on Language, Information and Computation",0,None
W01-1412,A Comparative Study on Translation Units for Bilingual Lexicon Extraction,2001,9,38,2,1,51335,kaoru yamamoto,Proceedings of the {ACL} 2001 Workshop on Data-Driven Methods in Machine Translation,0,"This paper presents on-going research on automatic extraction of bilingual lexicon from English-Japanese parallel corpora. The main objective of this paper is to examine various N-gram models of generating translation units for bilingual lexicon extraction. Three N-gram models, a baseline model (Bound-length N-gram) and two new models (Chunk-bound N-gram and Dependency-linked N-gram) are compared. An experiment with 10000 English-Japanese parallel sentences shows that Chunk-bound N-gram produces the best result in terms of accuracy (83%) as well as coverage (60%) and it improves approximately by 13% in accuracy and by 5-9% in coverage from the previously proposed baseline model."
W01-0507,Feature Space Restructuring for {SVM}s with Application to Text Categorization,2001,13,14,2,1,4800,hiroya takamura,Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,0,None
N01-1025,Chunking with Support Vector Machines,2001,24,484,2,1,29083,taku kudo,Second Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We apply Support Vector Machines (SVMs) to identify English base phrases (chunks). SVMs are known to achieve high generalization performance even with input data of high dimensional feature spaces. Furthermore, by the Kernel principle, SVMs can carry out training with smaller computational overhead independent of their dimensionality. We apply weighted voting of 8 SVMs-based systems trained with distinct chunk representations. Experimental results show that our approach achieves higher accuracy than previous approaches."
W00-1303,{J}apanese Dependency Structure Analysis Based on Support Vector Machines,2000,14,91,2,1,29083,taku kudo,2000 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,"This paper presents a method of Japanese dependency structure analysis based on Support Vector Machines (SVMs). Conventional parsing techniques based on Machine Learning framework, such as Decision Trees and Maximum Entropy Models, have difficulty in selecting useful features as well as finding appropriate combination of selected features. On the other hand, it is well-known that SVMs achieve high generalization performance even with input data of very high dimensional feature space. Furthermore, by introducing the Kernel principle, SVMs can carry out the training in high-dimensional spaces with a smaller computational cost independent of their dimensionality. We apply SVMs to Japanese dependency structure identification problem. Experimental results on Kyoto University corpus show that our system achieves the accuracy of 89.09% even with small training data (7958 sentences)."
W00-0803,{C}hinese-{J}apanese Cross Language Information Retrieval: A {H}an Character Based Approach,2000,22,10,2,0,54256,maruf hasan,{ACL}-2000 Workshop on Word Senses and Multi-linguality,0,"In this paper, we investigate cross language information retrieval (CLIR) for Chinese and Japanese texts utilizing the Han characters -- common ideographs used in writing Chinese, Japanese and Korean (CJK) languages. The Unicode encoding scheme, which encodes the superset of Han characters, is used as a common encoding platform to deal with the multilingual collection in a uniform manner. We discuss the importance of Han character semantics in document indexing and retrieval of the ideographic languages. We also analyse the baseline results of the cross language information retrieval using the common Han characters appeared in both Chinese and Japanese texts."
W00-0730,Use of Support Vector Learning for Chunk Identification,2000,6,252,2,0,54268,taku kudoh,Fourth Conference on Computational Natural Language Learning and the Second Learning Language in Logic Workshop,0,"In this paper, we explore the use of Support Vector Machines (SVMs) for CoNLL-2000 shared task, chunk identification. SVMs are so-called large margin classifiers and are well-known as their good generalization performance. We investigate how SVMs with a very large number of features perform with the classification task of chunk labelling."
O00-3003,{J}apanese-{C}hinese Cross-Language Information Retrieval: An Interlingua Approach,2000,33,6,2,0,9725,md hasan,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 5, Number 2, August 2000",0,"Electronically available multilingual information can be divided into two major categories: (1) alphabetic language information (English-like alphabetic languages) and (2) ideographic language information (Chinese-like ideographic languages). The information available in non-English alphabetic languages as well as in ideographic languages (especially, in Japanese and Chinese) is growing at an incredibly high rate in recent years. Due to the ideographic nature of Japanese and Chinese, complicated with the existence of several encoding standards in use, efficient processing (representation, indexing, retrieval, etc.) of such information became a tedious task. In this paper, we propose a Han Character (Kanji) oriented Interlingua model of indexing and retrieving Japanese and Chinese information. We report the results of mono- and cross- language information retrieval on a Kanji space where documents and queries are represented in terms of Kanji oriented vectors. We also employ a dimensionality reduction technique to compute a Kanji Conceptual Space (KCS) from the initial Kanji space, which can facilitate conceptual retrieval of both mono- and cross- language information for these languages. Similar indexing approaches for multiple European languages through term association (e.g., latent semantic indexing) or through conceptual mapping (using lexical ontology such as, WordNet) are being intensively explored. The Interlingua approach investigated here with Japanese and Chinese languages, and the term (or concept) association model investigated with the European languages are similar; and these approaches can be easily integrated. Therefore, the proposed Interlingua model can pave the way for handling multilingual information access and retrieval efficiently and uniformly."
matsumoto-yamashita-2000-using,Using Machine Learning Methods to Improve Quality of Tagged Corpora and Learning Models,2000,14,2,1,1,991,yuji matsumoto,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"Corpus-based learning methods for natural language processing now provide a consistent way to achieve systems with good performance. A number of statistical learning models have been proposed and are used in most of the tasks which used to be handled by rule-based systems. When the learning systems come to such a level as competitive as manually constructed systems, both large scale training corpora and good learning models are of great importance. In this paper, we first discuss that the main hindrances to the improvement of corpus-based learning systems are the inconsistencies or the errors existing in the training corpus and the defectiveness in the learning model. We then show that some machine learning methods are useful for effective identification of the erroneous source in the training corpus. Finally, we discuss how the various types of errors should be coped with so as to improve the learning environments."
C00-2135,Acquisition of Phrase-level Bilingual Correspondence using Dependency Structure,2000,9,55,2,1,51335,kaoru yamamoto,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,This paper describes a method to find phrase-level translation patterns from parallel corpora by applying dependency structure analysis. We use statistical dependency parsers to determine dependency relations between base phrases in a sentence. Our method is tested with a business expression corpus containing 10000 English-Japanese sentence pairs and achieved approximately 90% accuracy in extracting bilingual correspondences. The result shows that the use of dependency relation helps to acquire interesting translation patterns.
C00-1004,Extended Models and Tools for High-performance Part-of-speech,2000,2,3,2,1,13282,masayuki asahara,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,None
A00-2015,Analyzing Dependencies of {J}apanese Subordinate Clauses based on Statistics of Scope Embedding Preference,2000,7,6,4,0.846576,15895,takehito utsuro,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper proposes a statistical method for learning dependency preference of Japanese subordinate clauses, in which scope embedding preference of subordinate clauses is exploited as a useful information source for disambiguating dependencies between subordinate clauses. Estimated dependencies of subordinate clauses successfully increase the precision of an existing statistical dependency analyzer."
A00-1032,Language Independent Morphological Analysis,2000,13,16,2,0,54511,tatsuo yamashita,Sixth Applied Natural Language Processing Conference,0,"This paper proposes a framework of language independent morphological analysis and mainly concentrate on tokenization, the first process of morphological analysis. Although tokenization is usually not regarded as a difficult task in most segmented languages such as English, there are a number of problems in achieving precise treatment of lexical entries. We first introduce the concept of morpho-fragments, which are intermediate units between characters and lexical entries. We describe our approach to resolve problems arising in tokenization so as to attain a language independent morphological analyzer."
W99-0620,Learning Discourse Relations with Active Data Selection,1999,5,4,2,1,5972,tadashi nomoto,1999 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,None
W98-1511,{J}apanese Dependency Structure Analysis based on Lexicalized Statistics,1998,1,29,2,0,54704,masakazu fujio,Proceedings of the Third Conference on Empirical Methods for Natural Language Processing,0,None
W98-1304,Robust Parsing Using a Hidden {M}arkov Model,1998,11,1,2,1,55129,wide hogenhout,Finite State Methods in Natural Language Processing,0,"Recent approaches to statistical parsing include those that estimate an approximation of a stochastic, lexicalized grammar directly from a treebank and others that rebuild trees with a number of tree-constructing operators, which are applied in order according to a stochastic model when parsing a sentence. In this paper we take an entirely different approach to statistical parsing, as we propose a method for parsing using a Hidden Markov Model. We describe the stochastic model and the tree construction procedure, and we report results on the Wall Street Journal Corpus."
W98-1125,Discourse Parsing: A Decision Tree Approach,1998,12,8,2,1,5972,tadashi nomoto,Sixth Workshop on Very Large Corpora,0,None
P98-2163,Recognition of the Coherence Relation between Te-linked Clauses,1998,12,0,2,1,55279,akira oishi,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper describes a method for recognizing coherence relations between clauses which are linked by te in Japanese-a translational equivalent of English and. We consider that the coherence relations are categories each of which has a prototype structure as well as the relationships among them. By utilizing this organization of the relations, we can infer an appropriate relation from the semantic structures of the clauses between which that relation holds. We carried out an experiment and obtained the correct recognition ratio of 82% for the 280 sentences."
P98-2214,General-to-Specific Model Selection for Subcategorization Preference,1998,11,2,3,1,15895,takehito utsuro,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper proposes a novel method for learning probability models of subcategorization preference of verbs. We consider the issues of case dependencies and noun class generalization in a uniform way by employing the maximum entropy modeling method. We also propose a new model selection algorithm which starts from the most general model and gradually examines more specific models. In the experimental evaluation, it is shown that both of the case dependencies and specific sense restriction selected by the proposed method contribute to improving the performance in subcategorization preference resolution."
C98-2158,Recognition of the Coherence Relation between Te-linked Clauses,1998,12,0,2,1,55279,akira oishi,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper describes a method for recognizing coherence relations between clauses which are linked by te in Japanese-a translational equivalent of English and. We consider that the coherence relations are categories each of which has a prototype structure as well as the relationships among them. By utilizing this organization of the relations, we can infer an appropriate relation from the semantic structures of the clauses between which that relation holds. We carried out an experiment and obtained the correct recognition ratio of 82% for the 280 sentences."
C98-2209,General-to-Specific Model Selection for Subcategorization Preference,1998,11,2,3,1,15895,takehito utsuro,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper proposes a novel method for learning probability models of subcategorization preference of verbs. We consider the issues of case dependencies and noun class generalization in a uniform way by employing the maximum entropy modeling method. We also propose a new model selection algorithm which starts from the most general model and gradually examines more specific models. In the experimental evaluation, it is shown that both of the case dependencies and specific sense restriction selected by the proposed method contribute to improving the performance in subcategorization preference resolution."
C98-2222,Head-Driven Generation with {HPSG},1998,7,6,2,0.833333,33587,graham wilcock,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"As HPSG is head-driven, with clear semantic heads, semantic head-driven generation should be simple. We adapt van Noord's Prolog generator for use with an HPSG grammar in ProFIT. However, quantifiers and context factors are difficult to include in head-driven generation. We must adopt recent theoretical proposals for lexicalized scoping and context. With these revisions, head-driven generation with HPSG is not so simple, but it is possible."
W97-1003,A Preliminary Study of Word Clustering Based on Syntactic Behavior,1997,15,5,2,1,55129,wide hogenhout,{C}o{NLL}97: Computational Natural Language Learning,0,We show how a treebank can be used to cluster words on the basis of their syntactic behavior. The resulting clusters represent distinct types of behavior with much more precision than parts of speech. As an example we show how prepositions can be automatically subdivided by their syntactic behavior and discuss the appropriateness of such a subdivision. Applications of this work are also discussed. S works verb
W97-0113,Data Reliability and Its Effects on Automatic Abstracting,1997,12,7,2,1,5972,tadashi nomoto,Fifth Workshop on Very Large Corpora,0,None
P97-1030,Mistake-Driven Mixture of Hierarchical Tag Context Trees,1997,18,10,2,0,55350,masahiko haruno,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"This paper proposes a mistake-driven mixture method for learning a tag model. The method iteratively performs two procedures: 1. constructing a tag model based on the current data distribution and 2. updating the distribution by focusing on data that are not well predicted by the constructed model. The final tag model is constructed by mixing all the models according to their performance. To well reflect the data distribution, we represent each tag model as a hierarchical tag (i.e., NTT <proper noun <noun) context tree. By using the hierarchical tag context tree, the constituents of sequential tag models gradually change from broad coverage tags (e.g., noun) to specific exceptional words that cannot be captured by general tags. In other words, the method incorporates not only frequent connetions but also infrequent ones that are often considered to be collocational. We evaluate several tag models by implementing Japanese part-of-speech taggers that share all other conditions (i.e., dictionary and word model) other than their tag models. The experimental results show the proposed method significantly outperforms both hand-crafted and conventional statistical methods."
P97-1045,Automatic Extraction of Aspectual Information from a Monolingual Corpus,1997,12,0,2,1,55279,akira oishi,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"This paper describes an approach to extract the aspectual information of Japanese verb phrases from a monoligual corpus. We classify verbs into six categories by means of the aspectual features which are defined on the basis of the possibility of co-occurrence with aspectual forms and adverbs. A unique category could be identified for 96% of the target verbs. To evaluate the result of the experiment, we examined the meaning of -teiru which is one of the most fundamental aspectual markers in Japanese, and obtained the correct recognition score of 71% for the 200 sentences."
A97-1053,Learning Probabilistic Subcategorization Preference by Identifying Case Dependencies and Optimal Noun Class Generalization Level,1997,13,10,2,1,15895,takehito utsuro,Fifth Conference on Applied Natural Language Processing,0,"This paper proposes a novel method of learning probabilistic subcategorization preference. In the method, for the purpose of coping with the ambiguities of case dependencies and noun class generalization of argument/adjunct nouns, we introduce a data structure which represents a tuple of independent partial subcategorization frames. Each collocation of a verb and argument/adjunct nouns is assumed to be generated from one of the possible tuples of independent partial subcategorization frames. Parameters of subcategorization preference are then estimated so as to maximize the subcategorization preference function for each collocation of a verb and argument/adjunct nouns in the training corpus. We also describe the results of the experiments on learning probabilistic subcategorization preference from the EDR Japanese bracketed corpus, as well as those on evaluating the performance of subcategorization preference."
Y96-1024,A Proposal of {K}orean Conjugation System and its Application to Morphological Analysis,1996,0,1,2,0,55824,yoshitaka hirano,"Proceedings of the 11th Pacific Asia Conference on Language, Information and Computation",0,"This paper presents a new Korean verb conjugation system, which enables an easy treatment of Korean morphological phenomena such as contraction. This makes the size of the dictionary for ending forms to be small. We also introduce a Korean morphological analysis system. Korean morphological analysis system generally analyzes sentences within the segments(a part between spaces). We propose a system that considers the information beyond segmentation."
Y96-1040,Fast Statistical Grammar Induction,1996,11,0,2,1,55129,wide hogenhout,"Proceedings of the 11th Pacific Asia Conference on Language, Information and Computation",0,"The statistical induction of context free grammars from bracketed corpora with the Inside Outside Algorithm has often inspired researchers, but the computational complexity has made it impossible to generate a large scale grammar. The method we suggest achieves the same results as earlier research, but at a much smaller expense in computer time. We explain the modifications needed to the algorithm, give results of experiments and compare these to results reported in other literature."
W96-0107,Automatic Extraction of Word Sequence Correspondences in Parallel Corpora,1996,5,43,2,0,52999,mihoko kitamura,Fourth Workshop on Very Large Corpora,0,None
W96-0109,Exploiting Text Structure for Topic Identification,1996,14,7,2,1,5972,tadashi nomoto,Fourth Workshop on Very Large Corpora,0,None
C96-2128,Reversible delayed lexical choice in a bidirectional framework,1996,12,4,2,0.833333,33587,graham wilcock,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"We describe a bidirectional framework for natural language parsing and generation, using a typed feature formalism and an HPSG-based grammar with a parser and generator derived from parallel processing algorithms. We present an approach to delayed lexical choice in generation, based on subsumption within the sort hierarchy, using a lexicon of under-instantiated signs which are derived from the normal lexicon by lexical rules. We then show how delayed lexical choice can be used in parsing, so that some types of ill-formed inputs can be parsed, but well-formed outputs are generated, using the same shared linguistic information."
C96-1095,Towards a More Careful Evaluation of Broad Coverage Parsing Systems,1996,8,2,2,1,55129,wide hogenhout,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"Since treebanks have become available to researchers a wide variety of techniques has been used to make broad coverage parsing systems. This makes quantitative evaluation very important, but the current evaluation methods have a number of drawbacks such as arbitrary choices in the treebank and the difficulty in measuring statistical significance. We suggest a more detailed method for testing a parsing system using constituent boundaries, with a number of measures that give more information than current measures, and evaluate the quality of the test. We also show that statistical significance cannot be calculated in a straightforward way, and suggest a calculation method for the case of Bracket Recall."
Y95-1022,{HMM} Parameter Learning for {J}apanese Morphological Analyzer,1995,0,0,2,0,17433,koichi takeuchi,"Proceedings of the 10th Pacific Asia Conference on Language, Information and Computation",0,None
C94-2175,"Bilingual Text, Matching using Bilingual Dictionary and Statistics",1994,16,33,4,0.980392,15895,takehito utsuro,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,This paper describes a unified framework for bilingual text matching by combining existing hand-written bilingual dictionaries and statistical techniques. The process of bilingual text matching consists of two major steps: sentence alignment and structural matching of bilingual sentences. Statistical techniques are applied to estimate word correspondences not included in bilingual dictionaries. Estimated word correspondences are useful for improving both sentence alignment and structural matching.
P93-1004,Structural Matching of Parallel Texts,1993,11,96,1,1,991,yuji matsumoto,31st Annual Meeting of the Association for Computational Linguistics,1,"This paper describes a method for finding structural matching between parallel sentences of two languages, (such as Japanese and English). Parallel sentences are analyzed based on unification grammars, and structural matching is performed by making use of a similarity measure of word pairs in the two languages. Syntactic ambiguities are resolved simultaneously in the matching process. The results serve as a useful source for extracting linguistic and lexical knowledge."
C92-2088,Lexical Knowledge Acquisition from Bilingual Corpora,1992,8,20,2,0,15895,takehito utsuro,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"For practical research in natural language processing, it is indispensable to develop a large scale semantic dictionary for computers. It is especially important to improve the techniques for compiling semantic dictionaries from natural language texts such as those in existing human dictionaries or in large corpora. However, there are at least two difficulties in analyzing existing texts: the problem of syntactic ambiguities and the problem of polysemy. Our approach to solve these difficulties is to make use of translation examples in two distinct languages that have quite different syntactic structures and word meanings. The reason we took this approach is that in many cases both syntactic and semantic ambiguities are resolved by comparing analyzed results from both languages. In this paper, we propose a method for resolving the syntactic ambiguities of translation examples of bilingual corpora and a method for acquiring lexical knowledge, such as case frames of verbs and attribute sets of nouns."
