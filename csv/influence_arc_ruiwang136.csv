2020.acl-main.295,D14-1082,0,0.0348846,"d to lead to lower accuracy scores. 5.3.3 Effect of Different Parsers Restaurant Laptop 78.21 73.04 79.91 72.72 78.57 83.30 81.16 72.10 77.42 73.66 Twitter 71.67 71.76 71.82 75.57 70.95 Table 4: Results of ablation study, where “Ordinary” means using ordinary dependency trees, “Reshaped” denotes using the aspect-oriented trees, and “*-n:con” denote the aspect-oriented tree without using n:con. Dependency parsing plays a critical role in our method. To evaluate the impact of different parsers, we conduct a study based on the R-GAT model using two well-known dependency parsers: Stanford Parser (Chen and Manning, 2014) and Biafﬁne Parser (Dozat and Manning, 2016).6 Table 3 shows the performance of the two parsers in UAS and LAS metrics, followed by their performance for aspect-based sentiment analysis. From the table, we can ﬁnd that the better Biafﬁne parser results in higher sentiment classiﬁcation accuracies. Moreover, it further implies that while existing parsers can capture most of the syntactic structures correctly, our method has the potential to be further improved with the advances of parsing techniques. 6 The parsers are implemented by Stanford CoreNLP (Manning et al., 2014) and AllenNLP (Gardner"
2020.acl-main.295,D17-1047,0,0.227053,"The source code of this work is released for future research.1 2 Related Work Most recent research work on aspect-based sentiment analysis (ABSA) utilizes attention-based neural models to examine words surrounding a target aspect. They can be considered an implicit approach to exploiting sentence structure, since opinion words usually appear not far from aspects. Such approaches have led to promising progress. Among them, Wang et al. (2016b) proposed to use an attention-based LSTM to identify important sentiment information relating to a target aspect. 1 https://github.com/shenwzh3/RGAT-ABSA Chen et al. (2017) introduced a multi-layer attention mechanism to capture long-distance opinion words for aspects. For a similar purpose, Tang et al. (2016) employed Memory Network with multi-hop attention and external memory. Fan et al. (2018) proposed a multi-grained attention network with both ﬁne-grained and coarse-grained attentions. The pre-trained language model BERT (Devlin et al., 2018) has made successes in many classiﬁcation tasks including ABSA. For example, Xu et al. (2019) used an additional corpus to posttrain BERT and proved its effectiveness in both aspect extraction and ABSA. Sun et al. (2019"
2020.acl-main.295,P14-2009,0,0.754024,"mechanisms could attend to terrible with a high weight when evaluating the aspect noodles. Some other efforts explicitly leverage the syntactic structure of a sentence to establish the connections. Among them, early attempts rely on handcrafted syntactic rules (Qiu et al., 2011; Liu et al., 2013), though they are subject to the quantity and quality of the rules. Dependency-based parse trees are then used to provide more comprehensive syntactic information. For this purpose, a whole dependency tree can be encoded from leaves to root by a recursive neural network (RNN) (Lakkaraju et al., 2014; Dong et al., 2014; Nguyen and Shirai, 2015; Wang et al., 2016a), or the internal node distance can be computed and used for attention weight decay (He et al., 2018a). Recently, graph neural networks (GNNs) are explored to learn representations from the dependency trees (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019). The shortcomings of these approaches should not be overlooked. First, the dependency relations, which may indicate the connections between aspects and opinion words, are ignored. Second, empirically, only a small part of the parse tree is related to this 3229 Proceedings of the 58t"
2020.acl-main.295,P81-1022,0,0.536137,"Missing"
2020.acl-main.295,D18-1380,0,0.225842,"Missing"
2020.acl-main.295,W18-2501,0,0.031423,"Missing"
2020.acl-main.295,C18-1096,0,0.778247,"ucture of a sentence to establish the connections. Among them, early attempts rely on handcrafted syntactic rules (Qiu et al., 2011; Liu et al., 2013), though they are subject to the quantity and quality of the rules. Dependency-based parse trees are then used to provide more comprehensive syntactic information. For this purpose, a whole dependency tree can be encoded from leaves to root by a recursive neural network (RNN) (Lakkaraju et al., 2014; Dong et al., 2014; Nguyen and Shirai, 2015; Wang et al., 2016a), or the internal node distance can be computed and used for attention weight decay (He et al., 2018a). Recently, graph neural networks (GNNs) are explored to learn representations from the dependency trees (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019). The shortcomings of these approaches should not be overlooked. First, the dependency relations, which may indicate the connections between aspects and opinion words, are ignored. Second, empirically, only a small part of the parse tree is related to this 3229 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3229–3238 c July 5 - 10, 2020. 2020 Association for Computational Linguis"
2020.acl-main.295,P18-1192,0,0.406684,"ucture of a sentence to establish the connections. Among them, early attempts rely on handcrafted syntactic rules (Qiu et al., 2011; Liu et al., 2013), though they are subject to the quantity and quality of the rules. Dependency-based parse trees are then used to provide more comprehensive syntactic information. For this purpose, a whole dependency tree can be encoded from leaves to root by a recursive neural network (RNN) (Lakkaraju et al., 2014; Dong et al., 2014; Nguyen and Shirai, 2015; Wang et al., 2016a), or the internal node distance can be computed and used for attention weight decay (He et al., 2018a). Recently, graph neural networks (GNNs) are explored to learn representations from the dependency trees (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019). The shortcomings of these approaches should not be overlooked. First, the dependency relations, which may indicate the connections between aspects and opinion words, are ignored. Second, empirically, only a small part of the parse tree is related to this 3229 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3229–3238 c July 5 - 10, 2020. 2020 Association for Computational Linguis"
2020.acl-main.295,D19-1549,0,0.67455,"y are subject to the quantity and quality of the rules. Dependency-based parse trees are then used to provide more comprehensive syntactic information. For this purpose, a whole dependency tree can be encoded from leaves to root by a recursive neural network (RNN) (Lakkaraju et al., 2014; Dong et al., 2014; Nguyen and Shirai, 2015; Wang et al., 2016a), or the internal node distance can be computed and used for attention weight decay (He et al., 2018a). Recently, graph neural networks (GNNs) are explored to learn representations from the dependency trees (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019). The shortcomings of these approaches should not be overlooked. First, the dependency relations, which may indicate the connections between aspects and opinion words, are ignored. Second, empirically, only a small part of the parse tree is related to this 3229 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3229–3238 c July 5 - 10, 2020. 2020 Association for Computational Linguistics task and it is unnecessary to encode the whole tree (Zhang et al., 2018; He et al., 2018b). Finally, the encoding process is tree-dependent, making the batch operati"
2020.acl-main.295,P18-1087,0,0.166499,"Missing"
2020.acl-main.295,P14-5010,0,0.00512719,"Missing"
2020.acl-main.295,D15-1298,0,0.0763525,"Missing"
2020.acl-main.295,D14-1162,0,0.0853482,"stics of the three datasets. 5.1 Datasets 5.3 Three public sentiment analysis datasets are used in our experiments, two of them are the Laptop and Restaurant review datasets from the SemEval 2014 Task (Maria Pontiki and Manandhar, 2014),4 and the third is the Twitter dataset used by (Dong et al., 2014). Statistics of the three datasets can be found in Table 1. 5.1.1 Implementation Details The Biafﬁne Parser (Dozat and Manning, 2016) is used for dependency parsing. The dimension of the dependency relation embeddings is set to 300. For R-GAT, we use the 300-dimensional word embeddings of GLoVe (Pennington et al., 2014). For R-GAT+BERT, we use the last hidden states of the pre-trained BERT for word representations and ﬁne-tune them on our task. The PyTorch implementation of BERT 5 is used in the experiments. R-GAT is shown to prefer a high dropout rate in between [0.6, 0.8]. As for R-GAT+BERT, it works better with a low dropout rate of around 0.2. Our model is trained using the Adam optimizer (Kingma and Ba, 2014) with the default conﬁguration. 5.2 Baseline Methods A few mainstream models for aspect-based sentiment analysis are used for comparison, including: • Syntax-aware models: LSTM+SynATT (He et al., 20"
2020.acl-main.295,J11-1002,0,0.498869,"occasionally. We illustrate this problem with a real review So delicious was the noodles but terrible vegetables, in which the opinion word terrible is closer to the aspect noodles than delicious, and there could be terrible noodles appearing in some other reviews which makes these two words closely associated. Therefore, the attention mechanisms could attend to terrible with a high weight when evaluating the aspect noodles. Some other efforts explicitly leverage the syntactic structure of a sentence to establish the connections. Among them, early attempts rely on handcrafted syntactic rules (Qiu et al., 2011; Liu et al., 2013), though they are subject to the quantity and quality of the rules. Dependency-based parse trees are then used to provide more comprehensive syntactic information. For this purpose, a whole dependency tree can be encoded from leaves to root by a recursive neural network (RNN) (Lakkaraju et al., 2014; Dong et al., 2014; Nguyen and Shirai, 2015; Wang et al., 2016a), or the internal node distance can be computed and used for attention weight decay (He et al., 2018a). Recently, graph neural networks (GNNs) are explored to learn representations from the dependency trees (Zhang et"
2020.acl-main.295,N19-1035,0,0.277461,", 2013), though they are subject to the quantity and quality of the rules. Dependency-based parse trees are then used to provide more comprehensive syntactic information. For this purpose, a whole dependency tree can be encoded from leaves to root by a recursive neural network (RNN) (Lakkaraju et al., 2014; Dong et al., 2014; Nguyen and Shirai, 2015; Wang et al., 2016a), or the internal node distance can be computed and used for attention weight decay (He et al., 2018a). Recently, graph neural networks (GNNs) are explored to learn representations from the dependency trees (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019). The shortcomings of these approaches should not be overlooked. First, the dependency relations, which may indicate the connections between aspects and opinion words, are ignored. Second, empirically, only a small part of the parse tree is related to this 3229 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3229–3238 c July 5 - 10, 2020. 2020 Association for Computational Linguistics task and it is unnecessary to encode the whole tree (Zhang et al., 2018; He et al., 2018b). Finally, the encoding process is tree-dependent"
2020.acl-main.295,D19-1569,0,0.591376,", 2013), though they are subject to the quantity and quality of the rules. Dependency-based parse trees are then used to provide more comprehensive syntactic information. For this purpose, a whole dependency tree can be encoded from leaves to root by a recursive neural network (RNN) (Lakkaraju et al., 2014; Dong et al., 2014; Nguyen and Shirai, 2015; Wang et al., 2016a), or the internal node distance can be computed and used for attention weight decay (He et al., 2018a). Recently, graph neural networks (GNNs) are explored to learn representations from the dependency trees (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019). The shortcomings of these approaches should not be overlooked. First, the dependency relations, which may indicate the connections between aspects and opinion words, are ignored. Second, empirically, only a small part of the parse tree is related to this 3229 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3229–3238 c July 5 - 10, 2020. 2020 Association for Computational Linguistics task and it is unnecessary to encode the whole tree (Zhang et al., 2018; He et al., 2018b). Finally, the encoding process is tree-dependent"
2020.acl-main.295,D18-1244,0,0.145782,"learn representations from the dependency trees (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019). The shortcomings of these approaches should not be overlooked. First, the dependency relations, which may indicate the connections between aspects and opinion words, are ignored. Second, empirically, only a small part of the parse tree is related to this 3229 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3229–3238 c July 5 - 10, 2020. 2020 Association for Computational Linguistics task and it is unnecessary to encode the whole tree (Zhang et al., 2018; He et al., 2018b). Finally, the encoding process is tree-dependent, making the batch operation inconvenient during optimization. In this paper, we re-examine the syntax information and claim that revealing task-related syntactic structures is the key to address the above issues. We propose a novel aspect-oriented dependency tree structure constructed in three steps. Firstly, we obtain the dependency tree of a sentence using an ordinary parser. Secondly, we reshape the dependency tree to root it at a target aspect in question. Lastly, pruning of the tree is performed to retain only edges with"
2020.acl-main.295,D16-1021,0,0.214872,"(ABSA) utilizes attention-based neural models to examine words surrounding a target aspect. They can be considered an implicit approach to exploiting sentence structure, since opinion words usually appear not far from aspects. Such approaches have led to promising progress. Among them, Wang et al. (2016b) proposed to use an attention-based LSTM to identify important sentiment information relating to a target aspect. 1 https://github.com/shenwzh3/RGAT-ABSA Chen et al. (2017) introduced a multi-layer attention mechanism to capture long-distance opinion words for aspects. For a similar purpose, Tang et al. (2016) employed Memory Network with multi-hop attention and external memory. Fan et al. (2018) proposed a multi-grained attention network with both ﬁne-grained and coarse-grained attentions. The pre-trained language model BERT (Devlin et al., 2018) has made successes in many classiﬁcation tasks including ABSA. For example, Xu et al. (2019) used an additional corpus to posttrain BERT and proved its effectiveness in both aspect extraction and ABSA. Sun et al. (2019a) converted ABSA to a sentence-pair classiﬁcation task by constructing auxiliary sentences. Some other efforts try to directly include the"
2020.acl-main.295,P18-1088,0,0.489114,"Missing"
2020.acl-main.295,D16-1059,0,0.387627,"es towards one or more aspects appearing in a single sentence. An example of this task is, given a review great food but the service was dreadful, to determine the polarities towards the aspects food and service. Since the two aspects express quite opposite sentiments, just assigning a sentence-level sentiment polarity is inappropriate. In this regard, ABSA can provide better insights into user reviews compared with sentence-level sentiment analysis. ∗ Corresponding author. Intuitively, connecting aspects with their respective opinion words lies at the heart of this task. Most recent efforts (Wang et al., 2016b; Li et al., 2017; Ma et al., 2017; Fan et al., 2018) resort to assorted attention mechanisms to achieve this goal and have reported appealing results. However, due to the complexity of language morphology and syntax, these mechanisms fail occasionally. We illustrate this problem with a real review So delicious was the noodles but terrible vegetables, in which the opinion word terrible is closer to the aspect noodles than delicious, and there could be terrible noodles appearing in some other reviews which makes these two words closely associated. Therefore, the attention mechanisms could atte"
2020.acl-main.295,D16-1058,0,0.307563,"Missing"
2020.acl-main.295,N19-1242,0,0.126185,"Missing"
2020.acl-main.295,P18-1234,0,0.161518,"Missing"
2020.acl-main.295,D19-1464,0,0.312178,"Missing"
2020.acl-main.295,S14-2004,0,\N,Missing
2020.acl-main.297,W06-1651,0,0.0926525,"k An opinion consists of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issue of data scarcity for ORL, considering SRL is highly related to ORL and has a considerable amount of training data. Inspired by the similarity between ORL and SRL in task definition, Kim and Hovy (2006) and Ruppenhofer et al. (2008) address ORL with a well-trained SRL model by treating"
2020.acl-main.297,P17-4017,0,0.0949806,"TL model effectively boosts the F1 score by 9.29 over the syntaxagnostic baseline. In addition, we find that the contributions from syntactic knowledge do not fully overlap with contextualized word representations (BERT). Our best model achieves 4.34 higher F1 score than the current state-ofthe-art. 1 $ Cardoso Opinion and sentiment analysis has a wide range of real-world applications like social media monitoring (Bollen et al., 2011), stock market prediction (Nguyen et al., 2015), box office prediction (Yu et al., 2010), and general e-commerce applications (Kim et al., 2013; Hu et al., 2017; Cui et al., 2017). In particular, fine-grained opinion analysis aims to identify users’ opinions in a text, including opinion expressions, holders of the opinions, targets of the opinions, target-dependent attitude, and intensity of opinions (Marasovi´c and Frank, 2018), which is very important for understanding political stance, Corresponding author challenge facing Chavez Target is ... Figure 1: An Example of ORL (bottom) and syntactic dependency tree (top) for “Cardoso says challenge facing Chavezis is reestablishing normalcy.” Introduction ∗ says Holder Expression customers’ reviews, marketing trends, and"
2020.acl-main.297,N19-1423,0,0.237545,"cy arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representation of syntax provides more information while alleviating parsing errors. For the second barrier, considering that the pipeline methods are notorious for the error propagation problem, we introduce multi-task learning (MTL) frameworks, which have been widely used in many NLP models when predictions at various processing levels are needed (Collobert and Weston, 2008; Ruder, 2017). Apart from the syntactic information, contextualized word representations like BERT (Devlin et al., 2019) are widely used to compensate for the sparsity of task-specific training data. They compress distributional semantics of words from large corpora, making the local context fluent and natural. However, the long-distance dependencies between words are often ignored, which is ideally able to be captured by syntactic analysis. In summary, based on previous studies in using syntax to improve various tasks, this work investigates whether syntax can enhance the neural ORL model. Particularly, we try to answer the following three questions. • How to effectively integrate various syntactic information"
2020.acl-main.297,P19-1024,0,0.0190934,"nd the ORL model. We first obtain the edge-weighted graph from the decoder of a well-trained biaffine parser as a data preprocessing step, and then feed the graph into our D EP GCN in the form of an adjacency matrix A 1 . Then we feed the outputs of the ORL BiLSTM-based encoder as the initial inputs h0 to the D EP GCN. Finally, we feed the output of the D EP GCN to the CRF-based decoder, and update the ORL results under the guidance of the syntactic information. Moreover, we introduce dense connections to the multi-layer D EP GCN for extracting more structural information (Huang et al., 2017; Guo et al., 2019). Instead of only adding connections between adjacent layers, we use dense connections from each layer to all the subsequent layers. Formally, the input of node i at the l-th layer is: (l) 4.1 (0) (1) (l−1) xi = hi ⊕ hi ⊕ · · · ⊕ hi Dependency Graph Convolutional Networks (D EP GCN) (3) (l) In this subsection, we propose dependency graph convolutional networks (D EP GCN) to better encode the syntactic information from the edgeweighted graphs. On the one hand, compared with explicit 1-best parse trees, edge-weighted graphs where hi is the output of node i at the l-th layer. We also make residua"
2020.acl-main.297,P18-1192,0,0.0735118,"human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without syntactic dependency 3249 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249–3258 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relations, missing either “facing Chavez” or “challenge”. For the similar SRL task, many previous works have proposed to incorporate syntax into the neural models (Marcheggiani and Titov, 2017; He et al., 2018; Xia et al., 2019a). In contrast, few studies in the recent years explore this line of research for ORL. There are two barriers to apply syntactic dependency parsing to NLP tasks, i.e., 1) inaccuracy of the parsing results, and 2) error propagation of the processing pipeline. To overcome the first barrier, instead of employing the final discrete outputs (i.e., single 1-best dependency trees), we make use of the probability matrix of all dependency arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representation of syntax prov"
2020.acl-main.297,J13-3002,0,0.0940515,"dency relations, do not fully overlap with those from the contextualized word representations like BERT. Our overall model delivers a new stateof-the-art result on the benchmark MPQA corpus, with 4.34 absolute improvement over the previous best result. 2 Related work An opinion consists of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issue of data scarcity for ORL, conside"
2020.acl-main.297,P16-1087,0,0.537352,"s, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issue of data scarcity for ORL, considering SRL is highly related to ORL and has a considerable amount of training data. Inspired by the similarity between ORL and SRL in task definition, Kim and Hovy (2006) and Ruppenhofer et al. (2008) address ORL with a well-trained SRL model by treating opinion expressions as semantic predicates, and op"
2020.acl-main.297,W06-0301,0,0.113222,"from long-distance dependency relations, do not fully overlap with those from the contextualized word representations like BERT. Our overall model delivers a new stateof-the-art result on the benchmark MPQA corpus, with 4.34 absolute improvement over the previous best result. 2 Related work An opinion consists of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issu"
2020.acl-main.297,N18-1054,0,0.470419,"Missing"
2020.acl-main.297,D17-1159,0,0.406534,"ural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without syntactic dependency 3249 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249–3258 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relations, missing either “facing Chavez” or “challenge”. For the similar SRL task, many previous works have proposed to incorporate syntax into the neural models (Marcheggiani and Titov, 2017; He et al., 2018; Xia et al., 2019a). In contrast, few studies in the recent years explore this line of research for ORL. There are two barriers to apply syntactic dependency parsing to NLP tasks, i.e., 1) inaccuracy of the parsing results, and 2) error propagation of the processing pipeline. To overcome the first barrier, instead of employing the final discrete outputs (i.e., single 1-best dependency trees), we make use of the probability matrix of all dependency arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representati"
2020.acl-main.297,P16-1105,0,0.0190351,"SRL as an auxiliary task, and employ different MTL frameworks to learn the common grounds between ORL and SRL and distinguish task-specific knowledge. Zhang et al. (2019b) extract neural features from a welltrained SRL model as SRL-aware word representations, and then feed them into the input layer of ORL, aiming to alleviate the error propagation problem. 3250 Figure 2: The overall architecture of our models. Many previous works have shown that syntactic information is of great value for SRL and other NLP tasks (He et al., 2018; Zhang et al., 2019c; Strubell et al., 2018; Xia et al., 2019a; Miwa and Bansal, 2016; Zhang et al., 2019a). Xia et al. (2019b) use the relative position between predicate words and other words in a dependency tree to represent syntactic information, while Roth and Lapata (2016) employ LSTM to obtain the embedding of a dependency path. Tai et al. (2015) and Kipf and Welling (2016) propose TreeLSTM and graph convolution network (GCN) to encode the tree/graphstructural data respectively. Both TreeLSTM and GCN are commonly used techniques to encode parse trees (Miwa and Bansal, 2016; Marcheggiani and Titov, 2017; Bastings et al., 2017). Zhang et al. (2019a) and Xia et al. (2019a)"
2020.acl-main.297,P16-1113,0,0.0491495,"res from a welltrained SRL model as SRL-aware word representations, and then feed them into the input layer of ORL, aiming to alleviate the error propagation problem. 3250 Figure 2: The overall architecture of our models. Many previous works have shown that syntactic information is of great value for SRL and other NLP tasks (He et al., 2018; Zhang et al., 2019c; Strubell et al., 2018; Xia et al., 2019a; Miwa and Bansal, 2016; Zhang et al., 2019a). Xia et al. (2019b) use the relative position between predicate words and other words in a dependency tree to represent syntactic information, while Roth and Lapata (2016) employ LSTM to obtain the embedding of a dependency path. Tai et al. (2015) and Kipf and Welling (2016) propose TreeLSTM and graph convolution network (GCN) to encode the tree/graphstructural data respectively. Both TreeLSTM and GCN are commonly used techniques to encode parse trees (Miwa and Bansal, 2016; Marcheggiani and Titov, 2017; Bastings et al., 2017). Zhang et al. (2019a) and Xia et al. (2019a) extract the hidden states from the LSTM encoder of the parser model as syntax-aware word representations, and feed them to downstream tasks as extra inputs. In contrast, few works have proved t"
2020.acl-main.297,ruppenhofer-etal-2008-finding,0,0.26963,"ion mining task, opinion role labeling (ORL) aims to identify different roles relevant to each opinion, i.e., who expressed what kind of sentiment towards what (Liu, 2012). Due to the lack of large-scale labeled data, ORL remains a challenging task to tackle. As a reference point, semantic role labeling (SRL) is very similar to ORL in the problem definition, but has 10 times more labeled data and thus achieves much higher performance than ORL (80∼90 vs. 60∼70 in F1 score). Motivated by the correlations between the two tasks, SRL has been utilized to help the ORL task by many previous studies (Ruppenhofer et al., 2008; Marasovi´c and Frank, 2018; Zhang et al., 2019b). However, when opinion expressions and arguments compose complicated syntactic structures, it is difficult to correctly recognize the opinion arguments even with shallow semantic representation like SRL (Marasovi´c and Frank, 2018). To compensate for the limited scale of labeled data for data-driven approaches, linguistic knowledge like syntax provides structural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Targ"
2020.acl-main.297,D18-1548,0,0.0256489,"ic roles. Marasovi´c and Frank (2018) take SRL as an auxiliary task, and employ different MTL frameworks to learn the common grounds between ORL and SRL and distinguish task-specific knowledge. Zhang et al. (2019b) extract neural features from a welltrained SRL model as SRL-aware word representations, and then feed them into the input layer of ORL, aiming to alleviate the error propagation problem. 3250 Figure 2: The overall architecture of our models. Many previous works have shown that syntactic information is of great value for SRL and other NLP tasks (He et al., 2018; Zhang et al., 2019c; Strubell et al., 2018; Xia et al., 2019a; Miwa and Bansal, 2016; Zhang et al., 2019a). Xia et al. (2019b) use the relative position between predicate words and other words in a dependency tree to represent syntactic information, while Roth and Lapata (2016) employ LSTM to obtain the embedding of a dependency path. Tai et al. (2015) and Kipf and Welling (2016) propose TreeLSTM and graph convolution network (GCN) to encode the tree/graphstructural data respectively. Both TreeLSTM and GCN are commonly used techniques to encode parse trees (Miwa and Bansal, 2016; Marcheggiani and Titov, 2017; Bastings et al., 2017). Z"
2020.acl-main.297,P15-1150,0,0.0563738,"them into the input layer of ORL, aiming to alleviate the error propagation problem. 3250 Figure 2: The overall architecture of our models. Many previous works have shown that syntactic information is of great value for SRL and other NLP tasks (He et al., 2018; Zhang et al., 2019c; Strubell et al., 2018; Xia et al., 2019a; Miwa and Bansal, 2016; Zhang et al., 2019a). Xia et al. (2019b) use the relative position between predicate words and other words in a dependency tree to represent syntactic information, while Roth and Lapata (2016) employ LSTM to obtain the embedding of a dependency path. Tai et al. (2015) and Kipf and Welling (2016) propose TreeLSTM and graph convolution network (GCN) to encode the tree/graphstructural data respectively. Both TreeLSTM and GCN are commonly used techniques to encode parse trees (Miwa and Bansal, 2016; Marcheggiani and Titov, 2017; Bastings et al., 2017). Zhang et al. (2019a) and Xia et al. (2019a) extract the hidden states from the LSTM encoder of the parser model as syntax-aware word representations, and feed them to downstream tasks as extra inputs. In contrast, few works have proved that syntactic knowledge is useful in the neural ORL models. Yang and Cardie"
2020.acl-main.297,D19-1541,1,0.407543,"ng of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without syntactic dependency 3249 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249–3258 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relations, missing either “facing Chavez” or “challenge”. For the similar SRL task, many previous works have proposed to incorporate syntax into the neural models (Marcheggiani and Titov, 2017; He et al., 2018; Xia et al., 2019a). In contrast, few studies in the recent years explore this line of research for ORL. There are two barriers to apply syntactic dependency parsing to NLP tasks, i.e., 1) inaccuracy of the parsing results, and 2) error propagation of the processing pipeline. To overcome the first barrier, instead of employing the final discrete outputs (i.e., single 1-best dependency trees), we make use of the probability matrix of all dependency arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representation of syntax provides more informat"
2020.acl-main.297,N19-1075,0,0.138213,"ng of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without syntactic dependency 3249 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249–3258 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relations, missing either “facing Chavez” or “challenge”. For the similar SRL task, many previous works have proposed to incorporate syntax into the neural models (Marcheggiani and Titov, 2017; He et al., 2018; Xia et al., 2019a). In contrast, few studies in the recent years explore this line of research for ORL. There are two barriers to apply syntactic dependency parsing to NLP tasks, i.e., 1) inaccuracy of the parsing results, and 2) error propagation of the processing pipeline. To overcome the first barrier, instead of employing the final discrete outputs (i.e., single 1-best dependency trees), we make use of the probability matrix of all dependency arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representation of syntax provides more informat"
2020.acl-main.297,P13-1161,0,0.632022,"ts of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issue of data scarcity for ORL, considering SRL is highly related to ORL and has a considerable amount of training data. Inspired by the similarity between ORL and SRL in task definition, Kim and Hovy (2006) and Ruppenhofer et al. (2008) address ORL with a well-trained SRL model by treating opinion expressions as"
2020.acl-main.297,Q14-1039,0,0.218287,"alleviate the error propagation problem; and 3) contributions from syntactic information, especially from long-distance dependency relations, do not fully overlap with those from the contextualized word representations like BERT. Our overall model delivers a new stateof-the-art result on the benchmark MPQA corpus, with 4.34 absolute improvement over the previous best result. 2 Related work An opinion consists of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targe"
2020.acl-main.297,N19-1118,1,0.170002,"dentify different roles relevant to each opinion, i.e., who expressed what kind of sentiment towards what (Liu, 2012). Due to the lack of large-scale labeled data, ORL remains a challenging task to tackle. As a reference point, semantic role labeling (SRL) is very similar to ORL in the problem definition, but has 10 times more labeled data and thus achieves much higher performance than ORL (80∼90 vs. 60∼70 in F1 score). Motivated by the correlations between the two tasks, SRL has been utilized to help the ORL task by many previous studies (Ruppenhofer et al., 2008; Marasovi´c and Frank, 2018; Zhang et al., 2019b). However, when opinion expressions and arguments compose complicated syntactic structures, it is difficult to correctly recognize the opinion arguments even with shallow semantic representation like SRL (Marasovi´c and Frank, 2018). To compensate for the limited scale of labeled data for data-driven approaches, linguistic knowledge like syntax provides structural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without"
2020.acl-main.297,D14-1162,0,0.083367,"RT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) to obtain deep contextualized word representations as our extra inputs. In particular, we use BERT-base (uncased) model and extract representations from the top-1 hidden layer. Our experiments show that using the top-1 layer representations performs better than the more common use of aggregating top-4 hidden layers.2 Parameters. We follow the previous works of Zhang et al. (2019b) and Marasovi´c and Frank (2018) without much parameter tuning. Specifically, we use the pretrained 100-dimensional glove embeddings (Pennington et al., 2014). The BiLSTM layer number is set to 3, and the hidden output size is 200. We apply 0.33 dropout to word representation and the hidden states of the BiLSTM. We choose Adam (Kingma and Ba, 2014) to optimize model parameters with a learning rate 10−3 . The entire training instances are trained for 30 epochs with the batch size of 50, and the best-epoch model at the peak performance on the dev corpus is chosen. For the MTL, we train the batches of ORL and parsing in turn since this interleaving training can obtain better performance in our experiments. Besides, we use the corpus weighting trick to"
2020.acl-main.297,N19-1066,0,0.110957,"dentify different roles relevant to each opinion, i.e., who expressed what kind of sentiment towards what (Liu, 2012). Due to the lack of large-scale labeled data, ORL remains a challenging task to tackle. As a reference point, semantic role labeling (SRL) is very similar to ORL in the problem definition, but has 10 times more labeled data and thus achieves much higher performance than ORL (80∼90 vs. 60∼70 in F1 score). Motivated by the correlations between the two tasks, SRL has been utilized to help the ORL task by many previous studies (Ruppenhofer et al., 2008; Marasovi´c and Frank, 2018; Zhang et al., 2019b). However, when opinion expressions and arguments compose complicated syntactic structures, it is difficult to correctly recognize the opinion arguments even with shallow semantic representation like SRL (Marasovi´c and Frank, 2018). To compensate for the limited scale of labeled data for data-driven approaches, linguistic knowledge like syntax provides structural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without"
2020.acl-main.297,D19-1057,1,0.172248,"dentify different roles relevant to each opinion, i.e., who expressed what kind of sentiment towards what (Liu, 2012). Due to the lack of large-scale labeled data, ORL remains a challenging task to tackle. As a reference point, semantic role labeling (SRL) is very similar to ORL in the problem definition, but has 10 times more labeled data and thus achieves much higher performance than ORL (80∼90 vs. 60∼70 in F1 score). Motivated by the correlations between the two tasks, SRL has been utilized to help the ORL task by many previous studies (Ruppenhofer et al., 2008; Marasovi´c and Frank, 2018; Zhang et al., 2019b). However, when opinion expressions and arguments compose complicated syntactic structures, it is difficult to correctly recognize the opinion arguments even with shallow semantic representation like SRL (Marasovi´c and Frank, 2018). To compensate for the limited scale of labeled data for data-driven approaches, linguistic knowledge like syntax provides structural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without"
2020.acl-main.32,P18-1001,0,0.0144052,"yer) as shown in the right panel of Figure 2. It employs real distribution pair p~r and fake distribution pair p~f as input and then outputs Dout to identify the input sources (fake or real). Concretely, a higher value of Dout represents that D is more prone to predict the input as real and vice versa. 3.2 BAT with Gaussian (Gaussian-BAT) In BAT, the generator models topics based on the bag-of-words assumption as in most other neural topic models. To incorporate the word relatedness information captured in word embeddings (Mikolov et al., 2013a,b; Pennington et al., 2014; Joulin et al., 2017; Athiwaratkun et al., 2018) into the inference process, we modify the generator of BAT and propose Gaussian-BAT, in which G models each topic with a multivariate Gaussian as shown in Figure 3. V-dim K-dim N (~ 1 ; §1 ) µ~f » Dir(~ µf j~ ®) Document-topic distribution Word Embedding ~1 Á ... ... N (~ 2 ; §2 ) ~2 Á ... ... N (~ K ; §K ) ~K Á Gaussian distributions Topic-word distributions d~f p~f Documentfake distribution pair word distribution Generator Network (G) Figure 3: The generator of Gaussian-BAT. Concretely, Gaussian-BAT employs the multivariate Gaussian N (~ µk , Σk ) to model the k-th topic. Here, µ ~ k and"
2020.acl-main.32,E17-2068,0,0.0146325,"ayer and an output layer) as shown in the right panel of Figure 2. It employs real distribution pair p~r and fake distribution pair p~f as input and then outputs Dout to identify the input sources (fake or real). Concretely, a higher value of Dout represents that D is more prone to predict the input as real and vice versa. 3.2 BAT with Gaussian (Gaussian-BAT) In BAT, the generator models topics based on the bag-of-words assumption as in most other neural topic models. To incorporate the word relatedness information captured in word embeddings (Mikolov et al., 2013a,b; Pennington et al., 2014; Joulin et al., 2017; Athiwaratkun et al., 2018) into the inference process, we modify the generator of BAT and propose Gaussian-BAT, in which G models each topic with a multivariate Gaussian as shown in Figure 3. V-dim K-dim N (~ 1 ; §1 ) µ~f » Dir(~ µf j~ ®) Document-topic distribution Word Embedding ~1 Á ... ... N (~ 2 ; §2 ) ~2 Á ... ... N (~ K ; §K ) ~K Á Gaussian distributions Topic-word distributions d~f p~f Documentfake distribution pair word distribution Generator Network (G) Figure 3: The generator of Gaussian-BAT. Concretely, Gaussian-BAT employs the multivariate Gaussian N (~ µk , Σk ) to model the"
2020.acl-main.32,D14-1162,0,0.0834996,"ensional representation layer and an output layer) as shown in the right panel of Figure 2. It employs real distribution pair p~r and fake distribution pair p~f as input and then outputs Dout to identify the input sources (fake or real). Concretely, a higher value of Dout represents that D is more prone to predict the input as real and vice versa. 3.2 BAT with Gaussian (Gaussian-BAT) In BAT, the generator models topics based on the bag-of-words assumption as in most other neural topic models. To incorporate the word relatedness information captured in word embeddings (Mikolov et al., 2013a,b; Pennington et al., 2014; Joulin et al., 2017; Athiwaratkun et al., 2018) into the inference process, we modify the generator of BAT and propose Gaussian-BAT, in which G models each topic with a multivariate Gaussian as shown in Figure 3. V-dim K-dim N (~ 1 ; §1 ) µ~f » Dir(~ µf j~ ®) Document-topic distribution Word Embedding ~1 Á ... ... N (~ 2 ; §2 ) ~2 Á ... ... N (~ K ; §K ) ~K Á Gaussian distributions Topic-word distributions d~f p~f Documentfake distribution pair word distribution Generator Network (G) Figure 3: The generator of Gaussian-BAT. Concretely, Gaussian-BAT employs the multivariate Gaussian N (~ µ"
2020.acl-main.32,D19-1027,1,0.852245,"roximate the Dirichlet distribution, they are not exactly the same. An illustration of these two distributions is shown in Figure 1 in which the Logistic-Normal distribution does not exhibit multiple peaks at the vertices of the simplex as that in the Dirichlet distribution and as such, it is less capable to capture 340 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 340–350 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the multi-modality which is crucial in topic modeling (Wallach et al., 2009). To deal with the limitation, Wang et al. (2019a) proposed the Adversarialneural Topic Model (ATM) based on adversarial training, it uses a generator network to capture the semantic patterns lying behind the documents. However, given a document, ATM is not able to infer the document-topic distribution which is useful for downstream applications, such as text clustering. Moreover, ATM take the bag-of-words assumption and do not utilize any word relatedness information captured in word embeddings which have been proved to be crucial for better performance in many NLP tasks (Liu et al., 2018; Lei et al., 2018). To address these limitations, w"
2020.acl-main.32,P14-2114,1,0.857538,"Missing"
2020.acl-main.324,N19-1388,0,0.065454,"e attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et al., 2018a) has been proposed, using a combination of divers"
2020.acl-main.324,D18-1549,0,0.307174,"UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs. 1 Introduction Recently, neural machine translation (NMT) has been adapted to the unsupervised scenario in which NMT is trained without any bilingual data. Unsupervised NMT (UNMT) (Artetxe et al., 2018; Lample et al., 2018a) requires only monolingual corpora. UNMT achieves remarkable results by using a combination of diverse mechanisms (Lample et al., 2018b) such as an initialization with bilingual word embeddings, denoising auto-encoder (Vincent et al., 2010), back-translation (Sennrich et al., 2016a), and shared latent representation. More recently, Lample and Conneau (2019) achieves better UNMT performance by introducing the pretrained language model. However, conventional UNMT can only translate between a single language pair and cannot produce translation results for multiple language"
2020.acl-main.324,W19-5301,0,0.0735941,"Missing"
2020.acl-main.324,C18-1263,0,0.0276317,"e pairs. The other columns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et a"
2020.acl-main.324,P15-1166,0,0.0246013,"7.58 25.05 14.09 9.75 25.84 10.90 23.80 10.07 13.09 28.82 12.41 15.79 19.57 27.59 16.62 11.05 28.56 12.77 25.25 10.92 14.33 32.38 14.78 15.47 19.28 26.79 15.62 10.57 27.78 12.03 25.52 11.11 14.33 31.28 13.83 15.93 20.00 27.80 17.21 11.58 28.62 13.12 25.98 11.22 15.17 32.43 15.30 Average 15.61 17.15 19.13 18.63 19.53 Table 5: The +FT column shows BLEU scores from further training of the MUNMT and LBKD model on the English to non-English language pairs. The other columns show results from Table 2. 7 Related Work Multilingual NMT has attracted much attention in the machine translation community. Dong et al. (2015) first extended NMT from the translation of a single language pair to multiple language pairs, using a shared encoder and multiple decoders and 3532 Corpus SM MUNMT +FT LBKD +FT Cs-En De-En Es-En Et-En Fi-En Fr-En Hu-En It-En Lt-En Lv-En Ro-En Tr-En 20.62 21.31 25.53 19.48 7.62 25.86 14.48 24.33 1.72 0.95 28.52 12.99 20.09 21.95 25.37 19.60 7.19 25.41 14.54 24.77 14.04 14.90 28.38 15.65 21.50 22.41 26.24 21.61 8.06 26.30 15.99 25.54 15.27 15.57 29.61 18.47 21.25 22.81 26.59 21.31 7.80 26.48 15.34 25.35 15.84 15.33 30.18 17.35 22.17 23.07 26.78 22.61 8.34 26.76 16.07 25.86 16.86 15.87 30.39 19."
2020.acl-main.324,N16-1101,0,0.0218044,"15.99 25.54 15.27 15.57 29.61 18.47 21.25 22.81 26.59 21.31 7.80 26.48 15.34 25.35 15.84 15.33 30.18 17.35 22.17 23.07 26.78 22.61 8.34 26.76 16.07 25.86 16.86 15.87 30.39 19.48 Average 16.95 19.32 20.55 20.47 21.19 Table 6: The +FT column shows BLEU scores from further training of the MUNMT and LBKD model on the non-English to English language pairs. The other columns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) an"
2020.acl-main.324,R19-1050,0,0.0409383,"Missing"
2020.acl-main.324,N16-1162,0,0.0752553,"Missing"
2020.acl-main.324,Q17-1024,0,0.0420139,"Missing"
2020.acl-main.324,P07-2045,0,0.00631129,"n of the MUNMT and LBUNMT models, respectively, after encoding M j (Xi1 ) generated by the previous MUNMT model in the L1 → Lj direction. X j (M 1 (Xij )) and LB j (M 1 (Xij )) denote the softened Lj sentence probability distribution of the MUNMT and LBUNMT models, respectively, after encoding M 1 (Xij ) generated by the previous MUNMT model in the Lj → L1 direction. 5 5.1 Experiments Datasets To establish an MUNMT system, we considered 13 languages from WMT monolingual news crawl datasets: Cs, De, En, Es, Et, Fi, Fr, Hu, It, Lt, Lv, Ro, and Tr. For preprocessing, we used the Moses tokenizer (Koehn et al., 2007). For cleaning, we only applied the Moses script clean-corpus-n.perl to remove lines in the monolingual data containing more than 50 words. We then used a shared vocabulary for all languages, with 80,000 sub-word tokens based on BPE (Sennrich et al., 2016b). The statistics of the data are presented in Table 1. For Cs,De,En, we randomly extracted 50M monolingual news crawl data after cleaning; For other languages, we used all news crawl data after cleaning as shown in Table 1. Language Cs De En Es Et Fi Fr Hu It Lt Lv Ro Tr Sentences Words Sub-words 50.00M 50.00M 50.00M 36.33M 3.00M 15.31M 50.0"
2020.acl-main.324,C18-1054,0,0.023553,"ing of the MUNMT and LBKD model on the non-English to English language pairs. The other columns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem o"
2020.acl-main.324,W18-6309,0,0.0284044,"ns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et al., 2018a) has be"
2020.acl-main.324,W19-5330,1,0.703395,") concatenated two bilingual corpora as one monolingual corpus, and used monolingual embedding pretraining in the initialization step, to achieve remarkable results with some similar language pairs. Lample and Conneau (2019) achieved better UNMT performance by introducing a pretrained language model. Sun et al. (2019, 2020) proposed to train UNMT with cross-lingual language representation agreement, to further improve UNMT performance. Moreover, an unsupervised translation task that evaluated in the WMT19 news translation task (Barrault et al., 2019) attracted many researchers to participate (Marie et al., 2019; Li et al., 2019). For Multilingual UNMT, Xu et al. (2019) exploited multiple auxiliary languages for jointly boosting UNMT models via the Polygon-Net framework. Sen et al. (2019) proposed an MUNMT scheme that jointly trains multiple languages with a shared encoder and multiple decoders. In contrast with their use of multiple decoders, we have constructed a simpler MUNMT model with one encoder and one decoder. Further, we have extended the four or five languages used in their work to thirteen languages, for training our MUNMT model. 8 Conclusion and Future Work In this paper, we have introduc"
2020.acl-main.324,D18-1039,0,0.0403498,"LBKD model on the non-English to English language pairs. The other columns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs tra"
2020.acl-main.324,P18-1006,0,0.0664673,"(2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et al., 2018a) has been proposed, using a combination of diverse mechanisms such as initialization with bilingual word embeddings, denoising autoencoder (Vincent et al., 2010), back-translation (Sennrich et al., 2016a), and shared latent representation. Lample et al. (2018b) concatenated two bilingual corpora as one monolingual corpus, and used monolingual embedding pr"
2020.acl-main.324,P19-1117,0,0.0411944,"rom Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et al., 2018a) has been proposed, using"
2020.acl-main.324,W18-6327,0,0.027368,"nglish to English language pairs. The other columns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe e"
2020.acl-main.324,P19-1297,0,0.455981,"mple and Conneau (2019) achieves better UNMT performance by introducing the pretrained language model. However, conventional UNMT can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time (Wang et al., 2020). Multilingual UNMT (MUNMT) translating multiple languages at the same time can save substantial training time and resources. Moreover, the performance of MUNMT in similar languages can promote each other. Research on MUNMT has been limited and there are only a few pioneer studies. For example, Xu et al. (2019) and Sen et al. (2019) proposed a multilingual scheme that jointly trains multiple languages with multiple decoders. However, the performance of their MUNMT is much worse than our re-implemented individual baselines (shown in Tables 2 and 3) and the scale of their study is modest (i.e., 4-5 languages). In this paper, we empirically introduce an unified framework to translate among thirteen languages (including three language families and six language branches) using a single encoder and single decoder, making use of multilingual data to improve UNMT for all languages. On the basis of these empirical findings, we pr"
2020.acl-main.324,P18-1005,0,0.0216021,"bulary. The entire training of UNMT needs to consider back-translation between the two languages and their respective denoising processes. In summary, the entire UNMT model can be optimized by minimizing: Lall = LD + LB . 3 Denoising Auto-encoder LD = 2.3 2.4 A cross-lingual masked language model, which can encode two monolingual sentences into a shared latent space, is first trained. The pretrained crosslingual encoder is then used to initialize the whole UNMT model (Lample and Conneau, 2019). Compared with previous bilingual embedding pretraining (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019), this pretraining can provide much more crosslingual information, causing the UNMT model to achieve better performance and faster convergence. 2.2 where {C(Xi1 )} and {C(Xi2 )} are noisy sentences. PL1 →L1 and PL2 →L2 denote the reconstruction probability in language L1 and L2 , respectively. 3.1 (3) Multilingual UNMT (MUNMT) Multilingual Pretraining Motivated by Lample and Conneau (2019), we construct a multilingual masked language model, using a single encoder. For each language, the language model is trained by encoding the masked input and revertin"
2020.acl-main.324,P16-1009,0,0.571282,"language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs. 1 Introduction Recently, neural machine translation (NMT) has been adapted to the unsupervised scenario in which NMT is trained without any bilingual data. Unsupervised NMT (UNMT) (Artetxe et al., 2018; Lample et al., 2018a) requires only monolingual corpora. UNMT achieves remarkable results by using a combination of diverse mechanisms (Lample et al., 2018b) such as an initialization with bilingual word embeddings, denoising auto-encoder (Vincent et al., 2010), back-translation (Sennrich et al., 2016a), and shared latent representation. More recently, Lample and Conneau (2019) achieves better UNMT performance by introducing the pretrained language model. However, conventional UNMT can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time (Wang et al., 2020). Multilingual UNMT (MUNMT) translating multiple languages at the same time can save substantial training time and resources. Moreover, the performance of MUNMT in similar languages can promote each other. Research on MUNMT has been limited and there are only a"
2020.acl-main.324,P16-1162,0,0.832951,"language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs. 1 Introduction Recently, neural machine translation (NMT) has been adapted to the unsupervised scenario in which NMT is trained without any bilingual data. Unsupervised NMT (UNMT) (Artetxe et al., 2018; Lample et al., 2018a) requires only monolingual corpora. UNMT achieves remarkable results by using a combination of diverse mechanisms (Lample et al., 2018b) such as an initialization with bilingual word embeddings, denoising auto-encoder (Vincent et al., 2010), back-translation (Sennrich et al., 2016a), and shared latent representation. More recently, Lample and Conneau (2019) achieves better UNMT performance by introducing the pretrained language model. However, conventional UNMT can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time (Wang et al., 2020). Multilingual UNMT (MUNMT) translating multiple languages at the same time can save substantial training time and resources. Moreover, the performance of MUNMT in similar languages can promote each other. Research on MUNMT has been limited and there are only a"
2020.acl-main.324,P19-1119,1,0.531451,"to consider back-translation between the two languages and their respective denoising processes. In summary, the entire UNMT model can be optimized by minimizing: Lall = LD + LB . 3 Denoising Auto-encoder LD = 2.3 2.4 A cross-lingual masked language model, which can encode two monolingual sentences into a shared latent space, is first trained. The pretrained crosslingual encoder is then used to initialize the whole UNMT model (Lample and Conneau, 2019). Compared with previous bilingual embedding pretraining (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019), this pretraining can provide much more crosslingual information, causing the UNMT model to achieve better performance and faster convergence. 2.2 where {C(Xi1 )} and {C(Xi2 )} are noisy sentences. PL1 →L1 and PL2 →L2 denote the reconstruction probability in language L1 and L2 , respectively. 3.1 (3) Multilingual UNMT (MUNMT) Multilingual Pretraining Motivated by Lample and Conneau (2019), we construct a multilingual masked language model, using a single encoder. For each language, the language model is trained by encoding the masked input and reverting it with this encoder. This pretrained m"
2020.acl-main.324,D19-1089,0,0.0734585,"Missing"
2020.acl-main.324,P19-1583,0,0.0192416,"Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et al., 2018a) has been proposed, using a combination of diverse mechanisms such as initialization with bil"
2020.acl-main.324,W19-5325,0,\N,Missing
2020.acl-main.34,P05-1066,0,0.156159,"Missing"
2020.acl-main.34,P17-1012,0,0.0231835,"ule (ATT), and the second sub-layer is a position-wise fully connected feed-forward network (FNN). A residual connection (He et al., 2016) is applied between the sub-layers, and layer normalization (LN) (Ba et al., 2016) is performed. Formally, the l-th identical layer of this stack is as follows: l l−1 l−1 l−1 H = LN(ATTle (Ql−1 ) e , Ke , Ve ) + H l l Hl = LN(FFNle (H ) + H ). (1) l−1 l−1 {Ql−1 e , Ke , Ve } are query, key, and value vectors that are transformed from the (l-1)-th layer Hl−1 . For example, {Q0 , K 0 , V 0 } are packed from the H0 learned by the positional encoding mechanism (Gehring et al., 2017). Similarly, the decoder is composed of a stack of L identical layers. Compared with the stacked encoder, it contains an additional attention sublayer to compute alignment weights for the output of the encoder stack HL : l l−1 l−1 l−1 Si = LN(ATTld (Ql−1 i , Ki , Vi ) + Si ), l l L Cli = LN(ATTlc (Si , KL e , Ve ) + Si ), (2) Sli = LN(FFNld (Cli ) + Cli ), l−1 l−1 where Ql−1 are query, key, and d , Kd , and Vd value vectors, respectively, that are transformed from the (l-1)-th layer Sl−1 in time-step i. L {KL e , Ve } are transformed from the L-th layer of the encoder. The top layer of the dec"
2020.acl-main.34,N19-1122,0,0.0409928,"Missing"
2020.acl-main.34,D19-1088,0,0.0341217,"Missing"
2020.acl-main.34,N19-4009,0,0.0250087,"Missing"
2020.acl-main.34,P16-1162,0,0.0697775,"to further improve translation performance. 5 5.1 Experiments Setup The proposed methods were evaluated on the WMT14 English-to-German (EN-DE), WMT14 English-to-French (EN-FR), and WMT17 Chineseto-English (ZH-EN) tasks. The EN-DE corpus consists of 4M sentence pairs, the ZH-EN corpus of 22M sentence pairs, and the EN-FR corpus of 36M sentence pairs. We used the case-sensitive 4gram BLEU score as evaluation metric. The results of the newstest2014 test sets are reported for the EN-DE and EN-FR tasks, and the newstest2017 test set is reported for the ZH-EN task. The byte pair encoding algorithm (Sennrich et al., 2016) was applied to encode all sentences to limit the size of the vocabulary to 40K. The other configurations were identical to those in (Vaswani et al., 2017). The poposed models were implemented by using 360 EN-DE BLEU #Speed #Param Existing NMT systems Trans.base (Vaswani et al., 2017) 27.3 N/A 65.0M +Context-Aware SANs (Yang et al., 2019a) 28.26 N/A 106.9M +Convolutional SANs (Yang et al., 2019b) 28.18 N/A 88.0M +BIARN (Hao et al., 2019) 28.21 N/A 97.4M Trans.big (Vaswani et al., 2017) 28.4 N/A 213.0M +Context-Aware SANs (Yang et al., 2019a) 28.89 N/A 339.6M +Convolutional SANs (Yang et al., 2"
2020.acl-main.34,P07-1090,0,0.0422181,"tent or function words with UNK in a source sentence. Figure 1 shows that the BLEU scores of the test set decreased much ∗ Corresponding author Transformer (base) -Function Words -Content Words 20 more substantially when parts of content words were randomly replaced with UNK on the WMT14 English-to-German task, which is in line with the findings in He et al. (2019)’s work. To address this limitation, we propose a content word-aware NMT model that exploits the results of translation using a sequence of content words learned by a simple content word recognition method. Inspired by the works of (Setiawan et al., 2007, 2009; Zhang and Zhao, 2013), we first divide words in a sentence into content words and other function words depending on term frequencyinverse document frequency (TF-IDF) constraints. Two methods are designed to utilize the sequence of content word on the source and target sides: 1) We encode the content words of the source sentence as a new source representation, and learn an additional content word context vector based on it to improve translation performance; 2) A specific loss for content words of the target sentence is introduced to compensate for the original training objection, to ob"
2020.acl-main.34,P09-1037,0,0.0880705,"Missing"
2020.acl-main.34,N19-1407,0,0.0385394,"Missing"
2020.acl-main.638,D18-1547,0,0.166296,"Missing"
2020.acl-main.638,P19-1360,0,0.160563,"l language understanding (Gupta et al., 2018), dialogue state tracking (Zhong et al., 2018) and natural language ∗ Belief State: restaurant-{food=Indian, name=Curry Garden} Xiaojun Quan is the corresponding author of this paper. Most of this work was done when Kai Wang was working as an intern at Alibaba DAMO Academy. generation (Su et al., 2018). They can be modeled separately and combined into a pipeline system. Figure 1 shows a dialogue example, from which we can notice that the natural language generation subtask can be further divided into dialogue act prediction and response generation (Chen et al., 2019; Zhao et al., 2019; Wen et al., 2017). While the former is intended to predict the next action(s) based on current conversational state and database information, response generation is used to produce a natural language response based on the action(s). In order for dialogues to be natural and effective, responses should be ﬂuent, informative, and relevant. Nevertheless, current sequence-to-sequence models often generate uninformative responses like “I don’t know” (Li et al., 2016a), hindering the dialogues to continue or even leading to a failure. Some researchers (Pei et al., 2019; Mehri et"
2020.acl-main.638,N19-1423,0,0.0127991,"sed as baselines for comparison with our neural co-generation model (M AR C O), being categorized into three categories: • Without Act. Models in this category directly generate responses without act prediction, including LSTM (Budzianowski et al., 2018), Transformer (Vaswani et al., 2017), TokenMoE (Pei et al., 2019) and Structured Fusion (Mehri et al., 2019). • One-Hot Act. In SC-LSTM (Wen et al., 2015), dialogue acts are treated as triples and information ﬂow from acts to response generation is controlled by gates. HDSA (Chen et al., 2019) is a strong two-stage model, which relies on BERT (Devlin et al., 2019) to predict a one-hot act vector for response generation. Experiments 5.1 Dataset and Metrics MultiWOZ 2.0 (Budzianowski et al., 2018) is a large-scale multi-domain conversational datatset consisting of thousands of dialogues in seven domains. For fair comparison, we use the same validation set and test set as previous studies (Chen et al., 2019; Zhao et al., 2019; Budzianowski et al., 2018), each set including 1000 dialogues.2 We use the Inform Rate and Request Success metrics to evaluate dialog completion, with one measuring whether a system has provided an appropriate entity and the other a"
2020.acl-main.638,D18-1300,0,0.0163766,"ialogue system needs to generate a natural language response according to current belief state and related database records. Introduction Task-oriented dialogue systems aim to facilitate people with such services as hotel reservation and ticket booking through natural language conversations. Recent years have seen a rapid proliferation of interests in this task from both academia and industry (Bordes et al., 2017; Budzianowski et al., 2018; Wu et al., 2019). A standard architecture of these systems generally decomposes this task into several subtasks, including natural language understanding (Gupta et al., 2018), dialogue state tracking (Zhong et al., 2018) and natural language ∗ Belief State: restaurant-{food=Indian, name=Curry Garden} Xiaojun Quan is the corresponding author of this paper. Most of this work was done when Kai Wang was working as an intern at Alibaba DAMO Academy. generation (Su et al., 2018). They can be modeled separately and combined into a pipeline system. Figure 1 shows a dialogue example, from which we can notice that the natural language generation subtask can be further divided into dialogue act prediction and response generation (Chen et al., 2019; Zhao et al., 2019; Wen et"
2020.acl-main.638,D16-1127,0,0.429075,"natural language generation subtask can be further divided into dialogue act prediction and response generation (Chen et al., 2019; Zhao et al., 2019; Wen et al., 2017). While the former is intended to predict the next action(s) based on current conversational state and database information, response generation is used to produce a natural language response based on the action(s). In order for dialogues to be natural and effective, responses should be ﬂuent, informative, and relevant. Nevertheless, current sequence-to-sequence models often generate uninformative responses like “I don’t know” (Li et al., 2016a), hindering the dialogues to continue or even leading to a failure. Some researchers (Pei et al., 2019; Mehri et al., 7125 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7125–7134 c July 5 - 10, 2020. 2020 Association for Computational Linguistics domain Dialog Act Graph hotel root restaurant attraction hotel ... inform...phone Multiple Binary Classification (HDSA) action slot name inform request area reference phone &lt;sos&gt; rest. inform ... area phone Sequence Generation (Ours) Figure 2: Demonstration of hierarchical dialogue act structures (top"
2020.acl-main.638,W19-5921,0,0.511307,"can be treated as a triple (domainaction-slot) and all acts together are represented in a one-hot vector (Wen et al., 2015; Budzianowski et al., 2018). Such sparse representation makes the act space very large. To overcome this issue, Chen et al. (2019) took into account act structures and proposed to represent the dialogue acts with level-speciﬁc one-hot vectors. Each dimension of the vectors is predicted by a binary classiﬁer. To improve response generation, Pei et al. (2019) proposed to learn different expert decoders for different domains and acts, and combined them with a chair decoder. Mehri et al. (2019) applied a cold-fusion method (Sriram et al., 2018) to combine their response decoder with a language model. Zhao et al. (2019) treated dialogue acts as latent variables and used reinforcement learning to optimize them. Reinforcement learning was also applied to ﬁnd optimal dialogue policies in task-oriented dialogue systems (Su et al., 2017; 7126 Williams et al., 2017) or obtain higher dialog-level rewards in chatting (Li et al., 2016b; Serban et al., 2017). Besides, Chen et al. (2019) proposed to predict the acts explicitly with a compact act graph representation and employed hierarchical di"
2020.acl-main.638,D16-1096,0,0.0289183,"Missing"
2020.acl-main.638,P17-1163,0,0.0973928,"Missing"
2020.acl-main.638,P02-1040,0,0.108364,"taset and Metrics MultiWOZ 2.0 (Budzianowski et al., 2018) is a large-scale multi-domain conversational datatset consisting of thousands of dialogues in seven domains. For fair comparison, we use the same validation set and test set as previous studies (Chen et al., 2019; Zhao et al., 2019; Budzianowski et al., 2018), each set including 1000 dialogues.2 We use the Inform Rate and Request Success metrics to evaluate dialog completion, with one measuring whether a system has provided an appropriate entity and the other assessing if it has answered all requested attributes. Besides, we use BLEU (Papineni et al., 2002) to measure the ﬂuency of generated responses. To measure the overall system performance, we compute a combined score: (Inform Rate + Request Success) × 0.5 + BLEU as before (Budzianowski et al., 2018; Mehri et al., 2019; Pei et al., 2019). 5.2 Implementation Details The implementation3 is on a single Tesla P100 GPU with a batch size of 512. The dimension of 2 There are only ﬁve domains (restaurant, hotel, attract, taxi, train) of dialogues in the test set as the other two (hospital, police) have insufﬁcient dialogues. 3 https://github.com/InitialBug/ MarCo-Dialog Baselines • Sequential Act. S"
2020.acl-main.638,W18-5045,0,0.218283,"concurrently. Unlike those classiﬁcation approaches, act sequence generation not only preserves the interrelationships among dialogue acts but also allows close interactions with response generation. By attending to different acts, the response generation module can dynamically capture salient acts and produce higher-quality responses. Figure 2 demonstrates the difference between the classiﬁcation and the generation approaches for act prediction. As for training, most joint learning models rely on hand-crafted or tunable weights on development sets (Liu and Lane, 2017; Mrkˇsi´c et al., 2017; Rastogi et al., 2018). The challenge here is to combine two sequence generators with varied vocabularies and sequence lengths. The model is sensitive during training and nontrivial to generate an optimal weight. To address this issue, we opt for an uncertainty loss (Kendall et al., 2018) to adaptively adjust the weight according to task-speciﬁc uncertainty. We conduct extensive studies on a largescale task-oriented dataset to evaluate the model. The experimental results conﬁrm the effectiveness of our model with very favorable performance over several state-of-the-art methods. The contributions of this work includ"
2020.acl-main.638,W17-5518,0,0.0491313,"iﬁc one-hot vectors. Each dimension of the vectors is predicted by a binary classiﬁer. To improve response generation, Pei et al. (2019) proposed to learn different expert decoders for different domains and acts, and combined them with a chair decoder. Mehri et al. (2019) applied a cold-fusion method (Sriram et al., 2018) to combine their response decoder with a language model. Zhao et al. (2019) treated dialogue acts as latent variables and used reinforcement learning to optimize them. Reinforcement learning was also applied to ﬁnd optimal dialogue policies in task-oriented dialogue systems (Su et al., 2017; 7126 Williams et al., 2017) or obtain higher dialog-level rewards in chatting (Li et al., 2016b; Serban et al., 2017). Besides, Chen et al. (2019) proposed to predict the acts explicitly with a compact act graph representation and employed hierarchical disentangled self-attention to control response text generation. Unlike those pipeline architectures, joint learning approaches try to explore the interactions between act prediction and response generation. A large body of research in this direction uses a shared user utterance encoder and train natural language understanding jointly with dia"
2020.acl-main.638,N18-2010,0,0.0147304,"have seen a rapid proliferation of interests in this task from both academia and industry (Bordes et al., 2017; Budzianowski et al., 2018; Wu et al., 2019). A standard architecture of these systems generally decomposes this task into several subtasks, including natural language understanding (Gupta et al., 2018), dialogue state tracking (Zhong et al., 2018) and natural language ∗ Belief State: restaurant-{food=Indian, name=Curry Garden} Xiaojun Quan is the corresponding author of this paper. Most of this work was done when Kai Wang was working as an intern at Alibaba DAMO Academy. generation (Su et al., 2018). They can be modeled separately and combined into a pipeline system. Figure 1 shows a dialogue example, from which we can notice that the natural language generation subtask can be further divided into dialogue act prediction and response generation (Chen et al., 2019; Zhao et al., 2019; Wen et al., 2017). While the former is intended to predict the next action(s) based on current conversational state and database information, response generation is used to produce a natural language response based on the action(s). In order for dialogues to be natural and effective, responses should be ﬂuent"
2020.acl-main.638,N19-1123,0,0.396289,"nding (Gupta et al., 2018), dialogue state tracking (Zhong et al., 2018) and natural language ∗ Belief State: restaurant-{food=Indian, name=Curry Garden} Xiaojun Quan is the corresponding author of this paper. Most of this work was done when Kai Wang was working as an intern at Alibaba DAMO Academy. generation (Su et al., 2018). They can be modeled separately and combined into a pipeline system. Figure 1 shows a dialogue example, from which we can notice that the natural language generation subtask can be further divided into dialogue act prediction and response generation (Chen et al., 2019; Zhao et al., 2019; Wen et al., 2017). While the former is intended to predict the next action(s) based on current conversational state and database information, response generation is used to produce a natural language response based on the action(s). In order for dialogues to be natural and effective, responses should be ﬂuent, informative, and relevant. Nevertheless, current sequence-to-sequence models often generate uninformative responses like “I don’t know” (Li et al., 2016a), hindering the dialogues to continue or even leading to a failure. Some researchers (Pei et al., 2019; Mehri et al., 7125 Proceedin"
2020.acl-main.638,P16-1008,0,0.0364706,"Missing"
2020.acl-main.638,D15-1199,0,0.0940819,"Missing"
2020.acl-main.638,P17-1062,0,0.0126509,"ach dimension of the vectors is predicted by a binary classiﬁer. To improve response generation, Pei et al. (2019) proposed to learn different expert decoders for different domains and acts, and combined them with a chair decoder. Mehri et al. (2019) applied a cold-fusion method (Sriram et al., 2018) to combine their response decoder with a language model. Zhao et al. (2019) treated dialogue acts as latent variables and used reinforcement learning to optimize them. Reinforcement learning was also applied to ﬁnd optimal dialogue policies in task-oriented dialogue systems (Su et al., 2017; 7126 Williams et al., 2017) or obtain higher dialog-level rewards in chatting (Li et al., 2016b; Serban et al., 2017). Besides, Chen et al. (2019) proposed to predict the acts explicitly with a compact act graph representation and employed hierarchical disentangled self-attention to control response text generation. Unlike those pipeline architectures, joint learning approaches try to explore the interactions between act prediction and response generation. A large body of research in this direction uses a shared user utterance encoder and train natural language understanding jointly with dialogue state tracking (Mrkˇsi´"
2020.acl-main.638,P19-1078,0,0.0285425,"and their phone number is 012233023302. Would you like me to book a table3 for you? Figure 1: An example of dialogue from the MultiWOZ dataset, where the dialogue system needs to generate a natural language response according to current belief state and related database records. Introduction Task-oriented dialogue systems aim to facilitate people with such services as hotel reservation and ticket booking through natural language conversations. Recent years have seen a rapid proliferation of interests in this task from both academia and industry (Bordes et al., 2017; Budzianowski et al., 2018; Wu et al., 2019). A standard architecture of these systems generally decomposes this task into several subtasks, including natural language understanding (Gupta et al., 2018), dialogue state tracking (Zhong et al., 2018) and natural language ∗ Belief State: restaurant-{food=Indian, name=Curry Garden} Xiaojun Quan is the corresponding author of this paper. Most of this work was done when Kai Wang was working as an intern at Alibaba DAMO Academy. generation (Su et al., 2018). They can be modeled separately and combined into a pipeline system. Figure 1 shows a dialogue example, from which we can notice that the"
2020.acl-main.757,P18-1163,0,0.0362862,"Missing"
2020.acl-main.757,P05-1033,0,0.158897,"the learning of the gates with supervision automatically generated using pointwise mutual information. Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline. 1 Attention h si ti Introduction An essence to modeling translation is how to learn an effective context from a sentence pair. Statistical machine translation (SMT) models the source context from the source-side of a translation model and models the target context from a target-side language model (Koehn et al., 2003; Koehn, 2009; Chiang, 2005). These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Prior work on attention mechanism (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Chen et al., 2018; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020) have shown a better context representation is help"
2020.acl-main.757,K18-1010,0,0.0154869,"t context from a target-side language model (Koehn et al., 2003; Koehn, 2009; Chiang, 2005). These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Prior work on attention mechanism (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Chen et al., 2018; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020) have shown a better context representation is helpful to translation performance. zi 1 − zi + Transformer: I often play golf with my colleagues . Context Gates: I often play golf with my colleagues . Regularized : Context Gates I often play soccer with my colleagues . Figure 1: A running example to raise the context control problem. Both original and context gated Transformer obtain an unfaithful translation by wrongly translate “ti¯ q´ıu” into “play golf” because referring too much target context. By regularizing the context gates, the purposed method corrects the transla"
2020.acl-main.757,P09-5002,0,0.0325421,"thod to guide the learning of the gates with supervision automatically generated using pointwise mutual information. Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline. 1 Attention h si ti Introduction An essence to modeling translation is how to learn an effective context from a sentence pair. Statistical machine translation (SMT) models the source context from the source-side of a translation model and models the target context from a target-side language model (Koehn et al., 2003; Koehn, 2009; Chiang, 2005). These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Prior work on attention mechanism (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Chen et al., 2018; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020) have shown a better context represe"
2020.acl-main.757,N03-1017,0,0.0987422,"a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information. Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline. 1 Attention h si ti Introduction An essence to modeling translation is how to learn an effective context from a sentence pair. Statistical machine translation (SMT) models the source context from the source-side of a translation model and models the target context from a target-side language model (Koehn et al., 2003; Koehn, 2009; Chiang, 2005). These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Prior work on attention mechanism (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Chen et al., 2018; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020) have shown a better co"
2020.acl-main.757,D15-1166,0,0.0472561,"models the source context from the source-side of a translation model and models the target context from a target-side language model (Koehn et al., 2003; Koehn, 2009; Chiang, 2005). These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Prior work on attention mechanism (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Chen et al., 2018; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020) have shown a better context representation is helpful to translation performance. zi 1 − zi + Transformer: I often play golf with my colleagues . Context Gates: I often play golf with my colleagues . Regularized : Context Gates I often play soccer with my colleagues . Figure 1: A running example to raise the context control problem. Both original and context gated Transformer obtain an unfaithful translation by wrongly translate “ti¯ q´ıu” into “play golf” because referring too muc"
2020.acl-main.757,P18-2053,0,0.0354561,"Missing"
2020.acl-main.757,D16-1249,0,0.0143158,"urce-side of a translation model and models the target context from a target-side language model (Koehn et al., 2003; Koehn, 2009; Chiang, 2005). These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Prior work on attention mechanism (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Chen et al., 2018; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020) have shown a better context representation is helpful to translation performance. zi 1 − zi + Transformer: I often play golf with my colleagues . Context Gates: I often play golf with my colleagues . Regularized : Context Gates I often play soccer with my colleagues . Figure 1: A running example to raise the context control problem. Both original and context gated Transformer obtain an unfaithful translation by wrongly translate “ti¯ q´ıu” into “play golf” because referring too much target context. By regularizing t"
2020.acl-main.757,P02-1040,0,0.107896,"gy to measure the correlation between yi and a sentence (x or y<i ). Indeed, it is similar to use the average strategy, but we did not find its gains over max in our experiments. 3 Experiments The proposed methods are evaluated on NIST ZH⇒EN 3 , WMT14 EN⇒DE 4 , IWSLT14 DE⇒EN 5 and IWSLT17 FR⇒EN 6 tasks. To make our NMT models capable of open-vocabulary translation, all datasets are preprocessed with Byte Pair Encoding (Sennrich et al., 2015). All proposed methods are implemented on top of Transformer (Vaswani et al., 2017) which is the state-of-the-art NMT system. Case-insensitive BLEU score (Papineni et al., 2002) is used to evaluate translation quality of ZH⇒EN, DE⇒EN and FR⇒EN. For the fair comparison with the related work, EN⇒DE is evaluated with case-sensitive BLEU score. Setup details are described in Appendix A. 3.1 Tuning Regularization Coefficient In the beginning of our experiments, we tune the regularization coefficient λ on the DE⇒EN task. Table 2 shows the robustness of λ, because the translation performance only fluctuates slightly over various λ. In particular, the best performance 3 where C (µ) and C (ν) are word counts, C (µ, ν) is the co-occurrence count of words µ and ν, and Z is the"
2020.acl-main.757,Q17-1007,0,0.46272,"slation by wrongly translate “ti¯ q´ıu” into “play golf” because referring too much target context. By regularizing the context gates, the purposed method corrects the translation of “ti¯ q´ıu” into “play soccer”. The light font denotes the target words to be translated in the future. For original Transformer, the source and target context are added directly without any rebalancing. However, a standard NMT system is incapable of effectively controlling the contributions from source and target contexts (He et al., 2018) to deliver highly adequate translations as shown in Figure 1. As a result, Tu et al. (2017) carefully designed context gates to dynamically control the influence from source and target contexts and observed significant improvements in the recurrent neural network (RNN) based NMT. Although Transformer (Vaswani et al., 2017) delivers significant gains over RNN for translation, there are still one third translation errors related to context control problem as described in Section 3.3. Obviously, it is feasible to extend the context gates in RNN based NMT into Transformer, but an obstacle to accomplishing this goal is the complicated archi8555 Proceedings of the 58th Annual Meeting of t"
2020.acl-main.757,P19-1124,1,0.917702,"l Linguistics, pages 8555–8562 c July 5 - 10, 2020. 2020 Association for Computational Linguistics tecture in Transformer, where the source and target words are tightly coupled. Thus, it is challenging to put context gates into practice in Transformer. In this paper, under the Transformer architecture, we firstly provide a way to define the source and target contexts and then obtain our model by combining both source and target contexts with context gates, which actually induces a probabilistic model indicating whether the next generated word is contributed from the source or target sentence (Li et al., 2019). In our preliminary experiments, this model only achieves modest gains over Transformer because the context selection error reduction is very limited as described in Section 3.3. To further address this issue, we propose a probabilistic model whose loss function is derived from external supervision as regularization for the context gates. This probabilistic model is jointly trained with the context gates in NMT. As it is too costly to annotate this supervision for a large-scale training corpus manually, we instead propose a simple yet effective method to automatically generate supervision usi"
2020.acl-main.757,N18-1125,1,0.840967,"models the target context from a target-side language model (Koehn et al., 2003; Koehn, 2009; Chiang, 2005). These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Prior work on attention mechanism (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Chen et al., 2018; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020) have shown a better context representation is helpful to translation performance. zi 1 − zi + Transformer: I often play golf with my colleagues . Context Gates: I often play golf with my colleagues . Regularized : Context Gates I often play soccer with my colleagues . Figure 1: A running example to raise the context control problem. Both original and context gated Transformer obtain an unfaithful translation by wrongly translate “ti¯ q´ıu” into “play golf” because referring too much target context. By regularizing the context gates, the purposed metho"
2020.acl-main.757,C16-1291,1,0.731547,"ontext from the source-side of a translation model and models the target context from a target-side language model (Koehn et al., 2003; Koehn, 2009; Chiang, 2005). These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Prior work on attention mechanism (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Chen et al., 2018; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020) have shown a better context representation is helpful to translation performance. zi 1 − zi + Transformer: I often play golf with my colleagues . Context Gates: I often play golf with my colleagues . Regularized : Context Gates I often play soccer with my colleagues . Figure 1: A running example to raise the context control problem. Both original and context gated Transformer obtain an unfaithful translation by wrongly translate “ti¯ q´ıu” into “play golf” because referring too much target context."
2020.acl-main.757,D09-1051,0,0.022876,"golden zi∗ are inaccessible for each word yi in the training corpus, we ideally have to annotate it manually. However, it is costly for human to label such a large scale dataset. Instead, we propose an automatic method to generate its value in practice in the next subsection. 2.3 Generating Supervision zi∗ To decide whether yi is contributed from the source (x) or target sentence (y<i ) (Li et al., 2019), a metric to measure the correlation between a pair of words (hyi , xj i or hyi , yk i for k < i) is first required. This is closely related to a well-studied problem, i.e., word collocation (Liu et al., 2009), and we simply employ the pointwise mutual information (PMI) to measure the correlation between a word pair hµ, νi following Bouma (2009): (µ,ν) pmi (µ, ν) = log PP(µ)P (ν) C(µ,ν) = log Z + log C(µ)C(ν) , (6) possible (µ, ν) pairs. To obtain the context gates, we define two types of PMI according to different C (µ, ν) including two scenarios as follows. PMI in the Bilingual Scenario For each parallel sentence pair hx, yi in training set, C (yi , xj ) is added by one if both yi ∈ y and xj ∈ x. PMI in the Monolingual Scenario In the translation scenario, only the words in the preceding context"
2020.acl-main.757,D18-1049,0,0.0253974,"Missing"
2020.acl-main.757,D18-1036,0,0.0295875,"Missing"
2020.coling-main.266,C18-1233,0,0.0180067,"kinds of methods, syntax-agnostic methods, which focus on the SRL problem itself, and syntax-aware methods, which explore various ways to integrate syntactic knowledge into the SRL models. Zhou and Xu (2015) propose to use deep BiLSTMs for English spanbased SRL. He et al. (2017) further employ several deep learning advanced practices into the stacked BiLSTMs. With the rise of Transformer in machine translation, Tan et al. (2018) employ deep self-attention encoder for SRL, achieving strong performance. Marcheggiani et al. (2017) propose a simple and fast model with rich input representations. Cai et al. (2018) present a full end-to-end model which composed of BiLSTM encoder and BiAffine scorer. He et al. (2018a) first treat SRL as a predicate-argument-role tuple identification task. Following the trend, Li et al. (2019) extend this framework for both span-based and dependency-based SRL, with constraining the argument length to be 1 for dependency-based SRL. Syntactic knowledge has been explored in various ways to promote the performance of SRL models. Roth and Lapata (2016) propose to integrate the dependency path embeddings into the basic SRL model for dependency-based SRL. Strubell et al. (2018)"
2020.coling-main.266,W05-0620,0,0.133682,"Missing"
2020.coling-main.266,N19-1423,0,0.0299973,"cit to take advantage of heterogeneous syntactic knowledge, which we believe are highly complementary. Our baseline model follows the architecture of He et al. (2018a). Afterwards, we inject the heterogeneous syntactic knowledge into the base model using two proposed methods. For the explicit method, we try to encode the heterogeneous automatic dependency trees with the recent popular graph convolutional networks (GCN) (Kipf and Welling, 2016). For the implicit method, which is inspired by the powerful representations from pre-trained language models, like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), we introduce a method to extract implicit syntactic representations from the dependency parser trained with heterogeneous syntactic treebanks. It is well known that the main reason for the success of pre-trained language model representations is the use of large amounts of natural text. However, it is difficult to obtain and costly to annotate large amounts of syntactic data. Therefore, making full use of existing heterogeneous data is the most feasible and natural idea. Intuitively, the explicit method models the syntactic structure of a sentence, providing valuable syntactic position infor"
2020.coling-main.266,Q19-1019,0,0.0241109,"racting syntactic knowledge from heterogeneous dependency treebanks. First, we introduce the method for encoding singleton dependency trees and then detailedly describe the variations to extract heterogeneous syntactic knowledge. 3.1 Syntactic Representation We employ two different methods to fully encode the homogeneous syntactic trees, i.e., GCN that encodes the syntactic structures and implicit representations that encode the syntactic features. Explicit Method. Graph convolutional networks (GCN) are neural networks that work on graph structures, which have been explored in many NLP tasks (Guo et al., 2019; Zhang et al., 2020). Formally, we denote an undirected graph as G = (V, E), where V and E are the set of nodes and edges, respectively. The GCN computation flow of node v ∈ V at l-th layer is defined as ! X l (3) hlv = ρ Wl hl−1 u +b , u∈N (v) where Wl ∈ Rm×m is the weight matrix, bl ∈ Rm is the bias term, N (v) is the set of all one-hop neighbour nodes of v, and ρ is an activation function. Especially, h0u ∈ Rm is the initial input representation, and m is the representation dimension. In our work, we employ a 1-layer BiLSTM encoder over the input layer1 , and treat the BiLSTM outputs as th"
2020.coling-main.266,P17-1044,0,0.505726,"ct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syntactic knowledge into syntax-agnostic models. Roth and Lapata"
2020.coling-main.266,P18-2058,0,0.091085,"gnificant improvements over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syn"
2020.coling-main.266,P18-1192,0,0.575347,"gnificant improvements over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syn"
2020.coling-main.266,D19-1538,0,0.183322,"cate-argument-role tuple identification task. Following the trend, Li et al. (2019) extend this framework for both span-based and dependency-based SRL, with constraining the argument length to be 1 for dependency-based SRL. Syntactic knowledge has been explored in various ways to promote the performance of SRL models. Roth and Lapata (2016) propose to integrate the dependency path embeddings into the basic SRL model for dependency-based SRL. Strubell et al. (2018) propose a multi-task learning framework based on the self-attention encoder, which treats dependency parsing as an auxiliary task. He et al. (2019) propose an argument pruning method based on dependency tree positions for multilingual SRL. Recently, Xia et al. (2019a) propose a similar framework to extract syntactic representation for SRL, but they only focus on Chinese SRL. Xia et al. (2019b) compares four explicit methods to encode automatic dependency trees for SRL. Zhang et al. (2019) present different methods to encode dependency trees and compare various incorporation ways into a self-attention based SRL model. These previous works mainly focus on encoding single-sourced dependency treebank, which can only provide limited syntactic"
2020.coling-main.266,2020.acl-main.744,0,0.187854,"tperforming Ouchi et al. (2018) by +0.2 F1. In the end-to-end setting of the CoNLL-2005 dataset, our model achieves 86.95 and 80.53 F1 scores in the WSJ and Brown test data, respectively. Ablation study on heterogeneous treebanks. Even though integrating syntactic knowledge into SRL brings significant improvements to the CPB1.0 dataset, we also find that it is not obvious on the CoNLL2005 dataset. To know why the improvements on CoNLL-2005 are smaller than on the CPB1.0 dataset, 2986 Dev Models Predefined predicates. He et al. (2018a) He et al. (2018a) (w/ ELMo) Ouchi et al. (2018) (w/ ELMo)? Li et al. (2020) (w/ RoBERTa) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) End-to-end setting. He et al. (2018a) He et al. (2018a) (w/ ELMo) Baseline HybridHDP Baseline(w/ RoBERTa) HybridHDP (w/ RoBERTa) WSJ Brown P R F1 P R F1 P R F1 88.0 87.24 82.28 83.65 86.87 86.99 86.9 87.26 82.76 84.06 87.89 87.41 87.4 87.25 82.52 83.85 87.38 87.20 89.2 88.05 84.21 85.12 88.11 88.43 87.9 88.00 84.39 85.0 88.64 88.75 83.9 87.4 88.5 88.03 84.30 85.06 88.37 88.59 81.0 80.04 74.37 76.3 82.49 83.05 78.4 79.56 73.59 75.42 83.51 83.28 73.7 80.4 79.6 79.80 73.98 75.86 83.00 83.16 81.53 82.95 85.81 85.93 82.15"
2020.coling-main.266,K17-1041,0,0.102309,"s because of the development of deep learning. Previous works can mostly be divided into two kinds of methods, syntax-agnostic methods, which focus on the SRL problem itself, and syntax-aware methods, which explore various ways to integrate syntactic knowledge into the SRL models. Zhou and Xu (2015) propose to use deep BiLSTMs for English spanbased SRL. He et al. (2017) further employ several deep learning advanced practices into the stacked BiLSTMs. With the rise of Transformer in machine translation, Tan et al. (2018) employ deep self-attention encoder for SRL, achieving strong performance. Marcheggiani et al. (2017) propose a simple and fast model with rich input representations. Cai et al. (2018) present a full end-to-end model which composed of BiLSTM encoder and BiAffine scorer. He et al. (2018a) first treat SRL as a predicate-argument-role tuple identification task. Following the trend, Li et al. (2019) extend this framework for both span-based and dependency-based SRL, with constraining the argument length to be 1 for dependency-based SRL. Syntactic knowledge has been explored in various ways to promote the performance of SRL models. Roth and Lapata (2016) propose to integrate the dependency path em"
2020.coling-main.266,H94-1020,0,0.129263,"019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syntactic knowledge into syntax-agnostic models. Roth and Lapata (2016) propose to use dependency-based embeddings in a neural SRL model for dependency-based SRL. He et al. (2018b) introduce k-order pruning algorithm to prune arguments according to dependency trees. However, previous syntax-aware works mainly employ singleton/homogeneous automatic dependency trees, which are generated by a syntactic parser trained on a specific syntactic treebank, like Penn Treebank (PTB) (Marcus et al., 1994). Our work follows the syntax-aware approach and enhances SRL with heterogeneous syntactic knowledge. We define heterogeneous syntactic treebanks as treebanks that follow different annotation guidelines. All is well known, there exist many published dependency treebanks that follow different annotation guidelines, i.e., English PTB (Marcus et al., 1994), Universal Dependencies (UD) (Silveira et al., 2014), Penn Chinese Treebank (PCTB) (Xue et al., 2005), Chinese Dependency Treebank (CDT) (Che et ∗ Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 Internationa"
2020.coling-main.266,D18-1191,0,0.170603,"ate-argument structures. 4.3 Results and Analyses on English SRL Table 5 shows our results on English CoNLL-2005 development and test data, where WSJ is the indomain data and Brown is the out-of-domain data. Our implemented baseline model achieves slightly higher performance than the model (He et al., 2018a) we follow. The proposed methods can further improve our baseline model by +0.76 (p &lt; 1e-4) and +1.88 (p &lt; 1e-4) F1 scores on WSJ and Brown test data, respectively. With the help of RoBERTa representations, our full model achieves 88.59 F1 score on the test WSJ data, slightly outperforming Ouchi et al. (2018) by +0.2 F1. In the end-to-end setting of the CoNLL-2005 dataset, our model achieves 86.95 and 80.53 F1 scores in the WSJ and Brown test data, respectively. Ablation study on heterogeneous treebanks. Even though integrating syntactic knowledge into SRL brings significant improvements to the CPB1.0 dataset, we also find that it is not obvious on the CoNLL2005 dataset. To know why the improvements on CoNLL-2005 are smaller than on the CPB1.0 dataset, 2986 Dev Models Predefined predicates. He et al. (2018a) He et al. (2018a) (w/ ELMo) Ouchi et al. (2018) (w/ ELMo)? Li et al. (2020) (w/ RoBERTa) B"
2020.coling-main.266,D14-1162,0,0.0850612,"Missing"
2020.coling-main.266,N18-1202,0,0.0261584,"rspective of explicit and implicit to take advantage of heterogeneous syntactic knowledge, which we believe are highly complementary. Our baseline model follows the architecture of He et al. (2018a). Afterwards, we inject the heterogeneous syntactic knowledge into the base model using two proposed methods. For the explicit method, we try to encode the heterogeneous automatic dependency trees with the recent popular graph convolutional networks (GCN) (Kipf and Welling, 2016). For the implicit method, which is inspired by the powerful representations from pre-trained language models, like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), we introduce a method to extract implicit syntactic representations from the dependency parser trained with heterogeneous syntactic treebanks. It is well known that the main reason for the success of pre-trained language model representations is the use of large amounts of natural text. However, it is difficult to obtain and costly to annotate large amounts of syntactic data. Therefore, making full use of existing heterogeneous data is the most feasible and natural idea. Intuitively, the explicit method models the syntactic structure of a sentence, providing va"
2020.coling-main.266,P16-1113,0,0.444205,"tic knowledge brings significant improvements over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches"
2020.coling-main.266,D16-1212,0,0.0244291,"commonly used Chinese Proposition Bank 1.0 (CPB1.0) (Xue, 2008) and English CoNLL-2005 (Carreras and M`arquez, 2005) benchmarks. We implement our methods and baseline model with Pytorch, and our code, configurations, and models are released in https: //github.com/KiroSummer/HDP-SRL. Heterogeneous Dependency Treebanks. We employ PCTB7 and CDT as the heterogeneous dependency treebanks for Chinese, PTB and UD2 dependency treebanks for English. We employ BiAffine 2 We use the combination of EWT, GUM, LinES, and ParTUT of UD English corpus in our experiments. 2983 Dev Models Predefined predicates. Sha et al. (2016) Xia et al. (2017) Xia et al. (2019a) Xia et al. (2019a) (w/ BERT) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) End-to-end setting. Xia et al. (2019a) Xia et al. (2019a) (w/ BERT) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) Test P R F1 P R F1 81.62 84.92 87.17 86.93 82.36 85.59 86.65 87.02 83.39 81.99 85.25 86.91 86.97 81.94 84.86 87.48 87.93 80.59 84.45 87.11 87.57 77.69 79.67 83.91 87.54 81.26 84.65 87.29 87.75 80.35 83.4 85.47 86.73 80.89 84.23 86.31 86.41 82.39 85.92 80.62 83.81 85.89 86.57 79.82 83.67 86.30 87.04 78.22 82.89 86.14 86.23 81.73 85.57 7"
2020.coling-main.266,silveira-etal-2014-gold,0,0.054242,"Missing"
2020.coling-main.266,D18-1548,0,0.200322,"ents over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syntactic knowledge into syn"
2020.coling-main.266,P17-1189,0,0.0185216,"ese Proposition Bank 1.0 (CPB1.0) (Xue, 2008) and English CoNLL-2005 (Carreras and M`arquez, 2005) benchmarks. We implement our methods and baseline model with Pytorch, and our code, configurations, and models are released in https: //github.com/KiroSummer/HDP-SRL. Heterogeneous Dependency Treebanks. We employ PCTB7 and CDT as the heterogeneous dependency treebanks for Chinese, PTB and UD2 dependency treebanks for English. We employ BiAffine 2 We use the combination of EWT, GUM, LinES, and ParTUT of UD English corpus in our experiments. 2983 Dev Models Predefined predicates. Sha et al. (2016) Xia et al. (2017) Xia et al. (2019a) Xia et al. (2019a) (w/ BERT) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) End-to-end setting. Xia et al. (2019a) Xia et al. (2019a) (w/ BERT) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) Test P R F1 P R F1 81.62 84.92 87.17 86.93 82.36 85.59 86.65 87.02 83.39 81.99 85.25 86.91 86.97 81.94 84.86 87.48 87.93 80.59 84.45 87.11 87.57 77.69 79.67 83.91 87.54 81.26 84.65 87.29 87.75 80.35 83.4 85.47 86.73 80.89 84.23 86.31 86.41 82.39 85.92 80.62 83.81 85.89 86.57 79.82 83.67 86.30 87.04 78.22 82.89 86.14 86.23 81.73 85.57 79.01 83.28 86.22 8"
2020.coling-main.266,D19-1541,1,0.823683,"fied as: ! hlv =ρ X Wl rlu l +b , (5) u∈N (v) l where the weight matrix Wl increases its column dimension by dhidden per layer, i.e., Wl ∈ Rdhidden ×d (dl = d + dhidden × (l − 1)). Implicit Method. Recently popular pre-trained language model embeddings (such as ELMo and BERT) have received much attention. These language models are trained on large amounts of natural text and can produce powerful implicit representations, whose effectiveness is shown in many NLP tasks. Inspired by these pre-trained language model representations and previous works on syntactic representations (Yu et al., 2018; Xia et al., 2019a), we make a trial to train a syntactic parser and extract similar implicit syntactic representations for SRL. We choose the state-of-the-art BiAffine parser (Dozat and Manning, 2017) as our basic dependency parser module. Concisely, BiAffine parser consists of an input layer, BiLSTMs encoder layer, and BiAffine scorers layer, as shown by the right component of Figure 3. We extract the hidden outputs from the 3-layer BiLSTMs encoder of the dependency parser module and make a softmax weighted summation on the outputs as the implicit syntactic representations. 1 The ExpHDP module shares the sam"
2020.coling-main.266,N19-1075,0,0.556792,"fied as: ! hlv =ρ X Wl rlu l +b , (5) u∈N (v) l where the weight matrix Wl increases its column dimension by dhidden per layer, i.e., Wl ∈ Rdhidden ×d (dl = d + dhidden × (l − 1)). Implicit Method. Recently popular pre-trained language model embeddings (such as ELMo and BERT) have received much attention. These language models are trained on large amounts of natural text and can produce powerful implicit representations, whose effectiveness is shown in many NLP tasks. Inspired by these pre-trained language model representations and previous works on syntactic representations (Yu et al., 2018; Xia et al., 2019a), we make a trial to train a syntactic parser and extract similar implicit syntactic representations for SRL. We choose the state-of-the-art BiAffine parser (Dozat and Manning, 2017) as our basic dependency parser module. Concisely, BiAffine parser consists of an input layer, BiLSTMs encoder layer, and BiAffine scorers layer, as shown by the right component of Figure 3. We extract the hidden outputs from the 3-layer BiLSTMs encoder of the dependency parser module and make a softmax weighted summation on the outputs as the implicit syntactic representations. 1 The ExpHDP module shares the sam"
2020.coling-main.266,J08-2004,0,0.0450443,"pendency data completes the forward process. 3.3 Hybrid HDP Our model combines the two representations together, according to our intuition that explicit and implicit syntactic representations are highly complementary, which is denoted as “HybridHDP” (Hybrid Heterogeneous Dependency Parsing) in later sections. In detail, we concatenate the two heterogeneous l isr syntactic representations with the SRL input, formulated as xi = embword ⊕ repchar wi wi ⊕ hv ⊕ hi . 4 Experiments and Analysis 4.1 Experimental Setup We conduct experiments on the commonly used Chinese Proposition Bank 1.0 (CPB1.0) (Xue, 2008) and English CoNLL-2005 (Carreras and M`arquez, 2005) benchmarks. We implement our methods and baseline model with Pytorch, and our code, configurations, and models are released in https: //github.com/KiroSummer/HDP-SRL. Heterogeneous Dependency Treebanks. We employ PCTB7 and CDT as the heterogeneous dependency treebanks for Chinese, PTB and UD2 dependency treebanks for English. We employ BiAffine 2 We use the combination of EWT, GUM, LinES, and ParTUT of UD English corpus in our experiments. 2983 Dev Models Predefined predicates. Sha et al. (2016) Xia et al. (2017) Xia et al. (2019a) Xia et a"
2020.coling-main.266,C18-1047,0,0.0252679,"yer would be modified as: ! hlv =ρ X Wl rlu l +b , (5) u∈N (v) l where the weight matrix Wl increases its column dimension by dhidden per layer, i.e., Wl ∈ Rdhidden ×d (dl = d + dhidden × (l − 1)). Implicit Method. Recently popular pre-trained language model embeddings (such as ELMo and BERT) have received much attention. These language models are trained on large amounts of natural text and can produce powerful implicit representations, whose effectiveness is shown in many NLP tasks. Inspired by these pre-trained language model representations and previous works on syntactic representations (Yu et al., 2018; Xia et al., 2019a), we make a trial to train a syntactic parser and extract similar implicit syntactic representations for SRL. We choose the state-of-the-art BiAffine parser (Dozat and Manning, 2017) as our basic dependency parser module. Concisely, BiAffine parser consists of an input layer, BiLSTMs encoder layer, and BiAffine scorers layer, as shown by the right component of Figure 3. We extract the hidden outputs from the 3-layer BiLSTMs encoder of the dependency parser module and make a softmax weighted summation on the outputs as the implicit syntactic representations. 1 The ExpHDP mod"
2020.coling-main.266,D19-1057,1,0.801133,"propose to integrate the dependency path embeddings into the basic SRL model for dependency-based SRL. Strubell et al. (2018) propose a multi-task learning framework based on the self-attention encoder, which treats dependency parsing as an auxiliary task. He et al. (2019) propose an argument pruning method based on dependency tree positions for multilingual SRL. Recently, Xia et al. (2019a) propose a similar framework to extract syntactic representation for SRL, but they only focus on Chinese SRL. Xia et al. (2019b) compares four explicit methods to encode automatic dependency trees for SRL. Zhang et al. (2019) present different methods to encode dependency trees and compare various incorporation ways into a self-attention based SRL model. These previous works mainly focus on encoding single-sourced dependency treebank, which can only provide limited syntactic knowledge. Our work focus on exploiting heterogeneous dependency benchmarks and the results verify our intuition that heterogeneous syntactic knowledge can provide more valid information. 6 Conclusion We propose to encode heterogeneous syntactic knowledge with explicit and implicit methods to help SRL. For the explicit aspect, we propose ExpHD"
2020.coling-main.266,2020.acl-main.297,1,0.824494,"knowledge from heterogeneous dependency treebanks. First, we introduce the method for encoding singleton dependency trees and then detailedly describe the variations to extract heterogeneous syntactic knowledge. 3.1 Syntactic Representation We employ two different methods to fully encode the homogeneous syntactic trees, i.e., GCN that encodes the syntactic structures and implicit representations that encode the syntactic features. Explicit Method. Graph convolutional networks (GCN) are neural networks that work on graph structures, which have been explored in many NLP tasks (Guo et al., 2019; Zhang et al., 2020). Formally, we denote an undirected graph as G = (V, E), where V and E are the set of nodes and edges, respectively. The GCN computation flow of node v ∈ V at l-th layer is defined as ! X l (3) hlv = ρ Wl hl−1 u +b , u∈N (v) where Wl ∈ Rm×m is the weight matrix, bl ∈ Rm is the bias term, N (v) is the set of all one-hop neighbour nodes of v, and ρ is an activation function. Especially, h0u ∈ Rm is the initial input representation, and m is the representation dimension. In our work, we employ a 1-layer BiLSTM encoder over the input layer1 , and treat the BiLSTM outputs as the input of the GCN mo"
2020.coling-main.266,P15-1109,0,0.0199839,"apparently no because integrating syntactic knowledge into pre-trained language models has attracted some attention (Wang et al., 2020). And unitizing heterogeneous syntactic knowledge would be a direct and natural idea, which we leave for future work. 5 Related Work Recently, SRL has achieved significant improvements because of the development of deep learning. Previous works can mostly be divided into two kinds of methods, syntax-agnostic methods, which focus on the SRL problem itself, and syntax-aware methods, which explore various ways to integrate syntactic knowledge into the SRL models. Zhou and Xu (2015) propose to use deep BiLSTMs for English spanbased SRL. He et al. (2017) further employ several deep learning advanced practices into the stacked BiLSTMs. With the rise of Transformer in machine translation, Tan et al. (2018) employ deep self-attention encoder for SRL, achieving strong performance. Marcheggiani et al. (2017) propose a simple and fast model with rich input representations. Cai et al. (2018) present a full end-to-end model which composed of BiLSTM encoder and BiAffine scorer. He et al. (2018a) first treat SRL as a predicate-argument-role tuple identification task. Following the"
2020.coling-main.374,D18-1549,0,0.0786934,"e the robustness of the UNMT based systems. First of all, we clearly defined two types of noises in training sentences, i.e., word noise and word order noise, and empirically investigate its effect in the UNMT, then we propose adversarial training methods with denoising process in the UNMT. Experimental results on several language pairs show that our proposed methods substantially improved the robustness of the conventional UNMT systems in noisy scenarios. 1 Introduction Recently, unsupervised neural machine translation (UNMT) has attracted great interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Sun et al., 2020b). Typically, UNMT relies solely on monolingual corpora rather than bilingual parallel data in supervised neural machine translation (SNMT) to model translations between the source language and target language and has achieved remarkable results on several translation tasks (Conneau and Lample, 2019). However, previous work only focus on how to build stateof-the-art UNMT systems on the clean data and ignore the robustness of UNMT on the noisy data. In the real-world scenario, there often exists"
2020.coling-main.374,P18-1163,0,0.0194499,"put sentences, for example, word character misspelling, replacement, or word position misordering, etc. The translation model is sensitive to these perturbations, leading to various errors even the perturbations are small. The existing neural translation system, which lacks of robustness, is difficult to be widely applied to the noisy-data scenario (denoted as noisy scenario in the following sections). Therefore, the robustness of neural translation system is not only worthy of being studied, but also very essential in the real-world scenarios. The robustness of SNMT (Belinkov and Bisk, 2018; Cheng et al., 2018; Cheng et al., 2019; Karpukhin et al., 2019) has been well-studied. However, most previous work only focus on the effect of the word substitution for translation performance, and ignore the effect of word order for translation performance. Moreover, the noisy robustness of UNMT is much more difficult since the noisy input data may be relieved in some degree by the SNMT due to its supervised check in training. Currently, there is no study considering the noisy robustness of the UNMT. In this paper, we first define two types of noises which cover the noise types mentioned above, i.e., word nois"
2020.coling-main.374,P19-1425,0,0.025815,"xample, word character misspelling, replacement, or word position misordering, etc. The translation model is sensitive to these perturbations, leading to various errors even the perturbations are small. The existing neural translation system, which lacks of robustness, is difficult to be widely applied to the noisy-data scenario (denoted as noisy scenario in the following sections). Therefore, the robustness of neural translation system is not only worthy of being studied, but also very essential in the real-world scenarios. The robustness of SNMT (Belinkov and Bisk, 2018; Cheng et al., 2018; Cheng et al., 2019; Karpukhin et al., 2019) has been well-studied. However, most previous work only focus on the effect of the word substitution for translation performance, and ignore the effect of word order for translation performance. Moreover, the noisy robustness of UNMT is much more difficult since the noisy input data may be relieved in some degree by the SNMT due to its supervised check in training. Currently, there is no study considering the noisy robustness of the UNMT. In this paper, we first define two types of noises which cover the noise types mentioned above, i.e., word noise and word order noi"
2020.coling-main.374,P18-2006,0,0.0213336,"ask of WMT19 by combining UNMT and unsupervised statistical machine translation. However, previous work only focuses on how to build state-of-the-art UNMT systems and ignore the robustness of UNMT on the noisy data. In this paper, we propose adversarial training methods with denoising process in UNMT training to improve the robustness of the UNMT systems. Moreover, our proposed methods could improve the UNMT performance even in clean scenarios. Actually, Belinkov and Bisk (2018) pointed out that synthetic and natural noise both influenced the translation performance. Belinkov and Bisk (2018), Ebrahimi et al. (2018), and Karpukhin et al. (2019) designed character-level noise, which affects the spelling of a single word, to improve the model robustness. Meanwhile, both textual and phonetic embeddings were used to improve the robustness of SNMT to homophone noises (Liu et al., 2019). Adversarial examples, generated by gradient-based method, attacked the translation model to improve the robustness of SNMT (Cheng et al., 2019). In contrast with this work, we applied adversarial perturbation to the denoising training of UNMT , instead of translation training, to enhance the learning ability of UNMT model. 424"
2020.coling-main.374,D17-1215,0,0.08645,"Missing"
2020.coling-main.374,D19-5506,0,0.0949022,"er misspelling, replacement, or word position misordering, etc. The translation model is sensitive to these perturbations, leading to various errors even the perturbations are small. The existing neural translation system, which lacks of robustness, is difficult to be widely applied to the noisy-data scenario (denoted as noisy scenario in the following sections). Therefore, the robustness of neural translation system is not only worthy of being studied, but also very essential in the real-world scenarios. The robustness of SNMT (Belinkov and Bisk, 2018; Cheng et al., 2018; Cheng et al., 2019; Karpukhin et al., 2019) has been well-studied. However, most previous work only focus on the effect of the word substitution for translation performance, and ignore the effect of word order for translation performance. Moreover, the noisy robustness of UNMT is much more difficult since the noisy input data may be relieved in some degree by the SNMT due to its supervised check in training. Currently, there is no study considering the noisy robustness of the UNMT. In this paper, we first define two types of noises which cover the noise types mentioned above, i.e., word noise and word order noise. Then we empirically ∗"
2020.coling-main.374,P07-2045,0,0.00911683,"nal embedding (Position AT), and the combination of word and positional adversarial training (Both AT), all of which enrich robust information via adversarial perturbation. 5 Experiments 5.1 Datasets We considered two language pairs to do simulated experiments on the Fr↔En and German(De)↔En translation tasks. We used 50 million sentences from WMT monolingual news crawl datasets for each language. To make our experiments comparable with previous work (Conneau and Lample, 2019), we reported results on newstest2014 for Fr↔En and newstest2016 for De↔En. For preprocessing, we used Moses tokenizer (Koehn et al., 2007)1 for all languages. For cleaning, we only applied the Moses script clean-corpus-n.perl to remove lines in the monolingual data containing more than 50 tokens. For BPE (Sennrich et al., 2016b), we used a shared vocabulary for every language pair with 60K subword tokens based on BPE. 5.2 UNMT Settings We used a transformer-based XLM toolkit2 and followed settings of Conneau and Lample (2019) for UNMT: 6 layers for the encoder and the decoder. The dimension of hidden layers was set to 1024. The Adam optimizer (Kingma and Ba, 2015) was used to optimize the model parameters. The initial learning r"
2020.coling-main.374,P19-1291,0,0.0385075,"Missing"
2020.coling-main.374,W19-5330,1,0.743883,"noisy input. 6 Related Work Recently, UNMT (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019) that relies solely on monolingual corpora in each language via bilingual word embedding initialization, denoising auto-encoder, back-translation and sharing latent representations. More recently, Conneau and Lample (2019) and Song et al. (2019) introduced the pretrained cross-lingual language model to achieve state-of-the-art UNMT performance. Sun et al. (2020a) extended UNMT to the multilingual UNMT training on a large scale of European languages. Marie et al. (2019) won the first place in the unsupervised translation task of WMT19 by combining UNMT and unsupervised statistical machine translation. However, previous work only focuses on how to build state-of-the-art UNMT systems and ignore the robustness of UNMT on the noisy data. In this paper, we propose adversarial training methods with denoising process in UNMT training to improve the robustness of the UNMT systems. Moreover, our proposed methods could improve the UNMT performance even in clean scenarios. Actually, Belinkov and Bisk (2018) pointed out that synthetic and natural noise both influenced t"
2020.coling-main.374,D18-1050,0,0.0202074,"ut with different level of noise was 22.76 BLEU scores more in average for the word noise scenario, and 24.09 BLEU scores more in average for the word order noise scenario, compared with the UNMT system. These further demonstrate that our proposed Both AT mechanism is robust and can effectively alleviate the impact of two types of noise on translation performance. 5.5 Evaluation on MTNT dataset To better assess the effectiveness of our proposed adversarial training methods, we investigated the performance of UNMT with Both AT framework on the MTNT dataset, which is a noisy dataset proposed by Michel and Neubig (2018). The detailed statistics of MTNT data set is presented as shown in Table 3. To make our experiments comparable with previous work (Michel and Neubig, 2018; Zhou et al., 2019), we used the same MTNT parallel training data to fine-tune our proposed +Both AT system and used sacreBLEU (Post, 2018) to evaluate the translation performance. Corpus en-fr fr-en Training set Valid set Test set 36,058 852 1,020 19,161 886 1,022 Methods en-fr fr-en Michel and Neubig (2018) +Fine-tuning Zhou et al. (2019) +Fine-tuning 21.77 29.73 n/a n/a 23.27 30.29 24.50 31.70 31.60 39.00 33.80 41.30 +Both AT +Fine-tunin"
2020.coling-main.374,W18-6319,0,0.0183283,"e impact of two types of noise on translation performance. 5.5 Evaluation on MTNT dataset To better assess the effectiveness of our proposed adversarial training methods, we investigated the performance of UNMT with Both AT framework on the MTNT dataset, which is a noisy dataset proposed by Michel and Neubig (2018). The detailed statistics of MTNT data set is presented as shown in Table 3. To make our experiments comparable with previous work (Michel and Neubig, 2018; Zhou et al., 2019), we used the same MTNT parallel training data to fine-tune our proposed +Both AT system and used sacreBLEU (Post, 2018) to evaluate the translation performance. Corpus en-fr fr-en Training set Valid set Test set 36,058 852 1,020 19,161 886 1,022 Methods en-fr fr-en Michel and Neubig (2018) +Fine-tuning Zhou et al. (2019) +Fine-tuning 21.77 29.73 n/a n/a 23.27 30.29 24.50 31.70 31.60 39.00 33.80 41.30 +Both AT +Fine-tuning Table 3: Statistics of MTNT data set. Table 4: BLEU score on the En-Fr MTNT test set. As shown in Table 4, Our proposed +Both AT system significantly outperformed the previous work(Michel and Neubig, 2018; Zhou et al., 2019) by approximately 10 BLEU scores. The performance of our proposed sys"
2020.coling-main.374,P16-1009,0,0.166098,"e the model learning ability by introducing noise in the form of random token deleting and swapping in this input sentence. The denoising auto-encoder, which encodes a noisy version and reconstructs it with the decoder in the same language, acts as a language model during UNMT training. It is optimized by minimizing the objective function: LD = |X| X − log PL1 →L1 (Xi |C(Xi )) + |Y | X − log PL2 →L2 (Yi |C(Yi )), (2) i=1 i=1 where {C(Xi )} and {C(Yi )} are noisy sentences. PL1 →L1 and PL2 →L2 denote the reconstruction probability in the language L1 and L2 , respectively. Back-translation: It (Sennrich et al., 2016a) is adapted to train a translation system across different languages based on monolingual corpora. The pseudo-parallel sentence pairs {(YM (Xi ), Xi )} and {(XM (Yi ), Yi )} produced by the model at the previous iteration would be used to train the new translation model. The UNMT model would be improved through iterative back-translation. Therefore, the back-translation probability would be optimized by minimizing LB = |X| X − log PL2 →L1 (Xi |YM (Xi )) + i=1 |Y | X − log PL1 →L2 (Yi |XM (Yi )), (3) i=1 where PL1 →L2 and PL2 →L1 denote the translation probability across the two languages. Sh"
2020.coling-main.374,P16-1162,0,0.368756,"e the model learning ability by introducing noise in the form of random token deleting and swapping in this input sentence. The denoising auto-encoder, which encodes a noisy version and reconstructs it with the decoder in the same language, acts as a language model during UNMT training. It is optimized by minimizing the objective function: LD = |X| X − log PL1 →L1 (Xi |C(Xi )) + |Y | X − log PL2 →L2 (Yi |C(Yi )), (2) i=1 i=1 where {C(Xi )} and {C(Yi )} are noisy sentences. PL1 →L1 and PL2 →L2 denote the reconstruction probability in the language L1 and L2 , respectively. Back-translation: It (Sennrich et al., 2016a) is adapted to train a translation system across different languages based on monolingual corpora. The pseudo-parallel sentence pairs {(YM (Xi ), Xi )} and {(XM (Yi ), Yi )} produced by the model at the previous iteration would be used to train the new translation model. The UNMT model would be improved through iterative back-translation. Therefore, the back-translation probability would be optimized by minimizing LB = |X| X − log PL2 →L1 (Xi |YM (Xi )) + i=1 |Y | X − log PL1 →L2 (Yi |XM (Yi )), (3) i=1 where PL1 →L2 and PL2 →L1 denote the translation probability across the two languages. Sh"
2020.coling-main.374,P19-1119,1,0.906137,"s of noises in training sentences, i.e., word noise and word order noise, and empirically investigate its effect in the UNMT, then we propose adversarial training methods with denoising process in the UNMT. Experimental results on several language pairs show that our proposed methods substantially improved the robustness of the conventional UNMT systems in noisy scenarios. 1 Introduction Recently, unsupervised neural machine translation (UNMT) has attracted great interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Sun et al., 2020b). Typically, UNMT relies solely on monolingual corpora rather than bilingual parallel data in supervised neural machine translation (SNMT) to model translations between the source language and target language and has achieved remarkable results on several translation tasks (Conneau and Lample, 2019). However, previous work only focus on how to build stateof-the-art UNMT systems on the clean data and ignore the robustness of UNMT on the noisy data. In the real-world scenario, there often exists noises or perturbations in the input sentences, for example, word character missp"
2020.coling-main.374,2020.acl-main.324,1,0.900456,"ining sentences, i.e., word noise and word order noise, and empirically investigate its effect in the UNMT, then we propose adversarial training methods with denoising process in the UNMT. Experimental results on several language pairs show that our proposed methods substantially improved the robustness of the conventional UNMT systems in noisy scenarios. 1 Introduction Recently, unsupervised neural machine translation (UNMT) has attracted great interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Sun et al., 2020b). Typically, UNMT relies solely on monolingual corpora rather than bilingual parallel data in supervised neural machine translation (SNMT) to model translations between the source language and target language and has achieved remarkable results on several translation tasks (Conneau and Lample, 2019). However, previous work only focus on how to build stateof-the-art UNMT systems on the clean data and ignore the robustness of UNMT on the noisy data. In the real-world scenario, there often exists noises or perturbations in the input sentences, for example, word character misspelling, replacemen"
2020.coling-main.374,P18-1005,0,0.100191,"First of all, we clearly defined two types of noises in training sentences, i.e., word noise and word order noise, and empirically investigate its effect in the UNMT, then we propose adversarial training methods with denoising process in the UNMT. Experimental results on several language pairs show that our proposed methods substantially improved the robustness of the conventional UNMT systems in noisy scenarios. 1 Introduction Recently, unsupervised neural machine translation (UNMT) has attracted great interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Sun et al., 2020b). Typically, UNMT relies solely on monolingual corpora rather than bilingual parallel data in supervised neural machine translation (SNMT) to model translations between the source language and target language and has achieved remarkable results on several translation tasks (Conneau and Lample, 2019). However, previous work only focus on how to build stateof-the-art UNMT systems on the clean data and ignore the robustness of UNMT on the noisy data. In the real-world scenario, there often exists noises or perturbations in the input sent"
2020.coling-main.374,W19-5368,0,0.0186829,"with the UNMT system. These further demonstrate that our proposed Both AT mechanism is robust and can effectively alleviate the impact of two types of noise on translation performance. 5.5 Evaluation on MTNT dataset To better assess the effectiveness of our proposed adversarial training methods, we investigated the performance of UNMT with Both AT framework on the MTNT dataset, which is a noisy dataset proposed by Michel and Neubig (2018). The detailed statistics of MTNT data set is presented as shown in Table 3. To make our experiments comparable with previous work (Michel and Neubig, 2018; Zhou et al., 2019), we used the same MTNT parallel training data to fine-tune our proposed +Both AT system and used sacreBLEU (Post, 2018) to evaluate the translation performance. Corpus en-fr fr-en Training set Valid set Test set 36,058 852 1,020 19,161 886 1,022 Methods en-fr fr-en Michel and Neubig (2018) +Fine-tuning Zhou et al. (2019) +Fine-tuning 21.77 29.73 n/a n/a 23.27 30.29 24.50 31.70 31.60 39.00 33.80 41.30 +Both AT +Fine-tuning Table 3: Statistics of MTNT data set. Table 4: BLEU score on the En-Fr MTNT test set. As shown in Table 4, Our proposed +Both AT system significantly outperformed the previo"
2020.coling-main.374,W18-6401,0,\N,Missing
2020.coling-main.374,N19-1120,0,\N,Missing
2020.coling-main.374,W19-5301,0,\N,Missing
2020.coling-main.49,P07-1056,0,0.657022,"set (Ni et al., 2019) and Yelp 2020 challenge dataset5 . Amazon dataset contains 233 million reviews within 29 domains (Ni et al., 2019). The total number of yelp reviews is about 8 million. We preprocess the text via NLTK6 and transfer all the letters into lower. We filter the text that contains less than 50 tokens or more than 512 tokens and sample the rating data in dealing with class-imbalance problem. The statistics information of top-20 emoticons is shown in Table 1. Sentiment Analysis To verify the effectiveness of S ENTI X, We evaluate on the widely used crossdomain sentiment dataset (Blitzer et al., 2007), containing four domains: Books (B), DVD (D), Electronic (E), and Kitchen & Housewares (K). Following the setting of previous works (Ziser and Reichart, 2018; Qu et al., 2019), we test on 12 cross-domain tasks. The model is trained on the source domain and tested on the target domains. As before, we split 200 samples from 2000 samples in source domain as a development set to find the best hyperparameters. Besides, we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang e"
2020.coling-main.49,N19-1423,0,0.527806,"language expressions for sentimental text usually vary across different domains. For instance, “fast” has a positive sentiment towards “service” in the restaurant domain (Figure 1), while in the laptop domain, “fast” expresses a negative sentiment for “power consumption”. Furthermore, models trained on the source domain tend to overfit, since they learn domain-specific knowledge excessively. Therefore, many studies (Du et al., 2020; Ziser and Reichart, 2018; Li et al., 2018) propose to address this issue by extracting domain-invariant features. Recently, pre-trained language models like BERT (Devlin et al., 2019) have achieved the state-of-theart performance on multiple sentiment analysis tasks (Hoang et al., 2019; Munikar et al., 2019; Raffel et al., 2019). However, when they are directly applied to cross-domain sentiment analysis (Du et al., 2020), two problems arise: 1) Existing pre-trained models focus on learning the semantic content via self-supervision strategies, while ignoring sentiment-specific knowledge at the pre-training phrase; 2) ˚ Yuanbin Wu and Liang He are the corresponding authors of this paper. This work was conducted when Jie Zhou was interning at Alibaba DAMO Academy. This work i"
2020.coling-main.49,P14-2009,0,0.0274439,"al., 2019), we test on 12 cross-domain tasks. The model is trained on the source domain and tested on the target domains. As before, we split 200 samples from 2000 samples in source domain as a development set to find the best hyperparameters. Besides, we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang et al., 2015), and three aspect-based sentiment classification datasets: Restaurant14 (Pontiki et al., 2016) and Laptop14 (Pontiki et al., 2016), and Twitter dataset (Dong et al., 2014). 4.2 Baselines We compare our model with the following strong baselines for cross-domain sentiment analysis, including DANN (Ganin et al., 2016), PBLM (Ziser and Reichart, 2018), HATN (Li et al., 2018), ACAN (Qu et al., 2019), IATN (Zhang et al., 2019a) and BERT-DAAT (Du et al., 2020). BERT-DAAT is regarded as the state-of-the-art model, which uses BERT for cross-domain sentiment analysis with adversarial training. We adopt the results of these baselines reported in (Du et al., 2020). For in-domain sentiment analysis, we compare our model with SentiLR-B (Ke et al., 2019), which is one of the"
2020.coling-main.49,2020.acl-main.370,0,0.346247,"entiment analysis has become a promising direction, which transfers (invariant) sentiment knowledge from the source domain to the target domain1 . The major challenge here is that language expressions for sentimental text usually vary across different domains. For instance, “fast” has a positive sentiment towards “service” in the restaurant domain (Figure 1), while in the laptop domain, “fast” expresses a negative sentiment for “power consumption”. Furthermore, models trained on the source domain tend to overfit, since they learn domain-specific knowledge excessively. Therefore, many studies (Du et al., 2020; Ziser and Reichart, 2018; Li et al., 2018) propose to address this issue by extracting domain-invariant features. Recently, pre-trained language models like BERT (Devlin et al., 2019) have achieved the state-of-theart performance on multiple sentiment analysis tasks (Hoang et al., 2019; Munikar et al., 2019; Raffel et al., 2019). However, when they are directly applied to cross-domain sentiment analysis (Du et al., 2020), two problems arise: 1) Existing pre-trained models focus on learning the semantic content via self-supervision strategies, while ignoring sentiment-specific knowledge at th"
2020.coling-main.49,W19-6120,0,0.0132226,"a positive sentiment towards “service” in the restaurant domain (Figure 1), while in the laptop domain, “fast” expresses a negative sentiment for “power consumption”. Furthermore, models trained on the source domain tend to overfit, since they learn domain-specific knowledge excessively. Therefore, many studies (Du et al., 2020; Ziser and Reichart, 2018; Li et al., 2018) propose to address this issue by extracting domain-invariant features. Recently, pre-trained language models like BERT (Devlin et al., 2019) have achieved the state-of-theart performance on multiple sentiment analysis tasks (Hoang et al., 2019; Munikar et al., 2019; Raffel et al., 2019). However, when they are directly applied to cross-domain sentiment analysis (Du et al., 2020), two problems arise: 1) Existing pre-trained models focus on learning the semantic content via self-supervision strategies, while ignoring sentiment-specific knowledge at the pre-training phrase; 2) ˚ Yuanbin Wu and Liang He are the corresponding authors of this paper. This work was conducted when Jie Zhou was interning at Alibaba DAMO Academy. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// cre"
2020.coling-main.49,P11-1015,0,0.0839079,"iment dataset (Blitzer et al., 2007), containing four domains: Books (B), DVD (D), Electronic (E), and Kitchen & Housewares (K). Following the setting of previous works (Ziser and Reichart, 2018; Qu et al., 2019), we test on 12 cross-domain tasks. The model is trained on the source domain and tested on the target domains. As before, we split 200 samples from 2000 samples in source domain as a development set to find the best hyperparameters. Besides, we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang et al., 2015), and three aspect-based sentiment classification datasets: Restaurant14 (Pontiki et al., 2016) and Laptop14 (Pontiki et al., 2016), and Twitter dataset (Dong et al., 2014). 4.2 Baselines We compare our model with the following strong baselines for cross-domain sentiment analysis, including DANN (Ganin et al., 2016), PBLM (Ziser and Reichart, 2018), HATN (Li et al., 2018), ACAN (Qu et al., 2019), IATN (Zhang et al., 2019a) and BERT-DAAT (Du et al., 2020). BERT-DAAT is regarded as the state-of-the-art model, which uses BERT for cross-domain sentiment analysis with"
2020.coling-main.49,D19-1018,0,0.0185346,"): :D (8 <3 ;-) Count 160,970 113,440 86,546 78,492 74,247 # 11 12 13 14 15 Emoticon :/ =) :-( 8: 8) Count 67,615 66,156 64,391 53,073 46,124 # 16 17 18 19 20 Emoticon (: :P ;D :o) =( Count 40,062 31,573 15,718 12,952 11,917 Table 1: Statistics information of top-20 emoticons. 3.3 Joint Training Finally, we jointly optimize the token-level objective LT and the sentence-level objective LS . The overall loss is L “ LT ` LS , where LT “ Lw ` Ls ` Le and LS “ Lr . 4 Experimental Setup 4.1 Datasets Pre-training The pre-training phase is conducted on two large-scale datasets: Amazon review dataset (Ni et al., 2019) and Yelp 2020 challenge dataset5 . Amazon dataset contains 233 million reviews within 29 domains (Ni et al., 2019). The total number of yelp reviews is about 8 million. We preprocess the text via NLTK6 and transfer all the letters into lower. We filter the text that contains less than 50 tokens or more than 512 tokens and sample the rating data in dealing with class-imbalance problem. The statistics information of top-20 emoticons is shown in Table 1. Sentiment Analysis To verify the effectiveness of S ENTI X, We evaluate on the widely used crossdomain sentiment dataset (Blitzer et al., 2007)"
2020.coling-main.49,P18-1233,0,0.0203972,"ce (Accuracy) Epoch 2 Epoch 3 Epoch 4 73.85 84.05 85.45 50.00 52.45 50.80 92.30 92.05 91.10 91.45 92.25 92.75 Epoch 5 86.55 51.10 90.85 93.05 Table 5: The results of complexity and convergence. We list the costed time of each epoch and space complexity of trainable parameters in B Ñ E task with the same batchsize. 6 Related Work Cross-domain Sentiment Analysis Due to the heavy cost of obtaining large quantities of labeled data for each domain, many approaches have been proposed for cross-domain sentiment analysis (Blitzer et al., 2007; Yu and Jiang, 2016; Li et al., 2013; Zhang et al., 2019a; Peng et al., 2018). Most of the previous works focus on capturing the pivots that are useful for both source domain and target domain (Ziser and Reichart, 2018; Li et al., 2018). Domain adaptation adversarial training (Ganin et al., 2016) is widelyused to learn the domain-common sentiment knowledge (Li et al., 2017; Qu et al., 2019). Recently, Du et al. (2020) integrated BERT into cross-domain sentiment analysis tasks to learn the domain-shared feature representation. However, most of the existing work focuses on learning the domain-shared representation in training or fine-tuning, how to learn domain-invariant"
2020.coling-main.49,N18-1202,0,0.0591782,"domain (Ziser and Reichart, 2018; Li et al., 2018). Domain adaptation adversarial training (Ganin et al., 2016) is widelyused to learn the domain-common sentiment knowledge (Li et al., 2017; Qu et al., 2019). Recently, Du et al. (2020) integrated BERT into cross-domain sentiment analysis tasks to learn the domain-shared feature representation. However, most of the existing work focuses on learning the domain-shared representation in training or fine-tuning, how to learn domain-invariant sentiment knowledge from the pre-training phase has not been explored. Pre-trained Model Existing studies (Peters et al., 2018; Devlin et al., 2019) have proved that pretraining on large-scale unlabelled corpus obtains state-of-the-art performances in the field of natural language processing (Qiu et al., 2020). On the one hand, many studies applied pre-trained models to downstream tasks via fine-tuning (Devlin et al., 2019; Dodge et al., 2020; Sun et al., 2019b; Xu et al., 2019). Devlin et al. (2019) fine-tuned the BERT model on many downstream tasks, such as name entity recognition and sentiment analysis. Sun et al. (2019a) converted aspect-based sentiment analysis task into a sentence pair classification task to be"
2020.coling-main.49,D19-1005,0,0.0326198,"Missing"
2020.coling-main.49,N19-1258,0,0.115047,"million. We preprocess the text via NLTK6 and transfer all the letters into lower. We filter the text that contains less than 50 tokens or more than 512 tokens and sample the rating data in dealing with class-imbalance problem. The statistics information of top-20 emoticons is shown in Table 1. Sentiment Analysis To verify the effectiveness of S ENTI X, We evaluate on the widely used crossdomain sentiment dataset (Blitzer et al., 2007), containing four domains: Books (B), DVD (D), Electronic (E), and Kitchen & Housewares (K). Following the setting of previous works (Ziser and Reichart, 2018; Qu et al., 2019), we test on 12 cross-domain tasks. The model is trained on the source domain and tested on the target domains. As before, we split 200 samples from 2000 samples in source domain as a development set to find the best hyperparameters. Besides, we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang et al., 2015), and three aspect-based sentiment classification datasets: Restaurant14 (Pontiki et al., 2016) and Laptop14 (Pontiki et al., 2016), and Twitter dataset (Dong et al"
2020.coling-main.49,D13-1170,0,0.00654472,"s of S ENTI X, We evaluate on the widely used crossdomain sentiment dataset (Blitzer et al., 2007), containing four domains: Books (B), DVD (D), Electronic (E), and Kitchen & Housewares (K). Following the setting of previous works (Ziser and Reichart, 2018; Qu et al., 2019), we test on 12 cross-domain tasks. The model is trained on the source domain and tested on the target domains. As before, we split 200 samples from 2000 samples in source domain as a development set to find the best hyperparameters. Besides, we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang et al., 2015), and three aspect-based sentiment classification datasets: Restaurant14 (Pontiki et al., 2016) and Laptop14 (Pontiki et al., 2016), and Twitter dataset (Dong et al., 2014). 4.2 Baselines We compare our model with the following strong baselines for cross-domain sentiment analysis, including DANN (Ganin et al., 2016), PBLM (Ziser and Reichart, 2018), HATN (Li et al., 2018), ACAN (Qu et al., 2019), IATN (Zhang et al., 2019a) and BERT-DAAT (Du et al., 2020). BERT-DAAT is regarded as the state-of-the-art mode"
2020.coling-main.49,N19-1035,0,0.02339,"tion. However, most of the existing work focuses on learning the domain-shared representation in training or fine-tuning, how to learn domain-invariant sentiment knowledge from the pre-training phase has not been explored. Pre-trained Model Existing studies (Peters et al., 2018; Devlin et al., 2019) have proved that pretraining on large-scale unlabelled corpus obtains state-of-the-art performances in the field of natural language processing (Qiu et al., 2020). On the one hand, many studies applied pre-trained models to downstream tasks via fine-tuning (Devlin et al., 2019; Dodge et al., 2020; Sun et al., 2019b; Xu et al., 2019). Devlin et al. (2019) fine-tuned the BERT model on many downstream tasks, such as name entity recognition and sentiment analysis. Sun et al. (2019a) converted aspect-based sentiment analysis task into a sentence pair classification task to better utilize the powerful representation of BERT. On the other hand, some work proposed to add external knowledge into pre-training BERT to enhance the representations (Zhang et al., 2020). LIBERT (Lauscher et al., 2019) integrated linguistic knowledge through an additional linguistic constraint task. ERINE (Zhang et al., 2019b) and Kno"
2020.coling-main.49,2020.acl-main.374,0,0.0405769,"k into a sentence pair classification task to better utilize the powerful representation of BERT. On the other hand, some work proposed to add external knowledge into pre-training BERT to enhance the representations (Zhang et al., 2020). LIBERT (Lauscher et al., 2019) integrated linguistic knowledge through an additional linguistic constraint task. ERINE (Zhang et al., 2019b) and KnowBERT (Peters 576 et al., 2019) integrated entity representation into BERT. Alternatively, Levine et at. (2019) introduced a SenseBERT to improve lexical understanding by predicting tokens’ supersenses in WordNet. Tian et al. (2020) and Ke et al. (2019) integrated external knowledge to learn sentiment information. They focused on improving the performance with fine-tuning on downstream sentiment analysis tasks by training on a relatively small or one domain dataset. Different from the existing studies, we design several pre-training objectives via rich domain-invariant sentiment knowledge in large-scale multi-domain unlabeled data for cross-domain sentiment analysis. 7 Conclusions In this paper, we pre-train our S ENTI X model to induce a general low dimensional representation based on domain-invariant sentiment knowledg"
2020.coling-main.49,N19-1242,0,0.0236828,"of the existing work focuses on learning the domain-shared representation in training or fine-tuning, how to learn domain-invariant sentiment knowledge from the pre-training phase has not been explored. Pre-trained Model Existing studies (Peters et al., 2018; Devlin et al., 2019) have proved that pretraining on large-scale unlabelled corpus obtains state-of-the-art performances in the field of natural language processing (Qiu et al., 2020). On the one hand, many studies applied pre-trained models to downstream tasks via fine-tuning (Devlin et al., 2019; Dodge et al., 2020; Sun et al., 2019b; Xu et al., 2019). Devlin et al. (2019) fine-tuned the BERT model on many downstream tasks, such as name entity recognition and sentiment analysis. Sun et al. (2019a) converted aspect-based sentiment analysis task into a sentence pair classification task to better utilize the powerful representation of BERT. On the other hand, some work proposed to add external knowledge into pre-training BERT to enhance the representations (Zhang et al., 2020). LIBERT (Lauscher et al., 2019) integrated linguistic knowledge through an additional linguistic constraint task. ERINE (Zhang et al., 2019b) and KnowBERT (Peters 576 e"
2020.coling-main.49,D16-1023,0,0.0202871,"133M 2K 133M 2K Epoch 1 52.20 50.65 92.60 90.55 Convergence (Accuracy) Epoch 2 Epoch 3 Epoch 4 73.85 84.05 85.45 50.00 52.45 50.80 92.30 92.05 91.10 91.45 92.25 92.75 Epoch 5 86.55 51.10 90.85 93.05 Table 5: The results of complexity and convergence. We list the costed time of each epoch and space complexity of trainable parameters in B Ñ E task with the same batchsize. 6 Related Work Cross-domain Sentiment Analysis Due to the heavy cost of obtaining large quantities of labeled data for each domain, many approaches have been proposed for cross-domain sentiment analysis (Blitzer et al., 2007; Yu and Jiang, 2016; Li et al., 2013; Zhang et al., 2019a; Peng et al., 2018). Most of the previous works focus on capturing the pivots that are useful for both source domain and target domain (Ziser and Reichart, 2018; Li et al., 2018). Domain adaptation adversarial training (Ganin et al., 2016) is widelyused to learn the domain-common sentiment knowledge (Li et al., 2017; Qu et al., 2019). Recently, Du et al. (2020) integrated BERT into cross-domain sentiment analysis tasks to learn the domain-shared feature representation. However, most of the existing work focuses on learning the domain-shared representation"
2020.coling-main.49,P19-1139,0,0.330715,"we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang et al., 2015), and three aspect-based sentiment classification datasets: Restaurant14 (Pontiki et al., 2016) and Laptop14 (Pontiki et al., 2016), and Twitter dataset (Dong et al., 2014). 4.2 Baselines We compare our model with the following strong baselines for cross-domain sentiment analysis, including DANN (Ganin et al., 2016), PBLM (Ziser and Reichart, 2018), HATN (Li et al., 2018), ACAN (Qu et al., 2019), IATN (Zhang et al., 2019a) and BERT-DAAT (Du et al., 2020). BERT-DAAT is regarded as the state-of-the-art model, which uses BERT for cross-domain sentiment analysis with adversarial training. We adopt the results of these baselines reported in (Du et al., 2020). For in-domain sentiment analysis, we compare our model with SentiLR-B (Ke et al., 2019), which is one of the state-of-the-art models based on BERT. BERT is extensively compared in our experiments. To exclude the impact of the pre-training dataset, we also compare S ENTI X with BERT˚ , which pre-trains on the same dataset with standard MLM task. Moreover, to v"
2020.coling-main.49,N18-1112,0,0.264706,"has become a promising direction, which transfers (invariant) sentiment knowledge from the source domain to the target domain1 . The major challenge here is that language expressions for sentimental text usually vary across different domains. For instance, “fast” has a positive sentiment towards “service” in the restaurant domain (Figure 1), while in the laptop domain, “fast” expresses a negative sentiment for “power consumption”. Furthermore, models trained on the source domain tend to overfit, since they learn domain-specific knowledge excessively. Therefore, many studies (Du et al., 2020; Ziser and Reichart, 2018; Li et al., 2018) propose to address this issue by extracting domain-invariant features. Recently, pre-trained language models like BERT (Devlin et al., 2019) have achieved the state-of-theart performance on multiple sentiment analysis tasks (Hoang et al., 2019; Munikar et al., 2019; Raffel et al., 2019). However, when they are directly applied to cross-domain sentiment analysis (Du et al., 2020), two problems arise: 1) Existing pre-trained models focus on learning the semantic content via self-supervision strategies, while ignoring sentiment-specific knowledge at the pre-training phrase; 2)"
2020.emnlp-main.310,W13-0102,0,0.375472,"e compare the performance of our model with LDA (Blei et al., 2003), NVDM (Miao et al., 2016), ProdLDA (Srivastava and Sutton, 2017), GraphBTM (Zhu et al., 2018), ATM (Wang et al., 2019a) and W-LDA (Nan et al., 2019) using topic coherence measures (R¨oder et al., 2015). To quantify the understandability of the extracted topics, a topic coherence measure aggregates the relatedness scores of the topic words (topweighted words) of each topic, where the word relatedness scores are estimated based on word co-occurrence statistics on a large external corpus. For example, the NPMI coherence measure (Aletras and Stevenson, 2013) applies a sliding window of size 10 over the Wikipedia corpus to calculate NPMI (Bouma, 2009) for word pairs. We use three topic coherence measures in our experiments: C A (Aletras and Stevenson, 2013), C P (R¨oder et al., 2015), and NPMI. The topic coherence scores are calculated using Palmetto (R¨oder et al., 2015) 1 . Dataset CA CP NPMI LDA NVDM ProdLDA GraphBTM ATM W-LDA GTM 0.1769 0.1432 0.2155 0.2195 0.1720 0.2065 0.2465 0.2362 −0.2558 0.1859 0.2152 0.1914 0.2501 0.3451 0.0524 −0.0984 −0.0083 0.0082 0.0207 0.0400 0.0629 Grolier LDA NVDM ProdLDA ATM W-LDA GTM 0.2009 0.1457 0.1734 0.2189"
2020.emnlp-main.310,P19-1640,0,0.343387,"(VAE) (Kingma and Welling, 2013) to model topic inference and document generation. Specifically, NVDM consists of an encoder inferring topics from documents and a decoder generating documents from topics, where the latent topics are constrained by a Gaussian prior. Srivastava and ∗ Corresponding author. Sutton (2017) argued that Dirichlet distribution is a more appropriate prior for topic modeling than Gaussian in NVDM and proposed ProdLDA that approximates the Dirichlet prior with logistic normal. There are also attempts that directly enforced a Dirichlet prior on the document topics. W-LDA (Nan et al., 2019) models topics in the Wasserstein autoencoders (Tolstikhin et al., 2017) framework and achieves distribution matching by minimizing their Maximum Mean Discrepancy (MMD) (Gretton et al., 2012), while adversarial topic model (Wang et al., 2019a,b, 2020) directly generates documents from the Dirichlet prior and such a process is adversarially trained with a discriminator under the framework of Generative Adversarial Network (GAN) (Goodfellow et al., 2014). Recently, due to the effectiveness of Graph Neural Networks (GNNs) (Li et al., 2015; Kipf and Welling, 2016; Zhou et al., 2018) in embedding g"
2020.emnlp-main.310,P18-1150,0,0.0239637,"imizing their Maximum Mean Discrepancy (MMD) (Gretton et al., 2012), while adversarial topic model (Wang et al., 2019a,b, 2020) directly generates documents from the Dirichlet prior and such a process is adversarially trained with a discriminator under the framework of Generative Adversarial Network (GAN) (Goodfellow et al., 2014). Recently, due to the effectiveness of Graph Neural Networks (GNNs) (Li et al., 2015; Kipf and Welling, 2016; Zhou et al., 2018) in embedding graph structures, there is a surge of interests of applying GNN to natural language processing tasks (Yasunaga et al., 2017; Song et al., 2018; Yao et al., 2019). For example, GraphBTM (Zhu et al., 2018) is a neural topic model that incorporates the graph representation of a document to capture biterm cooccurrences in the document. To construct the graph, a sliding window over the document is employed and all word pairs in the window are connected. A limitation of GraphBTM is that only word relationships are considered while ignoring document relationships. Since a topic is possessed by a subset of documents in the corpus, we believe that the topical neighborhood of a document, i.e., documents with similar topics, would help determi"
2020.emnlp-main.310,2020.acl-main.32,1,0.875124,"Missing"
2020.emnlp-main.310,D19-1027,1,0.925371,"ned by a Gaussian prior. Srivastava and ∗ Corresponding author. Sutton (2017) argued that Dirichlet distribution is a more appropriate prior for topic modeling than Gaussian in NVDM and proposed ProdLDA that approximates the Dirichlet prior with logistic normal. There are also attempts that directly enforced a Dirichlet prior on the document topics. W-LDA (Nan et al., 2019) models topics in the Wasserstein autoencoders (Tolstikhin et al., 2017) framework and achieves distribution matching by minimizing their Maximum Mean Discrepancy (MMD) (Gretton et al., 2012), while adversarial topic model (Wang et al., 2019a,b, 2020) directly generates documents from the Dirichlet prior and such a process is adversarially trained with a discriminator under the framework of Generative Adversarial Network (GAN) (Goodfellow et al., 2014). Recently, due to the effectiveness of Graph Neural Networks (GNNs) (Li et al., 2015; Kipf and Welling, 2016; Zhou et al., 2018) in embedding graph structures, there is a surge of interests of applying GNN to natural language processing tasks (Yasunaga et al., 2017; Song et al., 2018; Yao et al., 2019). For example, GraphBTM (Zhu et al., 2018) is a neural topic model that incorpora"
2020.emnlp-main.310,K17-1045,0,0.0137902,"ibution matching by minimizing their Maximum Mean Discrepancy (MMD) (Gretton et al., 2012), while adversarial topic model (Wang et al., 2019a,b, 2020) directly generates documents from the Dirichlet prior and such a process is adversarially trained with a discriminator under the framework of Generative Adversarial Network (GAN) (Goodfellow et al., 2014). Recently, due to the effectiveness of Graph Neural Networks (GNNs) (Li et al., 2015; Kipf and Welling, 2016; Zhou et al., 2018) in embedding graph structures, there is a surge of interests of applying GNN to natural language processing tasks (Yasunaga et al., 2017; Song et al., 2018; Yao et al., 2019). For example, GraphBTM (Zhu et al., 2018) is a neural topic model that incorporates the graph representation of a document to capture biterm cooccurrences in the document. To construct the graph, a sliding window over the document is employed and all word pairs in the window are connected. A limitation of GraphBTM is that only word relationships are considered while ignoring document relationships. Since a topic is possessed by a subset of documents in the corpus, we believe that the topical neighborhood of a document, i.e., documents with similar topics,"
2020.emnlp-main.310,D10-1006,0,0.0473994,". By introducing the graph structure, the relationships between documents are established through their shared words and thus the topical representation of a document is enriched by aggregating information from its neighboring nodes using graph convolution. Extensive experiments on three datasets were conducted and the results demonstrate the effectiveness of the proposed approach. 1 Introduction Probabilistic topic models (Blei, 2012) are tools for discovering main themes from large corpora. The popular Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its variants (Lin and He, 2009; Zhao et al., 2010; Zhou et al., 2014) are effective in extracting coherent topics in an interpretable manner, but usually at the cost of designing sophisticated and model-specific learning algorithm. Recently, neural topic modeling that utilizes neuralnetwork-based black-box inference has been the main research direction in this field. Notably, NVDM (Miao et al., 2016) employs variational autoencoder (VAE) (Kingma and Welling, 2013) to model topic inference and document generation. Specifically, NVDM consists of an encoder inferring topics from documents and a decoder generating documents from topics, where th"
2020.emnlp-main.310,P14-2114,1,0.774239,"e graph structure, the relationships between documents are established through their shared words and thus the topical representation of a document is enriched by aggregating information from its neighboring nodes using graph convolution. Extensive experiments on three datasets were conducted and the results demonstrate the effectiveness of the proposed approach. 1 Introduction Probabilistic topic models (Blei, 2012) are tools for discovering main themes from large corpora. The popular Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its variants (Lin and He, 2009; Zhao et al., 2010; Zhou et al., 2014) are effective in extracting coherent topics in an interpretable manner, but usually at the cost of designing sophisticated and model-specific learning algorithm. Recently, neural topic modeling that utilizes neuralnetwork-based black-box inference has been the main research direction in this field. Notably, NVDM (Miao et al., 2016) employs variational autoencoder (VAE) (Kingma and Welling, 2013) to model topic inference and document generation. Specifically, NVDM consists of an encoder inferring topics from documents and a decoder generating documents from topics, where the latent topics are"
2020.emnlp-main.310,D18-1495,0,0.0571368,"Missing"
2020.findings-emnlp.102,W19-3712,0,0.0205236,"Missing"
2020.findings-emnlp.102,W09-1206,0,0.124486,"Missing"
2020.findings-emnlp.102,D17-1003,0,0.0215483,"ctorized baseline models on all 7 languages High-order parsing is one of the research hotspots in which first-order parsers meet performance bottlenecks; this has been extensively studied in the literature of syntactic dependency parsing(McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Martins et al., 2011; Ma and Zhao, 2012; Gormley et al., 2015; Zhang et al., 2020). In semantic parsing, Martins and Almeida (2014) proposed a way to encode high-order parts with hand-crafted features and introduced a novel co-parent part for semantic dependency parsing. Cao et al. (2017) proposed a quasi-second-order semantic dependency parser with dynamic programming. Wang et al. (2019) trained a second-order parser in an end-toend manner with the help of mean field variational inference and loopy belief propagation approximation. In SRL or related research field, there is also some related work on the improvement of performance by high-order structural information. On the Japanese NAIST Predict-Argument Structure (PAS) dataset, some works (Yoshikawa et al., 2011; Ouchi et al., 2015; Iida et al., 2015; Shibata et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2018) ma"
2020.findings-emnlp.102,D07-1101,0,0.5658,"rm AI Project, Cuttingedge Machine Reading Comprehension and Language Model. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT tenuretrack researcher startup fund “Toward Intelligent Machine Translation”. arcs. The types of features that the model can exploit in the inference depend on the information included in the factorized parts. Before the introduction of deep neural networks, in syntactic parsing (a kind of linguistic parsing), several works (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and McDonald, 2012; Ma and Zhao, 2012) showed that high-order parsers utilizing richer factorization information achieve higher accuracy than low-order ones due to the extensive decision history that can lead to significant improvements in inference (Chen et al., 2010). Semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Zhao and Kit, 2008; Zhao et al., 2009b, 2013) captures the predicate-argument structure of a given sentence, and it is defined as a shallow semantic parsing task, which is also a typical linguistic parsing task. Recent high-performing SRL mo"
2020.findings-emnlp.102,C10-2015,0,0.0284524,"gent Machine Translation”. arcs. The types of features that the model can exploit in the inference depend on the information included in the factorized parts. Before the introduction of deep neural networks, in syntactic parsing (a kind of linguistic parsing), several works (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and McDonald, 2012; Ma and Zhao, 2012) showed that high-order parsers utilizing richer factorization information achieve higher accuracy than low-order ones due to the extensive decision history that can lead to significant improvements in inference (Chen et al., 2010). Semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Zhao and Kit, 2008; Zhao et al., 2009b, 2013) captures the predicate-argument structure of a given sentence, and it is defined as a shallow semantic parsing task, which is also a typical linguistic parsing task. Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; He et al., 2018a; Strubell et al., 2018; He et al., 2018b; Cai et al., 2018), whether labeling arguments for a single predicate using sequence tagging model at a time or classifying the candidate predicateargument pairs, are (mainly) belong to first-"
2020.findings-emnlp.102,D19-1544,0,0.0301468,"Missing"
2020.findings-emnlp.102,N19-1423,0,0.00658232,"4 90.54 90.75 91.60 91.74 System German Zhao et al. (2009a) Lyu et al. (2019) Our baseline +HO Our baseline+B +HO+B Table 3: Precision, Recall, and Semantic-F 1 scores on German and Czech out-of-domain test sets. strategy from (Dozat and Manning, 2017; Wang et al., 2019). Please refer to Appendix A.1 for details. 4.2 Results And Analysis Main Results3 Table 1 presents the results on the standard English test set, WSJ (in-domain) and Brown (out-of-domain). For a fair comparison with previous works, we report three cases: not using pre-training, using ELMo (Peters et al., 2018), and using BERT (Devlin et al., 2019). Our single model achieves the best performance on the in-domain test set without syntactic information and extra resources for both types of setup, w/ and w/o preidentified predicate. On the out-of-domain test set, even though Zhou et al. (2019) obtains the highest score, their model is joint and likely achieves domain adaptation due to external tasks and resources. In general, our model achieves significant performance improvements in both in-domain and out-of-domain settings, especially while using pretraining out-of-domain. Furthermore, the results of using ELMo and BERT show that the str"
2020.findings-emnlp.102,J02-3001,0,0.0299432,"ures that the model can exploit in the inference depend on the information included in the factorized parts. Before the introduction of deep neural networks, in syntactic parsing (a kind of linguistic parsing), several works (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and McDonald, 2012; Ma and Zhao, 2012) showed that high-order parsers utilizing richer factorization information achieve higher accuracy than low-order ones due to the extensive decision history that can lead to significant improvements in inference (Chen et al., 2010). Semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Zhao and Kit, 2008; Zhao et al., 2009b, 2013) captures the predicate-argument structure of a given sentence, and it is defined as a shallow semantic parsing task, which is also a typical linguistic parsing task. Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; He et al., 2018a; Strubell et al., 2018; He et al., 2018b; Cai et al., 2018), whether labeling arguments for a single predicate using sequence tagging model at a time or classifying the candidate predicateargument pairs, are (mainly) belong to first-order parsers. High-order information is an overlooked po"
2020.findings-emnlp.102,Q15-1035,0,0.0131809,"with special focus on the impact of syntax and contextualized word representations and achieved new state-ofthe-art results on the CoNLL-2009 benchmarks of all languages, resulting in an effective model and outperforming strong factorized baseline models on all 7 languages High-order parsing is one of the research hotspots in which first-order parsers meet performance bottlenecks; this has been extensively studied in the literature of syntactic dependency parsing(McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Martins et al., 2011; Ma and Zhao, 2012; Gormley et al., 2015; Zhang et al., 2020). In semantic parsing, Martins and Almeida (2014) proposed a way to encode high-order parts with hand-crafted features and introduced a novel co-parent part for semantic dependency parsing. Cao et al. (2017) proposed a quasi-second-order semantic dependency parser with dynamic programming. Wang et al. (2019) trained a second-order parser in an end-toend manner with the help of mean field variational inference and loopy belief propagation approximation. In SRL or related research field, there is also some related work on the improvement of performance by high-order structur"
2020.findings-emnlp.102,L18-1550,0,0.0356114,"Missing"
2020.findings-emnlp.102,P18-2058,0,0.168978,"Ma and Zhao, 2012) showed that high-order parsers utilizing richer factorization information achieve higher accuracy than low-order ones due to the extensive decision history that can lead to significant improvements in inference (Chen et al., 2010). Semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Zhao and Kit, 2008; Zhao et al., 2009b, 2013) captures the predicate-argument structure of a given sentence, and it is defined as a shallow semantic parsing task, which is also a typical linguistic parsing task. Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; He et al., 2018a; Strubell et al., 2018; He et al., 2018b; Cai et al., 2018), whether labeling arguments for a single predicate using sequence tagging model at a time or classifying the candidate predicateargument pairs, are (mainly) belong to first-order parsers. High-order information is an overlooked potential performance enhancer; however, it does suffer from an enormous spatial complexity and an expensive time cost in the inference stage. As a result, most of the previous algorithms for highorder syntactic dependency tree parsing are not directly applicable to neural parsing. In addition, the target of"
2020.findings-emnlp.102,P17-1044,0,0.0338225,"and Collins, 2010; Zhang and McDonald, 2012; Ma and Zhao, 2012) showed that high-order parsers utilizing richer factorization information achieve higher accuracy than low-order ones due to the extensive decision history that can lead to significant improvements in inference (Chen et al., 2010). Semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Zhao and Kit, 2008; Zhao et al., 2009b, 2013) captures the predicate-argument structure of a given sentence, and it is defined as a shallow semantic parsing task, which is also a typical linguistic parsing task. Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; He et al., 2018a; Strubell et al., 2018; He et al., 2018b; Cai et al., 2018), whether labeling arguments for a single predicate using sequence tagging model at a time or classifying the candidate predicateargument pairs, are (mainly) belong to first-order parsers. High-order information is an overlooked potential performance enhancer; however, it does suffer from an enormous spatial complexity and an expensive time cost in the inference stage. As a result, most of the previous algorithms for highorder syntactic dependency tree parsing are not directly applicable to"
2020.findings-emnlp.102,D19-1538,1,0.876976,"el}. (cop−dep) , hj (gp−dep) , hj (gp−head) , hk ), where the weight matrix U2nd is (d × (d + 1) × (d + 1))-dimensional. 3.4 k6=i,j (t−1) (gp) Sk→i→j +Qk,i ( arc + G (t−1) ), Arc i → j exist exp(Si→j i,j = 1, Otherwise (t−1) where Gi,j (0) Qi,k (t−1) (gp) + Qj,k Si→j→k }, is the second-order voting scores, arc ), and t is the updating step. = softmax(Si,j Zheng et al. (2015) stated that multiple meanfield update iterations can be implemented by stacking Recurrent Neural Network (RNN) layers, as Pre-training w/ pre-identified predicate Cai et al. (2018) Kasai et al. (2019)∗ Zhou et al. (2019)† He et al. (2019)∗ Ours Variational Inference Layers In the first-order model, we adopt the negative likelihood of the golden structure as the loss to train the model, but in the second-order module of our proposed model, a similar approach will encounter the sparsity problem, as the maximum likelihood estimates cannot be obtained when the number of trainable variables is much larger than the number of observations. In other words, it is not feasible to directly approximate the real distribution with the output distribution of the second-order scorer because of the sparsity of the real distribution. Computing"
2020.findings-emnlp.102,P18-1192,1,0.780741,"Ma and Zhao, 2012) showed that high-order parsers utilizing richer factorization information achieve higher accuracy than low-order ones due to the extensive decision history that can lead to significant improvements in inference (Chen et al., 2010). Semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Zhao and Kit, 2008; Zhao et al., 2009b, 2013) captures the predicate-argument structure of a given sentence, and it is defined as a shallow semantic parsing task, which is also a typical linguistic parsing task. Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; He et al., 2018a; Strubell et al., 2018; He et al., 2018b; Cai et al., 2018), whether labeling arguments for a single predicate using sequence tagging model at a time or classifying the candidate predicateargument pairs, are (mainly) belong to first-order parsers. High-order information is an overlooked potential performance enhancer; however, it does suffer from an enormous spatial complexity and an expensive time cost in the inference stage. As a result, most of the previous algorithms for highorder syntactic dependency tree parsing are not directly applicable to neural parsing. In addition, the target of"
2020.findings-emnlp.102,W07-1522,0,0.0152328,"L example presented in right part of Figure 1, our second-order SRL model looks at several pairs of arcs: • sibling (Smith and Eisner, 2008; Martins et al., 2009): arguments of the same predicate; • co-parents (Martins and Almeida, 2014): predicates sharing the same argument; • grandparent (Carreras, 2007): predicate that is the argument of another predicate. Though some high-order structures have been studied by some related works (Yoshikawa et al., 2011; Ouchi et al., 2015; Shibata et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2018) in Japanese Predicate Argument Structure (PAS) (Iida et al., 2007) analysis and English SRL (Yang and Zong, 2014), the integration of multiple high-order structures into a single framework and exploring the high-order effects on multiple languages, different high-order structure combinations in a comprehensive way on popular CoNLL-2009 benchmark is the first considered in this paper and thus takes the shape of the main novelties of our work. 3 3.1 Model Overview SRL can be decomposed into four subtasks: predicate identification, predicate disambiguation, argument identification, and argument classification. Since the CoNLL-2009 shared task identified all pre"
2020.findings-emnlp.102,C18-1271,1,0.825621,"semantic role. For simple sentences (without HO), the baseline can already parse it very well, which also explains the reason why the improvement in some languages is not great. 5 Related Work The CoNLL-2009 shared task advocated performing SRL for multiple languages to promote multilingual NLP applications. (Zhao et al., 2009a) proposed an integrated approach by exploiting large1141 scale feature sets, while (Bj¨orkelund et al., 2009) used a generic feature selection procedure, which yielded significant gains in the multilingual SRL shared task. With the development of deep neural networks (Li et al., 2018a; Xiao et al., 2019; Zhou and Zhao, 2019; Zhang et al., 2019c,a; Li et al., 2019c; Luo et al., 2020; Li et al., 2019b; Zhang et al., 2019b) for NLP, most subsequent SRL works have focused on improving the performance of English, with occasional comparisons to other languages (Lei et al., 2015; Swayamdipta et al., 2016; Roth and Lapata, 2016; Marcheggiani et al., 2017; He et al., 2018b; Li et al., 2018b; Cai et al., 2018). Mulcaire et al. (2018) built a polyglot semantic role labeling system by combining resources from all languages in the CoNLL2009 shared task for exploiting the similarities"
2020.findings-emnlp.102,D15-1260,0,0.0357786,"Missing"
2020.findings-emnlp.102,D18-1262,1,0.868122,"semantic role. For simple sentences (without HO), the baseline can already parse it very well, which also explains the reason why the improvement in some languages is not great. 5 Related Work The CoNLL-2009 shared task advocated performing SRL for multiple languages to promote multilingual NLP applications. (Zhao et al., 2009a) proposed an integrated approach by exploiting large1141 scale feature sets, while (Bj¨orkelund et al., 2009) used a generic feature selection procedure, which yielded significant gains in the multilingual SRL shared task. With the development of deep neural networks (Li et al., 2018a; Xiao et al., 2019; Zhou and Zhao, 2019; Zhang et al., 2019c,a; Li et al., 2019c; Luo et al., 2020; Li et al., 2019b; Zhang et al., 2019b) for NLP, most subsequent SRL works have focused on improving the performance of English, with occasional comparisons to other languages (Lei et al., 2015; Swayamdipta et al., 2016; Roth and Lapata, 2016; Marcheggiani et al., 2017; He et al., 2018b; Li et al., 2018b; Cai et al., 2018). Mulcaire et al. (2018) built a polyglot semantic role labeling system by combining resources from all languages in the CoNLL2009 shared task for exploiting the similarities"
2020.findings-emnlp.102,N19-1075,0,0.148608,"Missing"
2020.findings-emnlp.102,W04-3250,0,0.0346504,"Missing"
2020.findings-emnlp.102,K19-2004,1,0.880526,"Missing"
2020.findings-emnlp.102,P10-1001,0,0.441175,"uttingedge Machine Reading Comprehension and Language Model. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT tenuretrack researcher startup fund “Toward Intelligent Machine Translation”. arcs. The types of features that the model can exploit in the inference depend on the information included in the factorized parts. Before the introduction of deep neural networks, in syntactic parsing (a kind of linguistic parsing), several works (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and McDonald, 2012; Ma and Zhao, 2012) showed that high-order parsers utilizing richer factorization information achieve higher accuracy than low-order ones due to the extensive decision history that can lead to significant improvements in inference (Chen et al., 2010). Semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Zhao and Kit, 2008; Zhao et al., 2009b, 2013) captures the predicate-argument structure of a given sentence, and it is defined as a shallow semantic parsing task, which is also a typical linguistic parsing task. Recent high-performing SRL models (He et al., 2017;"
2020.findings-emnlp.102,N18-2108,0,0.0519954,"Missing"
2020.findings-emnlp.102,N15-1121,0,0.049307,"Missing"
2020.findings-emnlp.102,P81-1022,0,0.452873,"Missing"
2020.findings-emnlp.102,D11-1022,0,0.0256301,"019) boosted multilingual SRL performance with special focus on the impact of syntax and contextualized word representations and achieved new state-ofthe-art results on the CoNLL-2009 benchmarks of all languages, resulting in an effective model and outperforming strong factorized baseline models on all 7 languages High-order parsing is one of the research hotspots in which first-order parsers meet performance bottlenecks; this has been extensively studied in the literature of syntactic dependency parsing(McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Martins et al., 2011; Ma and Zhao, 2012; Gormley et al., 2015; Zhang et al., 2020). In semantic parsing, Martins and Almeida (2014) proposed a way to encode high-order parts with hand-crafted features and introduced a novel co-parent part for semantic dependency parsing. Cao et al. (2017) proposed a quasi-second-order semantic dependency parser with dynamic programming. Wang et al. (2019) trained a second-order parser in an end-toend manner with the help of mean field variational inference and loopy belief propagation approximation. In SRL or related research field, there is also some related work on the improvem"
2020.findings-emnlp.102,P09-1039,0,0.0550447,"the neural network model. Given an input sentence with length L, order J of parsing model, the memory required is O(LJ+1 ). In the current GPU memory conditions, second-order J = 2 is the upper limit that can be explored in practice if without pruning. Therefore, we enumerate all three second-order structures as objects of study in SRL, as shown in the left part of Figure 1, namely sibling (sib), co-parents (cop), and grandparent (gp). As shown in the SRL example presented in right part of Figure 1, our second-order SRL model looks at several pairs of arcs: • sibling (Smith and Eisner, 2008; Martins et al., 2009): arguments of the same predicate; • co-parents (Martins and Almeida, 2014): predicates sharing the same argument; • grandparent (Carreras, 2007): predicate that is the argument of another predicate. Though some high-order structures have been studied by some related works (Yoshikawa et al., 2011; Ouchi et al., 2015; Shibata et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2018) in Japanese Predicate Argument Structure (PAS) (Iida et al., 2007) analysis and English SRL (Yang and Zong, 2014), the integration of multiple high-order structures into a single framework and exploring the hig"
2020.findings-emnlp.102,S14-2082,0,0.0170798,"and achieved new state-ofthe-art results on the CoNLL-2009 benchmarks of all languages, resulting in an effective model and outperforming strong factorized baseline models on all 7 languages High-order parsing is one of the research hotspots in which first-order parsers meet performance bottlenecks; this has been extensively studied in the literature of syntactic dependency parsing(McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Martins et al., 2011; Ma and Zhao, 2012; Gormley et al., 2015; Zhang et al., 2020). In semantic parsing, Martins and Almeida (2014) proposed a way to encode high-order parts with hand-crafted features and introduced a novel co-parent part for semantic dependency parsing. Cao et al. (2017) proposed a quasi-second-order semantic dependency parser with dynamic programming. Wang et al. (2019) trained a second-order parser in an end-toend manner with the help of mean field variational inference and loopy belief propagation approximation. In SRL or related research field, there is also some related work on the improvement of performance by high-order structural information. On the Japanese NAIST Predict-Argument Structure (PAS)"
2020.findings-emnlp.102,D14-1162,0,0.0877038,"Missing"
2020.findings-emnlp.102,N18-1202,0,0.0330803,".88 90.03 91.23 91.61 82.66 87.20 86.04 90.54 90.75 91.60 91.74 System German Zhao et al. (2009a) Lyu et al. (2019) Our baseline +HO Our baseline+B +HO+B Table 3: Precision, Recall, and Semantic-F 1 scores on German and Czech out-of-domain test sets. strategy from (Dozat and Manning, 2017; Wang et al., 2019). Please refer to Appendix A.1 for details. 4.2 Results And Analysis Main Results3 Table 1 presents the results on the standard English test set, WSJ (in-domain) and Brown (out-of-domain). For a fair comparison with previous works, we report three cases: not using pre-training, using ELMo (Peters et al., 2018), and using BERT (Devlin et al., 2019). Our single model achieves the best performance on the in-domain test set without syntactic information and extra resources for both types of setup, w/ and w/o preidentified predicate. On the out-of-domain test set, even though Zhou et al. (2019) obtains the highest score, their model is joint and likely achieves domain adaptation due to external tasks and resources. In general, our model achieves significant performance improvements in both in-domain and out-of-domain settings, especially while using pretraining out-of-domain. Furthermore, the results of"
2020.findings-emnlp.184,P00-1037,0,0.336539,"ining data and use the top k prediction of each character as the semantic confusion set. For candidates generation, we substitute each character in the chunkij with its semantically similar characters and keep the candidates that can be found in the V . Similar to shape confusion set, in practice, we only consider candidates that have 1 edit distance (1 substitution) with the chunkij . 2.3 Correction Selection In this section, we introduce the training strategy for correction selection and the features we used for global optimization. Most of the previous work follows the noisy channel model (Brill and Moore, 2000), which formulates the error correction tasks as: sc = arg max p(ˆ s|s) (2) sˆ where the s is the input sentence, and sˆ refers to a possible correction. The formula can be further rewritten through the Bayes rule as: sc = arg max sˆ p(s|ˆ s) · p(ˆ s) p(s) Name ed pyed n-chunk wlm cem n-py n-shape n-lm Table 1: The features used for the correction selection. s and sˆ refer to the input sentence and a correction. correction and input sentence through characterlevel and pronunciation-level. A longer chunk is usually more unambiguous than a shorter one, thus a correction with less n-chunk is ofte"
2020.findings-emnlp.184,2020.acl-main.81,0,0.715775,"lts on the CSC Datasets We first report the performance of the proposed method on the csc13 , csc14 and csc15 dataset. As shown in Table 3, when comparing to previous strong CSC systems, our proposed chunk-based method achieves a significant improvement on the three datasets. Zhao et al. (2017) employ a graph-based model and integrate spelling checking with word segmentation. However, their proposed method only propython-pinyin 8 https://github.com/google-research/ bert 9 https://github.com/kpu/kenlm 2035 Dataset csc13 csc14 csc15 Model Yeh et al. (2015) Zhao et al. (2017) Hong et al. (2019)* Cheng et al. (2020)‡ our method Zhao et al. (2017) Hong et al. (2019) Cheng et al. (2020)‡ our method Zhang et al. (2015) Hong et al. (2019) Zhang et al. (2020) Cheng et al. (2020)‡ our method Acc 74.80 83.20 70.0 70.0 70.09 74.2 80.9 76.82 Detection Level P R 44.31 37.67 55.90 46.99 61.19 75.67 61.0 53.5 58.27 54.53 78.65 54.80 80.27 53.27 67.6 60.0 73.7 73.2 70.97 64.00 88.11 62.00 F1 40.72 51.06 67.66 57.0 56.28 64.59 64.04 63.5 73.5 67.30 72.79 Acc 66.30 37.00 60.5 67.20 69.3 68.08 69.18 73.7 77.4 74.64 Correction Level P R 70.30 62.50 70.50 35.60 73.1 60.5 44.58 37.47 74.34 67.20 55.50 39.14 59.4 52.0 51.01"
2020.findings-emnlp.184,I05-3017,0,0.110928,"propose a chunk-based framework to correct single-character and multi-character word errors uniformly; and 3) we adopt a global optimization strategy to enable a sentence-level correction selection. The experimental results show that the proposed approach achieves a new state-of-the-art performance on three benchmark datasets, as well as an optical character recognition dataset. 1 • For English spelling check, the basic unit is the word. However, Chinese characters are continuously written without word delimiter, and the word definition varies across different linguistic theories (Xue, 2003; Emerson, 2005). It makes the sentence with spelling errors more ambiguous, and more challenging for the spell checkers to detect and correct the errors. • Chinese words usually consist of one to four characters and are much shorter than the English word. Spelling errors can drastically change the meaning of the word. Thus, the CSC task relies on the contextual semantic information to find the best correction. Introduction Spelling check is a task to automatically detect and correct spelling errors in human writings. Spelling check is well-studied for languages such as English, and many resources and tools h"
2020.findings-emnlp.184,D19-5522,0,0.0985463,"s research demonstrates that most of the Chinese spelling errors come from similar pronunciations, shapes, or meanings (Liu et al., 2011; Chen et al., 2011). Previous CSC models usually employ the characters with similar pronunciation or shape as the confusion set to reduce the search space, but the visually and phonologically irrelevant typos cannot be handled. Recent work aims at replacing the pronunciation and shape confusion sets with a dynamically generated confusion set by masked language models, which retrieve the semantically related candidates according to the contextual information (Hong et al., 2019). However, due to the lack of knowledge about human errors, masked language models correct the spelling errors ignoring the pronunciation or shape similarity. Therefore, combining the two comes as a natural solution. For the second challenge, early works rely on the segmentation results from a Chinese word segmentation system (Yu and Li, 2014). However, as the 2031 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2031–2040 c November 16 - 20, 2020. 2020 Association for Computational Linguistics segmentation system is trained on the clean corpus, the spelling errors"
2020.findings-emnlp.184,W13-4416,0,0.130413,"to perform the correction at the character-level directly, which are more robust to segmentation errors (Zhang et al., 2015; Hong et al., 2019; Zhang et al., 2020). However, the character-based model cannot effectively utilize the word-level semantic information, and the correction is also more difficult to interpret. In order to explore and utilize the word-level information, the word-based methods are designed to do word segmentation and spelling error corrections jointly. Previous works show that the wordbased correction models often perform better than their character-based counterparts (Jia et al., 2013; Hsieh et al., 2015; Yeh et al., 2015; Zhao et al., 2017). Since word-based correction models usually apply a pipeline of submodules and handle special cases (e.g., single-character words) individually, the complex architecture makes it difficult to perform global optimization. For the third challenge, previous works mainly rely on the local context features such as point-wise mutual information (PMI), part-of-speech (POS) n-gram, and perplexity from an n-gram language model (Liu et al., 2013; Zhang et al., 2015; Yeh et al., 2015). As these statistical features are limited within a fixed-size"
2020.findings-emnlp.184,W14-6835,0,0.178993,"annot be handled. Recent work aims at replacing the pronunciation and shape confusion sets with a dynamically generated confusion set by masked language models, which retrieve the semantically related candidates according to the contextual information (Hong et al., 2019). However, due to the lack of knowledge about human errors, masked language models correct the spelling errors ignoring the pronunciation or shape similarity. Therefore, combining the two comes as a natural solution. For the second challenge, early works rely on the segmentation results from a Chinese word segmentation system (Yu and Li, 2014). However, as the 2031 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2031–2040 c November 16 - 20, 2020. 2020 Association for Computational Linguistics segmentation system is trained on the clean corpus, the spelling errors often lead to incorrect segmentation results. The accumulated errors make the spell checking even more difficult. Thus, characterbased models are proposed to perform the correction at the character-level directly, which are more robust to segmentation errors (Zhang et al., 2015; Hong et al., 2019; Zhang et al., 2020). However, the character-ba"
2020.findings-emnlp.184,W13-4409,0,0.271371,"ow that the wordbased correction models often perform better than their character-based counterparts (Jia et al., 2013; Hsieh et al., 2015; Yeh et al., 2015; Zhao et al., 2017). Since word-based correction models usually apply a pipeline of submodules and handle special cases (e.g., single-character words) individually, the complex architecture makes it difficult to perform global optimization. For the third challenge, previous works mainly rely on the local context features such as point-wise mutual information (PMI), part-of-speech (POS) n-gram, and perplexity from an n-gram language model (Liu et al., 2013; Zhang et al., 2015; Yeh et al., 2015). As these statistical features are limited within a fixed-size window, it is difficult to capture the deep contextual information. In the paper, we propose a unified framework combining features and benefits from previous works. We employ confusion sets from similar pronunciations, shapes, and semantics to deal with different types of spelling errors. A chunk-based decoding approach is proposed to model both singlecharacter and multi-character words in a uniform way. We also finetune an error model based on the large-scale pretrained language model to in"
2020.findings-emnlp.184,P03-1021,0,0.148574,"sˆ. the number of chunks in sˆ. the perplexity of sˆ measured by a word-level n-gram language model. the improvement of log probability from a character error model. the number of chunks that are from the pronunciation confusion set. the number of chunks that are from the shape confusion set. the number of chunks that are from the semantic confusion set. j X (log p(ˆ ck |ck , s) − log p(ck |ck , s)) (6) k=i where p(ˆ ck |ck , s) is the probability of replacing ck with cˆk given the input sentence s.4 For combining different features, we apply the Minimum Error Rate Training (MERT) algothrim (Och, 2003). Given the top n outputs, the MERT algorithm optimizes the scoring function by learning to rerank the decoded sentences according to their similarity to the gold sentence. Rather than a local ranking, the MERT algorithm measures the similarity directly by sentence-level metrics to achieve a global optimization. 3 Experiments In the following sections, we will introduce the datasets and the experimental settings first, and 4 Note that we use p(ˆ ck |ck ) to simulate the error model p(s|ˆ s), because our error model is contextualized and the calculation costs will be huge if we calculate p(s|ˆ"
2020.findings-emnlp.184,W15-3106,0,0.0217379,"lts demonstrate that CSC errors require more contextual information even for single-character errors. For sentences with more errors, the recall rate increases rapidly when the beam size is small (e.g., beam size from 1 to 2). However, the recall rate does not increase significantly after the beam grows to an appropriate size (e.g., a beam size of 4). This experiment result illustrates that, for sentences with multiple errors, the bottleneck comes from the candidate selection. 4 Related Work Previous work of CSC is closely related to a series of shared tasks (Wu et al., 2013; Yu et al., 2014; Tseng et al., 2015). The workflow of CSC systems can be roughly divided into two phases, candidate generation and candidate selection. For the candidate generation phase, most of the previous work retrieves the candidates according to pronunciation or shape (Liu et al., 2011; Chen et al., 2011; Yu and Li, 2014; Yeh et al., 2015). Recently, Hong et al. (2019) propose to replace the traditional confusion sets with a dynamically generated one. They treat the CSC as a sequence 2038 labeling problem and finetune a pretrained masked language model to generate candidates. For reducing the false alarm rate, they filter"
2020.findings-emnlp.184,D18-1273,0,0.47439,"vel P R 70.30 62.50 70.50 35.60 73.1 60.5 44.58 37.47 74.34 67.20 55.50 39.14 59.4 52.0 51.01 47.65 77.43 51.04 79.72 51.45 66.6 59.1 66.7 66.2 60.08 54.18 87.33 57.64 F1 66.17 47.31 66.2 40.72 70.59 45.90 55.4 49.27 61.52 62.54 62.6 66.4 56.98 69.44 Table 3: The main results on csc13 , csc14 and csc15 datasets. *The csc13 detection-level performance of Hong et al. (2019) is obtained on the test set of correction task and thus incomparable with the results from other work. The results with ‡ are reproduced by rerunning the released code and evaluation scripts on the standard CSC datasets. The Wang et al. (2018) and Wang et al. (2019) calculate the performance on the character-level, which makes their results incomparable with other works. cesses the multi-character words. Two types of single-character words are handled by rules and an individual module. The separated modules make their system difficult to fully explore the annotated data and obtain a global optimization. Zhang et al. (2015) combine the character-level candidate generation with a two-stage filter model. For the first stage, they use a logistic regression classifier to reduce the size of candidates. In the second stage, they utilize t"
2020.findings-emnlp.184,2020.acl-main.82,0,0.703171,"ese word segmentation system (Yu and Li, 2014). However, as the 2031 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2031–2040 c November 16 - 20, 2020. 2020 Association for Computational Linguistics segmentation system is trained on the clean corpus, the spelling errors often lead to incorrect segmentation results. The accumulated errors make the spell checking even more difficult. Thus, characterbased models are proposed to perform the correction at the character-level directly, which are more robust to segmentation errors (Zhang et al., 2015; Hong et al., 2019; Zhang et al., 2020). However, the character-based model cannot effectively utilize the word-level semantic information, and the correction is also more difficult to interpret. In order to explore and utilize the word-level information, the word-based methods are designed to do word segmentation and spelling error corrections jointly. Previous works show that the wordbased correction models often perform better than their character-based counterparts (Jia et al., 2013; Hsieh et al., 2015; Yeh et al., 2015; Zhao et al., 2017). Since word-based correction models usually apply a pipeline of submodules and handle spe"
2020.findings-emnlp.184,W15-3107,0,0.600575,"on the segmentation results from a Chinese word segmentation system (Yu and Li, 2014). However, as the 2031 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2031–2040 c November 16 - 20, 2020. 2020 Association for Computational Linguistics segmentation system is trained on the clean corpus, the spelling errors often lead to incorrect segmentation results. The accumulated errors make the spell checking even more difficult. Thus, characterbased models are proposed to perform the correction at the character-level directly, which are more robust to segmentation errors (Zhang et al., 2015; Hong et al., 2019; Zhang et al., 2020). However, the character-based model cannot effectively utilize the word-level semantic information, and the correction is also more difficult to interpret. In order to explore and utilize the word-level information, the word-based methods are designed to do word segmentation and spelling error corrections jointly. Previous works show that the wordbased correction models often perform better than their character-based counterparts (Jia et al., 2013; Hsieh et al., 2015; Yeh et al., 2015; Zhao et al., 2017). Since word-based correction models usually apply"
2020.findings-emnlp.184,P19-1578,0,0.426703,"50 35.60 73.1 60.5 44.58 37.47 74.34 67.20 55.50 39.14 59.4 52.0 51.01 47.65 77.43 51.04 79.72 51.45 66.6 59.1 66.7 66.2 60.08 54.18 87.33 57.64 F1 66.17 47.31 66.2 40.72 70.59 45.90 55.4 49.27 61.52 62.54 62.6 66.4 56.98 69.44 Table 3: The main results on csc13 , csc14 and csc15 datasets. *The csc13 detection-level performance of Hong et al. (2019) is obtained on the test set of correction task and thus incomparable with the results from other work. The results with ‡ are reproduced by rerunning the released code and evaluation scripts on the standard CSC datasets. The Wang et al. (2018) and Wang et al. (2019) calculate the performance on the character-level, which makes their results incomparable with other works. cesses the multi-character words. Two types of single-character words are handled by rules and an individual module. The separated modules make their system difficult to fully explore the annotated data and obtain a global optimization. Zhang et al. (2015) combine the character-level candidate generation with a two-stage filter model. For the first stage, they use a logistic regression classifier to reduce the size of candidates. In the second stage, they utilize the online translation s"
2020.findings-emnlp.184,W13-4406,0,0.379161,"Missing"
2020.findings-emnlp.184,O03-4002,0,0.15328,"rors; 2) we propose a chunk-based framework to correct single-character and multi-character word errors uniformly; and 3) we adopt a global optimization strategy to enable a sentence-level correction selection. The experimental results show that the proposed approach achieves a new state-of-the-art performance on three benchmark datasets, as well as an optical character recognition dataset. 1 • For English spelling check, the basic unit is the word. However, Chinese characters are continuously written without word delimiter, and the word definition varies across different linguistic theories (Xue, 2003; Emerson, 2005). It makes the sentence with spelling errors more ambiguous, and more challenging for the spell checkers to detect and correct the errors. • Chinese words usually consist of one to four characters and are much shorter than the English word. Spelling errors can drastically change the meaning of the word. Thus, the CSC task relies on the contextual semantic information to find the best correction. Introduction Spelling check is a task to automatically detect and correct spelling errors in human writings. Spelling check is well-studied for languages such as English, and many resou"
2020.findings-emnlp.285,C16-1017,0,0.0209297,"our approach significantly improves the performance of fine-tuning on low-resource datasets, e.g., those consisting of only several thousand data samples. 3181 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3181–3186 c November 16 - 20, 2020. 2020 Association for Computational Linguistics 2 Related Work Label embeddings have been previously leveraged for image classification (Akata et al., 2015), multimodal learning between images and text (Kiros et al., 2014), text recognition in images (RodriguezSerrano and Perronnin, 2015), zero-shot learning (Li et al., 2015; Ma et al., 2016) and text classification (Zhang et al., 2017). Notably, LEAM (Wang et al., 2018b) jointly embeds words (tokens) and labels in a common latent space as a means to improve the performance on general text classification tasks. Further, Moreo et al. (2019) concatenates label embedding with word embeddings. However, this approach cannot be directly implemented into PLMs since the new (concatenated) embedding is not compatible with the pretrained parameters. We integrate label embeddings into the self-attention of BERT models, so the attention can be regularized to better focus on task-relevant info"
2020.findings-emnlp.285,P18-1216,1,0.924661,"performance during testing. * These authors contributed equally to this work Ricardo Henao1 A clear-cut solution to this problem is to focus more on samples that are more relevant to the target task during pretraining. However, this requires a task-specific pretraining, which in most cases is computational or time prohibitive. Another approach is to pretrain on an auxiliary dataset before fine-tuning on the target task (Phang et al., 2018). Such method requires the availability of an appropriate auxiliary datasets. Unfortunately, in some cases it may negatively impact the downstream transfer (Wang et al., 2018a). Label embeddings (Akata et al., 2015) can be regarded as a featurebased definition of a classification task, in which detailed information of the task is encoded. One natural question is whether we can combine the general knowledge in a PLM and the task-specific characterization contained within label embeddings for better fine-tuning on low-resource tasks. In this paper, we propose to utilize the label embeddings as a task-specific prior, complementary to the general prior already encoded during pretraining. We learn and integrate these label embeddings into BERT models (Devlin et al., 20"
2020.findings-emnlp.371,abdelali-etal-2014-amara,0,0.0286749,"erence agreement. Following previous studies, newstest 2016 was used to evaluate the en-ro language pair. For fr-ro, we sampled 5K sentence pairs from OPUS (Tiedemann, 2012) for evaluation, while for zh-ro, we use the religious and educational parallel data for out-of-domain evaluation and collected 2K news parallel sentences for in-domain evaluation. In detail, as data for fr-ro, we used GlobalVoices2 , OpenSubtitles (Lison and Tiedemann, 2016), and MultiParaCrawl3 , whereas for zh-ro, Bible-uedin (Christodouloupoulos and Steedman, 2015), Tanzil, and the QCRI Educational Domain Corpus (QED) (Abdelali et al., 2014) were used. Because these parallel corpora between zh-ro are in religious and educational domains only, which are far away from the news domain of training data, we also collected a parallel corpus (2K in size) of zh-ro for in-domain evaluation. The Moses scripts (Koehn and Knowles, 2017) were used for tokenization of en, fr, and ro, and the jieba toolkit4 was used for word segmentation on zh. In particular, following Sennrich et al. (2016), we removed diacritics from ro. For zh, to avoid confusion between Hong Kong Standard Traditional Chinese (zh hk: QED), Taiwan Standard Traditional Chinese"
2020.findings-emnlp.371,N19-1388,0,0.0312615,"2005)). PBSMT + NMT: (Lample et al., 2018b), XLM: (Conneau and Lample, 2019), MASS: (Song et al., 2019). In the form x[y], x and y respectively indicate results on in-domain and out-of-domain sets. Note, the BLEU used in ro→zh is based on Chinese words segmented by the jieba toolkit. en-ro fr-ro zh-ro en-fr-ro en-zh-ro en fr ro zh 6.5 / 64.3 6.9 / 60.1 7.4 / 53.8 4.1 / 68.7 4.2 / 68.4 - 4.9 / 68.3 4.9 / 68.5 5.3 / 65.8 5.0 / 68.1 5.5 / 64.9 11.5 / 52.9 11.4 / 53.4 Table 2: Perplexity / Accuracy for masked language modeling in different languages joint pre-training. UNMT Lample et al. (2018a); Aharoni et al. (2019); Song et al. (2019) have demonstrated the importance of pre-training, which is a key ingredient of UNMT. Conneau and Lample (2019) used masked language modeling (MLM) to pretrain the full model for the initialization step before applying a denoising autoencoder and BT training step. Therefore, we take the XLM architecture proposed by Conneau and Lample (2019) as our backbone baseline model. MUNMT Our method studies the impact of adding a reference language to the existing UNMT language pair, which makes our model essentially multilingual. Therefore, MUNMT is the baseline for comparison. We ad"
2020.findings-emnlp.371,N19-1121,0,0.059622,"irs, including unseen language pairs, transfer learning should be considered when low-resource languages are trained together with rich-resource ones. As discussed by Arivazhagan et al. (2019), MUNMT usually performs worse than pivot-based supervised NMT; however, the pivot-based method easily experiences a computationally expensive quadratic growth in the number of source languages and suffers from the error propagation problem. Arivazhagan et al. (2019) addressed the zeroshot generalization problem that some translation directions have not been optimized well due to a lack of parallel data. Al-Shedivat and Parikh (2019) introduced a consistent agreement-based training method that encourages the model to produce equivalent translations of parallel sentences in zero-shot translation, which share similarities with our RAT approach. However, in terms of a specific implementation, because of the differences between UNMT and NMT, we have provided three new UNMT methods, and have alleviated the problem of uncontrollable intermediate BT quality in UNMT. Arivazhagan et al. (2019) addressed the issue of transfer learning between language pairs with parallel data where there is a lack of parallel corpora in multilingua"
2020.findings-emnlp.371,P17-1042,0,0.0856606,"Missing"
2020.findings-emnlp.371,1981.tc-1.7,0,0.709837,"Missing"
2020.findings-emnlp.371,C18-1233,1,0.900017,"Missing"
2020.findings-emnlp.371,P05-1066,0,0.0305915,"Missing"
2020.findings-emnlp.371,2020.findings-emnlp.283,0,0.202912,"ize is set to 60K, and the model hyperparameters are consistent with those of XLM. The smoothing value  in RAT is set to 0.1. 4.3 Main Results and Analysis This section examines the effectiveness of the proposed RUNMT framework6 . The main results7 are presented in Table 1. Row #4 reports the replicated results of the XLM architecture (Conneau and Lample, 2019) based on the training of each language pair individually. Our UNMT basically reproduces XLM’s results, and it also 6 Code available at https://github.com/ bcmi220/runmt. 7 Notably, concurrent works (Liu et al., 2020; Bai et al., 2020; Garcia et al., 2020) also explore the case of using auxiliary parallel data effects under the MUNMT setting, where all of these works share similarities in multilingualism motivation. Due to the inconsistency of the parallel corpora used, the results are not directly comparable, so we don’t include their results in the table. 4156 makes some improvements over the original (probably because of differences in data sampling). Thus, our approach offers a strong baseline performance. Compared with the current stateof-the-art method MASS (Song et al., 2019), our baseline performance is slightly lower. This is because M"
2020.findings-emnlp.371,P18-1192,1,0.906335,"Missing"
2020.findings-emnlp.371,D19-1080,0,0.0441941,"Missing"
2020.findings-emnlp.371,W17-3204,0,0.0440239,"and collected 2K news parallel sentences for in-domain evaluation. In detail, as data for fr-ro, we used GlobalVoices2 , OpenSubtitles (Lison and Tiedemann, 2016), and MultiParaCrawl3 , whereas for zh-ro, Bible-uedin (Christodouloupoulos and Steedman, 2015), Tanzil, and the QCRI Educational Domain Corpus (QED) (Abdelali et al., 2014) were used. Because these parallel corpora between zh-ro are in religious and educational domains only, which are far away from the news domain of training data, we also collected a parallel corpus (2K in size) of zh-ro for in-domain evaluation. The Moses scripts (Koehn and Knowles, 2017) were used for tokenization of en, fr, and ro, and the jieba toolkit4 was used for word segmentation on zh. In particular, following Sennrich et al. (2016), we removed diacritics from ro. For zh, to avoid confusion between Hong Kong Standard Traditional Chinese (zh hk: QED), Taiwan Standard Traditional Chinese (zh tw: Bibleuedin), and Simplified Chinese (zh: Tanzil and monolingual training data), we used opencc5 to convert zh hk and zh tw to simplified Chinese. 4.2 Baselines Our baseline models follow XLM (Conneau and Lample, 2019), with the following refinements: 4155 2 http://casmacat.eu/cor"
2020.findings-emnlp.371,J82-2005,0,0.623357,"Missing"
2020.findings-emnlp.371,P19-1017,0,0.0906893,"ich share similarities with our RAT approach. However, in terms of a specific implementation, because of the differences between UNMT and NMT, we have provided three new UNMT methods, and have alleviated the problem of uncontrollable intermediate BT quality in UNMT. Arivazhagan et al. (2019) addressed the issue of transfer learning between language pairs with parallel data where there is a lack of parallel corpora in multilingual supervised NMT. As for the agreement in UNMT, (Sun et al., 2019) investigate the enhancement of unsupervised bilingual word embedding agreement in the UNMT training. Leng et al. (2019) propose a multi-hop UNMT that automatically selects a good translation path for a distant language pair during UNMT. Baijun et al. (2019) proposed a cross-lingual pre-training approach that makes use of the source–pivot data to pre-train the language model. As for the multilingualism, Liu et al. (2020) proposes a multilingual denoising pre-training technique to improve machine translation tasks. Bai et al. (2020) and Garcia et al. (2020) both studied the agreement across language pairs. Their method is much the same as one of our proposed approaches, XBT, which relies on the supervision signa"
2020.findings-emnlp.371,D18-1262,1,0.884281,"Missing"
2020.findings-emnlp.371,L16-1147,0,0.020699,"language pair parallel dataset is about 10M. In both scenarios, we evaluated each language pair except for en-fr and en-zh, for which the relevant parallel data was used for reference agreement. Following previous studies, newstest 2016 was used to evaluate the en-ro language pair. For fr-ro, we sampled 5K sentence pairs from OPUS (Tiedemann, 2012) for evaluation, while for zh-ro, we use the religious and educational parallel data for out-of-domain evaluation and collected 2K news parallel sentences for in-domain evaluation. In detail, as data for fr-ro, we used GlobalVoices2 , OpenSubtitles (Lison and Tiedemann, 2016), and MultiParaCrawl3 , whereas for zh-ro, Bible-uedin (Christodouloupoulos and Steedman, 2015), Tanzil, and the QCRI Educational Domain Corpus (QED) (Abdelali et al., 2014) were used. Because these parallel corpora between zh-ro are in religious and educational domains only, which are far away from the news domain of training data, we also collected a parallel corpus (2K in size) of zh-ro for in-domain evaluation. The Moses scripts (Koehn and Knowles, 2017) were used for tokenization of en, fr, and ro, and the jieba toolkit4 was used for word segmentation on zh. In particular, following Sennr"
2020.findings-emnlp.371,2020.tacl-1.47,0,0.167936,"the byte pair encoding (BPE) code size is set to 60K, and the model hyperparameters are consistent with those of XLM. The smoothing value  in RAT is set to 0.1. 4.3 Main Results and Analysis This section examines the effectiveness of the proposed RUNMT framework6 . The main results7 are presented in Table 1. Row #4 reports the replicated results of the XLM architecture (Conneau and Lample, 2019) based on the training of each language pair individually. Our UNMT basically reproduces XLM’s results, and it also 6 Code available at https://github.com/ bcmi220/runmt. 7 Notably, concurrent works (Liu et al., 2020; Bai et al., 2020; Garcia et al., 2020) also explore the case of using auxiliary parallel data effects under the MUNMT setting, where all of these works share similarities in multilingualism motivation. Due to the inconsistency of the parallel corpora used, the results are not directly comparable, so we don’t include their results in the table. 4156 makes some improvements over the original (probably because of differences in data sampling). Thus, our approach offers a strong baseline performance. Compared with the current stateof-the-art method MASS (Song et al., 2019), our baseline performa"
2020.findings-emnlp.371,N09-2056,1,0.895514,"Missing"
2020.findings-emnlp.371,D18-1549,0,0.061576,"benchmarks has achieved great success (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) because of advances in deep learning and the availability of large-scale parallel corpora; however, the applicability of MT systems is limited because of their reliance on large parallel corpora for the majority of language pairs. In real-world situations, the majority of language pairs have very little parallel data, although large volumes of monolingual data are available for each language. UNMT removes the dependence on parallel corpora, relying only on monolingual corpora in each language (Reddi et al., 2018; Lample et al., 2018a,b; Conneau and Lample, 2019; Li et al., 2019b). UNMT uses translation symmetry for dual learning in each language direction. Existing UNMT models are mainly built on the encoder– decoder schema. The essence of UNMT is to 4151 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4151–4162 c November 16 - 20, 2020. 2020 Association for Computational Linguistics learn unsupervised cross-lingual word alignment and/or sentence alignment. For unsupervised word alignment, the most popular methods are word embedding mapping (Conneau et al., 2017; Lample e"
2020.findings-emnlp.371,W16-2323,0,0.058197,"2016), and MultiParaCrawl3 , whereas for zh-ro, Bible-uedin (Christodouloupoulos and Steedman, 2015), Tanzil, and the QCRI Educational Domain Corpus (QED) (Abdelali et al., 2014) were used. Because these parallel corpora between zh-ro are in religious and educational domains only, which are far away from the news domain of training data, we also collected a parallel corpus (2K in size) of zh-ro for in-domain evaluation. The Moses scripts (Koehn and Knowles, 2017) were used for tokenization of en, fr, and ro, and the jieba toolkit4 was used for word segmentation on zh. In particular, following Sennrich et al. (2016), we removed diacritics from ro. For zh, to avoid confusion between Hong Kong Standard Traditional Chinese (zh hk: QED), Taiwan Standard Traditional Chinese (zh tw: Bibleuedin), and Simplified Chinese (zh: Tanzil and monolingual training data), we used opencc5 to convert zh hk and zh tw to simplified Chinese. 4.2 Baselines Our baseline models follow XLM (Conneau and Lample, 2019), with the following refinements: 4155 2 http://casmacat.eu/corpus/global-voices.html http://paracrawl.eu 4 https://github.com/fxsjy/jieba 5 https://github.com/BYVoid/OpenCC 3 en-fr-ro en-zh-ro en→ro ro→en fr→ro ro→fr"
2020.findings-emnlp.371,P19-1119,1,0.807626,"Missing"
2020.findings-emnlp.371,2020.acl-main.324,1,0.653027,"ation models (as the generation direction is the same as the training direction), but also train the BT models, i.e., T → S and T → R. This gives the RABT training approach shown in Figure 2(c). The learning objective of RABT can be described as: LRABT (S, T , R) = L(θT →S ) + L(θT →R ). 3.4 (8) Cross-lingual Back-translation The traditional BT analyzed in Section 2 and illustrated in Figure 2(a) allows us to train a T → S model with the help of an S → T model, and vice versa; however, this mutually beneficial training is performed entirely within one language pair. Multilingual UNMT (MUNMT) (Sun et al., 2020) is a special case of UNMT that is capable of translating between multiple source and target languages. Although multiple language pairs are trained jointly in MUNMT, there is an obvious shortcoming for BT: translating between language pairs that do not occur together during training, i.e., lack of optimization across language pairs. Joint training across language pairs can be performed through forced high-order BT in UNMT, which takes the form L1 → L2 → ... → LO+1 → L1 , where O is the translation order indicating the number of bridge languages in BT. This approach may fail because decoding t"
2020.findings-emnlp.371,N07-1061,1,0.653675,"Missing"
2020.findings-emnlp.371,P07-1108,0,0.24056,"Missing"
2020.findings-emnlp.371,P19-1230,1,0.787194,"Missing"
2020.findings-emnlp.371,L16-1561,0,0.0615968,"olingual sentences as those extracted from the WMT News Crawl datasets for the period 2007–2017 by Conneau and Lample (2019) for a fair comparison and limited the maximum number of sentences in each language to 50 million(M), which results in 50M, 50M, and 14M sentences, respectively. For Chinese, we combined all of the sentences available in the WMT News Crawl datasets with the source sentences from the WMT’17 Chinese–English translation task, leading to 26M sentences. For the parallel data of en-fr and en-zh introduced by the two experimental settings, we only use those provided by MultiUN (Ziemski et al., 2016). Finally, the size of the resulting language pair parallel dataset is about 10M. In both scenarios, we evaluated each language pair except for en-fr and en-zh, for which the relevant parallel data was used for reference agreement. Following previous studies, newstest 2016 was used to evaluate the en-ro language pair. For fr-ro, we sampled 5K sentence pairs from OPUS (Tiedemann, 2012) for evaluation, while for zh-ro, we use the religious and educational parallel data for out-of-domain evaluation and collected 2K news parallel sentences for in-domain evaluation. In detail, as data for fr-ro, we"
2020.findings-emnlp.371,tiedemann-2012-parallel,0,\N,Missing
2020.nlptea-1.6,W09-1119,0,0.0342198,"c F1 7.7 18.4 10.8 9.6 17.7 12.5 9.3 22.8 13.3 32.2 13.3 18.9 Table 2: Final results on the official evaluation testing data. “Run #1” represents the ensemble model with correction. “Run #2” represents the single best model with correction. “Run #3” represents the ensemble model with correction and CSC. ”Top 1” reports the highest F1 score with its precision and recall at different levels. Model BERT-CRF BERT-GCN-CRF BERT-CRF BERT-GCN-CRF and correct sentences for the seq2seq training without extra data. We used the CGED-2018 testing dataset as our validation dataset. We introduced the BIOES (Ratinov and Roth, 2009) scheme for tagging. Language Technology Plantform (LTP) (Che et al., 2010) was introduced to obtain the dependency tree. The hyper-parameters are selected according to the performance on the validation data through official metrics. For the GCN model, the hidden vector size was 256 with 2 layers. The batch size, learning rate, and GCN dropout were set to 32, 1e-5, 0.2. For the multi-task model, the batch size, learning rate and w are set to 32, 3e-5, 0.9. Transformer decoder parameters are initialized from the BERT parameters as much as possible. 4.2 Type R R M M Precision 42.6 36.2 36.3 32.8"
2020.nlptea-1.6,2020.findings-emnlp.184,1,0.785364,"Missing"
2020.nlptea-1.6,I17-4006,0,0.0600673,"this task. 3.2 h2 多 v1 h1 BERT 病 Graph Convolution Network The multi-layer GCN network accepts the high-level character information obtained by the BERT model and the adjacency matrix of the dependency tree. The convolution operation is adopted for each layer. f (A, H l ) = AHl Wlg (1) where Wlg ∈ RD×D is a trainable matrix for the l-th layer, A is the adjacency matrix of the dependency tree, Hl = (h1 , h2 , ..., hn ) is the hidden state of the characters. Words use the same input representation in the network to indicate the dependency relation of the characters. BERT-GCN-CRF Previous works (Yang et al., 2017; Fu et al., 2018) spent a lot of effort in feature engineering including pretrained features and parsing features. Part-ofspeech-tagging(POS), and dependency information are the most important parsing features, which indicates to us the task is closely associated with the structure of the sentence syntactic dependency. Specifically, the redundant error and the missing error sentences syntax tree are very different from the correct sentences as the Figure 2 shows. To understand the dependency structure of an input sentence better, we introduce the Graph Convolution Network (GCN) (Kipf and Well"
2020.nlptea-1.6,C10-3004,0,0.051755,"lts on the official evaluation testing data. “Run #1” represents the ensemble model with correction. “Run #2” represents the single best model with correction. “Run #3” represents the ensemble model with correction and CSC. ”Top 1” reports the highest F1 score with its precision and recall at different levels. Model BERT-CRF BERT-GCN-CRF BERT-CRF BERT-GCN-CRF and correct sentences for the seq2seq training without extra data. We used the CGED-2018 testing dataset as our validation dataset. We introduced the BIOES (Ratinov and Roth, 2009) scheme for tagging. Language Technology Plantform (LTP) (Che et al., 2010) was introduced to obtain the dependency tree. The hyper-parameters are selected according to the performance on the validation data through official metrics. For the GCN model, the hidden vector size was 256 with 2 layers. The batch size, learning rate, and GCN dropout were set to 32, 1e-5, 0.2. For the multi-task model, the batch size, learning rate and w are set to 32, 3e-5, 0.9. Transformer decoder parameters are initialized from the BERT parameters as much as possible. 4.2 Type R R M M Precision 42.6 36.2 36.3 32.8 Recall 28.3 34.8 26.6 30.0 F1 34.0 35.4 30.7 31.7 Table 3: The position le"
2020.nlptea-1.6,W18-3707,0,0.0871814,"v1 h1 BERT 病 Graph Convolution Network The multi-layer GCN network accepts the high-level character information obtained by the BERT model and the adjacency matrix of the dependency tree. The convolution operation is adopted for each layer. f (A, H l ) = AHl Wlg (1) where Wlg ∈ RD×D is a trainable matrix for the l-th layer, A is the adjacency matrix of the dependency tree, Hl = (h1 , h2 , ..., hn ) is the hidden state of the characters. Words use the same input representation in the network to indicate the dependency relation of the characters. BERT-GCN-CRF Previous works (Yang et al., 2017; Fu et al., 2018) spent a lot of effort in feature engineering including pretrained features and parsing features. Part-ofspeech-tagging(POS), and dependency information are the most important parsing features, which indicates to us the task is closely associated with the structure of the sentence syntactic dependency. Specifically, the redundant error and the missing error sentences syntax tree are very different from the correct sentences as the Figure 2 shows. To understand the dependency structure of an input sentence better, we introduce the Graph Convolution Network (GCN) (Kipf and Welling, 2016; Marcheg"
2020.nlptea-1.6,W18-3710,0,0.0273559,"insert 1 to 4 mask tokens to cover most of the cases and adopt the beam-search algorithm to reduce the search complexity. In the second method, we generate the candidates by a seq2seq model trained by mapping the wrong sentences to the correct sentences. According to the detection result, we keep generating next characters until the correct character appears within the beam-search algorithm, and then replace the incorrect span. (5) We use Viterbi Decoding (Huang et al., 2015) to inference answers. 3.3 Multi-task Most previous works trained their model by the sequence tags (Yang et al., 2017; Li and Qi, 2018; Fu et al., 2018). We utilize not only tags but also correct sentences during the training process. Correct sentences are important for providing better representation in the hidden state. Moreover, with the correct sentences, the model can have a better understanding of the original meaning of the input sentence. Therefore, we introduce the seq2seq task (Sutskever et al., 2014; Vaswani et al., 2017) treating the training process as multi-task learning. As shown in Figure 4, the sequence labeling model is the encoder in our structure combined with the transformer decoders to predict the truth"
2020.nlptea-1.6,D17-1159,0,0.0331148,", 2018) spent a lot of effort in feature engineering including pretrained features and parsing features. Part-ofspeech-tagging(POS), and dependency information are the most important parsing features, which indicates to us the task is closely associated with the structure of the sentence syntactic dependency. Specifically, the redundant error and the missing error sentences syntax tree are very different from the correct sentences as the Figure 2 shows. To understand the dependency structure of an input sentence better, we introduce the Graph Convolution Network (GCN) (Kipf and Welling, 2016; Marcheggiani and Titov, 2017). Figure 3 shows our BERT-GCN-CRF model architecture. We will explain each part in detail. Accumulated Output After the graph convolution network, we concatenate the representation Hl for the l-th layer and the BERT hidden state passing to a linear classifier as the input of the CRF layer. V = Linear(H0 ⊕ Hl ) (2) CRF Layer A CRF layer is introduced to predict the sequence tags for each token. Score(X, Y ) = n X Word Dependency We split the input sentences into words and obtain the dependency relation of 45 n X Vi,yi (3) exp(Score(X, Y )) P (Y |X) = P ˆ Yˆ exp(Score(X, Y )) (4) i=0 Ayi ,yi+1 +"
2020.wmt-1.22,D18-1399,0,0.0372296,"Missing"
2020.wmt-1.22,N18-1118,0,0.0256328,"al., 2017), sentence-level NMT has been based on strong independence and locality assumptions generally, in which the interrelations among these discourse (Jurafsky, 2000) elements were ignored. This results in that the translations may be perfect at the sentence-level but lack crucial properties of the text, hindering understanding (Maruf et al., 2019). To help to resolve ambiguities and inconsistencies in translations, some MT pioneers (Bar-Hillel, 1960; Xiong et al., 2013; Sennrich, 2018) exploit the underlying discourse structure information of a text to address this issue, while others (Bawden et al., 2018; Voita et al., 2018; Jean and Cho, 2019; Wang et al., 2019; Scherrer et al., 2019) extend the translation units with the context or use an additional context encoder and attention. It is worth noting that the essence of the document-level NMT claimed with additional context and attention is still sentence-level MT, whose translation is still output sentence by sentence. We named it as documentenhanced NMT more precisely. Due to computational efficiency and tractability concerns, the document-enhanced NMT models mostly used document embedding, document topic information, and limited past or fu"
2020.wmt-1.22,P18-1192,1,0.83718,"019) is adopted. In the unsupervised and low-resource track, we draw on the successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT mo"
2020.wmt-1.22,D19-5603,1,0.814693,"rmance degradation caused by domain inconsistency. For the final submission, an ensemble of several different trained models outputs the n-best predictions, and used the decoder trained with Marian toolkit to performs reranking to get the final system output. 2 2.1 Methodology XLM-enhanced NMT Pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLM (Conneau et al., 2019), XLNet (Yang et al., 2019), ALBERT (Lan et al., 2019) etc. have recently demonstrated a very dominant effect on natural language processing tasks. Several works (Clinchant et al., 2019; Imamura and Sumita, 2019; Zhu et al., 2020) leveraged a pre-trained BERT model for improving NMT and found that BERT can bring significantly better results over the baseline. Since BERT and other pre-trained language models are trained on large scale corpus beyond the data provided by the WMT20 organizers, the direct use of BERT will make the system submitted unconstrained. Using an XLM model, a variant of BERT, pre-trained from scratch on the monolingual data provided by the official to enhance our NMT model, is a good choice to keep the system constrained. Moreover, the XLM model has the advantages of simple traini"
2020.wmt-1.22,P19-4007,0,0.0555909,"Missing"
2020.wmt-1.22,J82-2005,0,0.698509,"Missing"
2020.wmt-1.22,P19-1285,0,0.0196835,"ting that the essence of the document-level NMT claimed with additional context and attention is still sentence-level MT, whose translation is still output sentence by sentence. We named it as documentenhanced NMT more precisely. Due to computational efficiency and tractability concerns, the document-enhanced NMT models mostly used document embedding, document topic information, and limited past or future context sentences, etc., rather than the truly whole document information. Recently, with the increase in computational power available to us and the well-designed neural network structures (Dai et al., 2019; Kitaev et al., 2019; Beltagy et al., 2020) for long sequence encoding, we are finally in a position to employ the whole document information for enhancing sentence-level NMT. In addition, we argue that since long sequences encoding is easier than decoding, truly whole document-level translation is still a long way off, since the bidirectional context is available in the encoder, but only the past is visible by the decoder. Longformer To make the long documents processed with Transformer (Vaswani et al., 2017) architecture feasible or easier, a modified Transformer architecture named Longform"
2020.wmt-1.22,P19-1120,0,0.0384069,"Missing"
2020.wmt-1.22,N19-1423,0,0.0103748,"the TF-IDF algorithm is employed to filter the training set according to the input of the test set, a training subset whose domain is more similar to the test set is obtained, and then used to finetune the model for reducing the performance degradation caused by domain inconsistency. For the final submission, an ensemble of several different trained models outputs the n-best predictions, and used the decoder trained with Marian toolkit to performs reranking to get the final system output. 2 2.1 Methodology XLM-enhanced NMT Pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLM (Conneau et al., 2019), XLNet (Yang et al., 2019), ALBERT (Lan et al., 2019) etc. have recently demonstrated a very dominant effect on natural language processing tasks. Several works (Clinchant et al., 2019; Imamura and Sumita, 2019; Zhu et al., 2020) leveraged a pre-trained BERT model for improving NMT and found that BERT can bring significantly better results over the baseline. Since BERT and other pre-trained language models are trained on large scale corpus beyond the data provided by the WMT20 organizers, the direct use of BERT will make the system submitted unconstrained. Using an"
2020.wmt-1.22,J93-1004,0,0.207938,"ull model with the obtained model. The PLM-encoder attention attnP and PLM-decoder attention attnPC are randomly initialized. EN-PL On the language pair EN-PL, we explored performance in two training data settings. The first is base data, including Europarl v10, Tilde Rapid corpus, and WikiMatrix bitext data, whose raw data is on the sentence-level. In the second setting base data + paracrawl, we converted the paragraph-level alignment data in Paracrawl to sentence-level alignment and incorporated it with the base data. In the conversion process, we adopted the method and program proposed by (Gale and Church, 1993) for aligning sentences based on a simple statistical model of character lengths, which uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter 224 Systems Transformer big +D2GPo XLM-enhanced Document-enhanced Ensemble ++TF-IDF finetune ++Re-ranking 19test Test BLEU BLEU chrF 37.2 37.7 38.9 39.2 40.0 40.2 40.5 48.6 48.8 49.1 0.418 0.422 0.427 (NSP) classification model provided by Google for document interval prediction to recover the documents. DE-HSB In RUNMT on EN-DE-"
2020.wmt-1.22,P07-2045,0,0.0121171,"s the adjustable parameters. ∀t ∈ Dterms ], (12) where Dterms indicates the all terms set in corpus D. We calculate the cosine similarity as final scores between the query and every source sentence in corpus, and ranked on the scores to get the topK pairs (K=1000 in our experiments) as the subtraining set for finetuning. 3 Data Preprocessing and Model Setup Before model training, we preprocessed the data uniformly and customized the processing according to the requirements of each model. We normalized punctuation, remove non-printing characters, and tokenize all data with the Moses tokenizer (Koehn et al., 2007) except for the Chinese. For Chinese, we removed the segmentation space in some training data and then use PKUSeg (Luo et al., 2019) toolkit to cut all Chinese sentences, so as to obtain unified word segmentation annotations. We use joint byte pair encodings (BPE) with 40K split operations for subword segmentation (Sennrich et al., 2016). In XLM-enhanced NMT and Documentenhanced NMT, we first train a basic NMT (Transformer big) model on the sentence-level data until convergence, then initialize the encoder and decoder of the XLM-enhanced NMT and Document-enhanced NMT full model with the obtain"
2020.wmt-1.22,C18-1271,1,0.818947,"In the unsupervised and low-resource track, we draw on the successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from"
2020.wmt-1.22,N19-4009,0,0.0819852,"(U1836222 and 61733011), Huawei-SJTU Long Term AI Project, Cuttingedge Machine Reading Comprehension and Language Model. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT tenuretrack researcher startup fund “Toward Intelligent Machine Translation”. (DE) ↔ Upper Sorbian (HSB) both directions are focused. Our baseline system in supervised track is based on the Transformer big architecture proposed by Vaswani et al. (2017), in which its opensource implementation version Fairseq (Ott et al., 2019) is adopted. In the unsupervised and low-resource track, we draw on the successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He e"
2020.wmt-1.22,N09-2056,1,0.713764,"eference agreement mechanism. Specifically, we proposed three kinds of reference agreement utilization approaches in (Li et al., 2020b): reference agreement translation (RAT), reference agreement back-translation (RABT), and cross-lingual back-translation (XBT). (6) LRABT (S, T , R) = L(θT →S ) + L(θT →R ). (9) XBT The parallel corpus between languages S and R can not only bring agreement in the translations of the same target language T , but also cross-lingual agreement, that is, using the target language as the bridge to form pivot translation (Wu and Wang, 2007; Utiyama and Isahara, 2007; Paul et al., 2009) patterns: S → T → R and R → T → S. In XBT, paired sentences s and r are translated to language T : t˜s and t˜r , and forms two new pseudo-parallel pairs: ht˜s , ri and ht˜r , si, which promote the training of translation T → R and T → S. The objective function of XBT is: RAT RAT utilizes the principle for translating paired sentences into the target language T of the source S and reference R language. Since the input the parallel, the both translation outputs should be the same. Given a parallel sentence pair hs, ri between language S and R, we would ideally have P(·|s; θS→T ) = P(·|r; θR→T )"
2020.wmt-1.22,N18-1202,0,0.00847445,"model training is finished, the TF-IDF algorithm is employed to filter the training set according to the input of the test set, a training subset whose domain is more similar to the test set is obtained, and then used to finetune the model for reducing the performance degradation caused by domain inconsistency. For the final submission, an ensemble of several different trained models outputs the n-best predictions, and used the decoder trained with Marian toolkit to performs reranking to get the final system output. 2 2.1 Methodology XLM-enhanced NMT Pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLM (Conneau et al., 2019), XLNet (Yang et al., 2019), ALBERT (Lan et al., 2019) etc. have recently demonstrated a very dominant effect on natural language processing tasks. Several works (Clinchant et al., 2019; Imamura and Sumita, 2019; Zhu et al., 2020) leveraged a pre-trained BERT model for improving NMT and found that BERT can bring significantly better results over the baseline. Since BERT and other pre-trained language models are trained on large scale corpus beyond the data provided by the WMT20 organizers, the direct use of BERT will make the system submi"
2020.wmt-1.22,2020.findings-emnlp.371,1,0.64243,"Missing"
2020.wmt-1.22,2021.ccl-1.108,0,0.100832,"Missing"
2020.wmt-1.22,P12-3005,0,0.0427849,"gned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences. For the Polish pre-trained XLM language model, we used all NewsCrawl monolingual data and some CommonCrawl monolingual data. Since the CommonCrawl data is very large and noisy and can potentially decrease the performance of LM if it is used in its raw form. We apply language identification filtering (langid; Lui and Baldwin (2012)), keeping sentences with correct languages. In order to filter out the sentences shorter than 5 words or longer than 150 words more precisely, we re-split sentences using Spacy (Honnibal and Montani, 2017) toolkit. EN-ZH In EN-ZH, the pre-training of Longformer as a document encoder is unique. As described in (Beltagy et al., 2020), the Longformer needs a large number of gradient updates to learn the local context first; before learning to utilize longer context. In the first phase of the staged training procedure, an initial RoBERTa (Liu et al., 2019) model implemented in Fairseq (Ott et al."
2020.wmt-1.22,2020.acl-main.571,1,0.800756,", 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorporating BERT into NMT (Zhu et al., 2020). Besides, we trained a bidirectional translation model of EN-"
2020.wmt-1.22,W18-6319,0,0.0144898,"uent phase, we trained the model on the paragraph text, doubled the window size and the sequence length, and halve the learning rate. For the paragraph text, the Wikidumps and NewsCommentary v15 have document intervals and can be used directly, while UN v1.0 has no document intervals but the sentence order is not interrupted. Therefore, we use the BERT Next Sentence Prediction Results and Analysis Results and ablations for PL→EN2 are shown in Table 1, EN→ZH in Table 2, unsupervised DE↔HSB in Table 3 and low-resource DE↔HSB in Table 4. We report case-sensitive SacreBLEU scores using SacreBLEU (Post, 2018) for EN-PL, DE-HSB, and BLEU based on characters for ENZH. In the results, “+” means addition based on baseline, and “++” means cumulative addition based on the previous one. In PL→EN, the introduction of ParaCrawl data improves the baseline performance on the dev dataset by about 4.2 BLEU. +D2GPo, XLM-enhanced NMT, Bidirectional NMT, and ensembling outperforms our strong baseline by 2 BLEU point. Finally, finetuning and reranking further gives another 0.5 BLEU. For EN→ZH, as with PL→EN, we see similar improvements with +D2GPo, XLM-enhanced NMT, ensembling and reranking. We also observe that t"
2020.wmt-1.22,D19-6506,0,0.0216909,"irections, the differences are also very obvious. To further expose the model to the direction difference and improve the effect of unidirectional translation, we further finetune the 220 bidirectional pre-trained model on the bilingual data. Take S2T translation as an example; the model is optimized as follows: L(θS→T ) = N X log p(y (n) |x(n) ), (5) n=1 where θS→T is the parameters of child model which is initialized with θparent . Similarly, the T2S child model can also be obtained. Due to the introduction of bidirectional translation in one model, follow the practice of Conneau and Lample (2019), shared subword vocabulary and shared encoder-decoder (source and target) embedding were employed to improves the alignment of embedding spaces across languages. In addition, since the encoder and decoder need to be able to handle two languages simultaneously, a language embedding was used to indicate the language being processed, so as to reduce confusion of the model. 2.3 Document-enhanced NMT In spite of its success (Vaswani et al., 2017), sentence-level NMT has been based on strong independence and locality assumptions generally, in which the interrelations among these discourse (Jurafsky"
2020.wmt-1.22,P16-1162,0,0.0703717,"cessing and Model Setup Before model training, we preprocessed the data uniformly and customized the processing according to the requirements of each model. We normalized punctuation, remove non-printing characters, and tokenize all data with the Moses tokenizer (Koehn et al., 2007) except for the Chinese. For Chinese, we removed the segmentation space in some training data and then use PKUSeg (Luo et al., 2019) toolkit to cut all Chinese sentences, so as to obtain unified word segmentation annotations. We use joint byte pair encodings (BPE) with 40K split operations for subword segmentation (Sennrich et al., 2016). In XLM-enhanced NMT and Documentenhanced NMT, we first train a basic NMT (Transformer big) model on the sentence-level data until convergence, then initialize the encoder and decoder of the XLM-enhanced NMT and Document-enhanced NMT full model with the obtained model. The PLM-encoder attention attnP and PLM-decoder attention attnPC are randomly initialized. EN-PL On the language pair EN-PL, we explored performance in two training data settings. The first is base data, including Europarl v10, Tilde Rapid corpus, and WikiMatrix bitext data, whose raw data is on the sentence-level. In the secon"
2020.wmt-1.22,2021.naacl-main.311,1,0.626638,"Missing"
2020.wmt-1.22,P95-1026,0,0.836605,"e unlabeled dataset U = {x(j) }L j=1 is used for the synthesis of pseudo-parallel corpora. While in UNMT, since the model is trained with backtranslation on unpaired monolingual data, the pseudo-parallel corpora is synthesized by the monolingual data, i.e., U = {x(m) }M m=1 . Considering the translation quality can’t effectively be evaluated across languages in machine translation with only the monolingual data, therefore the selection of the subset Q, is one of the key factors for self-training. It is usually selected based on some confidence scores (e.g. log probability or perplexity, PPL) (Yarowsky, 1995), but it is also possible for S to be the whole pseudo parallel data (Zhu and Goldberg, 2009). In the backward translation based on the pseudo-parallel data, the DAE method widely used in UNMT can alleviate the impact of the noise resulted from the synthesized sentences on model training, since the synthesized sentences are only used as input. However, in the forward translation training, the quality of noisy targets will directly affect the success of the model training. Therefore, the selection of synthetic parallel corpus becomes particularly critical. fθT →S 5: Calculate BT-BLEU B for two"
2020.wmt-1.22,N07-1061,1,0.7001,"Missing"
2020.wmt-1.22,P18-1117,0,0.0215559,"-level NMT has been based on strong independence and locality assumptions generally, in which the interrelations among these discourse (Jurafsky, 2000) elements were ignored. This results in that the translations may be perfect at the sentence-level but lack crucial properties of the text, hindering understanding (Maruf et al., 2019). To help to resolve ambiguities and inconsistencies in translations, some MT pioneers (Bar-Hillel, 1960; Xiong et al., 2013; Sennrich, 2018) exploit the underlying discourse structure information of a text to address this issue, while others (Bawden et al., 2018; Voita et al., 2018; Jean and Cho, 2019; Wang et al., 2019; Scherrer et al., 2019) extend the translation units with the context or use an additional context encoder and attention. It is worth noting that the essence of the document-level NMT claimed with additional context and attention is still sentence-level MT, whose translation is still output sentence by sentence. We named it as documentenhanced NMT more precisely. Due to computational efficiency and tractability concerns, the document-enhanced NMT models mostly used document embedding, document topic information, and limited past or future context sentenc"
2020.wmt-1.22,P07-1108,0,0.0458195,"truction training of UNMT through a proposed reference agreement mechanism. Specifically, we proposed three kinds of reference agreement utilization approaches in (Li et al., 2020b): reference agreement translation (RAT), reference agreement back-translation (RABT), and cross-lingual back-translation (XBT). (6) LRABT (S, T , R) = L(θT →S ) + L(θT →R ). (9) XBT The parallel corpus between languages S and R can not only bring agreement in the translations of the same target language T , but also cross-lingual agreement, that is, using the target language as the bridge to form pivot translation (Wu and Wang, 2007; Utiyama and Isahara, 2007; Paul et al., 2009) patterns: S → T → R and R → T → S. In XBT, paired sentences s and r are translated to language T : t˜s and t˜r , and forms two new pseudo-parallel pairs: ht˜s , ri and ht˜r , si, which promote the training of translation T → R and T → S. The objective function of XBT is: RAT RAT utilizes the principle for translating paired sentences into the target language T of the source S and reference R language. Since the input the parallel, the both translation outputs should be the same. Given a parallel sentence pair hs, ri between language S and R, we w"
2020.wmt-1.22,P19-1298,1,0.820418,"successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorporating BERT into NMT (Zhu et al., 2020)."
2020.wmt-1.22,D13-1163,0,0.0303897,"dicate the language being processed, so as to reduce confusion of the model. 2.3 Document-enhanced NMT In spite of its success (Vaswani et al., 2017), sentence-level NMT has been based on strong independence and locality assumptions generally, in which the interrelations among these discourse (Jurafsky, 2000) elements were ignored. This results in that the translations may be perfect at the sentence-level but lack crucial properties of the text, hindering understanding (Maruf et al., 2019). To help to resolve ambiguities and inconsistencies in translations, some MT pioneers (Bar-Hillel, 1960; Xiong et al., 2013; Sennrich, 2018) exploit the underlying discourse structure information of a text to address this issue, while others (Bawden et al., 2018; Voita et al., 2018; Jean and Cho, 2019; Wang et al., 2019; Scherrer et al., 2019) extend the translation units with the context or use an additional context encoder and attention. It is worth noting that the essence of the document-level NMT claimed with additional context and attention is still sentence-level MT, whose translation is still output sentence by sentence. We named it as documentenhanced NMT more precisely. Due to computational efficiency and"
2020.wmt-1.22,C18-1317,1,0.818967,"ed and low-resource track, we draw on the successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorpo"
2020.wmt-1.22,P19-1230,1,0.815888,"ce of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorporating BERT into NMT (Zhu et al., 2020). Besides, we trained a"
2020.wmt-1.22,D16-1163,0,0.0162403,"t , where [0, pnet 2 ) is the probability of attending to L, the final sum for the first attn in HEL and HD pnet pnet [ 2 , 1 − 2 ) is the probability for the whole HEL L equation, [1 − pnet , 1] is the probability and HD 2 L. for the second attn in in HEL and HD 2.2 Bidirectional NMT Machine translation, in general, is unidirectional, that is, from the source language to the target language. The encoder-decoder framework for NMT has been shown effective in large data scenarios, and the more high-quality bilingual training data, the better performance the model tends to achieve. Recent works (Zoph et al., 2016; Kim et al., 2019) on translation transfer learning (Torrey and Shavlik, 2010; Pan and Yang, 2009) from rich-resource language pairs to low-resource language pairs demonstrate that translation has some universal nature in essence between different language pairs. As the sourceto-target (S2T) forward translation and target-tosource (T2S) backward translation can be seen as two special language pairs in bilingual translation, it can make use of the translation universal nature to improve each other, i.e., dual learning (He et al., 2016). Based on this motivation, we developed a bidirectional NM"
2021.eacl-tutorials.5,D18-1399,0,0.0156808,"Missing"
2021.eacl-tutorials.5,P19-1019,0,0.0172519,"ination of diverse mechanisms such as an initialization with bilingual word embeddings, denoising auto-encoder, back-translation, and shared latent representation. 1.2 1.3 Recent Advances USMT and UNMT. Since 2016, statistical MT (SMT) has been significantly over-passed by NMT. Lample et al. (2018c) and Artetxe et al. (2018a) proposed an alternative method, that is, unsupervised statistical machine translation (USMT) method. However, in the supervised scenario, the performance of USMT method is comparable with that of UNMT. In addition, several works (Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019) combined UNMT and USMT to improve unsupervised machine translation performance. In WMTMethods Cross-lingual language representation initialization. In supervised NMT, language representation initialization is not so necessary, because the bilingual corpus can help NMT learn the crosslingual representation. In comparison, there is only monolingual corpus for UNMT. Therefore, the pretrained unsupervised bilingual word embedding 1 https://wangruinlp.github.io/unmt. html 17 Proceedings of EACL: Tutorials, pages 17–21 April 19 - 20, 2021. ©2020 Association for Computational Linguistics 2019, the u"
2021.eacl-tutorials.5,P19-1017,0,0.0216804,"-lingual language model (Lample and Conneau, 2019; Song et al., 2019) achieve better UNMT performance than the bilingual word embedding. In high-resource scenario, UNMT has achieved remarkable performance. However, the performance of low-resource UNMT is still far below expectations Multilingualism. To improve the low-resource UNMT, multi-lingual UNMT (MUNMT) is proposed (Sun et al., 2020; Liu et al., 2020). The translation of low-resouce and zero-shot language pairs can be enhanced by the similar languages in the shared latent representation. In addition, the pivotbased methods are proposed. Leng et al. (2019) introduced unsupervised pivot translation for distant language pairs. The SJTU-NICT team used monolingual corpus together with parallel third-party languages to enhance the low-resource UNMT performance (Li et al., 2020b) and their system achieved the best performance in WMT-2020 unsupervised task (Li et al., 2020a). 1.4 supervision, it is very difficult for UNMT to learn the syntactic correspondence. 2) There are too few shared words/subwords in the distant language pair to learn the shared latent representation for UNMT. Finally, we will show some potential solutions, such as 1) syntactic m"
2021.eacl-tutorials.5,J82-2005,0,0.513875,"Missing"
2021.eacl-tutorials.5,2020.wmt-1.22,1,0.763207,"Missing"
2021.eacl-tutorials.5,2020.findings-emnlp.371,1,0.880271,"Missing"
2021.eacl-tutorials.5,2020.tacl-1.47,0,0.0208541,"s on improving the language representation pre-training. Sun et al. (2019b) proposed to train UNMT jointly with bilingual word embedding agreement. More recently, it has been shown that the pre-trained cross-lingual language model (Lample and Conneau, 2019; Song et al., 2019) achieve better UNMT performance than the bilingual word embedding. In high-resource scenario, UNMT has achieved remarkable performance. However, the performance of low-resource UNMT is still far below expectations Multilingualism. To improve the low-resource UNMT, multi-lingual UNMT (MUNMT) is proposed (Sun et al., 2020; Liu et al., 2020). The translation of low-resouce and zero-shot language pairs can be enhanced by the similar languages in the shared latent representation. In addition, the pivotbased methods are proposed. Leng et al. (2019) introduced unsupervised pivot translation for distant language pairs. The SJTU-NICT team used monolingual corpus together with parallel third-party languages to enhance the low-resource UNMT performance (Li et al., 2020b) and their system achieved the best performance in WMT-2020 unsupervised task (Li et al., 2020a). 1.4 supervision, it is very difficult for UNMT to learn the syntactic co"
2021.eacl-tutorials.5,W19-5330,1,0.788036,"l language representation initialization. In supervised NMT, language representation initialization is not so necessary, because the bilingual corpus can help NMT learn the crosslingual representation. In comparison, there is only monolingual corpus for UNMT. Therefore, the pretrained unsupervised bilingual word embedding 1 https://wangruinlp.github.io/unmt. html 17 Proceedings of EACL: Tutorials, pages 17–21 April 19 - 20, 2021. ©2020 Association for Computational Linguistics 2019, the unsupervised MT task (German-Czech) first-time became the officially task of WMT, and the system from NICT (Marie et al., 2019) won the first place and achieved state-of-the-art performances by combining the USMT and UNMT. However, after the advanced pre-training technologies was developed, USMT became less important. Advanced Pre-Training Technologies. Similar as other NLP tasks, the quality of language representation pre-training significantly affects the performance of UNMT. Several works focus on improving the language representation pre-training. Sun et al. (2019b) proposed to train UNMT jointly with bilingual word embedding agreement. More recently, it has been shown that the pre-trained cross-lingual language m"
2021.eacl-tutorials.5,P16-1078,0,0.0253765,"ced unsupervised pivot translation for distant language pairs. The SJTU-NICT team used monolingual corpus together with parallel third-party languages to enhance the low-resource UNMT performance (Li et al., 2020b) and their system achieved the best performance in WMT-2020 unsupervised task (Li et al., 2020a). 1.4 supervision, it is very difficult for UNMT to learn the syntactic correspondence. 2) There are too few shared words/subwords in the distant language pair to learn the shared latent representation for UNMT. Finally, we will show some potential solutions, such as 1) syntactic methods (Eriguchi et al., 2016; Chen et al., 2017, 2018) and 2) artificial shared words/code-switching methods (Yang et al., 2020) and show the initial results. Domain adaptation methods for UNMT have not been well-studied although UNMT has recently achieved remarkable results in some specific domains for several language pairs. For UNMT, addition to inconsistent domains between training data and test data for supervised NMT, there also exist other inconsistent domains between monolingual training data in two languages. Actually, it is difficult for some language pairs to obtain enough source and target monolingual corpora"
2021.eacl-tutorials.5,P16-1009,0,0.021246,"ranslation to generate pseudo-parallel corpora at the beginning of the UNMT training. Denoising auto-encoder: Noise obtained by randomly performing local substitutions and word reorderings (Vincent et al., 2010), is added to the input sentences to improve model learning ability and regularization. The denoising auto-encoder model objective function would be optimized by maximizing the probability of encoding a noisy sentence and reconstructing it. Back-translation: The back-translation plays a key role in achieving unsupervised translation relying only on monolingual corpora in each language (Sennrich et al., 2016). The pseudo-parallel sentence pairs produced by the model at the previous iteration have been used to train the new translation model. Sharing latent representations: Encoders and decoders are (partially) shared for two languages. Therefore, the two languages must use the same vocabulary. The entire training of UNMT needs to consider back-translation between the two languages and their respective denoising processing. Unsupervised cross-lingual language representation initialization methods, together with mechanisms such as denoising and backtranslation, have advanced unsupervised neural mach"
2021.eacl-tutorials.5,P19-1119,1,0.846643,"Missing"
2021.eacl-tutorials.5,2020.acl-main.324,1,0.772073,"Several works focus on improving the language representation pre-training. Sun et al. (2019b) proposed to train UNMT jointly with bilingual word embedding agreement. More recently, it has been shown that the pre-trained cross-lingual language model (Lample and Conneau, 2019; Song et al., 2019) achieve better UNMT performance than the bilingual word embedding. In high-resource scenario, UNMT has achieved remarkable performance. However, the performance of low-resource UNMT is still far below expectations Multilingualism. To improve the low-resource UNMT, multi-lingual UNMT (MUNMT) is proposed (Sun et al., 2020; Liu et al., 2020). The translation of low-resouce and zero-shot language pairs can be enhanced by the similar languages in the shared latent representation. In addition, the pivotbased methods are proposed. Leng et al. (2019) introduced unsupervised pivot translation for distant language pairs. The SJTU-NICT team used monolingual corpus together with parallel third-party languages to enhance the low-resource UNMT performance (Li et al., 2020b) and their system achieved the best performance in WMT-2020 unsupervised task (Li et al., 2020a). 1.4 supervision, it is very difficult for UNMT to lea"
2021.eacl-tutorials.5,2020.emnlp-main.208,0,0.0688424,"us together with parallel third-party languages to enhance the low-resource UNMT performance (Li et al., 2020b) and their system achieved the best performance in WMT-2020 unsupervised task (Li et al., 2020a). 1.4 supervision, it is very difficult for UNMT to learn the syntactic correspondence. 2) There are too few shared words/subwords in the distant language pair to learn the shared latent representation for UNMT. Finally, we will show some potential solutions, such as 1) syntactic methods (Eriguchi et al., 2016; Chen et al., 2017, 2018) and 2) artificial shared words/code-switching methods (Yang et al., 2020) and show the initial results. Domain adaptation methods for UNMT have not been well-studied although UNMT has recently achieved remarkable results in some specific domains for several language pairs. For UNMT, addition to inconsistent domains between training data and test data for supervised NMT, there also exist other inconsistent domains between monolingual training data in two languages. Actually, it is difficult for some language pairs to obtain enough source and target monolingual corpora from the same domain in the real-world scenario. In this tutorial, we will empirically show differe"
2021.emnlp-main.185,D15-1075,0,0.25344,",shaohan.ljh,jian.sun,f.huang,luo.si}@alibaba-inc.com Abstract different learning schemes. Kiros et al. (2015); Logeswaran and Lee (2018); Hill et al. (2016) train Learning sentence embeddings from dialogues sentence encoders in a self-supervised manner with has drawn increasing attention due to its low web pages and books. Conneau et al. (2017); Cer annotation cost and high domain adaptabilet al. (2018); Reimers and Gurevych (2019) proity. Conventional approaches employ the pose to learn sentence embeddings on the supersiamese-network for this task, which obtains vised datasets such as SNLI (Bowman et al., 2015) the sentence embeddings through modeling and MNLI (Williams et al., 2018). Although the the context-response semantic relevance by apsupervised-learning approaches achieve better perplying a feed-forward network on top of the formance, they suffer from high cost of annotation sentence encoders. However, as the semantic textual similarity is commonly measured in building the training dataset, which makes them through the element-wise distance metrics (e.g. hard to adapt to other domains or languages. cosine and L2 distance), such architecture Recently, learning sentence embeddings from yields"
2021.emnlp-main.185,D18-2029,0,0.0906045,"mance. However, periments show that our approach achieves they concatenate the multi-turn dialogue context better performance when leveraging more diinto a long token sequence, failing to model interalogue context and remains robust when less sentence semantic relationships among the uttertraining data is provided. ances. Recently, more advanced methods such as (Reimers and Gurevych, 2019) achieve better per1 Introduction formance by employing BERT (Devlin et al., 2019) Sentence embeddings are used with success for as the sentence encoder. These works have in coma variety of NLP applications (Cer et al., 2018) mon that they employ a feed-forward network with and many prior methods have been proposed with a non-linear activation on top of the sentence en2396 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2396–2406 c November 7–11, 2021. 2021 Association for Computational Linguistics coders to model the context-response semantic relevance, thereby learning the sentence embeddings. However, such architecture presents two limitations: (1) It yields a large gap between training and evaluating, since the semantic textual similarity is commonly measured by th"
2021.emnlp-main.185,L18-1269,0,0.0142213,"MNLI datasets, achieving the new state-of-the-art performance. Lately, the contrastive self-supervised learning approaches have shown their effectiveness and merit in this area. Wu et al. (2020); Giorgi et al. (2020); Meng et al. (2021) incorporate the data augmentation methods including the word-level deletion, reordering, substitution, and the sentencelevel corruption into the pre-training of deep Transformer models to improve the sentence representation ability, achieving significantly better performance than BERT especially on the sentence-level tasks (Wang et al., 2018; Cer et al., 2017; Conneau and Kiela, 2018). Gao et al. (2021) apply a twice independent dropout to obtain two same-source embeddings from a single sentence as input. Through 3 Problem Formulation optimizing their cosine distance, SimCSE achieves Suppose that we have a dialogue remarkable gains over the previous baselines. Yan K = {Si }i=1 , where Si = et al. (2021) empirically study more data augmen- dataset D tation strategies in learning sentence embeddings, {u1 , · · · , uk−1 , r, uk+1 , · · · , ut } is the i-th dialogue session in D with t turn utterances. r is the and it also achieves remarkable performance as SimCSE. In this wor"
2021.emnlp-main.185,D17-1070,0,0.0178248,"sponse matching relationships. Our work is closely related to their works. We propose a novel dialogue-based contrastive learning approach, which directly models the context-response matching relationships without an intermediate MLP. We also consider the interactions between each utterance in the dialogue context and the response instead of simply treating the dialogue context as a long sequence. 2.2 Supervised Learning Approaches The supervised learning approaches mainly focus on training classification models with the SNLI and the MNLI datasets (Bowman et al., 2015; Williams et al., 2018). Conneau et al. (2017) demonstrate the superior performance of the supervised learning model on both the STS-benchmark (Cer et al., 2017) and the SICK-R tasks (Marelli et al., 2014). Based on this observation, Cer et al. (2018) further extend the supervised learning to the multi-task learning by introducing the QA prediction task, the Skip-Thought-like task (Henderson et al., 2017; Kiros et al., 2015), and the NLI classification task, achieving significant improvement over InferSent. Reimers and Gurevych (2019) employ BERT as sentence encoders in the siamese-network and finetune them with the SNLI and the MNLI data"
2021.emnlp-main.185,N19-1423,0,0.476909,"on measures, demonstrating the multi-turn dialogue context can improve ing its effectiveness. Further quantitative exthe sentence embedding performance. However, periments show that our approach achieves they concatenate the multi-turn dialogue context better performance when leveraging more diinto a long token sequence, failing to model interalogue context and remains robust when less sentence semantic relationships among the uttertraining data is provided. ances. Recently, more advanced methods such as (Reimers and Gurevych, 2019) achieve better per1 Introduction formance by employing BERT (Devlin et al., 2019) Sentence embeddings are used with success for as the sentence encoder. These works have in coma variety of NLP applications (Cer et al., 2018) mon that they employ a feed-forward network with and many prior methods have been proposed with a non-linear activation on top of the sentence en2396 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2396–2406 c November 7–11, 2021. 2021 Association for Computational Linguistics coders to model the context-response semantic relevance, thereby learning the sentence embeddings. However, such architecture presen"
2021.emnlp-main.185,2021.emnlp-main.552,0,0.0500556,"Missing"
2021.emnlp-main.185,2020.acl-main.740,0,0.0309319,"Missing"
2021.emnlp-main.185,2020.emnlp-main.733,0,0.076929,"are inherently limited and hard to achieve further improvement. Recently, the pre-trained language models such as BERT (Devlin et al., 2019) and GPT (Radford et al.) yield strong performances across many downstream tasks (Wang et al., 2018). However, BERT’s embeddings show poor performance without fine-tuning and many efforts have been devoted to alleviating this issue. Zhang et al. (2020) propose a self-supervised learning approach that derives meaningful BERT sentence embeddings by maximizing the mutual information between the global sentence embedding and all its local context embeddings. Li et al. (2020) argue that BERT induces a non-smooth anisotropic semantic space. They propose to use a flow-based generative module to transform BERT’s embeddings into isotropic semantic space. Similar to this work, Su et al. (2021) replace the flow-based generative module with a simple but efficient linear mapping layer, achieving competitive results with reported experiments in BERT-flow. For dialogue, Yang et al. (2018) train a siamese transformer network with single-turn inputresponse pairs extracted from Reddit. Such architecture is further extended in (Reimers and Gurevych, 2019) by replacing the trans"
2021.emnlp-main.185,marelli-etal-2014-sick,0,0.0249141,"ls the context-response matching relationships without an intermediate MLP. We also consider the interactions between each utterance in the dialogue context and the response instead of simply treating the dialogue context as a long sequence. 2.2 Supervised Learning Approaches The supervised learning approaches mainly focus on training classification models with the SNLI and the MNLI datasets (Bowman et al., 2015; Williams et al., 2018). Conneau et al. (2017) demonstrate the superior performance of the supervised learning model on both the STS-benchmark (Cer et al., 2017) and the SICK-R tasks (Marelli et al., 2014). Based on this observation, Cer et al. (2018) further extend the supervised learning to the multi-task learning by introducing the QA prediction task, the Skip-Thought-like task (Henderson et al., 2017; Kiros et al., 2015), and the NLI classification task, achieving significant improvement over InferSent. Reimers and Gurevych (2019) employ BERT as sentence encoders in the siamese-network and finetune them with the SNLI and the MNLI datasets, achieving the new state-of-the-art performance. Lately, the contrastive self-supervised learning approaches have shown their effectiveness and merit in t"
2021.emnlp-main.185,D14-1162,0,0.0885511,"ce embedding to the isotropic semantic space. For BERT, we use the [CLS] token embedding (denoted as BERT-CLS) and the average of the sequence output embeddings (denoted as BERT-avg) as the sentence embedding, and the same is true for domain-adaptive BERT. It should be noted that in related sentence embedding researches, domainadaptive BERT is rarely considered since the training datasets are relatively small. Fortunately, the large-scale dialogue datasets allow us to explore whether the domain-adaptive pre-training is helpful for our tasks. We also adopt the average of GloVe word embeddings (Pennington et al., 2014) (denoted as Avg. GloVe) as the sentence embedding to compare with our results. 5.2.2 Dialogue-based self-supervised learning methods In this line, we mainly consider the siamesenetworks commonly applied in dialogue-based researches. Considering none of the previous works (Yang et al., 2018; Henderson et al., 2020) employs the pre-trained language model as encoder, we re2401 Model Microsoft Corpus Corr. MAP MRR Jing Dong Corpus Corr. MAP MRR E-commerce Corpus Corr. MAP MRR Self-supervised models Avg. GloVe embeddings BERT-CLS BERT-avg BERT-flow BERT-whitening 36.64 22.34 40.95 45.56 26.70 31.5"
2021.emnlp-main.185,2020.findings-emnlp.196,0,0.0767416,"Missing"
2021.emnlp-main.185,N16-1162,0,0.0261775,"Missing"
2021.emnlp-main.185,D19-1410,0,0.130655,"in terms of MAP and Henderson et al. (2020) demonstrate that introducSpearman’s correlation measures, demonstrating the multi-turn dialogue context can improve ing its effectiveness. Further quantitative exthe sentence embedding performance. However, periments show that our approach achieves they concatenate the multi-turn dialogue context better performance when leveraging more diinto a long token sequence, failing to model interalogue context and remains robust when less sentence semantic relationships among the uttertraining data is provided. ances. Recently, more advanced methods such as (Reimers and Gurevych, 2019) achieve better per1 Introduction formance by employing BERT (Devlin et al., 2019) Sentence embeddings are used with success for as the sentence encoder. These works have in coma variety of NLP applications (Cer et al., 2018) mon that they employ a feed-forward network with and many prior methods have been proposed with a non-linear activation on top of the sentence en2396 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2396–2406 c November 7–11, 2021. 2021 Association for Computational Linguistics coders to model the context-response semantic rele"
2021.emnlp-main.185,W18-5446,0,0.0419368,"Missing"
2021.emnlp-main.185,N18-1101,0,0.11829,"learning schemes. Kiros et al. (2015); Logeswaran and Lee (2018); Hill et al. (2016) train Learning sentence embeddings from dialogues sentence encoders in a self-supervised manner with has drawn increasing attention due to its low web pages and books. Conneau et al. (2017); Cer annotation cost and high domain adaptabilet al. (2018); Reimers and Gurevych (2019) proity. Conventional approaches employ the pose to learn sentence embeddings on the supersiamese-network for this task, which obtains vised datasets such as SNLI (Bowman et al., 2015) the sentence embeddings through modeling and MNLI (Williams et al., 2018). Although the the context-response semantic relevance by apsupervised-learning approaches achieve better perplying a feed-forward network on top of the formance, they suffer from high cost of annotation sentence encoders. However, as the semantic textual similarity is commonly measured in building the training dataset, which makes them through the element-wise distance metrics (e.g. hard to adapt to other domains or languages. cosine and L2 distance), such architecture Recently, learning sentence embeddings from yields a large gap between training and evaluatdialogues has begun to attract inc"
2021.emnlp-main.185,2021.acl-long.393,0,0.026849,"closer, resulting in a decrease in Spearman’s correlation. However, as all positive samples in the candidates have identical labels, such degradation may not be fully reflected through the ranking metric (e.g. MAP) or even be covered as the number of retrieved positive samples changes. Impact of negative samples. We vary the number of negative samples for each positive sample within {1, 4, 9, 19}. Table 4 shows the experimental results, from which we find that both metrics improve slightly when the number of negative samples increases. Considering the similar observation in (Gao et al., 2021; Yan et al., 2021), we conclude this phenomenon may be related to the discrete nature of language. Specifically, as the generation of the sentence embeddings in our approach is guided and constrained by the token-level interaction mechanism, our model is more robust than the other contrastive learning approaches and is even effective when only one negative sample is provided. quality of the dialogue-based sentence embeddings. Evaluation results show that DialogueCSE achieves the best result over the baselines while adding no additional parameters. In the next step, we will study how to introduce more interactio"
2021.emnlp-main.185,W18-3022,0,0.356865,"esponse learning methods promising to achieve competembedding (i.e. the context-free embedding) itive or even superior performance against the according to the guidance of the multi-turn supervised-learning methods, especially under the context-response matching matrices. Then it low-resource conditions. pairs each context-aware embedding with its corresponding context-free embedding and fiWhile promising, the issue of how to effectively nally minimizes the contrastive loss across exploit the dialogues for this task has not been sufall pairs. We evaluate our model on three ficiently explored. Yang et al. (2018) propose to multi-turn dialogue datasets: the Microsoft train an input-response prediction model on Reddit Dialogue Corpus, the Jing Dong Dialogue dataset (Al-Rfou et al., 2016). Since they build their Corpus, and the E-commerce Dialogue Corarchitecture based on the single-turn dialogue, the pus. Evaluation results show that our apmulti-turn dialogue history is not fully exploited. proach significantly outperforms the baselines across all three datasets in terms of MAP and Henderson et al. (2020) demonstrate that introducSpearman’s correlation measures, demonstrating the multi-turn dialogue co"
2021.emnlp-main.185,2020.emnlp-main.124,0,0.0212252,"nces of the context in the Dialogue Corpus (ECD) (Zhang et al., 2018). To corpus. Hill et al. (2016) propose to predict the evaluate our model, we introduce two types of neighboring sentences as bag-of-words instead of tasks: the semantic retrieval (SR) task and the dialogue-based semantic textual similarity (D-STS) step-by-step decoding. Logeswaran and Lee (2018) perform sentence-level modeling by retrieving the task. Here we do not adopt the standard semantic ground-truth sentence from candidates under the textual similarity (STS) task (Cer et al., 2017) for two reasons: (1) As revealed in (Zhang et al., 2020), given context, achieving consistently better performance compared to the previous token-level modthe sentence embedding performance varies greatly eling approaches. The datasets used in these works as the domain of the training data changes. As a 1 dialogue dataset is always about several certain doAll the datasets will be publicly available at mains, evaluating on the STS benchmark may mis- https://github.com/wangruicn/DialogueCSE 2397 are typically built upon the corpus of web pages and books (Zhu et al., 2015). As the semantic connections are relatively weak in these corpora, the model pe"
2021.emnlp-main.185,C18-1317,0,0.0983426,"ches. • Extensive experiments show that DialogueCSE significantly outperforms the baselines, establishing the state-of-the-art results. 2 Related Work 2.1 Self-supervised Learning Approaches We train our model on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus (MDC) Early works on sentence embeddings mainly focus on the self-supervised learning approaches. Kiros (Li et al., 2018), the Jing Dong Dialogue Corpus et al. (2015) train a seq2seq network by decod(JDDC) (Chen et al., 2020), and the E-commerce ing the token-level sequences of the context in the Dialogue Corpus (ECD) (Zhang et al., 2018). To corpus. Hill et al. (2016) propose to predict the evaluate our model, we introduce two types of neighboring sentences as bag-of-words instead of tasks: the semantic retrieval (SR) task and the dialogue-based semantic textual similarity (D-STS) step-by-step decoding. Logeswaran and Lee (2018) perform sentence-level modeling by retrieving the task. Here we do not adopt the standard semantic ground-truth sentence from candidates under the textual similarity (STS) task (Cer et al., 2017) for two reasons: (1) As revealed in (Zhang et al., 2020), given context, achieving consistently better per"
2021.emnlp-main.218,N16-1030,0,0.0592786,"chanism used by Kiperwasser and Goldberg (2016). As the biaffine attention is widely used in other tasks like NER (Yu et al., 2020a) and semantic role labeling (Li et al., 2019b), we propose to use it for the entity relation extraction task in this work. 3 Entity Relation Extraction as Dependency Parsing While encoding VRDs, previous works take Both semantic entity relation extraction and depenentity labeling task as sequence labeling and re- dency parsing tasks aim to decide whether there implement the named entity recognition (NER) exists relation between two entities/words and asframework (Lample et al., 2016) but ignore layout sume that links always point from key/head unit information. Then, many works introduce a GCN- to value/modifier unit shown in Figure 1. Therebased module to encode layout information and fore, we can draw lessons from the dependency combine textual and visual information together parsing exploration as it has been studied for sev(Liu et al., 2019a; Yu et al., 2020b; Wei et al., 2020; eral decades and achieved great progress. BiCarbonell et al., 2021). In the GCN module, Liu affine parser, a strong model in dependency parsing, et al. (2019a); Yu et al. (2020b) take layout fe"
2021.emnlp-main.218,2021.acl-long.201,0,0.0654113,"Missing"
2021.emnlp-main.218,2020.coling-main.82,0,0.0990775,"Missing"
2021.emnlp-main.218,2020.acl-main.577,0,0.304298,"to the in-house customs data, achieving reliable performance in the production setting. 1 Introduction In real-life scenarios, there are many types of visually rich documents (VRDs), such as invoices, questionnaire forms, declaration materials and so on. These documents contain abundant layout information which helps us to understand the content while texts alone are not enough. In recent years, many works focus on how to extract key information from VRDs based on the results of OCR (Optical Character Recognition), which recognizes bounding boxes and texts within the boxes (Liu et al., 2019a; Yu et al., 2020b). Each bounding box contains 1) a group of words that belong together ∗ Corresponding author. The author’s contributions were carried out while at Alibaba Group. His current affiliation is Vipshop (China) Co., Ltd. from a semantic and spatial standpoint and 2) visual features such as layout, tabular structure and font size of the boxes in the document. We call such bounding boxes and texts within the boxes semantic entities1 , and each entity contains the word group and layout coordinates2 . Key information extraction (KIE) is such a task to analyze visually rich documents, which usually con"
2021.emnlp-main.218,N19-2005,0,0.347814,"l has been applied to the in-house customs data, achieving reliable performance in the production setting. 1 Introduction In real-life scenarios, there are many types of visually rich documents (VRDs), such as invoices, questionnaire forms, declaration materials and so on. These documents contain abundant layout information which helps us to understand the content while texts alone are not enough. In recent years, many works focus on how to extract key information from VRDs based on the results of OCR (Optical Character Recognition), which recognizes bounding boxes and texts within the boxes (Liu et al., 2019a; Yu et al., 2020b). Each bounding box contains 1) a group of words that belong together ∗ Corresponding author. The author’s contributions were carried out while at Alibaba Group. His current affiliation is Vipshop (China) Co., Ltd. from a semantic and spatial standpoint and 2) visual features such as layout, tabular structure and font size of the boxes in the document. We call such bounding boxes and texts within the boxes semantic entities1 , and each entity contains the word group and layout coordinates2 . Key information extraction (KIE) is such a task to analyze visually rich documents,"
2021.emnlp-main.218,2021.ccl-1.108,0,0.0217989,"Missing"
2021.emnlp-main.218,2020.acl-main.580,0,0.0138516,"u et al., 2019a; Yu et al., 2020b). • We apply our model to the real-world customs data with different layouts and achieve high performance in the production setting. 2 Related Work Visually rich document understanding includes many tasks, such as layout recognization (Zhong et al., 2019b; Li et al., 2020), table detection and recognition (Li et al., 2019a; Zhong et al., • At the relation scorer layer, we extract rela- 2019a) and key information extraction (Grali´nski tive position features between entities accord- et al., 2020; Guo et al., 2019; Huang et al., 2019; G. Jaume and Thiran, 2019; Majumder et al., 2020). ing to their coordinates. Our paper focuses on the key information extracApart from the above, inspired by the joint POS tion task which contains two subtasks, entity latagging and dependency parsing model (Nguyen beling and relation extraction. The former subtask 2760 Figure 2: The architecture of our proposed entity relation extraction model (left) and the biaffine parser model (right). tags entities with predefined labels, such as Task 3 on the SROIE data released by Huang et al. (2019), while the latter discovers relations between entities, such as Subtask C(3) on the FUNSD data (G. Jaum"
2021.emnlp-main.218,K18-2008,0,0.0418932,"Missing"
2021.emnlp-main.430,D19-1371,0,0.0331371,"Missing"
2021.emnlp-main.430,2020.acl-main.194,0,0.251997,"(Lowell et al., 2020), Sahin ¸ and Steedman (2019); Dai and Adel (2020) also investigated on randomly swapping tokens or text-spans in the input sequence as augmentation. However, such methods may be problematic for languages that rarely have inflectional morphemes, such as English, where words follow strict ordering (Sahin ¸ and Steedman, 2019). Therefore, we are not considering swap-based methods in our experiments. Other semi-supervised approaches for NER include CVT (Clark et al., 2018) which regularizes model predictions to be invariant when masking-out parts of the input data. Recently, Chen et al. (2020b) proposed an adapted version of virtual adversarial training with CRF, outperforming CVT on NER tasks. In the experiments, we show that our method can achieve better performance than both token replacement and SeqVAT in low-resource scenarios. 3 3.1 Methodology The NER model Algorithm 1 Computing the normalization term in CRF. Input: Assuming T > 1, l ∈ RT ×N , A ∈ RN ×N and s ∈ RN . Output: N M (l, A, s) as in eq (2). Initialization: Let l[t, :] be the tth row of l. Reshape s ∈ R1×N . Initialize variable p as p = s + l[1, :]. for t=2,· · · , T do p = logColumnSum(exp ( pT 1TN + A + 1N l[t,"
2021.emnlp-main.430,2020.repl4nlp-1.1,0,0.0466646,"Missing"
2021.emnlp-main.430,D18-1217,0,0.130569,"onsuming and more expensive to obtain than as a means for higher quality data augmentation for the sequence level annotations commonly used unsupervised consistency training in NER. Comfor classification tasks. Due to the scarcity of pared with token replacement, the key difficulty of labeled data, various semi-supervised approaches using paraphrasing is that the alignment of tokens have been investigated for training in low-resource between the original and paraphrased sequence is scenarios, i.e., only a small amount of labeled data unclear. However, since paraphrasing does not is available (Clark et al., 2018; Lowell et al., 2020). change the substance of the text, we can expect a Unsupervised consistency training is a common paraphrase to contain the same entities as in the approach to semi-supervised learning in NER. It en- original sequence. So motivated, instead of relying courages prediction consistency between the origi- on token-level consistency, we encourage consisnal and the augmented examples, by leveraging the tency on the occurrence of entities between predicavailability of a larger amount of unlabeled data. tions on the original and paraphrased sequences. Recently, Xie et al. (2019)"
2021.emnlp-main.430,I17-2016,0,0.0384041,"Missing"
2021.emnlp-main.430,2020.coling-main.343,0,0.0352483,"e variety of classification tasks tal results show that our method outperforms token with only tens or hundreds of labeled examples, replacement and other semi-supervised learning and even sometimes matching the performance approaches when annotations are scarce. 5303 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5303–5308 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 1: Illustration of our NER model. 2 Related Work In addition to token replacement discussed above (Lowell et al., 2020), Sahin ¸ and Steedman (2019); Dai and Adel (2020) also investigated on randomly swapping tokens or text-spans in the input sequence as augmentation. However, such methods may be problematic for languages that rarely have inflectional morphemes, such as English, where words follow strict ordering (Sahin ¸ and Steedman, 2019). Therefore, we are not considering swap-based methods in our experiments. Other semi-supervised approaches for NER include CVT (Clark et al., 2018) which regularizes model predictions to be invariant when masking-out parts of the input data. Recently, Chen et al. (2020b) proposed an adapted version of virtual adversarial"
2021.emnlp-main.430,W18-6125,0,0.0274706,"Missing"
2021.emnlp-main.430,N16-1030,0,0.0618845,"Missing"
2021.emnlp-main.796,P15-1040,0,0.147002,"human refinement. Second, we investigate the performance of unsupervised cross-lingual transfer for Chinese ORL based on the annotated corpus. We investigate the Contextual Parameter Generator Networks (PGN) in multilingual BERT with Adapter (known as parameter efficient in learning) method (Üstün et al., 2020) (we call it PGNAdapter) and discover the complementarity of the model transfer and corpus translation methods. We conduct experiments on the newly constructed Chinese dataset to evaluate our methods, together with the English MPQA corpus (Wiebe et al., 2005) and the Portuguese dataset (Almeida et al., 2015a) for cross-lingual transfer. We observe that for the unsupervised cross-lingual transfer from the English corpus, the translation-based method is better than the model transfer, and their combination leads to further improvements. Although the scale of the Portuguese corpus is much smaller, adding it into the multilingual transfer still outperforms the bilingual counterpart. To summarize, in this paper, we have the following contributions: • We manually translate and annotate a Chinese fine-grained ORL corpus for research purposes, especially for the cross-lingual ORL study. • We conduct cro"
2021.emnlp-main.796,2020.acl-main.297,1,0.787448,"and cons of the different approaches. (2012)’s semi-Markov CRF model outperforms the standard CRF, and Irsoy and Cardie (2014) and Liu et al. (2015) use recurrent neural network for opinion mining. Johansson and Moschitti (2013) and Katiyar and Cardie (2016a) propose joint models for opinion expressions, holders and targets. Opinion Role Labeling As for ORL, Marasovi´c and Frank (2018) exploit multi-task learning about how to use SRL information to improve ORL scores. Zhang et al. (2019a) utilize semantic role labeling to enhance ORL, where three different integrating approaches are compared. Bo et al. (2020) propose a dependency-based graph convolutional networks to enhance ORL with syntax information. All these studies focus on the English ORL by using supervised models, assuming that a training corpus is already available. In this work, we investigate Chinese ORL, building a benchmark dataset for Chinese manually and then studying unsupervised cross-lingual transferring for the task. Cross-Lingual Transfer Learning Crosslingual transfer learning has been extensively applied in NLP, including sentiment classification (Zhou et al., 2016), POS tagging (Täckström et al., 2013; Wisniewski et al., 20"
2021.emnlp-main.796,W06-1651,0,0.0788592,"nsfers establish a foundation for the future research of this task1 . [Chen]holder [voiced his condolences]expression to [the families]target . Figure 1: An example of fine-grained opinion mining. targets (Marasovi´c and Frank, 2017; Zhang et al., 2019a) when the expressions are given. Most of the previous researches focus on the English ORL, benefiting from the benchmark MPQA dataset (Wiebe et al., 2005) which includes spanbased annotations of opinion expressions, holders and targets. The task is commonly solved by sequence labeling models with the BIO conversion scheme (Kim and Hovy, 2006b; Choi et al., 2006; Yang and Cardie, 2013; Johansson and Moschitti, 2013). Recently, neural BiLSTM-CRF models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et al., 2011; Irsoy an"
2021.emnlp-main.796,H05-1045,0,0.0741382,"ds with BERT. About the Adapter 2 Related Work method which a pre-trained network added between the transformer encoder layer, there are many Fine-Grained Opinion Mining There have studies on using adapter modules (Rebuffi et al., been a number of studies in fine-grained opinion mining (Wilson et al., 2009; Qiu et al., 2011; Wie- 2018; Stickland and Murray, 2019; Houlsby et al., 2019). PGN is first proposed by Platanios et al. gand et al., 2016). Kim and Hovy (2006a) exploit a semantic role labeller to extract opinion hold- (2018) for universal neural machine translation task. ers and topics. Choi et al. (2005) and Breck et al. And Üstün et al. (2020) integrated that two methods above in dependency parsing which inspired (2007) model the task by sequence labeling with us to merge this idea into our ORL task. CRF to discover opinion holders and recognize opinion expressions, respectively. Yang and Cardie The transfer method doesn’t work in Eger et al. 10140 Raw Translated Revision Raw Translated Revision The president had sidelined Masire after accusing him . 这位总统在指控马西尔之后退居二线。 在指控马西尔之后，总统把他排挤到了一边。 Russian guards seize 87 kg of heroin on Tajik-Afghan border . 俄罗斯警卫在塔吉克-阿富汗边境抓获87公斤海洛因。 俄罗斯士兵在塔吉克-阿富汗边境缴"
2021.emnlp-main.796,D19-1056,0,0.0258772,"ataset for the Portuguese language. and it aims to discover useful structural informaUnsupervised cross-lingual transfer (Xu et al., tion of user opinions from unstructured text, which 2018) is one promising way to address the low is the relation between expression and entities such resource problem for ORL. Under the neural setas Who expressed what kind of sentiment towards ting, there are two representative categories of what?. The EXPRESSION conveys attitudes includ- methods: model transfer (McDonald et al., 2013; ing sentiments, agreements, beliefs, or intentions Swayamdipta et al., 2016; Daza and Frank, 2019) (e.g., voiced his condolences in Figure 1); the enti- and corpus translation (Zhang et al., 2019b). The ties consist of the HOLDER who expresses the opin- model transfer trains a model on a resource-rich lanion (e.g., Chen.) and the TARGET which the opin- guage by using only language-independent features ion is expressed to (e.g., the families) (Breck et al., such as multilingual BERT (Devlin et al., 2018; 2007; Yang and Cardie, 2012; Katiyar and Cardie, Pires et al., 2019) and then apply it to the target 2016a). Here we focus on the opinion role labeling language. The corpus translation appr"
2021.emnlp-main.796,K15-1012,0,0.0265524,"ese manually and then studying unsupervised cross-lingual transferring for the task. Cross-Lingual Transfer Learning Crosslingual transfer learning has been extensively applied in NLP, including sentiment classification (Zhou et al., 2016), POS tagging (Täckström et al., 2013; Wisniewski et al., 2014; Kim et al., 2017), named entity recognition (Zirikly and Hagiwara, 2015), semantic role labeling (Fei et al., 2020), and dependency parsing (McDonald et al., 2011; Tiedemann et al., 2014; Guo et al., 2016; Zhang et al., 2019b). Unsupervised cross-lingual transferring has received great interest (Duong et al., 2015; Xu et al., 2018), which is our major focus. The work of Zhang et al. (2019b) is mostly related to our study, which applies model transferring and corpus translation to dependency parsing. Our work focuses on ORL, applying the two approaches for the Chinese language. PGN-Adapter PGN-Adapter is used for merging different languages to same space by Adapter and PGN methods with BERT. About the Adapter 2 Related Work method which a pre-trained network added between the transformer encoder layer, there are many Fine-Grained Opinion Mining There have studies on using adapter modules (Rebuffi et al."
2021.emnlp-main.796,N13-1073,0,0.0337349,"t these issues. On the other hand, the automatic sentences may not match the style of native speakers, and we let our annotators rewrite these sentences. Table 1 shows two examples of the two conditions, respectively. Opinion Projection Third, we project all opinions (expressions, holders and targets) from the English sentence into its Chinese translation. Before the projection, we use the Stanford Segmentor tool for word segmentation3 . The overall projection is supported by automatic word alignments, which can be produced by using a word-alignment tool. Here we exploit the fast-align tool4 (Dyer et al., 2013) to calculate the alignment probabilities. Figure 2 shows an example to illustrate the projection process. Concretely, given an EnglishChinese sentence pair (e1 · · · en , c1 · · · cm ) and its English-to-Chinese alignment probabilities a(cj |ei ), the projection is performed as follows: (1) We incrementally obtain the text spans in the Chinese sentences for the opinion expressions as well as their holders and targets in the English sentence. (2) For each word ei in the English sentence, we find its corresponding word cpi in the Chinese sentence by using pi = arg maxj a(cj |ei ), resulting in"
2021.emnlp-main.796,C18-1071,0,0.0441554,"Missing"
2021.emnlp-main.796,2020.acl-main.627,1,0.858068,"Missing"
2021.emnlp-main.796,P81-1022,0,0.2155,"Missing"
2021.emnlp-main.796,D14-1080,0,0.026599,"nsfer still outperforms the bilingual counterpart. To summarize, in this paper, we have the following contributions: • We manually translate and annotate a Chinese fine-grained ORL corpus for research purposes, especially for the cross-lingual ORL study. • We conduct cross-lingual ORL (to Chinese) through unsupervised model transfer and corpus translation with PGN-Adapter, setting up strong baselines for future research. • We perform extensive experiments and analyses to demonstrate the pros and cons of the different approaches. (2012)’s semi-Markov CRF model outperforms the standard CRF, and Irsoy and Cardie (2014) and Liu et al. (2015) use recurrent neural network for opinion mining. Johansson and Moschitti (2013) and Katiyar and Cardie (2016a) propose joint models for opinion expressions, holders and targets. Opinion Role Labeling As for ORL, Marasovi´c and Frank (2018) exploit multi-task learning about how to use SRL information to improve ORL scores. Zhang et al. (2019a) utilize semantic role labeling to enhance ORL, where three different integrating approaches are compared. Bo et al. (2020) propose a dependency-based graph convolutional networks to enhance ORL with syntax information. All these stu"
2021.emnlp-main.796,J13-3002,0,0.106625,"re research of this task1 . [Chen]holder [voiced his condolences]expression to [the families]target . Figure 1: An example of fine-grained opinion mining. targets (Marasovi´c and Frank, 2017; Zhang et al., 2019a) when the expressions are given. Most of the previous researches focus on the English ORL, benefiting from the benchmark MPQA dataset (Wiebe et al., 2005) which includes spanbased annotations of opinion expressions, holders and targets. The task is commonly solved by sequence labeling models with the BIO conversion scheme (Kim and Hovy, 2006b; Choi et al., 2006; Yang and Cardie, 2013; Johansson and Moschitti, 2013). Recently, neural BiLSTM-CRF models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et al., 2011; Irsoy and et al. (2015a), which has annotated a small-scale Car"
2021.emnlp-main.796,P16-1087,0,0.272295,"f fine-grained opinion mining. targets (Marasovi´c and Frank, 2017; Zhang et al., 2019a) when the expressions are given. Most of the previous researches focus on the English ORL, benefiting from the benchmark MPQA dataset (Wiebe et al., 2005) which includes spanbased annotations of opinion expressions, holders and targets. The task is commonly solved by sequence labeling models with the BIO conversion scheme (Kim and Hovy, 2006b; Choi et al., 2006; Yang and Cardie, 2013; Johansson and Moschitti, 2013). Recently, neural BiLSTM-CRF models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et al., 2011; Irsoy and et al. (2015a), which has annotated a small-scale Cardie, 2014; Liu et al., 2015; Wiegand et al., 2016) dataset for the Portuguese language. and it aims to discover useful"
2021.emnlp-main.796,D17-1302,0,0.0200117,"pose a dependency-based graph convolutional networks to enhance ORL with syntax information. All these studies focus on the English ORL by using supervised models, assuming that a training corpus is already available. In this work, we investigate Chinese ORL, building a benchmark dataset for Chinese manually and then studying unsupervised cross-lingual transferring for the task. Cross-Lingual Transfer Learning Crosslingual transfer learning has been extensively applied in NLP, including sentiment classification (Zhou et al., 2016), POS tagging (Täckström et al., 2013; Wisniewski et al., 2014; Kim et al., 2017), named entity recognition (Zirikly and Hagiwara, 2015), semantic role labeling (Fei et al., 2020), and dependency parsing (McDonald et al., 2011; Tiedemann et al., 2014; Guo et al., 2016; Zhang et al., 2019b). Unsupervised cross-lingual transferring has received great interest (Duong et al., 2015; Xu et al., 2018), which is our major focus. The work of Zhang et al. (2019b) is mostly related to our study, which applies model transferring and corpus translation to dependency parsing. Our work focuses on ORL, applying the two approaches for the Chinese language. PGN-Adapter PGN-Adapter is used f"
2021.emnlp-main.796,W06-0301,0,0.350489,"and multilingual transfers establish a foundation for the future research of this task1 . [Chen]holder [voiced his condolences]expression to [the families]target . Figure 1: An example of fine-grained opinion mining. targets (Marasovi´c and Frank, 2017; Zhang et al., 2019a) when the expressions are given. Most of the previous researches focus on the English ORL, benefiting from the benchmark MPQA dataset (Wiebe et al., 2005) which includes spanbased annotations of opinion expressions, holders and targets. The task is commonly solved by sequence labeling models with the BIO conversion scheme (Kim and Hovy, 2006b; Choi et al., 2006; Yang and Cardie, 2013; Johansson and Moschitti, 2013). Recently, neural BiLSTM-CRF models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et"
2021.emnlp-main.796,P19-1314,0,0.0712203,": (1) sentence translation, (2) manual revision, (3) opinion projection, and (4) manual correction. The first and third steps formalize into automatic corpus translation, which has been used as one approach for unsupervised cross-lingual transfer, and the second and fourth steps are used to ensure the final quality. The whole construction is conducted at the sentence-level. Sentence Translation Neural machine translation (NMT) has achieved state-of-the-art performances for a range of language pairs (Vaswani et al., 2017). In particular, the state-of-the-art NMT can reach a BLEU score over 45 (Li et al., 2019). Thus it is applicable to use NMT for automatic sentence translation. Here we first translate all the English sentences of the MPQA dataset into Chinese by using the google translator2 automatically. Manual Revision Next, we let several native speakers check the translation quality, and make revisions to the imperfect translations. There can be two types of revisions. On the one hand, the translated sentences may have errors, and human 2 intervention is required to correct these issues. On the other hand, the automatic sentences may not match the style of native speakers, and we let our annot"
2021.emnlp-main.796,E17-2002,0,0.0515297,"Missing"
2021.emnlp-main.796,D15-1168,0,0.145657,"neural BiLSTM-CRF models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et al., 2011; Irsoy and et al. (2015a), which has annotated a small-scale Cardie, 2014; Liu et al., 2015; Wiegand et al., 2016) dataset for the Portuguese language. and it aims to discover useful structural informaUnsupervised cross-lingual transfer (Xu et al., tion of user opinions from unstructured text, which 2018) is one promising way to address the low is the relation between expression and entities such resource problem for ORL. Under the neural setas Who expressed what kind of sentiment towards ting, there are two representative categories of what?. The EXPRESSION conveys attitudes includ- methods: model transfer (McDonald et al., 2013; ing sentiments, agreements, beliefs, or intentions S"
2021.emnlp-main.796,L18-1236,0,0.040109,"Missing"
2021.emnlp-main.796,N18-1054,0,0.0350211,"Missing"
2021.emnlp-main.796,P13-2017,0,0.0337103,". (2015a), which has annotated a small-scale Cardie, 2014; Liu et al., 2015; Wiegand et al., 2016) dataset for the Portuguese language. and it aims to discover useful structural informaUnsupervised cross-lingual transfer (Xu et al., tion of user opinions from unstructured text, which 2018) is one promising way to address the low is the relation between expression and entities such resource problem for ORL. Under the neural setas Who expressed what kind of sentiment towards ting, there are two representative categories of what?. The EXPRESSION conveys attitudes includ- methods: model transfer (McDonald et al., 2013; ing sentiments, agreements, beliefs, or intentions Swayamdipta et al., 2016; Daza and Frank, 2019) (e.g., voiced his condolences in Figure 1); the enti- and corpus translation (Zhang et al., 2019b). The ties consist of the HOLDER who expresses the opin- model transfer trains a model on a resource-rich lanion (e.g., Chen.) and the TARGET which the opin- guage by using only language-independent features ion is expressed to (e.g., the families) (Breck et al., such as multilingual BERT (Devlin et al., 2018; 2007; Yang and Cardie, 2012; Katiyar and Cardie, Pires et al., 2019) and then apply it to"
2021.emnlp-main.796,D11-1006,0,0.0480317,"ing supervised models, assuming that a training corpus is already available. In this work, we investigate Chinese ORL, building a benchmark dataset for Chinese manually and then studying unsupervised cross-lingual transferring for the task. Cross-Lingual Transfer Learning Crosslingual transfer learning has been extensively applied in NLP, including sentiment classification (Zhou et al., 2016), POS tagging (Täckström et al., 2013; Wisniewski et al., 2014; Kim et al., 2017), named entity recognition (Zirikly and Hagiwara, 2015), semantic role labeling (Fei et al., 2020), and dependency parsing (McDonald et al., 2011; Tiedemann et al., 2014; Guo et al., 2016; Zhang et al., 2019b). Unsupervised cross-lingual transferring has received great interest (Duong et al., 2015; Xu et al., 2018), which is our major focus. The work of Zhang et al. (2019b) is mostly related to our study, which applies model transferring and corpus translation to dependency parsing. Our work focuses on ORL, applying the two approaches for the Chinese language. PGN-Adapter PGN-Adapter is used for merging different languages to same space by Adapter and PGN methods with BERT. About the Adapter 2 Related Work method which a pre-trained ne"
2021.emnlp-main.796,P19-1493,0,0.0375505,"Missing"
2021.emnlp-main.796,D18-1039,0,0.0558953,"Missing"
2021.emnlp-main.796,J11-1002,0,0.445765,", 2006b; Choi et al., 2006; Yang and Cardie, 2013; Johansson and Moschitti, 2013). Recently, neural BiLSTM-CRF models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et al., 2011; Irsoy and et al. (2015a), which has annotated a small-scale Cardie, 2014; Liu et al., 2015; Wiegand et al., 2016) dataset for the Portuguese language. and it aims to discover useful structural informaUnsupervised cross-lingual transfer (Xu et al., tion of user opinions from unstructured text, which 2018) is one promising way to address the low is the relation between expression and entities such resource problem for ORL. Under the neural setas Who expressed what kind of sentiment towards ting, there are two representative categories of what?. The EXPRESSION conveys attitudes includ- methods:"
2021.emnlp-main.796,K16-1019,0,0.0122267,"5; Wiegand et al., 2016) dataset for the Portuguese language. and it aims to discover useful structural informaUnsupervised cross-lingual transfer (Xu et al., tion of user opinions from unstructured text, which 2018) is one promising way to address the low is the relation between expression and entities such resource problem for ORL. Under the neural setas Who expressed what kind of sentiment towards ting, there are two representative categories of what?. The EXPRESSION conveys attitudes includ- methods: model transfer (McDonald et al., 2013; ing sentiments, agreements, beliefs, or intentions Swayamdipta et al., 2016; Daza and Frank, 2019) (e.g., voiced his condolences in Figure 1); the enti- and corpus translation (Zhang et al., 2019b). The ties consist of the HOLDER who expresses the opin- model transfer trains a model on a resource-rich lanion (e.g., Chen.) and the TARGET which the opin- guage by using only language-independent features ion is expressed to (e.g., the families) (Breck et al., such as multilingual BERT (Devlin et al., 2018; 2007; Yang and Cardie, 2012; Katiyar and Cardie, Pires et al., 2019) and then apply it to the target 2016a). Here we focus on the opinion role labeling language. The"
2021.emnlp-main.796,Q13-1001,0,0.0127775,"ing approaches are compared. Bo et al. (2020) propose a dependency-based graph convolutional networks to enhance ORL with syntax information. All these studies focus on the English ORL by using supervised models, assuming that a training corpus is already available. In this work, we investigate Chinese ORL, building a benchmark dataset for Chinese manually and then studying unsupervised cross-lingual transferring for the task. Cross-Lingual Transfer Learning Crosslingual transfer learning has been extensively applied in NLP, including sentiment classification (Zhou et al., 2016), POS tagging (Täckström et al., 2013; Wisniewski et al., 2014; Kim et al., 2017), named entity recognition (Zirikly and Hagiwara, 2015), semantic role labeling (Fei et al., 2020), and dependency parsing (McDonald et al., 2011; Tiedemann et al., 2014; Guo et al., 2016; Zhang et al., 2019b). Unsupervised cross-lingual transferring has received great interest (Duong et al., 2015; Xu et al., 2018), which is our major focus. The work of Zhang et al. (2019b) is mostly related to our study, which applies model transferring and corpus translation to dependency parsing. Our work focuses on ORL, applying the two approaches for the Chinese"
2021.emnlp-main.796,W14-1614,0,0.0259925,"rforms the bilingual counterpart. To summarize, in this paper, we have the following contributions: • We manually translate and annotate a Chinese fine-grained ORL corpus for research purposes, especially for the cross-lingual ORL study. • We conduct cross-lingual ORL (to Chinese) through unsupervised model transfer and corpus translation with PGN-Adapter, setting up strong baselines for future research. • We perform extensive experiments and analyses to demonstrate the pros and cons of the different approaches. (2012)’s semi-Markov CRF model outperforms the standard CRF, and Irsoy and Cardie (2014) and Liu et al. (2015) use recurrent neural network for opinion mining. Johansson and Moschitti (2013) and Katiyar and Cardie (2016a) propose joint models for opinion expressions, holders and targets. Opinion Role Labeling As for ORL, Marasovi´c and Frank (2018) exploit multi-task learning about how to use SRL information to improve ORL scores. Zhang et al. (2019a) utilize semantic role labeling to enhance ORL, where three different integrating approaches are compared. Bo et al. (2020) propose a dependency-based graph convolutional networks to enhance ORL with syntax information. All these stu"
2021.emnlp-main.796,2020.emnlp-main.180,0,0.025557,"Missing"
2021.emnlp-main.796,N16-1094,0,0.0190372,"models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et al., 2011; Irsoy and et al. (2015a), which has annotated a small-scale Cardie, 2014; Liu et al., 2015; Wiegand et al., 2016) dataset for the Portuguese language. and it aims to discover useful structural informaUnsupervised cross-lingual transfer (Xu et al., tion of user opinions from unstructured text, which 2018) is one promising way to address the low is the relation between expression and entities such resource problem for ORL. Under the neural setas Who expressed what kind of sentiment towards ting, there are two representative categories of what?. The EXPRESSION conveys attitudes includ- methods: model transfer (McDonald et al., 2013; ing sentiments, agreements, beliefs, or intentions Swayamdipta et al., 2016"
2021.emnlp-main.796,D18-1268,0,0.0657663,"Missing"
2021.emnlp-main.796,D12-1122,0,0.0256885,"EXPRESSION conveys attitudes includ- methods: model transfer (McDonald et al., 2013; ing sentiments, agreements, beliefs, or intentions Swayamdipta et al., 2016; Daza and Frank, 2019) (e.g., voiced his condolences in Figure 1); the enti- and corpus translation (Zhang et al., 2019b). The ties consist of the HOLDER who expresses the opin- model transfer trains a model on a resource-rich lanion (e.g., Chen.) and the TARGET which the opin- guage by using only language-independent features ion is expressed to (e.g., the families) (Breck et al., such as multilingual BERT (Devlin et al., 2018; 2007; Yang and Cardie, 2012; Katiyar and Cardie, Pires et al., 2019) and then apply it to the target 2016a). Here we focus on the opinion role labeling language. The corpus translation approach firstly (ORL) task which is to identify opinion holders and obtains parallel corpora through either human or ∗ Corresponding author. machine translation and then projects the annota1 We release the code and way of obtaining Chitions from the source language to the target side. nese dataset at https://github.com/zenRRan/ ChineseORL-with-Corpus-Translation. In this work, we present the first study of the 10139 Proceedings of the 20"
2021.emnlp-main.796,P13-1161,0,0.0193987,"foundation for the future research of this task1 . [Chen]holder [voiced his condolences]expression to [the families]target . Figure 1: An example of fine-grained opinion mining. targets (Marasovi´c and Frank, 2017; Zhang et al., 2019a) when the expressions are given. Most of the previous researches focus on the English ORL, benefiting from the benchmark MPQA dataset (Wiebe et al., 2005) which includes spanbased annotations of opinion expressions, holders and targets. The task is commonly solved by sequence labeling models with the BIO conversion scheme (Kim and Hovy, 2006b; Choi et al., 2006; Yang and Cardie, 2013; Johansson and Moschitti, 2013). Recently, neural BiLSTM-CRF models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et al., 2011; Irsoy and et al. (2015a), which"
2021.emnlp-main.796,N19-1066,1,0.866988,"Missing"
2021.emnlp-main.796,D19-1092,1,0.899403,"Missing"
2021.emnlp-main.796,P15-2064,0,0.0178979,"etworks to enhance ORL with syntax information. All these studies focus on the English ORL by using supervised models, assuming that a training corpus is already available. In this work, we investigate Chinese ORL, building a benchmark dataset for Chinese manually and then studying unsupervised cross-lingual transferring for the task. Cross-Lingual Transfer Learning Crosslingual transfer learning has been extensively applied in NLP, including sentiment classification (Zhou et al., 2016), POS tagging (Täckström et al., 2013; Wisniewski et al., 2014; Kim et al., 2017), named entity recognition (Zirikly and Hagiwara, 2015), semantic role labeling (Fei et al., 2020), and dependency parsing (McDonald et al., 2011; Tiedemann et al., 2014; Guo et al., 2016; Zhang et al., 2019b). Unsupervised cross-lingual transferring has received great interest (Duong et al., 2015; Xu et al., 2018), which is our major focus. The work of Zhang et al. (2019b) is mostly related to our study, which applies model transferring and corpus translation to dependency parsing. Our work focuses on ORL, applying the two approaches for the Chinese language. PGN-Adapter PGN-Adapter is used for merging different languages to same space by Adapter"
2021.emnlp-main.796,wilson-2008-annotating,0,0.218516,"nce MT system enhances the translation-based approach. 2) Projection strategy is also different from ours (Section 5.1). We choose to project the non-cross labels only, in order to ensure the mapping quality. 3) In addition, the more advanced methods like PGN, Adapter and BERT also play a significant role in cross-lingual tasks. 3 The Construction of Chinese Dataset We manually construct a Chinese ORL dataset to facilitate our research. In order to reduce the overall cost, we exploit corpus translation to assist the construction process, converting the English MPQA corpus (Wiebe et al., 2005; Wilson, 2008) into Chinese. The conversion contains the following four steps by order: (1) sentence translation, (2) manual revision, (3) opinion projection, and (4) manual correction. The first and third steps formalize into automatic corpus translation, which has been used as one approach for unsupervised cross-lingual transfer, and the second and fourth steps are used to ensure the final quality. The whole construction is conducted at the sentence-level. Sentence Translation Neural machine translation (NMT) has achieved state-of-the-art performances for a range of language pairs (Vaswani et al., 2017)."
2021.emnlp-main.796,J09-3003,0,0.173216,"scheme (Kim and Hovy, 2006b; Choi et al., 2006; Yang and Cardie, 2013; Johansson and Moschitti, 2013). Recently, neural BiLSTM-CRF models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et al., 2011; Irsoy and et al. (2015a), which has annotated a small-scale Cardie, 2014; Liu et al., 2015; Wiegand et al., 2016) dataset for the Portuguese language. and it aims to discover useful structural informaUnsupervised cross-lingual transfer (Xu et al., tion of user opinions from unstructured text, which 2018) is one promising way to address the low is the relation between expression and entities such resource problem for ORL. Under the neural setas Who expressed what kind of sentiment towards ting, there are two representative categories of what?. The EXPRESSION conveys attitude"
2021.emnlp-main.796,D14-1187,0,0.0232509,"red. Bo et al. (2020) propose a dependency-based graph convolutional networks to enhance ORL with syntax information. All these studies focus on the English ORL by using supervised models, assuming that a training corpus is already available. In this work, we investigate Chinese ORL, building a benchmark dataset for Chinese manually and then studying unsupervised cross-lingual transferring for the task. Cross-Lingual Transfer Learning Crosslingual transfer learning has been extensively applied in NLP, including sentiment classification (Zhou et al., 2016), POS tagging (Täckström et al., 2013; Wisniewski et al., 2014; Kim et al., 2017), named entity recognition (Zirikly and Hagiwara, 2015), semantic role labeling (Fei et al., 2020), and dependency parsing (McDonald et al., 2011; Tiedemann et al., 2014; Guo et al., 2016; Zhang et al., 2019b). Unsupervised cross-lingual transferring has received great interest (Duong et al., 2015; Xu et al., 2018), which is our major focus. The work of Zhang et al. (2019b) is mostly related to our study, which applies model transferring and corpus translation to dependency parsing. Our work focuses on ORL, applying the two approaches for the Chinese language. PGN-Adapter PG"
2021.emnlp-tutorials.6,D17-1304,1,0.866418,"Missing"
2021.emnlp-tutorials.6,2020.acl-main.153,0,0.0183686,"et al., 2020) and so on. On the other hand, linguistic prior knowledge (i.e. alignment, bilingual lexicon, phrase table, and knowledge graphs) to alleviate the problem of inadequacy target translations which are caused by the language model property of the encoderdecoder framework (Feng et al., 2017; Zhang et al., 2017; Zhao et al., 2018a; Wang et al., 2018b). Moreover, linguistic differences between the source language and target language can learn natural language representations that are easy to be understood by the translation model, for example, word order difference (Chen et al., 2019; Ding et al., 2020), morphological differences (Ji et al., 2019) and so on. Meanwhile, linguistic shared feature between the source language and target language can also enhance the understanding and generation of natural language in MT, for example, shared words (Artetxe et al., 2018), image information (Yin et al., 2020), video information (Wang et al., 2020) and so on. 2 Relevance to the Computational Linguistics Community The topics included in this tutorial, i.e., syntax parsing, SRL, and MT, are all the classic ones to the entire NLP/CL community. This tutorial is primarily towards researchers who have a b"
2021.emnlp-tutorials.6,P17-1044,0,0.0994261,"other researchers. 6 Diversity Considerations N/A 7 Specification of Any Prerequisites for the Attendees This tutorial is primarily aimed at researchers who have a basic understanding of NLP. 8 Presenters Small reading list • Deep Learning: Deep learning (LeCun et al., 2015) • Syntactic Parsing: Deep biaffine attention for neural dependency parsing (Dozat and Manning, 2016) and Constituency parsing with a self-attentive encoder (Kitaev and Klein, 2018). • SRL: Syntax for semantic role labeling, to be, or not to be (He et al., 2018b) and Deep semantic role labeling: What works and whats next (He et al., 2017b). • Machine Translation: Statistical machine translation (Koehn, 2009) and Neural machine translation by jointly learning to align and translate (Bahdanau et al., 2015). 29 conferences, such as CWMT, CCL, etc. He served as the area chairs of ICLR-2021 and NAACL2021. 3. Dr. Kehai Chen, Postdoctoral Researcher, Advanced Translation Technology Laboratory, National Institute of Information and Communications Technology (NICT), Japan khchen@nict.go.jp https://chenkehai.github.io His research focuses on linguistic-motivated machine translation (MT), a classic NLP task in AI. He has published more"
2021.emnlp-tutorials.6,N19-1205,0,0.0200961,"tion based transformer (Vaswani et al., 2017) has become new state-of-the-art architecture in NMT and gives a series of new state-of-the-art benchmarks (Bojar et al., 2018; Marie et al., 2018; Wang et al., 2018a; Marie et al., 2019). Syntax information has been shown that it can improve the performances of the recurrent neural network (RNN) based NMT on conditions (Eriguchi et al., 2016, 2017; Chen et al., 2017a; Li et al., 2017; Wu et al., 2017; Chen et al., 2017b, 2018). However, so far it has not been shown significantly widely useful in self-attention based NMT. There are only a few work (Ma et al., 2019) adopted the syntactic information into the positional embedding of Transformer. We will give a detailed analysis on this issue by surveying the key technique details. common sense, such as over/under-translation questions (Tu et al., 2016), troublesome words modeling (Zhao et al., 2018b) and so on; (2) to have some basic abilities of human translator, for example, word importance modeling (Chen et al., 2020), translation refinement (Song et al., 2020), structured information (Xu et al., 2020), diverse feature (Chen et al., 2020) and so on. On the other hand, linguistic prior knowledge (i.e. a"
2021.emnlp-tutorials.6,P18-1192,1,0.87376,"Missing"
2021.emnlp-tutorials.6,K17-1041,0,0.0300525,"Missing"
2021.emnlp-tutorials.6,W19-5330,1,0.835416,"Knight, 2001; Mi et al., 2008). In some scenarios, especially when the domain of the MT corpus is similar to the domain of the parsing corpus, the performance of tree based SMT is better than phrase based SMT (Koehn, 2009). For NMT, it so far achieves significant progress by using end-toend based structure since 2014 (Sutskever et al., 2014; Bahdanau et al., 2015). Recently, selfattention based transformer (Vaswani et al., 2017) has become new state-of-the-art architecture in NMT and gives a series of new state-of-the-art benchmarks (Bojar et al., 2018; Marie et al., 2018; Wang et al., 2018a; Marie et al., 2019). Syntax information has been shown that it can improve the performances of the recurrent neural network (RNN) based NMT on conditions (Eriguchi et al., 2016, 2017; Chen et al., 2017a; Li et al., 2017; Wu et al., 2017; Chen et al., 2017b, 2018). However, so far it has not been shown significantly widely useful in self-attention based NMT. There are only a few work (Ma et al., 2019) adopted the syntactic information into the positional embedding of Transformer. We will give a detailed analysis on this issue by surveying the key technique details. common sense, such as over/under-translation que"
2021.emnlp-tutorials.6,W18-6419,1,0.827852,"ds have been well developed (Yamada and Knight, 2001; Mi et al., 2008). In some scenarios, especially when the domain of the MT corpus is similar to the domain of the parsing corpus, the performance of tree based SMT is better than phrase based SMT (Koehn, 2009). For NMT, it so far achieves significant progress by using end-toend based structure since 2014 (Sutskever et al., 2014; Bahdanau et al., 2015). Recently, selfattention based transformer (Vaswani et al., 2017) has become new state-of-the-art architecture in NMT and gives a series of new state-of-the-art benchmarks (Bojar et al., 2018; Marie et al., 2018; Wang et al., 2018a; Marie et al., 2019). Syntax information has been shown that it can improve the performances of the recurrent neural network (RNN) based NMT on conditions (Eriguchi et al., 2016, 2017; Chen et al., 2017a; Li et al., 2017; Wu et al., 2017; Chen et al., 2017b, 2018). However, so far it has not been shown significantly widely useful in self-attention based NMT. There are only a few work (Ma et al., 2019) adopted the syntactic information into the positional embedding of Transformer. We will give a detailed analysis on this issue by surveying the key technique details. common"
2021.emnlp-tutorials.6,P17-1065,0,0.0143432,"). For NMT, it so far achieves significant progress by using end-toend based structure since 2014 (Sutskever et al., 2014; Bahdanau et al., 2015). Recently, selfattention based transformer (Vaswani et al., 2017) has become new state-of-the-art architecture in NMT and gives a series of new state-of-the-art benchmarks (Bojar et al., 2018; Marie et al., 2018; Wang et al., 2018a; Marie et al., 2019). Syntax information has been shown that it can improve the performances of the recurrent neural network (RNN) based NMT on conditions (Eriguchi et al., 2016, 2017; Chen et al., 2017a; Li et al., 2017; Wu et al., 2017; Chen et al., 2017b, 2018). However, so far it has not been shown significantly widely useful in self-attention based NMT. There are only a few work (Ma et al., 2019) adopted the syntactic information into the positional embedding of Transformer. We will give a detailed analysis on this issue by surveying the key technique details. common sense, such as over/under-translation questions (Tu et al., 2016), troublesome words modeling (Zhao et al., 2018b) and so on; (2) to have some basic abilities of human translator, for example, word importance modeling (Chen et al., 2020), translation refinem"
2021.emnlp-tutorials.6,P08-1023,0,0.0723227,"0” denotes mainly studies in zero/lowresource scenarios; “-” denotes negative or little impact. The mark in the rightmost column indicates whether it is overall effective when all marked factors to the left are combined. Syntax in MT also endures a methodology change from statistical machine translation (SMT) (Brown et al., 1993) to neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) as the task of SRL. For typical SMT, besides phrase based SMT (Och et al., 1999; Koehn et al., 2003), syntactic (tree) based methods have been well developed (Yamada and Knight, 2001; Mi et al., 2008). In some scenarios, especially when the domain of the MT corpus is similar to the domain of the parsing corpus, the performance of tree based SMT is better than phrase based SMT (Koehn, 2009). For NMT, it so far achieves significant progress by using end-toend based structure since 2014 (Sutskever et al., 2014; Bahdanau et al., 2015). Recently, selfattention based transformer (Vaswani et al., 2017) has become new state-of-the-art architecture in NMT and gives a series of new state-of-the-art benchmarks (Bojar et al., 2018; Marie et al., 2018; Wang et al., 2018a; Marie et al., 2019). Syntax in"
2021.emnlp-tutorials.6,2020.acl-main.37,0,0.0482381,"Missing"
2021.emnlp-tutorials.6,W99-0604,0,0.716643,"Missing"
2021.emnlp-tutorials.6,P01-1067,0,0.347375,"moderate contribution; “0” denotes mainly studies in zero/lowresource scenarios; “-” denotes negative or little impact. The mark in the rightmost column indicates whether it is overall effective when all marked factors to the left are combined. Syntax in MT also endures a methodology change from statistical machine translation (SMT) (Brown et al., 1993) to neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) as the task of SRL. For typical SMT, besides phrase based SMT (Och et al., 1999; Koehn et al., 2003), syntactic (tree) based methods have been well developed (Yamada and Knight, 2001; Mi et al., 2008). In some scenarios, especially when the domain of the MT corpus is similar to the domain of the parsing corpus, the performance of tree based SMT is better than phrase based SMT (Koehn, 2009). For NMT, it so far achieves significant progress by using end-toend based structure since 2014 (Sutskever et al., 2014; Bahdanau et al., 2015). Recently, selfattention based transformer (Vaswani et al., 2017) has become new state-of-the-art architecture in NMT and gives a series of new state-of-the-art benchmarks (Bojar et al., 2018; Marie et al., 2018; Wang et al., 2018a; Marie et al."
2021.emnlp-tutorials.6,J05-1004,0,0.426004,"Missing"
2021.emnlp-tutorials.6,2020.acl-main.273,0,0.0253252,"ng et al., 2017; Zhao et al., 2018a; Wang et al., 2018b). Moreover, linguistic differences between the source language and target language can learn natural language representations that are easy to be understood by the translation model, for example, word order difference (Chen et al., 2019; Ding et al., 2020), morphological differences (Ji et al., 2019) and so on. Meanwhile, linguistic shared feature between the source language and target language can also enhance the understanding and generation of natural language in MT, for example, shared words (Artetxe et al., 2018), image information (Yin et al., 2020), video information (Wang et al., 2020) and so on. 2 Relevance to the Computational Linguistics Community The topics included in this tutorial, i.e., syntax parsing, SRL, and MT, are all the classic ones to the entire NLP/CL community. This tutorial is primarily towards researchers who have a basic understanding of deep learning based NLP. We believe that this tutorial would help the audience more deeply understand the relationship between three classic NLP tasks, i.e., syntax parsing and SRL/MT. Linguistic in MT. In addition, we will investigate why linguistic cognition and prior knowledge ca"
2021.emnlp-tutorials.6,P17-1139,0,0.0115986,"Tu et al., 2016), troublesome words modeling (Zhao et al., 2018b) and so on; (2) to have some basic abilities of human translator, for example, word importance modeling (Chen et al., 2020), translation refinement (Song et al., 2020), structured information (Xu et al., 2020), diverse feature (Chen et al., 2020) and so on. On the other hand, linguistic prior knowledge (i.e. alignment, bilingual lexicon, phrase table, and knowledge graphs) to alleviate the problem of inadequacy target translations which are caused by the language model property of the encoderdecoder framework (Feng et al., 2017; Zhang et al., 2017; Zhao et al., 2018a; Wang et al., 2018b). Moreover, linguistic differences between the source language and target language can learn natural language representations that are easy to be understood by the translation model, for example, word order difference (Chen et al., 2019; Ding et al., 2020), morphological differences (Ji et al., 2019) and so on. Meanwhile, linguistic shared feature between the source language and target language can also enhance the understanding and generation of natural language in MT, for example, shared words (Artetxe et al., 2018), image information (Yin et al., 202"
2021.emnlp-tutorials.6,P16-1008,0,0.0123334,"information has been shown that it can improve the performances of the recurrent neural network (RNN) based NMT on conditions (Eriguchi et al., 2016, 2017; Chen et al., 2017a; Li et al., 2017; Wu et al., 2017; Chen et al., 2017b, 2018). However, so far it has not been shown significantly widely useful in self-attention based NMT. There are only a few work (Ma et al., 2019) adopted the syntactic information into the positional embedding of Transformer. We will give a detailed analysis on this issue by surveying the key technique details. common sense, such as over/under-translation questions (Tu et al., 2016), troublesome words modeling (Zhao et al., 2018b) and so on; (2) to have some basic abilities of human translator, for example, word importance modeling (Chen et al., 2020), translation refinement (Song et al., 2020), structured information (Xu et al., 2020), diverse feature (Chen et al., 2020) and so on. On the other hand, linguistic prior knowledge (i.e. alignment, bilingual lexicon, phrase table, and knowledge graphs) to alleviate the problem of inadequacy target translations which are caused by the language model property of the encoderdecoder framework (Feng et al., 2017; Zhang et al., 20"
2021.emnlp-tutorials.6,P15-1109,0,0.0900917,"Missing"
2021.findings-acl.70,2020.tacl-1.5,0,0.0222889,"n, and velocity, into one symbol and reduces duplicated position events. Although such MIDI-like approaches avoid 2.3 Masking Strategies in Pre-training Masking strategies play a key role in NLP pretraining. For example, BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) randomly mask some tokens in an input sequence and learn to predict the masked tokens. Furthermore, since adjacent tokens may form a word or a phrase, some works consider masking consecutive tokens. For example, MASS (Song et al., 2019) randomly masks a fragment of several consecutive tokens in the input, and SpanBERT (Joshi et al., 2020) randomly masks contiguous spans instead of tokens. However, symbolic music is different from language. First, symbolic music contains structural (e.g., bar, position) and diverse information (e.g., tempo, instrument, and pitch), while natural language can be regarded as homogeneous data, which only contains text. Second, music and language follow different rules. Specifically, the language rules include grammar and spelling, while the music rules include beat, chord, etc. Accordingly, the masking strategies for symbolic music need to be specifically designed; otherwise, it may limit the poten"
2021.findings-acl.70,2021.ccl-1.108,0,0.084385,"Missing"
2021.findings-emnlp.367,2021.americasnlp-1.8,0,0.0251169,"elines candidates from beam search is using candidates which also leverage multiple candidates in this secas data augmentation. By pairing every candidate tion. with its corresponding ground-truth text, we can construct a new dataset to train FastCorrect base- ROVER ROVER (Fiscus, 1997) is a traditional line. The size of new dataset is n times larger, baseline7 to make use of multiple candidates, which where n is the beam size. We compare the accu- aligns multiple candidates first and then votes for the final token on each position by occurrence racy of data augmentation and FastCorrect 2 in (Amith et al., 2021). In our experiments, we try difference beam sizes, and the results are shown in both the default open-source alignment method and Figure 3. We have several observations: our proposed alignment method. First, simply using multiple candidates as data augmentation cannot yield better result comparing Fusion Fusion is another method for utilizing with only using the best multiple scored by ASR multiple candidates (Liu et al., 2021; Lohrenz et al., model, showing that the key component to benefit 2021). Specifically, Fusion uses a shared encoder from multiple candidates is to take advantage of vot"
2021.findings-emnlp.367,D18-2012,0,0.0301655,", AISHELL1) without pretraining on pseudo data. We crawl 400M unpaired text from the internet and randomly add noise (insertion, deletion or substitution with a homophone dictionary) to the text to construct the pseudo dataset for AISHELL-1 pretraining. The ratio of noise and the probability distribution of noise type (insertion, deletion and substitution) are determined by the word error rate (WER) and statistics of the training corpus for correction, which is constructed by using an ASR model to transcribe the AISHELL-1 training set as mentioned in the above paragraph. We use SentencePiece (Kudo and Richardson, 2018) to learn subword and apply to all the text above. The dictionary size is set to 40K. 4.2 ASR Model We use the ESPnet (Watanabe et al., 2018) toolkit to train an ASR model on AISHELL-1 dataset. In order to verify FastCorrect 2 in a competitive setDecoder We use the same Transformer decoder ting, we utilize several advanced techniques to train architecture as FastCorrect, which takes the ada strong ASR model, including Conformer architecjusted source token of one candidate as input and ture (Gulati et al., 2020), SpecAugment (Park et al., generates the corrected sentence in parallel. 2019), and"
2021.findings-emnlp.367,D18-1149,0,0.0296213,"xuta@microsoft.com 1 See the second paragraph in Section 1. and giving some clues about the pronunciation of 4328 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4328–4337 November 7–11, 2021. ©2021 Association for Computational Linguistics ground-truth token. In this paper, we propose FastCorrect 2, an error correction model that takes multiple ASR candidates as input to leverage this voting effect for better correction accuracy. In order to satisfy the latency constraint for industrial deployment, we leverage non-autoregressive (NAR) generation (Gu et al., 2019; Lee et al., 2018; Guo et al., 2019, 2020; Leng et al., 2021) for fast inference. FastCorrect 2 consists of an encoder that processes multiple source sentences, a duration predictor to predict the number of target tokens corresponding to each source token, and a decoder that generates the target sentence in parallel from the adjusted source sentence, where the adjustment is based on the predicted duration of each source token. We describe the challenges when supporting multiple candidates and introduce our corresponding designs to address these challenges as follows. • Since the lengths of multiple candidates"
2021.findings-emnlp.367,N19-4009,0,0.0270279,"s 6, 512, respectively. The duration predictor consists of 5 layers of 1D convolutional network with ReLU activation and 2 linear layers to output a scalar, all of which has a hidden size of 512. Each convolutional layer is followed by layer normalization (Ba et al., 2016) and dropout. The kernel size of the convolutional network is 3. The candidate predictor additionally contains a global mean pooling layer to predict the candidate loss. We train all correction models on 8 NVIDIA V100 GPUs, with a batch size of 6000 tokens. We use standard training hyper-parameters of Transformer in Fairseq (Ott et al., 2019). To simulate the industrial scenario, we test the inference speed of correction models in three conditions: 1) NVIDIA P40 GPU, 2) 1-core CPU, and 3) 4-core CPU6 . The test batch size is set to 1 to simulate the online serving condition. 4.4 Baseline Systems We compare FastCorrect 2 with several correction models based on both autoregressive architecture and non-autoregressive architecture. Since FastCorrect 2 takes advantage of beam search candidates, we also compare with the cascaded pipeline of correction and re-scoring. FastCorrect We train the FastCorrect baseline following the setting in"
2021.findings-emnlp.406,W13-2322,0,0.0531625,"itive results with the SOTA model, and the speed is an order of magnitude faster. Detailed analyses are conducted to gain more insights into our proposed model and the effectiveness of the pre-training technique. 1 face-01 ARG0 nature ARG1 poss really humankind caprice Figure 1: AMR example of the sentence “Facing the caprice of nature, humankind is really insignificant.” Abstract meaning representation (AMR) parsing aims to abstract semantics from a natural language sentence into a rooted, directed, and labeled graph, where the nodes represent concepts and edges represent semantic relations (Banarescu et al., 2013). Figure 1 gives an example. One main challenge of AMR parsing is the lack of large-scale annotated data, which limits the model representative ability. To alleviate the problem and boost the performance, early works propose to use silver (pseudo) data that are generated from some released AMR parsing models Corresponding author. condition degree ARG1 Introduction ∗ significant-02 polarity (van Noord and Bos, 2017; Konstas et al., 2017). Apart from AMR silver data, Xu et al. (2020) try to use other kinds of large-scale silver data to train a pre-trained model, such as constituent parsing data"
2021.findings-emnlp.406,D19-1393,0,0.18002,"t the performance, early works propose to use silver (pseudo) data that are generated from some released AMR parsing models Corresponding author. condition degree ARG1 Introduction ∗ significant-02 polarity (van Noord and Bos, 2017; Konstas et al., 2017). Apart from AMR silver data, Xu et al. (2020) try to use other kinds of large-scale silver data to train a pre-trained model, such as constituent parsing data and machine translation data. With the development of pre-trained language models, recent works try to use pre-trained language models to enhance the model input representative ability (Cai and Lam, 2019; Zhou et al., 2021). Most of the them use pre-trained models in the model encoder side since it naturally provides powerful contextualized representations for sentences. Recently, Bevilacqua et al. (2021) propose a seq2seq AMR parser based on BART (Lewis et al., 2020), which is one encoder-decoder fashion pre-trained language model. They first convert the AMR graph into a text sequence with symbols indicating the concepts’ graph positions. Then, they propose to fine-tune the sentence sequence and AMR graph sequence on BART, achieving large improvements compared with previous works, including"
2021.findings-emnlp.406,2020.acl-main.119,0,0.143065,"The remaining question is how we do concept generation and edge classification with Transformer, which is usually used for encoding sequences. Our answer is giving the Transformer attention mechanisms more meanings. In detail, we try to demonstrate that the self-attention in the decoder captures the semantic relation that can guide establishing the connections for the concepts and the cross-attention implicitly links the concept with its surface word, which is similar to the core of attention-based machine translation. Based on the inspirations, we use the copy mechanism (Zhang et al., 2019b; Cai and Lam, 2020) to copy words or lemmas as candidate concepts for concept prediction, in which we treat the cross-attention between the encoder and decoder as the probability. Another source of candidate concepts is the extracted concept vocabulary from the training data. For edge classification, we directly treat part of the decoder self-attention values as the edge scores between concept nodes. and the speed is an order of magnitude faster. Our contributions are threefold: (I) We propose a simple Transformer-based AMR parser, which only needs to add one external bi-affine scorer for the relation classifica"
2021.findings-emnlp.406,E17-1051,0,0.0189556,"tization, part-of-speech tagging, and named entity tagging. The dependency relations are obtained by the bi-affine dependency parser (Dozat and Manning, 2017) implemented in SuPar (Zhang et al., 2020). Previous works (Zhang et al., 2019b; Cai and Lam, 2020) usually use graph recategorization to reduce the complexity and sparseness of the AMR graph. In this work, we use the same script from Cai and Lam (2020) for preand post-processing. Our models are trained with Adam (Kingma and Ba, 2015) optimizer and learning rate with warm-up same as to Vaswani et al. (2017). We use the evaluation tool of Damonte et al. (2017) to test our model. Training Criterion. We train our models for at most 2,020 epochs and choose the best model to evaluate the test data according to the performance on development data. Methods Zhang et al. (2019a) Zhang et al. (2019b) Cai and Lam (2020) TAMR Smatch Unlabeled-Smatch 76.3 – 77.0 80.0 80.2 82.8 80.3 83.5 Table 2: Smatch scores of our model TAMR and comparison with previous seq2graph-based models on AMR2.0 test data. Silver Data Dev Test JAMR – 67.0? TAMR 80.6 80.3 S PRING – 83.8 Pre-train Dev Test 70.5 70.8 81.5 81.2 83.0 82.7 Fine-tune Dev Test 80.8 81.0 82.4 82.2 83.8 83.7 Ta"
2021.findings-emnlp.406,N19-1423,0,0.0334058,"he model architecture in detail and show how to adapt the AMR parsing process into Transformer in the following sections. 3.2 Input Layer. Encoder Input. The model input of each word wi in the sentence s is composed of its character representation which is generated by a convolutional neural network (CNN) (Kalchbrenner et al., 2014), randomly initialized lemma, part-of-speech tag, named entity tag, and dependency label embeddings (Xia et al., 2019), which is denoted as lem P oS NE fi = repchar wi ⊕ embwi ⊕ embwi ⊕ embwi ⊕ DL embwi , where ⊕ means the concatenation operation. We also use BERT (Devlin et al., 2019) to enhance the word representation. To get the wordbased representations, we make average pooling to sub-word-based representations. And due to the GPU limitation, we fix the BERT model parameters as Zhang et al. (2019b). The final model input representation for wi is computed as xw i = √ wi |s dim ∗ (MLP(fi ) + MLP(repBERT )) + embpi , where dim is the embedding dimension and embpi is the i-th sinusoidal position embedding. Decoder Input. In the decoder, we use the concatenation of the concept character representation and the randomly initialized concept embedding denoted as √ as the concept"
2021.findings-emnlp.406,P14-1134,0,0.0332947,"-based AMR parser, which only needs to add one external bi-affine scorer for the relation classification. (II) We investigate how to ensemble different models via the proposed stack pre-training method. (III) Detailed analyses show more insights into our model and several interesting findings of utilizing the silver data. 2 Related Work AMR parsing approaches can mostly be categorized into four classes: pipeline-based, transitionbased, seq2seq-based, and seq2graph-based approaches. Pipeline-based approaches mainly consist of two steps: 1) concept identification and 2) relation identification. Flanigan et al. (2014) is the first AMR parsing work (JAMR) that treats concept identification as a sequence labeling problem and relation identification as a maximum-scoring connected graph searching problem, in which they also propose an influential rule-based aligner for aligning the concepts and words. Lyu and Titov (2018) treat Second, to achieve competitive performance the alignment as latent variables and propose a joint with the current SOTA model, we seek to use sil- model for AMR parsing. Zhang et al. (2019a) first ver data to enhance the model representative abil- use the attention-based copy mechanism t"
2021.findings-emnlp.406,P14-1062,0,0.0281972,"≤ i ≤ m, 1 ≤ j ≤ m, r ∈ R} is the set of edges in the graph. R is the set of AMR relations. Overall, our Transformer-based model consists of the following modules, i.e., input layer, encoder layer, decoder layer, concept generator, edge generator, and relation classifier. We will describe the model architecture in detail and show how to adapt the AMR parsing process into Transformer in the following sections. 3.2 Input Layer. Encoder Input. The model input of each word wi in the sentence s is composed of its character representation which is generated by a convolutional neural network (CNN) (Kalchbrenner et al., 2014), randomly initialized lemma, part-of-speech tag, named entity tag, and dependency label embeddings (Xia et al., 2019), which is denoted as lem P oS NE fi = repchar wi ⊕ embwi ⊕ embwi ⊕ embwi ⊕ DL embwi , where ⊕ means the concatenation operation. We also use BERT (Devlin et al., 2019) to enhance the word representation. To get the wordbased representations, we make average pooling to sub-word-based representations. And due to the GPU limitation, we fix the BERT model parameters as Zhang et al. (2019b). The final model input representation for wi is computed as xw i = √ wi |s dim ∗ (MLP(fi ) +"
2021.findings-emnlp.406,P17-1014,0,0.292417,"ntics from a natural language sentence into a rooted, directed, and labeled graph, where the nodes represent concepts and edges represent semantic relations (Banarescu et al., 2013). Figure 1 gives an example. One main challenge of AMR parsing is the lack of large-scale annotated data, which limits the model representative ability. To alleviate the problem and boost the performance, early works propose to use silver (pseudo) data that are generated from some released AMR parsing models Corresponding author. condition degree ARG1 Introduction ∗ significant-02 polarity (van Noord and Bos, 2017; Konstas et al., 2017). Apart from AMR silver data, Xu et al. (2020) try to use other kinds of large-scale silver data to train a pre-trained model, such as constituent parsing data and machine translation data. With the development of pre-trained language models, recent works try to use pre-trained language models to enhance the model input representative ability (Cai and Lam, 2019; Zhou et al., 2021). Most of the them use pre-trained models in the model encoder side since it naturally provides powerful contextualized representations for sentences. Recently, Bevilacqua et al. (2021) propose a seq2seq AMR parser ba"
2021.findings-emnlp.406,2020.acl-main.703,0,0.537065,"AMR silver data, Xu et al. (2020) try to use other kinds of large-scale silver data to train a pre-trained model, such as constituent parsing data and machine translation data. With the development of pre-trained language models, recent works try to use pre-trained language models to enhance the model input representative ability (Cai and Lam, 2019; Zhou et al., 2021). Most of the them use pre-trained models in the model encoder side since it naturally provides powerful contextualized representations for sentences. Recently, Bevilacqua et al. (2021) propose a seq2seq AMR parser based on BART (Lewis et al., 2020), which is one encoder-decoder fashion pre-trained language model. They first convert the AMR graph into a text sequence with symbols indicating the concepts’ graph positions. Then, they propose to fine-tune the sentence sequence and AMR graph sequence on BART, achieving large improvements compared with previous works, including those with BERT. However, it makes the model relatively slower, which parses 31 tokens per second. We think there are two main reasons: 1) the 12-layer Transformer decoder and 2) the longer converted graph sequences that include the added symbols. In this work, we inve"
2021.findings-emnlp.406,2020.emnlp-main.196,1,0.846884,"d, directed, and labeled graph, where the nodes represent concepts and edges represent semantic relations (Banarescu et al., 2013). Figure 1 gives an example. One main challenge of AMR parsing is the lack of large-scale annotated data, which limits the model representative ability. To alleviate the problem and boost the performance, early works propose to use silver (pseudo) data that are generated from some released AMR parsing models Corresponding author. condition degree ARG1 Introduction ∗ significant-02 polarity (van Noord and Bos, 2017; Konstas et al., 2017). Apart from AMR silver data, Xu et al. (2020) try to use other kinds of large-scale silver data to train a pre-trained model, such as constituent parsing data and machine translation data. With the development of pre-trained language models, recent works try to use pre-trained language models to enhance the model input representative ability (Cai and Lam, 2019; Zhou et al., 2021). Most of the them use pre-trained models in the model encoder side since it naturally provides powerful contextualized representations for sentences. Recently, Bevilacqua et al. (2021) propose a seq2seq AMR parser based on BART (Lewis et al., 2020), which is one"
2021.findings-emnlp.406,P19-1009,0,0.0365327,"Missing"
2021.findings-emnlp.406,D19-1392,0,0.0256032,"Missing"
2021.findings-emnlp.406,2020.acl-main.302,1,0.801999,"00 BERT-base-cased 4 8 1024 100 Table 1: Hyper-parameter settings. representation and BERT is fixed in our work due to the GPU memory limitation. The encoder and decoder consist of 4 and 8 Transformer blocks, respectively. Each Transformer block has 8 heads, the feed-forward hidden size is 1024, and the hidden size is 512. Implementation Details. We use Stanford CoreNLP (Manning et al., 2014) for tokenization, lemmatization, part-of-speech tagging, and named entity tagging. The dependency relations are obtained by the bi-affine dependency parser (Dozat and Manning, 2017) implemented in SuPar (Zhang et al., 2020). Previous works (Zhang et al., 2019b; Cai and Lam, 2020) usually use graph recategorization to reduce the complexity and sparseness of the AMR graph. In this work, we use the same script from Cai and Lam (2020) for preand post-processing. Our models are trained with Adam (Kingma and Ba, 2015) optimizer and learning rate with warm-up same as to Vaswani et al. (2017). We use the evaluation tool of Damonte et al. (2017) to test our model. Training Criterion. We train our models for at most 2,020 epochs and choose the best model to evaluate the test data according to the performance on developmen"
2021.findings-emnlp.406,P18-1037,0,0.065492,"he silver data. 2 Related Work AMR parsing approaches can mostly be categorized into four classes: pipeline-based, transitionbased, seq2seq-based, and seq2graph-based approaches. Pipeline-based approaches mainly consist of two steps: 1) concept identification and 2) relation identification. Flanigan et al. (2014) is the first AMR parsing work (JAMR) that treats concept identification as a sequence labeling problem and relation identification as a maximum-scoring connected graph searching problem, in which they also propose an influential rule-based aligner for aligning the concepts and words. Lyu and Titov (2018) treat Second, to achieve competitive performance the alignment as latent variables and propose a joint with the current SOTA model, we seek to use sil- model for AMR parsing. Zhang et al. (2019a) first ver data to enhance the model representative abil- use the attention-based copy mechanism to predict ity. Specifically, we employ three different per- concepts in a BiLSTM encoder-decoder framework formance AMR models (denoted as “father” mod- and then use the bi-affine scorer for edge and relaels) to generate three different performance silver tion prediction based on the predicted concepts. d"
2021.findings-emnlp.406,P14-5010,0,0.00312283,"4733 Input Layer character lemma PoS tag NER tag dependency label concept BERT Encoder Layer Transformer encoder Decoder Layer Transformer decoder Concept Generator hidden size Relation Classifier hidden size 32 300 32 16 64 300 BERT-base-cased 4 8 1024 100 Table 1: Hyper-parameter settings. representation and BERT is fixed in our work due to the GPU memory limitation. The encoder and decoder consist of 4 and 8 Transformer blocks, respectively. Each Transformer block has 8 heads, the feed-forward hidden size is 1024, and the hidden size is 512. Implementation Details. We use Stanford CoreNLP (Manning et al., 2014) for tokenization, lemmatization, part-of-speech tagging, and named entity tagging. The dependency relations are obtained by the bi-affine dependency parser (Dozat and Manning, 2017) implemented in SuPar (Zhang et al., 2020). Previous works (Zhang et al., 2019b; Cai and Lam, 2020) usually use graph recategorization to reduce the complexity and sparseness of the AMR graph. In this work, we use the same script from Cai and Lam (2020) for preand post-processing. Our models are trained with Adam (Kingma and Ba, 2015) optimizer and learning rate with warm-up same as to Vaswani et al. (2017). We use"
2021.findings-emnlp.406,P19-1451,0,0.0698639,"e bi-affine scorer for edge and relaels) to generate three different performance silver tion prediction based on the predicted concepts. data and try to investigate several questions which Transition-based methods aim to design a seare seldom discussed in previous works: 1) What ries of actions to generate the AMR graph. Wang are the best learning schedules to build pre-trained et al. (2016) propose to transform the sentence’s demodels with silver data and later fine-tune with pendency tree into its AMR graph. Ballesteros and the gold-standard data, respectively? 2) Are all Al-Onaizan (2017); Naseem et al. (2019) use Stackthe different performance silver data beneficial for LSTM transition-based AMR parser that transour model, even its father model lags behind our forms the sentence into the AMR graph, which model? and 3) Whether using multiple different is different from Wang et al. (2016). With the performance silver data can provide more informa- rise of Transformer, Astudillo et al. (2020); Zhou tion than the best performance one or not, i.e., can et al. (2021) propose to use Stack-Transformer for the higher performance silver data benefits from transition-based AMR parsing. lower performance silv"
2021.findings-emnlp.406,S16-1181,0,0.0190748,"of actions to generate the AMR graph. Wang are the best learning schedules to build pre-trained et al. (2016) propose to transform the sentence’s demodels with silver data and later fine-tune with pendency tree into its AMR graph. Ballesteros and the gold-standard data, respectively? 2) Are all Al-Onaizan (2017); Naseem et al. (2019) use Stackthe different performance silver data beneficial for LSTM transition-based AMR parser that transour model, even its father model lags behind our forms the sentence into the AMR graph, which model? and 3) Whether using multiple different is different from Wang et al. (2016). With the performance silver data can provide more informa- rise of Transformer, Astudillo et al. (2020); Zhou tion than the best performance one or not, i.e., can et al. (2021) propose to use Stack-Transformer for the higher performance silver data benefits from transition-based AMR parsing. lower performance silver data? Based on the anSeq2seq-based approaches convert the AMR swers to these questions, which are shown in Secgraph generation problem into a symbolic sequence tion 6.2, we propose a stack pre-training technique generation problem, where the hierarchy structure for effectively us"
2021.naacl-main.144,P17-4017,0,0.0467976,"Missing"
2021.naacl-main.144,N19-1423,0,0.126883,"ng et al. (2019a) extract the semanthat our proposed unified model achieves superior tic representations from a pre-trained SRL model performance compared with previously proposed BMESO-based works. Our contributions are: (i) and feed them into the opinion mining model, achieving substantial improvements. Zhang et al. we propose a unified span-based model for opinion (2020) incorporate the powerful contextual repremining in the end-to-end fashion that also supports sentations of bi-directional encoder representations the given-expression setting, (ii) we successfully from Transformers (BERT) (Devlin et al., 2019) integrate syntactic constituents knowledge into our and external dependency syntactic knowledge. model with MTL and GCN, achieving promising improvements, (iii) detailed analyses demonstrate To solve or alleviate the weaknesses of the prethe effectiveness of our unified model and the use- viously proposed BMESO-based models, we profulness of integrating constituent syntactic knowl- pose a new method to unifiedly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classifica"
2021.naacl-main.144,Q19-1019,0,0.0464782,"Missing"
2021.naacl-main.144,P18-2058,0,0.0219271,"edly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classification as an MTL problem. Besides, to boost the opinion mining performance and motivated by the span-based task formalism, we explore to incorporate syntactic constituents into our model. Utilizing span-based representations have been investigated for many other NLP tasks, such as named entity recognition (NER) (Tan et al., 2020), constituency parsing (Kitaev and Klein, 2018), and semantic role labeling (SRL) (He et al., 2018). Generally, NER is a single span classification problem, constituency parsing is a span-based structure prediction problem, and SRL is a word-span classification problem. Different from them, in our methodology, OM is a span-span classification problem. 3 The S PAN OM Model 3.1 Task Definition. Given an input sentence s = w1 , w2 , ..., wn , our model aims to predict the gold-standard opinion structures Y ⊆ E × O × R, where E = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of expressions, O = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of opinion roles , and R is the set of opinion relations (holder"
2021.naacl-main.144,P14-1062,0,0.0186777,"char representation, and contextual word representation to compose the model input, denoted as: xi = embword ⊕ repchar ⊕ repcontext , (1) wi wi wi |s Encoder Layer. + M LPbexp (hb ) + M LPeexp (he ), srol = M LP rol (spanrol b,e ) (5) + M LPbrol (hb ) + M LPerol (he ). We can observe that for a sentence with n words, the numbers of candidate spans for expressions and roles are both n∗(n+1) , while the number of 2 gold expressions and roles are much fewer. To alleviate the unbalanced number of gold samples where ⊕ means the concatenate operation. We use the convolutional neural networks (CNN) (Kalchbrenner et al., 2014) to generate the character representations over the characters of words. 1797 1 We omit the process of span boundary module in Figure 2 for clarity. O Classification Layer MLP Holder OM Target MLP Encoder MLP Representation Layer OM Constituent MTL Input seriously needs equipment for detecting drugs GCN Constituent Encoder Input Layer GCN Input OM Encoder Layer Encoder Input MTL+GCN GCN Figure 2: The model architecture of our unified span-based opinion mining model (left) and syntactic constituent integration methods (right). and negative samples, we adapt the focal loss that is widely used in"
2021.naacl-main.144,P16-1087,0,0.342622,"rporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinio"
2021.naacl-main.144,D14-1162,0,0.0862699,"62.04 53.27 57.76 Proportional F1 Holder Target Overall 46.62 34.29 55.62 41.65 48.90 61.20 49.88 55.68 Table 1: Experimental results of our span-based opinion mining model and comparison with previous works on the MPQA2.0 dataset in the end-to-end setting. “-” means results are not reported in their paper. Exact P R F1 Zhang et al. (2019b) 60.21 48.52 53.04 S PAN OM 64.85 52.60 58.06 S PAN OM+BERT 67.15 60.63 63.71 Models Table 2: Results and comparison of the expression prediction on the exact metric in the end-to-end setting. 5.2 Hyper-parameters. We employ the 300-dimension GloVe vector (Pennington et al., 2014) as our pre-trained word embeddings. The character embeddings are randomly initialized and a CNN with kernel sizes of 3, 4, 5 is used to capture the character representations. For the contextual representations, we extract the representations from the base BERT by making a weighted summation over the last four layer outputs. The hidden size of the BiLSTM layer is set to 300 and we employ 2-layer BiLSTMs to encode the input representations. The dimension of opinion expression and role representations is 300 and the hidden size of expression, role, and relation classifiers is 150. We use 3-layer"
2021.naacl-main.144,P13-1161,0,0.210861,"thod. In addition, incorporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap"
2021.naacl-main.144,Q14-1039,0,0.0244727,"n Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1795–1804 June 6–11, 2021. ©2021 Association for Computational Linguistics belongs to an expression, 0 otherwise), thus one sample is expanded n times if one sentence has n expressions, which is inefficient (Marasovi´c and Frank, 2018; Zhang et al., 2020). 2) The BMESObased method is weak to capture long-range dependencies and prefers to predict shorter opinion role spans (Zhang et al., 2020). 2 Related Work There are several task settings for opinion mining in the community: 1) Breck et al. (2007); Yang and Cardie (2014) focus on labeling the expressions. 2) Katiyar and Cardie (2016); Zhang et al. (2019b); Quan et al. (2019) discover the opinion structures in the end-to-end setting, i.e, based on the systemMotivated by the span-based representations of atic expressions. 3) Marasovi´c and Frank (2018); opinion expressions and roles, we propose a unified Zhang et al. (2019a, 2020) identify the opinion span-based opinion mining model (S PAN OM) that roles based on the given expressions. Our work can solve or alleviate the aforementioned weak- follows the end-to-end setting and also supports nesses. First, we tre"
2021.naacl-main.144,P18-1249,0,0.388081,"ing constituent syntactic knowl- pose a new method to unifiedly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classification as an MTL problem. Besides, to boost the opinion mining performance and motivated by the span-based task formalism, we explore to incorporate syntactic constituents into our model. Utilizing span-based representations have been investigated for many other NLP tasks, such as named entity recognition (NER) (Tan et al., 2020), constituency parsing (Kitaev and Klein, 2018), and semantic role labeling (SRL) (He et al., 2018). Generally, NER is a single span classification problem, constituency parsing is a span-based structure prediction problem, and SRL is a word-span classification problem. Different from them, in our methodology, OM is a span-span classification problem. 3 The S PAN OM Model 3.1 Task Definition. Given an input sentence s = w1 , w2 , ..., wn , our model aims to predict the gold-standard opinion structures Y ⊆ E × O × R, where E = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of expressions, O = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of opinion ro"
2021.naacl-main.144,2020.acl-main.297,1,0.617524,"promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinion structures between different expresOpi"
2021.naacl-main.144,N19-1066,0,0.12942,"etting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinion structures between different expresOpinion mining (OM), which aims to find the opin- sions in one sentence. Figure 1 gives an example, ion structures of “Who expressed what opinions in which some overlapped opinion relations have towards what.” in one sentence, has achieved much been discarded by previous works (Katiyar and attention in recent years (Katiyar and Cardie, 2016; Cardie, 2016), such as [happy, he loves being EnMarasovi´c and Frank, 2018; Zhang et al., 2019b, derly Park, Target] and [loves, he, Holder]. There 2020). The opinion analysis has many NLP appli- are also other works which focus only on predicting cations, such as social media monitoring (Bollen opinions roles based on the gold-standard expreset al., 2011) and e-commerce applications (Cui sions, which also follow the BMESO-based method et al., 2017). The commonly used benchmark (Marasovi´c and Frank, 2018; Zhang et al., 2020). However, they also suffer from some weaknesses: ∗ Rui Wang’s contributions were carried out while at 1) the expressions are usually fed into the model inAlibaba"
2021.naacl-main.144,N18-1054,0,0.041995,"Missing"
2021.naacl-main.144,J93-2004,0,0.0741147,"F1 score of 67.66. Finally, we try to combine the two kinds of methods and the results are shown in the last major row. It is clear that combining the MTL method with OntoNotes and the GCN method with ParserPTB achieves better results than the reversed one. Therefore, our constituent-enhanced opinion mining model follows this combination. Besides, we can also see the relative lower results of “OntoNotes+PTB” in “+MTL” and “+GCN” settings, which is strange Which source of constituent knowledge is better? There are two main constituent syntax corpus in the community, i.e., Penn Treebank (PTB) (Marcus et al., 1993) and OntoNotes5.0 (Weischedel et al., 2013). The PTB corpus contains about 39k training data and mainly focuses on news data, while the OntoNotes5.0 corpus contains about 75k training data and focuses on multi-domain data (news, web, telephone conversation, and etc.). It is a worthy question to explore which is better for our span-based OM model, or what kind of combination is better. We compare them with various combinations on the BERT-based model, whose results are shown in Table 5. First, the sec7 We use the code of Kitaev and Klein (2018) to train the OntoNotes conond major row shows the"
2021.naacl-main.144,D18-1244,0,0.0229945,"information to expressions and roles. 4.2 The GCN Method. The MTL method enhances our OM model from the aspect of model representative ability by jointly modeling opinion mining and partial constituency parsing. We argue that modeling the syntactic constituent structure is also beneficial for OM because it provides valuable syntactic information for a sentence. Therefore, we try to employ the recently popular GCN (Kipf and Welling, 2016) to encode the constituent structure. However, the conventional GCN is not suitable for constituency trees, because it usually works on the dependency trees (Zhang et al., 2018, 2020) where the nodes are the surface words in a sentence. While, in constituent trees, there exists a certain number of non-terminal nodes3 , such as “NP”, “VP”, “SBAR” and so on. So it is hard to directly apply conventional GCN on the constituent trees. In the following, we first introduce the definition and workflow of typical GCN and then describe our modification. Formally, we denote an undirected graph as G = (V, E), where V and E are the set of nodes and edges, respectively. The GCN computation flow of node v ∈ V at l-th layer is defined as: ! X l hlv = ρ Wl hl−1 (11) u +b , u∈N (v) 3"
2021.naacl-main.311,D18-1549,0,0.0207702,"adequate training corpus for one language. In this paper, we first define and analyze the unbalanced training data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems. 1 Introduction Recently, unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has attracted a high level of interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Wu et al., 2019; Sun et al., 2019, 2020b). With the help of cross-lingual language model pretraining (Lample and Conneau, 2019; Song et al., 2019; Sun et al., 2020a), the denoising auto-encoder (Vincent et al., 2010), and backtranslation (Sennrich et al., 2016a), UNMT has achieved remarkable results in several translation tasks. However, in real-world scenarios, in contrast to the many large corpora available for high-resource languages such as English and French, massive monolingual corpora do not exist for some extremely low-re"
2021.naacl-main.311,P07-2045,0,0.0126746,"gual WMT news crawl datasets3 for each language. For the high-resource languages En and Fr, we randomly extracted 50M sentences. For the low-resource languages Ro and Et, we used all available monolingual news crawl training data. To make our experiments comparable with previous work (Lample and Conneau, 2019), we report the results on newstest2014 for Fr–En, newstest2016 for Ro–En, and newstest2018 for Et–En. Language En Fr Ro Et Sentences Words 50.00M 50.00M 8.92M 3.00M 1.15B 1.19B 207.07M 51.39M Table 2: Statistics of the monolingual corpora. For preprocessing, we used the Moses tokenizer (Koehn et al., 2007). To clean the data, we only applied the Moses script clean-corpus-n.perl to remove lines from the monolingual data containing more than 50 words. We used a shared vocabulary for all language pairs, with 60,000 subword tokens based on BPE (Sennrich et al., 2016b). learning rate was 0.0001, β1 = 0.9, and β2 = 0.98. We trained a specific cross-lingual language model for each different training dataset. The language model was used to initialize the full parameters of the UNMT system. Eight V100 GPUs were used to train all UNMT models. We used the casesensitive 4-gram BLEU score computed by the mu"
2021.naacl-main.311,P16-1009,0,0.302263,"hat the proposed methods substantially outperform conventional UNMT systems. 1 Introduction Recently, unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has attracted a high level of interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Wu et al., 2019; Sun et al., 2019, 2020b). With the help of cross-lingual language model pretraining (Lample and Conneau, 2019; Song et al., 2019; Sun et al., 2020a), the denoising auto-encoder (Vincent et al., 2010), and backtranslation (Sennrich et al., 2016a), UNMT has achieved remarkable results in several translation tasks. However, in real-world scenarios, in contrast to the many large corpora available for high-resource languages such as English and French, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian. Data size (sentences) En-Fr Fr-En 50M En and 50M Fr (Baseline) 25M En and 25M Fr 50M En and 2M Fr 2M En and 50M Fr 2M En and 2M Fr 36.63 36.59 31.01 31.84 30.91 34.38 34.34 31.06 30.21 29.86 Table 1: UNMT performance (BLEU score) for different training data sizes on En–Fr language pairs. T"
2021.naacl-main.311,P16-1162,0,0.856645,"hat the proposed methods substantially outperform conventional UNMT systems. 1 Introduction Recently, unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has attracted a high level of interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Wu et al., 2019; Sun et al., 2019, 2020b). With the help of cross-lingual language model pretraining (Lample and Conneau, 2019; Song et al., 2019; Sun et al., 2020a), the denoising auto-encoder (Vincent et al., 2010), and backtranslation (Sennrich et al., 2016a), UNMT has achieved remarkable results in several translation tasks. However, in real-world scenarios, in contrast to the many large corpora available for high-resource languages such as English and French, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian. Data size (sentences) En-Fr Fr-En 50M En and 50M Fr (Baseline) 25M En and 25M Fr 50M En and 2M Fr 2M En and 50M Fr 2M En and 2M Fr 36.63 36.59 31.01 31.84 30.91 34.38 34.34 31.06 30.21 29.86 Table 1: UNMT performance (BLEU score) for different training data sizes on En–Fr language pairs. T"
2021.naacl-main.311,P95-1026,0,0.892071,"∗ (Y |X) logPM U (X|Y ) logPM U (Y |X), U ∗ (X|Y ) (1) where P (X) and P (Y ) are the empirical data distribution from monolingual corpora {X}, {Y }, and PM U (Y |X) and PM U (X|Y ) are the conditional distributions generated by the UNMT model. In ∗ addition, M U denotes the model at the previous iteration for generating new pseudo-parallel sentence pairs to update the UNMT model. Self-training proposed by Scudder (1965), is a semi-supervised approach that utilizes unannotated data to create better models. Self-training has been successfully applied to many natural language processing tasks (Yarowsky, 1995; McClosky et al., 2006; Zhang and Zong, 2016; He et al., 2020). Recently, He et al. (2020) empirically found that noisy self-training could improve the performance of supervised machine translation and synthetic data could play a positive role, even as a target. 4 Self-training Mechanism for UNMT Based on these previous empirical findings and analyses, we propose a self-training mechanism to generate synthetic training data for UNMT to alleviate poor performance in the unbalanced training data scenario. The synthetic data increases the diversity of low-resource language data, further enhancin"
2021.naacl-main.311,D16-1160,0,0.0127253,", U ∗ (X|Y ) (1) where P (X) and P (Y ) are the empirical data distribution from monolingual corpora {X}, {Y }, and PM U (Y |X) and PM U (X|Y ) are the conditional distributions generated by the UNMT model. In ∗ addition, M U denotes the model at the previous iteration for generating new pseudo-parallel sentence pairs to update the UNMT model. Self-training proposed by Scudder (1965), is a semi-supervised approach that utilizes unannotated data to create better models. Self-training has been successfully applied to many natural language processing tasks (Yarowsky, 1995; McClosky et al., 2006; Zhang and Zong, 2016; He et al., 2020). Recently, He et al. (2020) empirically found that noisy self-training could improve the performance of supervised machine translation and synthetic data could play a positive role, even as a target. 4 Self-training Mechanism for UNMT Based on these previous empirical findings and analyses, we propose a self-training mechanism to generate synthetic training data for UNMT to alleviate poor performance in the unbalanced training data scenario. The synthetic data increases the diversity of low-resource language data, further enhancing the performance of the translation, even 3"
2021.naacl-main.311,P19-1119,1,0.628784,"raining data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems. 1 Introduction Recently, unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has attracted a high level of interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Wu et al., 2019; Sun et al., 2019, 2020b). With the help of cross-lingual language model pretraining (Lample and Conneau, 2019; Song et al., 2019; Sun et al., 2020a), the denoising auto-encoder (Vincent et al., 2010), and backtranslation (Sennrich et al., 2016a), UNMT has achieved remarkable results in several translation tasks. However, in real-world scenarios, in contrast to the many large corpora available for high-resource languages such as English and French, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian. Data size (sentences) En-Fr Fr-En 50M En and 50M Fr (Baseline)"
2021.naacl-main.311,2020.acl-main.324,1,0.817815,"improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems. 1 Introduction Recently, unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has attracted a high level of interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Wu et al., 2019; Sun et al., 2019, 2020b). With the help of cross-lingual language model pretraining (Lample and Conneau, 2019; Song et al., 2019; Sun et al., 2020a), the denoising auto-encoder (Vincent et al., 2010), and backtranslation (Sennrich et al., 2016a), UNMT has achieved remarkable results in several translation tasks. However, in real-world scenarios, in contrast to the many large corpora available for high-resource languages such as English and French, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian. Data size (sentences) En-Fr Fr-En 50M En and 50M Fr (Baseline) 25M En and 25M Fr 50M En and 2M Fr 2M En and 50M Fr 2M En and 2M Fr 36.63 36.59 31.01 31.84 30.91 34.38 34.34 31.06 30.21 29.86 Ta"
2021.naacl-main.311,N19-1120,0,0.040071,"Missing"
C12-2127,W11-2832,0,0.0132891,"ich set of annotation as input and is tightened to specific frameworks or internal representation, making the reuse of other natural language processing components difficult. Inspired by the successful application of statistical methods in natural language analysis, researchers shifted towards using standardized linguistic annotations to learn generation models. In particular, the now ever-so-popular dependency representation for syntacto-semantic structures has made its way into the sentence realization task, as evident by the recent Generation Challenge 2011 Surface Realization Shared Task (Belz et al., 2011). Given the full-connectedness of the input structure, the task of surface realization concerns mainly about the linearization process1 , which shall determine the ordering of words in the dependency structures. While the earlier work like Langkilde and Knight (1998) showed that the N -gram language models can work well on the tree linearization, more recent study shows that improvements can be achieved by combining the language model outputs with discriminative classifiers (Filippova and Strube, 2009; Bohnet et al., 2010). On the other hand, we see that relatively few results have been report"
C12-2127,D12-1085,0,0.0229219,"Missing"
C12-2127,C10-1012,0,0.0634205,"by the recent Generation Challenge 2011 Surface Realization Shared Task (Belz et al., 2011). Given the full-connectedness of the input structure, the task of surface realization concerns mainly about the linearization process1 , which shall determine the ordering of words in the dependency structures. While the earlier work like Langkilde and Knight (1998) showed that the N -gram language models can work well on the tree linearization, more recent study shows that improvements can be achieved by combining the language model outputs with discriminative classifiers (Filippova and Strube, 2009; Bohnet et al., 2010). On the other hand, we see that relatively few results have been reported on a grammar-based approach, where linearization rules are used instead to determine the word order within a given structured input. In this paper, we use tree linearization grammars to specify the local linearization constraints in bilexical single-headed dependency trees. Unlexicalized linearization rules and their probabilities can be learned easily from the treebank. By using a dependency parser, we expand the grammar extraction to automatically parsed dependency structures as well. The linearization model is fully"
C12-2127,I05-1015,0,0.0855798,"Missing"
C12-2127,P08-1022,0,0.0578704,"Missing"
C12-2127,N09-2057,0,0.0763653,"realization task, as evident by the recent Generation Challenge 2011 Surface Realization Shared Task (Belz et al., 2011). Given the full-connectedness of the input structure, the task of surface realization concerns mainly about the linearization process1 , which shall determine the ordering of words in the dependency structures. While the earlier work like Langkilde and Knight (1998) showed that the N -gram language models can work well on the tree linearization, more recent study shows that improvements can be achieved by combining the language model outputs with discriminative classifiers (Filippova and Strube, 2009; Bohnet et al., 2010). On the other hand, we see that relatively few results have been reported on a grammar-based approach, where linearization rules are used instead to determine the word order within a given structured input. In this paper, we use tree linearization grammars to specify the local linearization constraints in bilexical single-headed dependency trees. Unlexicalized linearization rules and their probabilities can be learned easily from the treebank. By using a dependency parser, we expand the grammar extraction to automatically parsed dependency structures as well. The lineari"
C12-2127,W09-1201,1,0.864841,"Missing"
C12-2127,P98-1116,0,0.0694121,"nalysis, researchers shifted towards using standardized linguistic annotations to learn generation models. In particular, the now ever-so-popular dependency representation for syntacto-semantic structures has made its way into the sentence realization task, as evident by the recent Generation Challenge 2011 Surface Realization Shared Task (Belz et al., 2011). Given the full-connectedness of the input structure, the task of surface realization concerns mainly about the linearization process1 , which shall determine the ordering of words in the dependency structures. While the earlier work like Langkilde and Knight (1998) showed that the N -gram language models can work well on the tree linearization, more recent study shows that improvements can be achieved by combining the language model outputs with discriminative classifiers (Filippova and Strube, 2009; Bohnet et al., 2010). On the other hand, we see that relatively few results have been reported on a grammar-based approach, where linearization rules are used instead to determine the word order within a given structured input. In this paper, we use tree linearization grammars to specify the local linearization constraints in bilexical single-headed depende"
C12-2127,N06-1020,0,0.104081,"Missing"
C12-2127,H05-1066,0,0.0611513,"points suggests that a better ranking model can potentially achieve further improvements on the linearization. This will be discussed in Section 4.3. 4.2 Experiments with Automatically Parsed Data Self-training has been shown to be effective for parser training (McClosky et al., 2006). It expands the training observations on new texts with hypothesized annotation produced by a base model. In our case, we can obtain further linearization observations from unannotated sentences, and rely on a parser to produce the dependency structures7 . We use a state-of-the-art dependency parser, MSTParser (McDonald et al., 2005), and train it with the same data with gold-standard dependency annotations using the second order features and a projective decoder. For the additional data, we use a fragment of the NANC corpus (765670 sequences 〈n|sub j, ad v|mod, v|hd, n1 |o b j〉 and 〈n2 |sub j, v|hd, n1 |o b j, ad v|mod〉. On top of such instances from all the configurations, we train a tri-gram model. 5 The features we use include token features, lemma and part-of-speech, and the dependency relation. We differentiate parent and children nodes by adding different prefixes. 6 In the CoNLL data, the coarse-grained POS is the"
C12-2127,P02-1040,0,0.0835036,"find all such possibilities. With such lazy expansion of the search frontier, only the immediate candidates are added to the local agenda of each node. 4 Experiments For the evaluation, we use the dependency treebanks for multiple languages from the CoNLL-shared task 20092 (Hajiˇc et al., 2009). Additional unlabeled English texts from L.A. Times & Washington Post of the North American News Text (NANC) (Supplement)3 are used for training the English models. Testing results are reported on the development sets of the CoNLL dependency treebanks. In addition to the automatic metrics such as BLEU (Papineni et al., 2002) and Ulam’s distance (Birch et al., 2010), we also manually evaluate the quality of the system outputs (Section 4.3). 4.1 Basic Models For the basic models, we compare our grammar-based approaches with three baselines, Random, N-Gram, and Rank. The first baseline simply produces a random order of the words; the second model can be viewed as a simplified version of (Guo et al., 2011)’s basic model4 ; and the third model 2 3 4 http://ufal.mff.cuni.cz/conll2009-st/index.html http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC98T30 Instead of using grammatical functions derived from le"
C12-2127,C04-1097,0,0.024745,"e lifting of non-projective edges in a dependency tree can be combined with the use of linearization rules in our approach in the future. Note that although we test our models on the same data source as the surface realization shared task8 , subtle differences in the preprocessing of the data and/or the evaluation scripts make the direct comparison to previously reported results difficult. Some comparison of different approaches and reported results will be discussed in the next section. 5 Discussion and Future Work Several works on statistical surface realization have been reported recently. Ringger et al. (2004) proposed several models, and achieved 83.6 BLEU score on the same data source.They also tested their approaches on French and German data, but with predicate-argument structures as input. One of the interesting features of our approach is the generative nature of the model. Unlike the previous work of (Filippova and Strube, 2009; Bohnet et al., 2010) who relied on discriminative modeling for the selection of the realization, our approach actually produces the realization probabilities, and does not rely on ad hoc pruning of the search space. Filippova and Strube (2009) (and their previous pap"
C12-2127,C08-1038,0,\N,Missing
C12-2127,C98-1112,0,\N,Missing
C16-1295,D11-1033,0,0.575583,"corpora, which are also called in-domain or related-domain corpora, can enhance the performance of SMT effectively. Otherwise the irrelevant additional corpora, which are also called outof-domain corpora, may not benefit SMT (Koehn and Schroeder, 2007). SMT adaptation means selecting useful part from mix-domain (mixture of in-domain and out-ofdomain) data, for SMT performance enhancement. The core task in adaptation is about how to select the useful data. Existing works have considered selection strategies with various granularities, though most of them only focus on sentence-level selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al."
C16-1295,2011.iwslt-evaluation.18,0,0.251266,"ion probabilities of connecting phrases calculated by NN can also be used to enhance SMT, and the experimental results will be shown in Section 5.4. 3.3 Integration into SMT The thresholds of Pop and Dminus are tuned using development data. Selected phrase pairs are added into the in-domain PT. Because they are not so useful as the in-domain ones, a penalty score is added. For in-domain phrase pairs, the penalty is set as 1; for the out-of-domain ones the penalty is set as e (= 2.71828...). Other phrase scores (lexical weights et. al.) are used as they are. This penalty setting is similar to (Bisazza et al., 2011). Penalty weights, together with all of existing score weights, will be further tuned by MERT (Och, 2003). The phrase pairs in re-ordering model are selected using the same way as PT. The selected monolingual n-grams are added to the original LM, and the probabilities are re-normalized by SRILM (Stolcke, 2002; Stolcke et al., 2011). 4 Experiments 4.1 Data sets The proposed methods are evaluated on two data sets. 1) IWSLT 2014 French (FR) to English (EN) corpus4 is used as in-domain data and dev2010 and test2010/2011 (Niehues and Waibel, 2012), are selected as development (dev) and test data, r"
C16-1295,P16-1039,1,0.833935,"Missing"
C16-1295,P13-1141,0,0.0645321,"Missing"
C16-1295,N13-1114,0,0.0207534,"Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific mo"
C16-1295,P13-1126,0,0.0188404,"Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific mo"
C16-1295,P08-1010,0,0.178554,"focus on sentence-level selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and mono"
C16-1295,P13-2119,0,0.636704,"elated-domain corpora, can enhance the performance of SMT effectively. Otherwise the irrelevant additional corpora, which are also called outof-domain corpora, may not benefit SMT (Koehn and Schroeder, 2007). SMT adaptation means selecting useful part from mix-domain (mixture of in-domain and out-ofdomain) data, for SMT performance enhancement. The core task in adaptation is about how to select the useful data. Existing works have considered selection strategies with various granularities, though most of them only focus on sentence-level selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et"
C16-1295,2015.mtsummit-papers.10,0,0.834828,"ild lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific model. ∗ Corresponding authors. H. Zhao and B. L. Lu were partially supported by Cai Yuanpei Program (CSC No. 201304490199 and 201304490171), National Natural Science Foundation of China (No. 61672343, 61170114 and 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technolog"
C16-1295,D10-1044,0,0.0735489,"et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the abov"
C16-1295,D14-1062,0,0.238959,"Missing"
C16-1295,C14-1182,0,0.301219,"Missing"
C16-1295,D15-1147,0,0.484005,"selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific model. ∗ Corresponding authors. H. Zhao and B. L. Lu were partially supported by Cai Yuanpei Program (CSC No. 201304490199 and 201304490171), National Natural Science Foundation of China (No. 61672343, 61170114 and 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shangha"
C16-1295,W07-0733,0,0.496687,"ed method is evaluated on IWSLT/NIST data sets, and the results show that phrase based SMT performances are significantly improved (up to +1.6 in comparison with phrase based SMT baseline system and +0.9 in comparison with existing methods). 1 Introduction Large corpora are important for Statistical Machine Translation (SMT) training. However only the relevant additional corpora, which are also called in-domain or related-domain corpora, can enhance the performance of SMT effectively. Otherwise the irrelevant additional corpora, which are also called outof-domain corpora, may not benefit SMT (Koehn and Schroeder, 2007). SMT adaptation means selecting useful part from mix-domain (mixture of in-domain and out-ofdomain) data, for SMT performance enhancement. The core task in adaptation is about how to select the useful data. Existing works have considered selection strategies with various granularities, though most of them only focus on sentence-level selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it i"
C16-1295,W04-3250,0,0.163106,"methods have been applied to a series of NLP tasks, such as Chinese word segmentation and parsing (Cai and Zhao, 2016; Zhang et al., 2016). 4 https://wit3.fbk.eu/mt.php?release=2014-01 5 http://statmt.org/wmt15/translation-task.html 6 http://www.itl.nist.gov/iad/mig/tests/mt/2006/ 3138 4.2 Common Setting The basic settings of IWSLT-2014 FR to EN and NIST-06 CN to EN phrase based translation baseline systems are followed. 5-gram interpolated KN (Kneser and Ney, 1995) LMs are trained. Translation performances are measured by case-insensitive BLEU (Papineni et al., 2002) with significance test (Koehn, 2004) and METEOR (Lavie and Agarwal, 2007). MERT (Och, 2003) (BLEU based) is run three times for each system and the average BLEU/METEOR scores are recorded. 4-layer CSTM (Schwenk, 2012) are applied to NN translation models: phrase length limit is set as seven, shared projection layer of dimension 320 for each word (that is 2240 for seven words), projection layer of dimension 768, hidden layer of dimension 512. The dimensions of input/output layers for both in/out-of-domain CSTMs follows the size of vocabularies of source/target words from in-domain corpora. That is 72K/57K for IWSLT and 149/112K f"
C16-1295,W07-0734,0,0.0163639,"d to a series of NLP tasks, such as Chinese word segmentation and parsing (Cai and Zhao, 2016; Zhang et al., 2016). 4 https://wit3.fbk.eu/mt.php?release=2014-01 5 http://statmt.org/wmt15/translation-task.html 6 http://www.itl.nist.gov/iad/mig/tests/mt/2006/ 3138 4.2 Common Setting The basic settings of IWSLT-2014 FR to EN and NIST-06 CN to EN phrase based translation baseline systems are followed. 5-gram interpolated KN (Kneser and Ney, 1995) LMs are trained. Translation performances are measured by case-insensitive BLEU (Papineni et al., 2002) with significance test (Koehn, 2004) and METEOR (Lavie and Agarwal, 2007). MERT (Och, 2003) (BLEU based) is run three times for each system and the average BLEU/METEOR scores are recorded. 4-layer CSTM (Schwenk, 2012) are applied to NN translation models: phrase length limit is set as seven, shared projection layer of dimension 320 for each word (that is 2240 for seven words), projection layer of dimension 768, hidden layer of dimension 512. The dimensions of input/output layers for both in/out-of-domain CSTMs follows the size of vocabularies of source/target words from in-domain corpora. That is 72K/57K for IWSLT and 149/112K for NIST. Since out-of-domain corpora"
C16-1295,D12-1088,0,0.0610705,"Missing"
C16-1295,N13-1074,0,0.0171997,"ee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT p"
C16-1295,C14-1105,0,0.0122047,"problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific model. ∗ Corresponding authors. H. Zhao and B. L. Lu were partially"
C16-1295,P10-2041,0,0.0842928,"evel selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwh"
C16-1295,2012.iwslt-papers.3,0,0.0240338,"e used as they are. This penalty setting is similar to (Bisazza et al., 2011). Penalty weights, together with all of existing score weights, will be further tuned by MERT (Och, 2003). The phrase pairs in re-ordering model are selected using the same way as PT. The selected monolingual n-grams are added to the original LM, and the probabilities are re-normalized by SRILM (Stolcke, 2002; Stolcke et al., 2011). 4 Experiments 4.1 Data sets The proposed methods are evaluated on two data sets. 1) IWSLT 2014 French (FR) to English (EN) corpus4 is used as in-domain data and dev2010 and test2010/2011 (Niehues and Waibel, 2012), are selected as development (dev) and test data, respectively. Out-of-domain corpora contain Common Crawl, Europarl v7, News Commentary v10 and United Nation (UN) FR-EN parallel corpora5 . 2) NIST 2006 Chinese (CN) to English corpus6 is used as in-domain corpus, which follows the setting of (Wang et al., 2014b) and mainly consists of news and blog texts. Chinese to English UN data set (LDC2013T06) and NTCIR-9 (Goto et al., 2011) patent data are used as out-of-domain bilingual (Bil) parallel corpora. The English patent data in NTCIR-8 (Fujii et al., 2010) is also used as additional out-of-dom"
C16-1295,P03-1021,0,0.0187056,"ts will be shown in Section 5.4. 3.3 Integration into SMT The thresholds of Pop and Dminus are tuned using development data. Selected phrase pairs are added into the in-domain PT. Because they are not so useful as the in-domain ones, a penalty score is added. For in-domain phrase pairs, the penalty is set as 1; for the out-of-domain ones the penalty is set as e (= 2.71828...). Other phrase scores (lexical weights et. al.) are used as they are. This penalty setting is similar to (Bisazza et al., 2011). Penalty weights, together with all of existing score weights, will be further tuned by MERT (Och, 2003). The phrase pairs in re-ordering model are selected using the same way as PT. The selected monolingual n-grams are added to the original LM, and the probabilities are re-normalized by SRILM (Stolcke, 2002; Stolcke et al., 2011). 4 Experiments 4.1 Data sets The proposed methods are evaluated on two data sets. 1) IWSLT 2014 French (FR) to English (EN) corpus4 is used as in-domain data and dev2010 and test2010/2011 (Niehues and Waibel, 2012), are selected as development (dev) and test data, respectively. Out-of-domain corpora contain Common Crawl, Europarl v7, News Commentary v10 and United Nati"
C16-1295,P02-1040,0,0.0955457,"ics on data sets (‘B’ for billions). 3 NN based methods have been applied to a series of NLP tasks, such as Chinese word segmentation and parsing (Cai and Zhao, 2016; Zhang et al., 2016). 4 https://wit3.fbk.eu/mt.php?release=2014-01 5 http://statmt.org/wmt15/translation-task.html 6 http://www.itl.nist.gov/iad/mig/tests/mt/2006/ 3138 4.2 Common Setting The basic settings of IWSLT-2014 FR to EN and NIST-06 CN to EN phrase based translation baseline systems are followed. 5-gram interpolated KN (Kneser and Ney, 1995) LMs are trained. Translation performances are measured by case-insensitive BLEU (Papineni et al., 2002) with significance test (Koehn, 2004) and METEOR (Lavie and Agarwal, 2007). MERT (Och, 2003) (BLEU based) is run three times for each system and the average BLEU/METEOR scores are recorded. 4-layer CSTM (Schwenk, 2012) are applied to NN translation models: phrase length limit is set as seven, shared projection layer of dimension 320 for each word (that is 2240 for seven words), projection layer of dimension 768, hidden layer of dimension 512. The dimensions of input/output layers for both in/out-of-domain CSTMs follows the size of vocabularies of source/target words from in-domain corpora. Tha"
C16-1295,C12-2104,0,0.173123,"and Pout (E|F ) by N N T Mout should be lower. This hypothesis is partially motivated by (Axelrod et al., 2011), which use bilingual cross-entropy difference to distinguish in-domain and out-of-domain data. The translation probability of a phrase-pair is estimated as, P (E|F ) = P (e1 , ..., eq |f1 , ..., fp ), (1) where fs (s ∈ [1, p]) and et (t ∈ [1, q]) are source and target words, respectively. Originally, P (e1 , ..., eq |f1 , ..., fp ) = q ∏ P (ek |e1 , ..., ek−1 , f1 , ...fp ). (2) k=1 The structure of NN based translation model is similar to Continuous Space Translation Model (CSTM) (Schwenk, 2012). For the purpose of adaptation, the dependence between target words is dropped2 and the probabilities of different length target phrase are normalized. For an incomplete source phrase, i.e. with less than seven words, we set the projections of the missing words to zero. The normalized translation probability Q(E|F ) can be approximately computed by the following equation, v u q u∏ q P (ek |f1 , ...fp ). Q(E|F ) ≈ t (3) k=1 Finally, the minus Dminus (E|F ) is used to rank connecting phrase pairs from mix-domain PT, Dminus (E|F ) = Qin (E|F ) − Qin (E|F ). (4) where Qin (E|F ) and Qin (E|F ) ar"
C16-1295,P13-1082,0,0.203536,"). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific model. ∗ Corresponding authors. H. Zhao and B."
C16-1295,E12-1055,0,0.0662449,"input/output layers for both in/out-of-domain CSTMs follows the size of vocabularies of source/target words from in-domain corpora. That is 72K/57K for IWSLT and 149/112K for NIST. Since out-of-domain corpora are huge, part of them are resampled (resample coefficient 0.01 for IWSLT and NIST). Several related existing methods are selected as baselines7 : Koehn and Schroeder (2007)’s method for using two (in and out-of-domain) TMs and LMs together, entropy based method for TM (Ling et al., 2012) and LM (Stolcke, 1998) adaptation (pruning), (Duh et al., 2013) for NNLM based sentence adaptation, (Sennrich, 2012) for TM weights combination, and (Bisazza et al., 2011) for TM fill-up. In Table Tables 2 and 3, ‘in-domain’, ‘out-of-domain’ and ‘mix-domain’ indicate training all models using corresponding corpora, ‘in+NN’ indicates applying NN based adaptation directly for all phrases, and ‘in+connect’ indicates adding all connecting phrases and n-grams to in-domain PT and LM, respectively. For tuning methods, ‘in+connect+OP/NN’ indicates tuning connecting phrase pairs and n-grams using Occurring Probability (OP) and NN, respectively. Only the best preforming systems (for both the baselines and proposed me"
C16-1295,D14-1023,1,0.921449,"Missing"
C16-1295,P14-2122,1,0.886787,"ails: http://creativecommons.org/licenses/by/4.0/ 3135 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3135–3145, Osaka, Japan, December 11-17 2016. Instead of focusing on sentence selection or single model adaptation, we propose a phrase adaptation method, which is applied to both bilingual phrase pair and monolingual n-gram selection. It is based on a linguistic observation that the translation hypotheses of a phrase-based SMT system are concatenations of phrases from Phrase Table (PT), which has been applied to LM growing (Wang et al., 2014a; Wang et al., 2015). As a straightforward linear method, it is much efficient in comparison with NN based non-linear methods. The remainder of this paper is organized as follows. Section 2 will introduce the connecting phrase based adaptation method. The size of adapted connecting phrase will be tuned in Section 3. Empirical results will be shown in Section 4. We will discuss the methods and conduct extension experiments in Section 5. The last section will conclude this paper. 2 Connecting Phrase based Adaptation Suppose that two phrases ‘would like to learn’ and ‘Chinese as second language’"
C16-1295,P16-1131,1,0.811994,"Missing"
C16-1295,W14-3348,0,\N,Missing
C16-1295,C12-1010,0,\N,Missing
C18-1111,D16-1162,0,0.024687,"Incorporation How to use external knowledge such as dictionaries and knowledge bases for NMT remains a big research question. In domain adaptation, the use of domain specific dictionaries is a very crucial problem. In the practical perspective, many translation companies have created domain specific dictionaries but not domain specific corpora. If we can study a good way to use domain specific dictionaries, it will significantly promote the practical use of MT. There are some studies that try to use dictionaries for NMT, but the usage is limited to help low frequent or rare word translation (Arthur et al., 2016; Zhang and Zong, 2016a). Arcan and Buitelaar (2017) use a domain specific dictionary for terminology translation, 1312 but they simply apply the unknown word replacement method proposed by Luong et al. (2015), which suffers from noisy attention. 6.3 Multilingual and Multi-Domain Adaptation It may not always be possible to use an out-of-domain parallel corpus in the same language pair and thus it is important to use data from other languages (Johnson et al., 2016). This approach is known as crosslingual transfer learning, which transfers NMT model parameters among multiple languages. It is kno"
C18-1111,D11-1033,0,0.671459,"rcome the problem of the lack of substantial data in specific domains and languages. Most SMT domain adaptation methods can be broken down broadly into two main categories: 3.1 Data Centric This category focuses on selecting or generating the domain-related data using existing in-domain data. i) When there are sufficient parallel corpora from other domains, the main idea is to score the outdomain data using models trained from the in-domain and out-of-domain data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and parallel phrase pairs (C"
C18-1111,2010.amta-papers.16,0,0.0586287,"tences is crucial for good translation. To address this problem, a common method in SMT is to firstly classify the domains and then translate input sentences in classified domains using corresponding models (Huck et al., 2015). Xu et al. (2007) perform domain classification for a Chinese-English translation task. The classifiers operate on whole documents rather than on individual sentences, using LM interpolation and vocabulary similarities. Huck et al. (2015) extend the work of Xu et al. (2007) on the sentence level. They use LMs and maximum entropy classifiers to predict the target domain. Banerjee et al. (2010) build a support vector machine classifier using tf-idf features over bigrams of stemmed content words. Classification is carried out on the level of individual sentences. Wang et al. (2012) rely on averaged perceptron classifiers with various phrase-based features. For NMT, Kobus et al. (2016) propose an NMT domain control method, by appending either domain tags or features to the word embedding layer of NMT. They adopt an in-house classifier to distinguish the domain information. Li et al. (2016) propose to search similar sentences in the training data using the test sentence as a query, and"
C18-1111,2011.iwslt-evaluation.18,0,0.0265031,"tion or generation that are not related to NMT. Therefore, these methods can only achieve modest improvements in NMT (Wang et al., 2017a). 1307 Target Synthetic Source-Target Translate NMT Figure 3: Synthetic data generation for NMT (Sennrich et al., 2016b). 3.2 Model Centric This category focuses on interpolating the models from different domains. i) Model level interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re"
C18-1111,W17-4712,0,0.328656,"s is the first comprehensive survey of domain adaptation for NMT. In this paper, similar to SMT, we categorize domain adaptation for NMT into two main categories: data centric and model centric. The data centric category focuses on the data being used rather than specialized models for domain adaptation. The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; G¨ulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ulc¸ehre et al., 2015; Dakwale and Monz, 2017; Khayral"
C18-1111,2015.iwslt-evaluation.1,0,0.0685094,"Missing"
C18-1111,2016.amta-researchers.8,0,0.0555854,"1 Data Centric This category focuses on selecting or generating the domain-related data using existing in-domain data. i) When there are sufficient parallel corpora from other domains, the main idea is to score the outdomain data using models trained from the in-domain and out-of-domain data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and parallel phrase pairs (Chu, 2015; Wang et al., 2016). Most of the data centric-based methods in SMT can be directly applied to NMT. However, most of these methods adopt the criteria of data selecti"
C18-1111,W17-3205,0,0.462343,"monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; G¨ulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ulc¸ehre et al., 2015; Dakwale and Monz, 2017; Khayrallah et al., 2017). An overview of these two categories is shown in Figure 1. Note that as model centric methods also use either monolingual or parallel corpora, there are overlaps between these two categories. The remainder of this paper is structured as follows: We first give a brief introduction of NMT, and describe the reason for the diff"
C18-1111,P17-1110,0,0.254114,"monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; G¨ulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ulc¸ehre et al., 2015; Dakwale and Monz, 2017; Khayrallah et al., 2017). An overview of these two categories is shown in Figure 1. Note that as model centric methods also use either monolingual or parallel corpora, there are overlaps between these two categories. The remainder of this paper is structured as follows: We first give a brief introduction of NMT, and describe the reason for the diff"
C18-1111,P16-1185,0,0.0525452,"have been done in the perspective of computer vision (Csurka, 2017) and machine learning (Pan and Yang, 2010; Weiss et al., 2016). However, such survey has not been done for NMT. To the best of our knowledge, this is the first comprehensive survey of domain adaptation for NMT. In this paper, similar to SMT, we categorize domain adaptation for NMT into two main categories: data centric and model centric. The data centric category focuses on the data being used rather than specialized models for domain adaptation. The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu e"
C18-1111,D14-1179,0,0.0198359,"Missing"
C18-1111,P17-2061,1,0.891539,"d Knowles, 2017). Leveraging out-of-domain parallel corpora and in-domain monolingual corpora to improve indomain translation is known as domain adaptation for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel corpus has 1M sentence pairs (Goto et al., 2013), but for the spoken language domain parallel corpus there are only 200k sentences available (Cettolo et al., 2015). MT typically performs poorly in a resource poor or domain mismatching scenario and thus it is important to leverage the spoken language domain data with the patent domain data (Chu et al., 2017). Furthermore, there are monolingual corpora containing millions of sentences for the spoken language domain, which can also be leveraged (Sennrich et al., 2016b). There are many studies of domain adaptation for SMT, which can be mainly divided into two categories: data centric and model centric. Data centric methods focus on either selecting training data from out-of-domain parallel corpora based a language model (LM) (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014; Durrani et al., 2015; Chen et al., 2016) or generating pseudo parallel data (Utiyama and"
C18-1111,D17-1158,0,0.28159,"uter vision (Csurka, 2017) and machine learning (Pan and Yang, 2010; Weiss et al., 2016). However, such survey has not been done for NMT. To the best of our knowledge, this is the first comprehensive survey of domain adaptation for NMT. In this paper, similar to SMT, we categorize domain adaptation for NMT into two main categories: data centric and model centric. The data centric category focuses on the data being used rather than specialized models for domain adaptation. The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the N"
C18-1111,P13-2119,0,0.149871,"rich scenarios (Bojar et al., 2017; Nakazawa et al., 2017). However, currently, high quality parallel corpora of sufficient size are only available for a few language pairs such as languages paired with English and several European language pairs. Furthermore, for each language pair the sizes of the domain specific corpora and the number of domains available are limited. As such, for the majority of language pairs and domains, only few or no parallel corpora are available. It has been known that both vanilla SMT and NMT perform poorly for domain specific translation in low resource scenarios (Duh et al., 2013; Sennrich et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). High quality domain specific machine translation (MT) systems are in high demand whereas general purpose MT has limited applications. In addition, general purpose translation systems usually perform poorly and hence it is important to develop translation systems for specific domains (Koehn and Knowles, 2017). Leveraging out-of-domain parallel corpora and in-domain monolingual corpora to improve indomain translation is known as domain adaptation for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English pa"
C18-1111,2015.mtsummit-papers.10,0,0.361618,"MT domain adaptation methods can be broken down broadly into two main categories: 3.1 Data Centric This category focuses on selecting or generating the domain-related data using existing in-domain data. i) When there are sufficient parallel corpora from other domains, the main idea is to score the outdomain data using models trained from the in-domain and out-of-domain data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and parallel phrase pairs (Chu, 2015; Wang et al., 2016). Most of the data centric-based methods in SMT can be directly"
C18-1111,P08-1115,0,0.0405479,"lc¸ehre et al., 2015), the next word hypotheses generated by an NMT model is rescored by the weighted sum of the NMT and RNNLM probabilities (Figure 8). Ensembling Freitag and Al-Onaizan (2016) propose to ensemble the out-of-domain domain and the fine tuned in-domain models. Their motivation is exactly the same as the work of Dakwale and Monz (2017), which is preventing degradation of out-of-domain translation after fine tuning on in-domain data. Neural Lattice Search Khayrallah et al. (2017) propose a stack-based decoding algorithm over word lattices, while the lattices are generated by SMT (Dyer et al., 2008). In their domain adaptation experiments, they show that stack-based decoding is better than conventional decoding. 5 Domain Adaptation in Real-World Scenarios A domain adaptation method should be adopted according to the certain scenarios. For example, when there are some pseudo parallel in-domain data in the out-of-domain data, sentence selection is preferred; when only additional monolingual data is available, LM and NMT fusion can be adopted. In many cases, both out-of-domain parallel data and monolingual in-domain data are available, making the combination 1311 .&/01&apos; *,&1,&(,* 2,&1,&(,*&apos;"
C18-1111,W17-4713,0,0.049474,"Classification is carried out on the level of individual sentences. Wang et al. (2012) rely on averaged perceptron classifiers with various phrase-based features. For NMT, Kobus et al. (2016) propose an NMT domain control method, by appending either domain tags or features to the word embedding layer of NMT. They adopt an in-house classifier to distinguish the domain information. Li et al. (2016) propose to search similar sentences in the training data using the test sentence as a query, and then fine tune the NMT model using the retrieved training sentences for translating the test sentence. Farajian et al. (2017) follow the strategy of Li et al. (2016), but propose to dynamically set the hyperparameters (i.e., learning rate and number of epochs) of the learning algorithm based on the similarity of the input sentence and the retrieved sentences for updating the NMT model. Figure 9 shows an overview of domain adaptation for MT in the input domain unknown scenario. 6 6.1 Future Directions Domain Adaptation for State-of-the-art NMT Architectures Since the success of RNN based NMT (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), other architectures of NMT have been developed. One represen"
C18-1111,N16-1101,0,0.0279015,"3 Multilingual and Multi-Domain Adaptation It may not always be possible to use an out-of-domain parallel corpus in the same language pair and thus it is important to use data from other languages (Johnson et al., 2016). This approach is known as crosslingual transfer learning, which transfers NMT model parameters among multiple languages. It is known that a multilingual model, which relies on parameter sharing, helps in improving the translation quality for low resource languages especially when the target language is the same (Zoph et al., 2016). There are studies where either multilingual (Firat et al., 2016; Johnson et al., 2017) or multi-domain models (Sajjad et al., 2017) are trained, but none that attempt to package multiple language pairs and multiple domains into a single translation system. Even if out-of-domain data in the same language pair exists, it is possible that using both multilingual and multi-domain data can boost the translation performance. Therefore, we think that multilingual and multi-domain adaptation for NMT can be another future direction. Chu and Dabre (2018) conduct a preliminary study for this topic. 6.4 Adversarial Domain Adaptation and Domain Generation Generative a"
C18-1111,W07-0717,0,0.0696966,"criteria of data selection or generation that are not related to NMT. Therefore, these methods can only achieve modest improvements in NMT (Wang et al., 2017a). 1307 Target Synthetic Source-Target Translate NMT Figure 3: Synthetic data generation for NMT (Sennrich et al., 2016b). 3.2 Model Centric This category focuses on interpolating the models from different domains. i) Model level interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight"
C18-1111,D10-1044,0,0.0359406,"els from different domains. i) Model level interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed to interpolate model/data like SMT does. For modellevel interpolation, the most related NMT technique is model ensemble (Jean et al., 2015). For instancelevel interpolation, the most related method is to assign a weight in N"
C18-1111,C14-1182,0,0.0639158,"Missing"
C18-1111,2015.mtsummit-papers.19,0,0.0583303,"nd any particular techniques in this paper but recommend readers to choose the best method for their own scenarios. Most of the above domain adaptation studies assume that the domain of the data is given. However, in a practical view such as an online translation engine, the domain of the sentences input by the users are not given. For such scenario, predicting the domains of the input sentences is crucial for good translation. To address this problem, a common method in SMT is to firstly classify the domains and then translate input sentences in classified domains using corresponding models (Huck et al., 2015). Xu et al. (2007) perform domain classification for a Chinese-English translation task. The classifiers operate on whole documents rather than on individual sentences, using LM interpolation and vocabulary similarities. Huck et al. (2015) extend the work of Xu et al. (2007) on the sentence level. They use LMs and maximum entropy classifiers to predict the target domain. Banerjee et al. (2010) build a support vector machine classifier using tf-idf features over bigrams of stemmed content words. Classification is carried out on the level of individual sentences. Wang et al. (2012) rely on avera"
C18-1111,2016.amta-researchers.7,0,0.0292902,"est improvements in NMT (Wang et al., 2017a). 1307 Target Synthetic Source-Target Translate NMT Figure 3: Synthetic data generation for NMT (Sennrich et al., 2016b). 3.2 Model Centric This category focuses on interpolating the models from different domains. i) Model level interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed"
C18-1111,Q13-1035,0,0.0256294,"n discriminator (Tzeng et al., 2017). They have been applied to domain adaptation tasks in computer vision and machine learning (Tzeng et al., 2017; Motiian et al., 2017; Volpi et al., 2017; Zhao et al., 2017; Pei et al., 2018). Recently, some of the adversarial methods began to be introduced into some NLP tasks (Liu et al., 2017; Chen et al., 2017b) and NMT (Britz et al., 2017). Most of the existing methods focus on adapting from a general domain into a specific domain. In the real scenario, training data and test data have different distributions and the target domains are sometimes unseen. Irvine et al. (2013) analyze the translation errors in such scenarios. Domain generalization aims to apply knowledge gained from labeled source domains to unseen target domains (Li et al., 2018). It provides a way to match the distribution of training data and test data in real-world MT, which may be a future trend of domain adaptation for NMT. 7 Conclusion Domain adaptation for NMT is a rather new but very important research topic to promote MT for practical use. In this paper, we gave the first comprehensive review of the techniques mainly being developed in the last two years. We compared domain adaptation tec"
C18-1111,P15-1001,0,0.0355654,"in adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed to interpolate model/data like SMT does. For modellevel interpolation, the most related NMT technique is model ensemble (Jean et al., 2015). For instancelevel interpolation, the most related method is to assign a weight in NMT objective function (Chen et al., 2017a; Wang et al., 2017b). However, the model structures of SMT and NMT are quite different. SMT is a combination of several independent models; in comparison, NMT is an integral model itself. Therefore, most of these methods cannot be directly applied to NMT. 4 4.1 Domain Adaptation for NMT Data Centric 4.1.1 Using Monolingual Corpora Unlike SMT, in-domain monolingual data cannot be used as an LM for conventional NMT directly, and many studies have been conducted for this."
C18-1111,P07-1034,0,0.108018,".2 Model Centric This category focuses on interpolating the models from different domains. i) Model level interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed to interpolate model/data like SMT does. For modellevel interpolation, the most related NMT technique is model ensemble (Jean et al., 2015). For instancelevel inte"
C18-1111,I17-2004,0,0.09754,"., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; G¨ulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ulc¸ehre et al., 2015; Dakwale and Monz, 2017; Khayrallah et al., 2017). An overview of these two categories is shown in Figure 1. Note that as model centric methods also use either monolingual or parallel corpora, there are overlaps between these two categories. The remainder of this paper is structured as follows: We first give a brief introduction of NMT, and describe the reason for the difficulty of low resource domains and languages in NMT (Section 2); Next, we briefly review the historical domain adaptation techniques being developed for SMT (Section 3); Under these background knowledge, we then present and compare the domain adaptation methods for 1305 Fig"
C18-1111,W17-3204,0,0.0549232,"However, currently, high quality parallel corpora of sufficient size are only available for a few language pairs such as languages paired with English and several European language pairs. Furthermore, for each language pair the sizes of the domain specific corpora and the number of domains available are limited. As such, for the majority of language pairs and domains, only few or no parallel corpora are available. It has been known that both vanilla SMT and NMT perform poorly for domain specific translation in low resource scenarios (Duh et al., 2013; Sennrich et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). High quality domain specific machine translation (MT) systems are in high demand whereas general purpose MT has limited applications. In addition, general purpose translation systems usually perform poorly and hence it is important to develop translation systems for specific domains (Koehn and Knowles, 2017). Leveraging out-of-domain parallel corpora and in-domain monolingual corpora to improve indomain translation is known as domain adaptation for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel corpus has 1M sentence pairs (Goto et al., 2013"
C18-1111,P07-2045,0,0.0069565,"both out-of-domain parallel corpora as well as monolingual corpora for in-domain translation, is very important for domainspecific translation. In this paper, we give a comprehensive survey of the state-of-the-art domain adaptation techniques for NMT. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) allows for end-to-end training of a translation system without the need to deal with word alignments, translation rules and complicated decoding algorithms, which are characteristics of statistical machine translation (SMT) systems (Koehn et al., 2007). NMT yields the state-of-the-art translation performance in resource rich scenarios (Bojar et al., 2017; Nakazawa et al., 2017). However, currently, high quality parallel corpora of sufficient size are only available for a few language pairs such as languages paired with English and several European language pairs. Furthermore, for each language pair the sizes of the domain specific corpora and the number of domains available are limited. As such, for the majority of language pairs and domains, only few or no parallel corpora are available. It has been known that both vanilla SMT and NMT perf"
C18-1111,W11-2132,0,0.0630701,"models trained from the in-domain and out-of-domain data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and parallel phrase pairs (Chu, 2015; Wang et al., 2016). Most of the data centric-based methods in SMT can be directly applied to NMT. However, most of these methods adopt the criteria of data selection or generation that are not related to NMT. Therefore, these methods can only achieve modest improvements in NMT (Wang et al., 2017a). 1307 Target Synthetic Source-Target Translate NMT Figure 3: Synthetic data generation for NMT (Sennri"
C18-1111,P17-1001,0,0.0288747,"ed in unsupervised machine learning, which are introduced by (Goodfellow et al., 2014). Adversarial methods have become popular in domain adaptation (Ganin et al., 2016), which minimize an approximate domain discrepancy distance through an adversarial objective with respect to a domain discriminator (Tzeng et al., 2017). They have been applied to domain adaptation tasks in computer vision and machine learning (Tzeng et al., 2017; Motiian et al., 2017; Volpi et al., 2017; Zhao et al., 2017; Pei et al., 2018). Recently, some of the adversarial methods began to be introduced into some NLP tasks (Liu et al., 2017; Chen et al., 2017b) and NMT (Britz et al., 2017). Most of the existing methods focus on adapting from a general domain into a specific domain. In the real scenario, training data and test data have different distributions and the target domains are sometimes unseen. Irvine et al. (2013) analyze the translation errors in such scenarios. Domain generalization aims to apply knowledge gained from labeled source domains to unseen target domains (Li et al., 2018). It provides a way to match the distribution of training data and test data in real-world MT, which may be a future trend of domain adap"
C18-1111,2015.iwslt-evaluation.11,0,0.0985932,"uses on the data being used rather than specialized models for domain adaptation. The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; G¨ulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ulc¸ehre et al., 2015; Dakwale and Monz, 2017; Khayrallah et al., 2017). An overview of these two categories is shown in Figure 1. Note that as model centric methods also use either monolingual or parallel corpora, there are overlaps between these two categories. The remainder of th"
C18-1111,P15-1002,0,0.0245704,"oblem. In the practical perspective, many translation companies have created domain specific dictionaries but not domain specific corpora. If we can study a good way to use domain specific dictionaries, it will significantly promote the practical use of MT. There are some studies that try to use dictionaries for NMT, but the usage is limited to help low frequent or rare word translation (Arthur et al., 2016; Zhang and Zong, 2016a). Arcan and Buitelaar (2017) use a domain specific dictionary for terminology translation, 1312 but they simply apply the unknown word replacement method proposed by Luong et al. (2015), which suffers from noisy attention. 6.3 Multilingual and Multi-Domain Adaptation It may not always be possible to use an out-of-domain parallel corpus in the same language pair and thus it is important to use data from other languages (Johnson et al., 2016). This approach is known as crosslingual transfer learning, which transfers NMT model parameters among multiple languages. It is known that a multilingual model, which relies on parameter sharing, helps in improving the translation quality for low resource languages especially when the target language is the same (Zoph et al., 2016). There"
C18-1111,2012.iwslt-papers.7,0,0.0152304,"el interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed to interpolate model/data like SMT does. For modellevel interpolation, the most related NMT technique is model ensemble (Jean et al., 2015). For instancelevel interpolation, the most related method is to assign a weight in NMT objective function (Chen et al., 2017a;"
C18-1111,P17-2062,0,0.014107,"n data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and parallel phrase pairs (Chu, 2015; Wang et al., 2016). Most of the data centric-based methods in SMT can be directly applied to NMT. However, most of these methods adopt the criteria of data selection or generation that are not related to NMT. Therefore, these methods can only achieve modest improvements in NMT (Wang et al., 2017a). 1307 Target Synthetic Source-Target Translate NMT Figure 3: Synthetic data generation for NMT (Sennrich et al., 2016b). 3.2 Model Centric This category fo"
C18-1111,D09-1074,0,0.0380629,"on interpolating the models from different domains. i) Model level interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed to interpolate model/data like SMT does. For modellevel interpolation, the most related NMT technique is model ensemble (Jean et al., 2015). For instancelevel interpolation, the most related method is to"
C18-1111,D17-1156,0,0.061316,"7; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; G¨ulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ulc¸ehre et al., 2015; Dakwale and Monz, 2017; Khayrallah et al., 2017). An overview of these two categories is shown in Figure 1. Note that as model centric methods also use either monolingual or parallel corpora, there are overlaps between these two categories. The remainder of this paper is structured as follows: We first give a brief introduction of NMT, and describe the reason for the difficulty of low resource domains and languages in NMT (Section 2); Next, we briefly rev"
C18-1111,P10-2041,0,0.105905,"ve been proposed to overcome the problem of the lack of substantial data in specific domains and languages. Most SMT domain adaptation methods can be broken down broadly into two main categories: 3.1 Data Centric This category focuses on selecting or generating the domain-related data using existing in-domain data. i) When there are sufficient parallel corpora from other domains, the main idea is to score the outdomain data using models trained from the in-domain and out-of-domain data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and pa"
C18-1111,W17-5701,0,0.0369516,"ecific translation. In this paper, we give a comprehensive survey of the state-of-the-art domain adaptation techniques for NMT. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) allows for end-to-end training of a translation system without the need to deal with word alignments, translation rules and complicated decoding algorithms, which are characteristics of statistical machine translation (SMT) systems (Koehn et al., 2007). NMT yields the state-of-the-art translation performance in resource rich scenarios (Bojar et al., 2017; Nakazawa et al., 2017). However, currently, high quality parallel corpora of sufficient size are only available for a few language pairs such as languages paired with English and several European language pairs. Furthermore, for each language pair the sizes of the domain specific corpora and the number of domains available are limited. As such, for the majority of language pairs and domains, only few or no parallel corpora are available. It has been known that both vanilla SMT and NMT perform poorly for domain specific translation in low resource scenarios (Duh et al., 2013; Sennrich et al., 2013; Zoph et al., 2016"
C18-1111,2012.amta-papers.19,0,0.0364559,"t are not related to NMT. Therefore, these methods can only achieve modest improvements in NMT (Wang et al., 2017a). 1307 Target Synthetic Source-Target Translate NMT Figure 3: Synthetic data generation for NMT (Sennrich et al., 2016b). 3.2 Model Centric This category focuses on interpolating the models from different domains. i) Model level interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 20"
C18-1111,2011.iwslt-evaluation.10,0,0.0475759,"Missing"
C18-1111,E17-2045,0,0.0482425,"of our knowledge, this is the first comprehensive survey of domain adaptation for NMT. In this paper, similar to SMT, we categorize domain adaptation for NMT into two main categories: data centric and model centric. The data centric category focuses on the data being used rather than specialized models for domain adaptation. The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; G¨ulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ulc¸ehre et al., 2015; Dakwale and"
C18-1111,P13-1082,0,0.174721,"jar et al., 2017; Nakazawa et al., 2017). However, currently, high quality parallel corpora of sufficient size are only available for a few language pairs such as languages paired with English and several European language pairs. Furthermore, for each language pair the sizes of the domain specific corpora and the number of domains available are limited. As such, for the majority of language pairs and domains, only few or no parallel corpora are available. It has been known that both vanilla SMT and NMT perform poorly for domain specific translation in low resource scenarios (Duh et al., 2013; Sennrich et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). High quality domain specific machine translation (MT) systems are in high demand whereas general purpose MT has limited applications. In addition, general purpose translation systems usually perform poorly and hence it is important to develop translation systems for specific domains (Koehn and Knowles, 2017). Leveraging out-of-domain parallel corpora and in-domain monolingual corpora to improve indomain translation is known as domain adaptation for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel co"
C18-1111,N16-1005,0,0.393461,"for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel corpus has 1M sentence pairs (Goto et al., 2013), but for the spoken language domain parallel corpus there are only 200k sentences available (Cettolo et al., 2015). MT typically performs poorly in a resource poor or domain mismatching scenario and thus it is important to leverage the spoken language domain data with the patent domain data (Chu et al., 2017). Furthermore, there are monolingual corpora containing millions of sentences for the spoken language domain, which can also be leveraged (Sennrich et al., 2016b). There are many studies of domain adaptation for SMT, which can be mainly divided into two categories: data centric and model centric. Data centric methods focus on either selecting training data from out-of-domain parallel corpora based a language model (LM) (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014; Durrani et al., 2015; Chen et al., 2016) or generating pseudo parallel data (Utiyama and Isahara, 2003; Wang et al., 2014; Chu, 2015; Wang et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons"
C18-1111,P16-1009,0,0.711904,"for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel corpus has 1M sentence pairs (Goto et al., 2013), but for the spoken language domain parallel corpus there are only 200k sentences available (Cettolo et al., 2015). MT typically performs poorly in a resource poor or domain mismatching scenario and thus it is important to leverage the spoken language domain data with the patent domain data (Chu et al., 2017). Furthermore, there are monolingual corpora containing millions of sentences for the spoken language domain, which can also be leveraged (Sennrich et al., 2016b). There are many studies of domain adaptation for SMT, which can be mainly divided into two categories: data centric and model centric. Data centric methods focus on either selecting training data from out-of-domain parallel corpora based a language model (LM) (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014; Durrani et al., 2015; Chen et al., 2016) or generating pseudo parallel data (Utiyama and Isahara, 2003; Wang et al., 2014; Chu, 2015; Wang et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons"
C18-1111,P16-1162,0,0.886962,"for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel corpus has 1M sentence pairs (Goto et al., 2013), but for the spoken language domain parallel corpus there are only 200k sentences available (Cettolo et al., 2015). MT typically performs poorly in a resource poor or domain mismatching scenario and thus it is important to leverage the spoken language domain data with the patent domain data (Chu et al., 2017). Furthermore, there are monolingual corpora containing millions of sentences for the spoken language domain, which can also be leveraged (Sennrich et al., 2016b). There are many studies of domain adaptation for SMT, which can be mainly divided into two categories: data centric and model centric. Data centric methods focus on either selecting training data from out-of-domain parallel corpora based a language model (LM) (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014; Durrani et al., 2015; Chen et al., 2016) or generating pseudo parallel data (Utiyama and Isahara, 2003; Wang et al., 2014; Chu, 2015; Wang et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons"
C18-1111,W10-1759,0,0.027937,"nd Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed to interpolate model/data like SMT does. For modellevel interpolation, the most related NMT technique is model ensemble (Jean et al., 2015). For instancelevel interpolation, the most related method is to assign a weight in NMT objective function (Chen et al., 2017a; Wang et al., 2017b). However, the model structures of SMT and NMT are quite different. SMT is a combination of several independent models; in comparison, NMT is an integral model itself. Therefore, most of these methods cannot be directly applied to NMT. 4 4.1 Do"
C18-1111,2012.amta-papers.21,0,0.0144443,"mains. i) Model level interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed to interpolate model/data like SMT does. For modellevel interpolation, the most related NMT technique is model ensemble (Jean et al., 2015). For instancelevel interpolation, the most related method is to assign a weight in NMT objective functi"
C18-1111,P03-1010,0,0.0159135,"in idea is to score the outdomain data using models trained from the in-domain and out-of-domain data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and parallel phrase pairs (Chu, 2015; Wang et al., 2016). Most of the data centric-based methods in SMT can be directly applied to NMT. However, most of these methods adopt the criteria of data selection or generation that are not related to NMT. Therefore, these methods can only achieve modest improvements in NMT (Wang et al., 2017a). 1307 Target Synthetic Source-Target Translate NMT Figure 3: Sy"
C18-1111,D17-1147,0,0.0517826,"Missing"
C18-1111,2012.amta-papers.18,0,0.0196613,"nding models (Huck et al., 2015). Xu et al. (2007) perform domain classification for a Chinese-English translation task. The classifiers operate on whole documents rather than on individual sentences, using LM interpolation and vocabulary similarities. Huck et al. (2015) extend the work of Xu et al. (2007) on the sentence level. They use LMs and maximum entropy classifiers to predict the target domain. Banerjee et al. (2010) build a support vector machine classifier using tf-idf features over bigrams of stemmed content words. Classification is carried out on the level of individual sentences. Wang et al. (2012) rely on averaged perceptron classifiers with various phrase-based features. For NMT, Kobus et al. (2016) propose an NMT domain control method, by appending either domain tags or features to the word embedding layer of NMT. They adopt an in-house classifier to distinguish the domain information. Li et al. (2016) propose to search similar sentences in the training data using the test sentence as a query, and then fine tune the NMT model using the retrieved training sentences for translating the test sentence. Farajian et al. (2017) follow the strategy of Li et al. (2016), but propose to dynamic"
C18-1111,D14-1023,1,0.846543,"Ms (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and parallel phrase pairs (Chu, 2015; Wang et al., 2016). Most of the data centric-based methods in SMT can be directly applied to NMT. However, most of these methods adopt the criteria of data selection or generation that are not related to NMT. Therefore, these methods can only achieve modest improvements in NMT (Wang et al., 2017a). 1307 Target Synthetic Source-Target Translate NMT Figure 3: Synthetic data generation for NMT (Sennrich et al., 2016b). 3.2 Model Centric This category focuses on interpolating the models from different domains. i) Model level interpolation. Several SMT models,"
C18-1111,C16-1295,1,0.943601,"r domain specific translation in low resource scenarios (Duh et al., 2013; Sennrich et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). High quality domain specific machine translation (MT) systems are in high demand whereas general purpose MT has limited applications. In addition, general purpose translation systems usually perform poorly and hence it is important to develop translation systems for specific domains (Koehn and Knowles, 2017). Leveraging out-of-domain parallel corpora and in-domain monolingual corpora to improve indomain translation is known as domain adaptation for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel corpus has 1M sentence pairs (Goto et al., 2013), but for the spoken language domain parallel corpus there are only 200k sentences available (Cettolo et al., 2015). MT typically performs poorly in a resource poor or domain mismatching scenario and thus it is important to leverage the spoken language domain data with the patent domain data (Chu et al., 2017). Furthermore, there are monolingual corpora containing millions of sentences for the spoken language domain, which can also be leveraged (Sennrich et al., 2016b)."
C18-1111,P17-2089,1,0.867528,"Missing"
C18-1111,D17-1155,1,0.900044,"ehensive survey of domain adaptation for NMT. In this paper, similar to SMT, we categorize domain adaptation for NMT into two main categories: data centric and model centric. The data centric category focuses on the data being used rather than specialized models for domain adaptation. The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; G¨ulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ulc¸ehre et al., 2015; Dakwale and Monz, 2017; Khayrallah et al., 2017)."
C18-1111,C18-1269,0,0.0248969,"of linear models, which means the instance weight cannot be integrated into NMT directly. There is only one work concerning instance weighting in NMT (Wang et al., 2017b). They set a weight for the objective function, and this weight is learned from the cross-entropy by an indomain LM and an out-of-domain LM (Axelrod et al., 2011) (Figure 5). Instead of instance weighting, Chen et al. (2017a) modify the NMT cost function with a domain classifier. The output probability of the domain classifier is transferred into the domain weight. This classifier is trained using development data. Recently, Wang et al. (2018) proposed a joint framework of sentence selection and weighting for NMT. Fine Tuning Fine tuning is the conventional way for domain adaptation (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016). In this method, an NMT 1309 Sentence-1 Sentence-1 Score-1 Sentence-2 Score-2 … … Sentence-i Score-i … … Sentence-N Sentence-N Sentence-2 … Sentence-i Sentence Scoring by cross-entropy Sentence-1 Weight-1 Sentence-2 Weight-2 … … Sentence-i Weight-i … … … Score-N Sentence-N Weight-N Instance Weighting Figure 5: Instance weighting for NMT (Wang et al., 201"
C18-1111,2007.mtsummit-papers.68,0,0.100536,"chniques in this paper but recommend readers to choose the best method for their own scenarios. Most of the above domain adaptation studies assume that the domain of the data is given. However, in a practical view such as an online translation engine, the domain of the sentences input by the users are not given. For such scenario, predicting the domains of the input sentences is crucial for good translation. To address this problem, a common method in SMT is to firstly classify the domains and then translate input sentences in classified domains using corresponding models (Huck et al., 2015). Xu et al. (2007) perform domain classification for a Chinese-English translation task. The classifiers operate on whole documents rather than on individual sentences, using LM interpolation and vocabulary similarities. Huck et al. (2015) extend the work of Xu et al. (2007) on the sentence level. They use LMs and maximum entropy classifiers to predict the target domain. Banerjee et al. (2010) build a support vector machine classifier using tf-idf features over bigrams of stemmed content words. Classification is carried out on the level of individual sentences. Wang et al. (2012) rely on averaged perceptron cla"
C18-1111,D16-1160,0,0.233559,"ain adaptation surveys have been done in the perspective of computer vision (Csurka, 2017) and machine learning (Pan and Yang, 2010; Weiss et al., 2016). However, such survey has not been done for NMT. To the best of our knowledge, this is the first comprehensive survey of domain adaptation for NMT. In this paper, similar to SMT, we categorize domain adaptation for NMT into two main categories: data centric and model centric. The data centric category focuses on the data being used rather than specialized models for domain adaptation. The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale"
C18-1111,D16-1163,0,0.155622,"awa et al., 2017). However, currently, high quality parallel corpora of sufficient size are only available for a few language pairs such as languages paired with English and several European language pairs. Furthermore, for each language pair the sizes of the domain specific corpora and the number of domains available are limited. As such, for the majority of language pairs and domains, only few or no parallel corpora are available. It has been known that both vanilla SMT and NMT perform poorly for domain specific translation in low resource scenarios (Duh et al., 2013; Sennrich et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). High quality domain specific machine translation (MT) systems are in high demand whereas general purpose MT has limited applications. In addition, general purpose translation systems usually perform poorly and hence it is important to develop translation systems for specific domains (Koehn and Knowles, 2017). Leveraging out-of-domain parallel corpora and in-domain monolingual corpora to improve indomain translation is known as domain adaptation for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel corpus has 1M sentenc"
C18-1111,W17-4715,0,\N,Missing
C18-1111,W14-7001,0,\N,Missing
C18-1111,W17-4717,0,\N,Missing
C18-1111,2020.wmt-1.63,0,\N,Missing
C18-1111,2020.coling-tutorials.3,1,\N,Missing
D09-1082,P08-1118,0,0.0895901,"Missing"
D09-1082,P98-1013,0,0.00670965,"ilarity between T and H, this way of splitting is more reasonable. However, RTE systems using semantic role labelers has not shown very promising results, although SRL has been successfully used in many other NLP tasks, e.g. information extraction, question answering, etc. According to our analysis of the data, there are mainly three reasons: a) the limited coverage of the verb frames or predicates; b) the undetermined relationships between two frames or predicates; and c) the unsatisfying performance of an automatic SRL system. For instance, Burchardt et al. (2007) attempted to use FrameNet (Baker et al., 1998) for the RTE-3 challenge, but did not show substantial improvement. With the recent CoNLL challenges, more and more robust and accurate SRL systems are ready for use, especially for the PAS identification. For the lexical semantics, we also discover that, if we relax the matching criteria (from similarity to relatedness), heterougeous resources can contribute to the coverage differently and then the effectiveness of PAS will be shown as well. Related Work Although the term of Textual Relatedness has not been widely used by the community (as far as we know), many researchers have already incorp"
D09-1082,W07-1402,0,0.0122446,"approaches based on overlapping information or similarity between T and H, this way of splitting is more reasonable. However, RTE systems using semantic role labelers has not shown very promising results, although SRL has been successfully used in many other NLP tasks, e.g. information extraction, question answering, etc. According to our analysis of the data, there are mainly three reasons: a) the limited coverage of the verb frames or predicates; b) the undetermined relationships between two frames or predicates; and c) the unsatisfying performance of an automatic SRL system. For instance, Burchardt et al. (2007) attempted to use FrameNet (Baker et al., 1998) for the RTE-3 challenge, but did not show substantial improvement. With the recent CoNLL challenges, more and more robust and accurate SRL systems are ready for use, especially for the PAS identification. For the lexical semantics, we also discover that, if we relax the matching criteria (from similarity to relatedness), heterougeous resources can contribute to the coverage differently and then the effectiveness of PAS will be shown as well. Related Work Although the term of Textual Relatedness has not been widely used by the community (as far as"
D09-1082,P06-1114,0,0.0448584,"f being classified as a Non-entailment (N) case (=C∪U) against E case in the traditional two-way annotation. Furthermore, many state-of-the-art RTE approaches which are based on overlapping information or similarity functions between T and H, in fact over-cover the E cases, and sometimes, cover the C cases as well. Therefore, in this paper, we Recognizing Textual Entailment (RTE) (Dagan et al., 2006) is a task to detect whether one Hypothesis (H) can be inferred (or entailed) by a Text (T). Being a challenging task, it has been shown that it is helpful to applications like question answering (Harabagiu and Hickl, 2006). The recent research on RTE extends the two-way annotation into three-way1 2 , making it even more difficult, but more linguistic-motivated. The straightforward strategy is to treat it as a three-way classification task, but the performance suffers a significant drop even when using the same classifier and the same feature model. In fact, it can also be dealt with as an extension to the traditional two-way classification, e.g., by identi1 3 http://nlp.stanford.edu/RTE3-pilot/ http://www.nist.gov/tac/tracks/2008/ rte/rte.08.guidelines.html See more details about the annotation guideline at htt"
D09-1082,D08-1021,0,0.0126294,"work, we would like to see whether the PAS can help the second-stage classification as well, e.g. the semantic dependency of negation (AM-NEG) could be helpful for the contraction recognition. Furthermore, since the PAS is usually a bag of unconnected graphs, we could find a way to joint them together, in order to consider both inter- and intra- sentential inferences based on it. In addition, this approach has the potential to be integrated with other RTE modules. For instance, for the predicate alignment, we may consider to use DIRT rules (Lin and Pantel, 2001) or other paraphrase resources (Callison-Burch, 2008), and for the argument alignment, external named-entity recognizer and anaphora resolver would be very helpful. Even more, we also plan to compare/combine it with other methods which are not based on overlapping information between T and H. Impact of the Lexical Resources We did an ablation test of the lexical resources used in our alignment module. Recall that we have applied three lexical resources, VerbOcean for the predicate relatedness function, WordNet for the argument relatedness function, and Normalized Google Distance for both. Table 3 shows the performances of the system without each"
D09-1082,W04-3205,0,0.0232268,")=1  Other Otherwise Now, the only missing components in our definition is the relatedness functions between predicates, arguments, and semantic dependencies. Fortunately, many people have done research on 787 semantic relatedness in lexical semantics that we could use. Therefore, these functions can be realized by different string matching algorithms and/or lexical resources. Since the meaning of relevance is rather wide, apart from the string matching of the lemmas, we also incorporate various resources, from distributionally collected ones to hand-crafted ontologies. We choose VerbOcean (Chklovski and Pantel, 2004) to obtain the relatedness between predicates (after using WordNet (Fellbaum, 1998) to change all the nominal predicates into verbs) and use WordNet for the argument alignment. For the verb relations in VerbOcean, we consider all of them as related; and for WordNet, we not only use the synonyms, hyponyms, and hypernyms, but antonyms as well. Consequently, we simplify these basic relatedness functions into a binary decision. If the corresponding strings are matched or the relations mentioned above exist, the two predicates, arguments, or dependencies are related; otherwise, not. In addition, th"
D09-1082,D08-1084,0,0.123612,"Missing"
D09-1082,H05-1066,0,0.011741,"d produces as outputs the semantic dependencies. The head words of the arguments (including modifiers) are annotated as a direct dependent of the corresponding predicate words, labeled with the type of the semantic relation (Arg0, Arg1 . . . , and various ArgMs). Note that for the application of SRL in RTE task, the PropBank and NomBank notation appears to be more accessible and robust than the the FrameNet notation (with much more detailed roles or frame elements bond to specific verb frames). As input, the SRL system requires syntactic dependency analysis. We use the open source MST Parser (McDonald et al., 2005), trained also on the Wall Street Journal Sections of the Penn Treebank, using a projective decoder with secondorder features. Then the SRL system goes through a pipeline of 4-stage processing: predicate identification (PI) identifies words that evokes a semantic predicate; argument identification (AI) identifies the arguments of the predicates; argument classification (AC) labels the argument with the semantic relations (roles); and predicate classification (PC) further differentiate different use of the predicate word. All components are built as maximal entropy based classifiers, with their"
D09-1082,W08-2121,0,0.036144,"Missing"
D09-1082,W08-2126,1,0.910985,"the K cases instead of E cases is more effective. While in lexical semantics, semantic relatedness is a weaker concept than semantic similarity, there is no counterpart at the sentence or text level. Therefore, in this paper, we propose a Recognizing Textual Relatedness (RTR) task as a subtask or the first step of RTE. By doing so, we choose predicate-argument structure (PAS) as the feature representation, which has already been shown quite useful in the previous RTE challenges (Wang and Neumann, 2007). In order to obtain the PAS, we utilize a Semantic Role Labeling (SRL) system developed by Zhang et al. (2008). Although SRL has been shown to be effective for many tasks, e.g. information extraction, question answering, etc., it has not been successfully used for RTE, mainly due to the low coverage of the verb frame or semantic role resources or the low performance of the automatic SRL systems. The recent CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) have been focusing on semantic dependency parsing along with the traditional syntactic dependency parsing. The PAS from the system output is almost ready for use to build applications based on it. Therefore, another focus of this paper"
D09-1082,C98-1013,0,\N,Missing
D09-1082,W07-1401,0,\N,Missing
D09-1082,W09-1201,1,\N,Missing
D12-1084,W99-0201,0,0.0270387,"chine translation (MT) community provides in many varieties. Bannard and Callison-Burch (2005) as well as Zhao et al. (2008) take one language as the pivot and match two possible translations in the other languages as paraphrases if they share a common pivot 917 phrase. As parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. The paraphrasing task is also strongly related to cross-document event coreference resolution, which is tackled by similar techniques used by the available paraphrasing systems (Bagga and Baldwin, 1999; Tomadaki and Salway, 2005). Most work in paraphrase acquisition has dealt with sentence-level paraphrases, e.g., (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Dolan et al., 2004; Quirk et al., 2004). Our approach for sentential paraphrase extraction is related to the one introduced by Barzilay and Lee (2003), who also employ multiple sequence alignment (MSA). However, they use MSA at the sentence level rather than at the discourse level. We take some core ideas from our previous work on mining script information (Regneri et al., 2010). In this earlier work, we focused on event structu"
D12-1084,P05-1074,0,0.12189,"ora have been quite well studied. Barzilay and McKeown (2001) use different English translations of the same novels (i.e., monolingual parallel corpora), while others (Quirk et al., 2004) experiment on multiple sources of the same news/events, i.e., monolingual comparable corpora. Commonly used (candidate) comparable corpora are news articles written by different news agencies within a limited time window (Wang and Callison-Burch, 2011). Other studies focus on extracting paraphrases from large bilingual parallel corpora, which the machine translation (MT) community provides in many varieties. Bannard and Callison-Burch (2005) as well as Zhao et al. (2008) take one language as the pivot and match two possible translations in the other languages as paraphrases if they share a common pivot 917 phrase. As parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. The paraphrasing task is also strongly related to cross-document event coreference resolution, which is tackled by similar techniques used by the available paraphrasing systems (Bagga and Baldwin, 1999; Tomadaki and Salway, 2005). Most work in paraphrase acquisition has d"
D12-1084,N03-1003,0,0.556549,"2001; Szpektor et al., 2004; Dolan et al., 2004). One of the intuitively appropriate data sources for such collections are parallel or comparable corpora: if two texts are translations of the same foreign document, or if they describe the same underlying scenario, they should contain a reasonable number of sentence pairs that convey the same meaning. Most approaches that extract paraphrases from parallel texts employ some type of pattern matchRui Wang Language Technology Lab DFKI GmbH Saarbrücken, Germany ruiwang@dfki.de ing: sentences with the same meaning are assumed to share many n-grams (Barzilay and Lee, 2003; Callison-Burch, 2008, among others), many words in their context (Barzilay and McKeown, 2001) or certain slots in a dependency path (Lin and Pantel, 2001; Szpektor et al., 2004). Discourse structure has only marginally been considered for this task: For example, Dolan et al. (2004) extract the first sentences from comparable articles and take them as paraphrases. Another approach (Deléger and Zweigenbaum, 2009) matches similar paragraphs in comparable texts, creating smaller comparable documents for paraphrase extraction. We believe that discourse structure delivers important information for"
D12-1084,P01-1008,0,0.798798,"sources for such collections are parallel or comparable corpora: if two texts are translations of the same foreign document, or if they describe the same underlying scenario, they should contain a reasonable number of sentence pairs that convey the same meaning. Most approaches that extract paraphrases from parallel texts employ some type of pattern matchRui Wang Language Technology Lab DFKI GmbH Saarbrücken, Germany ruiwang@dfki.de ing: sentences with the same meaning are assumed to share many n-grams (Barzilay and Lee, 2003; Callison-Burch, 2008, among others), many words in their context (Barzilay and McKeown, 2001) or certain slots in a dependency path (Lin and Pantel, 2001; Szpektor et al., 2004). Discourse structure has only marginally been considered for this task: For example, Dolan et al. (2004) extract the first sentences from comparable articles and take them as paraphrases. Another approach (Deléger and Zweigenbaum, 2009) matches similar paragraphs in comparable texts, creating smaller comparable documents for paraphrase extraction. We believe that discourse structure delivers important information for the extraction of paraphrases. Sentences that play the same role in a certain discourse and ha"
D12-1084,P99-1071,0,0.165896,"Missing"
D12-1084,D08-1021,0,0.352775,"2004; Dolan et al., 2004). One of the intuitively appropriate data sources for such collections are parallel or comparable corpora: if two texts are translations of the same foreign document, or if they describe the same underlying scenario, they should contain a reasonable number of sentence pairs that convey the same meaning. Most approaches that extract paraphrases from parallel texts employ some type of pattern matchRui Wang Language Technology Lab DFKI GmbH Saarbrücken, Germany ruiwang@dfki.de ing: sentences with the same meaning are assumed to share many n-grams (Barzilay and Lee, 2003; Callison-Burch, 2008, among others), many words in their context (Barzilay and McKeown, 2001) or certain slots in a dependency path (Lin and Pantel, 2001; Szpektor et al., 2004). Discourse structure has only marginally been considered for this task: For example, Dolan et al. (2004) extract the first sentences from comparable articles and take them as paraphrases. Another approach (Deléger and Zweigenbaum, 2009) matches similar paragraphs in comparable texts, creating smaller comparable documents for paraphrase extraction. We believe that discourse structure delivers important information for the extraction of par"
D12-1084,P09-1053,0,0.0348161,"and accuracy. It is hard to do a direct comparison with stateof-the-art paraphrase recognition systems, because most are evaluated on different corpora, e.g., the Microsoft paraphrase corpus (Dolan and Brockett, 2005, MSR). We cannot apply our system to the MSR corpus, because we take complete texts as in923 put, while the MSR corpus solely delivers sentence pairs. While the MSR corpus is larger than our collection, the wording variations in its paraphrase pairs are usually lower than for our examples. Thus the final numbers of previous approaches might be vaguely comparable with our results: Das and Smith (2009) present two systems reaching f-scores of 0.82 and 0.83, with a precision of 0.75 and 0.80. Both precision and f-scores of our msa-based systems lie within the same range. Heilman and Smith (2010) introduce a recall-oriented system, which reaches an f-score of 0.81 by a precision of 0.76. Compared to this system, our approach results in better precision values. All further computations bases on the system using MSA and the vector space model (MSA + VEC), because it achieves the highest precision and accuracy values. 6.2 Paraphrase Fragment Evaluation We also manually evaluate precision on para"
D12-1084,W09-3102,0,0.0158111,"texts employ some type of pattern matchRui Wang Language Technology Lab DFKI GmbH Saarbrücken, Germany ruiwang@dfki.de ing: sentences with the same meaning are assumed to share many n-grams (Barzilay and Lee, 2003; Callison-Burch, 2008, among others), many words in their context (Barzilay and McKeown, 2001) or certain slots in a dependency path (Lin and Pantel, 2001; Szpektor et al., 2004). Discourse structure has only marginally been considered for this task: For example, Dolan et al. (2004) extract the first sentences from comparable articles and take them as paraphrases. Another approach (Deléger and Zweigenbaum, 2009) matches similar paragraphs in comparable texts, creating smaller comparable documents for paraphrase extraction. We believe that discourse structure delivers important information for the extraction of paraphrases. Sentences that play the same role in a certain discourse and have a similar discourse context can be paraphrases, even if a semantic similarity model does not consider them very similar. This extends the widely applied distributional hypothesis to the discourse level: According to the distributional hypothesis, entities are similar if they share similar contexts. In our case, entit"
D12-1084,E09-1025,1,0.821102,"applicational point of view, sentential paraphrases are difficult to use in other NLP tasks. At the phrasal level, interchangeable patterns (Shinyama et al., 2002; Shinyama and Sekine, 2003) or inference rules (Lin and Pantel, 2001) are extracted. In both cases, each pattern or rule contains one or several slots, which are restricted to certain type of words, e.g., named entities (NE) or content words. They are quite successful in NE-centered tasks, like information extraction, but their level of generalization or coverage is insufficient for applications like Recognizing Textual Entailment (Dinu and Wang, 2009). The research on general paraphrase fragment extraction at the sub-sentential level is mainly based on phrase pair extraction techniques from the MT literature. Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log-likelihood-ratio of word translation probability. Quirk et al. (2007) extract fragments using a generative model of noisy translations. Our own work (Wang and Callison-Burch, 2011) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora. Our current approach also uses word-word alignme"
D12-1084,I05-5002,0,0.0509199,"e comparison of the two MSA-based systems highlights the great benefit of using structural knowledge: Both MSA + BLEU and MSA + VEC have comparable f-scores and accuracy. The advantage from using the vector-space model that is still obvious for the clustering baselines is nearly evened out when adding discourse knowledge as a backbone. However, the vector model still results in nominally higher precision and accuracy. It is hard to do a direct comparison with stateof-the-art paraphrase recognition systems, because most are evaluated on different corpora, e.g., the Microsoft paraphrase corpus (Dolan and Brockett, 2005, MSR). We cannot apply our system to the MSR corpus, because we take complete texts as in923 put, while the MSR corpus solely delivers sentence pairs. While the MSR corpus is larger than our collection, the wording variations in its paraphrase pairs are usually lower than for our examples. Thus the final numbers of previous approaches might be vaguely comparable with our results: Das and Smith (2009) present two systems reaching f-scores of 0.82 and 0.83, with a precision of 0.75 and 0.80. Both precision and f-scores of our msa-based systems lie within the same range. Heilman and Smith (2010)"
D12-1084,C04-1051,0,0.23237,"in the other languages as paraphrases if they share a common pivot 917 phrase. As parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. The paraphrasing task is also strongly related to cross-document event coreference resolution, which is tackled by similar techniques used by the available paraphrasing systems (Bagga and Baldwin, 1999; Tomadaki and Salway, 2005). Most work in paraphrase acquisition has dealt with sentence-level paraphrases, e.g., (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Dolan et al., 2004; Quirk et al., 2004). Our approach for sentential paraphrase extraction is related to the one introduced by Barzilay and Lee (2003), who also employ multiple sequence alignment (MSA). However, they use MSA at the sentence level rather than at the discourse level. We take some core ideas from our previous work on mining script information (Regneri et al., 2010). In this earlier work, we focused on event structures and their possible realizations in natural language. The corpus used in those experiments were short crowd-sourced descriptions of everyday tasks written in bullet point style. We al"
D12-1084,D11-1108,0,0.0611934,"Missing"
D12-1084,N09-2061,0,0.0125651,"nt second step. 4 Sentence Matching with MSA This section explains how we apply MSA to extract sentence-level paraphrases from a comparable corpus. As our input data, we manually collect recaps for House M.D. episodes from different sources on the web1 . House episodes have an intermediate length (∼45 min), which results in recaps of a con1 e.g. http://house.wikia.com – for a detailed list of URLs, please check the supplementary material or contact the authors. 919 venient size (40 to 150 sentences). The result is one comparable document collection per episode. We applied a sentence splitter (Gillick, 2009) to the documents and treat them as sequences of sentences for further processing. Sequence alignment takes as its input two sequences consisting of elements of some alphabet, and an alphabet-specific score function cm over pairs of sequence elements. For insertions and deletions, the algorithm additionally takes gap costs (cgap ). Multiple Sequence Alignment generalizes pairwise alignment to arbitrarily many sequences. MSA has its main application area in bioinformatics, where it is used to identify equivalent parts of DNA (Durbin et al., 1998). Our alphabet consists of sentences, and a seque"
D12-1084,N10-1145,0,0.0185646,"Dolan and Brockett, 2005, MSR). We cannot apply our system to the MSR corpus, because we take complete texts as in923 put, while the MSR corpus solely delivers sentence pairs. While the MSR corpus is larger than our collection, the wording variations in its paraphrase pairs are usually lower than for our examples. Thus the final numbers of previous approaches might be vaguely comparable with our results: Das and Smith (2009) present two systems reaching f-scores of 0.82 and 0.83, with a precision of 0.75 and 0.80. Both precision and f-scores of our msa-based systems lie within the same range. Heilman and Smith (2010) introduce a recall-oriented system, which reaches an f-score of 0.81 by a precision of 0.76. Compared to this system, our approach results in better precision values. All further computations bases on the system using MSA and the vector space model (MSA + VEC), because it achieves the highest precision and accuracy values. 6.2 Paraphrase Fragment Evaluation We also manually evaluate precision on paraphrase fragments, and additionally describe the productivity of the different setups, providing some intuition about the methods’ recall. Gold-Standard We randomly collect 150 fragment pairs for e"
D12-1084,P03-1054,0,0.0154655,"Missing"
D12-1084,W11-1902,0,0.0865887,"Missing"
D12-1084,D09-1040,0,0.0890607,"Missing"
D12-1084,P06-1011,0,0.0862075,"ne, 2003) or inference rules (Lin and Pantel, 2001) are extracted. In both cases, each pattern or rule contains one or several slots, which are restricted to certain type of words, e.g., named entities (NE) or content words. They are quite successful in NE-centered tasks, like information extraction, but their level of generalization or coverage is insufficient for applications like Recognizing Textual Entailment (Dinu and Wang, 2009). The research on general paraphrase fragment extraction at the sub-sentential level is mainly based on phrase pair extraction techniques from the MT literature. Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log-likelihood-ratio of word translation probability. Quirk et al. (2007) extract fragments using a generative model of noisy translations. Our own work (Wang and Callison-Burch, 2011) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora. Our current approach also uses word-word alignment, however, we use syntactic dependency trees to compute grammatical fragments. Our use of dependency trees is inspired by the constituent-tree-based experiments of Callison-Burch (2008)"
D12-1084,J03-1002,0,0.00506993,"11). In particular, the dependency structures of the parser’s output are used for VP2 http://nlp.stanford.edu/software/ corenlp.shtml 920 fragment extraction (Sec. 5.3). The output from the coreference resolution system is used to cluster all mentions referring to the same entity and to select one as the representative mention. If the representative mention is not a pronoun, we modify the original texts by replacing all pronoun mentions in the cluster with the syntactic head of the representative mention. Note that the coreference resolution system is applied to each recap as a whole. GIZA++ (Och and Ney, 2003) is a widely used word aligner for MT systems. We amend the input data by copying identical word pairs 10 times and adding them as additional ‘sentence’ pairs (Byrne et al., 2003), in order to emphasize the higher alignment probability between identical words. We run GIZA++ for bi-directional word alignment and obtain a lexical translation table. 5.2 Fragment Extraction As mentioned in Sec. 2, we choose to use alignmentbased approaches to this task, which allows us to use many existing MT techniques and tools. We mainly follow our previous approach (Wang and CallisonBurch, 2011), which is a mo"
D12-1084,P02-1040,0,0.0845448,"to the gold standard (paraphrases are members of paraphrasecoll ), taking f-score as follows: f -score = 2 ∗ precision ∗ recall precision + recall We also compute accuracy as the overall fraction of correct labels (negative and positive ones). Our main system uses MSA (denoted by MSA afterwards) with vector-based similarities (VEC) as a 922 scoring function. The gap costs are optimized for f-score, resulting in cgap = 0.4 To show the contribution of MSA’s structural component and compare it to the vector model’s contribution, we create a second MSA-based system that uses MSA with BLEU scores (Papineni et al., 2002) as scoring function (MSA + BLEU). BLEU establishes the average 1-to-4-gram overlap of two sentences. The gap costs for this baseline were optimized separately, ending up with cgap = 1. In order to quantify the contribution of the alignment, we create a discourse-unaware baseline by dropping the MSA and using a state-of-the-art clustering algorithm (Noack, 2007) fed with the vector space model scores (CLUSTER + VEC). The algorithm partitions the set of sentences into paraphrase clusters such that the most similar sentences end up in one cluster. This does not require any parameter tuning. We a"
D12-1084,W04-3219,0,0.325979,"tep (Sec. 4) and the subsequent paraphrase fragment extraction (Sec. 5). We present both automatic and manual evaluation of the two system components (Sec. 6). Finally, we conclude the paper and give some hints for future work (Sec. 7). 2 Related Work Previous paraphrase extraction approaches can be roughly characterized under two aspects: 1) data source and 2) granularity of the output. Both parallel corpora and comparable corpora have been quite well studied. Barzilay and McKeown (2001) use different English translations of the same novels (i.e., monolingual parallel corpora), while others (Quirk et al., 2004) experiment on multiple sources of the same news/events, i.e., monolingual comparable corpora. Commonly used (candidate) comparable corpora are news articles written by different news agencies within a limited time window (Wang and Callison-Burch, 2011). Other studies focus on extracting paraphrases from large bilingual parallel corpora, which the machine translation (MT) community provides in many varieties. Bannard and Callison-Burch (2005) as well as Zhao et al. (2008) take one language as the pivot and match two possible translations in the other languages as paraphrases if they share a co"
D12-1084,2007.mtsummit-papers.50,0,0.0220801,"to certain type of words, e.g., named entities (NE) or content words. They are quite successful in NE-centered tasks, like information extraction, but their level of generalization or coverage is insufficient for applications like Recognizing Textual Entailment (Dinu and Wang, 2009). The research on general paraphrase fragment extraction at the sub-sentential level is mainly based on phrase pair extraction techniques from the MT literature. Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log-likelihood-ratio of word translation probability. Quirk et al. (2007) extract fragments using a generative model of noisy translations. Our own work (Wang and Callison-Burch, 2011) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora. Our current approach also uses word-word alignment, however, we use syntactic dependency trees to compute grammatical fragments. Our use of dependency trees is inspired by the constituent-tree-based experiments of Callison-Burch (2008). 3 recaps of House M.D. parallel corpus with parallel 1 discourse structures + discourse information + semantic similarity + word alignments + core"
D12-1084,P10-1100,1,0.856761,"ques used by the available paraphrasing systems (Bagga and Baldwin, 1999; Tomadaki and Salway, 2005). Most work in paraphrase acquisition has dealt with sentence-level paraphrases, e.g., (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Dolan et al., 2004; Quirk et al., 2004). Our approach for sentential paraphrase extraction is related to the one introduced by Barzilay and Lee (2003), who also employ multiple sequence alignment (MSA). However, they use MSA at the sentence level rather than at the discourse level. We take some core ideas from our previous work on mining script information (Regneri et al., 2010). In this earlier work, we focused on event structures and their possible realizations in natural language. The corpus used in those experiments were short crowd-sourced descriptions of everyday tasks written in bullet point style. We aligned them with a hand-crafted similarity measure that was specifically designed for this text type. In this current work, we target the general task of extracting paraphrases for events rather than the much more specific scriptrelated task. The current approach uses a domainindependent similarity measure instead of a specific hand-crafted similarity score and"
D12-1084,W03-1609,0,0.083253,"n bullet point style. We aligned them with a hand-crafted similarity measure that was specifically designed for this text type. In this current work, we target the general task of extracting paraphrases for events rather than the much more specific scriptrelated task. The current approach uses a domainindependent similarity measure instead of a specific hand-crafted similarity score and is thus applicable to standard texts. From an applicational point of view, sentential paraphrases are difficult to use in other NLP tasks. At the phrasal level, interchangeable patterns (Shinyama et al., 2002; Shinyama and Sekine, 2003) or inference rules (Lin and Pantel, 2001) are extracted. In both cases, each pattern or rule contains one or several slots, which are restricted to certain type of words, e.g., named entities (NE) or content words. They are quite successful in NE-centered tasks, like information extraction, but their level of generalization or coverage is insufficient for applications like Recognizing Textual Entailment (Dinu and Wang, 2009). The research on general paraphrase fragment extraction at the sub-sentential level is mainly based on phrase pair extraction techniques from the MT literature. Munteanu"
D12-1084,W04-3206,0,0.103038,"Missing"
D12-1084,I11-1127,0,0.0265704,"paraphrases. Aligning a sentence with a gap can be thought of as an insertion or deletion. Each alignment has a score which is the sum of all scores for substitutions and all costs for insertions and deletions. Informally, the alignment score is the sum of all scores for each pair of cells (c1 , c2 ), if c1 and c2 are in the same row. If either c1 or c2 is a gap, the pair’s score is cgap . If both cells contain sentences, the score is cm (c1 , c2 ). Fern and Stevenson (2009) showed that sophisticated similarity measures improve paraphrasing, so we apply a state-of-the-art vector space model (Thater et al., 2011) as our score function. The vector space model provides contextualized similarities of words, i.e. the vector of each word is disambiguated by the context the current instance occurs in. cm (c1 , c2 ) returns the model’s similarity score for c1 and c2 . We re-implement a standard MSA algorithm (Needleman and Wunsch, 1970) which approximates the best MSA given the input sequences, cm and cgap . This algorithm recursively aligns two sequences at a time, treating the resulting alignment as a new sequence. This does not necessarily result in the globally optimal alignment, because the order in whi"
D12-1084,W11-1208,1,0.931776,"ated Work Previous paraphrase extraction approaches can be roughly characterized under two aspects: 1) data source and 2) granularity of the output. Both parallel corpora and comparable corpora have been quite well studied. Barzilay and McKeown (2001) use different English translations of the same novels (i.e., monolingual parallel corpora), while others (Quirk et al., 2004) experiment on multiple sources of the same news/events, i.e., monolingual comparable corpora. Commonly used (candidate) comparable corpora are news articles written by different news agencies within a limited time window (Wang and Callison-Burch, 2011). Other studies focus on extracting paraphrases from large bilingual parallel corpora, which the machine translation (MT) community provides in many varieties. Bannard and Callison-Burch (2005) as well as Zhao et al. (2008) take one language as the pivot and match two possible translations in the other languages as paraphrases if they share a common pivot 917 phrase. As parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. The paraphrasing task is also strongly related to cross-document event corefere"
D12-1084,P08-1089,0,0.102622,"d McKeown (2001) use different English translations of the same novels (i.e., monolingual parallel corpora), while others (Quirk et al., 2004) experiment on multiple sources of the same news/events, i.e., monolingual comparable corpora. Commonly used (candidate) comparable corpora are news articles written by different news agencies within a limited time window (Wang and Callison-Burch, 2011). Other studies focus on extracting paraphrases from large bilingual parallel corpora, which the machine translation (MT) community provides in many varieties. Bannard and Callison-Burch (2005) as well as Zhao et al. (2008) take one language as the pivot and match two possible translations in the other languages as paraphrases if they share a common pivot 917 phrase. As parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. The paraphrasing task is also strongly related to cross-document event coreference resolution, which is tackled by similar techniques used by the available paraphrasing systems (Bagga and Baldwin, 1999; Tomadaki and Salway, 2005). Most work in paraphrase acquisition has dealt with sentence-level parap"
D12-1084,C10-1149,0,0.0887594,"Missing"
D12-1084,2003.mtsummit-systems.3,0,\N,Missing
D12-1084,W07-1401,0,\N,Missing
D13-1082,P96-1041,0,0.290232,"translations. However, CSLMs have not been used in the first pass decoding of SMT, because using CSLMs in decoding takes a lot of time. In contrast, we propose a method for converting CSLMs into back-off n-gram language models (BNLMs) so that we can use converted CSLMs in decoding. We show that they outperform the original BNLMs and are comparable with the traditional use of CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in prac"
D13-1082,W04-3250,0,0.0751864,"development data. After we selected the interpolation weight, we applied MERT again to the 2,000 sentence development data to tune the weight parameters.2 We call this BNLM CONV42. We also obtained CONV746 by re-writing BNLM746 with CSLM42 2 We aware that the interpolation weight might be determined by minimizing the perplexity on the development data. However, we opted to directly maximize the BLEU score. LMs BNLM42 CONV42 BNLM746 CONV746 1st pass 31.60 32.58 32.83 33.22 rerank 32.44 32.98 33.36 33.54 Table 1: Comparison of BLEU scores We also performed the paired bootstrap resampling test (Koehn, 2004).3 We sampled 2000 samples for each significance test. Table 2 shows the results of a statistical significance test, in which the “1st” is short for the “1st pass”. The marks indicate whether the LM to the left of a mark is significantly better than that above the mark at a certain level. (“≫”: significantly better at α = 0.01, “>”: α = 0.05, “−”: not significantly better at α = 0.05) First, as shown in the tables, the reranking by applying CSLM42 increased the BLEU scores for all language models. This observation is in accordance with those of previous work (Schwenk, 2010; Huang et al., 2013)"
D13-1082,2012.iwslt-papers.3,0,0.182408,"CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little work on reducing using costs. Since the using costs of CSLMs are very high, it is dif"
D13-1082,J03-1002,0,0.01,"n that case, we use the probabilities in the BNLM as they are. 4 Experiments 4.1 Common settings We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data consisted of 1 M, 2,000, and 2,000 sentences, respectively. We followed the settings of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) except that we used various language models to compare them. We used the MOSES phrasebased SMT system (Koehn et al., 2003), together with Giza++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. The translation performance was measured by the case-insensitive BLEU scores on the tokenized test data. We used mteval-v13a.pl for calculating BLEU scores.1 1 It is available at http://www.itl.nist.gov/iad/ mig/tests/mt/2009/ 847 We used the 14 standard SMT features: five translation model scores, one word penalty score, seven distortion scores and one language model score. Each of the different language models was used to calculate the language model score. As the baseline BNLM, we trained a 5-gram BNLM with modified Kne"
D13-1082,P03-1021,0,0.0144364,"e BNLM as they are. 4 Experiments 4.1 Common settings We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data consisted of 1 M, 2,000, and 2,000 sentences, respectively. We followed the settings of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) except that we used various language models to compare them. We used the MOSES phrasebased SMT system (Koehn et al., 2003), together with Giza++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. The translation performance was measured by the case-insensitive BLEU scores on the tokenized test data. We used mteval-v13a.pl for calculating BLEU scores.1 1 It is available at http://www.itl.nist.gov/iad/ mig/tests/mt/2009/ 847 We used the 14 standard SMT features: five translation model scores, one word penalty score, seven distortion scores and one language model score. Each of the different language models was used to calculate the language model score. As the baseline BNLM, we trained a 5-gram BNLM with modified Kneser-Ney smoothing using the English"
D13-1082,P02-1040,0,0.0964055,"e processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little work on reducing using costs. Since the using costs of CSLMs are very high, it is difficult to use CSLMs in decoding directly. A common approach in SMT using CSLMs i"
D13-1082,P06-2093,0,0.271464,"ey outperform the original BNLMs and are comparable with the traditional use of CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little"
D13-1082,W12-2702,0,0.0213596,"comparable with the traditional use of CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little work on reducing using costs. Since the"
D13-1082,D10-1076,0,0.552441,"inal BNLMs and are comparable with the traditional use of CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little work on reducing u"
D13-1082,N12-1005,0,0.0565609,"raditional use of CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little work on reducing using costs. Since the using costs of CSL"
D13-1082,N03-1017,0,\N,Missing
D14-1023,P14-1142,1,0.292552,"i ∈ V0 Pb (wi |hi ) otherwise 0 (1) where V0 is the short-list,∑Pc (·) is the probability calculated by CSLM, w∈V0 Pc (w|hi ) is the summary of probabilities of the neuron for all the words in the short-list, Pb (·) is the probability calculated by the BNLM, and Ps (hi ) = ∑ Pb (v|hi ). (2) v∈V0 We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. Existing CSLM Converting Methods 2.2 Existing Converting Methods Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calc"
D14-1023,W07-0733,0,0.109437,"Missing"
D14-1023,P07-2045,0,0.00563399,"ased and the BLEU scores trended to increase. These indicated that our proposed method can give better probability estimation for LM and better performance for SMT. (2) In comparison with the grown LMs in ArGrown n-grams Input Corpus CSLM Interpolate BNLM Output Grown n-grams with Probabilities Grown LM Figure 1: NN based bilingual LM growing. 4 Experiments and Results 4.1 Experiment Setting up The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences,"
D14-1023,D13-1106,0,0.0135132,"accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural network based LM or translation model for SMT (Devlin et al., 2014; Liu et al., 2014; Auli et al., 2013). However, the decoding speed using n-gram LM is still state-ofthe-art one. Some approaches calculate the probabilities of the n-grams n-grams before decoding, and store them in the n-gram format (Wang et al., 2013a; Arsoy et al., 2013; Arsoy et al., 2014). The ‘converted CSLM’ can be directly used in SMT. Though more n-grams which are not in the trainSince larger n-gram Language Model (LM) usually performs better in Statistical Machine Translation (SMT), how to construct efficient large LM is an important topic in SMT. However, most of the existing LM growing methods need an extra monolingual"
D14-1023,W04-3250,0,0.0727048,"est lists of SMT. Our previous converted LM, Arsoy’s grown LMs and bilingual grown LMs were interpolated with the original BNLMs, using default setting of SRILM5 . To reduce the randomness of MERT, we used two methods for tuning the weights of different SMT features, and two BLEU scores are corresponding to these two methods. The BLEU-s indicated that the same weights of the BNLM (BN) features were used for all the SMT systems. The BLEU-i indicated that the MERT was run independently by three times and the average BLEU scores were taken. We also performed the paired bootstrap resampling test (Koehn, 2004)6 . Two thousands samples were sampled for each significance test. The marks at the right of the BLEU score indicated whether the LMs were significantly better/worse than the Arsoy’s grown LMs with the same IDs for SMT (“++/−−”: significantly better/worse at α = 0.01, “+/−”: α = 0.05, no mark: not significantly better/worse at α = 0.05). From the results shown in Table 1, we can get the following observations: (1) Nearly all the bilingual grown LMs outperformed both BNLM and our previous converted LM on PPL and BLEU. As the size of grown LMs is increased, the PPL always decreased and the BLEU"
D14-1023,2012.eamt-1.60,0,0.0244855,"200240, China 2 Multilingual Translation Laboratory, MASTAR Project, National Institute of Information and Communications Technology, 3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan 3 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China wangrui.nlp@gmail.com, {zhaohai, blu}@cs.sjtu.edu.cn, {mutiyama, eiichiro.sumita}@nict.go.jp Abstract 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, whic"
D14-1023,P14-1140,0,0.0260099,"Missing"
D14-1023,P14-1129,0,0.0325731,"Missing"
D14-1023,2012.iwslt-papers.3,0,0.044911,"s very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural network based LM or translati"
D14-1023,J03-1002,0,0.00542098,"e. These indicated that our proposed method can give better probability estimation for LM and better performance for SMT. (2) In comparison with the grown LMs in ArGrown n-grams Input Corpus CSLM Interpolate BNLM Output Grown n-grams with Probabilities Grown LM Figure 1: NN based bilingual LM growing. 4 Experiments and Results 4.1 Experiment Setting up The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences, respectively. Using SRILM (Stolcke, 2002;"
D14-1023,I13-1170,1,0.794008,"ed by CSLM, w∈V0 Pc (w|hi ) is the summary of probabilities of the neuron for all the words in the short-list, Pb (·) is the probability calculated by the BNLM, and Ps (hi ) = ∑ Pb (v|hi ). (2) v∈V0 We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. Existing CSLM Converting Methods 2.2 Existing Converting Methods Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calculated by using the same method as our previous approach. That is, the proposed new method is the"
D14-1023,P03-1021,0,0.0161605,"can give better probability estimation for LM and better performance for SMT. (2) In comparison with the grown LMs in ArGrown n-grams Input Corpus CSLM Interpolate BNLM Output Grown n-grams with Probabilities Grown LM Figure 1: NN based bilingual LM growing. 4 Experiments and Results 4.1 Experiment Setting up The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences, respectively. Using SRILM (Stolcke, 2002; Stolcke et al., 2011), we trained"
D14-1023,P95-1030,0,0.0389552,"rpus in SMT. The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus. 1 Introduction ‘Language Model (LM) Growing’ refers to adding n-grams outside the corpus together with their probabilities into the original LM. This operation is useful as it can make LM perform better through letting it become larger and larger, by only using a small training corpus. There are various methods for adding n-grams selected by different criteria from a monolingual corpus (Ristad and Thomas, 1995; Niesler and Woodland, 1996; Siu and Ostendorf, 2000; Siivola et al., 2007). However, all of these approaches need additional corpora. Meanwhile the extra corpora from different domains will not result in better LMs (Clarkson and Robinson, 1997; Iyer et al., 1997; Bellegarda, 2004; Koehn and Schroeder, ∗ Part of this work was done as Rui Wang visited in NICT. 189 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189–195, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics ing corpus can be generated by using so"
D14-1023,D13-1140,0,0.0268942,"Missing"
D14-1023,P06-2093,0,0.0965172,", blu}@cs.sjtu.edu.cn, {mutiyama, eiichiro.sumita}@nict.go.jp Abstract 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram"
D14-1023,D13-1082,1,0.598465,"Missing"
D14-1023,W12-2702,0,0.0279048,".go.jp Abstract 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to i"
D14-1023,I13-1069,1,0.740408,"Missing"
D14-1023,C12-2131,1,0.530178,"ist,∑Pc (·) is the probability calculated by CSLM, w∈V0 Pc (w|hi ) is the summary of probabilities of the neuron for all the words in the short-list, Pb (·) is the probability calculated by the BNLM, and Ps (hi ) = ∑ Pb (v|hi ). (2) v∈V0 We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. Existing CSLM Converting Methods 2.2 Existing Converting Methods Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calculated by using the same method as our previous approach."
D14-1023,C12-3067,1,0.224044,"re V0 is the short-list,∑Pc (·) is the probability calculated by CSLM, w∈V0 Pc (w|hi ) is the summary of probabilities of the neuron for all the words in the short-list, Pb (·) is the probability calculated by the BNLM, and Ps (hi ) = ∑ Pb (v|hi ). (2) v∈V0 We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. Existing CSLM Converting Methods 2.2 Existing Converting Methods Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calculated by using the same method as our"
D14-1023,D10-1076,0,0.35607,"{mutiyama, eiichiro.sumita}@nict.go.jp Abstract 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are seve"
D14-1023,N12-1005,0,0.0314071,"In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural ne"
D17-1155,D11-1033,0,0.732683,"two instance weighting technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT EnglishGerman/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points. 1 Introduction In Statistical Machine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as"
D17-1155,2015.iwslt-evaluation.1,0,0.0573695,"Missing"
D17-1155,2016.amta-researchers.8,0,0.201897,"it some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rous"
D17-1155,P17-2061,0,0.26518,"anslation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain model can be ensembled (Jean et al., 2015). ii) an NMT further training (fine-tuning) method (Luong and Manning, 2015). The training is performed in two steps: first, the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Recently, Chu et al. (2017) make an empirical comparison of NMT further training (Luong and Manning, 2015) and domain control (Kobus et al., 2016), which applied word-level domain features to word embedding layer. This approach provides natural baselines for comparison. To the best of our knowledge, there is no existing work concerning instance weighting in 1482 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1482–1488 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics NMT. The main challenge is that NMT is not a liner model or combin"
D17-1155,P13-2119,0,0.0568442,"corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT"
D17-1155,C16-1299,0,0.0285091,"Missing"
D17-1155,2015.mtsummit-papers.10,0,0.103141,"een shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foste"
D17-1155,D10-1044,0,0.0589565,"2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain mod"
D17-1155,D14-1062,0,0.0330635,"Missing"
D17-1155,C14-1182,0,0.0385476,"Missing"
D17-1155,P07-1034,0,0.35209,"by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directl"
D17-1155,kobus-etal-2017-domain,0,0.0736135,"Missing"
D17-1155,W04-3250,0,0.163481,"Missing"
D17-1155,P07-2045,0,0.024644,"Missing"
D17-1155,2015.iwslt-evaluation.11,0,0.461478,"technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT EnglishGerman/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points. 1 Introduction In Statistical Machine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translati"
D17-1155,D15-1166,0,0.185744,"ies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT EnglishGerman/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points. 1 Introduction In Statistical Machine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translati"
D17-1155,D09-1074,0,0.0172604,"Missing"
D17-1155,P10-2041,0,0.135114,"hine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity"
D17-1155,P02-1040,0,0.116309,"Missing"
D17-1155,2011.iwslt-evaluation.10,0,0.120719,"Missing"
D17-1155,E12-1055,0,0.0316873,"domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural M"
D17-1155,E17-3017,0,0.0358134,"Missing"
D17-1155,P13-1082,0,0.0411855,"uages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT"
D17-1155,W10-1759,0,0.0888214,",b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model"
D17-1155,P17-2089,1,0.366616,"Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain model can be ensembled (Jean et al., 2015). ii) an NMT further training (fine-tuning) method (Luong and Manning, 2015). The training is performed in two steps: first, the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Recently, Chu et al. (2017) make an empirical comparison of NMT further training (Luong and Manning, 2015) and domain contr"
D17-1155,C16-1295,1,0.845483,"ral PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain model can be ensembled (Jean et al., 2015). ii) an NMT further t"
D17-1304,W09-2307,0,0.0166967,"annotation vectors H and dependency annotation vectors D. The current context vector csi and cdi are compute by eq.(4), respectively: J X … CNN yi-1 … Figure 2: SDRNMT-1 for the i-th time step. csi = … … x7 CNN ci … x6 VU2 ? i,1 ? i,1 ? ? i,2 i,2 … i,J x5 U2=&lt;x3, x1, x4, x7 , ε&gt; CNN d1 x4 (14) Experiment Setting up We carry out experiments on Chinese-to-English translation. The training dataset consists of 1.42M 2 λ can be tuned according to a subset FBIS of training data and be set as 0.6 in the experiments. sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test. The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We als"
D17-1304,P05-1066,0,0.206266,"Experiment Setting up We carry out experiments on Chinese-to-English translation. The training dataset consists of 1.42M 2 λ can be tuned according to a subset FBIS of training data and be set as 0.6 in the experiments. sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test. The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We also compare with a state-of-the-art syntax enhanced NMT method (Sennrich and Haddow, 2016). For a fair comparison, we only utilize dependency information for (Sennrich and Haddow, 2016), called Sennrich-deponly. We try our best to re-implement the baseline methods on Nematus toolkit 4 (Sennrich et al.,"
D17-1304,P14-1129,0,0.0116576,"or VUj for Uj . In our experiment, the output of the output layer is 1 × d-dimension vector. It should be noted that the dependency unit is similar to the source dependency feature of Sennrich and Haddow (2016) and the SDR is the same to the source-side representation of Chen et al. (2017). In comparison with Sennrich and Haddow (2016), who concatenate the source dependency labels and word together to enhance the Encoder of NMT, we adapt a separate attention mechanism together with a CNN dependency Encoder. Compared with Chen et al. (2017), which expands the famous neural network joint model (Devlin et al., 2014) with source dependency information to improve the phrase pair translation probability estimation for SMT, we focus on source dependency information to enhance attention probability estimation and to learn corresponding dependency context and RNN hidden state for improving translation. 4 NMT with SDR In this section, we propose two novel NMT models SDRNMT-1 and SDRNMT-2, both of which can make use of source dependency information SDR to enhance Encoder and Decoder of NMT. 4.1 greatly tackle the sparsity issues associated with large dependency units. Motivated by (Sennrich and Haddow, 2016), we"
D17-1304,P16-1078,0,0.0619476,"h translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). I"
D17-1304,P17-2012,0,0.0146372,"ovements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent preliminary exploration (S"
D17-1304,N16-1101,0,0.0209728,"cially on long sentences. Empirical results on NIST Chinese-toEnglish translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; H"
D17-1304,D14-1176,0,0.0464241,"Missing"
D17-1304,P17-1177,0,0.0492407,"achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent pr"
D17-1304,D13-1176,0,0.0524365,"successfully introduced into statistical machine translation. However, there are only a few preliminary attempts for Neural Machine Translation (NMT), such as concatenating representations of source word and its dependency label together. In this paper, we propose a novel attentional NMT with source dependency representation to improve translation performance of NMT, especially on long sentences. Empirical results on NIST Chinese-toEnglish translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency"
D17-1304,W15-4906,0,0.0209798,"6; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent preliminary exploration (Sennrich and Haddow, 2016), in which vector representations of source word and its dependency label are simply concatenated as source input, achieving state-ofthe-art performance in NMT (Bojar et al., 2016). In this paper, we propose a novel NMT with source dependency representation to improve translation performance. Compared with the simple approach of vector concatenation, we learn the Source Dependency Representation (SDR) to compute dependency context vect"
D17-1304,P07-2045,0,0.0114868,"data and be set as 0.6 in the experiments. sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test. The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We also compare with a state-of-the-art syntax enhanced NMT method (Sennrich and Haddow, 2016). For a fair comparison, we only utilize dependency information for (Sennrich and Haddow, 2016), called Sennrich-deponly. We try our best to re-implement the baseline methods on Nematus toolkit 4 (Sennrich et al., 2017). For all NMT systems, we limit the source and target vocabularies to 30K, and the maximum sentence length is 80. The word embedding dimension is 620,5 and the hidden l"
D17-1304,P17-1064,0,0.0668486,"that our method achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has"
D17-1304,P02-1040,0,0.11802,"? i,1 ? ? i,2 i,2 … i,J x5 U2=&lt;x3, x1, x4, x7 , ε&gt; CNN d1 x4 (14) Experiment Setting up We carry out experiments on Chinese-to-English translation. The training dataset consists of 1.42M 2 λ can be tuned according to a subset FBIS of training data and be set as 0.6 in the experiments. sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test. The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We also compare with a state-of-the-art syntax enhanced NMT method (Sennrich and Haddow, 2016). For a fair comparison, we only utilize dependency information for (Sennrich and Haddow, 2016), called Sennrich-deponly. We try our best to re-"
D17-1304,E17-3017,0,0.043072,"Missing"
D17-1304,W16-2209,0,0.149106,"), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent preliminary exploration (Sennrich and Haddow, 2016), in which vector representations of source word and its dependency label are simply concatenated as source input, achieving state-ofthe-art performance in NMT (Bojar et al., 2016). In this paper, we propose a novel NMT with source dependency representation to improve translation performance. Compared with the simple approach of vector concatenation, we learn the Source Dependency Representation (SDR) to compute dependency context vectors and alignment matrices in a more sophisticated manner, which has the potential to make full use of source dependency information. To this end, we create a de"
D17-1304,N16-1004,0,0.0215619,"Missing"
D17-1304,W16-2301,0,\N,Missing
D19-1027,P11-1040,0,0.034788,"bute-value pairs and mapped them to manually generated schemes for extracting the natural disaster events. Similarly, to extract the city-traffic related event, Anantharam et al. (2015) viewed the task as a sequential tagging problem and proposed an approach based on the conditional random fields. Zhang (2018) proposed an event extraction approach based on imitation learning, especially on inverse reinforcement learning. Open-domain event extraction aims to extract events without limiting the specific types of events. To analyze individual messages and induce a canonical value for each event, Benson et al. (2011) proposed an approach based on a structured graphical model. By representing an event with a binary tuple which is constituted by a named entity and a date, Ritter et al. (2012) employed some statistic to measure the strength of associations between a named entity and a date. The proposed system relies on a supervised labeler trained on annotated data. In (Abdelhaq et al., 2013), Abdelhaq et al. developed a realtime event extraction system called EvenTweet, and each event is represented as a triple constituted by time, location and keywords. To extract more information, Wang el al. (2015) deve"
D19-1027,P18-1046,0,0.0568217,"Missing"
D19-1027,P14-2114,1,0.952857,"sociations between a named entity and a date. The proposed system relies on a supervised labeler trained on annotated data. In (Abdelhaq et al., 2013), Abdelhaq et al. developed a realtime event extraction system called EvenTweet, and each event is represented as a triple constituted by time, location and keywords. To extract more information, Wang el al. (2015) developed a system employing the links in tweets and combing tweets with linked articles to identify events. Xia el al. (2015) combined texts with the location information to detect the events with low spatial and temporal deviations. Zhou et al. (2014; 2017) represented event as a quadruple and proposed two Bayesian models to extract events from tweets. • We propose a novel Adversarial-neural Event Model (AEM), which is, to the best of our knowledge, the first attempt of using adversarial training for open-domain event extraction. • Unlike existing Bayesian graphical modeling approaches, AEM is able to extract events from different text sources (short and long). And a significant improvement on computational efficiency is also observed. • Experimental results on three datasets show that AEM outperforms the baselines in terms of accuracy, r"
D19-1027,E17-1076,1,0.515958,"default setting. The vertical axis represents methods/parameter settings, the horizontal axis denotes the corresponding performance value. All blue histograms with different intensity are those obtained by AEM. with the default configuration. Google dataset, we further divide the non-location named entities into two categories (‘person’ and ‘organization’) and employ a quadruple <organization, location, person, keyword&gt; to denote an event in news articles. We also remove common stopwords and only keep the recognized named entities and the tokens which are verbs, nouns or adjectives. • DPEMM (Zhou et al., 2017) is a nonparametric mixture model for event extraction. It addresses the limitation of LEM that the number of events should be known beforehand. We implement the model with the default configuration. 4.2 Experimental Results For social media text corpus (FSD and Twitter), a named entity tagger3 specifically built for Twitter is used to extract named entities including locations from tweets. A Twitter Part-of-Speech (POS) tagger (Gimpel et al., 2010) is used for POS tagging and only words tagged with nouns, verbs and adjectives are retained as keywords. For the Google dataset, we use the Stanfo"
D19-1027,P11-2008,0,\N,Missing
D19-1057,W13-2322,0,0.0123005,"the SRL task, most of the previous research focuses on 1) PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) annotations, i.e., the CoNLL 2005 (Carreras and M`arquez, 2005) and CoNLL 2009 (Hajiˇc et al., 2009) shared tasks; 2) OntoNotes annotations (Weischedel et al., 2011), i.e., the CoNLL 2005 and CoNLL 2012 datasets and more; 3) and FrameNet (Baker et al., 1998) annotations. For the non-English languages, not all of them are widely available. Apart from these, in the broad range of semantic processing, other formalisms non-exhaustively include abstract meaning representation (Banarescu et al., 2013), universal decompositional semantics (White et al., 2016), and semantic dependency parsing (Oepen et al., 2015). Abend and Rappoport (2017) give a better overview of various semantic representations. In this paper, we primarily work on the Chinese and English datasets from the • We introduce the relation-aware approach to employ syntactic dependencies into the selfattention-based SRL model. • We compare our approach with previous studies, and achieve state-of-the-art results with and without external resources, i.e., in the so-called closed and open settings. 2 Related work Traditional semant"
D19-1057,W07-1402,0,0.0496309,"d models use BiLSTM Introduction The task of semantic role labeling (SRL) is to recognize arguments for a given predicate in one sentence and assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, Novemb"
D19-1057,W05-0620,0,0.570923,"Missing"
D19-1057,P81-1022,0,0.476074,"Missing"
D19-1057,D15-1112,0,0.249955,"Missing"
D19-1057,W11-1608,0,0.0274094,"ndencies. For the main architecture of the SRL model, many neural-network-based models use BiLSTM Introduction The task of semantic role labeling (SRL) is to recognize arguments for a given predicate in one sentence and assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conferen"
D19-1057,W04-2705,0,0.255775,"Missing"
D19-1057,J02-3001,0,0.484379,"onal semantics (White et al., 2016), and semantic dependency parsing (Oepen et al., 2015). Abend and Rappoport (2017) give a better overview of various semantic representations. In this paper, we primarily work on the Chinese and English datasets from the • We introduce the relation-aware approach to employ syntactic dependencies into the selfattention-based SRL model. • We compare our approach with previous studies, and achieve state-of-the-art results with and without external resources, i.e., in the so-called closed and open settings. 2 Related work Traditional semantic role labeling task (Gildea and Jurafsky, 2002) presumes that the syntactic structure of the sentence is given, either being a constituent tree or a dependency tree, like in the CoNLL shared tasks (Carreras and M`arquez, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). 617 Positional Embedding encodes the order of the input word sequence. We follow Vaswani et al. (2017) to use time positional embedding, which is formulated as follows: P E(t, 2i) = sin(t/100002i/d ) P E(t, 2i + 1) = cos(t/100002i/d ) (1) where t is the position, i means the dimension, and d is the dimension of the model input embedding. 3.1.2 Encoder Layer The self-attent"
D19-1057,S15-2153,0,0.0398758,"2004) annotations, i.e., the CoNLL 2005 (Carreras and M`arquez, 2005) and CoNLL 2009 (Hajiˇc et al., 2009) shared tasks; 2) OntoNotes annotations (Weischedel et al., 2011), i.e., the CoNLL 2005 and CoNLL 2012 datasets and more; 3) and FrameNet (Baker et al., 1998) annotations. For the non-English languages, not all of them are widely available. Apart from these, in the broad range of semantic processing, other formalisms non-exhaustively include abstract meaning representation (Banarescu et al., 2013), universal decompositional semantics (White et al., 2016), and semantic dependency parsing (Oepen et al., 2015). Abend and Rappoport (2017) give a better overview of various semantic representations. In this paper, we primarily work on the Chinese and English datasets from the • We introduce the relation-aware approach to employ syntactic dependencies into the selfattention-based SRL model. • We compare our approach with previous studies, and achieve state-of-the-art results with and without external resources, i.e., in the so-called closed and open settings. 2 Related work Traditional semantic role labeling task (Gildea and Jurafsky, 2002) presumes that the syntactic structure of the sentence is given"
D19-1057,J05-1004,0,0.520095,"ive multi-head self-attention model to incorporate syntax, which is called LISA (Linguistically-Informed Self-Attention). We follow their approach of replacing one attention head with the dependency head information, but use a softer way to capture the pairwise relationship between input elements (Shaw et al., 2018). • We present detailed experiments on different aspects of incorporating syntactic information into the SRL model, in what quality, in which representation and how to integrate. For the datasets and annotations of the SRL task, most of the previous research focuses on 1) PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) annotations, i.e., the CoNLL 2005 (Carreras and M`arquez, 2005) and CoNLL 2009 (Hajiˇc et al., 2009) shared tasks; 2) OntoNotes annotations (Weischedel et al., 2011), i.e., the CoNLL 2005 and CoNLL 2012 datasets and more; 3) and FrameNet (Baker et al., 1998) annotations. For the non-English languages, not all of them are widely available. Apart from these, in the broad range of semantic processing, other formalisms non-exhaustively include abstract meaning representation (Banarescu et al., 2013), universal decompositional semantics (White et al., 2016), and s"
D19-1057,P18-2058,0,0.176608,"Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018) replace one attention head with pre-trained syntactic dependency information, which can be viewed as a hard way to inject syntax into the neural model. Enlightened by the machine translation model proposed by Shaw et al. (20"
D19-1057,D14-1162,0,0.0991902,"onfigurations, and report the labeled precision (P), labeled recall (R) and labeled f-score (F1) for the semantic dependencies. Word Representations Most of our experiments are conducted in the closed setting without any external word embeddings or data resources than those provided by the CoNLL-2009 datasets. In the closed setting, word embedding is initialized by a Gaussian distribution with mean 0 and variance √1d , where d is the dimension of embedding size of each layer. For the experiments with external resources in the open setting, we utilize 1) word embeddings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pretrained on Wikipedia and Gigaword for English; and 2) ELMo3 (Peters et al., 2018) and BERT4 (Devlin et al., 2018), two recently proposed effective deep contextualized word representations5 . where MD is the one-hot dependency head matrix and ER means the embedding of dependency relation information, such as R EL or R EL PATH. 3.3.3 test UAS LAS 80.70 78.46 89.05 85.60 92.14 89.23 Table 1: Syntactic dependency performance for different parsers. AUTO indicates the automatic dependency trees provided by the Co"
D19-1057,P17-1044,0,0.649356,"ructural information into the SRL model; 2) Deeper integration of the syntactic information achieves better results than the simple concatenation to the inputs; 3) External pre-trained contextualized word representations help to boost the SRL performance further, which is not entirely overlapping with the syntactic information. In summary, the contributions of our work are: Roth and Lapata (2016) utilize an LSTM model to obtain embeddings from the syntactic dependency paths; while Marcheggiani and Titov (2017) construct Graph Convolutional Networks to encode the dependency structure. Although He et al. (2017)’s approach is a pure end-to-end learning, they have included an analysis of adding syntactic dependency information into English SRL in the discussion section. Cai et al. (2018) have compared syntax-agnostic and syntax-aware approaches and Xia et al. (2019) have compared different ways to represent and encode the syntactic knowledge. In another line of research, Tan et al. (2017) utilize the Transformer network for the encoder instead of the BiLSTM. Strubell et al. (2018) present a novel and effective multi-head self-attention model to incorporate syntax, which is called LISA (Linguistically-"
D19-1057,N18-1202,0,0.0608331,"are conducted in the closed setting without any external word embeddings or data resources than those provided by the CoNLL-2009 datasets. In the closed setting, word embedding is initialized by a Gaussian distribution with mean 0 and variance √1d , where d is the dimension of embedding size of each layer. For the experiments with external resources in the open setting, we utilize 1) word embeddings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pretrained on Wikipedia and Gigaword for English; and 2) ELMo3 (Peters et al., 2018) and BERT4 (Devlin et al., 2018), two recently proposed effective deep contextualized word representations5 . where MD is the one-hot dependency head matrix and ER means the embedding of dependency relation information, such as R EL or R EL PATH. 3.3.3 test UAS LAS 80.70 78.46 89.05 85.60 92.14 89.23 Table 1: Syntactic dependency performance for different parsers. AUTO indicates the automatic dependency trees provided by the CoNLL-09 Chinese dataset. B I AFFINE means the trees are generated by BiaffineParser with pre-trained word embedding on the Gigaword corpus while B IAFFINE B ERT is the sa"
D19-1057,C08-1050,0,0.128118,"al Results on the Chinese Test Data Based on the above experiments and analyses, we present the overall results of our model in this subsection. We train the three models (I NPUT, LISA, and R EL AWE) with their best settings without any external knowledge as C LOSED, and we take the same models with B ERT as O PEN. The D EP PATH &R EL PATH from G OLD without external knowledge serves as the G OLD for reference. Since we have been focusing on the task of argument identification and labeling, for both C LOSED and O PEN, we follow Roth and Lapata (2016) to use existing systems’ predicate senses (Johansson and Nugues, 2008) to exclude them from comparison. Table 7 shows that our O PEN model achieves more than 3 points of f1-score than the stateof-the-art result, and R EL AWE with D EP PATH &R EL PATH achieves the best in both C LOSED and O PEN settings. Notice that our best C LOSED model can almost perform as well as the state-of-the-art model while the latter utilizes pretrained word embeddings. Besides, performance gap between three models under O PEN setting is very small. It indicates that the representation ability of BERT is so powerful and may contains rich syntactic information. At last, the G OLD result"
D19-1057,W17-4305,0,0.0916819,"of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018)"
D19-1057,P16-1113,0,0.641761,"ong with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Str"
D19-1057,D18-1262,1,0.84272,"Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018) replace one attention head with pre-trained syntactic dependency information, which can be viewed as a hard way to inject syntax into the neural model. Enlightened by the machine translation model proposed"
D19-1057,N18-2074,0,0.114208,"e et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018) replace one attention head with pre-trained syntactic dependency information, which can be viewed as a hard way to inject syntax into the neural model. Enlightened by the machine translation model proposed by Shaw et al. (2018), we introduce the Relation-Aware method to incorporate syntactic dependencies, which is a softer way to encode richer structural information. Various experiments for the Chinese SRL on the CoNLL-2009 dataset are conducted to evaluate our hypotheses. From the empirical results, we observe that: 1) The quality of the syntactic information is essential when we incorporate structural information into the SRL model; 2) Deeper integration of the syntactic information achieves better results than the simple concatenation to the inputs; 3) External pre-trained contextualized word representations help"
D19-1057,K17-1041,0,0.201985,"For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018) replace one attention head with pre-trained syntactic dependency information, which can be viewed as a hard way to inject syntax into the neural mo"
D19-1057,D07-1002,0,0.127289,"icate in one sentence and assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly"
D19-1057,D16-1177,0,0.0617731,"Missing"
D19-1057,P14-2120,0,0.015133,"oduction The task of semantic role labeling (SRL) is to recognize arguments for a given predicate in one sentence and assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Assoc"
D19-1057,N19-1075,0,0.449868,"rformance for the Chinese SRL task on the CoNLL-2009 dataset. 1 $ $ 中国 China SBJ ROOT 鼓励 encourage 外商 foreign merchant 投资 invest 农业 agriculture COMP COMP COMP Figure 1: An example of one sentence with its syntactic dependency tree and semantic roles. Arcs above the sentence are semantic role annotations for the predicate “鼓励 (encourage)” and below the sentence are syntactic dependency annotations of the whole sentence. The meaning of this sentence is “China encourages foreign merchants to invest in agriculture”. et al. (2017) have observed that only good syntax helps with the SRL performance. Xia et al. (2019) have explored what kind of syntactic information or structure is better suited for the SRL model. Cai et al. (2018) have compared syntax-agnostic and syntax-aware approaches and claim that the syntax-agnostic model surpasses the syntax-aware ones. In this paper, we focus on analyzing the relationship between the syntactic dependency information and the SRL performance. In particular, we investigate the following four aspects: 1) Quality of the syntactic information: whether the performance of the syntactic parser output affects the SRL performance; 2) Representation of the syntactic informati"
D19-1057,D18-1548,0,0.193596,"Missing"
D19-1057,P16-2033,0,0.0392647,"nd assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into tw"
D19-1057,P03-1002,0,0.334907,") is to recognize arguments for a given predicate in one sentence and assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neur"
D19-1057,P15-1109,0,0.0304126,"the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018) replace one attention head with pre-trained syntactic dependency information, which can be viewed as a hard way to inje"
D19-1057,W08-2121,0,0.366947,"Missing"
D19-1117,N18-1150,0,0.138583,"lient information and avoiding repetitions, we augment the vanilla attention model from both local and global aspects. We propose an attention refinement unit paired with local variance loss to impose supervision on the attention model at each decoding step, and a global variance loss to optimize the attention distributions of all decoding steps from the global perspective. The performances on the CNN/Daily Mail dataset verify the effectiveness of our methods. 1 Introduction Abstractive document summarization (Rush et al., 2015; Nallapati et al., 2016; Tan et al., 2017; Chen and Bansal, 2018; Celikyilmaz et al., 2018) attempts to produce a condensed representation of the most salient information of the document, aspects of which may not appear as parts of the original input text. One popular framework used in abstractive summarization is the sequence-tosequence model introduced by Sutskever et al. (2014). The attention mechanism (Bahdanau et al., 2014) is proposed to enhance the sequenceto-sequence model by allowing salient features to dynamically come to the forefront as needed to make up for the incapability of memorizing the long input source. However, when it comes to longer documents, basic attention"
D19-1117,P18-1063,0,0.159616,"reproducing the most salient information and avoiding repetitions, we augment the vanilla attention model from both local and global aspects. We propose an attention refinement unit paired with local variance loss to impose supervision on the attention model at each decoding step, and a global variance loss to optimize the attention distributions of all decoding steps from the global perspective. The performances on the CNN/Daily Mail dataset verify the effectiveness of our methods. 1 Introduction Abstractive document summarization (Rush et al., 2015; Nallapati et al., 2016; Tan et al., 2017; Chen and Bansal, 2018; Celikyilmaz et al., 2018) attempts to produce a condensed representation of the most salient information of the document, aspects of which may not appear as parts of the original input text. One popular framework used in abstractive summarization is the sequence-tosequence model introduced by Sutskever et al. (2014). The attention mechanism (Bahdanau et al., 2014) is proposed to enhance the sequenceto-sequence model by allowing salient features to dynamically come to the forefront as needed to make up for the incapability of memorizing the long input source. However, when it comes to longer"
D19-1117,W14-3348,0,0.00980747,"somewhat similar with the coverage mechanism (Tu et al., 2016; See et al., 2017), which is also designed for solving the repetition problem. The coverage mechanism introduces a coverage vector to keep track of previous decisions at each decoding step and adds it into the attention calculation. However, when the high attention on certain position is wrongly assigned during previous timesteps, the coverage mechanism hinders the correct assignment of attention in later steps. We conduct our experiments on the CNN/Daily Mail dataset and achieve comparable results on ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) with the state-of-the-art models. 1222 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1222–1228, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Our model surpasses the strong pointer-generator baseline (w/o coverage) (See et al., 2017) on all ROUGE metrics by a large margin. As far as we know, we are the first to introduce explicit loss functions to optimize the attention. More importantly, the idea behind our model is simple but effe"
D19-1117,D18-1443,0,0.101591,"k used in abstractive summarization is the sequence-tosequence model introduced by Sutskever et al. (2014). The attention mechanism (Bahdanau et al., 2014) is proposed to enhance the sequenceto-sequence model by allowing salient features to dynamically come to the forefront as needed to make up for the incapability of memorizing the long input source. However, when it comes to longer documents, basic attention mechanism may lead to distraction and fail to attend to the relatively salient parts. Therefore, some works focus on designing various attentions to tackle this issue (Tan et al., 2017; Gehrmann et al., 2018). We follow this line of research and propose an effective attention refinement unit (ARU). Consider the following case. Even with a preliminary idea of which parts of source document should be focused on (attention), sometimes people may still have trouble in deciding which exact part should be emphasized for the next word (the output of the decoder). To make a more correct decision on what to write next, people always adjust the concentrated content by reconsidering the current state of what has been summarized already. Thus, ARU is designed as an update unit based on current decoding state,"
D19-1117,N18-1065,0,0.069631,"Missing"
D19-1117,P18-1014,0,0.0151083,"nism solving saliency and repetition problem, it generates many trivial facts. With ARU, the model successfully concentrates on the salient information, however, it also suffers from serious repetition problem. Further optimized by the variance loss, our model can avoid repetition and generate summary with salient information. Besides, our generated summary contains fewer trivial facts compared to the PGN+Coverage model. 4 Related Work The exploration on document summarization can be broadly divided into extractive and abstractive summarization. The extractive methods (Nallapati et al., 2017; Jadhav and Rajan, 2018; Shi 1225 Article: poundland has been been forced to pull decorative plastic easter eggs from their shelves over fears children may choke - because they look like cadbury mini eggs . trading standards officials in buckinghamshire and surrey raised the alarm over the chinese made decorations , as they were ‘ likely to contravene food imitation safety rules ’ . the eggs have now been withdrawn nationwide ahead of the easter break . scroll down for video . poundland has been been forced to pull decorative plastic easter eggs from their shelves over fears they may choke - because they look like c"
D19-1117,D18-1440,0,0.0114126,"oblem (Tan et al., 2017; Shi et al., 2018; Gehrmann et al., 2018). To obtain more salient information, Chen et al. (2016) proposes a new attention mechanism to distract them in the decoding step to better grasp the overall meaning of input documents. We optimize attention using an attention refinement unit under the novel variance loss supervision. As far as we know, we are the first to propose explicit losses to refine the attention model in abstractive document summarization tasks. Recently many models (Paulus et al., 2018; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhou et al., 2018; Jiang and Bansal, 2018) have emerged taking advantage of reinforcement learning (RL) to solve the discrepancy issue in seq2seq model and have yielded the state-of-the-art performance. 5 Conclusion In this paper, we propose simple but effective methods to optimize the vanilla attention mechanism in abstarctive document summarization. The results on CNN/Daily Mail dataset demonstrate the effectiveness of our methods. We argue that these simple methods are also adaptable to other summarization models with attention. Further exploration on this and combination with other approaches like RL remains as our future explorat"
D19-1117,W04-1013,0,0.00939796,"global variance loss is somewhat similar with the coverage mechanism (Tu et al., 2016; See et al., 2017), which is also designed for solving the repetition problem. The coverage mechanism introduces a coverage vector to keep track of previous decisions at each decoding step and adds it into the attention calculation. However, when the high attention on certain position is wrongly assigned during previous timesteps, the coverage mechanism hinders the correct assignment of attention in later steps. We conduct our experiments on the CNN/Daily Mail dataset and achieve comparable results on ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) with the state-of-the-art models. 1222 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1222–1228, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Our model surpasses the strong pointer-generator baseline (w/o coverage) (See et al., 2017) on all ROUGE metrics by a large margin. As far as we know, we are the first to introduce explicit loss functions to optimize the attention. More importantly, the i"
D19-1117,K16-1028,0,0.472867,"obtain a powerful attention helping with reproducing the most salient information and avoiding repetitions, we augment the vanilla attention model from both local and global aspects. We propose an attention refinement unit paired with local variance loss to impose supervision on the attention model at each decoding step, and a global variance loss to optimize the attention distributions of all decoding steps from the global perspective. The performances on the CNN/Daily Mail dataset verify the effectiveness of our methods. 1 Introduction Abstractive document summarization (Rush et al., 2015; Nallapati et al., 2016; Tan et al., 2017; Chen and Bansal, 2018; Celikyilmaz et al., 2018) attempts to produce a condensed representation of the most salient information of the document, aspects of which may not appear as parts of the original input text. One popular framework used in abstractive summarization is the sequence-tosequence model introduced by Sutskever et al. (2014). The attention mechanism (Bahdanau et al., 2014) is proposed to enhance the sequenceto-sequence model by allowing salient features to dynamically come to the forefront as needed to make up for the incapability of memorizing the long input"
D19-1117,D15-1044,0,0.222899,"rization models. To obtain a powerful attention helping with reproducing the most salient information and avoiding repetitions, we augment the vanilla attention model from both local and global aspects. We propose an attention refinement unit paired with local variance loss to impose supervision on the attention model at each decoding step, and a global variance loss to optimize the attention distributions of all decoding steps from the global perspective. The performances on the CNN/Daily Mail dataset verify the effectiveness of our methods. 1 Introduction Abstractive document summarization (Rush et al., 2015; Nallapati et al., 2016; Tan et al., 2017; Chen and Bansal, 2018; Celikyilmaz et al., 2018) attempts to produce a condensed representation of the most salient information of the document, aspects of which may not appear as parts of the original input text. One popular framework used in abstractive summarization is the sequence-tosequence model introduced by Sutskever et al. (2014). The attention mechanism (Bahdanau et al., 2014) is proposed to enhance the sequenceto-sequence model by allowing salient features to dynamically come to the forefront as needed to make up for the incapability of me"
D19-1117,P17-1099,0,0.100053,"ve to more accurate results compared to the soft attention. To maintain good performance of hard attention as well as the advantage of endto-end trainability of soft attention, we introduce a local variance loss to encourage the model to put most of the attention on just a few parts of input states at each decoding step. Additionally, we propose a global variance loss to directly optimize the attention from the global perspective by preventing assigning high weights to the same locations multiple times. The global variance loss is somewhat similar with the coverage mechanism (Tu et al., 2016; See et al., 2017), which is also designed for solving the repetition problem. The coverage mechanism introduces a coverage vector to keep track of previous decisions at each decoding step and adds it into the attention calculation. However, when the high attention on certain position is wrongly assigned during previous timesteps, the coverage mechanism hinders the correct assignment of attention in later steps. We conduct our experiments on the CNN/Daily Mail dataset and achieve comparable results on ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) with the state-of-the-art models. 1222 Proceedings of"
D19-1117,D18-1065,0,0.0187722,"xact part should be emphasized for the next word (the output of the decoder). To make a more correct decision on what to write next, people always adjust the concentrated content by reconsidering the current state of what has been summarized already. Thus, ARU is designed as an update unit based on current decoding state, aiming to retain the attention on salient parts but weaken the attention on irrelevant parts of input. The de facto standard attention mechanism is a soft attention that assigns attention weights to all input encoder states, while according to previous work (Xu et al., 2015; Shankar et al., 2018), a well-trained hard attention on exact one input state is conducive to more accurate results compared to the soft attention. To maintain good performance of hard attention as well as the advantage of endto-end trainability of soft attention, we introduce a local variance loss to encourage the model to put most of the attention on just a few parts of input states at each decoding step. Additionally, we propose a global variance loss to directly optimize the attention from the global perspective by preventing assigning high weights to the same locations multiple times. The global variance loss"
D19-1117,P17-1108,0,0.0473861,"Missing"
D19-1117,P16-1008,0,0.399804,"state is conducive to more accurate results compared to the soft attention. To maintain good performance of hard attention as well as the advantage of endto-end trainability of soft attention, we introduce a local variance loss to encourage the model to put most of the attention on just a few parts of input states at each decoding step. Additionally, we propose a global variance loss to directly optimize the attention from the global perspective by preventing assigning high weights to the same locations multiple times. The global variance loss is somewhat similar with the coverage mechanism (Tu et al., 2016; See et al., 2017), which is also designed for solving the repetition problem. The coverage mechanism introduces a coverage vector to keep track of previous decisions at each decoding step and adds it into the attention calculation. However, when the high attention on certain position is wrongly assigned during previous timesteps, the coverage mechanism hinders the correct assignment of attention in later steps. We conduct our experiments on the CNN/Daily Mail dataset and achieve comparable results on ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) with the state-of-the-art models. 1"
D19-1117,P18-1061,0,0.0156421,"dies on saliency problem (Tan et al., 2017; Shi et al., 2018; Gehrmann et al., 2018). To obtain more salient information, Chen et al. (2016) proposes a new attention mechanism to distract them in the decoding step to better grasp the overall meaning of input documents. We optimize attention using an attention refinement unit under the novel variance loss supervision. As far as we know, we are the first to propose explicit losses to refine the attention model in abstractive document summarization tasks. Recently many models (Paulus et al., 2018; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhou et al., 2018; Jiang and Bansal, 2018) have emerged taking advantage of reinforcement learning (RL) to solve the discrepancy issue in seq2seq model and have yielded the state-of-the-art performance. 5 Conclusion In this paper, we propose simple but effective methods to optimize the vanilla attention mechanism in abstarctive document summarization. The results on CNN/Daily Mail dataset demonstrate the effectiveness of our methods. We argue that these simple methods are also adaptable to other summarization models with attention. Further exploration on this and combination with other approaches like RL remai"
D19-1139,P19-1174,1,0.410366,"er than the baseline Transformer (base or big) at the significance level p &lt;0.01 (Collins et al., 2005). BLEU Transformer (base) +RPEHead the highest BLEU scores at dr =320 and dr =256, respectively. This means that the original partial input representation and our RPE can complement each other to improve translation performance. +MPRHead 28 27.5 5.3 0 64 128 192 256 320 384 448 512 dr Figure 2: The BLEU scores on the different dr . et al., 2018). Besides, we reported results of the existing works (Vaswani et al., 2017; Chen et al., 2018; Shaw et al., 2018; Hao et al., 2019; Liu et al., 2019; Chen et al., 2019). We reimplemented the baseline Transformer, Relative PEs, and DiSAN models on the OpenNMT toolkit (Klein et al., 2017). All the models were trained for 200k batches and evaluated on a single V100 GPU. The multi-bleu.perl was used as the evaluation metric to obtain the case-sensitive 4gram BLEU score of EN-DE and ZH-EN tasks. 5.2 Effect of RPEs In this work, we extracted dr dimensions of each word vector to learn recurrent embeddings. To explore the relation between dr and translation performance, Figure 2 shows the translation performance on the different dr . For +RPEHead (or +MPRHead), with"
D19-1139,P18-1008,0,0.205951,"the input to selfattention networks (SANs), achieving state-of-theart translation performance with several language pairs (Vaswani et al., 2017; Dou et al., 2018; Zhang et al., 2018a; Marie et al., 2018, 2019). ∗ In spite of their success, the input representation only involves static order dependencies based on discrete numerical information. That is, any word in the entire vocabulary has the same PE on the same position index. As a result, the dependencies encoded by the original PEs are independent of word content, which may further hinder the improvement of translation capacity. Recently, Chen et al. (2018) and Hao et al. (2019) introduced the additional source representation learned by an RNN-based encoder into Transformer to alleviate this issue, and reported improvements on the WMT’14 English-to-German translation task. Inspired by their works (Chen et al., 2018; Hao et al., 2019), we propose an simple and efficient recurrent positional embedding approach to capture order dependencies based on word content in a sentence, thus learning a more effective sentence representation for the Transformer. In addition, we designed two simple multi-head self-attentions to introduce these learned RPEs and"
D19-1139,D18-1317,0,0.0146913,"e a recurrent positional embedding approach based on part of word embedding (6) where Wr ∈ Rdr ×dr is a parameter matrix and br ∈ Rdr is a bias item.1 Note that the xrj is derived from part of the word embedding xj . Finally, there is a sequence R ={r1 , · · · , rJ }, called as recurrent positional embeddings (RPEs). In this work, a bidirectional RNN and a forward RNN (Bahdanau et al., 2015) are used to learn source RPEs and target RPEs, respectively. Noth that the RNN is also replaced by other neural networks for learning order dependency information, such as GRU (Cho et al., 2014), and SRU (Li et al., 2018a). In addition, other sub-sequence {xp1 , · · · , xpJ } is used to gain the reduced dimension input representation P={p1 , · · · , pJ } according to the Section 2.1. Both of R and P will be together as the input to the encoder (or decoder) to learn a more effective source (or target) representation for the Transformer. 4 Neural Machine Translation with RPE To make use of these learned RPEs, we propose two simple methods: RPE head (RPEHead) selfattention and mixed positional representation head (MPRHead) self-attention. Both of RPEHead and MPRHead can utilize RPEs to learn sentence representat"
D19-1139,C18-1271,0,0.0316368,"e a recurrent positional embedding approach based on part of word embedding (6) where Wr ∈ Rdr ×dr is a parameter matrix and br ∈ Rdr is a bias item.1 Note that the xrj is derived from part of the word embedding xj . Finally, there is a sequence R ={r1 , · · · , rJ }, called as recurrent positional embeddings (RPEs). In this work, a bidirectional RNN and a forward RNN (Bahdanau et al., 2015) are used to learn source RPEs and target RPEs, respectively. Noth that the RNN is also replaced by other neural networks for learning order dependency information, such as GRU (Cho et al., 2014), and SRU (Li et al., 2018a). In addition, other sub-sequence {xp1 , · · · , xpJ } is used to gain the reduced dimension input representation P={p1 , · · · , pJ } according to the Section 2.1. Both of R and P will be together as the input to the encoder (or decoder) to learn a more effective source (or target) representation for the Transformer. 4 Neural Machine Translation with RPE To make use of these learned RPEs, we propose two simple methods: RPE head (RPEHead) selfattention and mixed positional representation head (MPRHead) self-attention. Both of RPEHead and MPRHead can utilize RPEs to learn sentence representat"
D19-1139,P19-1352,0,0.0214787,"significantly better than the baseline Transformer (base or big) at the significance level p &lt;0.01 (Collins et al., 2005). BLEU Transformer (base) +RPEHead the highest BLEU scores at dr =320 and dr =256, respectively. This means that the original partial input representation and our RPE can complement each other to improve translation performance. +MPRHead 28 27.5 5.3 0 64 128 192 256 320 384 448 512 dr Figure 2: The BLEU scores on the different dr . et al., 2018). Besides, we reported results of the existing works (Vaswani et al., 2017; Chen et al., 2018; Shaw et al., 2018; Hao et al., 2019; Liu et al., 2019; Chen et al., 2019). We reimplemented the baseline Transformer, Relative PEs, and DiSAN models on the OpenNMT toolkit (Klein et al., 2017). All the models were trained for 200k batches and evaluated on a single V100 GPU. The multi-bleu.perl was used as the evaluation metric to obtain the case-sensitive 4gram BLEU score of EN-DE and ZH-EN tasks. 5.2 Effect of RPEs In this work, we extracted dr dimensions of each word vector to learn recurrent embeddings. To explore the relation between dr and translation performance, Figure 2 shows the translation performance on the different dr . For +RPEHead"
D19-1139,D14-1179,0,0.090697,"Missing"
D19-1139,W19-5330,1,0.891821,"Missing"
D19-1139,P05-1066,0,0.659276,"Architecture Existing NMT systems Transformer (base) Transformer (big) RNMT+SAN Transformer (base)+BiARN Transformer (big)+BiARN Our NMT systems Transformer (base) +Relative PE +DiSAN +RPEHead +MPRHead Transformer (big) +MPRHead newstest2014 #Param 27.3 28.4 28.49 28.21 28.98 65M 213M 378.9M 97.4M 323.5M 27.25 27.60 27.66 28.11* 28.35* 28.22 29.11* 97.35M 97.42M 97.39M 97.84M 97.72M 272.6M 289.1M Table 1: Results for EN-DE translation task. The mark “*” after scores indicates that the model was significantly better than the baseline Transformer (base or big) at the significance level p &lt;0.01 (Collins et al., 2005). BLEU Transformer (base) +RPEHead the highest BLEU scores at dr =320 and dr =256, respectively. This means that the original partial input representation and our RPE can complement each other to improve translation performance. +MPRHead 28 27.5 5.3 0 64 128 192 256 320 384 448 512 dr Figure 2: The BLEU scores on the different dr . et al., 2018). Besides, we reported results of the existing works (Vaswani et al., 2017; Chen et al., 2018; Shaw et al., 2018; Hao et al., 2019; Liu et al., 2019; Chen et al., 2019). We reimplemented the baseline Transformer, Relative PEs, and DiSAN models on the Op"
D19-1139,W18-6419,1,0.850701,"t and convolutional neural networks, rely on a positional embedding (PE) approach to encode order information into the input representation. PE is typically learned based on the position index of each word and is added to corresponding word embedding. This allows the Transformer to encode order dependencies between words in addition to the words themselves. Finally, the Transformer uses these combined vectors as the input to selfattention networks (SANs), achieving state-of-theart translation performance with several language pairs (Vaswani et al., 2017; Dou et al., 2018; Zhang et al., 2018a; Marie et al., 2018, 2019). ∗ In spite of their success, the input representation only involves static order dependencies based on discrete numerical information. That is, any word in the entire vocabulary has the same PE on the same position index. As a result, the dependencies encoded by the original PEs are independent of word content, which may further hinder the improvement of translation capacity. Recently, Chen et al. (2018) and Hao et al. (2019) introduced the additional source representation learned by an RNN-based encoder into Transformer to alleviate this issue, and reported improvements on the WMT’14"
D19-1139,D18-1457,0,0.0176619,"Vaswani et al., 2017), without recurrent and convolutional neural networks, rely on a positional embedding (PE) approach to encode order information into the input representation. PE is typically learned based on the position index of each word and is added to corresponding word embedding. This allows the Transformer to encode order dependencies between words in addition to the words themselves. Finally, the Transformer uses these combined vectors as the input to selfattention networks (SANs), achieving state-of-theart translation performance with several language pairs (Vaswani et al., 2017; Dou et al., 2018; Zhang et al., 2018a; Marie et al., 2018, 2019). ∗ In spite of their success, the input representation only involves static order dependencies based on discrete numerical information. That is, any word in the entire vocabulary has the same PE on the same position index. As a result, the dependencies encoded by the original PEs are independent of word content, which may further hinder the improvement of translation capacity. Recently, Chen et al. (2018) and Hao et al. (2019) introduced the additional source representation learned by an RNN-based encoder into Transformer to alleviate this issue"
D19-1139,P16-1162,0,0.0431505,"r architecture. 5 5.1 Experiments Experimental Setup The proposed methods were evaluated on the WMT’14 English to German (EN-DE) and NIST Chinese-to-English (ZH-EN) translation tasks. The ZH-EN training set includes 1.28 million bilingual sentence pairs from the LDC corpora, where the NIST06 and the NIST02/NIST03/NIST04 data sets were used as the development and test sets, respectively. The EN-DE training set includes 4.43 million bilingual sentence pairs of the WMT’14 corpora, where the newstest2013 and newstest2014 data sets were used as the development and test sets, respectively. The BPE (Sennrich et al., 2016) was adopted and the vocabulary size was set as 32K. The dimension of all input and output layers was set to 512, and that of the inner feedforward neural network layer was set to 2048. The total heads of all multi-head modules were set to 8 in both encoder and decoder layers. In each training batch, there was a set of sentence pairs containing approximately 4096*4 source tokens and 4096*4 target tokens. For the other setting not mentioned, we followed the setting in Vaswani et al. (2017). Baseline systems included a vanilla Transformer (Vaswani et al., 2017), Relative PEs (Shaw et al., 2018),"
D19-1139,N19-1122,0,0.0571523,"ion networks (SANs), achieving state-of-theart translation performance with several language pairs (Vaswani et al., 2017; Dou et al., 2018; Zhang et al., 2018a; Marie et al., 2018, 2019). ∗ In spite of their success, the input representation only involves static order dependencies based on discrete numerical information. That is, any word in the entire vocabulary has the same PE on the same position index. As a result, the dependencies encoded by the original PEs are independent of word content, which may further hinder the improvement of translation capacity. Recently, Chen et al. (2018) and Hao et al. (2019) introduced the additional source representation learned by an RNN-based encoder into Transformer to alleviate this issue, and reported improvements on the WMT’14 English-to-German translation task. Inspired by their works (Chen et al., 2018; Hao et al., 2019), we propose an simple and efficient recurrent positional embedding approach to capture order dependencies based on word content in a sentence, thus learning a more effective sentence representation for the Transformer. In addition, we designed two simple multi-head self-attentions to introduce these learned RPEs and original input repres"
D19-1139,N18-2074,0,0.243236,"nrich et al., 2016) was adopted and the vocabulary size was set as 32K. The dimension of all input and output layers was set to 512, and that of the inner feedforward neural network layer was set to 2048. The total heads of all multi-head modules were set to 8 in both encoder and decoder layers. In each training batch, there was a set of sentence pairs containing approximately 4096*4 source tokens and 4096*4 target tokens. For the other setting not mentioned, we followed the setting in Vaswani et al. (2017). Baseline systems included a vanilla Transformer (Vaswani et al., 2017), Relative PEs (Shaw et al., 2018), and directional SAN (DiSAN) (Shen 1363 System Vaswani et al. (2017) Chen et al. (2018) Hao et al. (2019) This work Architecture Existing NMT systems Transformer (base) Transformer (big) RNMT+SAN Transformer (base)+BiARN Transformer (big)+BiARN Our NMT systems Transformer (base) +Relative PE +DiSAN +RPEHead +MPRHead Transformer (big) +MPRHead newstest2014 #Param 27.3 28.4 28.49 28.21 28.98 65M 213M 378.9M 97.4M 323.5M 27.25 27.60 27.66 28.11* 28.35* 28.22 29.11* 97.35M 97.42M 97.39M 97.84M 97.72M 272.6M 289.1M Table 1: Results for EN-DE translation task. The mark “*” after scores indicates th"
D19-1139,P19-1119,1,0.863476,"Missing"
D19-1139,P18-1166,0,0.130583,"Missing"
D19-1139,C18-1153,0,0.027204,"17), without recurrent and convolutional neural networks, rely on a positional embedding (PE) approach to encode order information into the input representation. PE is typically learned based on the position index of each word and is added to corresponding word embedding. This allows the Transformer to encode order dependencies between words in addition to the words themselves. Finally, the Transformer uses these combined vectors as the input to selfattention networks (SANs), achieving state-of-theart translation performance with several language pairs (Vaswani et al., 2017; Dou et al., 2018; Zhang et al., 2018a; Marie et al., 2018, 2019). ∗ In spite of their success, the input representation only involves static order dependencies based on discrete numerical information. That is, any word in the entire vocabulary has the same PE on the same position index. As a result, the dependencies encoded by the original PEs are independent of word content, which may further hinder the improvement of translation capacity. Recently, Chen et al. (2018) and Hao et al. (2019) introduced the additional source representation learned by an RNN-based encoder into Transformer to alleviate this issue, and reported impro"
D19-5209,D18-1549,0,0.0213276,"train from scratch a new source-to-target NMT system. TLM Before training NMT, we used all training corpora including parallel data and monolingual data to train a translation language model (TLM) using XLM3 in order to pretrain the NMT model on 8 GPUs4 . The parameters for training the language model were set as listed in Table 3. --lgs ’en-my’ --mlm steps ’en,my,en-my,my-en’ --emb dim 1024 --n layers 6 --n heads 8 --dropout 0.1 --attention dropout 0.1 --gelu activation true --batch size 32 --bptt 256 --optimizer adam,lr=0.0001 3.4 UNMT To the best of our knowledge, unsupervised NMT (UNMT) (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Lample and Conneau, 2019) has achieved remarkable results on some similar language pairs. To obtain a better picture of the feasibility of UNMT, we also set up a UNMT system for one truly low-resource and distant language pair: En-My. We tried to train a Transformer-based UNMT model that relies solely on monolingual corpora, with the pre-trained cross-lingual language model using XLM toolkit. Note that this cross-lingual language model was trained solely on monolingual corpora shown in Section 2. We used these m"
D19-5209,W19-5330,1,0.708168,"r for English. The truecaser was trained on the English data, after tokenization. For Myanmar, we used the original tokens. For cleaning, we only applied the Moses script clean-n-corpus.perl to remove lines in the parallel data containing more than 80 tokens and replaced characters forbidden by Moses. Data Preprocessing As parallel data to train our systems, we used all the provided parallel data for all our targeted ∗ Rui and Haipeng have equal contribution to this paper. This work was conductd when Haipeng visited NICT as an internship student. 1 This system is based on our WMT-2019 system (Marie et al., 2019). 2 http://data.statmt.org/news-crawl/ 90 Proceedings of the 6th Workshop on Asian Translation, pages 90–93 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 3 MT Systems --lgs ’en-my’ --encoder only false --emb dim 1024 --n layers 6 --n heads 8 --dropout 0.1 --attention dropout 0.1 --gelu activation true --tokens per batch 2000 --batch size 32 --bptt 256 --optimizer adam inverse sqrt,beta1=0.9, beta2=0.98,lr=0.0001 --eval bleu true To build competitive NMT systems, we chose to rely on the Transformer architecture (Vaswani et al., 2017) since it has been show"
D19-5209,P02-1040,0,0.110492,"oolkit. Note that this cross-lingual language model was trained solely on monolingual corpora shown in Section 2. We used these monolingual corpora to train the UNMT model for 50000 iterations. The En-My UNMT system was trained on 8 GPUs, with the parameters listed in Table 6. Table 3: Parameters for training TLM. 3.2 Back-translation NMT We trained a Transformer-based NMT model with the pre-trained TLM using XLM toolkit. Our NMT system was consistently trained on 8 GPUs, with the following parameters listed in Table 4. We performed NMT decoding with a single model according to the best BLEU (Papineni et al., 2002) and the perplexity scores. 3 https://github.com/facebookresearch/ XLM 4 NVIDIA @ Tesla @ V100 32Gb. 91 Systems UNMT NMT NMT NMT+TLM NMT+TLM NMT+TLM+back-translation ALT UCSY X X X X X X X X X MONO My-En En-My X 0.81 8.06 14.97 18.42 21.33 29.89 0.31 10.50 14.15 16.12 19.73 19.01 X X Table 5: Results (BLEU-cased) of our MT systems on the test set. ALT denotes that ALT training data was used in this system; UCSY denotes that UCSY training data was used in this system; MONO denotes monolingual training data was used in this system. +TLM denotes that language model pretraining was used in this sy"
D19-5209,P16-1009,0,0.035489,"al., 2017) since it has been shown to outperform, in quality and efficiency, the two other mainstream architectures for NMT known as deep recurrent neural network (deep-RNN) and convolutional neural network (CNN). We chose to rely on the Transformer-based NMT initialized by a pretrained cross-lingual language model (Lample and Conneau, 2019) to train our NMT systems since it had been shown to be efficient in the low-resource language pairs. In order to limit the size of the vocabulary of the NMT model, we segmented tokens in the training data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b). We determined 60k BPE operations jointly on the training data for English and Myanmar, and used a shared vocabulary for both languages with 60k tokens based on BPE. 3.1 Table 4: Parameters for training NMT. 3.3 We also tried back-translation method (Sennrich et al., 2016a) to make use of monolingual corpora for English-to-Myanmar translation task. Parallel data for training NMT can be augmented with synthetic parallel data, generated through backtranslation, to significantly improve translation quality. For back-translation generation, we used an NMT system, trained on the parallel data pr"
D19-5209,P16-1162,0,0.0522179,"al., 2017) since it has been shown to outperform, in quality and efficiency, the two other mainstream architectures for NMT known as deep recurrent neural network (deep-RNN) and convolutional neural network (CNN). We chose to rely on the Transformer-based NMT initialized by a pretrained cross-lingual language model (Lample and Conneau, 2019) to train our NMT systems since it had been shown to be efficient in the low-resource language pairs. In order to limit the size of the vocabulary of the NMT model, we segmented tokens in the training data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b). We determined 60k BPE operations jointly on the training data for English and Myanmar, and used a shared vocabulary for both languages with 60k tokens based on BPE. 3.1 Table 4: Parameters for training NMT. 3.3 We also tried back-translation method (Sennrich et al., 2016a) to make use of monolingual corpora for English-to-Myanmar translation task. Parallel data for training NMT can be augmented with synthetic parallel data, generated through backtranslation, to significantly improve translation quality. For back-translation generation, we used an NMT system, trained on the parallel data pr"
D19-5209,P19-1119,1,0.8583,"Missing"
D19-5209,P18-1005,0,0.0212656,"MT system. TLM Before training NMT, we used all training corpora including parallel data and monolingual data to train a translation language model (TLM) using XLM3 in order to pretrain the NMT model on 8 GPUs4 . The parameters for training the language model were set as listed in Table 3. --lgs ’en-my’ --mlm steps ’en,my,en-my,my-en’ --emb dim 1024 --n layers 6 --n heads 8 --dropout 0.1 --attention dropout 0.1 --gelu activation true --batch size 32 --bptt 256 --optimizer adam,lr=0.0001 3.4 UNMT To the best of our knowledge, unsupervised NMT (UNMT) (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Lample and Conneau, 2019) has achieved remarkable results on some similar language pairs. To obtain a better picture of the feasibility of UNMT, we also set up a UNMT system for one truly low-resource and distant language pair: En-My. We tried to train a Transformer-based UNMT model that relies solely on monolingual corpora, with the pre-trained cross-lingual language model using XLM toolkit. Note that this cross-lingual language model was trained solely on monolingual corpora shown in Section 2. We used these monolingual corpora to train the UNMT mode"
E09-1025,W07-1422,0,0.0394271,"Missing"
E09-1025,W07-1409,0,0.0678962,"ncy parse trees of T and H, which aims to capture the essential information they convey. The rest of the paper is organized as follows: Section 2 introduces the inference rule collection Introduction Textual inference plays an important role in many natural language processing (NLP) tasks. In recent years, the recognizing textual entailment (RTE) (Dagan et al., 2006) challenge, which focuses on detecting semantic inference, has attracted a lot of attention. Given a text T (several sentences) and a hypothesis H (one sentence), the goal is to detect if H can be inferred from T. Studies such as (Clark et al., 2007) attest that lexical substitution (e.g. synonyms, antonyms) or simple syntactic variation account for the entailment only in a small number of pairs. Thus, one essential issue is to identify more complex expressions which, in appropriate contexts, convey the same (or similar) meaning. However, more generally, we are also interested in pairs of expressions in which only a uni-directional inference relation holds1 . 1 2 We will use the term inference rule to stand for such concept; the two expressions can be actual paraphrases if the relation is bi-directional Another line of work on acquiring p"
E09-1025,P08-1118,0,0.0486946,"Missing"
E09-1025,W07-1401,0,0.327678,"Missing"
E09-1025,W07-1421,0,0.123603,"Missing"
E09-1025,W07-1414,0,0.0415875,"patterns are embedded block the entailment (e.g. through negative markers, modifiers, embedding verbs not preserving entailment)7 • The rule is correct in a limited number of contexts, but the current context is not the correct one. To sum up, making use of the knowledge encoded with such rules is not a trivial task. If rules are used strictly in concordance with their definition, their utility is limited to a very small number of entailment pairs. For this reason, 1) instead of forcing the anchor values to be identical as most previous work, we allow more flexible rule matching (similar to (Marsi et al., 2007)) and 2) furthermore, we control the rule application process using a text representation based on dependency structure. T: Libya’s case against Britain and the US concerns the dispute over their demand for extradition of Libyans charged with blowing up a Pan Am jet over Lockerbie in 1988. H: One case involved the extradition of Libyan suspects 4.2 in the Pan Am Lockerbie bombing. Tree Skeleton The Tree Skeleton (TS) structure was proposed by (Wang and Neumann, 2007), and can be viewed as an extended version of the predicate-argument structure. Since it contains not only the predicate and its"
E09-1025,W06-3907,0,0.0211903,"inference rule is needed, subsequent issues arise from the way these fragments of text interact with the surrounding context. Assuming we have a correct rule present in an entailment pair, the cases in which the pair is still not a positive case of entailment can be summarized as follows: T For their discovery of ulcer-causing bacteria, Australian doctors Robin Warren and Barry Marshall have received the 2005 Nobel Prize in Physiology or Medicine. H Robin Warren was awarded a Nobel Prize. Notice that, in order to match the inference rules with two anchors, the number of the dependency 7 See (Nairn et al., 2006) for a detailed analysis of these aspects. 8 Here we also use Minipar for the reason of consistence • The entailment rule is present in parts of the text which are not relevant to the entailment 215 select the RTE pairs in which we find a tree skeleton and match an inference rule. The first number in our table entries represents how many of such pairs we have identified, out the 1600 of development and test pairs. For these pairs we simply predict positive entailment and the second entry represents what percentage of these pairs are indeed positive entailment. Our work does not focus on buildi"
E09-1025,N03-1024,0,0.0106679,"variation account for the entailment only in a small number of pairs. Thus, one essential issue is to identify more complex expressions which, in appropriate contexts, convey the same (or similar) meaning. However, more generally, we are also interested in pairs of expressions in which only a uni-directional inference relation holds1 . 1 2 We will use the term inference rule to stand for such concept; the two expressions can be actual paraphrases if the relation is bi-directional Another line of work on acquiring paraphrases uses comparable corpora, for instance (Barzilay and McKeown, 2001), (Pang et al., 2003) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 211–219, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 211 X put emphasis on Y ≈ X pay attention to Y ≈ X attach importance to Y ≈ X increase spending on Y ≈ X place emphasis on Y ≈ Y priority of X ≈ X focus on Y we use, based on the Discovery of Inference Rules from Text (henceforth DIRT) algorithm and discusses previous work on applying it to the RTE task. Section 3 focuses on the rule collection itself and on the methods in which we use an external lexical resource to ex"
E09-1025,I05-5011,0,0.164568,"Missing"
E09-1025,W04-3206,0,0.10438,"Missing"
E09-1025,P07-1058,0,0.0494239,"self and on the methods in which we use an external lexical resource to extend and refine it. Section 4 discusses the application of the rules for the RTE data, describing the structure representation we use to identify the appropriate context for the rule application. The experimental results will be presented in Section 5, followed by an error analysis and discussions in Section 6. Finally Section 7 will conclude the paper and point out future work directions. 2 Table 1: Example of DIRT algorithm output. Most confident paraphrases of X put emphasis on Y Such rules can be informally defined (Szpektor et al., 2007) as directional relations between two text patterns with variables. The left-handside pattern is assumed to entail the right-handside pattern in certain contexts, under the same variable instantiation. The definition relaxes the intuition of inference, as we only require the entailment to hold in some and not all contexts, motivated by the fact that such inferences occur often in natural text. Background A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005). In our work we use the DIRT collection because it is th"
E09-1025,P08-1078,0,0.02782,"ng in similar contexts are similar. An inference rule in DIRT is a pair of binary relations h pattern1 (X, Y ), pattern2 (X, Y ) i which stand in an inference relation. pattern1 and pattern2 are chains in dependency trees3 while X and Y are placeholders for nouns at the end of this chain. The two patterns will constitute a candidate paraphrase if the sets of X and Y values exhibit relevant overlap. In the following example, the two patterns are prevent and provide protection against. subj Current work on inference rules focuses on making such resources more precise. (Basili et al., 2007) and (Szpektor et al., 2008) propose attaching selectional preferences to inference rules. These are semantic classes which correspond to the anchor values of an inference rule and have the role of making precise the context in which the rule can be applied 5 . This aspect is very important and we plan to address it in our future work. However in this paper we investigate the first and more basic issue: how to successfully use rules in their current form. obj X ←−−− prevent −−→ Y subj obj mod 4 pcomp X ←−−− provide −−→ protection −−−→ against −−−−→ Y 3 obtained with the Minipar parser (Lin, 1998) 212 For simplification,"
E09-1025,W07-1406,1,0.915205,"Missing"
E09-1025,P06-1051,0,0.167228,"Missing"
E09-1025,P01-1008,0,\N,Missing
I17-1002,P05-1066,0,0.210454,"Missing"
I17-1002,P16-2058,0,0.0359599,"Missing"
I17-1002,W04-3208,0,0.0171983,"the OOV’s semantic information. Second, we also extend the contextaware smoothing method to in-vocabulary words, which enhances encoder and decoder of NMT by more effectively utilizing context information by the learned CAR. To this end, we proposed two unique neural networks to learn the contextaware representation for each word depending on its context words in a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focu"
I17-1002,P15-1001,0,0.0317148,"n 5 reports the experimental results obtained in the Chineseto-English task. Finally, we conclude the contributions of the paper and discuss the further work in Section 6. 12 proposed method can smooth the representation of word and reduce the unk’s negative effect in attention model, context annotations and decoding hidden states, thus improving the performance of NMT. these methods improved the translation of OOV, they must learn external bilingual dictionary information in advance. From the point of vocabulary size, many works tried to use a large vocabulary size, thus covering more words. Jean et al. (2015) proposed a method based on importance sampling that allowed NMT model to use a very large target vocabulary for relieving the OOV phenomenon in NMT, which are only designed to reduce the computational complexity in training, not for decoding. Arthur et al. (2016) introduced discrete translation lexicons into NMT to imrpove the translations of these low-frequency words. Mi et al. (2016) proposed a vocabulary manipulation approach by limiting the number of vocabulary being predicted by each batch or sentence, to reduce both the training and the decoding complexity. These methods focused on the"
I17-1002,P02-1051,0,0.0320588,"a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the correct translation of the unknown words with external resources and ignored the negative effect for other words. Compared with PBSMT, due to high computational cost, NMT has a more limited vocabulary size and severe OOV phenomenon. The existing PBSMT methods that used external resources to translate unknown words for SMT are hard to be directly intro"
I17-1002,D16-1162,0,0.0215161,"ect in attention model, context annotations and decoding hidden states, thus improving the performance of NMT. these methods improved the translation of OOV, they must learn external bilingual dictionary information in advance. From the point of vocabulary size, many works tried to use a large vocabulary size, thus covering more words. Jean et al. (2015) proposed a method based on importance sampling that allowed NMT model to use a very large target vocabulary for relieving the OOV phenomenon in NMT, which are only designed to reduce the computational complexity in training, not for decoding. Arthur et al. (2016) introduced discrete translation lexicons into NMT to imrpove the translations of these low-frequency words. Mi et al. (2016) proposed a vocabulary manipulation approach by limiting the number of vocabulary being predicted by each batch or sentence, to reduce both the training and the decoding complexity. These methods focused on the translation of OOV itself and ignored the other negative effect caused by the OOV, such as the translations of the words around the OOV. 3 Context-Aware Representation Intuitively, when one understands natural language sentence, especially including polysemy words"
I17-1002,D13-1176,0,0.372214,"representation of unk. To alleviate this problem, we propose a novel contextaware smoothing method to dynamically learn a sentence-specific vector for each word (including OOV words) depending on its local context words in a sentence. The learned context-aware representation is integrated into the NMT to improve the translation performance. Empirical results on NIST Chinese-to-English translation task show that the proposed approach achieves 1.78 BLEU improvements on average over a strong attentional NMT, and outperforms some existing systems. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), has shown prominent performances in comparison with the conventional Phrase Based Statistical Machine Translation (PBSMT) (Koehn et al., 2003). In NMT, a source sentence is converted into a vector representation by an RNN called encoder, then another RNN ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. 11 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 11–20, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP x1 x2 x3 x4 xu … xJ v1"
I17-1002,P97-1017,0,0.104382,"each word depending on its context words in a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the correct translation of the unknown words with external resources and ignored the negative effect for other words. Compared with PBSMT, due to high computational cost, NMT has a more limited vocabulary size and severe OOV phenomenon. The existing PBSMT methods that used external resources to translate u"
I17-1002,P07-2045,0,0.0134503,"Missing"
I17-1002,P09-1089,0,0.0223355,"effectively utilizing context information by the learned CAR. To this end, we proposed two unique neural networks to learn the contextaware representation for each word depending on its context words in a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the correct translation of the unknown words with external resources and ignored the negative effect for other words. Compared with PBSMT, due to"
I17-1002,N03-1017,0,0.0655639,"cal context words in a sentence. The learned context-aware representation is integrated into the NMT to improve the translation performance. Empirical results on NIST Chinese-to-English translation task show that the proposed approach achieves 1.78 BLEU improvements on average over a strong attentional NMT, and outperforms some existing systems. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), has shown prominent performances in comparison with the conventional Phrase Based Statistical Machine Translation (PBSMT) (Koehn et al., 2003). In NMT, a source sentence is converted into a vector representation by an RNN called encoder, then another RNN ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. 11 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 11–20, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP x1 x2 x3 x4 xu … xJ v1 v2 v3 v4 vu … vJ 1 2 3 5 … J 4 … They are beating each other for a dispute x1 x2 x3 v1 v2 v3 1 2 3 (a) … Attention α x4 x5 … v4 v5 … vJ 5 … J 4 xJ … Decoder Encoder Trg2 Encoder Src1 他们 想 通过 打"
I17-1002,P02-1040,0,0.100942,"Missing"
I17-1002,E17-3017,0,0.069622,"Missing"
I17-1002,P16-1100,0,0.0170641,"y, suppose there is a source language sentence, {x1 , x2 , . . . , xj , . . . , xJ }. If the context window is set as 2n (n = 2), the context of each word xi is defined as its historical n words and future n words: Recently, many works exploited the granularity translation unit from words to smaller subwords or characters. Sennrich et al. (2016) introduced a simpler and more effective approach to encode rare and unknown words as sequences of subword units by Byte Pair Encoding (Gage, 1994). This is based on the intuition that various word classes are translatable via smaller units than words. Luong and Manning (2016) segmented the known words into character sequence, and learned the unknown word representation by characterlevel recurrent neural networks, thus achieving open vocabulary NMT. Li et al. (2016) replaced OOVs with in-vocabulary words by semantic similarity to reduce the negative effect for words around the OOVs. Costa-juss`a and Fonollosa (2016) presented a character-based NMT, in which character-level embeddings were in combination with convolutional and highway layers to replace the standard lookup-based word representations. These methods extended the vocabulary to a larger or unlimited voca"
I17-1002,P16-1162,0,0.370792,"presentation (CAR) for each word. 3.1 Feedforward Context-of-Words Model Inspired by the representation learning of word (Bengio et al., 2003), the proposed FCWM includes an input layer, a projection layer, and a non-linear output layer, as shown in Figure 2 (a). Specifically, suppose there is a source language sentence, {x1 , x2 , . . . , xj , . . . , xJ }. If the context window is set as 2n (n = 2), the context of each word xi is defined as its historical n words and future n words: Recently, many works exploited the granularity translation unit from words to smaller subwords or characters. Sennrich et al. (2016) introduced a simpler and more effective approach to encode rare and unknown words as sequences of subword units by Byte Pair Encoding (Gage, 1994). This is based on the intuition that various word classes are translatable via smaller units than words. Luong and Manning (2016) segmented the known words into character sequence, and learned the unknown word representation by characterlevel recurrent neural networks, thus achieving open vocabulary NMT. Li et al. (2016) replaced OOVs with in-vocabulary words by semantic similarity to reduce the negative effect for words around the OOVs. Costa-juss"
I17-1002,P15-1002,0,0.0275471,"d the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the correct translation of the unknown words with external resources and ignored the negative effect for other words. Compared with PBSMT, due to high computational cost, NMT has a more limited vocabulary size and severe OOV phenomenon. The existing PBSMT methods that used external resources to translate unknown words for SMT are hard to be directly introduced into NMT, because of NMT’s soft-alignment mechanism (Bahdanau et al., 2015). To relieve the negative effect of unknown words for NMT, Luong et al. (2015) proposed a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence, and to translate every OOV in a post-processing step using a external bilingual dictionary. Although The remainder of the paper is organized as follows. Section 2 introduces the related work in the NMT. Section 3 presents two novel neural models to learn the CAR for each word. Section 4 integrates the CAR into the NMT by using smoothing strategies. Section 5 reports the experimental results obtained in the Chineseto-Engl"
I17-1002,C04-1089,0,0.0293475,"ion. Second, we also extend the contextaware smoothing method to in-vocabulary words, which enhances encoder and decoder of NMT by more effectively utilizing context information by the learned CAR. To this end, we proposed two unique neural networks to learn the contextaware representation for each word depending on its context words in a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the corr"
I17-1002,D09-1040,0,0.0346227,"nd decoder of NMT by more effectively utilizing context information by the learned CAR. To this end, we proposed two unique neural networks to learn the contextaware representation for each word depending on its context words in a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the correct translation of the unknown words with external resources and ignored the negative effect for other words. Co"
I17-1002,P16-2021,0,0.0221027,"oved the translation of OOV, they must learn external bilingual dictionary information in advance. From the point of vocabulary size, many works tried to use a large vocabulary size, thus covering more words. Jean et al. (2015) proposed a method based on importance sampling that allowed NMT model to use a very large target vocabulary for relieving the OOV phenomenon in NMT, which are only designed to reduce the computational complexity in training, not for decoding. Arthur et al. (2016) introduced discrete translation lexicons into NMT to imrpove the translations of these low-frequency words. Mi et al. (2016) proposed a vocabulary manipulation approach by limiting the number of vocabulary being predicted by each batch or sentence, to reduce both the training and the decoding complexity. These methods focused on the translation of OOV itself and ignored the other negative effect caused by the OOV, such as the translations of the words around the OOV. 3 Context-Aware Representation Intuitively, when one understands natural language sentence, especially including polysemy words or OOVs, one often inferences the meaning of these words depending on its context words. Context plays an important role in"
K19-2004,P18-2058,0,0.0637148,"rsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expression of the candidate graph nodes, (2) Non-strictly anchored"
K19-2004,P13-1023,0,0.215914,"of transducing natural language text into AMR, which is a graph-based formalism used for capturing sentence-level semantics. The AMR framework backgrounds notions of compositionality and derivation, therefore, without explicit correspondence between graph nodes and lexical units. In the representation of AMR framework, the graph nodes are obtained by composition, derivation, lexical decomposition, normalization towards verb senses and so on. The features of the AMR graphs built on these graph nodes is similar UCCA UCCA is a multi-layer linguistic framework for semantic annotation proposed by Abend and Rappoport (2013). UCCA aims to recognize the level of semantic granularity which abstracts away from syntactic paraphrases in a typologicallymotivated, cross-linguistic fashion and does not need to rely on language-specific resources. In the representation of the UCCA framework, some nodes have a one-to-one correspondence with the span in the sentence, which is called 3 MRP-transformed UCCA graph differs from on the terminal nodes from the original UCCA graph. In the original UCCA graph representation, terminal nodes refer to words, and in the MRP-transformed UCCA graph, terminal nodes refer to the lowest lay"
K19-2004,D19-1538,1,0.812819,"2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expression of the candidate graph nodes, (2) Non-strictly anchored (UCCA): treats it as a special constituent tree parsing task a"
K19-2004,W13-2322,0,0.120818,"Missing"
K19-2004,P18-1192,1,0.922788,"rsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expression of the candidate graph nodes, (2) Non-strictly anchored"
K19-2004,W12-3602,0,0.323392,"BERT as the encoder. In the training phase, in order to prevent the nodes from falling into local optimum and the edges unable to get enough training, we use the random sampling method on the golden graph nodes to push as many correct nodes as possible to join the edge training. According to the official results of the evaluation, our system ranked second place in the overall F1 metric among the 16 participating systems. On the DM framework, our system achieved the best results. Our system on other 4 frameworks (PSD, EDS, UCCA, and AMR) are all ranked the third place. 2 2.1 DM and PSD The DM (Ivanova et al., 2012) and PSD (Hajic et al., 2012; Miyao et al., 2014) are two independently developed syntactic-semantic annotations which project semantic forms onto bilexical dependencies in a lossy manner. In the representation of the DM and PSD frameworks, the graph nodes and surface lexical units are strictly anchored. There is an explicit, one-to-many anchoring onto sub-strings of the underlying sentence. These graphs are neither fully connected nor rooted. The graphs of DM and PSD have the following features: • There is only a one-to-one correspondence1 between the graph node and the span in the sentence."
K19-2004,C18-1233,1,0.869457,"Missing"
K19-2004,S19-2002,0,0.248133,"ersal Scenarios” and NICT tenuretrack researcher startup fund “Toward Intelligent Machine Translation”. 45 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored ("
K19-2004,N19-1423,0,0.0142873,"edge target label. Overall, we use multi-tasking learning strategy, shared hidden representation, The top node uses the same mechanism as node scoring, using binary crossentropy as loss implementation. The node pos tag and node frame label use independent feed-forward classifier, using cross-entropy as loss implementation. The edge source label and edge target label use a biaffine scorer consistent with the edge label, using cross-entropy loss as well. We accumulate the loss of all goals together. Neural Architecture Our model builds the candidate graph nodes representation based on the BERT (Devlin et al., 2019) encoder outputs, i.e., for each token wi , the contextualized vector from BERT encoder is denoted as xi . The candidate span (i, j) representation h consists of two endpoint contextualized vectors (xi , xj ) where i and j are the start and end position of the span in the sentence: h = [xi ; xj ]. 4.2 For the UCCA framework, we directly adopt the minimal span-based parser of Stern et al. (2017) on the converted constituent trees. A constituency tree can be regarded as a collection of labeled spans over a sentence. There are two components in the constituent parsing model: one is to assign the"
K19-2004,C18-1271,1,0.840306,"lation”. 45 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge tr"
K19-2004,D18-1262,1,0.847064,"lation”. 45 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge tr"
K19-2004,flickinger-etal-2014-towards,0,0.0124799,"ection, we will introduce this shared task and our modeling approach. Our key idea is to use a graph-based approach rather than a transition-based one; therefore, all the modeling and optimization methods we have on these frameworks are graph-based. The CoNLL shared 46 terminal nodes3 . Other nodes do not have any corresponding relationship with the span, which is introduced as a notion of a semantic constituency that transcends the pure dependency graphs to represent the semantic granularity. The UCCA graph has the following features: (2006) which encode the English Resource Semantics (ERS) (Flickinger et al., 2014). The EDS conversion from under-specified logical forms of the full ERS to variable-free graphs discards partial semantic information which makes the graph abstractly. In the representation of the EDS framework, the graph nodes are independent of surface lexical units. For each graph node, there is an explicit, many-to-many anchoring onto sub-strings of the underlying sentence. The EDS graph has the following features: • There is a one-to-one correspondence between the terminal nodes and the spans in the sentence. • Graph nodes may have multiple parents, among which one is annotated as the pri"
K19-2004,D18-1198,0,0.0419396,"ch span s ∈ Sd do 6: for each word w ∈ s do 7: find a maximum range parent node np of word w whose range size is less than s; 8: move node np to be the child of n(t), and concatenate the original edge label with “ancestor-d” where d represents the original number of edges between the ancestor of np and n(t); 9: remove all the children words of np from s; 10: end for 11: end for 12: until T (t) is a constituent tree 13: set Tc = T (t) Anonymization in AMR Framework Anonymization is an important AMR preprocessing method to reduce the data sparsity issue (Werling et al., 2015; Peng et al., 2017; Guo and Lu, 2018). Following the practice of Zhang et al. (2019a), we first remove senses, wiki links, and polarity attributes in the training dataset. Secondly, we anonymize sub-graphs of named entities which is labeled by one of AMR’s finegrained entity types that contain a name role, and other entities which end with -entity6 . 4 Models To handle different flavors of representation, our system has three types of models: Anchoring-based Pruning Parsing Model, Constituent Parsing Model, Seq2seq-based Parsing Model. 4.1 Anchoring-based Pruning Parsing Model The anchoring-based pruning parsing model is suitable"
K19-2004,K18-2006,1,0.849176,"lation”. 45 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge tr"
K19-2004,K18-2001,0,0.0750259,"Missing"
K19-2004,hajic-etal-2012-announcing,0,0.427424,"Missing"
K19-2004,P14-5010,0,0.00246285,"ne correspondence with its usage pattern string (like “ACT PAT”) in the case of word determination, and the usage pattern has duplicates among different words, the number is much smaller than all item ids size; thus it is more suitable as a learning goal. In the subsequent recovery process, we can use lemma and the usage pattern to restore to the item id. Tokenization, Lemmatization, and Anchor conversion Since the sentence in the training dataset is the original text and no tokenization is performed, and the subsequent processing requires the word root form, we use the Stanford NLP toolkit4 (Manning et al., 2014) to tokenize and lemmatize the original text. As the graph node anchor in the original data is defined at the character level, we need to convert the anchor to the word level. In this process, due to the difference in tokenization criteria and the existence of tokenizing errors, some graph nodes will be converted into the same one in the process of conversion to word-level anchor. Therefore, we performed some post-processing modifications on the tokenization results of the Stanford NLP toolkit to ensure that the graph nodes after the conversion to the word level anchor correspond to the previo"
K19-2004,P19-1298,1,0.811368,"ing representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expression of the candidate graph nodes, (2) Non-strictly anchored (UCCA): treats it as a special constituent tree parsing task and uses an additional component to recover the remote edges, and (3) Completely unanchored (i.e., AMR): uses the Seq2seq model to generate the nodes and the"
K19-2004,J93-2004,0,0.0644555,"propbank/accept-v.xml :: accept.01 :: 7] ACT()[accept.01 :: 0 :: :: 3]{} PAT()[accept.01 :: 1 :: :: 6]{} ORIG()[accept.01 :: 2 :: :: 3, accept.01 :: 1 :: :: 2]{} ev-w21f1 ACT PAT [propbank/access-v.xml :: access.01 :: 2] ACT()[access.01 :: 0 :: :: 1]{} PAT()[access.01 :: 1 :: :: 2]{} acclaim ev-w22f1 ACT PAT [propbank/acclaim-v.xml :: acclaim.01 :: 1] ACT(sub)[]{} PAT(obj1, ving)[acclaim.01 :: 1 :: :: 1]{} Data and Preprocessing ev-w22f2 3.1 Data The CoNLL shared task provides a training dataset of 5 subtasks, of which DM, PSD, and EDS are from Wall Street Journal (WSJ) text of Penn Treebank (Marcus et al., 1993) and contain 35,656 sentences. The UCCA training data comes from the English Web Treebank’s reviews text (Bies et al., 2012) and the English Wikipedia celebrity articles, with a total data volume of 5,672 sentences. AMR annotation data are drawn from a variety of texts, including online discussion forums, newswires, folktales, novels, and Wikipedia articles, which contain a total of 56,240 sentences. 3.2 ACT PAT ?CAUS ACT(sub)[]{} PAT(obj1)[]{} CAUS(for[objpp, ving])[]{} Figure 1: Examples of the most frequent frame-toframeset mapping extracted from “rng pb links.txt”. 3.3 Frame Label Projecti"
K19-2004,S14-2056,0,0.515659,"r to prevent the nodes from falling into local optimum and the edges unable to get enough training, we use the random sampling method on the golden graph nodes to push as many correct nodes as possible to join the edge training. According to the official results of the evaluation, our system ranked second place in the overall F1 metric among the 16 participating systems. On the DM framework, our system achieved the best results. Our system on other 4 frameworks (PSD, EDS, UCCA, and AMR) are all ranked the third place. 2 2.1 DM and PSD The DM (Ivanova et al., 2012) and PSD (Hajic et al., 2012; Miyao et al., 2014) are two independently developed syntactic-semantic annotations which project semantic forms onto bilexical dependencies in a lossy manner. In the representation of the DM and PSD frameworks, the graph nodes and surface lexical units are strictly anchored. There is an explicit, one-to-many anchoring onto sub-strings of the underlying sentence. These graphs are neither fully connected nor rooted. The graphs of DM and PSD have the following features: • There is only a one-to-one correspondence1 between the graph node and the span in the sentence. • Graph nodes can have multiple in-edges or out-e"
K19-2004,P19-1009,0,0.09286,"Missing"
K19-2004,P05-1013,0,0.103018,"transformation is carried out: the pseudo node has a one-to-one relationship with the span in the sentence. The edge between nodes in the graph is transformed into the edge of the pseudo node, and two attributes are added for the edge: the source node label and the target node label which are used to indicate the node label in the original EDS graph. In this way, the many-to-one relationship is converted into a one-to-one relationship. After conversion, we can model the problem using in the same way as DM and PSD as described in Subsection 2.1. 2.3 Based on the above features and inspired by Nivre and Nilsson (2005), we transform the tree composed of primary edges (and nodes) into a constituent syntax tree structure, which is modeled using the constituent syntax tree parsing schema. Use an additional classifier for the remote edges prediction and recovery. 2.4 AMR Abstract Meaning Representation (AMR) (Banarescu et al., 2013) parsing is the task of transducing natural language text into AMR, which is a graph-based formalism used for capturing sentence-level semantics. The AMR framework backgrounds notions of compositionality and derivation, therefore, without explicit correspondence between graph nodes a"
K19-2004,K19-2001,0,0.0683072,"Missing"
K19-2004,P19-1230,1,0.841784,"s of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expres"
K19-2004,E17-1035,0,0.0249758,"of n(t); 5: for each span s ∈ Sd do 6: for each word w ∈ s do 7: find a maximum range parent node np of word w whose range size is less than s; 8: move node np to be the child of n(t), and concatenate the original edge label with “ancestor-d” where d represents the original number of edges between the ancestor of np and n(t); 9: remove all the children words of np from s; 10: end for 11: end for 12: until T (t) is a constituent tree 13: set Tc = T (t) Anonymization in AMR Framework Anonymization is an important AMR preprocessing method to reduce the data sparsity issue (Werling et al., 2015; Peng et al., 2017; Guo and Lu, 2018). Following the practice of Zhang et al. (2019a), we first remove senses, wiki links, and polarity attributes in the training dataset. Secondly, we anonymize sub-graphs of named entities which is labeled by one of AMR’s finegrained entity types that contain a name role, and other entities which end with -entity6 . 4 Models To handle different flavors of representation, our system has three types of models: Anchoring-based Pruning Parsing Model, Constituent Parsing Model, Seq2seq-based Parsing Model. 4.1 Anchoring-based Pruning Parsing Model The anchoring-based pruning parsin"
K19-2004,P17-1076,0,0.0518095,"ent with the edge label, using cross-entropy loss as well. We accumulate the loss of all goals together. Neural Architecture Our model builds the candidate graph nodes representation based on the BERT (Devlin et al., 2019) encoder outputs, i.e., for each token wi , the contextualized vector from BERT encoder is denoted as xi . The candidate span (i, j) representation h consists of two endpoint contextualized vectors (xi , xj ) where i and j are the start and end position of the span in the sentence: h = [xi ; xj ]. 4.2 For the UCCA framework, we directly adopt the minimal span-based parser of Stern et al. (2017) on the converted constituent trees. A constituency tree can be regarded as a collection of labeled spans over a sentence. There are two components in the constituent parsing model: one is to assign the scores directly to span existence which determines the tree structure, and the other one assigns scores to span labels which provides the labeled outputs. (1) The node unary scorer φnode (·) is implemented with feed-forward networks based on the candidate graph nodes representation h: φnode (·) = sigmoid(MLPnode (h)). Constituent Parsing Model (2) Neural Architecture In this model, we also buil"
K19-2004,P19-1119,1,0.795869,"Missing"
K19-2004,P15-1095,0,0.0223824,"spans Sd in the range of n(t); 5: for each span s ∈ Sd do 6: for each word w ∈ s do 7: find a maximum range parent node np of word w whose range size is less than s; 8: move node np to be the child of n(t), and concatenate the original edge label with “ancestor-d” where d represents the original number of edges between the ancestor of np and n(t); 9: remove all the children words of np from s; 10: end for 11: end for 12: until T (t) is a constituent tree 13: set Tc = T (t) Anonymization in AMR Framework Anonymization is an important AMR preprocessing method to reduce the data sparsity issue (Werling et al., 2015; Peng et al., 2017; Guo and Lu, 2018). Following the practice of Zhang et al. (2019a), we first remove senses, wiki links, and polarity attributes in the training dataset. Secondly, we anonymize sub-graphs of named entities which is labeled by one of AMR’s finegrained entity types that contain a name role, and other entities which end with -entity6 . 4 Models To handle different flavors of representation, our system has three types of models: Anchoring-based Pruning Parsing Model, Constituent Parsing Model, Seq2seq-based Parsing Model. 4.1 Anchoring-based Pruning Parsing Model The anchoring-b"
K19-2014,hajic-etal-2012-announcing,0,0.40014,"Missing"
K19-2014,W12-3602,0,0.222709,"a Li2∗, Min Zhang2 1 Alibaba Group, China 2 School of Computer Science and Technology, Soochow University, China {shiyu.zy, junjie.junjiecao, masi.wr}@alibaba-inc.com wjiang0501@stu.suda.edu.cn, kirosummer.nlp@gmail.com {zhli13, minzhang}@suda.edu.cn Abstract Semantic Dependency Parsing (SDP) aims to parse the predicate-argument relationships for all words in the input sentence, leading to bilexical semantic dependency graphs (Oepen et al., 2014, 2015, 2016). This shared task focuses on two different formal types of SDP representations, i.e., DELPH-IN MRS Bi-Lexical Dependencies (abbr. as DM, Ivanova et al., 2012) and Prague Semantic Dependencies (abbr. as PSD, Hajiˇc et al., 2012; Miyao et al., 2014). They are both classified as Flavor 0 representations in the sense that every node in the graph must anchor to one and only one token unit, and vice verse. Compared with syntactic dependency trees, some nodes in an SDP graph may have no incoming edges and some may have multiple ones. Borrowing the idea of Dozat and Manning (2018), we encode the input word sequence with BiLSTMs and predict the edges and labels between words with two MLPs. We also predict the POS tag and frame of each word jointly under the"
K19-2014,S19-2002,1,0.758372,"istic framework (Flavor 1) firstly proposed by Abend and Rappoport (2013). In UCCA graphs, input words are leaf (or terminal) nodes. One non-terminal node governs one or more nodes, which may be discontinuous; and one node can have multiple governing (parent) nodes through multiple edges, consisting of a single primary edge and other remote edges. Relationships between nodes are given by edge labels. The primary edges form a tree structure, whereas the remote edges introduce reentrancy, forming directed acyclic graphs (DAGs).2 We directly adopt the previous graph-based UCCA parser proposed by Jiang et al. (2019), treating UCCA graph parsing as constituent parsing and remote edge recovery under the MTL framework. In this paper, we describe our participating systems in the shared task on CrossFramework Meaning Representation Parsing (MRP) at the 2019 Conference for Computational Language Learning (CoNLL). The task includes five frameworks for graph-based meaning representations, i.e., DM, PSD, EDS, UCCA, and AMR. One common characteristic of our systems is that we employ graph-based methods instead of transition-based methods when predicting edges between nodes. For SDP, we jointly perform edge predict"
K19-2014,N16-1030,0,0.0442481,"ds SDP We construct our SDP parser based on the ideas of Dozat and Manning (2017) and Dozat and Manning (2018). Note that lemmas, POS tags and frames are also included in the MRP evaluation metrics, so our method is a bit different from Dozat and Manning (2018). Edge Prediction. Our basic edge prediction model is similar to the Dozat and Manning (2017) and Dozat and Manning (2018). The input words are first mapped into a dense vector composed by pretrained word embeddings and character-level features. xi = eword ⊕ echar i i where echar is extracted by the bidirectional i character-level LSTM (Lample et al., 2016). They are then fed into a multilayer bidirectional wordlevel LSTM to get contextualized representations. Finally, two modules are applied to predict edges. One is to predict whether or not a directed edge exists between two words (keeping the edges between pairs of words with positive scores); and the other is to predict the most probable label for each potential edge (choosing the label with maximum score). Each of them has two seperate MLPs for head and dependent representations and a biaffine layer for scoring. The training loss is the sum of sigmoid cross-entropy loss for edges and softma"
K19-2014,P13-1023,0,0.187952,"entations in the sense that every node in the graph must anchor to one and only one token unit, and vice verse. Compared with syntactic dependency trees, some nodes in an SDP graph may have no incoming edges and some may have multiple ones. Borrowing the idea of Dozat and Manning (2018), we encode the input word sequence with BiLSTMs and predict the edges and labels between words with two MLPs. We also predict the POS tag and frame of each word jointly under the MTL framework. Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework (Flavor 1) firstly proposed by Abend and Rappoport (2013). In UCCA graphs, input words are leaf (or terminal) nodes. One non-terminal node governs one or more nodes, which may be discontinuous; and one node can have multiple governing (parent) nodes through multiple edges, consisting of a single primary edge and other remote edges. Relationships between nodes are given by edge labels. The primary edges form a tree structure, whereas the remote edges introduce reentrancy, forming directed acyclic graphs (DAGs).2 We directly adopt the previous graph-based UCCA parser proposed by Jiang et al. (2019), treating UCCA graph parsing as constituent parsing a"
K19-2014,P18-1037,0,0.437775,"ency SRL (semantic role labeling) to produce nodes. For the edge prediction, the widely-used Biaffine model is used. Abstract meaning representation (AMR), proposed by Banarescu et al. (2013), is a broadcoverage sentence-level semantic formalism (Flavor 2) to encode the meaning of natural language sentences. AMR can be regarded as a rooted labeled directed acyclic graph. Nodes in AMR graphs represent concepts, and labeled directed edges are relations between the concepts. Due to the time limitation and the complexity of the AMR parsing problem, we directly employ the stateof-the-art parser of Lyu and Titov (2018), which treats AMR parsing as a graph prediction problem. Methodology Summarization. Our participating systems can be characterized in the following aspects: under the MTL framework and it is adopted by the DM, PSD, and UCCA models. For both EDS and AMR, we first produce nodes and then predict edges in a pipeline architecture. We have not attempted to jointly solve multiple semantic frameworks via MTL yet. • BERT. We observe that using BERT as our extra inputs is effective for all the models, except AMR. It is also interesting that BERTlarge does not produce more improvements over BERT-base ba"
K19-2014,W13-2322,0,0.464322,"rings. Such alignment information is not provided in the shared task and seems difficult for us to induce due to time limitation. Therefore, we divide the EDS task into two-stage task: node prediction and edge prediction, and treat both as sequence labeling. To tackle with the explicit, many-to-many relationship between nodes and sub-strings of the underlying sentence (via anchoring), we introduce a similar method used in dependency SRL (semantic role labeling) to produce nodes. For the edge prediction, the widely-used Biaffine model is used. Abstract meaning representation (AMR), proposed by Banarescu et al. (2013), is a broadcoverage sentence-level semantic formalism (Flavor 2) to encode the meaning of natural language sentences. AMR can be regarded as a rooted labeled directed acyclic graph. Nodes in AMR graphs represent concepts, and labeled directed edges are relations between the concepts. Due to the time limitation and the complexity of the AMR parsing problem, we directly employ the stateof-the-art parser of Lyu and Titov (2018), which treats AMR parsing as a graph prediction problem. Methodology Summarization. Our participating systems can be characterized in the following aspects: under the MTL"
K19-2014,S14-2056,0,0.253914,"how University, China {shiyu.zy, junjie.junjiecao, masi.wr}@alibaba-inc.com wjiang0501@stu.suda.edu.cn, kirosummer.nlp@gmail.com {zhli13, minzhang}@suda.edu.cn Abstract Semantic Dependency Parsing (SDP) aims to parse the predicate-argument relationships for all words in the input sentence, leading to bilexical semantic dependency graphs (Oepen et al., 2014, 2015, 2016). This shared task focuses on two different formal types of SDP representations, i.e., DELPH-IN MRS Bi-Lexical Dependencies (abbr. as DM, Ivanova et al., 2012) and Prague Semantic Dependencies (abbr. as PSD, Hajiˇc et al., 2012; Miyao et al., 2014). They are both classified as Flavor 0 representations in the sense that every node in the graph must anchor to one and only one token unit, and vice verse. Compared with syntactic dependency trees, some nodes in an SDP graph may have no incoming edges and some may have multiple ones. Borrowing the idea of Dozat and Manning (2018), we encode the input word sequence with BiLSTMs and predict the edges and labels between words with two MLPs. We also predict the POS tag and frame of each word jointly under the MTL framework. Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguis"
K19-2014,P17-1112,0,0.0478012,"amework and followed by our corresponding approaches. ∗ 1 2 The full UCCA scheme also has implicit nodes and linkage relations, which are excluded in the shared task. Corresponding author http://mrp.nlpl.eu/index.php?page=1 149 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 149–157 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2014 Elementary Dependency Structure (EDS) is a graph-structured semantic representation formalism (Flavor 1) proposed by Oepen and Lønning (2006). Buys and Blunsom (2017) introduce a neural encoder-decoder transition-based model to obtain the EDS graph. They use external knowledge to generate nodes3 . Chen et al. (2018) introduce a novel SHRG (Synchronous Hyperedge Replacement Grammar) extraction algorithm which requires a syntactic tree and alignments between conceptual edges and surface strings. Such alignment information is not provided in the shared task and seems difficult for us to induce due to time limitation. Therefore, we divide the EDS task into two-stage task: node prediction and edge prediction, and treat both as sequence labeling. To tackle with"
K19-2014,K19-2001,0,0.157991,"Missing"
K19-2014,P18-1038,0,0.0536348,"ared task. Corresponding author http://mrp.nlpl.eu/index.php?page=1 149 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 149–157 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2014 Elementary Dependency Structure (EDS) is a graph-structured semantic representation formalism (Flavor 1) proposed by Oepen and Lønning (2006). Buys and Blunsom (2017) introduce a neural encoder-decoder transition-based model to obtain the EDS graph. They use external knowledge to generate nodes3 . Chen et al. (2018) introduce a novel SHRG (Synchronous Hyperedge Replacement Grammar) extraction algorithm which requires a syntactic tree and alignments between conceptual edges and surface strings. Such alignment information is not provided in the shared task and seems difficult for us to induce due to time limitation. Therefore, we divide the EDS task into two-stage task: node prediction and edge prediction, and treat both as sequence labeling. To tackle with the explicit, many-to-many relationship between nodes and sub-strings of the underlying sentence (via anchoring), we introduce a similar method used in"
K19-2014,L16-1630,0,0.0887716,"Missing"
K19-2014,S15-2153,0,0.651634,"Missing"
K19-2014,P18-2077,0,0.232099,"ency graphs (Oepen et al., 2014, 2015, 2016). This shared task focuses on two different formal types of SDP representations, i.e., DELPH-IN MRS Bi-Lexical Dependencies (abbr. as DM, Ivanova et al., 2012) and Prague Semantic Dependencies (abbr. as PSD, Hajiˇc et al., 2012; Miyao et al., 2014). They are both classified as Flavor 0 representations in the sense that every node in the graph must anchor to one and only one token unit, and vice verse. Compared with syntactic dependency trees, some nodes in an SDP graph may have no incoming edges and some may have multiple ones. Borrowing the idea of Dozat and Manning (2018), we encode the input word sequence with BiLSTMs and predict the edges and labels between words with two MLPs. We also predict the POS tag and frame of each word jointly under the MTL framework. Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework (Flavor 1) firstly proposed by Abend and Rappoport (2013). In UCCA graphs, input words are leaf (or terminal) nodes. One non-terminal node governs one or more nodes, which may be discontinuous; and one node can have multiple governing (parent) nodes through multiple edges, consisting of a single primary edge and othe"
K19-2014,S14-2008,0,0.350225,"Missing"
K19-2014,oepen-lonning-2006-discriminant,0,0.184117,"ef introduction of each framework and followed by our corresponding approaches. ∗ 1 2 The full UCCA scheme also has implicit nodes and linkage relations, which are excluded in the shared task. Corresponding author http://mrp.nlpl.eu/index.php?page=1 149 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 149–157 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2014 Elementary Dependency Structure (EDS) is a graph-structured semantic representation formalism (Flavor 1) proposed by Oepen and Lønning (2006). Buys and Blunsom (2017) introduce a neural encoder-decoder transition-based model to obtain the EDS graph. They use external knowledge to generate nodes3 . Chen et al. (2018) introduce a novel SHRG (Synchronous Hyperedge Replacement Grammar) extraction algorithm which requires a syntactic tree and alignments between conceptual edges and surface strings. Such alignment information is not provided in the shared task and seems difficult for us to induce due to time limitation. Therefore, we divide the EDS task into two-stage task: node prediction and edge prediction, and treat both as sequence"
K19-2014,D14-1162,0,0.096733,"rs. Concept Identification Model. The concept identification model chooses a concept c conditioned on the aligned word k based on the BiLSTM state hk , which is defined as Pθ (c|hk , wk ). For more details about the re-categorization and candidate concept, please refer to Lyu and Titov (2018). Relation Identification Model. The relation identification model is arc-factored as: Pφ (R|a, c, w) = m Y 3 Experiments This section describes model parameters used in our models, and the overall results of all the five tasks. 3.1 Model Parameters In both SDP and UCCA tasks, we use 100dimensional GloVe (Pennington et al., 2014) as pretrained embedding and random initialized 50dimensional char embedding. The char lstm output is 100-dimensional. We also utilize the BERT embeddings extracted from the last four transformer layers. The final BERT representation is their normalized weighted sum, which is concatenated with the word embeddings. The other parameters are the same with the previous works (Dozat and Manning, 2018; Jiang et al., 2019). In EDS task, external resources we use are: 1) word embeddings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese; and 2) BERT 10 (Devlin et al.,"
K19-2014,P17-1076,0,0.0287443,"MLP MLP Shared BiLSTMs ... xi ... Figure 2: An example of newest version of UCCA. Figure 1: The framework of our SDP Parser. To handle discontinuous node, we trace bottomup from a discontinuous leaf node until we find the specific node whose parent is the lowest common ancestor (LCA) of the discontinuous node and leaf node. Finally we move the edge to make the specific node become the child of the discontinuous, with “-ancestor” added behind the edge label. Please refer to Jiang et al. (2019) for more conversion details. Constituent Parsing. We directly adopt the minimal span-based parser of Stern et al. (2017). Given an input sentence X = {x0 , x1 , · · · , xn }, each word xi is mapped into a dense vector xi and fed into bidirectional LSTM layers. The top-layer output of each position are used to represent the span as cross-entropy loss for labels. 0 ` = `label + `edge Lexical Taggers. This SDP task is more difficult than the ealier 2014 and 2015 SDP tasks (Oepen et al., 2014, 2015), since the gold tokenization result, lemmas, and POS tags are not available in the parser input data and the predictions are parts of the MRP evaluation metrics. We use automatic tokenization result and lemmas provided"
P09-1043,P08-1108,0,0.462384,"f the previous work have been focusing on constituent-based parsing, while the domain adaptation of the dependency parsing has not been fully explored. the graph-based approach where the best parse tree is acquired by searching for a spanning tree which maximizes the score on either a partially or a fully connected graph with all words in the sentence as nodes (Eisner, 1996; McDonald et al., 2005b). As reported in various evaluation competitions, the two systems achieved comparable performances. More recently, approaches of combining these two parsers achieved even better dependency accuracy (Nivre and McDonald, 2008). Granted for the differences between their approaches, both systems heavily rely on machine learning methods to estimate the parsing model from an annotated corpus as training set. Due to the heavy cost of developing high quality large scale syntactically annotated corpora, even for a resource-rich language like English, only very few of them meets the criteria for training a general purpose statistical parsing model. For instance, the text style of WSJ is newswire, and most of the sentences are statements. Being lack of non-statements in the training data could cause problems, when the testi"
P09-1043,W06-2920,0,0.0208099,"out-domain tests.† 1 Introduction Syntactic dependency parsing is attracting more and more research focus in recent years, partially due to its theory-neutral representation, but also thanks to its wide deployment in various NLP tasks (machine translation, textual entailment recognition, question answering, information extraction, etc.). In combination with machine learning methods, several statistical dependency parsing models have reached comparable high parsing accuracy (McDonald et al., 2005b; Nivre et al., 2007b). In the meantime, successful continuation of CoNLL Shared Tasks since 2006 (Buchholz and Marsi, 2006; Nivre et al., 2007a; Surdeanu et al., 2008) have witnessed how easy it has become to train a statistical syntactic dependency parser provided that there is annotated treebank. While the dissemination continues towards various languages, several issues arise with such purely data-driven approaches. One common observation is that statistical parser performance drops significantly when tested on a dataset different from the training set. For instance, when using 2 Parser Domain Adaptation In recent years, two statistical dependency parsing systems, MaltParser (Nivre et al., 2007b) and MSTParser"
P09-1043,P07-1032,0,0.048683,"ecursively embeds smaller feature structures for lower level phrases or words. For the purpose of dependency backbone extraction, we only look at the derivation tree which corresponds to the constituent tree of an HPSG analysis, with all non-terminal nodes labeled by the names of the grammar rules applied. Figure 2 shows an example. Note that all grammar rules in ERG are either unary or binary, giving us relatively deep trees when compared with annotations such as Penn Treebank. Conceptually, this conversion is similar to the conversions from deeper structures to GR reprsentations reported by Clark and Curran (2007) and Miyao et al. (2007). 380 3.3 the CoNLL shared task dependency structures, minor systematic differences still exist for some phenomena. For example, the possessive “’s” is annotated to be governed by its preceding word in CoNLL dependency; while in HPSG, it is treated as the head of a “specifier-head” construction, hence governing the preceding word in the dependency backbone. With several simple tree rewriting rules, we are able to fix the most frequent inconsistencies. With the rule-based backbone extraction and repair, we can finally turn our HPSG parser outputs into dependency structur"
P09-1043,C96-1058,0,0.476231,"ACL and AFNLP ported on approaches of incorporating linguistic features to make the parser less dependent on the nature of training and testing dataset, without resorting to huge amount of unlabeled out-domain data. In addition, most of the previous work have been focusing on constituent-based parsing, while the domain adaptation of the dependency parsing has not been fully explored. the graph-based approach where the best parse tree is acquired by searching for a spanning tree which maximizes the score on either a partially or a fully connected graph with all words in the sentence as nodes (Eisner, 1996; McDonald et al., 2005b). As reported in various evaluation competitions, the two systems achieved comparable performances. More recently, approaches of combining these two parsers achieved even better dependency accuracy (Nivre and McDonald, 2008). Granted for the differences between their approaches, both systems heavily rely on machine learning methods to estimate the parsing model from an annotated corpus as training set. Due to the heavy cost of developing high quality large scale syntactically annotated corpora, even for a resource-rich language like English, only very few of them meets"
P09-1043,2004.tmi-1.2,0,0.0608373,"Missing"
P09-1043,W01-0521,0,0.3282,", 1994) has been successfully applied in several parsing systems for more than a dozen of languages. Some of these grammars, such as the English Resource Grammar (ERG; Flickinger (2002)), have undergone over decades of continuous development, and provide precise linguistic analyses for a broad range of phenomena. These linguistic knowledge are encoded in highly generalized form according to linguists’ reflection for the target languages, and tend to be largely independent from any specific domain. There has been a substantial amount of work on parser adaptation, especially from WSJ to B ROWN. Gildea (2001) compared results from different combinations of the training and testing data to demonstrate that the size of the feature model can be reduced via excluding “domain-dependent” features, while the performance could still be preserved. Furthermore, he also pointed out that if the additional training data is heterogeneous from the original one, the parser will not obtain a substantially better performance. Bacchiani et al. (2006) generalized the previous approaches using a maximum a posteriori (MAP) framework and proposed both supervised and unsupervised adaptation of statistical parsers. McClos"
P09-1043,P07-1079,0,0.033636,"ebanks to help overcome the insufficient data problem for deep parse selection models. locative expression cannot be easily predicted at the pure syntactic level. This also suggests a joint learning of syntactic and semantic dependencies, as proposed in the CoNLL 2008 Shared Task. Instances of wrong HPSG analyses have also been observed as one source of errors. For most of the cases, a correct reading exists, but not picked by our parse selection model. This happens more often with the WSJ test set, partially contributing to the low performance. 5 Conclusion & Future Work Similar to our work, Sagae et al. (2007) also considered the combination of dependency parsing with an HPSG parser, although their work was to use statistical dependency parser outputs as soft constraints to improve the HPSG parsing. Nevertheless, a similar backbone extraction algorithm was used to map between different representations. Similar work also exists in the constituentbased approaches, where CFG backbones were used to improve the efficiency and robustness of HPSG parsers (Matsuzaki et al., 2007; Zhang and Kordoni, 2008). In this paper, we restricted our investigation on the syntactic evaluation using labeled/unlabeled att"
P09-1043,J93-2004,0,0.0360867,"Missing"
P09-1043,W08-2121,0,0.0455886,"Missing"
P09-1043,P06-1043,0,0.0617612,"(2001) compared results from different combinations of the training and testing data to demonstrate that the size of the feature model can be reduced via excluding “domain-dependent” features, while the performance could still be preserved. Furthermore, he also pointed out that if the additional training data is heterogeneous from the original one, the parser will not obtain a substantially better performance. Bacchiani et al. (2006) generalized the previous approaches using a maximum a posteriori (MAP) framework and proposed both supervised and unsupervised adaptation of statistical parsers. McClosky et al. (2006) and McClosky et al. (2008) have shown that out-domain parser performance can be improved with selftraining on a large amount of unlabeled data. Most of these approaches focused on the machine learning perspective instead of the linguistic knowledge embraced in the parsers. Little study has been reThe main issue of parsing with precision grammars is that broad coverage and high precision on linguistic phenomena do not directly guarantee robustness of the parser with noisy real world texts. Also, the detailed linguistic analysis is not always of the highest interest to all NLP applications. It"
P09-1043,zhang-kordoni-2008-robust,1,0.888165,"Missing"
P09-1043,C08-1071,0,0.0164343,"m different combinations of the training and testing data to demonstrate that the size of the feature model can be reduced via excluding “domain-dependent” features, while the performance could still be preserved. Furthermore, he also pointed out that if the additional training data is heterogeneous from the original one, the parser will not obtain a substantially better performance. Bacchiani et al. (2006) generalized the previous approaches using a maximum a posteriori (MAP) framework and proposed both supervised and unsupervised adaptation of statistical parsers. McClosky et al. (2006) and McClosky et al. (2008) have shown that out-domain parser performance can be improved with selftraining on a large amount of unlabeled data. Most of these approaches focused on the machine learning perspective instead of the linguistic knowledge embraced in the parsers. Little study has been reThe main issue of parsing with precision grammars is that broad coverage and high precision on linguistic phenomena do not directly guarantee robustness of the parser with noisy real world texts. Also, the detailed linguistic analysis is not always of the highest interest to all NLP applications. It is not always straightforwa"
P09-1043,W08-2126,1,0.872839,"Missing"
P09-1043,P05-1012,0,0.0568089,"o the features models of statistical parsers. As mordern grammar-based parsers has achieved high runtime efficency (with our HPSG parser parsing at an average speed of ∼3 sentences per second), this adds up to an acceptable overhead. 3.3.1 Feature Model with MSTParser As mentioned before, MSTParser is a graphbased statistical dependency parser, whose learning procedure can be viewed as the assignment of different weights to all kinds of dependency arcs. Therefore, the feature model focuses on each kind of head-child pair in the dependency tree, and mainly contains four categories of features (Mcdonald et al., 2005a): basic uni-gram features, basic bi-gram features, in-between POS features, and surrounding POS features. It is emphasized by the authors that the last two categories contribute a large improvement to the performance and bring the parser to the state-of-the-art accuracy. Therefore, we extend this feature set by adding four more feature categories, which are similar to the original ones, but the dependency relation was replaced by the dependency backbone of the HPSG outputs. The extended feature set is shown in Table 1. Robust Parsing with HPSG As mentioned in Section 2, one pitfall of using"
P09-1043,H05-1066,0,0.769228,"s which, when combined with state-of-the-art statistical dependency parsing models, achieves performance improvements on out-domain tests.† 1 Introduction Syntactic dependency parsing is attracting more and more research focus in recent years, partially due to its theory-neutral representation, but also thanks to its wide deployment in various NLP tasks (machine translation, textual entailment recognition, question answering, information extraction, etc.). In combination with machine learning methods, several statistical dependency parsing models have reached comparable high parsing accuracy (McDonald et al., 2005b; Nivre et al., 2007b). In the meantime, successful continuation of CoNLL Shared Tasks since 2006 (Buchholz and Marsi, 2006; Nivre et al., 2007a; Surdeanu et al., 2008) have witnessed how easy it has become to train a statistical syntactic dependency parser provided that there is annotated treebank. While the dissemination continues towards various languages, several issues arise with such purely data-driven approaches. One common observation is that statistical parser performance drops significantly when tested on a dataset different from the training set. For instance, when using 2 Parser D"
P09-1043,P99-1052,0,\N,Missing
P09-1043,D07-1096,0,\N,Missing
P11-4002,adolphs-etal-2008-fine,1,0.869457,"Missing"
P11-4002,bird-etal-2008-acl,0,0.166713,"rmation and avoiding duplication of work have become urgent issues to be addressed by the scientific community. The organization and preservation of scientific knowledge in scientific publications, vulgo text documents, thwarts these efforts. From a viewpoint of a computer scientist, scientific papers are just ‘unstructured information’. At least in our own scientific community, Computational Linguistics, it is generally assumed that NLP could help to support search in such document collections. The ACL Anthology1 is a comprehensive electronic collection of scientific papers in our own field (Bird et al., 2008). It is updated regularly with new publications, but also older papers have been scanned and are made available electronically. We have implemented the ACL Anthology Searchbench2 for two reasons: Our first aim is to provide a more targeted search facility in this collection than standard web search on the anthology website. In this sense, the Searchbench is meant to become a service to our own community. Our second motivation is to use the developed system as a showcase for the progress that has been made over the last years in precision-oriented deep linguistic parsing in terms of both effici"
P11-4002,A00-1031,0,0.0549587,"y) and the parse ranking model, there are no further adaptations to genre or domain of the text corpus. This implies that the NLP workflow could be easily and modularly adapted to other (scientific or nonscientific) domains—mainly thanks to the generic and comprehensive language modelling in the ERG. The NLP preprocessing component workflow is implemented using the Heart of Gold NLP middleware architecture (Sch¨afer, 2006). It starts with sentence boundary detection (SBR) and regular expression-based tokenization using its built-in component JTok, followed by the trigram-based PoS tagger TnT (Brants, 2000) trained on the Penn Treebank (Marcus et al., 1993) and the named entity recognizer SProUT (Dro˙zd˙zy´nski et al., 2004). 2.1 Precise Preprocessing Integration with Chart Mapping Tagger output is combined with information from the named entity recognizer, e.g. delivering hypothetical information on citation expressions. The combined result is delivered as input to the deep parser PET (Callmeier, 2000) running the ERG. Here, citations, for example, can be treated as either persons, locations or appositions. Concerning punctuation, the ERG can make use of information on opening and closing quota"
P11-4002,E09-1001,0,0.0403726,"sion Semantics (MRS) representation format (Copestake et al., 2005). It consists of elementary predications for each word and larger constituents, connected via argument positions and variables, from which predicate-argument structure can be extracted. MRS representations resulting from deep parsing are still relatively close to linguistic structures and contain more detailed information than a user would like to query and search for. Therefore, an additional extraction and abstraction step is performed before storing semantic structures in the search index. Firstly, MRS is converted to DMRS (Copestake, 2009), a dependency-style version of MRS that eases extraction of predicate-argument structure using the implementation in LKB (Copestake, 2002). The representation format we devised for the search index we call semantic tuples, in fact quintuples <subject, predicate, first object, second object, adjuncts&gt;; example in Figure 3. The basic extraction algorithm consists of the following three steps: (1) calculate the closure for each elementary predication based on the EQ (variable equivalence) relation, and group the predicates and entities in each closure respectively; (2) extract the relations of t"
P11-4002,C10-1026,0,0.0468336,"Missing"
P11-4002,flickinger-etal-2010-wikiwoods,0,0.18021,"Missing"
P11-4002,J93-2004,0,0.0478019,"further adaptations to genre or domain of the text corpus. This implies that the NLP workflow could be easily and modularly adapted to other (scientific or nonscientific) domains—mainly thanks to the generic and comprehensive language modelling in the ERG. The NLP preprocessing component workflow is implemented using the Heart of Gold NLP middleware architecture (Sch¨afer, 2006). It starts with sentence boundary detection (SBR) and regular expression-based tokenization using its built-in component JTok, followed by the trigram-based PoS tagger TnT (Brants, 2000) trained on the Penn Treebank (Marcus et al., 1993) and the named entity recognizer SProUT (Dro˙zd˙zy´nski et al., 2004). 2.1 Precise Preprocessing Integration with Chart Mapping Tagger output is combined with information from the named entity recognizer, e.g. delivering hypothetical information on citation expressions. The combined result is delivered as input to the deep parser PET (Callmeier, 2000) running the ERG. Here, citations, for example, can be treated as either persons, locations or appositions. Concerning punctuation, the ERG can make use of information on opening and closing quotation marks. Such information is often not explicit"
P11-4002,W09-3607,0,0.0617167,"Results View and in the currently selected paper of the Document View. Besides a header displaying the metadata of the currently selected paper (including the automatically extracted topics on the right), the Document View provides three subviews of the selected paper: (1) the Document Content View is a raw list of the sentences of the paper and provides different kinds of interaction with these sentences; (2) the PDF View shows the original PDF version of the paper; (3) the Citations View provides citation information includ6 Affiliations have been added using the ACL Anthology Network data (Radev et al., 2009). ing link to the ACL Anthology Network (Radev et al., 2009). Figure 4 shows the search result for a query combining a statement (‘obtain improvements’), a topic ‘dependency parsing’ and the publication year 2008. As can be seen in the Results View, six papers match these filters; sentences with semantically similar predicates and passive voice are found, too. 4.1 Semantic Search The main feature which distinguishes the ACL Anthology Searchbench from other search applications for scientific papers is the semantic search in paper content. This enables the search for (semantic) statements in the"
P11-4002,W10-0402,1,0.906769,"Missing"
P11-4002,W06-2714,1,0.889517,"Missing"
P17-2089,D11-1033,0,0.776056,"2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima’an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016). These methods select useful sentences from the whole corpus, so they can be directly applied to NMT. However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains a"
P17-2089,2014.iwslt-evaluation.1,0,0.0320202,"Missing"
P17-2089,P15-1001,0,0.0256648,"er, we exploit the NMT’s internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points. 1 Introduction Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016). An ever increasing amount of data is becoming available for NMT training. However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance. Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015). To the best of our knowledge, there are only 560 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566 c Vancouver,"
P17-2089,2016.amta-researchers.8,0,0.537509,"hod can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima’an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016). These methods select useful sentences from the whole corpus, so they can be directly applied to NMT. However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains are typically able to improve translation performance. Recently Neural Machine Translation (NMT) has become prominent in the field. However, most of the existing domain adaptation methods only foc"
P17-2089,D15-1147,0,0.0659529,"Missing"
P17-2089,P07-2045,0,0.0260817,"Missing"
P17-2089,P13-2119,0,0.356115,"e training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima’an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016). These methods select useful sentences from the whole corpus, so they can be directly applied to NMT. However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains are typically able"
P17-2089,2015.mtsummit-papers.10,0,0.319092,"a few works concerning NMT adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). Most traditional adaptation methods focus on Phrase-Based Statistical Machine Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows. For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015). Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation. Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and"
P17-2089,2015.iwslt-evaluation.11,0,0.719606,"Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows. For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015). Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation. Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelr"
P17-2089,P16-2021,0,0.0875788,"bedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points. 1 Introduction Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016). An ever increasing amount of data is becoming available for NMT training. However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance. Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015). To the best of our knowledge, there are only 560 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566 c Vancouver, Canada, July 30 - August 4, 2017."
P17-2089,D14-1062,0,0.0613509,"Missing"
P17-2089,P10-2041,0,0.40117,"od (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima’an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016). These methods select useful sentences from the whole corpus, so they can be directly applied to NMT. However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural Although new corpora are becoming increasingly available for machine translation, only those that belong to the sam"
P17-2089,P02-1040,0,0.117011,"Missing"
P17-2089,E12-1055,0,0.0294002,"ma,eiichiro.sumita}@nict.go.jp Abstract a few works concerning NMT adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). Most traditional adaptation methods focus on Phrase-Based Statistical Machine Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows. For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015). Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation. Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained"
P17-2089,P13-1082,0,0.0794448,"ta}@nict.go.jp Abstract a few works concerning NMT adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). Most traditional adaptation methods focus on Phrase-Based Statistical Machine Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows. For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015). Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation. Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and"
P17-2089,P16-1008,0,0.0219846,"NMT’s internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points. 1 Introduction Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016). An ever increasing amount of data is becoming available for NMT training. However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance. Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015). To the best of our knowledge, there are only 560 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566 c Vancouver, Canada, July 30"
P17-2089,C16-1295,1,0.865382,"ms apply a similar sentence representation. In comparison, we adopt a transition layer between the source and target layers and don’t use test data. 2 It is possible to use a sample of the out-of-domain data. In this paper, we use all of them. 561 vector center CFin as d(vf , CFin ) and out-ofdomain vector center CFout as d(vf , CFout ), respectively. We use the difference δ of these two distances to classify each sentence: δf = d(vf , CFin ) − d(vf , CFout ). blog texts. The statistics on data sets were shown in Table 1. These adaptation corpora settings were nearly the same as that used in (Wang et al., 2016). The differences were: (5) By using an English-to-French NMT system NEF , we can obtain a target sentence embedding ve , in-domain target vector center CEin and out-of-domain target vector center CEout . Corresponding distance difference δe is, δe = d(ve , CEin ) − d(ve , CEout ). • For IWSLT, they chose FR-EN translation task, which is popular in PBSMT. We chose EN-FR, which is more popular in NMT; • For NIST, they chose 02-05 as dev set, and we chose 02-04. Because we would report results on two test sets (MT05 and MT06) in comparison with only one (MT06). (6) δf , δe and δf e = δf + δe can"
P17-2089,P14-2122,1,0.851211,", the maximum sequence length were 50, and the beam size for decoding was 10. Default dropout were applied. We used a mini-batch Stochastic Gradient Descent (SGD) algorithm together with ADADELTA optimizer (Zeiler, 2012). Training was conducted on a single Tesla K80 GPU. Each NMT model was trained for 500K batches, taking 7-10 days. For sentence embedding and selection, it only took several hours to process all of sentences in the training data, because decoding was not necessary. • NIST 2006 Chinese (ZH) to English corpus5 was used as the in-domain training corpus, following the settings of (Wang et al., 2014). Chinese-to-English UN data set (LDC2013T06) and NTCIR-9 (Goto et al., 2011) patent data set were used as out-ofdomain data. NIST MT 2002-2004 and NIST MT 2005/2006 were used as the development and test data, respectively. We are aware of that there are additional NIST corpora in a similar domain, but because this task was for domain adaptation, we only selected a small subset, which is mainly focused on news and 3 https://wit3.fbk.eu/mt.php?release=2014-01 http://statmt.org/wmt15/translation-task.html 5 http://www.itl.nist.gov/iad/mig/tests/mt/2006/ 4 6 https://github.com/lisa-groundhog/ Gro"
P17-2089,D16-1050,0,0.0127489,"urce sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points. 1 Introduction Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016). An ever increasing amount of data is becoming available for NMT training. However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance. Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015). To the best of our knowledge, there are only 560 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for"
P18-2048,2015.iwslt-evaluation.11,0,0.0402112,"r to accelerate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance. 1 Introduction Recently neural machine translation (NMT) has been prominently used to perform various translation tasks (Luong and Manning, 2015; Bojar et al., 2017). However, NMT is much more time-consuming than traditional phrasebased statistical machine translation (PBSMT) due to its deep neural network structure. To improve the efficiency of NMT training, most of the studies focus on reducing the number of parameters in the model (See et al., 2016; Crego et al., 2016; Hubara et al., 2016) and implementing parallelism 298 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 298–304 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics smal"
P18-2048,D15-1166,0,0.0664039,"erate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance. 1 Introduction Recently neural machine translation (NMT) has been prominently used to perform various translation tasks (Luong and Manning, 2015; Bojar et al., 2017). However, NMT is much more time-consuming than traditional phrasebased statistical machine translation (PBSMT) due to its deep neural network structure. To improve the efficiency of NMT training, most of the studies focus on reducing the number of parameters in the model (See et al., 2016; Crego et al., 2016; Hubara et al., 2016) and implementing parallelism 298 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 298–304 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics smal"
P18-2048,P02-1040,0,0.100028,"Missing"
P18-2048,K16-1029,0,0.0408088,"Missing"
P18-2048,E17-3017,0,0.0769889,"Missing"
P18-2048,P17-2089,1,0.867888,"Missing"
P18-2048,kocmi-bojar-2017-curriculum,0,0.279027,"n, Kyoto, Japan {wangrui, mutiyama, eiichiro.sumita}@nict.go.jp Abstract in the data or in the model (Wu et al., 2016; Kalchbrenner et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Although these technologies have been adopted, deep networks have to be improved to achieve state-of-the-art performance in order to handle very large datasets and several training iterations. Therefore, some researchers have proposed to accelerate the NMT training by resampling a smaller subset of the data that makes a relatively high contribution, to improve the training efficiency of NMT. Specifically, Kocmi and Bojar (2017) empirically investigated curriculum learning based on the sentence length and word rank. Wang et al. (2017a) proposed a static sentence-selection method for domain adaptation using the internal sentence embedding of NMT. They also proposed a sentence weighting method with dynamic weight adjustment (Wang et al., 2017b). Wees et al. (2017) used domain-based cross-entropy as a criterion to gradually fine-tune the NMT training in a dynamical manner. All of these criteria (Wang et al., 2017a,b; Wees et al., 2017) are calculated before performing the NMT training based on the domain information and"
P18-2048,W04-3250,0,0.397163,"Missing"
P18-2048,D17-1155,1,0.83141,"2016; Kalchbrenner et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Although these technologies have been adopted, deep networks have to be improved to achieve state-of-the-art performance in order to handle very large datasets and several training iterations. Therefore, some researchers have proposed to accelerate the NMT training by resampling a smaller subset of the data that makes a relatively high contribution, to improve the training efficiency of NMT. Specifically, Kocmi and Bojar (2017) empirically investigated curriculum learning based on the sentence length and word rank. Wang et al. (2017a) proposed a static sentence-selection method for domain adaptation using the internal sentence embedding of NMT. They also proposed a sentence weighting method with dynamic weight adjustment (Wang et al., 2017b). Wees et al. (2017) used domain-based cross-entropy as a criterion to gradually fine-tune the NMT training in a dynamical manner. All of these criteria (Wang et al., 2017a,b; Wees et al., 2017) are calculated before performing the NMT training based on the domain information and are fixed while performing the complete procedure. Zhang et al. (2017) adopted the sentence-level training"
P18-2048,P07-2045,0,0.0140916,"Missing"
P18-2048,I17-2046,0,0.302474,"on the sentence length and word rank. Wang et al. (2017a) proposed a static sentence-selection method for domain adaptation using the internal sentence embedding of NMT. They also proposed a sentence weighting method with dynamic weight adjustment (Wang et al., 2017b). Wees et al. (2017) used domain-based cross-entropy as a criterion to gradually fine-tune the NMT training in a dynamical manner. All of these criteria (Wang et al., 2017a,b; Wees et al., 2017) are calculated before performing the NMT training based on the domain information and are fixed while performing the complete procedure. Zhang et al. (2017) adopted the sentence-level training cost as a dynamic criterion to gradually fine-tune the NMT training. This approach was developed based on the idea that the training cost is a useful measure to determine the translation quality of a sentence. However, some of the sentences that can be potentially improved by training may be deleted using this method. In addition, all of the above works primarily focused on NMT translation performance, instead of training efficiency. In this study, we propose a method of dynamic sentence sampling (DSS) to improve the NMT training efficiency. First, the diff"
P18-2048,W17-4717,0,\N,Missing
P19-1119,D16-1250,0,0.0277561,"ods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT. 1 • 1) There is a positive correlation between the quality of the pre-trained UBWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng Sun was an internship research fellow at NICT when conducting this work. Based on these two findi"
P19-1119,P17-1042,0,0.0444313,"The supervised BWE (Mikolov et al., 2013), which exploits similarities between the source language and the target language through a linear transformation matrix, serves as the basis for many NLP tasks, such as machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Chen et al., 2018b; Zhang and Zhao, 2019), dependency parsing (Zhang et al., 2016; Li et al., 2018), semantic role labeling (He et al., 2018; Li et al., 2019). However, the lack of a large wordpair dictionary poses a major practical problem for many language pairs. UBWE has attracted considerable attention. For example, Artetxe et al. (2017) proposed a self-learning framework to learn BWE with a 25-word dictionary, and Artetxe et al. (2018a) extended previous work without any word dictionary via fully unsupervised initialization. Zhang et al. (2017) and Conneau et al. (2018) proposed UBWE methods via generative adversarial network training. Recently, several UBWE methods (Conneau et al., 2018; Artetxe et al., 2018a) have been applied to UNMT (Artetxe et al., 2018c; Lample et al., 2018a). These rely solely on monolingual corpora in each language via UBWE initialization, denoising auto-encoder, and back-translation. A shared encode"
P19-1119,P18-1073,0,0.117216,"BWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng Sun was an internship research fellow at NICT when conducting this work. Based on these two findings, we hypothesize that the learning of UNMT with UBWE agreement would enhance UNMT performance. In detail, we propose two approaches, UBWE agreement regularization and UBWE adversarial training, to maintain the quality of UBWE during NMT"
P19-1119,D18-1399,0,0.0466684,"Missing"
P19-1119,P19-1019,0,0.0982651,"Missing"
P19-1119,J82-2005,0,0.756838,"Missing"
P19-1119,Q17-1010,0,0.039396,"UNMT Baseline + UBWE agreement regularization + UBWE adversarial training De-En n/a 13.33 14.62 21.0 21.23 22.38++ 22.67++ En-De n/a 9.64 10.86 17.2 17.06 18.04++ 18.29++ Fr-En 15.56 14.31 15.58 24.2 24.50 25.21++ 25.87++ En-Fr 15.13 15.05 16.97 25.1 25.37 27.86++ 28.38++ Ja-En n/a n/a n/a n/a 14.09 16.36++ 17.22++ En-Ja n/a n/a n/a n/a 21.63 23.01++ 23.64++ Table 2: Performance (BLEU score) of UNMT. “++” after a score indicates that the proposed method was significantly better than the UNMT baseline at significance level p <0.01. the embeddings for each language independently with fastText3 (Bojanowski et al., 2017) (default settings). The word embeddings were normalized by length and mean centered before bilingual projection. We then used VecMap4 (Artetxe et al., 2018a) (default settings) to project two monolingual word embeddings into one space. To evaluate the quality of UBWE, we selected the accuracy of word translation using the top-1 predicted candidate in the MUSE test set as the criterion. 5.3 UNMT Settings In the training process for UNMT, we used the transformer-based UNMT toolkit5 and the settings of Lample et al. (2018b). That is, we used four 3 https://github.com/facebookresearch/ fastText 4"
P19-1119,C16-1171,1,0.803699,"orm conventional UNMT. 1 • 1) There is a positive correlation between the quality of the pre-trained UBWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng Sun was an internship research fellow at NICT when conducting this work. Based on these two findings, we hypothesize that the learning of UNMT with UBWE agreement would enhance UNMT performance. In detail, we propose two approache"
P19-1119,D17-1304,1,0.782183,"Missing"
P19-1119,C18-1271,0,0.0472765,"seline UBWE agreement regularization UBWE adversarial training Parameters 120,141K 120,141K 120,764K Speed 3784 3741 3733 Table 4: Analysis on parameters and training speed (number of processed words per second on one P100). 7 Related Work The supervised BWE (Mikolov et al., 2013), which exploits similarities between the source language and the target language through a linear transformation matrix, serves as the basis for many NLP tasks, such as machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Chen et al., 2018b; Zhang and Zhao, 2019), dependency parsing (Zhang et al., 2016; Li et al., 2018), semantic role labeling (He et al., 2018; Li et al., 2019). However, the lack of a large wordpair dictionary poses a major practical problem for many language pairs. UBWE has attracted considerable attention. For example, Artetxe et al. (2017) proposed a self-learning framework to learn BWE with a 25-word dictionary, and Artetxe et al. (2018a) extended previous work without any word dictionary via fully unsupervised initialization. Zhang et al. (2017) and Conneau et al. (2018) proposed UBWE methods via generative adversarial network training. Recently, several UBWE methods (Conneau et al., 20"
P19-1119,E14-1049,0,0.0395704,"erformance of UNMT is significantly affected by the performance of UBWE. Thus, we propose two methods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT. 1 • 1) There is a positive correlation between the quality of the pre-trained UBWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng S"
P19-1119,P18-1192,0,0.062598,"adversarial training Parameters 120,141K 120,141K 120,764K Speed 3784 3741 3733 Table 4: Analysis on parameters and training speed (number of processed words per second on one P100). 7 Related Work The supervised BWE (Mikolov et al., 2013), which exploits similarities between the source language and the target language through a linear transformation matrix, serves as the basis for many NLP tasks, such as machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Chen et al., 2018b; Zhang and Zhao, 2019), dependency parsing (Zhang et al., 2016; Li et al., 2018), semantic role labeling (He et al., 2018; Li et al., 2019). However, the lack of a large wordpair dictionary poses a major practical problem for many language pairs. UBWE has attracted considerable attention. For example, Artetxe et al. (2017) proposed a self-learning framework to learn BWE with a 25-word dictionary, and Artetxe et al. (2018a) extended previous work without any word dictionary via fully unsupervised initialization. Zhang et al. (2017) and Conneau et al. (2018) proposed UBWE methods via generative adversarial network training. Recently, several UBWE methods (Conneau et al., 2018; Artetxe et al., 2018a) have been appl"
P19-1119,N16-1162,0,0.097674,"Missing"
P19-1119,N15-1028,0,0.0983804,"Missing"
P19-1119,W18-6419,1,0.821417,"ur method. In addition, an alternative unsupervised method based on statistical machine translation (SMT) was proposed (Lample et al., 2018b; Artetxe et al., 2018b). The unsupervised machine translation performance was improved through combining UNMT and unsupervised SMT (Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). More recently, Lample and Conneau (2019) achieved 1242 better UNMT performance through introducing the pretrained language model. Neural network based language model has been shown helpful in supervised machine translation (Wang et al., 2014; Wang et al., 2018; Marie et al., 2018). We think that the proposed agreement mechanism can work with the pretrained language model. 8 Conclusion UBWE is a fundamental component of UNMT. In previous methods, the pre-trained UBWE is only used to initialize the word embedding of UNMT. In this study, we found that the performance of UNMT is significantly affected by the quality of UBWE, not only in the initialization stage, but also during UNMT training. Based on this finding, we proposed two joint learning methods to train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods can mitigat"
P19-1119,P16-1009,0,0.124354,"et al., 2016). The denoising auto-encoder, which encodes a noisy version and reconstructs it with the decoder in the same language, is optimized by minimizing the Lauto = EX∼φL1 [−logPL1 →L1 (X|C(X)] + EY ∼φL2 [−logPL2 →L2 (Y |C(Y )], (2) where C(X) and C(Y ) are noisy versions of sentences X and Y , PL1 →L1 (PL2 →L2 ) denotes the reconstruction probability in the language L1 (L2 ). 2.3 Back-translation The denoising auto-encoder acts as a language model that has been trained in one language and does not consider the final goal of translating between two languages. Therefore, backtranslation (Sennrich et al., 2016) was adapted to train translation systems in a true translation setting based on monolingual corpora. Formally, given the sentences X and Y , the sentences YP (X) and XP (Y ) would be produced by the model at the previous iteration. The pseudo-parallel sentence pair (YP (X), X) and (XP (Y ), Y ) would be obtained to train the new translation model. Finally, the back-translation process is optimized by minimizing the following objective function: Lbt = EX∼φL1 [−logPL2 →L1 (X|YP (X)] + EY ∼φL2 [−logPL1 →L2 (Y |XP (Y )], (3) where PL1 →L2 (PL2 →L1 ) denotes the translation probability across two"
P19-1119,P18-1072,0,0.043009,"Missing"
P19-1119,P17-1179,0,0.327433,"NMT. 1 • 1) There is a positive correlation between the quality of the pre-trained UBWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng Sun was an internship research fellow at NICT when conducting this work. Based on these two findings, we hypothesize that the learning of UNMT with UBWE agreement would enhance UNMT performance. In detail, we propose two approaches, UBWE agreement reg"
P19-1119,P16-1131,0,0.0253662,"eed of the model. Baseline UBWE agreement regularization UBWE adversarial training Parameters 120,141K 120,141K 120,764K Speed 3784 3741 3733 Table 4: Analysis on parameters and training speed (number of processed words per second on one P100). 7 Related Work The supervised BWE (Mikolov et al., 2013), which exploits similarities between the source language and the target language through a linear transformation matrix, serves as the basis for many NLP tasks, such as machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Chen et al., 2018b; Zhang and Zhao, 2019), dependency parsing (Zhang et al., 2016; Li et al., 2018), semantic role labeling (He et al., 2018; Li et al., 2019). However, the lack of a large wordpair dictionary poses a major practical problem for many language pairs. UBWE has attracted considerable attention. For example, Artetxe et al. (2017) proposed a self-learning framework to learn BWE with a 25-word dictionary, and Artetxe et al. (2018a) extended previous work without any word dictionary via fully unsupervised initialization. Zhang et al. (2017) and Conneau et al. (2018) proposed UBWE methods via generative adversarial network training. Recently, several UBWE methods ("
P19-1119,C18-1269,0,0.0524389,"Missing"
P19-1119,D14-1023,1,0.81634,"s initialization process for UBWE in our method. In addition, an alternative unsupervised method based on statistical machine translation (SMT) was proposed (Lample et al., 2018b; Artetxe et al., 2018b). The unsupervised machine translation performance was improved through combining UNMT and unsupervised SMT (Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). More recently, Lample and Conneau (2019) achieved 1242 better UNMT performance through introducing the pretrained language model. Neural network based language model has been shown helpful in supervised machine translation (Wang et al., 2014; Wang et al., 2018; Marie et al., 2018). We think that the proposed agreement mechanism can work with the pretrained language model. 8 Conclusion UBWE is a fundamental component of UNMT. In previous methods, the pre-trained UBWE is only used to initialize the word embedding of UNMT. In this study, we found that the performance of UNMT is significantly affected by the quality of UBWE, not only in the initialization stage, but also during UNMT training. Based on this finding, we proposed two joint learning methods to train UNMT with UBWE agreement. Empirical results on several language pairs sh"
P19-1119,N15-1104,0,0.0375279,"gnificantly affected by the performance of UBWE. Thus, we propose two methods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT. 1 • 1) There is a positive correlation between the quality of the pre-trained UBWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng Sun was an internshi"
P19-1119,P18-1005,0,0.321075,"racy Base-Fr enc-En dec AR-Fr-En AR-Fr enc-En dec AT-Fr-En UBWEAT-Fr accuracy enc-En dec AR-Fr-En BLEU 20 40 60 80 100 120 140 Epoch Base-Ja-En UBWE accuracy UBWE accuracy Base-Ja enc-En dec AR-Ja-En AR-Ja enc-En dec Base-Fr-En BLEU AT-Ja-En UBWE AT-Ja accuracy enc-En dec AT-Fr-En BLEU AR-Ja-En BLEU (a) Fr-En Base-Ja-En BLEU AT-Ja-En BLEU (b) Ja-En Figure 4: The trends of UBWE quality and BLEU score for baseline (Base), UBWE agreement regularization (AR), and UBWE adversarial training (AT) during UNMT training on the Fr-En and Ja-En dataset Methods Artetxe et al. (2018c) Lample et al. (2018a) Yang et al. (2018) Lample et al. (2018b) UNMT Baseline + UBWE agreement regularization + UBWE adversarial training De-En n/a 13.33 14.62 21.0 21.23 22.38++ 22.67++ En-De n/a 9.64 10.86 17.2 17.06 18.04++ 18.29++ Fr-En 15.56 14.31 15.58 24.2 24.50 25.21++ 25.87++ En-Fr 15.13 15.05 16.97 25.1 25.37 27.86++ 28.38++ Ja-En n/a n/a n/a n/a 14.09 16.36++ 17.22++ En-Ja n/a n/a n/a n/a 21.63 23.01++ 23.64++ Table 2: Performance (BLEU score) of UNMT. “++” after a score indicates that the proposed method was significantly better than the UNMT baseline at significance level p <0.01. the embeddings for each language indepen"
P19-1174,J93-2003,0,0.124809,"lel bilingual sentence pairs in advance to form a reordering model. This reordering model is then integrated into the translation decoding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words in a sentence to generate a fluent translation. Inspired by a distortion method originating in SMT (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006), there is a quite recent preliminary exploration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional position-based attention inside the fixed-size window, and reported a considerable improvement on the classical RNN-based NMT. This means that the word reordering information is also beneficial to the NMT. The Transformer (Vaswani et al., 2017) translation system relies on self-attention networks (SANs), and has attracted growing interesting in the machine translation community. The Transfor"
P19-1174,P17-1177,0,0.0856638,"utively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al., 2010, 2013) address this issue by directly modeling the reordered sequence of input words, as opposed to the reordering operations that generated it. Operation sequence models are n-gram models that include lexical translation operations and reordering operations in a single generative story, thereby combining elements from the previous three model families (Durrani et al., 2011, 2013, 2014). Their method were further extended by source syntax information (Chen et al., 2017c, 2018b) to improve the performance of SMT. Moreover, to address data sparsity (Guta et al., 2015) caused by a mass of reordering rules, Li et al. (2013, 2014) modeled ITG-based reordering rules in the translation by using neural networks. In particular, the NN-based reordering models can not only capture semantic similarity but also ITG reordering constraints (Wu, 1996, 1997) in the translation context. This neural network modeling method is further applied to capture reordering information and syntactic coherence. 2.2 Modeling Ordering for NMT The attention-based NMT focused on neural netwo"
P19-1174,D17-1304,1,0.836837,"Missing"
P19-1174,P05-1066,0,0.763417,"are sentence representation for machine translation. The proposed translation models outperform the state-of-the-art NMT baselines systems with a similar number of parameters and achieve comparable results compared to NMT systems with much more parameters. 2 2.1 Related Work Reordering Model for PBSMT In PBSMT, there has been a substantial amount of research works about reordering model, which was used as a key component to ensure the generation of fluent target translation. Bisazza and Federico (2016) divided these reordering models into four groups: Phrase orientation models (Tillman, 2004; Collins et al., 2005; Nagata et al., 2006; Zens and Ney, 2006; Galley and Manning, 2008; Cherry, 2013), simply known as lexicalized reordering models, predict whether the next translated source span should be placed on the right (monotone), the left (swap), or anywhere else (discontinuous) of the last translated one. Jump models (Al-Onaizan and Papineni, 2006; Green et al., 2010) predict the direction and length of the jump that is performed between consecutively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al., 2010, 2013) address t"
P19-1174,P13-2071,0,0.149567,"Missing"
P19-1174,C14-1041,0,0.0608164,"Missing"
P19-1174,P11-1105,0,0.0991813,"Missing"
P19-1174,P16-1078,0,0.0203508,"g RNN-based NMT for improving the performance of translations. Du and Way (2017) 1788 and Kawara et al. (2018) reported that the prereordering method had an negative impact on the NMT for the ASPEC JA-EN translation task. In particular, Kawara et al. (2018) assumed that one reason is the isolation between pre-ordering and NMT models, where both models are trained using independent optimization functions. In addition, several research works have been proposed to explicitly introduce syntax structure into the RNN-based NMT for encoding syntax ordering dependencies into sentence representations (Eriguchi et al., 2016; Li et al., 2017; Chen et al., 2017a,b; Wang et al., 2017b; Chen et al., 2018a). Recently, the neural Transformer translation system (Vaswani et al., 2017), which relies solely on self-attention networks, used a fixed order sequence of positional embeddings to encode order dependencies between words in a sentence. 3 3.1 Background Positional Encoding Mechanism Transformer (Vaswani et al., 2017) typically uses a positional encoding mechanism to encode order dependencies between words in a sentence. Formally, given a embedding sequence of source sentence of length J, X={x1 , · · · , xJ }, the p"
P19-1174,2010.amta-papers.22,0,0.0465578,"illman, 2004; Collins et al., 2005; Nagata et al., 2006; Zens and Ney, 2006; Galley and Manning, 2008; Cherry, 2013), simply known as lexicalized reordering models, predict whether the next translated source span should be placed on the right (monotone), the left (swap), or anywhere else (discontinuous) of the last translated one. Jump models (Al-Onaizan and Papineni, 2006; Green et al., 2010) predict the direction and length of the jump that is performed between consecutively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al., 2010, 2013) address this issue by directly modeling the reordered sequence of input words, as opposed to the reordering operations that generated it. Operation sequence models are n-gram models that include lexical translation operations and reordering operations in a single generative story, thereby combining elements from the previous three model families (Durrani et al., 2011, 2013, 2014). Their method were further extended by source syntax information (Chen et al., 2017c, 2018b) to improve the performance of SMT. Moreover, to address data sparsity (Guta et al., 2015) caused by a mass of reorde"
P19-1174,N13-1003,0,0.0494421,"Missing"
P19-1174,P13-1032,0,0.0308283,"Missing"
P19-1174,P05-1033,0,0.216411,"The reordering model plays an important role in phrase-based statistical machine translation (PBSMT), especially for translation between distant language pairs with large differences in word order, such as Chinese-to-English and Japaneseto-English translations (Galley and Manning, 2008; Goto et al., 2013). Typically, the traditional PBSMT learns large-scale reordering rules from parallel bilingual sentence pairs in advance to form a reordering model. This reordering model is then integrated into the translation decoding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words in a sentence to generate a fluent translation. Inspired by a distortion method originating in SMT (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006), there is a quite recent preliminary exploration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional p"
P19-1174,D08-1089,0,0.301049,"into both the encoder and the decoder in the Transformer translation system. Experimental results on WMT’14 English-toGerman, NIST Chinese-to-English, and WAT ASPEC Japanese-to-English translation tasks demonstrate that the proposed methods can significantly improve the performance of the Transformer translation system. 1 Introduction The reordering model plays an important role in phrase-based statistical machine translation (PBSMT), especially for translation between distant language pairs with large differences in word order, such as Chinese-to-English and Japaneseto-English translations (Galley and Manning, 2008; Goto et al., 2013). Typically, the traditional PBSMT learns large-scale reordering rules from parallel bilingual sentence pairs in advance to form a reordering model. This reordering model is then integrated into the translation decoding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words"
P19-1174,P17-1012,0,0.0293336,"ration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional position-based attention inside the fixed-size window, and reported a considerable improvement on the classical RNN-based NMT. This means that the word reordering information is also beneficial to the NMT. The Transformer (Vaswani et al., 2017) translation system relies on self-attention networks (SANs), and has attracted growing interesting in the machine translation community. The Transformer generates an ordered sequence of positional embeddings by a positional encoding mechanism (Gehring et al., 2017a) to explicitly encode the order of dependencies between words in a sentence. The Transformer is adept at parallelizing of performing (multi-head) and stacking (multi-layer) SANs to learn the sentence representation to predict translation, and has delivered state-of-the-art performance on various translation tasks (Bojar et al., 2018; Marie et al., 2018). However, these positional embeddings focus on sequentially encoding order relations between words, and does not explicitly consider reordering information in a sentence, which may degrade the performance of Transformer translation systems. T"
P19-1174,N10-1129,0,0.03344,"Missing"
P19-1174,D15-1165,0,0.0408643,"Missing"
P19-1174,P18-1192,0,0.0268229,"reordering embedding was learned by considering the relationship between the positional embedding of a word and that of the entire sentence. The proposed reordering embedding can be easily introduced to the existing Transformer translation system to predict translations. Experiments showed that our method can significantly improve the performance of Transformer. In future work, we will further explore the effectiveness of the reordering mechanism and apply it to other natural language processing tasks, such dependency parsing (Zhang et al., 2016; Li et al., 2018), and semantic role labeling (He et al., 2018; Li et al., 2019). We are grateful to the anonymous reviewers and the area chair for their insightful comments and suggestions. This work was partially conducted under the program “Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology” of the Ministry of Internal Affairs and Communications (MIC), Japan. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT tenure-track researcher startup fund “Toward"
P19-1174,P17-4012,0,0.10494,"Missing"
P19-1174,N03-1017,0,0.379592,"ce pairs in advance to form a reordering model. This reordering model is then integrated into the translation decoding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words in a sentence to generate a fluent translation. Inspired by a distortion method originating in SMT (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006), there is a quite recent preliminary exploration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional position-based attention inside the fixed-size window, and reported a considerable improvement on the classical RNN-based NMT. This means that the word reordering information is also beneficial to the NMT. The Transformer (Vaswani et al., 2017) translation system relies on self-attention networks (SANs), and has attracted growing interesting in the machine translation community. The Transformer generates an ord"
P19-1174,P17-1064,0,0.0207119,"roving the performance of translations. Du and Way (2017) 1788 and Kawara et al. (2018) reported that the prereordering method had an negative impact on the NMT for the ASPEC JA-EN translation task. In particular, Kawara et al. (2018) assumed that one reason is the isolation between pre-ordering and NMT models, where both models are trained using independent optimization functions. In addition, several research works have been proposed to explicitly introduce syntax structure into the RNN-based NMT for encoding syntax ordering dependencies into sentence representations (Eriguchi et al., 2016; Li et al., 2017; Chen et al., 2017a,b; Wang et al., 2017b; Chen et al., 2018a). Recently, the neural Transformer translation system (Vaswani et al., 2017), which relies solely on self-attention networks, used a fixed order sequence of positional embeddings to encode order dependencies between words in a sentence. 3 3.1 Background Positional Encoding Mechanism Transformer (Vaswani et al., 2017) typically uses a positional encoding mechanism to encode order dependencies between words in a sentence. Formally, given a embedding sequence of source sentence of length J, X={x1 , · · · , xJ }, the positional embeddi"
P19-1174,D13-1054,0,0.0422787,"Missing"
P19-1174,C14-1179,0,0.0358165,"Missing"
P19-1174,C18-1271,0,0.0358463,"echanism to capture knowledge of reordering. A reordering embedding was learned by considering the relationship between the positional embedding of a word and that of the entire sentence. The proposed reordering embedding can be easily introduced to the existing Transformer translation system to predict translations. Experiments showed that our method can significantly improve the performance of Transformer. In future work, we will further explore the effectiveness of the reordering mechanism and apply it to other natural language processing tasks, such dependency parsing (Zhang et al., 2016; Li et al., 2018), and semantic role labeling (He et al., 2018; Li et al., 2019). We are grateful to the anonymous reviewers and the area chair for their insightful comments and suggestions. This work was partially conducted under the program “Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology” of the Ministry of Internal Affairs and Communications (MIC), Japan. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT"
P19-1174,P18-3004,0,0.15555,"Missing"
P19-1174,W18-6419,1,0.792532,"anslation system relies on self-attention networks (SANs), and has attracted growing interesting in the machine translation community. The Transformer generates an ordered sequence of positional embeddings by a positional encoding mechanism (Gehring et al., 2017a) to explicitly encode the order of dependencies between words in a sentence. The Transformer is adept at parallelizing of performing (multi-head) and stacking (multi-layer) SANs to learn the sentence representation to predict translation, and has delivered state-of-the-art performance on various translation tasks (Bojar et al., 2018; Marie et al., 2018). However, these positional embeddings focus on sequentially encoding order relations between words, and does not explicitly consider reordering information in a sentence, which may degrade the performance of Transformer translation systems. Thus, the reordering problem in NMT has not been studied extensively, especially in Transformer. In this paper, we propose a reordering mechanism for the Transformer translation system. We dynamically penalize the given positional embedding of a word depending on its contextual information, thus generating a reordering embedding for each word. The reorderi"
P19-1174,D16-1096,0,0.0297414,"Missing"
P19-1174,P06-1090,0,0.185574,"ation for machine translation. The proposed translation models outperform the state-of-the-art NMT baselines systems with a similar number of parameters and achieve comparable results compared to NMT systems with much more parameters. 2 2.1 Related Work Reordering Model for PBSMT In PBSMT, there has been a substantial amount of research works about reordering model, which was used as a key component to ensure the generation of fluent target translation. Bisazza and Federico (2016) divided these reordering models into four groups: Phrase orientation models (Tillman, 2004; Collins et al., 2005; Nagata et al., 2006; Zens and Ney, 2006; Galley and Manning, 2008; Cherry, 2013), simply known as lexicalized reordering models, predict whether the next translated source span should be placed on the right (monotone), the left (swap), or anywhere else (discontinuous) of the last translated one. Jump models (Al-Onaizan and Papineni, 2006; Green et al., 2010) predict the direction and length of the jump that is performed between consecutively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al., 2010, 2013) address this issue by directly"
P19-1174,W18-6319,0,0.0149356,"ng was set to 0.1, and the attention dropout and residual dropout were p = 0.1. The Adam optimizer (Kingma and Ba, 2014) was used to tune the parameters of the model. The learning rate was varied under a warm-up strategy with warmup steps of 8,000. For evaluation, we validated the model with an interval of 1,000 batches on the dev set. Following the training of 200,000 batches, the model with the highest BLEU score of the dev set was selected to evaluate on the test sets. During the decoding, the beam size was set to four. All models were trained and evaluated on a single P100 GPU. SacreBELU (Post, 2018) was used as the evaluation metric of EN-DE, and the multi-bleu.perl1 was used the evaluation metric of ZH-EN and JA-EN tasks. The signtest (Collins et al., 2005) was as statistical significance test. We re-implemented all methods (“this work” in the tables) on the OpenNMT toolkit (Klein et al., 1 https://github.com/mosessmt/mosesdecoder/tree/RELEASE-4.0/scripts/generic/multibleu.perl 2017). 6.4 Main Results To validate the effectiveness of our methods, the proposed models were first evaluated on the WMT14 EN-DE translation task as in the original Transformer translation system (Vaswani et al."
P19-1174,P16-1162,0,0.0678125,"ative positional embeddings into the selfattention mechanism of Transformer. Additional PE (control experiment): uses original absolute positional embeddings to enhance the position information of each SAN layer instead of the proposed reordering embeddings. Pre-reordering: a pre-ordering method (Goto et al., 2013) for JA-EN translation task was used to adjust the order of Japanese words in both the training, dev, and test datasets, and thus reordered each source sentence into the similar order as its target sentence. 6.3 System Setting For all models (base), the byte pair encoding algorithm (Sennrich et al., 2016) was adopted and the size of the vocabulary was set to 32,000. The number of dimensions of all input and output 1791 System Wu et al. (2016) Gehring et al. (2017b) Vaswani et al. (2017) Vaswani et al. (2017) this work Architecture newstest2014 Existing NMT systems GNMT 26.3 CONVS2S 26.36 Transformer (base) 27.3 Transformer (big) 28.4 Our NMT systems Transformer (base) 27.24 +Additional PEs 27.10 +Relative PEs 27.63 +Encoder REs 28.03++ +Decoder REs 27.61+ +Both REs 28.22++ Transformer (big) 28.34 +Both REs 29.11++ #Speed1 #Speed2 #Params N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 65.0M 213.0M 991"
P19-1174,N18-2074,0,0.0292439,"T07, LDC2004T08, and LDC2005T06. The MT06 and the MT02/MT03/MT04/MT05/MT08 datasets were used as the dev set and test set, respectively. 3) For the JA-EN translation task, the training dataset consisted of two million bilingual sentence pairs from the ASPEC corpus (Nakazawa et al., 2016). The dev set consisted of 1,790 sentence pairs and the test set of 1,812 sentence pairs. 6.2 Baseline Systems These baseline systems included: Transformer: a vanilla Transformer with absolute positional embedding (Vaswani et al., 2017), for example Transformer (base) and Transformer (big) models. Relative PE (Shaw et al., 2018): incorporates relative positional embeddings into the selfattention mechanism of Transformer. Additional PE (control experiment): uses original absolute positional embeddings to enhance the position information of each SAN layer instead of the proposed reordering embeddings. Pre-reordering: a pre-ordering method (Goto et al., 2013) for JA-EN translation task was used to adjust the order of Japanese words in both the training, dev, and test datasets, and thus reordered each source sentence into the similar order as its target sentence. 6.3 System Setting For all models (base), the byte pair en"
P19-1174,N04-4026,0,0.113282,"n reordering-aware sentence representation for machine translation. The proposed translation models outperform the state-of-the-art NMT baselines systems with a similar number of parameters and achieve comparable results compared to NMT systems with much more parameters. 2 2.1 Related Work Reordering Model for PBSMT In PBSMT, there has been a substantial amount of research works about reordering model, which was used as a key component to ensure the generation of fluent target translation. Bisazza and Federico (2016) divided these reordering models into four groups: Phrase orientation models (Tillman, 2004; Collins et al., 2005; Nagata et al., 2006; Zens and Ney, 2006; Galley and Manning, 2008; Cherry, 2013), simply known as lexicalized reordering models, predict whether the next translated source span should be placed on the right (monotone), the left (swap), or anywhere else (discontinuous) of the last translated one. Jump models (Al-Onaizan and Papineni, 2006; Green et al., 2010) predict the direction and length of the jump that is performed between consecutively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al.,"
P19-1174,P16-1008,0,0.0276056,"networks. In particular, the NN-based reordering models can not only capture semantic similarity but also ITG reordering constraints (Wu, 1996, 1997) in the translation context. This neural network modeling method is further applied to capture reordering information and syntactic coherence. 2.2 Modeling Ordering for NMT The attention-based NMT focused on neural networks themselves to implicitly capture order dependencies between words (Sutskever et al., 2014; Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Zhang et al., 2018). Coverage model can partially model the word order information (Tu et al., 2016; Mi et al., 2016). Inspired by a distortion method (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) originated from SMT, Zhang et al. (2017) proposed an additional position-based attention to enable the existing content-based attention to attend to the source words regarding both semantic requirement and the word reordering penalty. Pre-reordering, a pre-processing to make the source-side word orders close to those of the target side, has been proven very helpful for the SMT in improving translation quality. Moreover, neural networks were used to pre-reorder the sources"
P19-1174,P17-2089,1,0.579914,"Missing"
P19-1174,D17-1155,1,0.862726,"f reordering rules, Li et al. (2013, 2014) modeled ITG-based reordering rules in the translation by using neural networks. In particular, the NN-based reordering models can not only capture semantic similarity but also ITG reordering constraints (Wu, 1996, 1997) in the translation context. This neural network modeling method is further applied to capture reordering information and syntactic coherence. 2.2 Modeling Ordering for NMT The attention-based NMT focused on neural networks themselves to implicitly capture order dependencies between words (Sutskever et al., 2014; Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Zhang et al., 2018). Coverage model can partially model the word order information (Tu et al., 2016; Mi et al., 2016). Inspired by a distortion method (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) originated from SMT, Zhang et al. (2017) proposed an additional position-based attention to enable the existing content-based attention to attend to the source words regarding both semantic requirement and the word reordering penalty. Pre-reordering, a pre-processing to make the source-side word orders close to those of the target side, has been proven very helpfu"
P19-1174,P18-2048,1,0.899817,"Missing"
P19-1174,P96-1021,0,0.289443,"eordering operations in a single generative story, thereby combining elements from the previous three model families (Durrani et al., 2011, 2013, 2014). Their method were further extended by source syntax information (Chen et al., 2017c, 2018b) to improve the performance of SMT. Moreover, to address data sparsity (Guta et al., 2015) caused by a mass of reordering rules, Li et al. (2013, 2014) modeled ITG-based reordering rules in the translation by using neural networks. In particular, the NN-based reordering models can not only capture semantic similarity but also ITG reordering constraints (Wu, 1996, 1997) in the translation context. This neural network modeling method is further applied to capture reordering information and syntactic coherence. 2.2 Modeling Ordering for NMT The attention-based NMT focused on neural networks themselves to implicitly capture order dependencies between words (Sutskever et al., 2014; Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Zhang et al., 2018). Coverage model can partially model the word order information (Tu et al., 2016; Mi et al., 2016). Inspired by a distortion method (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) orig"
P19-1174,J97-3002,0,0.637414,"Missing"
P19-1174,P06-1066,0,0.0442704,"model plays an important role in phrase-based statistical machine translation (PBSMT), especially for translation between distant language pairs with large differences in word order, such as Chinese-to-English and Japaneseto-English translations (Galley and Manning, 2008; Goto et al., 2013). Typically, the traditional PBSMT learns large-scale reordering rules from parallel bilingual sentence pairs in advance to form a reordering model. This reordering model is then integrated into the translation decoding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words in a sentence to generate a fluent translation. Inspired by a distortion method originating in SMT (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006), there is a quite recent preliminary exploration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional position-based attent"
P19-1174,W06-3108,0,0.0419669,"nslation. The proposed translation models outperform the state-of-the-art NMT baselines systems with a similar number of parameters and achieve comparable results compared to NMT systems with much more parameters. 2 2.1 Related Work Reordering Model for PBSMT In PBSMT, there has been a substantial amount of research works about reordering model, which was used as a key component to ensure the generation of fluent target translation. Bisazza and Federico (2016) divided these reordering models into four groups: Phrase orientation models (Tillman, 2004; Collins et al., 2005; Nagata et al., 2006; Zens and Ney, 2006; Galley and Manning, 2008; Cherry, 2013), simply known as lexicalized reordering models, predict whether the next translated source span should be placed on the right (monotone), the left (swap), or anywhere else (discontinuous) of the last translated one. Jump models (Al-Onaizan and Papineni, 2006; Green et al., 2010) predict the direction and length of the jump that is performed between consecutively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al., 2010, 2013) address this issue by directly modeling the reorde"
P19-1174,P17-1140,0,0.233489,"ding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words in a sentence to generate a fluent translation. Inspired by a distortion method originating in SMT (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006), there is a quite recent preliminary exploration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional position-based attention inside the fixed-size window, and reported a considerable improvement on the classical RNN-based NMT. This means that the word reordering information is also beneficial to the NMT. The Transformer (Vaswani et al., 2017) translation system relies on self-attention networks (SANs), and has attracted growing interesting in the machine translation community. The Transformer generates an ordered sequence of positional embeddings by a positional encoding mechanism (Gehring et al., 2017a) to explicitly enc"
P19-1174,D18-1511,1,0.794361,". (2013, 2014) modeled ITG-based reordering rules in the translation by using neural networks. In particular, the NN-based reordering models can not only capture semantic similarity but also ITG reordering constraints (Wu, 1996, 1997) in the translation context. This neural network modeling method is further applied to capture reordering information and syntactic coherence. 2.2 Modeling Ordering for NMT The attention-based NMT focused on neural networks themselves to implicitly capture order dependencies between words (Sutskever et al., 2014; Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Zhang et al., 2018). Coverage model can partially model the word order information (Tu et al., 2016; Mi et al., 2016). Inspired by a distortion method (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) originated from SMT, Zhang et al. (2017) proposed an additional position-based attention to enable the existing content-based attention to attend to the source words regarding both semantic requirement and the word reordering penalty. Pre-reordering, a pre-processing to make the source-side word orders close to those of the target side, has been proven very helpful for the SMT in improving tra"
P19-1174,P16-1131,0,0.0292113,"posed a reordering mechanism to capture knowledge of reordering. A reordering embedding was learned by considering the relationship between the positional embedding of a word and that of the entire sentence. The proposed reordering embedding can be easily introduced to the existing Transformer translation system to predict translations. Experiments showed that our method can significantly improve the performance of Transformer. In future work, we will further explore the effectiveness of the reordering mechanism and apply it to other natural language processing tasks, such dependency parsing (Zhang et al., 2016; Li et al., 2018), and semantic role labeling (He et al., 2018; Li et al., 2019). We are grateful to the anonymous reviewers and the area chair for their insightful comments and suggestions. This work was partially conducted under the program “Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology” of the Ministry of Internal Affairs and Communications (MIC), Japan. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Sc"
P19-1174,D18-1036,0,0.0254378,"93; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) originated from SMT, Zhang et al. (2017) proposed an additional position-based attention to enable the existing content-based attention to attend to the source words regarding both semantic requirement and the word reordering penalty. Pre-reordering, a pre-processing to make the source-side word orders close to those of the target side, has been proven very helpful for the SMT in improving translation quality. Moreover, neural networks were used to pre-reorder the sourceside word orders close to those of the target side (Du and Way, 2017; Zhao et al., 2018b; Kawara et al., 2018), and thus were input to the existing RNN-based NMT for improving the performance of translations. Du and Way (2017) 1788 and Kawara et al. (2018) reported that the prereordering method had an negative impact on the NMT for the ASPEC JA-EN translation task. In particular, Kawara et al. (2018) assumed that one reason is the isolation between pre-ordering and NMT models, where both models are trained using independent optimization functions. In addition, several research works have been proposed to explicitly introduce syntax structure into the RNN-based NMT for encoding s"
P19-1174,L18-1143,0,0.0642441,"93; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) originated from SMT, Zhang et al. (2017) proposed an additional position-based attention to enable the existing content-based attention to attend to the source words regarding both semantic requirement and the word reordering penalty. Pre-reordering, a pre-processing to make the source-side word orders close to those of the target side, has been proven very helpful for the SMT in improving translation quality. Moreover, neural networks were used to pre-reorder the sourceside word orders close to those of the target side (Du and Way, 2017; Zhao et al., 2018b; Kawara et al., 2018), and thus were input to the existing RNN-based NMT for improving the performance of translations. Du and Way (2017) 1788 and Kawara et al. (2018) reported that the prereordering method had an negative impact on the NMT for the ASPEC JA-EN translation task. In particular, Kawara et al. (2018) assumed that one reason is the isolation between pre-ordering and NMT models, where both models are trained using independent optimization functions. In addition, several research works have been proposed to explicitly introduce syntax structure into the RNN-based NMT for encoding s"
P19-1207,P00-1041,0,0.28294,"expected to watch World Cup. Table 7: Examples of the generated templates and summaries by our model. ‘#’ refers to masked numbers. 5 Related Work Abstractive sentence summarization, a task analogous to headline generation or sentence compression, aims to generate a brief summary given a short source article. Early studies in this problem mainly focus on statistical or linguistic-rule-based methods, including those based on extractive and compression (Jing and McKeown, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2010), templates (Zhou and Hovy, 2004) and statistical machine translation (Banko et al., 2000). The advent of large-scale summarization corpora accelerates the development of various neural network methods. Rush et al. (2015) first applied an attention-based sequence-to-sequence model for abstractive summarization, which includes a convolutional neural network (CNN) encoder and a feed-forward network decoder. Chopra et al. (2016) replaced the decoder with a recurrent neural network (RNN). Nallapati et al. (2016) further changed the sequence-to-sequence model to a fully RNN-based model. Besides, Gu et al. (2016) found that this task benefits from copying words from the source articles a"
P19-1207,P18-1015,0,0.1924,"summaries in no need of any training data. However, it is unrealistic to create all the templates manually since this work requires considerable domain knowledge and is also labor-intensive. Fortunately, the summaries of some specific training articles can provide similar guidance to the summarization as hard templates. Accordingly, these summaries are referred to as soft templates, or templates for simplicity, in this paper. Despite their potential in relieving the verbosity and insufficiency problems of natural language data, templates have not been exploited to full advantage. For example, Cao et al. (2018a) simply concatenated template encoding after the source article in their summarization work. To this end, we propose a Bi-directional Selective Encoding with Template (BiSET) model for abstractive sentence summarization. Our model involves a novel bi-directional selective layer with two gates to mutually select key information from an article and its template to assist with summary generation. Due to the limitations in obtaining handcrafted templates, we further propose a multi-stage process for automatic retrieval of high-quality templates from training corpus. Extensive experiments were co"
P19-1207,A00-2024,0,0.315638,"lion tv viewers expected for opening World Cup match. billions around world watch the Olympic Games opening ceremony. #.# billions around world expected to watch World Cup. Table 7: Examples of the generated templates and summaries by our model. ‘#’ refers to masked numbers. 5 Related Work Abstractive sentence summarization, a task analogous to headline generation or sentence compression, aims to generate a brief summary given a short source article. Early studies in this problem mainly focus on statistical or linguistic-rule-based methods, including those based on extractive and compression (Jing and McKeown, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2010), templates (Zhou and Hovy, 2004) and statistical machine translation (Banko et al., 2000). The advent of large-scale summarization corpora accelerates the development of various neural network methods. Rush et al. (2015) first applied an attention-based sequence-to-sequence model for abstractive summarization, which includes a convolutional neural network (CNN) encoder and a feed-forward network decoder. Chopra et al. (2016) replaced the decoder with a recurrent neural network (RNN). Nallapati et al. (2016) further changed the sequence-to-sequ"
P19-1207,P14-1062,0,0.0191004,"on for each source article/candidate template. Then, a similarity matrix S ∈ Rm×n is calculated for a given article representation, S ∈ Rm×d , and a template representation, T ∈ Rn×d : sij = f (Si , Tj ) Most previous work uses dot product or bilinear function (Chen et al., 2016) for the similarity, yet we find the family of Euclidean distance perform much better for our task. Therefore, we define the similarity function as: (5) Pooling Layer. This layer is intended to filter out unnecessary information in the matrix S. Before applying such pooling operations as max-pooling and k-max pooling (Kalchbrenner et al., 2014) over the similarity matrix, we note there are repeated words in the source article, which we only want to count once. For this reason, we first identify some salient weights from S: q = maxcolumn (S) p = k-max(q) (7) a = ReLU (Wa p + b1 ) (8) s = σ(Ws a + b2 ) (9) (3) where f is the similarity function, and the common options for f include:  T  dot product x y, T f (x, y) = x W y, (4) bilinear function   kx − yk, Euclidean distance f (x, y) = exp(−kx − yk2 ) select k most important weights, p ∈ Rk . Finally, we apply a two-layer feed-forward network to output a similarity score for the s"
P19-1207,P16-1223,0,0.0724219,"Missing"
P19-1207,N16-1012,0,0.600749,"rization dataset were conducted and the results show that the template-equipped BiSET model manages to improve the summarization performance significantly with a new state of the art. 1 Introduction Abstractive summarization aims to shorten a source article or paragraph by rewriting while preserving the main idea. Due to the difficulties in rewriting long documents, a large body of research on this topic has focused on paragraph-level article summarization. Among them, sequence-tosequence models have become the mainstream and some have achieved state-of-the-art performance (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016). In general, the only available information for these models during decoding is simply the source article representations from the encoder and the generated words from the previous time steps (Nallapati et al., 2016; Gu et al., 2016; Lin et al., 2018), while the previous words are also generated based on the article representations. Since natural language text is complicated and verbose in nature, and training data is insufficient in size to help the models distinguish important article information from noise, sequence-to∗ Corresponding author. sequence models tend to"
P19-1207,J10-3005,0,0.0298733,"atch. billions around world watch the Olympic Games opening ceremony. #.# billions around world expected to watch World Cup. Table 7: Examples of the generated templates and summaries by our model. ‘#’ refers to masked numbers. 5 Related Work Abstractive sentence summarization, a task analogous to headline generation or sentence compression, aims to generate a brief summary given a short source article. Early studies in this problem mainly focus on statistical or linguistic-rule-based methods, including those based on extractive and compression (Jing and McKeown, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2010), templates (Zhou and Hovy, 2004) and statistical machine translation (Banko et al., 2000). The advent of large-scale summarization corpora accelerates the development of various neural network methods. Rush et al. (2015) first applied an attention-based sequence-to-sequence model for abstractive summarization, which includes a convolutional neural network (CNN) encoder and a feed-forward network decoder. Chopra et al. (2016) replaced the decoder with a recurrent neural network (RNN). Nallapati et al. (2016) further changed the sequence-to-sequence model to a fully RNN-based model. Besides, Gu"
P19-1207,P16-1154,0,0.249828,"aragraph by rewriting while preserving the main idea. Due to the difficulties in rewriting long documents, a large body of research on this topic has focused on paragraph-level article summarization. Among them, sequence-tosequence models have become the mainstream and some have achieved state-of-the-art performance (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016). In general, the only available information for these models during decoding is simply the source article representations from the encoder and the generated words from the previous time steps (Nallapati et al., 2016; Gu et al., 2016; Lin et al., 2018), while the previous words are also generated based on the article representations. Since natural language text is complicated and verbose in nature, and training data is insufficient in size to help the models distinguish important article information from noise, sequence-to∗ Corresponding author. sequence models tend to deteriorate with the accumulation of word generation, e.g., they generate irrelevant and repeated words frequently (Koehn and Knowles, 2017). Template-based summarization (Zhou and Hovy, 2004) is an effective approach to traditional abstractive summarizatio"
P19-1207,P16-1014,0,0.0194828,"development of various neural network methods. Rush et al. (2015) first applied an attention-based sequence-to-sequence model for abstractive summarization, which includes a convolutional neural network (CNN) encoder and a feed-forward network decoder. Chopra et al. (2016) replaced the decoder with a recurrent neural network (RNN). Nallapati et al. (2016) further changed the sequence-to-sequence model to a fully RNN-based model. Besides, Gu et al. (2016) found that this task benefits from copying words from the source articles and proposed the CopyNet correspondingly. With a similar purpose, Gulcehre et al. (2016) proposed to use a switch gate to control when to copy from the source article and when to generate from the vocabulary. Zhou et al. (2017) employed a selective gate to filter out unimportant information when encoding. Some other work attempts to incorporate external knowledge for abstractive summarization. For example, Nallapati et al. (2016) proposed to en6 Conclusion In this paper, we presented a novel Bi-directional Selective Encoding with Template (BiSET) model for abstractive sentence summarization. To counteract the verbosity and insufficiency of training data, we proposed to retrieve h"
P19-1207,W17-3204,0,0.0202047,"source article representations from the encoder and the generated words from the previous time steps (Nallapati et al., 2016; Gu et al., 2016; Lin et al., 2018), while the previous words are also generated based on the article representations. Since natural language text is complicated and verbose in nature, and training data is insufficient in size to help the models distinguish important article information from noise, sequence-to∗ Corresponding author. sequence models tend to deteriorate with the accumulation of word generation, e.g., they generate irrelevant and repeated words frequently (Koehn and Knowles, 2017). Template-based summarization (Zhou and Hovy, 2004) is an effective approach to traditional abstractive summarization, in which a number of hard templates are manually created by domain experts, and key snippets are then extracted and populated into the templates to form the final summaries. The advantage of such approach is it can guarantee concise and coherent summaries in no need of any training data. However, it is unrealistic to create all the templates manually since this work requires considerable domain knowledge and is also labor-intensive. Fortunately, the summaries of some specific"
P19-1207,N03-1020,0,0.80734,"ce de(13) ct = M X αt,i zis (16) (17) (18) i=1 After that, a simple concatenation layer is used to combine the hidden state hct and the context vector ct into a new hidden state hat : hat = tanh(Wha [ct ; hct ]) (19) which will be mapped to a new representation of 2156 vocabulary size and fed through a softmax layer to output the target word distribution: p(wt |w1 , ..., wt−1 ) = sof tmax(Wp hat ) 2.5 (20) Training The Retrieve module involves an unsupervised process with traditional indexing and retrieval techniques. For Fast Rerank, since there is no ground truth available, we use ROUGE-13 (Lin and Hovy, 2003) to evaluate the saliency of a candidate template with respect to the gold summary of current source article. Therefore, the loss function is defined as: N 1 X ∗ [s log s + (1 − s∗ ) log(1 − s)] N i=1 (21) where s is a score predicted by Equation 9, and N is the product of the training set size, D, and the number of retrieved templates for each article. For the BiSET module, the loss function is chosen as the negative log-likelihood between the generated summary, w, and the true summary, w∗ : Lr (θ) = − Lw (θ) = − D L 1 XX ∗(i) (i) log p(wj |wj−1 , x(i) , y (i) ) D i=1 j=1 (22) where L is the"
P19-1207,P18-2027,0,0.0540335,"ting while preserving the main idea. Due to the difficulties in rewriting long documents, a large body of research on this topic has focused on paragraph-level article summarization. Among them, sequence-tosequence models have become the mainstream and some have achieved state-of-the-art performance (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016). In general, the only available information for these models during decoding is simply the source article representations from the encoder and the generated words from the previous time steps (Nallapati et al., 2016; Gu et al., 2016; Lin et al., 2018), while the previous words are also generated based on the article representations. Since natural language text is complicated and verbose in nature, and training data is insufficient in size to help the models distinguish important article information from noise, sequence-to∗ Corresponding author. sequence models tend to deteriorate with the accumulation of word generation, e.g., they generate irrelevant and repeated words frequently (Koehn and Knowles, 2017). Template-based summarization (Zhou and Hovy, 2004) is an effective approach to traditional abstractive summarization, in which a numbe"
P19-1207,D15-1166,0,0.0386611,"n in Figure 2, BiSET contains two selective gates: Template-to-Article (T2A) gate and Article-toTemplate (A2T) gate. The role of T2A is to use a template to filter the source article representation: gi = σ(Wsh hsi + Wth ht hgi = hsi ⊗ gi + bs ) gree d is computed by: d = σ((hs )T Wd ht + bd ) The final source article representation is calculated as the weighted sum of hsi and hgi : zis = dhgi + (1 − d)hsi (14) which allows a flexible manner for template incorporation and helps to resist errors when lowquality templates are given. The decoder layer. This layer includes an ordinary RNN decoder (Luong et al., 2015). At each time step t, the decoder reads the word wt−1 and hidden state hct−1 generated in the previous step, and gives a new hidden state for the current step: hct = RN N (wt−1 , hct−1 ) (15) where the hidden state is initialized with the original source article representation, hs . We then compute the attention between hct and the final article representation z s to obtain a context vector ct : εt,i = (zis )T Wc hct exp(εt,i ) αt,i = PM i=1 exp(εt,i ) (11) (12) where ht is the concatenation of the last forward − → t hidden state, hn , and the first backward hidden ← − state, ht1 , of the tem"
P19-1207,K16-1028,0,0.403532,"conducted and the results show that the template-equipped BiSET model manages to improve the summarization performance significantly with a new state of the art. 1 Introduction Abstractive summarization aims to shorten a source article or paragraph by rewriting while preserving the main idea. Due to the difficulties in rewriting long documents, a large body of research on this topic has focused on paragraph-level article summarization. Among them, sequence-tosequence models have become the mainstream and some have achieved state-of-the-art performance (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016). In general, the only available information for these models during decoding is simply the source article representations from the encoder and the generated words from the previous time steps (Nallapati et al., 2016; Gu et al., 2016; Lin et al., 2018), while the previous words are also generated based on the article representations. Since natural language text is complicated and verbose in nature, and training data is insufficient in size to help the models distinguish important article information from noise, sequence-to∗ Corresponding author. sequence models tend to deteriorate with the acc"
P19-1207,W12-3018,0,0.0768382,"Missing"
P19-1207,D15-1044,0,0.627986,"on a standard summarization dataset were conducted and the results show that the template-equipped BiSET model manages to improve the summarization performance significantly with a new state of the art. 1 Introduction Abstractive summarization aims to shorten a source article or paragraph by rewriting while preserving the main idea. Due to the difficulties in rewriting long documents, a large body of research on this topic has focused on paragraph-level article summarization. Among them, sequence-tosequence models have become the mainstream and some have achieved state-of-the-art performance (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016). In general, the only available information for these models during decoding is simply the source article representations from the encoder and the generated words from the previous time steps (Nallapati et al., 2016; Gu et al., 2016; Lin et al., 2018), while the previous words are also generated based on the article representations. Since natural language text is complicated and verbose in nature, and training data is insufficient in size to help the models distinguish important article information from noise, sequence-to∗ Corresponding author. se"
P19-1207,W04-1000,0,0.509913,"generated words from the previous time steps (Nallapati et al., 2016; Gu et al., 2016; Lin et al., 2018), while the previous words are also generated based on the article representations. Since natural language text is complicated and verbose in nature, and training data is insufficient in size to help the models distinguish important article information from noise, sequence-to∗ Corresponding author. sequence models tend to deteriorate with the accumulation of word generation, e.g., they generate irrelevant and repeated words frequently (Koehn and Knowles, 2017). Template-based summarization (Zhou and Hovy, 2004) is an effective approach to traditional abstractive summarization, in which a number of hard templates are manually created by domain experts, and key snippets are then extracted and populated into the templates to form the final summaries. The advantage of such approach is it can guarantee concise and coherent summaries in no need of any training data. However, it is unrealistic to create all the templates manually since this work requires considerable domain knowledge and is also labor-intensive. Fortunately, the summaries of some specific training articles can provide similar guidance to t"
P19-1207,P17-1101,0,0.787968,"ng with Template model (BiSET) and (b) the bi-directional selective layer. umn of S by softmax, giving rise to two new matrices S and S. After that, the Dynamic Coattention Network (DCN) attention is applied to compute the bi-directional attention: A = S · ht and T B = S · S · hs , where A denotes article-totemplate attention and B is template-to-article attention. Finally, we obtain the template-aware article representation {zis }M i=1 : zis 2.4 = [hsi ; Ai ; hsi ⊗ Ai ; hsi ⊗ Bi ] (10) BiSET Inspired by the research in machine reading comprehension (Seo et al., 2017) and selective mechanism (Zhou et al., 2017), we propose a novel Bi-directional Selective Encoding with Template (BiSET) model for abstractive sentence summarization. The core idea behind BiSET is to involve templates to assist with article representation and summary generation. As shown in Figure 2, BiSET contains two selective gates: Template-to-Article (T2A) gate and Article-toTemplate (A2T) gate. The role of T2A is to use a template to filter the source article representation: gi = σ(Wsh hsi + Wth ht hgi = hsi ⊗ gi + bs ) gree d is computed by: d = σ((hs )T Wd ht + bd ) The final source article representation is calculated as the we"
P19-1229,P16-1231,0,0.0859338,"Missing"
P19-1229,K18-2005,0,0.0589861,"1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, with the surge of web data (or user generated content), cross-domain parsing has become the major challenge for applying syntactic analysis in realistic NLP systems. To meet this challenge, the community has organized several shared tasks to attract more research attention (Nivre et al., 2007; Hajiˇc et al., 2009; Petrov and McDonald, 2012). 2386 Proceedings of the 57th Annual Meeting of"
P19-1229,D14-1082,0,0.145105,"ency from the head wh to the modifier wm ∗ ł this Introduction Corresponding author The two domain-specific datasets, plus another one for product comment texts, are also used in the NLPCC-2019 shared task (http://hlt.suda.edu.cn/index. php/Nlpcc-2019-shared-task) on cross-domain Chinese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the"
P19-1229,D18-1217,0,0.105638,"Missing"
P19-1229,P07-1033,0,0.607207,"Missing"
P19-1229,N19-1423,0,0.066859,"Missing"
P19-1229,P15-1033,0,0.0218671,"the modifier wm ∗ ł this Introduction Corresponding author The two domain-specific datasets, plus another one for product comment texts, are also used in the NLPCC-2019 shared task (http://hlt.suda.edu.cn/index. php/Nlpcc-2019-shared-task) on cross-domain Chinese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation"
P19-1229,N09-1068,0,0.0306646,"meta-training to learn to compute the point-to-set distance between a target-domain example and a source domain. Semi-supervised domain adaptation assumes there exist some (usually very small-scale) labeled target-domain data, which can be used to directly learn the domain-specific distributions or features. Daum´e III (2007) propose a simple yet effective feature augmentation approach that performs well on a number of sequence labeling tasks. The idea is to distinguish domain-specific and general features by making a copy of each feature for each domain plus a shared (general) pseudo domain. Finkel and Manning (2009) further propose a hierarchical Bayesian extension of this idea. As pointed by Finkel and Manning (2009), those two works can be understood as MTL under the traditional discrete-feature ML framework. Kim et al. (2017) propose a neural mixture of experts approach for cross-domain intent classification and slot tagging. Different from the unsupervised method of Guo et al. (2018), they use a small amount of target-domain labeled data to train an attention module for the computation of example-to-domain distances. In the parsing community, Flannery and Mori (2015) propose to annotate partially lab"
P19-1229,W15-2202,0,0.0307514,"shared (general) pseudo domain. Finkel and Manning (2009) further propose a hierarchical Bayesian extension of this idea. As pointed by Finkel and Manning (2009), those two works can be understood as MTL under the traditional discrete-feature ML framework. Kim et al. (2017) propose a neural mixture of experts approach for cross-domain intent classification and slot tagging. Different from the unsupervised method of Guo et al. (2018), they use a small amount of target-domain labeled data to train an attention module for the computation of example-to-domain distances. In the parsing community, Flannery and Mori (2015) propose to annotate partially labeled target-domain data with active learning for cross-domain Japanese dependency parsing. Similarly, Joshi et al. (2018) annotate a few dozen partially labeled target-domain sentences with a few brackets for cross-domain constituent parsing. Both results report large improvement and show the usefulness of even small amount of target-domain annotation, showing the great potential of semi-supervised domain adaptation for parsing. 6 Conclusions This work addresses the task of semi-supervised domain adaptation for Chinese dependency parsing, based on our two newl"
P19-1229,I11-1100,0,0.0717653,"Missing"
P19-1229,C16-1002,0,0.0777974,"erse annotation guideline) for a language. Inspired by their work, we propose to concatenate each word position with an extra domain embedding to indicate which domain this training sentence comes from, as illustrated in Figure 3. In this way, we expect the model can fully utilize both training datasets, since most parameters are shared except the two domain embedding vectors, and learn to distinguish the domain-specific and general features as well. (3) Multi-task learning (MTL) aims to incorporate labeled data of multiple related tasks for improving performance (Collobert and Weston, 2008). Guo et al. (2016) first employ MTL to improve parsing performance by utilizing multiple heterogeneous treebanks and treating each treebank as a separate task. As shown in Figure 4, we make a straightforward extension to the biaffine parser to realize multi-task learning. The sourcedomain and target-domain parsing are treated as xi ... Figure 4: The framework of MTL. two individual tasks with shared parameters for word/tag embeddings and BiLSTMs. The main weakness of MTL is that the model cannot make full use of the source-domain labeled data, since the source-domain training data only contributes to the traini"
P19-1229,D18-1498,0,0.0496972,"ubset from the source-domain training data to train the parsing model, instead of using all the labeled data (Plank and van Noord, 2011; Khan et al., 2013). The multi-source domain adaptation problem assumes there are labeled datasets for multiple source domains. Given a target domain, the challenge is how to effectively combine knowledge in the source domains. McClosky et al. (2010) first raise this scenario for constituent parsing. They employ a regression model to predict crossdomain performance, and then use the values to combine parsing models independently trained on each source domain. Guo et al. (2018) employ a similar idea of mixture of experts under the neural MTL framework, and conduct experiments on sentiment classification and POS tagging tasks. They employ meta-training to learn to compute the point-to-set distance between a target-domain example and a source domain. Semi-supervised domain adaptation assumes there exist some (usually very small-scale) labeled target-domain data, which can be used to directly learn the domain-specific distributions or features. Daum´e III (2007) propose a simple yet effective feature augmentation approach that performs well on a number of sequence labe"
P19-1229,P18-1252,1,0.921905,"this work, we choose two typical domain-aware web texts for annotation, i.e., product blogs and web fictions. This section introduces the details about the data annotation procedure. Data selection. The product blog (PB) texts are crawled from the Taobao headline website, which contains articles written by users mainly on description and comparison of different commercial products. After data cleaning and automatic word segmentation, we have collected about 340K sentences. Then, we select 10 thousand sentences with [5, 25] words for manual annotation following the active learning workflow of Jiang et al. (2018). The remaining sentences are used as unlabeled data. For web fictions, we follow the work on cross-domain word segmentation of Zhang et al. (2014), and adopt the popular novel named as “Zhuxian” (ZX, also known as “Jade dynasty”). Among their annotated 4,555 sentences, we select about 3,400 sentences with [5, 45] words for annotation. The remaining 32K sentences of ZX are used as unlabeled data in this work. Annotation guideline. After comparing several publicly available guidelines for dependency parsing including the universal dependencies (UD) (McDonald et al., 2013), we adopt the guidelin"
P19-1229,P18-1110,0,0.0770222,"Missing"
P19-1229,R13-1046,0,0.0528327,"Missing"
P19-1229,P17-1060,0,0.0488226,", which can be used to directly learn the domain-specific distributions or features. Daum´e III (2007) propose a simple yet effective feature augmentation approach that performs well on a number of sequence labeling tasks. The idea is to distinguish domain-specific and general features by making a copy of each feature for each domain plus a shared (general) pseudo domain. Finkel and Manning (2009) further propose a hierarchical Bayesian extension of this idea. As pointed by Finkel and Manning (2009), those two works can be understood as MTL under the traditional discrete-feature ML framework. Kim et al. (2017) propose a neural mixture of experts approach for cross-domain intent classification and slot tagging. Different from the unsupervised method of Guo et al. (2018), they use a small amount of target-domain labeled data to train an attention module for the computation of example-to-domain distances. In the parsing community, Flannery and Mori (2015) propose to annotate partially labeled target-domain data with active learning for cross-domain Japanese dependency parsing. Similarly, Joshi et al. (2018) annotate a few dozen partially labeled target-domain sentences with a few brackets for cross-do"
P19-1229,Q16-1023,0,0.122391,"Missing"
P19-1229,P18-1249,0,0.0262471,"g has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, with the surge of web data (or user generated content), cross-domain parsing has become the major challenge for applying syntactic analysis in realistic NLP systems. To meet this challenge, the community has organized several shared tasks to attract more research attention (Nivre et al., 2007; Hajiˇc et al., 2009; Petrov and McDonald, 2012). 2386 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
P19-1229,D14-1108,0,0.0607182,"Missing"
P19-1229,P14-1043,1,0.843831,"e 4: The framework of MTL. two individual tasks with shared parameters for word/tag embeddings and BiLSTMs. The main weakness of MTL is that the model cannot make full use of the source-domain labeled data, since the source-domain training data only contributes to the training of the shared parameters. The corpus weighting strategy. For all above three approaches, the target-domain labeled data would be overwhelmed by the source-domain data during training if directly combined, since there usually exists a very big gap in their scale. Therefore, we employ the simple corpus weighting strategy (Li et al., 2014) as a useful trick. Before each iteration, we randomly sample training sentences separately from the target- and sourcedomain training data in the proportion of 1 : M . Then we merge and randomly shuffle the sampled data for one-iteration training. We treat M ≥ 1 as a hyper-parameter tuned on the dev data. 3.3 Utilizing Unlabeled Data Besides labeled data, how to exploit unlabeled data, both target- and source-domain, has been an interesting and important direction for crossdomain parsing for a long time, as discussed in Section 5. Recently, Peters et al. (2018) introduce embeddings from langu"
P19-1229,P18-1130,0,0.0228167,"product comment texts, are also used in the NLPCC-2019 shared task (http://hlt.suda.edu.cn/index. php/Nlpcc-2019-shared-task) on cross-domain Chinese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, with the surge of web data (or user generated content), cross-domain parsing has become the majo"
P19-1229,P06-1043,0,0.131945,"arch, we try to give a brief (and far from complete) review on some representative approaches of high relevance with syntactic parsing. Unsupervised domain adaptation. Due to the lack of sufficient labeled data, most previous works focuses on unsupervised domain adapta2392 tion, assuming there is only labeled data for the source domain. Researchers make great effort to learn useful features from large-scale unlabeled target-domain data, which is usually much easier to collect. As a typical semi-supervised approach, self-training is shown to be very useful for cross-domain constituent parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015). There are also many failed works on applying self-training for in-domain and crossdomain dependency parsing. Sagae and Tsujii (2007) apply co-training to the CoNLL-2007 cross-domain dependency parsing task and report positive gains (Nivre et al., 2007). In contrast, Dredze et al. (2007) experiment with many domain adaptation approaches with no success on the same datasets and suggest the major obstacle comes from the divergent annotation guideline adopted by the target-domain evaluation data. Source-domain data selection is another interesting researc"
P19-1229,N10-1004,0,0.035199,"s from the divergent annotation guideline adopted by the target-domain evaluation data. Source-domain data selection is another interesting research direction. Given a target domain, the idea is to automatically select a most relevant subset from the source-domain training data to train the parsing model, instead of using all the labeled data (Plank and van Noord, 2011; Khan et al., 2013). The multi-source domain adaptation problem assumes there are labeled datasets for multiple source domains. Given a target domain, the challenge is how to effectively combine knowledge in the source domains. McClosky et al. (2010) first raise this scenario for constituent parsing. They employ a regression model to predict crossdomain performance, and then use the values to combine parsing models independently trained on each source domain. Guo et al. (2018) employ a similar idea of mixture of experts under the neural MTL framework, and conduct experiments on sentiment classification and POS tagging tasks. They employ meta-training to learn to compute the point-to-set distance between a target-domain example and a source domain. Semi-supervised domain adaptation assumes there exist some (usually very small-scale) labele"
P19-1229,D07-1111,0,0.070654,"the lack of sufficient labeled data, most previous works focuses on unsupervised domain adapta2392 tion, assuming there is only labeled data for the source domain. Researchers make great effort to learn useful features from large-scale unlabeled target-domain data, which is usually much easier to collect. As a typical semi-supervised approach, self-training is shown to be very useful for cross-domain constituent parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015). There are also many failed works on applying self-training for in-domain and crossdomain dependency parsing. Sagae and Tsujii (2007) apply co-training to the CoNLL-2007 cross-domain dependency parsing task and report positive gains (Nivre et al., 2007). In contrast, Dredze et al. (2007) experiment with many domain adaptation approaches with no success on the same datasets and suggest the major obstacle comes from the divergent annotation guideline adopted by the target-domain evaluation data. Source-domain data selection is another interesting research direction. Given a target domain, the idea is to automatically select a most relevant subset from the source-domain training data to train the parsing model, instead of usin"
P19-1229,D14-1122,0,0.0233107,"sume there is no labeled target-domain training data and thus focus on unsupervised domain adaptation. So far, approaches in this direction have made limited progress, due to the intrinsic difficulty of both domain adaptation and parsing (see discussions in Section 5). On the other hand, due to the extreme complexity and heavy cost, progress on syntactic data annotation on new-domain texts has been very slow, and only several small-scale datasets on web texts have been built, mostly as evaluation data for cross-domain parsing (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). To meet the above challenges, this paper presents two newly-annotated large-scale domainaware datasets (over 12K sentences), and try to tackle the task of semi-supervised domain adaptation for Chinese dependency parsing. With the access of both labeled and unlabeled targetdomain data, we propose and evaluate several simple approaches and conduct error analysis in order to investigate the following three questions: Q1: How to effectively combine the source- and target-domain labeled training data? Q2: How to utilize the target-domain unlabeled data for further improvements? Q3: Given a certai"
P19-1229,W15-2201,0,0.286589,"lete) review on some representative approaches of high relevance with syntactic parsing. Unsupervised domain adaptation. Due to the lack of sufficient labeled data, most previous works focuses on unsupervised domain adapta2392 tion, assuming there is only labeled data for the source domain. Researchers make great effort to learn useful features from large-scale unlabeled target-domain data, which is usually much easier to collect. As a typical semi-supervised approach, self-training is shown to be very useful for cross-domain constituent parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015). There are also many failed works on applying self-training for in-domain and crossdomain dependency parsing. Sagae and Tsujii (2007) apply co-training to the CoNLL-2007 cross-domain dependency parsing task and report positive gains (Nivre et al., 2007). In contrast, Dredze et al. (2007) experiment with many domain adaptation approaches with no success on the same datasets and suggest the major obstacle comes from the divergent annotation guideline adopted by the target-domain evaluation data. Source-domain data selection is another interesting research direction. Given a target domain, the i"
P19-1229,E14-1062,0,0.0554062,"about the data annotation procedure. Data selection. The product blog (PB) texts are crawled from the Taobao headline website, which contains articles written by users mainly on description and comparison of different commercial products. After data cleaning and automatic word segmentation, we have collected about 340K sentences. Then, we select 10 thousand sentences with [5, 25] words for manual annotation following the active learning workflow of Jiang et al. (2018). The remaining sentences are used as unlabeled data. For web fictions, we follow the work on cross-domain word segmentation of Zhang et al. (2014), and adopt the popular novel named as “Zhuxian” (ZX, also known as “Jade dynasty”). Among their annotated 4,555 sentences, we select about 3,400 sentences with [5, 45] words for annotation. The remaining 32K sentences of ZX are used as unlabeled data in this work. Annotation guideline. After comparing several publicly available guidelines for dependency parsing including the universal dependencies (UD) (McDonald et al., 2013), we adopt the guideline released by Jiang et al. (2018) based on three considerations. First, their guideline contains 20 relations specifically designed to capture Chin"
P19-1229,P15-1117,0,0.0182641,"ł this Introduction Corresponding author The two domain-specific datasets, plus another one for product comment texts, are also used in the NLPCC-2019 shared task (http://hlt.suda.edu.cn/index. php/Nlpcc-2019-shared-task) on cross-domain Chinese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, w"
P19-1229,N18-1202,0,0.26759,"inese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, with the surge of web data (or user generated content), cross-domain parsing has become the major challenge for applying syntactic analysis in realistic NLP systems. To meet this challenge, the community has organized several shared tasks to a"
P19-1229,P11-1157,0,0.0874358,"Missing"
P19-1296,D17-1304,1,0.807793,"Missing"
P19-1296,I17-1002,1,0.860783,"Missing"
P19-1296,P18-1192,0,0.0281199,"mize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance. 1 Introduction Neural network based methods have been applied to several natural language processing tasks (Zhang et al., 2016; Li et al., 2018; Chen et al., 2018; Li et al., 2019; He et al., 2018). In neural machine translation (NMT), unlike conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (Bahdanau et al., 2015). It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target words. ∗ Mingming Yang was an internship research fellow at NICT when co"
P19-1296,P18-1164,0,0.250005,"conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (Bahdanau et al., 2015). It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target words. ∗ Mingming Yang was an internship research fellow at NICT when conducting this work. Based on this hypothesis, Kuang et al. (2018) proposed a direct bridging model, which directly connects source and target word embeddings seeking to minimize errors in the translation. Tu et al. (2017) incorporated a reconstructor module into NMT, which reconstructs the input source sentence from the hidden layer of the output target sentence to enhance source representation. However, in previous studies, the training objective function was usually based on word-level and lacked explicit sentencelevel relationships (Zhang and Zhao, 2019). Although Transformer model (Vaswani et al., 2017) has archived state-of-the-art performance of NMT,"
P19-1296,C18-1271,0,0.0150768,"ose a sentencelevel agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance. 1 Introduction Neural network based methods have been applied to several natural language processing tasks (Zhang et al., 2016; Li et al., 2018; Chen et al., 2018; Li et al., 2019; He et al., 2018). In neural machine translation (NMT), unlike conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (Bahdanau et al., 2015). It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target words. ∗ Mingming"
P19-1296,D15-1166,0,0.122472,"Missing"
P19-1296,P02-1040,0,0.103188,"Missing"
P19-1296,W18-6319,0,0.0337367,"Missing"
P19-1296,W16-0533,0,0.0307462,"Although Transformer model (Vaswani et al., 2017) has archived state-of-the-art performance of NMT, more attention is paid to the words-level relationship via self-attention networks. Sentence-level agreement method has been applied to many natural language processing tasks. Aliguliyev (2009) used sentence similarity measure technique for automatic text summarization. Liang et al. (2010) have shown that the sentence similarity algorithm based on VSM is beneficial to address the FAQ problem. Su et al. (2016) presented a sentence similarity method for spoken dialogue system to improve accuracy. Rei and Cummins (2016) proposed sentence similarity measures to improve the estimation of topical relevance. Wang et al. (2017b; 2018) used sentence similarity to select sentences with the similar domains. The above methods only considered monolingual sentence-level agreement. In human translation, a translator’s primary concern is to translate a sentence through its entire meaning rather than word-by-word meaning. Therefore, in early machine translation studies, such as example-based machine translation (Nagao, 1984; Nio et al., 2013), use the sentence similarity matching between the sentences to be translated and"
P19-1296,W16-2323,0,0.0605544,"Missing"
P19-1296,P16-1162,0,0.158734,"Missing"
P19-1296,P16-1131,0,0.017386,"this paper, we propose a sentencelevel agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance. 1 Introduction Neural network based methods have been applied to several natural language processing tasks (Zhang et al., 2016; Li et al., 2018; Chen et al., 2018; Li et al., 2019; He et al., 2018). In neural machine translation (NMT), unlike conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (Bahdanau et al., 2015). It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target w"
P19-1296,P16-1008,0,0.0212731,"nction in between: FFN(x) = max(0, xW1 + b1 )W2 + b2 , (2) where W1 and W2 are both linear transformation networks, b1 and b2 are both bias. We define Henc as the sentence representation of X via the self-attention layers in encoder, and Hdec as the sentence representation of words Y via embedding layers in decoder. The parameters of Transformer are trained to minimize the following objective function on a set of training examples {(X n , Y n )}N n=1 : Lmle = − 3 N Iy 1 XX n logP (yin |y<i , Henc , Hdec ). N n=1 i=1 (3) Agreement on Source and Target Sentence Some studies (Luong et al., 2015; Tu et al., 2016; Chen et al., 2017a,b; Kuang et al., 2018) showed that improving word alignment is beneficial to machine translation. Their idea is based on word-level agreement and make the embeddings of source words and corresponding target words similar. In this paper, we investigate the sentence-level relationship between the source and target sentences. We propose a sentence-level agreement method which can make the sentencelevel semantics of the source and target closer. The entire architecture of the proposed method is illustrated in Figure 1. 3.1 Sentence-Level Agreement First, we need to get the sen"
P19-1296,P17-2089,1,0.919928,"Missing"
P19-1296,C18-1269,0,0.0400623,"Missing"
P19-1296,D17-1155,1,0.822564,"ntion is paid to the words-level relationship via self-attention networks. Sentence-level agreement method has been applied to many natural language processing tasks. Aliguliyev (2009) used sentence similarity measure technique for automatic text summarization. Liang et al. (2010) have shown that the sentence similarity algorithm based on VSM is beneficial to address the FAQ problem. Su et al. (2016) presented a sentence similarity method for spoken dialogue system to improve accuracy. Rei and Cummins (2016) proposed sentence similarity measures to improve the estimation of topical relevance. Wang et al. (2017b; 2018) used sentence similarity to select sentences with the similar domains. The above methods only considered monolingual sentence-level agreement. In human translation, a translator’s primary concern is to translate a sentence through its entire meaning rather than word-by-word meaning. Therefore, in early machine translation studies, such as example-based machine translation (Nagao, 1984; Nio et al., 2013), use the sentence similarity matching between the sentences to be translated and the sentences in the 3076 Proceedings of the 57th Annual Meeting of the Association for Computational L"
P19-1298,P18-1192,1,0.778278,"ignore long-range order. To show the complementary of these two methods, we also placed all edges of lattice in a single sequence in a relative right order based on their first character and use normal positional encodings and our LSA method; we obtained a BLEU of 40.90 which is 0.13 higher than single LSA model. From this, we can see that long-range position information is indeed beneficial to our LSA model. 3 Related Work Neural network based methods have been applied to several natural language processing tasks (Li et al., 2018; Zhang et al., 2019; Chen et al., 2018, 2017; Li et al., 2019; He et al., 2018; Zhou and Zhao, 2019), especially to NMT (Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Wang et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2019). Our work is related to the source side representations for NMT. Generally, the NMT model uses the word as a basic unit for source sentences modeling. In order to obtain better source side representations and avoid OOV problems, recent research has modeled source sentences at character level (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2016), subword level (Sennrich et al., 2015; Kudo, 2018; Wu and Zhao"
P19-1298,P16-1039,1,0.824319,"Missing"
P19-1298,C08-1049,0,0.0377163,"these methods show better translation performance than the word level model. As models mentioned above only use 1-best segmentation as inputs, lattice which can pack many different segmentations in a compact form has been widely used in statistical machine translation (SMT) (Xu et al., 2005; Dyer et al., 2008) and RNN-based NMT (Su et al., 2017; Sperber et al., 2017). To enhance the representaions of the input, lattice has also been applied in many other NLP tasks such as named entity recognition (Zhang and Yang, 2018), Chinese word segmentation (Yang et al., 2019) and part-of-speech tagging (Jiang et al., 2008; Wang et al., 2013). 6 Conclusions In this paper, we have proposed two methods to incorporate lattice representations into Transformer. Experimental results in two datasets on word-level and subword-level respectively validate the effectiveness of the proposed approaches. Different from Veliˇckovi´c et al. (2017), our work also provides an attempt to encode a simple labeled graph into Transformer and can be used in any tasks which need Transformer encoder to learn sequence representation. All analysis experiments conducted on NIST dataset. 3094 References Jimmy Lei Ba, Jamie Ryan Kiros, and G"
P19-1298,P17-2096,1,0.729925,"354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT tenure-track researcher startup fund “Toward Intelligent Machine Translation”. NMT can be factorized in character (Costa-Jussa and Fonollosa, 2016), word (Sutskever et al., 2014), or subword (Sennrich et al., 2015) level. However, only using 1-best segmentation as inputs limits NMT encoders to express source sequences sufficiently and reliably. Many East Asian languages, including Chinese are written without explicit word boundary, so that their sentences need to be segmented into words firstly (Zhao et al., 2019; Cai et al., 2017; Cai and Zhao, 2016; Zhao et al., 2013; Zhao and Kit, 2011). By different segmentors, each sentence can be segmented into multiple forms as shown in Figure 1. Even for those alphabetical languages with clear word boundary like English, there is still an issue about selecting a proper subword vocabulary size, which determines the segmentation granularities for word representation. In order to handle this problem, Morishita et al. (2018) used hierarchical subword features to represent sequence with different subword granularities. Su et al. (2017) proposed the first word-lattice based recurrent"
P19-1298,D18-1461,0,0.019381,"Recurrent Units (GRUs) (Cho et al., 2014) to take in multiple sequence segmentation representations. Sperber et al. (2017) incorporated posterior scores to Tree-LSTM for building a lattice encoder in speech translation. All these existing methods serve for RNN-based NMT model, where lattices can be formulized as directed graphs and the inherent directed structure of RNN facilitates the construction of lattice. Meanwhile, the selfattention mechanism is good at learning the dependency between characters in parallel, which can partially compare and learn information from multiple segmentations (Cherry et al., 2018). Therefore, it is challenging to directly apply the lattice structure to Transformer. In this work, we explore an efficient way of integrating lattice into Transformer. Our method can not only process multiple sequences segmented in different ways to improve translation quality, but also maintain the characteristics of parallel computation in the Transformer. 2 2.1 Background its pre suc Conditions i&lt;j=p&lt;q p&lt;q=i&lt;j i≤p&lt;q≤j p≤i&lt;j≤q i &lt; p &lt; j &lt; q or p&lt;i&lt;q&lt;j i&lt;j&lt;p&lt;q p&lt;q&lt;i&lt;j Self-Attention Transformer employs H attention heads to perform self-attention over a sequence individually and finally appl"
P19-1298,P05-1066,0,0.255638,"Missing"
P19-1298,P16-2058,0,0.0264019,"Missing"
P19-1298,P17-4012,0,0.0323983,"operations (Sennrich et al., 2015) to get different segmented sentences for building subword lattices. 16K BPE merge operations are employed on the target side. We set batch size to 1024 tokens and accumulated gradient 16 times before a backpropagation. During training, we set all dropout to 0.3 and chose the Adam optimizer (Kingma and Ba, 2014) with β1 = 0.9, β2 = 0.98 and  = 10−9 for parameters tuning. During decoding, we used beam search algorithm and set the beam size to 20. All other configurations were the same with Vaswani et al. (2017). We implemented our model based on the OpenNMT (Klein et al., 2017) and trained and evaluated all models on a single NVIDIA GeForce GTX 1080 Ti GPU. 4.2 Overall Performance From Table 2, we see that our LPE and LSA models both outperform the Transformer baseline model of 0.58 and 0.42 BLEU respectively. When we combine LPE and LSA together, we get a gain of 0.91 BLEU points. Table 3 shows that our method also works well on the subword level. The base Transformer system has about 90M parameters and our LPE and LSA models introduce 0 and 6k parameters over it, respectively, which shows that our lattice approach improves Transformer with little parameter accumul"
P19-1298,P18-1007,0,0.0210495,", 2019; He et al., 2018; Zhou and Zhao, 2019), especially to NMT (Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Wang et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2019). Our work is related to the source side representations for NMT. Generally, the NMT model uses the word as a basic unit for source sentences modeling. In order to obtain better source side representations and avoid OOV problems, recent research has modeled source sentences at character level (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2016), subword level (Sennrich et al., 2015; Kudo, 2018; Wu and Zhao, 2018) and mixed character-word level (Luong and Manning, 2016). All these methods show better translation performance than the word level model. As models mentioned above only use 1-best segmentation as inputs, lattice which can pack many different segmentations in a compact form has been widely used in statistical machine translation (SMT) (Xu et al., 2005; Dyer et al., 2008) and RNN-based NMT (Su et al., 2017; Sperber et al., 2017). To enhance the representaions of the input, lattice has also been applied in many other NLP tasks such as named entity recognition (Zhang and Yang"
P19-1298,Q17-1026,0,0.0394117,"Missing"
P19-1298,C18-1271,1,0.624191,"es, including Chinese are written without explicit word boundary, so that their sentences need to be segmented into words firstly (Zhao et al., 2019; Cai et al., 2017; Cai and Zhao, 2016; Zhao et al., 2013; Zhao and Kit, 2011). By different segmentors, each sentence can be segmented into multiple forms as shown in Figure 1. Even for those alphabetical languages with clear word boundary like English, there is still an issue about selecting a proper subword vocabulary size, which determines the segmentation granularities for word representation. In order to handle this problem, Morishita et al. (2018) used hierarchical subword features to represent sequence with different subword granularities. Su et al. (2017) proposed the first word-lattice based recurrent neural network 3090 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3090–3097 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (RNN) encoders which extended Gated Recurrent Units (GRUs) (Cho et al., 2014) to take in multiple sequence segmentation representations. Sperber et al. (2017) incorporated posterior scores to Tree-LSTM for building a latti"
P19-1298,P16-1100,0,0.0307649,"T (Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Wang et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2019). Our work is related to the source side representations for NMT. Generally, the NMT model uses the word as a basic unit for source sentences modeling. In order to obtain better source side representations and avoid OOV problems, recent research has modeled source sentences at character level (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2016), subword level (Sennrich et al., 2015; Kudo, 2018; Wu and Zhao, 2018) and mixed character-word level (Luong and Manning, 2016). All these methods show better translation performance than the word level model. As models mentioned above only use 1-best segmentation as inputs, lattice which can pack many different segmentations in a compact form has been widely used in statistical machine translation (SMT) (Xu et al., 2005; Dyer et al., 2008) and RNN-based NMT (Su et al., 2017; Sperber et al., 2017). To enhance the representaions of the input, lattice has also been applied in many other NLP tasks such as named entity recognition (Zhang and Yang, 2018), Chinese word segmentation (Yang et al., 2019) and part-of-speech tag"
P19-1298,W18-6419,1,0.820368,":8 :fu-zong-cai (4)Lattice Figure 1: Incorporating three different segmentation for a lattice graph. The original sentence is “mao-yifa-zhan-ju-fu-zong-cai”. In Chinese it is “贸易发展局 副总裁”. In English it means “The vice president of Trade Development Council” Introduction Neural machine translation (NMT) has achieved great progress with the evolvement of model structures under an encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014). Recently, the self-attention based Transformer model has achieved state-of-theart performance on multiple language pairs (Vaswani et al., 2017; Marie et al., 2018). Both representations of source and target sentences in ∗ mao-yi Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100) and key projects of National Natural Science Foundation of China (No. U1836222 and No. 61733011). Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT tenure-track researcher startup fund “Toward Intelligent Machine Translation”. NMT can be factorized in character (Costa-Jussa and F"
P19-1298,C18-1269,0,0.105041,"Missing"
P19-1298,C18-1052,0,0.0773082,"ast Asian languages, including Chinese are written without explicit word boundary, so that their sentences need to be segmented into words firstly (Zhao et al., 2019; Cai et al., 2017; Cai and Zhao, 2016; Zhao et al., 2013; Zhao and Kit, 2011). By different segmentors, each sentence can be segmented into multiple forms as shown in Figure 1. Even for those alphabetical languages with clear word boundary like English, there is still an issue about selecting a proper subword vocabulary size, which determines the segmentation granularities for word representation. In order to handle this problem, Morishita et al. (2018) used hierarchical subword features to represent sequence with different subword granularities. Su et al. (2017) proposed the first word-lattice based recurrent neural network 3090 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3090–3097 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (RNN) encoders which extended Gated Recurrent Units (GRUs) (Cho et al., 2014) to take in multiple sequence segmentation representations. Sperber et al. (2017) incorporated posterior scores to Tree-LSTM for building a latti"
P19-1298,D17-1155,1,0.790059,"placed all edges of lattice in a single sequence in a relative right order based on their first character and use normal positional encodings and our LSA method; we obtained a BLEU of 40.90 which is 0.13 higher than single LSA model. From this, we can see that long-range position information is indeed beneficial to our LSA model. 3 Related Work Neural network based methods have been applied to several natural language processing tasks (Li et al., 2018; Zhang et al., 2019; Chen et al., 2018, 2017; Li et al., 2019; He et al., 2018; Zhou and Zhao, 2019), especially to NMT (Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Wang et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2019). Our work is related to the source side representations for NMT. Generally, the NMT model uses the word as a basic unit for source sentences modeling. In order to obtain better source side representations and avoid OOV problems, recent research has modeled source sentences at character level (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2016), subword level (Sennrich et al., 2015; Kudo, 2018; Wu and Zhao, 2018) and mixed character-word level (Luong and Manning, 2016). All these methods"
P19-1298,P02-1040,0,0.104083,". 4 Experiments 4.1 Setup We conducted experiments on the NIST ChineseEnglish (Zh-En) and IWSLT 2016 EnglishGerman (En-De) datasets. The Zh-En corpus consists of 1.25M sentence pairs and the En-De corpus consists of 191K sentence pairs. For ZhEn task, we chose the NIST 2005 dataset as the validation set and the NIST 2002, 2003, 2004, 2006, and 2008 datasets as test sets. For EnDe task, tst2012 was used as validation set and tst2013 and tst2014 were used as test sets. For both tasks, sentence pairs with either side longer than 50 were dropped. We used the case-sensitive 4-gram NIST BLEU score (Papineni et al., 2002) as the evaluation metric and sign-test (Collins et al., 2005) for statistical significance test. For Zh-En task, we followed Su et al. (2017) to use the toolkit2 to train segmenters on PKU, MSR (Emerson, 2005), and CTB corpora (Xue et al., 2005), then we generated word lattices with different segmented training data. Both source and target vocabularies are limited to 30K. For En-De task, we adopted 8K, 16K and 32K BPE merge operations (Sennrich et al., 2015) to get different segmented sentences for building subword lattices. 16K BPE merge operations are employed on the target side. We set bat"
P19-1298,P18-2048,1,0.895037,"Missing"
P19-1298,N18-2074,0,0.0490038,"Missing"
P19-1298,D17-1145,0,0.0654155,"ies for word representation. In order to handle this problem, Morishita et al. (2018) used hierarchical subword features to represent sequence with different subword granularities. Su et al. (2017) proposed the first word-lattice based recurrent neural network 3090 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3090–3097 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (RNN) encoders which extended Gated Recurrent Units (GRUs) (Cho et al., 2014) to take in multiple sequence segmentation representations. Sperber et al. (2017) incorporated posterior scores to Tree-LSTM for building a lattice encoder in speech translation. All these existing methods serve for RNN-based NMT model, where lattices can be formulized as directed graphs and the inherent directed structure of RNN facilitates the construction of lattice. Meanwhile, the selfattention mechanism is good at learning the dependency between characters in parallel, which can partially compare and learn information from multiple segmentations (Cherry et al., 2018). Therefore, it is challenging to directly apply the lattice structure to Transformer. In this work, we"
P19-1298,P13-2110,0,0.0287774,"etter translation performance than the word level model. As models mentioned above only use 1-best segmentation as inputs, lattice which can pack many different segmentations in a compact form has been widely used in statistical machine translation (SMT) (Xu et al., 2005; Dyer et al., 2008) and RNN-based NMT (Su et al., 2017; Sperber et al., 2017). To enhance the representaions of the input, lattice has also been applied in many other NLP tasks such as named entity recognition (Zhang and Yang, 2018), Chinese word segmentation (Yang et al., 2019) and part-of-speech tagging (Jiang et al., 2008; Wang et al., 2013). 6 Conclusions In this paper, we have proposed two methods to incorporate lattice representations into Transformer. Experimental results in two datasets on word-level and subword-level respectively validate the effectiveness of the proposed approaches. Different from Veliˇckovi´c et al. (2017), our work also provides an attempt to encode a simple labeled graph into Transformer and can be used in any tasks which need Transformer encoder to learn sequence representation. All analysis experiments conducted on NIST dataset. 3094 References Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 20"
P19-1298,2005.iwslt-1.18,0,0.0473858,"presentations and avoid OOV problems, recent research has modeled source sentences at character level (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2016), subword level (Sennrich et al., 2015; Kudo, 2018; Wu and Zhao, 2018) and mixed character-word level (Luong and Manning, 2016). All these methods show better translation performance than the word level model. As models mentioned above only use 1-best segmentation as inputs, lattice which can pack many different segmentations in a compact form has been widely used in statistical machine translation (SMT) (Xu et al., 2005; Dyer et al., 2008) and RNN-based NMT (Su et al., 2017; Sperber et al., 2017). To enhance the representaions of the input, lattice has also been applied in many other NLP tasks such as named entity recognition (Zhang and Yang, 2018), Chinese word segmentation (Yang et al., 2019) and part-of-speech tagging (Jiang et al., 2008; Wang et al., 2013). 6 Conclusions In this paper, we have proposed two methods to incorporate lattice representations into Transformer. Experimental results in two datasets on word-level and subword-level respectively validate the effectiveness of the proposed approaches."
P19-1298,N19-1278,0,0.040054,"Missing"
P19-1298,C16-1288,0,0.0234602,"Missing"
P19-1298,P17-2089,1,0.744874,"Missing"
P19-1298,P18-1144,0,0.0448059,"15; Kudo, 2018; Wu and Zhao, 2018) and mixed character-word level (Luong and Manning, 2016). All these methods show better translation performance than the word level model. As models mentioned above only use 1-best segmentation as inputs, lattice which can pack many different segmentations in a compact form has been widely used in statistical machine translation (SMT) (Xu et al., 2005; Dyer et al., 2008) and RNN-based NMT (Su et al., 2017; Sperber et al., 2017). To enhance the representaions of the input, lattice has also been applied in many other NLP tasks such as named entity recognition (Zhang and Yang, 2018), Chinese word segmentation (Yang et al., 2019) and part-of-speech tagging (Jiang et al., 2008; Wang et al., 2013). 6 Conclusions In this paper, we have proposed two methods to incorporate lattice representations into Transformer. Experimental results in two datasets on word-level and subword-level respectively validate the effectiveness of the proposed approaches. Different from Veliˇckovi´c et al. (2017), our work also provides an attempt to encode a simple labeled graph into Transformer and can be used in any tasks which need Transformer encoder to learn sequence representation. All analysi"
P19-1298,D18-1511,1,0.673041,"e in a relative right order based on their first character and use normal positional encodings and our LSA method; we obtained a BLEU of 40.90 which is 0.13 higher than single LSA model. From this, we can see that long-range position information is indeed beneficial to our LSA model. 3 Related Work Neural network based methods have been applied to several natural language processing tasks (Li et al., 2018; Zhang et al., 2019; Chen et al., 2018, 2017; Li et al., 2019; He et al., 2018; Zhou and Zhao, 2019), especially to NMT (Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Wang et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2019). Our work is related to the source side representations for NMT. Generally, the NMT model uses the word as a basic unit for source sentences modeling. In order to obtain better source side representations and avoid OOV problems, recent research has modeled source sentences at character level (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2016), subword level (Sennrich et al., 2015; Kudo, 2018; Wu and Zhao, 2018) and mixed character-word level (Luong and Manning, 2016). All these methods show better translation performance than the wo"
P19-1298,P19-1230,1,0.678425,"order. To show the complementary of these two methods, we also placed all edges of lattice in a single sequence in a relative right order based on their first character and use normal positional encodings and our LSA method; we obtained a BLEU of 40.90 which is 0.13 higher than single LSA model. From this, we can see that long-range position information is indeed beneficial to our LSA model. 3 Related Work Neural network based methods have been applied to several natural language processing tasks (Li et al., 2018; Zhang et al., 2019; Chen et al., 2018, 2017; Li et al., 2019; He et al., 2018; Zhou and Zhao, 2019), especially to NMT (Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Wang et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2019). Our work is related to the source side representations for NMT. Generally, the NMT model uses the word as a basic unit for source sentences modeling. In order to obtain better source side representations and avoid OOV problems, recent research has modeled source sentences at character level (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2016), subword level (Sennrich et al., 2015; Kudo, 2018; Wu and Zhao, 2018) and mixed char"
P19-1298,P08-1115,0,\N,Missing
P19-1298,I05-3017,0,\N,Missing
P19-1298,D17-1304,1,\N,Missing
P19-1298,P16-1162,0,\N,Missing
Q14-1013,D09-1062,0,0.0259905,"tically motivated rules and assume the existence of independent keywords in sentences which indicate comparative relations. Therefore, these methods fall short of extracting comparative relations based on domain dependent information. Both Johansson and Moschitti (2011) and Wu et al. (2011) formulate fine-grained sentiment analysis as a learning problem with structured outputs. However, they focus only on polarity classification of expressions and require annotation of sentimentbearing expressions for training as well. While ILP has been previously applied for inference in sentiment analysis (Choi and Cardie, 2009; Somasundaran and Wiebe, 2009; Wu et al., 2011), our task requires a complete ILP reformulation due to 1) the absence of annotated sentiment expressions and 2) the constraints imposed by the joint extraction of both sentiment polarity and comparative relations. 3 System Overview This section gives an overview of the whole system for extracting sentiment-oriented relation instances. Prior to presenting the system architecture, we introduce the essential concepts and the definitions of two kinds of directed hypergraphs as the representation of correlated relation instances extracted from senten"
Q14-1013,P10-2050,0,0.0187524,", showing that S ENTI -LSSVM model can effectively learn from a training corpus without explicitly annotated subjective expressions and that its performance significantly outperforms state-of-the-art systems. 2 Related Work There are ample works on analyzing sentiment polarities and entity comparisons, but the majority of them studied the two tasks in isolation. Most prior approaches for fine-grained sentiment analysis focus on polarity classification. Supervised approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al."
Q14-1013,W06-1651,0,0.0376951,"VM based model can learn from training corpora that do not contain explicit annotations of sentiment-bearing expressions, and it can simultaneously recognize instances of both binary (polarity) and ternary (comparative) relations with regard to entity mentions of interest. The empirical evaluation shows that our approach significantly outperforms stateof-the-art systems across domains (cameras and movies) and across genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community. 1 Introduction Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text. Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level. Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). In practice, however, differTo the best of our knowledge, the only existing s"
Q14-1013,C08-1031,0,0.179373,"aluable resource for the community. 1 Introduction Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text. Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level. Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). In practice, however, differTo the best of our knowledge, the only existing system capable of extracting both comparisons and sentiment polarities is a rule-based system proposed by Ding et al. (2009). We argue that it is better to tackle the task by using a unified model with structured outputs. It allows us to consider a set of correlated relation instances jointly and characterize their interaction through a set of soft and hard constraints. For example, we can encode constraints to discourage an attribute to participate in a polarity relation and a comparative relation at the same time."
Q14-1013,P11-2018,0,0.108016,"cameras and movies) and across genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community. 1 Introduction Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text. Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level. Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). In practice, however, differTo the best of our knowledge, the only existing system capable of extracting both comparisons and sentiment polarities is a rule-based system proposed by Ding et al. (2009). We argue that it is better to tackle the task by using a unified model with structured outputs. It allows us to consider a set of correlated relation instances jointly and characterize their interaction through a set of soft and hard constraints. For example, we can encode c"
Q14-1013,W06-0301,0,0.0142575,"sed approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). Approaches rely on linguistically motivated rules and assume the existence of independent keywords in sentences whic"
Q14-1013,P03-1054,0,0.0042962,"X Structural Inference In order to find the best eMRG for a given sentence with a well trained model, we need to determine the most likely relation type for each relation candidate and support the corresponding assertions with proper textual evidences. We formulate this task as an Integer Linear Programming (ILP). Instead of considering all constituents of a sentence, we empirically select a subset as textual evidences for each relation candidate. 6.1 Textual Evidence Candidates Selection Textual evidences are selected based on the constituent trees of sentences parsed by the Stanford parser (Klein and Manning, 2003). For each mention in a sentence, we first locate a constituent in the tree with the maximal overlap by Jaccard similarity. Starting from this constituent, we consider two types of candidates: type I candidates are constituents at the highest level which contain neither any word of another mention nor any contrast conjunctions such as “but”; type II candidates are constituents at the highest level which cover exactly two mentions of an edge and do not overlap with any other mentions. For a binary edge connecting an entity mention and an attribute mention, we consider a type I candidate startin"
Q14-1013,P09-1039,0,0.1849,"xtracts sets 158 S ENTI -LSSVM Model The task of sentiment-oriented relation extraction is to determine the most likely sSoR in a sentence. Since sSoRs are derived from the corresponding MRG s as described in Section 3, the task is reduced to find the most likely MRG for each sentence. Since an MRG is created by assigning relation types to a subset of all relation candidates, which are possible tuples of mentions with unknown relation types, the number of MRGs can be extremely high. To tackle the task, one solution is to employ an edge-factored linear model in the framework of structural SVM (Martins et al., 2009; Tsochantaridis et al., 2004). The model suggests that a bag of features should be specified for each relation candidate, and then the model predicts the most likely candidate sets along with their relation types to form the optimal MRGs. As we observed, for a relation candidate, the most informative features are the words near its entity mentions in the original text. However, if we represent a candidate by all these words, it is very likely that the instances of different relation types share overly similar features, because a mention is often involved in more than one relation candidate, a"
Q14-1013,P07-1055,0,0.0322612,"f them studied the two tasks in isolation. Most prior approaches for fine-grained sentiment analysis focus on polarity classification. Supervised approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathi"
Q14-1013,H05-1043,0,0.0415718,"ation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). Approaches rely on linguistically motivated rules and assume the existence of independent keywords in sentences which indicate comparative relations. Therefore, these methods fall shor"
Q14-1013,C10-1103,1,0.854867,"ng to entity A. Co-occurrence: We have mentioned the cooccurrence feature in Equation 2, indicated by Φc (a, a0 ). It captures the co-occurrence of two labeled edges incident to the same entity mention. Note that the co-occurrence feature function is considered only if there is a contrast conjunction such as “but” between the non-shared entity mentions incident to the two labeled edges. Senti-predictors: Following the idea of (Qu et al., 2012), we encode the prediction results from the rule-based phrase-level multi-relation predictor (Ding et al., 2009) and from the bag-of-opinions predictor (Qu et al., 2010) as features based on the textual evidence. The output of the first predictor is an integer value, while the output of the second predictor is a sentiment relation, such as “positive”, “negative”, “better” or “worse”. We map the relational outputs into integer values and then encode the outputs from both predictors as senti-predictor features. Others: The commonly used part-of-speech tags are also included as features. Moreover, for an edge candidate, a set of binary features are used to denote the types of the edge and its entity mentions. For instance, a binary feature indicates whether an e"
Q14-1013,D12-1014,1,0.913576,"son and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). Approaches rely on linguistically motivated rules and assume the existence of independent keywords in sentences which indicate comparative relations. Therefore, these methods fall short of extracting comparative relations based on domain dependent information. Both Johansson and"
Q14-1013,D12-1110,0,0.0421273,"approaches for fine-grained sentiment analysis focus on polarity classification. Supervised approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). Approaches rely on linguistic"
Q14-1013,P09-1026,0,0.0139075,"and assume the existence of independent keywords in sentences which indicate comparative relations. Therefore, these methods fall short of extracting comparative relations based on domain dependent information. Both Johansson and Moschitti (2011) and Wu et al. (2011) formulate fine-grained sentiment analysis as a learning problem with structured outputs. However, they focus only on polarity classification of expressions and require annotation of sentimentbearing expressions for training as well. While ILP has been previously applied for inference in sentiment analysis (Choi and Cardie, 2009; Somasundaran and Wiebe, 2009; Wu et al., 2011), our task requires a complete ILP reformulation due to 1) the absence of annotated sentiment expressions and 2) the constraints imposed by the joint extraction of both sentiment polarity and comparative relations. 3 System Overview This section gives an overview of the whole system for extracting sentiment-oriented relation instances. Prior to presenting the system architecture, we introduce the essential concepts and the definitions of two kinds of directed hypergraphs as the representation of correlated relation instances extracted from sentences. 3.1 Concepts and Definiti"
Q14-1013,J11-2001,0,0.0385546,"Missing"
Q14-1013,P10-1059,0,0.0197331,"anon 7D, textitprice). However, constructing a fully annotated training corpus for this task is labor-intensive and requires strong linguistic background. We minimize this overhead by applying a simplified annotation scheme, in which annotators mark mentions of entities and attributes, disambiguate the entities, and label instances of relations for each sentence. Based on the new scheme, we have created a small Sentiment Relation Graph (SRG) corpus for the domains of cameras and movies, which significantly differs from the corpora used in prior work (Wei and Gulla, 2010; Kessler et al., 2010; Toprak et al., 2010; Wiebe et al., 2005; Hu and Liu, 2004) in the following ways: i) both sentiment polarities and comparative relations are annotated; ii) all mentioned entities are disambiguated; and iii) no subjective expressions are annotated, unless they are part of entity mentions. The new annotation scheme raises a new challenge for learning algorithms in that they need to automatically find textual evidences for each annotated relation during training. For example, with the sentence “I like the Rebel a little better, but that is another price jump”, simply assigning a sentimentbearing expression to the n"
Q14-1013,P10-1042,0,0.129447,"l Linguistics. and preferred(Nikon D7000, Canon 7D, textitprice). However, constructing a fully annotated training corpus for this task is labor-intensive and requires strong linguistic background. We minimize this overhead by applying a simplified annotation scheme, in which annotators mark mentions of entities and attributes, disambiguate the entities, and label instances of relations for each sentence. Based on the new scheme, we have created a small Sentiment Relation Graph (SRG) corpus for the domains of cameras and movies, which significantly differs from the corpora used in prior work (Wei and Gulla, 2010; Kessler et al., 2010; Toprak et al., 2010; Wiebe et al., 2005; Hu and Liu, 2004) in the following ways: i) both sentiment polarities and comparative relations are annotated; ii) all mentioned entities are disambiguated; and iii) no subjective expressions are annotated, unless they are part of entity mentions. The new annotation scheme raises a new challenge for learning algorithms in that they need to automatically find textual evidences for each annotated relation during training. For example, with the sentence “I like the Rebel a little better, but that is another price jump”, simply assig"
Q14-1013,H05-1044,0,0.180832,"genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community. 1 Introduction Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text. Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level. Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). In practice, however, differTo the best of our knowledge, the only existing system capable of extracting both comparisons and sentiment polarities is a rule-based system proposed by Ding et al. (2009). We argue that it is better to tackle the task by using a unified model with structured outputs. It allows us to consider a set of correlated relation instances jointly and characterize their interaction through a set of soft and hard constraints. For example, we can encode constraints to discoura"
Q14-1013,D11-1123,0,0.0218512,"hod requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). Approaches rely on linguistically motivated rules and assume the existence of independent keywords in sentences which indicate comparative relations. Therefore, these methods fall short of extracting comparative relations based on domain dependent information. Both Johansson and Moschitti (2011) and Wu et al. (2011) formulate fine-grained sentiment analysis as a learning problem with structured outputs. However, they focus only on polarity classification of expressions and require annotation of sentimentbearing expressions for training as well. While ILP has been previously applied for inference in sentiment analysis (Choi and Cardie, 2009; Somasundaran and Wiebe, 2009; Wu et al., 2011), our task requires a complete ILP reformulation due to 1) the absence of annotated sentiment expressions and 2) the constraints imposed by the joint extraction of both sentiment polarity and comparative relations. 3 Syste"
Q14-1013,D11-1016,0,0.0189865,"rn from a training corpus without explicitly annotated subjective expressions and that its performance significantly outperforms state-of-the-art systems. 2 Related Work There are ample works on analyzing sentiment polarities and entity comparisons, but the majority of them studied the two tasks in isolation. Most prior approaches for fine-grained sentiment analysis focus on polarity classification. Supervised approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no"
Q14-1013,W11-0323,0,0.0179712,"utperforms stateof-the-art systems across domains (cameras and movies) and across genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community. 1 Introduction Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text. Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level. Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). In practice, however, differTo the best of our knowledge, the only existing system capable of extracting both comparisons and sentiment polarities is a rule-based system proposed by Ding et al. (2009). We argue that it is better to tackle the task by using a unified model with structured outputs. It allows us to consider a set of correlated relation instances jointly and characterize their interaction through a se"
Q14-1013,H05-2017,0,\N,Missing
regneri-etal-2014-aligning,I05-5002,0,\N,Missing
regneri-etal-2014-aligning,wang-sporleder-2010-constructing,1,\N,Missing
regneri-etal-2014-aligning,shima-mitamura-2012-diversifiable,0,\N,Missing
regneri-etal-2014-aligning,W11-1208,1,\N,Missing
regneri-etal-2014-aligning,W04-3219,0,\N,Missing
regneri-etal-2014-aligning,E12-1002,0,\N,Missing
regneri-etal-2014-aligning,D12-1058,0,\N,Missing
regneri-etal-2014-aligning,W03-1609,0,\N,Missing
regneri-etal-2014-aligning,W08-2121,0,\N,Missing
regneri-etal-2014-aligning,C04-1051,0,\N,Missing
regneri-etal-2014-aligning,W09-2503,0,\N,Missing
regneri-etal-2014-aligning,W08-2126,1,\N,Missing
regneri-etal-2014-aligning,D09-1040,0,\N,Missing
regneri-etal-2014-aligning,P03-1054,0,\N,Missing
regneri-etal-2014-aligning,P01-1008,0,\N,Missing
regneri-etal-2014-aligning,D08-1021,0,\N,Missing
regneri-etal-2014-aligning,Q13-1003,1,\N,Missing
regneri-etal-2014-aligning,P99-1071,0,\N,Missing
regneri-etal-2014-aligning,C10-1149,0,\N,Missing
regneri-etal-2014-aligning,H05-1066,0,\N,Missing
regneri-etal-2014-aligning,N03-1003,0,\N,Missing
regneri-etal-2014-aligning,D11-1108,0,\N,Missing
regneri-etal-2014-aligning,P08-1089,0,\N,Missing
regneri-etal-2014-aligning,P05-1074,0,\N,Missing
regneri-etal-2014-aligning,W10-4217,0,\N,Missing
regneri-etal-2014-aligning,J03-1002,0,\N,Missing
regneri-etal-2014-aligning,P11-1020,0,\N,Missing
regneri-etal-2014-aligning,P10-1100,1,\N,Missing
regneri-etal-2014-aligning,P08-1116,0,\N,Missing
regneri-etal-2014-aligning,2005.mtsummit-papers.11,0,\N,Missing
regneri-etal-2014-aligning,E12-1073,0,\N,Missing
regneri-etal-2014-aligning,D12-1066,0,\N,Missing
regneri-etal-2014-aligning,D12-1084,1,\N,Missing
regneri-etal-2014-aligning,W04-3206,0,\N,Missing
S10-1061,H05-1066,0,0.038749,"he meaning of the input text. In particular, after tokenization and POS tagging, we did dependency parsing and semantic role labeling. In addition, HPSG parsing is a filter for ungrammatical hypotheses. Tokenization and POS Tagging We use the Penn Treebank style tokenization throughout the various processing stages. TnT, an HMM-based POS tagger trained with Wall Street Journal sections of the PTB, was used to automatically predict the part-of-speech of each token in the texts and hypotheses. Dependency Parsing For obtaining the syntactic dependencies, we use two dependency parsers, MSTParser (McDonald et al., 2005) and MaltParser (Nivre et al., 2007). MSTParser is a graphbased dependency parser where the best parse tree is acquired by searching for a spanning tree 272 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 272–275, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Preprocessing Dependency Path Extraction T H HPSG Parsing Dependency Parsing Semantic Role Labeling Path Extraction Feature-based Classification Dependency Triple Extraction Feature Extraction SVM-based Classification Yes/No No Figure 1: Workflow of the System 2.2"
S10-1061,S10-1009,0,0.0434839,"Missing"
S10-1061,W08-2126,1,\N,Missing
S10-1061,W07-1401,0,\N,Missing
U16-1011,W03-0407,0,0.139725,"Missing"
U16-1011,W14-4807,0,0.156566,"ub-grams by analysing different combinations of words. The other is a Long Short-Term Memory (LSTM) network which learns the representation of a term by recursively composing the embeddings of an input word and the composed value from its precedent, hypothesising that the meaning of a term can be learnt from the sequential combination of each constituent word. Each network connects to a logistic regression layer to perform classifications. Our model is evaluated on two benchmark domain-specific corpora, namely GENIA (biology domain) (Kim et al., 2003), and ACL RDTEC (computer science domain) (Handschuh and QasemiZadeh, 2014). The evaluation shows that our model outperforms the C-value algorithm (Frantzi et al., 2000) that is often treated as the benchmark in term extraction. We also trained two classifiers using the standard supervised learning approach, and demonstrate that cotraining deep neural networks is an effective approach to reduce the usage of labelled data while maintaining a competitive performance. This paper is organised as follows. In Section 2, we briefly review the related work. Section 3 introduces our proposed model, and in Section 4 we describe our evaluation datasets and discuss the experimen"
U16-1011,W13-3214,0,0.014027,"et al. (2014) use Gated Recurrent Unit (GRU) network to encode and decode the semantic compositionality of sentences for machine translation. Chung et al. (2015) propose an even deeper architecture named Gated Feedback Recurrent network that stacks multiple recurrent layers for character-level language modelling. Other network architectures for learning semantic compositionality include recursive networks (Socher et al., 2010; Socher et al., 2012), which require using POS tagging texts to produce syntactical tree structures as a prior. Hybrid networks, such as recurrent-convolutional network (Kalchbrenner and Blunsom, 2013; Lai et al., 2015), are designed for capturing documentlevel semantics. 3 Proposed Model The model consists of two classifiers, as shown in Figure 1. The left classifier is a CNN network, 105 and the right one is a LSTM network. Both networks take pre-trained word embedding vectors as inputs to learn the representations of terms independently. The output layer is a logistic regression layer for both networks. Two neural networks are trained using the Co-training algorithm. The Co-training algorithm requires two separate views of the data, which traditionally are two sets of manually selected"
U16-1011,D14-1181,0,0.00738957,"Missing"
U16-1011,D16-1076,0,0.0254962,"Missing"
U16-1011,W04-2405,0,0.0548014,"hell classify web pages based on words appearing in the content of a web page, and words in hyperlinks pointing to the web page. Co-training starts with training each classifier on a small labelled dataset, then each classifier is used to predict a subset of the unlabelled data. The most confident predictions are subsequently added to the training set to re-train each classifier. This process is iterated a fixed number of times. Co-training algorithms have been applied to many NLP tasks where labelled data are in scarce, including statistical parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and coreference resolution (Ng and Cardie, 2003), which demonstrate that it generally improves the performance without requiring additional labelled data. Our second objective is to eliminate the efRui Wang, Wei Liu and Chris McDonald. 2016. Featureless Domain-Specific Term Extraction with Minimal Labelled Data. In Proceedings of Australasian Language Technology Association Workshop, pages 103−112. fort of feature engineering by using deep learning models. Applying deep neural networks directly to NLP tasks without feature engineering is also described as NLP from scratch (Collobert et al.,"
U16-1011,N03-1023,0,0.0475034,"ing in the content of a web page, and words in hyperlinks pointing to the web page. Co-training starts with training each classifier on a small labelled dataset, then each classifier is used to predict a subset of the unlabelled data. The most confident predictions are subsequently added to the training set to re-train each classifier. This process is iterated a fixed number of times. Co-training algorithms have been applied to many NLP tasks where labelled data are in scarce, including statistical parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and coreference resolution (Ng and Cardie, 2003), which demonstrate that it generally improves the performance without requiring additional labelled data. Our second objective is to eliminate the efRui Wang, Wei Liu and Chris McDonald. 2016. Featureless Domain-Specific Term Extraction with Minimal Labelled Data. In Proceedings of Australasian Language Technology Association Workshop, pages 103−112. fort of feature engineering by using deep learning models. Applying deep neural networks directly to NLP tasks without feature engineering is also described as NLP from scratch (Collobert et al., 2011). As a result of such training, words are rep"
U16-1011,N01-1023,0,0.0603887,"n a classifier. For example, Blum and Mitchell classify web pages based on words appearing in the content of a web page, and words in hyperlinks pointing to the web page. Co-training starts with training each classifier on a small labelled dataset, then each classifier is used to predict a subset of the unlabelled data. The most confident predictions are subsequently added to the training set to re-train each classifier. This process is iterated a fixed number of times. Co-training algorithms have been applied to many NLP tasks where labelled data are in scarce, including statistical parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and coreference resolution (Ng and Cardie, 2003), which demonstrate that it generally improves the performance without requiring additional labelled data. Our second objective is to eliminate the efRui Wang, Wei Liu and Chris McDonald. 2016. Featureless Domain-Specific Term Extraction with Minimal Labelled Data. In Proceedings of Australasian Language Technology Association Workshop, pages 103−112. fort of feature engineering by using deep learning models. Applying deep neural networks directly to NLP tasks without feature engineering is also descr"
U16-1011,D12-1110,0,0.0477761,"ks modelling natural languages and their long-term dependencies was attempted by Mikolov et al. (2010). More recently, Sutskever et al. (2014) use LSTM network, and Cho et al. (2014) use Gated Recurrent Unit (GRU) network to encode and decode the semantic compositionality of sentences for machine translation. Chung et al. (2015) propose an even deeper architecture named Gated Feedback Recurrent network that stacks multiple recurrent layers for character-level language modelling. Other network architectures for learning semantic compositionality include recursive networks (Socher et al., 2010; Socher et al., 2012), which require using POS tagging texts to produce syntactical tree structures as a prior. Hybrid networks, such as recurrent-convolutional network (Kalchbrenner and Blunsom, 2013; Lai et al., 2015), are designed for capturing documentlevel semantics. 3 Proposed Model The model consists of two classifiers, as shown in Figure 1. The left classifier is a CNN network, 105 and the right one is a LSTM network. Both networks take pre-trained word embedding vectors as inputs to learn the representations of terms independently. The output layer is a logistic regression layer for both networks. Two neu"
U16-1011,Y10-1036,0,0.194543,"Section 5 summarises our study with an outlook to the future work. 104 2 2.1 Related Work Supervised Term Extraction Supervised machine learning approaches for domain-specific term extraction start with candidate identification, usually by employing a phrase chunker based on pre-identified part-of-speech (POS) patterns, then uses manually selected features to train a classifier. The feature may include linguistic, statistical, and semantic features (Foo and Merkel, 2010; Nazar and Cabr´e, 2012; da Silva Conrado et al., 2013). The work closely related to ours is Fault-Tolerant Learning (FTL) (Yang et al., 2010) inspired by Transfer Learning (Ando and Zhang, 2005) and Cotraining (Blum and Mitchell, 1998). FTL builds two support vector machine (SVM) classifiers using manually selected features, whereas our model uses deep neural networks taking pre-trained word embeddings as inputs, without using any manually selected feature. 2.2 Learning Representations of Words and Semantic Compositionality Word embeddings are proposed to overcome the curse of dimensionality problem by Bengio et al. (2006), who developed a probabilistic neural language model using a feed-forward multi-layer neural network, which de"
W07-1406,W05-1203,0,0.0906859,"Missing"
W07-1406,P03-1054,0,0.00220743,"Missing"
W07-1406,P06-1051,0,0.179252,"Missing"
W08-2126,C04-1186,0,0.0550371,"ypes (POSes marked as predicates for at least 50 times in the training set). This helps to significantly improve the system efficiency in both training and prediction time without sacrificing prediction accuracy. It should be noted that the prediction of nominal predicates are generally much more difficult (based on CoNLL 2008 shared task annotation). The PI model achieved 96.32 F-score on WSJ with verbal predicates, but only 84.74 on nominal ones. Argument Identification After PI, the arguments to the predicted predicates are identified with the AI component. Similar to the approach taken in Hacioglu (2004), we use a statistical classifier to select from a set of candidate nodes in a dependency tree. However, instead of selecting from a set of neighboring nodes from the predicate word 2 , we define the concept of argument path as a chain of dependency relations from the predicate to the argument in the dependency tree. For instance, an argument path [ |] indicates that if the predicate is syntactically depending as  on a node which has a  child, then the  node 2 Hacioglu (2004) defines a tree-structured family of a predicate as a measure of locality. It is a set of dependency rela"
W08-2126,H05-1066,0,0.0287923,"predictions. In particular, the second part can be further divided into four stages: predicate identification (PI), argument identification (AI), argument classification (AC), and predicate classification (PC). Maximum entropy-based machine learning techniques are used in both components which we will see in detail in the following sections. 3 • Root attachments: the number of tokens attached to the ROOT node by the parser in one sentence Syntactic Dependency Parsing For obtaining syntactic dependencies, we have combined the results of two state-of-the-art dependency parsers: the MST parser (McDonald et al., 2005) and the MaltParser (Nivre et al., 2007). The MST parser formalizes dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. A major advantage of their framework is the ability to naturally and efficiently model both projective and non-projective parses. To learn these structures they used online largemargin learning that empirically provides state-ofthe-art performance. The MaltParser is a transition-based incremental dependency parser, which is language-independent and data-driven. It contains a deterministic algorithm, which can be viewed as a variant of the bas"
W08-2126,W08-2121,0,0.168141,"Missing"
W08-2126,U05-1006,1,0.88562,"Missing"
W08-2126,W07-1217,1,0.85909,"Missing"
W09-1204,adolphs-etal-2008-fine,1,0.829188,"e, rejects the conventional assumptions underlying the PTB (and derived tools). It opts for an analysis of punctuation akin to affixation (rather than as standalone tokens), does not break up contracted negated auxiliaries, and splits hyphenated words like illadvised into two tokens (the hyphen being part of the first component). Thus, a string like Don’t you! in the CoNLL data is tokenized as the four-element sequence hdo, n’t, you, !i,2 whereas the ERG analysis has only two leaf nodes: hdon’t, you!i. Fortunately, the DELPH-IN toolchain recently incorporated a mechanism called chart mapping (Adolphs et al., 2008), which allows one to map flexibly from ‘external’ input to grammar-internal assumptions, while keeping track of external token identities and their contributions to the final analysis. The February 2009 release of the ERG already had this machinery in place (with the goal of supporting extant, PTB-trained PoS taggers in pre-processing input to the deep parser), and we found that only a tiny number of additional chart mapping rules was required to ‘fix up’ CoNLL-specific deviations from the PTB tradition. With the help of the original developers, we created new chart mapping configurations for"
W09-1204,burchardt-etal-2006-salsa,0,0.127541,"Missing"
W09-1204,W09-1201,1,0.865296,"Missing"
W09-1204,kawahara-etal-2002-construction,0,0.0194058,"Missing"
W09-1204,W02-2018,0,0.00915684,"mprovement observed after using the HPSG features. Therefore, we did not include it in the final submission. 5 Semantic Role Labeling The semantic role labeling component used in the submitted system is similar to the one described by Zhang et al. (2008). Since predicates are indicated in the data, the predicate identification module is removed from this year’s system. Argument identification, argument classification and predicate classification are the three sub-components in the pipeline. All of them are MaxEnt-based classifiers. For parameter estimation, we use the open source TADM system (Malouf, 2002). The active features used in various steps of SRL are fine tuned separately for different languages using development datasets. The significance of feature types varies across languages and datasets. SYN SRL Closed ood Closed ood Open ood ca 82.67 67.34 - zh 73.63 73.20 - cs 75.58 71.29 78.28 77.78 - en 87.90 81.50 77.85 67.07 78.13 (↑0.28) 68.11 (↑1.04) de 84.57 75.06 62.95 54.87 64.31 (↑1.36) 58.42 (↑3.55) ja 91.47 64.71 65.95 (↑1.24) - es 82.69 67.81 68.24 (↑0.43) - Table 2: Summary of System Performance on Multiple Languages In the open challenge, two groups of extra features from HPSG pa"
W09-1204,J05-1004,0,0.0711747,"Missing"
W09-1204,W02-1210,0,0.017282,"atively detailed, hand-coded linguistic knowledge— including lexical argument structure and the linking of syntactic functions to thematic arguments—and are intended as general-purpose resources, applicable to both parsing and generation. Semantics in DELPH-IN is cast in the Minimal Recursion Semantics framework (MRS; Copestake, Flickinger, Pollard, & Sag, 2005), essentially predicate – argument structures with provision for underspecified scopal relations. For the 2009 ‘open’ task, we used the DELPH-IN grammars for English (ERG; Flickinger, 2000), German (GG; Crysmann, 2005), Japanese (JaCY; Siegel & Bender, 2002), and Spanish (SRG; Marimon, Bel, & Seghezzi, 2007). The grammars vary in their stage of development: the ERG comprises some 15 years of continuous development, whereas work on the SRG only started about five years ago, with GG and JaCY ranging somewhere inbetween. 3.1 Overall Setup We applied the DELPH-IN grammars to the CoNLL data using the PET parser (Callmeier, 2002) running 1 See http://www.delph-in.net for background. 32 it through the [incr tsdb()] environment (Oepen & Carroll, 2000), for parallelization and distribution. Also, [incr tsdb()] provides facilities for (re-)training the Max"
W09-1204,W08-2121,0,0.100771,"Missing"
W09-1204,taule-etal-2008-ancora,0,0.0657859,"Missing"
W09-1204,W07-2207,1,0.835309,"with the TADM software, using tenfold cross-validation and exact match ranking accuracy (against the binarized training distribution) to optimize estimation hyper-parameters 3.3 Deep Parsing Features HPSG parsing coverage and average cpu time per input for the four languages with DELPH-IN grammars are summarized in Table 1. The PoS-based unknown word mechanism was active for all grammars but no other robustness measures (which tend to lower the quality of results) were used, i.e. only complete spanning HPSG analyses were accepted. Parse times are for 1-best parsing, using selective unpacking (Zhang, Oepen, & Carroll, 2007). HPSG parsing outputs are available in several different forms. We investigated two types of structures: syntactic derivations and MRS meaningrepresentations. Representative features were extracted from both structures and selectively used in the statistical syntactic dependency parsing and semantic role labeling modules for the ‘open’ challenge. 3 We also experimented with using DA scores directly as empirical probabilities in the training distribution (or some function of DA, to make it fall off more sharply), but none of these methods seemed to further improve parse selection performance."
W09-1204,W08-2126,1,0.789115,"Cluster of Multimodal Computing and Interaction for the support of the work. The second author is funded by the PIRE PhD scholarship program. Participation of the third author in this work was supported by the University of Oslo, as part of its research partnership with the Center for the Study of Language and Information at Stanford University. Our deep parsing experimentation was executed on the TITAN HPC facilities at the University of Oslo. 31 performance. This makes the task a nice testbed for the cross-fertilization of various language processing techniques. As an example of such work, Zhang et al. (2008) have shown in the past that deep linguistic parsing outputs can be integrated to help improve the performance of the English semantic role labeling task. But several questions remain unanswered. First, the integration only experimented with the semantic role labeling part of the task. It is not clear whether syntactic dependency parsing can also benefit from grammar-based parsing results. Second, the English grammar used to achieve the improvement is one of the largest and most mature hand-crafted linguistic grammars. It is not clear whether similar improvements can be achieved with less deve"
W09-3710,W07-1422,0,0.0662688,"Missing"
W09-3710,P01-1008,0,0.0814156,"Missing"
W09-3710,W07-1409,0,0.124253,"r the inference rule application. The evaluation of our approach on the RTE data shows promising results on precision and the error analysis suggests future improvements. 1 Introduction Textual inference plays an important role in many natural language processing (NLP) tasks, such as question answering [7]. In recent years, the recognizing textual entailment (RTE) [4] challenge, which focuses on detecting semantic inference, has attracted a lot of attention. Given a text T (several sentences) and a hypothesis H (one sentence), the goal is to detect if H can be inferred from T. Studies such as [3] attest that lexical substitution (e.g. synonyms, antonyms) or simple syntactic variation accounts for the entailment only in a small number of pairs. Thus, one essential issue is to identify more complex expressions which, in appropriate contexts, convey the same (or similar) meaning. More generally, we are also interested in pairs of expressions in which only a uni-directional inference relation holds1 . A typical example is the following RTE pair in which accelerate to in H is used as an alternative formulation for reach speed of in T. 1 We will use the term inference rule to stand for such"
W09-3710,P08-1118,0,0.0633227,"Missing"
W09-3710,W07-1401,0,0.241809,"Missing"
W09-3710,P06-1114,0,0.0250533,"cognizing textual entailment (RTE). We start with an automatically acquired collection and then propose methods to refine it and obtain more rules using a hand-crafted lexical resource. Following this, we derive a dependency-based representation from texts, which aims to provide a proper base for the inference rule application. The evaluation of our approach on the RTE data shows promising results on precision and the error analysis suggests future improvements. 1 Introduction Textual inference plays an important role in many natural language processing (NLP) tasks, such as question answering [7]. In recent years, the recognizing textual entailment (RTE) [4] challenge, which focuses on detecting semantic inference, has attracted a lot of attention. Given a text T (several sentences) and a hypothesis H (one sentence), the goal is to detect if H can be inferred from T. Studies such as [3] attest that lexical substitution (e.g. synonyms, antonyms) or simple syntactic variation accounts for the entailment only in a small number of pairs. Thus, one essential issue is to identify more complex expressions which, in appropriate contexts, convey the same (or similar) meaning. More generally, w"
W09-3710,W07-1421,0,0.384821,"Missing"
W09-3710,W07-1414,0,0.0547149,"Missing"
W09-3710,N03-1024,0,0.0268329,"Missing"
W09-3710,P07-1058,0,0.0361982,"Missing"
W09-3710,W04-3206,0,0.0326978,"Missing"
W09-3710,W07-1406,1,0.853485,"Missing"
W09-3710,D07-1017,0,\N,Missing
W10-0725,W05-1209,0,0.019039,"wers. Therefore, the research questions we could ask are, 1. Are these hypotheses really those ones people interested in? 2. Are hypotheses different if we construct them in other ways? 2 Related Work The early related research was done by Cooper et al. (1996), where they manually construct a textbookstyle corpus aiming at different semantic phenomena involved in inference. However, the dataset is not large enough to train a robust machine-learningbased RTE system. The recent research from the RTE community focused on acquiring large quantities of textual entailment pairs from news headlines (Burger and Ferro, 2005) and negative examples from sequential sentences with transitional discourse connectives (Hickl et al., 2006). Although the quality of the data collected were quite good, most of the positive examples are similar to summarization and the negative examples are more like a comparison/contrast between two sentences instead of a contradiction. Those data are the real sentences used in news articles, but the way of obtaining them is not necessarily the (only) best way to 163 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 163–167,"
W10-0725,P05-1045,0,0.00693946,"e example as a guide along with the instructions. 4 Experiments and Results The texts we use in our experiments are the development set of the RTE-5 challenge (Bentivogli et al., Total Extracted NEs Facts 244 Counter-Facts 121 Generated Hypotheses Facts 790 Counter-Facts 203 Average (per Text) 1.19 1.11 3.85 1.86 Table 1: The statistics of the (valid) data we collect. The Total column presents the number of extracted NEs and generated hypotheses and the Average column shows the average numbers per text respectively. 2009), and we preprocess the data using the Stanford named-entity recognizer (Finkel et al., 2005). In all, it contains 600 T-H pairs, and we use the texts to generate facts and counter-facts and hypotheses as references. We put our task online through CrowdFlower2 , and on average, we pay one cent for each (counter-)fact to the Turkers. CrowdFlower can help with finding trustful Turkers and the data were collected within a few hours. To get a sense of the quality of the data we collect, we mainly focus on analyzing the following three aspects: 1) the statistics of the datasets themselves; 2) the comparison between the data we collect and the original RTE dataset; and 3) the comparison bet"
W10-0725,D08-1027,0,0.23399,"Missing"
W10-0725,wang-sporleder-2010-constructing,1,\N,Missing
W10-0725,W07-1401,0,\N,Missing
W10-4144,P05-1022,0,0.225381,"Missing"
W10-4144,P02-1034,0,0.0384164,"best parse tree given by the Parse Reranker will be the result for Task 2.2; and the final output of the system will be the result for Task 2.1. Since we have already mentioned the Berkeley Parser in the related work, we will focus on the other two modules in the rest of this section. 3.1 Parse Reranker We follow Collins and Koo (2005)’s discriminative reranking model to score possible parse trees of each sentence given by the Berkeley Parser. Previous research on English shows that structured perceptron (Collins, 2002) is one of the strongest machine learning algorithms for parse reranking (Collins and Duffy, 2002; Gao et al., 2007). In our system, we use the averaged perceptron algorithm to do parameter estimation. Algorithm 1 illustrates the learning procedure. The parameter vector w is initialized to (0, ..., 0). The learner processes all the instances (t is from 1 to n) in each iteration (i). If current hypothesis (w) fails to predict xt , the learner update w through calculating the difference between Φ(xt , yt∗ ) and Φ(xt , yt ). At the end of each iteration, the learner save the current model as w + i, and finally all these models will be added up to get aw. 3.2 Features We use an example to sho"
W10-4144,J05-1003,0,0.0744491,"e of the same sentence. For example, we can check whether the boundary predictions given by the TCT parser are agreed by the PCTB parser. Since the PCTB parser is trained on a different treebank from TCT, our reranking model can be seen as a method to use a heterogenous resource. The best parse tree given by the Parse Reranker will be the result for Task 2.2; and the final output of the system will be the result for Task 2.1. Since we have already mentioned the Berkeley Parser in the related work, we will focus on the other two modules in the rest of this section. 3.1 Parse Reranker We follow Collins and Koo (2005)’s discriminative reranking model to score possible parse trees of each sentence given by the Berkeley Parser. Previous research on English shows that structured perceptron (Collins, 2002) is one of the strongest machine learning algorithms for parse reranking (Collins and Duffy, 2002; Gao et al., 2007). In our system, we use the averaged perceptron algorithm to do parameter estimation. Algorithm 1 illustrates the learning procedure. The parameter vector w is initialized to (0, ..., 0). The learner processes all the instances (t is from 1 to n) in each iteration (i). If current hypothesis (w)"
W10-4144,W02-1001,0,0.300956,"rom TCT, our reranking model can be seen as a method to use a heterogenous resource. The best parse tree given by the Parse Reranker will be the result for Task 2.2; and the final output of the system will be the result for Task 2.1. Since we have already mentioned the Berkeley Parser in the related work, we will focus on the other two modules in the rest of this section. 3.1 Parse Reranker We follow Collins and Koo (2005)’s discriminative reranking model to score possible parse trees of each sentence given by the Berkeley Parser. Previous research on English shows that structured perceptron (Collins, 2002) is one of the strongest machine learning algorithms for parse reranking (Collins and Duffy, 2002; Gao et al., 2007). In our system, we use the averaged perceptron algorithm to do parameter estimation. Algorithm 1 illustrates the learning procedure. The parameter vector w is initialized to (0, ..., 0). The learner processes all the instances (t is from 1 to n) in each iteration (i). If current hypothesis (w) fails to predict xt , the learner update w through calculating the difference between Φ(xt , yt∗ ) and Φ(xt , yt ). At the end of each iteration, the learner save the current model as w +"
W10-4144,P07-1104,0,0.0134447,"the Parse Reranker will be the result for Task 2.2; and the final output of the system will be the result for Task 2.1. Since we have already mentioned the Berkeley Parser in the related work, we will focus on the other two modules in the rest of this section. 3.1 Parse Reranker We follow Collins and Koo (2005)’s discriminative reranking model to score possible parse trees of each sentence given by the Berkeley Parser. Previous research on English shows that structured perceptron (Collins, 2002) is one of the strongest machine learning algorithms for parse reranking (Collins and Duffy, 2002; Gao et al., 2007). In our system, we use the averaged perceptron algorithm to do parameter estimation. Algorithm 1 illustrates the learning procedure. The parameter vector w is initialized to (0, ..., 0). The learner processes all the instances (t is from 1 to n) in each iteration (i). If current hypothesis (w) fails to predict xt , the learner update w through calculating the difference between Φ(xt , yt∗ ) and Φ(xt , yt ). At the end of each iteration, the learner save the current model as w + i, and finally all these models will be added up to get aw. 3.2 Features We use an example to show the features we e"
W10-4144,N07-1051,0,0.0495543,"Missing"
W10-4144,P06-1055,0,0.202281,"Missing"
W11-1208,P05-1074,1,0.881787,"ion, while they are not generalized enough to be applied to other tasks or they have a rather small coverage, e.g. RTE (Dinu and Wang, 2009). To our best knowledge, there is few focused study on general paraphrase fragments extraction at the sub-sentential level, from comparable corpora. A recent study by Belz and Kow (2010) mainly aimed at natural language generation, which they performed a small scale experiment on a specific topic, i.e., British hills. 53 Given the available parallel corpora from the MT community, there are studies focusing on extracting paraphrases from bilingual corpora (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). The way they do is to treat one language as an pivot and equate two phrases in the other languages as paraphrases if they share a common pivot phrase. Paraphrase extraction draws on phrase pair extraction from the translation literature. Since parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. As for the MT research, the standard statistical MT systems require large size of parallel corpora for training and then extract sub-sentential translation phrases."
W11-1208,N03-1003,0,0.17802,"SMT phrase tables e.g., Munteanu and Marcu (2006) Table 1: Previous work in paraphrase acquisition and machine translation. 2 Related Work Roughly speaking, there are three dimensions to characterize the previous work in paraphrase acquisition and machine translation, whether the data comes from monolingual or bilingual corpora, whether the corpora are parallel or comparable, and whether the output is at the sentence level or at the sub-sentential level. Table 1 gives one example in each category. Paraphrase acquisition is mostly done at the sentence-level, e.g., (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Dolan et al., 2004), which is not straightforward to be used as a resource for other NLP applications. Quirk et al. (2004) adopted the MT approach to “translate” one sentence into a paraphrased one. As for the corpora, Barzilay and McKeown (2001) took different English translations of the same novels (i.e., monolingual parallel corpora), while the others experimented on multiple sources of the same news/events, i.e., monolingual comparable corpora. At the sub-sentential level, interchangeable patterns (Shinyama et al., 2002; Shinyama and Sekine, 2003) or inference rules (Lin and Pantel, 2001"
W11-1208,P01-1008,0,0.844413,"ther) small set of candidate words in the Chris Callison-Burch Computer Science Department Johns Hopkins University 3400 N. Charles Street (CSEB 226-B) Baltimore, MD 21218, USA ccb@cs.jhu.edu target language; while in principle, each paraphrase can have infinite number of “target” expressions, which reflects the variety of each human language. A variety of paraphrase extraction approaches have been proposed recently, and they require different types of training data. Some require bilingual parallel corpora (Callison-Burch, 2008; Zhao et al., 2008), others require monolingual parallel corpora (Barzilay and McKeown, 2001; Ibrahim et al., 2003) or monolingual comparable corpora (Dolan et al., 2004). In this paper, we focus on extracting paraphrase fragments from monolingual corpora, because this is the most abundant source of data. Additionally, this would potentially allow us to extract paraphrases for a variety of languages that have monolingual corpora, but which do not have easily accessible parallel corpora. This paper makes the following contributions: 1. We adapt a translation fragment pair extraction method to paraphrase extraction, i.e., from bilingual corpora to monolingual corpora. 2. We construct a"
W11-1208,P99-1071,0,0.444385,"Missing"
W11-1208,W10-4217,0,0.0478864,"ws/events, i.e., monolingual comparable corpora. At the sub-sentential level, interchangeable patterns (Shinyama et al., 2002; Shinyama and Sekine, 2003) or inference rules (Lin and Pantel, 2001) are extracted, which are quite successful in namedentity-centered tasks, like information extraction, while they are not generalized enough to be applied to other tasks or they have a rather small coverage, e.g. RTE (Dinu and Wang, 2009). To our best knowledge, there is few focused study on general paraphrase fragments extraction at the sub-sentential level, from comparable corpora. A recent study by Belz and Kow (2010) mainly aimed at natural language generation, which they performed a small scale experiment on a specific topic, i.e., British hills. 53 Given the available parallel corpora from the MT community, there are studies focusing on extracting paraphrases from bilingual corpora (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). The way they do is to treat one language as an pivot and equate two phrases in the other languages as paraphrases if they share a common pivot phrase. Paraphrase extraction draws on phrase pair extraction from the translation literature. Since parall"
W11-1208,D08-1021,1,0.938988,"rase “parallel” corpora. Furthermore, in MT, certain words can be translated into a (rather) small set of candidate words in the Chris Callison-Burch Computer Science Department Johns Hopkins University 3400 N. Charles Street (CSEB 226-B) Baltimore, MD 21218, USA ccb@cs.jhu.edu target language; while in principle, each paraphrase can have infinite number of “target” expressions, which reflects the variety of each human language. A variety of paraphrase extraction approaches have been proposed recently, and they require different types of training data. Some require bilingual parallel corpora (Callison-Burch, 2008; Zhao et al., 2008), others require monolingual parallel corpora (Barzilay and McKeown, 2001; Ibrahim et al., 2003) or monolingual comparable corpora (Dolan et al., 2004). In this paper, we focus on extracting paraphrase fragments from monolingual corpora, because this is the most abundant source of data. Additionally, this would potentially allow us to extract paraphrases for a variety of languages that have monolingual corpora, but which do not have easily accessible parallel corpora. This paper makes the following contributions: 1. We adapt a translation fragment pair extraction method to"
W11-1208,E09-1025,1,0.867989,", Barzilay and McKeown (2001) took different English translations of the same novels (i.e., monolingual parallel corpora), while the others experimented on multiple sources of the same news/events, i.e., monolingual comparable corpora. At the sub-sentential level, interchangeable patterns (Shinyama et al., 2002; Shinyama and Sekine, 2003) or inference rules (Lin and Pantel, 2001) are extracted, which are quite successful in namedentity-centered tasks, like information extraction, while they are not generalized enough to be applied to other tasks or they have a rather small coverage, e.g. RTE (Dinu and Wang, 2009). To our best knowledge, there is few focused study on general paraphrase fragments extraction at the sub-sentential level, from comparable corpora. A recent study by Belz and Kow (2010) mainly aimed at natural language generation, which they performed a small scale experiment on a specific topic, i.e., British hills. 53 Given the available parallel corpora from the MT community, there are studies focusing on extracting paraphrases from bilingual corpora (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). The way they do is to treat one language as an pivot and equate"
W11-1208,I05-5002,0,0.0489442,"in Table 5). An alternative way is to follow the linguistic definition of a phrase, e.g. noun phrase (NP), verb phrase (VP), etc. In this case, we need to use (at least) a chunker to preprocess the text and obtain the proper boundary of each fragment and we used the OpenNLP chunker. We finalize our paraphrase collection by filtering out identical fragment pairs, subsumed fragment pairs (one fragment is fully contained in the other), and fragment having only one word. Apart from sentence pairs collected from the comparable corpora, we also did experiments on the existing MSR paraphrase corpus (Dolan and Brockett, 2005), which is a collection of manually annotated sentential paraphrases. The evaluation on both collections is done by the MTurk. Each task contains 8 pairs of fragments to be evaluated, plus one positive control using identical fragment pairs, and one negative control using a pair of random fragments. All the fragments are shown with the corresponding sentences from where they are extracted5 . The question being asked is 5 We thought about evaluating pairs of isolated fragments, 57 “How are the two highlighted phrases related?”, and the possible answers are, “These phrases refer to the same thin"
W11-1208,C04-1051,0,0.834894,"tment Johns Hopkins University 3400 N. Charles Street (CSEB 226-B) Baltimore, MD 21218, USA ccb@cs.jhu.edu target language; while in principle, each paraphrase can have infinite number of “target” expressions, which reflects the variety of each human language. A variety of paraphrase extraction approaches have been proposed recently, and they require different types of training data. Some require bilingual parallel corpora (Callison-Burch, 2008; Zhao et al., 2008), others require monolingual parallel corpora (Barzilay and McKeown, 2001; Ibrahim et al., 2003) or monolingual comparable corpora (Dolan et al., 2004). In this paper, we focus on extracting paraphrase fragments from monolingual corpora, because this is the most abundant source of data. Additionally, this would potentially allow us to extract paraphrases for a variety of languages that have monolingual corpora, but which do not have easily accessible parallel corpora. This paper makes the following contributions: 1. We adapt a translation fragment pair extraction method to paraphrase extraction, i.e., from bilingual corpora to monolingual corpora. 2. We construct a large collection of paraphrase fragments from monolingual comparable corpora"
W11-1208,W04-3208,0,0.0235461,"the translation literature. Since parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. As for the MT research, the standard statistical MT systems require large size of parallel corpora for training and then extract sub-sentential translation phrases. Apart from the limited parallel corpora, comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (Fung and Lo, 1998; Koehn and Knight, 2000; Vogel, 2003; Fung and Cheung, 2004a; Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Wu and Fung, 2005). A recent study by Smith et al. (2010) extracted parallel sentences from comparable corpora to extend the existing resources. At the sub-sentential level, Munteanu and Marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the loglikelihood-ratio of word translation probability. They exploit the possibility of making use of reports within a limited time window, which are about the same event or having overlapping contents, but in different languages. Quirk et al. (2007) extracted fragments"
W11-1208,C04-1151,0,0.0254662,"the translation literature. Since parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. As for the MT research, the standard statistical MT systems require large size of parallel corpora for training and then extract sub-sentential translation phrases. Apart from the limited parallel corpora, comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (Fung and Lo, 1998; Koehn and Knight, 2000; Vogel, 2003; Fung and Cheung, 2004a; Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Wu and Fung, 2005). A recent study by Smith et al. (2010) extracted parallel sentences from comparable corpora to extend the existing resources. At the sub-sentential level, Munteanu and Marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the loglikelihood-ratio of word translation probability. They exploit the possibility of making use of reports within a limited time window, which are about the same event or having overlapping contents, but in different languages. Quirk et al. (2007) extracted fragments"
W11-1208,P98-1069,0,0.103069,"aphrase extraction draws on phrase pair extraction from the translation literature. Since parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. As for the MT research, the standard statistical MT systems require large size of parallel corpora for training and then extract sub-sentential translation phrases. Apart from the limited parallel corpora, comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (Fung and Lo, 1998; Koehn and Knight, 2000; Vogel, 2003; Fung and Cheung, 2004a; Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Wu and Fung, 2005). A recent study by Smith et al. (2010) extracted parallel sentences from comparable corpora to extend the existing resources. At the sub-sentential level, Munteanu and Marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the loglikelihood-ratio of word translation probability. They exploit the possibility of making use of reports within a limited time window, which are about the same event or having overlapping contents, but in"
W11-1208,W03-1608,0,0.152754,"words in the Chris Callison-Burch Computer Science Department Johns Hopkins University 3400 N. Charles Street (CSEB 226-B) Baltimore, MD 21218, USA ccb@cs.jhu.edu target language; while in principle, each paraphrase can have infinite number of “target” expressions, which reflects the variety of each human language. A variety of paraphrase extraction approaches have been proposed recently, and they require different types of training data. Some require bilingual parallel corpora (Callison-Burch, 2008; Zhao et al., 2008), others require monolingual parallel corpora (Barzilay and McKeown, 2001; Ibrahim et al., 2003) or monolingual comparable corpora (Dolan et al., 2004). In this paper, we focus on extracting paraphrase fragments from monolingual corpora, because this is the most abundant source of data. Additionally, this would potentially allow us to extract paraphrases for a variety of languages that have monolingual corpora, but which do not have easily accessible parallel corpora. This paper makes the following contributions: 1. We adapt a translation fragment pair extraction method to paraphrase extraction, i.e., from bilingual corpora to monolingual corpora. 2. We construct a large collection of pa"
W11-1208,N06-1014,0,0.0333808,"sentence pairs to feed our fragment extraction method. 3.3 Fragment Pair Extraction The basic procedure is to 1) establish alignments between words or n-grams and 2) extract target paraphrase fragments. For the first step, we use two approaches. One is to change the common substring alignment problem from string to word sequence and we extend the longest common substring (LCS) extraction algorithm to multiple common n-grams. An alternative way is to use a normal word aligner (widely used as the first step in MT systems) to accomplish the job. For our experiments, we use the BerkeleyAligner4 (Liang et al., 2006) by feeding it a dictionary of pairs of identical words along with the paired sentences. We can also combine these two methods by performing the LCS alignment first and adding additional word alignments from the aligner. These form the three configurations of our system (Table 2). Following Munteanu and Marcu (2006), we use both positive and negative lexical associations for the alignment. The positive association measures 4 http://code.google.com/p/ berkeleyaligner/ 56 how likely one word will be aligned to another (value from 0 to 1); and the negative associations indicates how unlikely an a"
W11-1208,D09-1040,1,0.931897,"Missing"
W11-1208,J05-4003,0,0.0292508,"ra have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. As for the MT research, the standard statistical MT systems require large size of parallel corpora for training and then extract sub-sentential translation phrases. Apart from the limited parallel corpora, comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (Fung and Lo, 1998; Koehn and Knight, 2000; Vogel, 2003; Fung and Cheung, 2004a; Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Wu and Fung, 2005). A recent study by Smith et al. (2010) extracted parallel sentences from comparable corpora to extend the existing resources. At the sub-sentential level, Munteanu and Marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the loglikelihood-ratio of word translation probability. They exploit the possibility of making use of reports within a limited time window, which are about the same event or having overlapping contents, but in different languages. Quirk et al. (2007) extracted fragments using a generative model of noisy translations. Th"
W11-1208,P06-1011,0,0.492896,"ng and Using Comparable Corpora, pages 52–60, 49th Annual Meeting of the Association for Computational Linguistics, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics Corpora Sentence level Paraphrase acquisition Parallel e.g., Barzilay and McKeown (2001) Monolingual Comparable e.g., Quirk et al. (2004) Bilingual Parallel N/A Statistical machine translation Most SMT systems Parallel Bilingual Comparable e.g., Fung and Lo (1998) Sub-sentential level This paper e.g., Shinyama et al. (2002) & This paper e.g., Bannard and Callison-Burch (2005) SMT phrase tables e.g., Munteanu and Marcu (2006) Table 1: Previous work in paraphrase acquisition and machine translation. 2 Related Work Roughly speaking, there are three dimensions to characterize the previous work in paraphrase acquisition and machine translation, whether the data comes from monolingual or bilingual corpora, whether the corpora are parallel or comparable, and whether the output is at the sentence level or at the sub-sentential level. Table 1 gives one example in each category. Paraphrase acquisition is mostly done at the sentence-level, e.g., (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Dolan et al., 2004), which"
W11-1208,W04-3219,0,0.22536,"mparable corpora and achieve similar quality from a manually-checked paraphrase corpus. 3. We evaluate both intermediate and final results of the paraphrase collection, using the crowdsourcing technique, which is effective, fast, and cheap. 52 Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 52–60, 49th Annual Meeting of the Association for Computational Linguistics, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics Corpora Sentence level Paraphrase acquisition Parallel e.g., Barzilay and McKeown (2001) Monolingual Comparable e.g., Quirk et al. (2004) Bilingual Parallel N/A Statistical machine translation Most SMT systems Parallel Bilingual Comparable e.g., Fung and Lo (1998) Sub-sentential level This paper e.g., Shinyama et al. (2002) & This paper e.g., Bannard and Callison-Burch (2005) SMT phrase tables e.g., Munteanu and Marcu (2006) Table 1: Previous work in paraphrase acquisition and machine translation. 2 Related Work Roughly speaking, there are three dimensions to characterize the previous work in paraphrase acquisition and machine translation, whether the data comes from monolingual or bilingual corpora, whether the corpora are par"
W11-1208,2007.mtsummit-papers.50,0,0.040726,"000; Vogel, 2003; Fung and Cheung, 2004a; Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Wu and Fung, 2005). A recent study by Smith et al. (2010) extracted parallel sentences from comparable corpora to extend the existing resources. At the sub-sentential level, Munteanu and Marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the loglikelihood-ratio of word translation probability. They exploit the possibility of making use of reports within a limited time window, which are about the same event or having overlapping contents, but in different languages. Quirk et al. (2007) extracted fragments using a generative model of noisy translations. They show that even in non-parallel corpora, useful parallel words or phrases can still be found and the size of such data is much larger than that of Corpora (Gigaword) Paraphrase Collection (MSR) Paraphrase Collecton (CCB) Document Pair Extraction Sentence Pair Extraction Fragment Pair Extraction Comparability N-Gram Overlapping Interchangeability <doc&gt; . .. in 1995 ... </doc&gt; <doc&gt; . .. Jan., 1995 ... </doc&gt; <sent&gt; NATO ... in 1995 ... </sent&gt; <sent&gt; In 1995, NATO ... </sent&gt; <frag&gt; the finance chief </frag&gt; Paraphrased Fr"
W11-1208,W03-1609,0,0.130246,"ce-level, e.g., (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Dolan et al., 2004), which is not straightforward to be used as a resource for other NLP applications. Quirk et al. (2004) adopted the MT approach to “translate” one sentence into a paraphrased one. As for the corpora, Barzilay and McKeown (2001) took different English translations of the same novels (i.e., monolingual parallel corpora), while the others experimented on multiple sources of the same news/events, i.e., monolingual comparable corpora. At the sub-sentential level, interchangeable patterns (Shinyama et al., 2002; Shinyama and Sekine, 2003) or inference rules (Lin and Pantel, 2001) are extracted, which are quite successful in namedentity-centered tasks, like information extraction, while they are not generalized enough to be applied to other tasks or they have a rather small coverage, e.g. RTE (Dinu and Wang, 2009). To our best knowledge, there is few focused study on general paraphrase fragments extraction at the sub-sentential level, from comparable corpora. A recent study by Belz and Kow (2010) mainly aimed at natural language generation, which they performed a small scale experiment on a specific topic, i.e., British hills."
W11-1208,N10-1063,0,0.0263253,"guage concept, large quantities of paraphrase pairs can be extracted. As for the MT research, the standard statistical MT systems require large size of parallel corpora for training and then extract sub-sentential translation phrases. Apart from the limited parallel corpora, comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (Fung and Lo, 1998; Koehn and Knight, 2000; Vogel, 2003; Fung and Cheung, 2004a; Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Wu and Fung, 2005). A recent study by Smith et al. (2010) extracted parallel sentences from comparable corpora to extend the existing resources. At the sub-sentential level, Munteanu and Marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the loglikelihood-ratio of word translation probability. They exploit the possibility of making use of reports within a limited time window, which are about the same event or having overlapping contents, but in different languages. Quirk et al. (2007) extracted fragments using a generative model of noisy translations. They show that even in non-parallel corpora, useful parallel"
W11-1208,D08-1027,0,0.0451073,"Missing"
W11-1208,E03-1050,0,0.0343029,"raction from the translation literature. Since parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. As for the MT research, the standard statistical MT systems require large size of parallel corpora for training and then extract sub-sentential translation phrases. Apart from the limited parallel corpora, comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (Fung and Lo, 1998; Koehn and Knight, 2000; Vogel, 2003; Fung and Cheung, 2004a; Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Wu and Fung, 2005). A recent study by Smith et al. (2010) extracted parallel sentences from comparable corpora to extend the existing resources. At the sub-sentential level, Munteanu and Marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the loglikelihood-ratio of word translation probability. They exploit the possibility of making use of reports within a limited time window, which are about the same event or having overlapping contents, but in different languages. Quirk et al. (20"
W11-1208,I05-1023,0,0.0255674,"ays of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. As for the MT research, the standard statistical MT systems require large size of parallel corpora for training and then extract sub-sentential translation phrases. Apart from the limited parallel corpora, comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (Fung and Lo, 1998; Koehn and Knight, 2000; Vogel, 2003; Fung and Cheung, 2004a; Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Wu and Fung, 2005). A recent study by Smith et al. (2010) extracted parallel sentences from comparable corpora to extend the existing resources. At the sub-sentential level, Munteanu and Marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the loglikelihood-ratio of word translation probability. They exploit the possibility of making use of reports within a limited time window, which are about the same event or having overlapping contents, but in different languages. Quirk et al. (2007) extracted fragments using a generative model of noisy translations. They show that even in"
W11-1208,P08-1089,0,0.585996,"a. Furthermore, in MT, certain words can be translated into a (rather) small set of candidate words in the Chris Callison-Burch Computer Science Department Johns Hopkins University 3400 N. Charles Street (CSEB 226-B) Baltimore, MD 21218, USA ccb@cs.jhu.edu target language; while in principle, each paraphrase can have infinite number of “target” expressions, which reflects the variety of each human language. A variety of paraphrase extraction approaches have been proposed recently, and they require different types of training data. Some require bilingual parallel corpora (Callison-Burch, 2008; Zhao et al., 2008), others require monolingual parallel corpora (Barzilay and McKeown, 2001; Ibrahim et al., 2003) or monolingual comparable corpora (Dolan et al., 2004). In this paper, we focus on extracting paraphrase fragments from monolingual corpora, because this is the most abundant source of data. Additionally, this would potentially allow us to extract paraphrases for a variety of languages that have monolingual corpora, but which do not have easily accessible parallel corpora. This paper makes the following contributions: 1. We adapt a translation fragment pair extraction method to paraphrase extractio"
W11-1208,C98-1066,0,\N,Missing
W11-1208,W03-1004,0,\N,Missing
W11-1208,W07-1401,0,\N,Missing
W11-3216,J93-2003,0,0.0192027,"d Ch-En transliterations, there was a discussion on whether to use the intermediate phonemic interpretation, i.e., Pinyin. Li et al. (2004) showed empirically that by skipping the intermediate phonemic interpretation (denoted as grapheme-based methods), the transliteration error rate was reduced significantly, since the mapping between Pinyin and Chinese characters was not trivial. Oh et al. (2009) had a more generalized version of Li et al. (2004)’s system as well as other 2.1 Phrase-based SMT The basic architecture of a phrase-based SMT system is an instance of the noisy-channel approaches (Brown et al., 1993). In the context of transliteration, the term “phrase” in phrase-based SMT would refer to a sequence of characters chosen by its statistical rather then any grammatical properties. The transliteration of a name s in the source language into a name t in the target language is modeled as: 101 Proceedings of the 2011 Named Entities Workshop, IJCNLP 2011, pages 101–105, Chiang Mai, Thailand, November 12, 2011. have the transliteration probability defined as: arg max P (t|s) = arg max(P (t)P (s|t)); t P (s, t, α) = t K X k=1 (1) where < e, c &gt;k is the k th aligned pair of translation units. Therefo"
W11-3216,N03-1017,0,0.00791569,"SRILM toolkit (Stolcke, 2002). The translation model is built from the character alignments given the M2MJSC model and we did not construct any distortion models. M2MJSC is first applied to the training set to divide each source name in parallel with the corresponding target name into the same number of segments. These segments are then considered as words that are one-to-one aligned. The PBSMT system takes multiple segments, namely phrases, as translation units. The phrase extraction follows the heuristic that starts with the given word alignment and expands to the adjacent alignment points (Koehn et al., 2003). The translation probabilities of the extracted phrases are estimated accordingly. As the last step, we split all the segments in the translation model into characters to allow more straightforward integration into the original PBSMT system that relies on character based inputs. 3.2.2 Moses decoder We used the open-source SMT decoder Moses (Koehn et al., 2007). Moses allows a log-linear model to combine various models and implements an efficient beam search algorithm that quickly finds the best translation among the large number of hypotheses. In order to adapt the SMT decoder to the translit"
W11-3216,P04-1021,0,0.604184,"flexibility of the PBSMT system. Introduction Machine transliteration has drawn a lot of attention in the previous years. In particular, the previous two shared tasks (Li et al., 2009; Li et al., 2010) attracted more than 30 participants. This year’s task only focuses on the transliteration generation task. As our first attempt in this area, we participated in English-to-Chinese transliteration (En-Ch) and Chinese-to-English back transliteration (Ch-En) tasks. For En-Ch and Ch-En transliterations, there was a discussion on whether to use the intermediate phonemic interpretation, i.e., Pinyin. Li et al. (2004) showed empirically that by skipping the intermediate phonemic interpretation (denoted as grapheme-based methods), the transliteration error rate was reduced significantly, since the mapping between Pinyin and Chinese characters was not trivial. Oh et al. (2009) had a more generalized version of Li et al. (2004)’s system as well as other 2.1 Phrase-based SMT The basic architecture of a phrase-based SMT system is an instance of the noisy-channel approaches (Brown et al., 1993). In the context of transliteration, the term “phrase” in phrase-based SMT would refer to a sequence of characters chose"
W11-3216,P07-1016,0,0.0413608,"Missing"
W11-3216,W09-3501,0,0.156905,"systems do not consider the key features of the transliteration task, which, on the other hand, have been emphasized by the joint source channel models. Our primary system is a standard phrase-based statistical machine translation (PBSMT) system with a modification based on the Multi-to-Multi Joint Source Channel model. We hope the combination could benefit from the simplicity of a joint source channel model without losing the flexibility of the PBSMT system. Introduction Machine transliteration has drawn a lot of attention in the previous years. In particular, the previous two shared tasks (Li et al., 2009; Li et al., 2010) attracted more than 30 participants. This year’s task only focuses on the transliteration generation task. As our first attempt in this area, we participated in English-to-Chinese transliteration (En-Ch) and Chinese-to-English back transliteration (Ch-En) tasks. For En-Ch and Ch-En transliterations, there was a discussion on whether to use the intermediate phonemic interpretation, i.e., Pinyin. Li et al. (2004) showed empirically that by skipping the intermediate phonemic interpretation (denoted as grapheme-based methods), the transliteration error rate was reduced significa"
W11-3216,W10-2401,0,0.278245,"onsider the key features of the transliteration task, which, on the other hand, have been emphasized by the joint source channel models. Our primary system is a standard phrase-based statistical machine translation (PBSMT) system with a modification based on the Multi-to-Multi Joint Source Channel model. We hope the combination could benefit from the simplicity of a joint source channel model without losing the flexibility of the PBSMT system. Introduction Machine transliteration has drawn a lot of attention in the previous years. In particular, the previous two shared tasks (Li et al., 2009; Li et al., 2010) attracted more than 30 participants. This year’s task only focuses on the transliteration generation task. As our first attempt in this area, we participated in English-to-Chinese transliteration (En-Ch) and Chinese-to-English back transliteration (Ch-En) tasks. For En-Ch and Ch-En transliterations, there was a discussion on whether to use the intermediate phonemic interpretation, i.e., Pinyin. Li et al. (2004) showed empirically that by skipping the intermediate phonemic interpretation (denoted as grapheme-based methods), the transliteration error rate was reduced significantly, since the ma"
W11-3216,J04-4002,0,0.0681232,"can be used in the transliterated western names. Accordingly, for each source name, there are only a limited set of candidate transliterations, unlike the infinite target set for the general translation task. It is critical to take into account these characteristics mentioned above when utilizing an SMT system for transliteration. First, the distortion model, one of the major components in a standard PBSMT system, is redundant for transliteration. Including the unnecessary model expands the search space and makes it more difficult to find the good candidates. Second, the word alignment model (Och and Ney, 2004) in a PBSMT system also assumes flexible ordering of correspondence to some extent. This could introduce additional noise to the translation models if applied directly to transliteration tasks without any modifications. 2.2 P (< e, c &gt;k |< e, c &gt;k−1 k−n+1 ) t = argmax P (s, t, α) (2) s = argmax P (s, t, α) (3) s,α t,α The alignment statistics can be obtained with an Expectation-Maximization procedure over the training corpus. For English-Chinese bidirectional transliteration, Li et al. (2004) assumed that each Chinese character aligns with a sequence of one or more letters in English. This ass"
W11-3216,P03-1021,0,0.00488314,"t to 0. 103 Tasks English-to-Chinese English-to-Chinese Chinese-to-English Chinese-to-English System M2MJC+PBSMT M2MJC M2MJC+PBSMT M2MJC ACC 0.320 0.260 0.133 0.117 Mean F 0.674 0.638 0.746 0.731 MRR 0.397 0.340 0.210 0.177 Map ref 0.308 0.251 0.133 0.117 Table 2: Official results 3.2.3 5 Parameter tuning The system integrates all the models into a more complex discriminative model in a log linear formulation. The weights for the individual models can be optimized on development data so that the system outputs are as close as possible to correct candidates. Minimum error rate training (MERT) (Och, 2003) is one of the common method for balancing between features on different bases. We used Z-MERT (Zaidan, 2009) to search for the set of feature weights that maximizes the official f-score evaluation metric on the development set. Moreover, we extracted a small development set of 500 names randomly from the official development set. The rest of the official development set served as a development test set, so we could run additional experiments on the provided data set apart from our submission. The feature weights we used for our submission are obtained from the complete development set. 4 Conc"
W11-3216,D09-1069,0,0.0185288,"ocuses on the transliteration generation task. As our first attempt in this area, we participated in English-to-Chinese transliteration (En-Ch) and Chinese-to-English back transliteration (Ch-En) tasks. For En-Ch and Ch-En transliterations, there was a discussion on whether to use the intermediate phonemic interpretation, i.e., Pinyin. Li et al. (2004) showed empirically that by skipping the intermediate phonemic interpretation (denoted as grapheme-based methods), the transliteration error rate was reduced significantly, since the mapping between Pinyin and Chinese characters was not trivial. Oh et al. (2009) had a more generalized version of Li et al. (2004)’s system as well as other 2.1 Phrase-based SMT The basic architecture of a phrase-based SMT system is an instance of the noisy-channel approaches (Brown et al., 1993). In the context of transliteration, the term “phrase” in phrase-based SMT would refer to a sequence of characters chosen by its statistical rather then any grammatical properties. The transliteration of a name s in the source language into a name t in the target language is modeled as: 101 Proceedings of the 2011 Named Entities Workshop, IJCNLP 2011, pages 101–105, Chiang Mai, T"
W11-3216,P07-2045,0,\N,Missing
W11-3216,J98-4003,0,\N,Missing
W11-3404,I05-2035,0,0.0775161,"Missing"
W11-3404,P08-1111,0,0.0461164,"Missing"
W11-3404,C10-1122,0,0.252129,"er to its head. Unlike Yu et al. (2010) who separate complement list into LCOMPS and RCOMPS, we keep all complements on the same complement list (hCOMPS),iand use an additional boolean feature RC ± to indicate whether the complement is to the right or to the left of the head. The grammar currently contains about 20 rule schemata. It should be noted that most of these rule schemata are very general. They are be used to handle multiple types of constructions, some of which will be illustrated below. 2.2 HEAD Yu et al. (2010) introduce an extra valence feature (TOPIC) for the topic construction. Tse and Curran (2010) distinguish two types of topics,gap or non-gap. Both solutions are rather similar to ours nonetheless. 2.4 Numeral-classifier structures are analyzed as a phrase with rule SPEC - HEAD, and they together serve as a specifier to the head noun. A feature “ CL” in the HEAD type of noun identifies the suitable groups of classifiers. Demonstratives are also treated as specifiers to nouns (similar to the double specifier account in (Ng, 1997)), though specific word order constraints are further enforced for the correct NP structure. Both specifiers of nouns are optional. The numeral before the class"
W11-3404,Y09-2048,0,0.107667,"ice sentences in Chinese. Similar to the analysis of BA , we use a specialized unary rule to promote the complement of the verb into the subject list, and "" # HEAD noun change the original subject into a Locative phrases serve as both pre-verbal and post-verbal modifiers, and generally take the form of zai + NP + Loc, e.g. 在 桌子 上 (on the table), 在 房子 东面 (to the east of the house), etc. Locative phrases can always serve as pre-verbal modifiers. But only certain verbs can take postverbal locatives with the HEAD - ADJ rule. The treatment of locative phrases as normal prepositional phrases as in (Wang et al., 2009) may lead to massive over-generation. The analysis of temporal phrases is similar to the locative phrases. 2.7 INDEX There are various discussions on BA in the literature. Bender (2000) considered it as a verb, Gao (2000) and Wang et al. (2009) treated it as a casemarker, and Yu et al. (2010) as a preposition. We categorize BA as a special coverb. This makes it similar to prepositions. But it will be subcategorized by (instead of modifying) the verb phrase. • non-gapping D E where neither of the above two cases applies 2.6 1 moves the direct object of a verb 20 2.10 Resultative verb compound o"
W11-3404,C10-2162,0,0.199621,"no special treatment involved • Temporal or location topics are treated as modifiers with ADJ - HEAD • A special rule SUBJ 2- HEAD is used to fill topics headed by noun or verb into the SPR valence of the main sentence. This is also referred to as the “double subject” constructions An HPSG Analysis of Mandarin Design of sign & schemata The design of the HPSG sign in MCG is compatible with the design in the LinGO Grammar Matrix. Four valcence features were employed: SUBJ for subjects, COMPS for complements, SPR for specifiers, and SPEC for back-reference from the specifier to its head. Unlike Yu et al. (2010) who separate complement list into LCOMPS and RCOMPS, we keep all complements on the same complement list (hCOMPS),iand use an additional boolean feature RC ± to indicate whether the complement is to the right or to the left of the head. The grammar currently contains about 20 rule schemata. It should be noted that most of these rule schemata are very general. They are be used to handle multiple types of constructions, some of which will be illustrated below. 2.2 HEAD Yu et al. (2010) introduce an extra valence feature (TOPIC) for the topic construction. Tse and Curran (2010) distinguish two t"
W11-3404,C94-2144,0,0.645808,"Missing"
W12-0116,J99-2004,0,0.0674614,"y Koehn and Hoang (2007), as an extension of the traditional phrasebased SMT framework. Instead of using only the word form of the text, it allows the system to take a vector of factors to represent each token, both for the source and target languages. The vector of factors can be used for different levels of linguistic annotations, like lemma, part-of-speech (POS), or other linguistic features. Furthermore, this extension actually allows us to incorporate various kinds of features if they can be (somehow) represented as annotations to the tokens. The process is quite similar to supertagging (Bangalore and Joshi, 1999), which assigns “rich descriptions (supertags) that impose complex 2 http://www.bultreebank.org/dpbtb/ http://ufal.mff.cuni.cz/conll2009-st/ task-description.html 3 constraints in a local context”. In our case, all the linguistic features (factors) associated with each token form a supertag to that token. Singh and Bandyopadhyay (2010) had a similar idea of incorporating linguistic features, while they worked on Manipuri-English bidirectional translation. Our approach is slightly different from (Birch et al., 2007) and (Hassan et al., 2007), who mainly used the supertags on the target language"
W12-0116,W02-1502,0,0.116822,"49 50 48 49 41 49 2 0 3 5 5 6 11 12 5 3 7 5 0 4 3 2 3 5 5 10 7 20 5 4 32 37 34 46 51 34 29 23 30 8 34 39 41 31 37 37 44 38 52 51 5 46 40 43 44 34 43 38 34 34 9 44 40 38 47 41 40 40 36 76 94 Total 487 479 483 520 501 482 443 410 433 242 485 482 478 487 483 480 503 471 652 689 Table 4: Manual evaluation of the grammaticality exist quite extensive implemented formal HPSG grammars for English (Copestake and Flickinger, 2000), German (M¨uller and Kasper, 2000), and Japanese (Siegel, 2000; Siegel and Bender, 2002). HPSG is the underlying theory of the international initiative LinGO Grammar Matrix (Bender et al., 2002). At the moment, precise and linguistically motivated grammars, customized on the base of the Grammar Matrix, have been or are being developed for Norwegian, French, Korean, Italian, Modern Greek, Spanish, Portuguese, Chinese, etc. There also exists a first version of the Bulgarian Resource Grammar - BURGER. In the research reported here, we use the linguistic modeled knowledge from the existing English and Bulgarian grammars. Since the Bulgarian grammar has limited coverage on news data, dependency parsing has been performed instead. Then, mapping rules have been defined for the construction"
W12-0116,W07-0702,0,0.290181,"annotations to the tokens. The process is quite similar to supertagging (Bangalore and Joshi, 1999), which assigns “rich descriptions (supertags) that impose complex 2 http://www.bultreebank.org/dpbtb/ http://ufal.mff.cuni.cz/conll2009-st/ task-description.html 3 constraints in a local context”. In our case, all the linguistic features (factors) associated with each token form a supertag to that token. Singh and Bandyopadhyay (2010) had a similar idea of incorporating linguistic features, while they worked on Manipuri-English bidirectional translation. Our approach is slightly different from (Birch et al., 2007) and (Hassan et al., 2007), who mainly used the supertags on the target language side, English. We primarily experiment with the source language side, Bulgarian. This potentially huge feature space provides us with various possibilities of using our linguistic resources developed in and out of our project. In particular, we consider the following factors on the source language side (Bulgarian): • WF - word form is just the original text token. • L EMMA is the lexical invariant of the original word form. We use the lemmatizer described in Section 3, which operates on the output from the POS tag"
W12-0116,2005.mtsummit-osmtw.3,0,0.300693,"ammars. Since the Bulgarian grammar has limited coverage on news data, dependency parsing has been performed instead. Then, mapping rules have been defined for the construction of RMRSes. However, the MRS representation is still quite close to the syntactic level, which is not fully language independent. This requires a transfer at the MRS level, if we want to do translation from the source language to the target language. The transfer is usually implemented in the form of rewriting rules. For instance, in the Norwegian LOGON project (Oepen et al., 2004), the transfer rules were hand-written (Bond et al., 2005; Oepen et al., 2007), which included a large amount of manual work. Graham and van Genabith (2008) and Graham et al. (2009) explored the automatic rule induction approach in a transfer-based MT setting involving two lexical functional grammars (LFGs), which was still restricted by the performance of both the parser and the generator. Lack of robustness for target side generation is one of the main issues, when various ill-formed or fragmented structures come out after transfer. Oepen et al. (2007) use their generator to generate text fragments instead of full sentences, in order to increase t"
W12-0116,W11-2103,0,0.0710508,"Missing"
W12-0116,W09-0405,0,0.0263186,"Missing"
W12-0116,copestake-flickinger-2000-open,0,0.587553,"emantic analysis of Bulgarian text is inspired by the work on MRS and RMRS (Robust Minimal Recursion Semantic) (see (Copestake, 2003) and (Copestake, 2007)) and the previous work on transfer of dependency analyses into RMRS structures described in (Spreyer and Frank, 2005) and (Jakob et al., 2010). In this section we present first a short overview of MRS and RMRS. Then we discuss the new features added on the basis of the RMRS structures. MRS is introduced as an underspecified semantic formalism (Copestake et al., 2005). It is used to support semantic analyses in the English HPSG grammar ERG (Copestake and Flickinger, 2000), but also in other grammar formalisms like LFG. The main idea is that the formalism avoids spelling out the complete set of readings resulting from the interaction of scope bearing operators and quantifiers, instead providing a single underspecified representation from which the complete set of readings can be constructed. Here we will present only basic definitions from (Copestake et al., 2005). For more details the cited publication should be consulted. An MRS structure is a tuple h GT , R, C i, where GT is the top handle, R is a bag of EPs (elementary predicates) and C is a bag of handle c"
W12-0116,E12-1050,1,0.83541,"Missing"
W12-0116,gimenez-marquez-2004-svmtool,0,0.107385,"Missing"
W12-0116,2008.eamt-1.10,0,0.341103,"Missing"
W12-0116,P07-1037,0,0.183167,"Missing"
W12-0116,jakob-etal-2010-mapping,0,0.197379,"eech (POS), or other linguistic features. Furthermore, this extension actually allows us to incorporate various kinds of features if they can be (somehow) represented as annotations to the tokens. The process is quite similar to supertagging (Bangalore and Joshi, 1999), which assigns “rich descriptions (supertags) that impose complex 2 http://www.bultreebank.org/dpbtb/ http://ufal.mff.cuni.cz/conll2009-st/ task-description.html 3 constraints in a local context”. In our case, all the linguistic features (factors) associated with each token form a supertag to that token. Singh and Bandyopadhyay (2010) had a similar idea of incorporating linguistic features, while they worked on Manipuri-English bidirectional translation. Our approach is slightly different from (Birch et al., 2007) and (Hassan et al., 2007), who mainly used the supertags on the target language side, English. We primarily experiment with the source language side, Bulgarian. This potentially huge feature space provides us with various possibilities of using our linguistic resources developed in and out of our project. In particular, we consider the following factors on the source language side (Bulgarian): • WF - word form is"
W12-0116,D07-1091,0,0.414107,"Missing"
W12-0116,P07-2045,0,0.0120552,"n the last two columns of Table 1. All these factors encoded within the corpus provide us with a rich selection of factors for different experiments. Some of them are presented within the next section. The model of encoding MRS information in the corpus as additional features does not depend on the actual semantic analysis — MRS or RMRS, because both of them provide enough semantic information. 6 Experiments 6.1 Experiments with the Bulgarian raw corpus To run the experiments, we use the phrase-based translation model provided by the open-source statistical machine translation system, Moses4 (Koehn et al., 2007). For training the translation model, the parallel corpora (mentioned in Section 2) were preprocessed with the tokenizer and lowercase converter provided by Moses. Then the procedure is quite standard: • We run GIZA++ (Och and Ney, 2003) for bidirectional word alignment, and then obtain the lexical translation table and phrase table. • A tri-gram language model is estimated using the SRILM toolkit (Stolcke, 2002). • Minimum error rate training (MERT) (Och, 2003) is applied to tune the weights for the set of feature weights that maximizes the official f-score evaluation metric on the developmen"
W12-0116,2009.mtsummit-papers.7,0,0.0526821,"various ways to do the combination/integration. Thurmair (2009) summarized several different architectures of hybrid systems using SMT and RBMT systems. Some widely used ones are: 1) using an SMT to post-edit the outputs of an RBMT; 2) selecting the best translations from several hypotheses coming from different SMT/RBMT systems; and 3) selecting the best segments (phrases or words) from different hypotheses. For the language pair Bulgarian-English, there has not been much study on it, mainly due to the lack of resources, including corpora, preprocessors, etc. There was a system published by Koehn et al. (2009), which was trained and tested on the European Union law data, but not on other domains like news. They reported a very high BLEU score (Papineni et al., 2002) on the BulgarianEnglish translation direction (61.3), which inspired us to further investigate this direction. In this paper, we focus on the Bulgarian-toEnglish translation and mainly explore the approach of annotating the SMT baseline with linguistic features derived from the preprocessing and hand-crafted grammars. There are three motivations behind our approach: 1) the SMT baseline trained on a decent amount of parallel corpora outp"
W12-0116,J10-4005,0,0.0307297,"hine translation (SMT) system (as backbone) with deep linguistic features (as factors). The motivation is to take advantages of the robustness of the SMT system and the linguistic knowledge of morphological analysis and the hand-crafted grammar through system combination approach. The preliminary evaluation has shown very promising results in terms of BLEU scores (38.85) and the manual analysis also confirms the high quality of the translation the system delivers. 1 Introduction In the recent years, machine translation (MT) has achieved significant improvement in terms of translation quality (Koehn, 2010). Both data-driven approaches (e.g., statistical MT (SMT)) and knowledge-based (e.g., rule-based MT (RBMT)) have achieved comparable results shown in the evaluation campaigns (CallisonBurch et al., 2011). However, according to the human evaluation, the final outputs of the MT systems are still far from satisfactory. Fortunately, recent error analysis shows that the two trends of the MT approaches tend to be complementary to each other, in terms of the types of the errors they made (Thurmair, 2005; Chen et al., 2009). Roughly speaking, RBMT systems often have missing lexicon and thus lack of ro"
W12-0116,J03-1002,0,0.00388788,"n in the corpus as additional features does not depend on the actual semantic analysis — MRS or RMRS, because both of them provide enough semantic information. 6 Experiments 6.1 Experiments with the Bulgarian raw corpus To run the experiments, we use the phrase-based translation model provided by the open-source statistical machine translation system, Moses4 (Koehn et al., 2007). For training the translation model, the parallel corpora (mentioned in Section 2) were preprocessed with the tokenizer and lowercase converter provided by Moses. Then the procedure is quite standard: • We run GIZA++ (Och and Ney, 2003) for bidirectional word alignment, and then obtain the lexical translation table and phrase table. • A tri-gram language model is estimated using the SRILM toolkit (Stolcke, 2002). • Minimum error rate training (MERT) (Och, 2003) is applied to tune the weights for the set of feature weights that maximizes the official f-score evaluation metric on the development set. The rest of the parameters we use the default setting provided by Moses. 4 http://www.statmt.org/moses/ We split the corpora into the training set, the development set and the test set. For SETIMES, the split is 100,000/500/1,000"
W12-0116,P03-1021,0,0.0201119,"ments, we use the phrase-based translation model provided by the open-source statistical machine translation system, Moses4 (Koehn et al., 2007). For training the translation model, the parallel corpora (mentioned in Section 2) were preprocessed with the tokenizer and lowercase converter provided by Moses. Then the procedure is quite standard: • We run GIZA++ (Och and Ney, 2003) for bidirectional word alignment, and then obtain the lexical translation table and phrase table. • A tri-gram language model is estimated using the SRILM toolkit (Stolcke, 2002). • Minimum error rate training (MERT) (Och, 2003) is applied to tune the weights for the set of feature weights that maximizes the official f-score evaluation metric on the development set. The rest of the parameters we use the default setting provided by Moses. 4 http://www.statmt.org/moses/ We split the corpora into the training set, the development set and the test set. For SETIMES, the split is 100,000/500/1,000 and for EMEA, it is 700,000/500/1,000. For reference, we also run tests on the JRC-Acquis corpus5 . The final results under the standard evaluation metrics are shown in the following table in terms of BLEU (Papineni et al., 2002)"
W12-0116,2004.tmi-1.2,0,0.372979,"Missing"
W12-0116,2007.tmi-papers.18,0,0.672773,"Missing"
W12-0116,P02-1040,0,0.0879012,"me widely used ones are: 1) using an SMT to post-edit the outputs of an RBMT; 2) selecting the best translations from several hypotheses coming from different SMT/RBMT systems; and 3) selecting the best segments (phrases or words) from different hypotheses. For the language pair Bulgarian-English, there has not been much study on it, mainly due to the lack of resources, including corpora, preprocessors, etc. There was a system published by Koehn et al. (2009), which was trained and tested on the European Union law data, but not on other domains like news. They reported a very high BLEU score (Papineni et al., 2002) on the BulgarianEnglish translation direction (61.3), which inspired us to further investigate this direction. In this paper, we focus on the Bulgarian-toEnglish translation and mainly explore the approach of annotating the SMT baseline with linguistic features derived from the preprocessing and hand-crafted grammars. There are three motivations behind our approach: 1) the SMT baseline trained on a decent amount of parallel corpora outputs surprisingly good results, in terms of both statistical evaluation metrics and preliminary manual evaluation; 2) the augmented model gives 119 Proceedings"
W12-0116,W02-1210,0,0.25698,"R EFERENCE 1 20 20 20 15 15 20 32 45 34 101 19 19 20 19 19 19 15 20 0 0 2 47 48 47 34 38 48 48 41 47 32 49 49 49 50 48 49 41 49 2 0 3 5 5 6 11 12 5 3 7 5 0 4 3 2 3 5 5 10 7 20 5 4 32 37 34 46 51 34 29 23 30 8 34 39 41 31 37 37 44 38 52 51 5 46 40 43 44 34 43 38 34 34 9 44 40 38 47 41 40 40 36 76 94 Total 487 479 483 520 501 482 443 410 433 242 485 482 478 487 483 480 503 471 652 689 Table 4: Manual evaluation of the grammaticality exist quite extensive implemented formal HPSG grammars for English (Copestake and Flickinger, 2000), German (M¨uller and Kasper, 2000), and Japanese (Siegel, 2000; Siegel and Bender, 2002). HPSG is the underlying theory of the international initiative LinGO Grammar Matrix (Bender et al., 2002). At the moment, precise and linguistically motivated grammars, customized on the base of the Grammar Matrix, have been or are being developed for Norwegian, French, Korean, Italian, Modern Greek, Spanish, Portuguese, Chinese, etc. There also exists a first version of the Bulgarian Resource Grammar - BURGER. In the research reported here, we use the linguistic modeled knowledge from the existing English and Bulgarian grammars. Since the Bulgarian grammar has limited coverage on news data,"
W12-0116,W10-3811,0,0.19524,", like lemma, part-of-speech (POS), or other linguistic features. Furthermore, this extension actually allows us to incorporate various kinds of features if they can be (somehow) represented as annotations to the tokens. The process is quite similar to supertagging (Bangalore and Joshi, 1999), which assigns “rich descriptions (supertags) that impose complex 2 http://www.bultreebank.org/dpbtb/ http://ufal.mff.cuni.cz/conll2009-st/ task-description.html 3 constraints in a local context”. In our case, all the linguistic features (factors) associated with each token form a supertag to that token. Singh and Bandyopadhyay (2010) had a similar idea of incorporating linguistic features, while they worked on Manipuri-English bidirectional translation. Our approach is slightly different from (Birch et al., 2007) and (Hassan et al., 2007), who mainly used the supertags on the target language side, English. We primarily experiment with the source language side, Bulgarian. This potentially huge feature space provides us with various possibilities of using our linguistic resources developed in and out of our project. In particular, we consider the following factors on the source language side (Bulgarian): • WF - word form is"
W12-0116,2009.mtsummit-posters.21,0,0.0438651,"ling linguistic phenomena requiring syntactic information better. SMT systems, on Petya Osenova and Kiril Simov Linguistic Modelling Department, IICT Bulgarian Academy of Sciences Sofia, Bulgaria {petya,kivs}@bultreebank.org the contrary, are in general more robust, but sometimes output ungrammatical sentences. In fact, instead of competing with each other, there is also a line of research trying to combine the advantages of the two sides using a hybrid framework. Although many systems can be put under the umbrella of “hybrid” systems, there are various ways to do the combination/integration. Thurmair (2009) summarized several different architectures of hybrid systems using SMT and RBMT systems. Some widely used ones are: 1) using an SMT to post-edit the outputs of an RBMT; 2) selecting the best translations from several hypotheses coming from different SMT/RBMT systems; and 3) selecting the best segments (phrases or words) from different hypotheses. For the language pair Bulgarian-English, there has not been much study on it, mainly due to the lack of resources, including corpora, preprocessors, etc. There was a system published by Koehn et al. (2009), which was trained and tested on the Europea"
W12-4202,P08-1087,0,0.0178885,"04; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token level. Birch et al. (2007) and Hassan et al. (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). However, all of them attempt to enrich the English part of the language pairs being translated. For the language pairs like Bulgarian-English, there has not been much study on it, mainly due to the lack of resources, including corpora, preprocessors, etc, on the Bulgarian part. There was a system published by Koehn et al. (2009), which was trained and tested on the European Union law data, but not on other popular domains like news. They reported a very high BLEU"
W12-4202,W07-0702,0,0.52452,"m deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token level. Birch et al. (2007) and Hassan et al. (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). However, all of them attempt to enrich the English part of the language pairs being translated. For the language pairs like Bulgarian-English, there has not been much study on it, mainly due to the lack of resources, including corpora, preprocessors, etc, on the Bulgarian part. There was a system published by Koehn et al. (2009), which was t"
W12-4202,2005.mtsummit-osmtw.3,0,0.426421,", 2003) and (Copestake, 2007)) and the previous work on transfer of dependency analyses into RMRS structures described in (Spreyer and Frank, 2005) and (Jakob et al., 2010). Although being a semantic representation, MRS is still quite close to the syntactic level, which is not fully language independent. This requires a transfer at the MRS level, if we want to do translation from the source language to the target language. The transfer is usually implemented in the form of rewriting rules. For instance, in the Norwegian LOGON project (Oepen et al., 2004), the transfer rules were hand-written (Bond et al., 2005; Oepen et al., 2007), which included a large amount of manual work. Graham and van Genabith (2008) and Graham et al. (2009) explored the automatic rule induction approach in a transfer-based MT setting involving two lexical functional grammars (LFGs)1 , which was still restricted by the performance of both the parser and the generator. Lack of robustness for target side generation is one of the main issues, when various ill-formed or fragmented structures come out after transfer. Oepen et al. (2007) used their generator to generate text fragments instead of full sentences, in order to increas"
W12-4202,W11-2103,0,0.0593074,"language processing. The same story happens in the machine translation community. Along with the success of statistical machine translation (SMT) models (summarized by Koehn (2010)), various approaches have been proposed to include linguistic information, ranging from early work by Wu (1997) to recent work by Chiang (2010), from deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token level. Birch et al. (2007) and Hassan et al. (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). However, all of them a"
W12-4202,P10-1146,0,0.0203833,"nual analysis confirms the high quality of the translation the system delivers. The whole framework is also extensible for incorporating information provided by different sources. 1 Introduction Incorporating linguistic knowledge into statistical models is an everlasting topic in natural language processing. The same story happens in the machine translation community. Along with the success of statistical machine translation (SMT) models (summarized by Koehn (2010)), various approaches have been proposed to include linguistic information, ranging from early work by Wu (1997) to recent work by Chiang (2010), from deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token le"
W12-4202,copestake-flickinger-2000-open,0,0.223024,"e) kompanii (companies) politicite (politicians-the) zloupotrebyavat (abuse) s (with) dyrzhavnite (state-the) predpriyatiya (enterprises). The translation in the original source is : “electricity audits prove politicians abusing public companies.” The result from the linguistic processing are presented in Table 1. As for the deep linguistic knowledge, we also extract features from the semantic analysis — Minimal Recursion Semantics (MRS). MRS is introduced as an underspecified semantic formalism (Copestake et al., 2005). It is used to support semantic analyses in the English HPSG grammar ERG (Copestake and Flickinger, 2000), but also in other grammar formalisms like LFG. The main idea is that the formalism avoids spelling out the complete set of readings resulting from the interaction of scope bearing operators and quantifiers, instead providing a single underspecified representation from which the complete set of readings can be constructed. Here we will present only basic definitions from (Copestake et al., 2005). For more details the cited publication should be consulted. An MRS structure is a tuple h GT , R, C i, where GT is the top handle, R is a bag of EPs (elementary predicates) and C is a bag of handle c"
W12-4202,W11-2107,0,0.0245914,"el is estimated using the SRILM toolkit (Stolcke, 2002). For the rest of the parameters we use the default setting provided by Moses. Notice that, since on the target language side (i.e., English) we do not have any other factors than the word form, the factor-based models we use here only differentiate from each other in the translation phase, i.e., there is no ‘generation’ models involved. 4.1 Automatic Evaluation Metrics The baseline results (non-factored model) under the standard evaluation metrics are shown in the first row of Table 3 in terms of BLEU (Papineni et al., 2002) and M ETEOR (Denkowski and Lavie, 2011). We then design various configurations to test the effectiveness of different linguistic annotations described in Section 3. The detailed configurations we considered are shown in the first column of Table 3. The first impression is that the BLEU scores in general are high. These models can be roughly 4 http://www.statmt.org/moses/ OPUS — an open source parallel corpus, http:// opus.lingfil.uu.se/ 6 http://optima.jrc.it/Acquis/ 7 We did not preform MERT (Och, 2003), as it is quite computationally heavy for such various configurations. 5 14 grouped into six categories (separated by double line"
W12-4202,N04-1035,0,0.038652,"for incorporating information provided by different sources. 1 Introduction Incorporating linguistic knowledge into statistical models is an everlasting topic in natural language processing. The same story happens in the machine translation community. Along with the success of statistical machine translation (SMT) models (summarized by Koehn (2010)), various approaches have been proposed to include linguistic information, ranging from early work by Wu (1997) to recent work by Chiang (2010), from deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token level. Birch et al. (2007) and Hassan et al. (2007) have shown the effectiveness of adding supertags on the target side, and Av"
W12-4202,E12-1050,1,0.611273,"ating linguistic features, while they worked on Manipuri-English bidirectional translation. Our approach is slightly different from (Birch et al., 2007) and (Hassan et al., 2007), who mainly used the supertags on the target language side, English. Instead, we primarily experiment with the source language side, Bulgarian. This potentially huge feature space provides us with various possibilities of using our linguistic resources developed within and out of our project. Firstly, the data was processed by the NLP pipe for Bulgarian (Savkov et al., 2012) including a morphological tagger, GTagger (Georgiev et al., 2012), a lemmatizer and a dependency parser2 . Then we consider the following factors on the source language side (Bulgarian): • WF – word form is just the original text token. • L EMMA is the lexical invariant of the original word form. We use the lemmatizer, which operates on the output from the POS tagging. Thus, the 3rd person, plural, imperfect tense verb form ‘varvyaha’ (‘walking-were’, They were walking) is lemmatized as the 1st person, present tense verb ‘varvya’. 2 We have trained the MaltParser3 (Nivre et al., 2007) on the dependency version of BulTreeBank: http://www. bultreebank.org/dpb"
W12-4202,2008.eamt-1.10,0,0.210197,"Missing"
W12-4202,P07-1037,0,0.241861,"Missing"
W12-4202,jakob-etal-2010-mapping,0,0.203626,"Missing"
W12-4202,D07-1091,0,0.349531,"ation (SMT) models (summarized by Koehn (2010)), various approaches have been proposed to include linguistic information, ranging from early work by Wu (1997) to recent work by Chiang (2010), from deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token level. Birch et al. (2007) and Hassan et al. (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). However, all of them attempt to enrich the English part of the language pairs being translated. For the language pairs like Bulgarian-English, there ha"
W12-4202,P07-2045,0,0.0097964,"nformation provided by the MRS, e.g., we throw away the scopal information and the other arguments of the relations. Those kinds of information is not straightforward to be represented in such ‘tagging’-style models, which will be tackled in the future. The extra information for the example sentence is represented in Table 2. All these factors encoded within the corpus provide us with a rich selection of features for different experiments. 4 Experiments To run the experiments, we use the phrase-based translation model provided by the open-source statistical machine translation system, Moses4 (Koehn et al., 2007). For training the translation model, the SETIMES parallel corpus has been used, which is part of the OPUS parallel corpus5 . As for the choice of the datasets, the language is more diverse in the news articles, compared with other corpora in more controlled settings, e.g., the JRC-Acquis corpus6 used by Koehn et al. (2009). We split the corpus into the training set and the test set by 150,000 and 1,000 sentence pairs respectively7 . Both datasets are preprocessed with the tokenizer and lowercase converter provided by Moses. Then the procedure is quite standard: We run GIZA++ (Och and Ney, 200"
W12-4202,2009.mtsummit-papers.7,0,0.444698,"token level. Birch et al. (2007) and Hassan et al. (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). However, all of them attempt to enrich the English part of the language pairs being translated. For the language pairs like Bulgarian-English, there has not been much study on it, mainly due to the lack of resources, including corpora, preprocessors, etc, on the Bulgarian part. There was a system published by Koehn et al. (2009), which was trained and tested on the European Union law data, but not on other popular domains like news. They reported a very high BLEU score (Papineni et al., 2002) on the Bulgarian-English translation direction (61.3). Apart from being morphologically-rich, Bulgarian has a number of challenging linguistic phenomena to consider, including free word order, long distance dependency, coreference relations, clitic doubling, etc. For instance, the following two sentences: (1) Momcheto j go dava buketa na Boy-the her-dat it-acc gives bouquet-the to momicheto. girl-the. The boy gives the bouquet t"
W12-4202,J10-4005,0,0.0428019,"morphological analysis as well as the hand-crafted grammar resources. The automatic evaluation has shown promising results and our extensive manual analysis confirms the high quality of the translation the system delivers. The whole framework is also extensible for incorporating information provided by different sources. 1 Introduction Incorporating linguistic knowledge into statistical models is an everlasting topic in natural language processing. The same story happens in the machine translation community. Along with the success of statistical machine translation (SMT) models (summarized by Koehn (2010)), various approaches have been proposed to include linguistic information, ranging from early work by Wu (1997) to recent work by Chiang (2010), from deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT"
W12-4202,P06-1077,0,0.0256157,"ormation provided by different sources. 1 Introduction Incorporating linguistic knowledge into statistical models is an everlasting topic in natural language processing. The same story happens in the machine translation community. Along with the success of statistical machine translation (SMT) models (summarized by Koehn (2010)), various approaches have been proposed to include linguistic information, ranging from early work by Wu (1997) to recent work by Chiang (2010), from deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token level. Birch et al. (2007) and Hassan et al. (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn"
W12-4202,J03-1002,0,0.00273886,"hn et al., 2007). For training the translation model, the SETIMES parallel corpus has been used, which is part of the OPUS parallel corpus5 . As for the choice of the datasets, the language is more diverse in the news articles, compared with other corpora in more controlled settings, e.g., the JRC-Acquis corpus6 used by Koehn et al. (2009). We split the corpus into the training set and the test set by 150,000 and 1,000 sentence pairs respectively7 . Both datasets are preprocessed with the tokenizer and lowercase converter provided by Moses. Then the procedure is quite standard: We run GIZA++ (Och and Ney, 2003) for bi-directional word alignment, and then obtain the lexical translation table and phrase table. A tri-gram language model is estimated using the SRILM toolkit (Stolcke, 2002). For the rest of the parameters we use the default setting provided by Moses. Notice that, since on the target language side (i.e., English) we do not have any other factors than the word form, the factor-based models we use here only differentiate from each other in the translation phase, i.e., there is no ‘generation’ models involved. 4.1 Automatic Evaluation Metrics The baseline results (non-factored model) under t"
W12-4202,P03-1021,0,0.00953047,"rd evaluation metrics are shown in the first row of Table 3 in terms of BLEU (Papineni et al., 2002) and M ETEOR (Denkowski and Lavie, 2011). We then design various configurations to test the effectiveness of different linguistic annotations described in Section 3. The detailed configurations we considered are shown in the first column of Table 3. The first impression is that the BLEU scores in general are high. These models can be roughly 4 http://www.statmt.org/moses/ OPUS — an open source parallel corpus, http:// opus.lingfil.uu.se/ 6 http://optima.jrc.it/Acquis/ 7 We did not preform MERT (Och, 2003), as it is quite computationally heavy for such various configurations. 5 14 grouped into six categories (separated by double lines): word form with linguistic features; lemma with linguistic features; models with dependency features; MRS elementary predicates (EP) and the type of the main argument of the predicate (E OV); EP features without word forms; and EP features with MRS ARGn features. In terms of the resulting scores, POS and Lemma seem to be effective features, as Model 2 has the highest BLEU score and Model 4 the best M ETEOR score. Model 3 indicates that linguistic features also im"
W12-4202,2004.tmi-1.2,0,0.501316,"Missing"
W12-4202,2007.tmi-papers.18,0,0.819256,"Missing"
W12-4202,P02-1040,0,0.0846502,"ocused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). However, all of them attempt to enrich the English part of the language pairs being translated. For the language pairs like Bulgarian-English, there has not been much study on it, mainly due to the lack of resources, including corpora, preprocessors, etc, on the Bulgarian part. There was a system published by Koehn et al. (2009), which was trained and tested on the European Union law data, but not on other popular domains like news. They reported a very high BLEU score (Papineni et al., 2002) on the Bulgarian-English translation direction (61.3). Apart from being morphologically-rich, Bulgarian has a number of challenging linguistic phenomena to consider, including free word order, long distance dependency, coreference relations, clitic doubling, etc. For instance, the following two sentences: (1) Momcheto j go dava buketa na Boy-the her-dat it-acc gives bouquet-the to momicheto. girl-the. The boy gives the bouquet to the girl. (2) Momcheto j go dava. Boy-the her-dat it-acc gives. The boy gives it to her. 10 Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure"
W12-4202,W10-3811,0,0.355234,"llows the system to take a vector of factors to represent each token, both for the source and target languages. The vector of factors can be used for different levels of linguistic annotations, like lemma, part-of-speech, or other linguistic features, if they can be (somehow) represented as annotations to each token. The process is quite similar to supertagging (Bangalore and Joshi, 1999), which assigns “rich descriptions (supertags) that impose complex constraints in a local context”. In our case, all the linguistic features (factors) associated with each token form a supertag to that token. Singh and Bandyopadhyay (2010) had a similar idea of incorporating linguistic features, while they worked on Manipuri-English bidirectional translation. Our approach is slightly different from (Birch et al., 2007) and (Hassan et al., 2007), who mainly used the supertags on the target language side, English. Instead, we primarily experiment with the source language side, Bulgarian. This potentially huge feature space provides us with various possibilities of using our linguistic resources developed within and out of our project. Firstly, the data was processed by the NLP pipe for Bulgarian (Savkov et al., 2012) including a"
W12-4202,J97-3002,0,0.029077,"results and our extensive manual analysis confirms the high quality of the translation the system delivers. The whole framework is also extensible for incorporating information provided by different sources. 1 Introduction Incorporating linguistic knowledge into statistical models is an everlasting topic in natural language processing. The same story happens in the machine translation community. Along with the success of statistical machine translation (SMT) models (summarized by Koehn (2010)), various approaches have been proposed to include linguistic information, ranging from early work by Wu (1997) to recent work by Chiang (2010), from deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate ling"
W12-4202,S10-1009,0,0.0463388,"Missing"
W12-4202,P08-1064,0,0.0203049,"by different sources. 1 Introduction Incorporating linguistic knowledge into statistical models is an everlasting topic in natural language processing. The same story happens in the machine translation community. Along with the success of statistical machine translation (SMT) models (summarized by Koehn (2010)), various approaches have been proposed to include linguistic information, ranging from early work by Wu (1997) to recent work by Chiang (2010), from deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token level. Birch et al. (2007) and Hassan et al. (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused o"
W12-4202,J99-2004,0,\N,Missing
W12-4202,savkov-etal-2012-linguistic,1,\N,Missing
W14-7009,P06-1077,0,0.0422079,"ne Translation (SMT), phrase-based SMT (Koehn et al., 2003), works well on the translation between short phrases as well as on long sentences pairs with similar word orders. However the phrase-based SMT has limited capacity for long distance reordering since it does not consider the syntactic information in the translation. A great deal of recent research enhances the translation results by adding syntactic features, such syntactic-based SMT 64 Proceedings of the 1st Workshop on Asian Translation (WAT2014), pages 64‒68, Tokyo, Japan, 4th October 2014. 2014 Copyright is held by the author(s). (Liu et al., 2006), or forest-based SMT (Mi et al., 2008). The language-independent methods parse the input sentences and then train reordering model from these parsed trees, such as Quirk et al. (2005), Li et al. (2007). These approaches improve the translation considerably but they are time consuming during decoding. Syntactic reordering approaches effectively improve the translation results by transforming the order of sourceside language into target-like order. Previous studies have proposed the syntactic reordering approach which is regarded as a preprocessing step on the source side language. Xia et al. ("
W14-7009,W14-7001,0,0.0332219,"Missing"
W14-7009,P05-1034,0,0.0529734,"the phrase-based SMT has limited capacity for long distance reordering since it does not consider the syntactic information in the translation. A great deal of recent research enhances the translation results by adding syntactic features, such syntactic-based SMT 64 Proceedings of the 1st Workshop on Asian Translation (WAT2014), pages 64‒68, Tokyo, Japan, 4th October 2014. 2014 Copyright is held by the author(s). (Liu et al., 2006), or forest-based SMT (Mi et al., 2008). The language-independent methods parse the input sentences and then train reordering model from these parsed trees, such as Quirk et al. (2005), Li et al. (2007). These approaches improve the translation considerably but they are time consuming during decoding. Syntactic reordering approaches effectively improve the translation results by transforming the order of sourceside language into target-like order. Previous studies have proposed the syntactic reordering approach which is regarded as a preprocessing step on the source side language. Xia et al. (2004) automatically extract reordering rules on source side for English to French translation. Collins et al. (2005) improve the German to English translation by combining morphologica"
W14-7009,P07-1091,0,0.049027,"Missing"
W14-7009,C04-1073,0,0.134474,"Missing"
W14-7009,P05-1066,0,0.0588204,"nces and then train reordering model from these parsed trees, such as Quirk et al. (2005), Li et al. (2007). These approaches improve the translation considerably but they are time consuming during decoding. Syntactic reordering approaches effectively improve the translation results by transforming the order of sourceside language into target-like order. Previous studies have proposed the syntactic reordering approach which is regarded as a preprocessing step on the source side language. Xia et al. (2004) automatically extract reordering rules on source side for English to French translation. Collins et al. (2005) improve the German to English translation by combining morphological and syntactical information into SMT system. Wang et al. (2007) extract the syntactic reordering rules manually on Chinese to English translation. Isozaki et al. (2010) propose a simple head finalized approach to reorder English into Japanese order based on Enju parser. We apply the head finalized method to our system and achieve improvement on the translation from English to Japanese. Dan et al. (2012) carry out a similar head finalized approach for Chinese to Japanese translation using a Chinese Enju parser (Yu et al., 201"
W14-7009,D07-1077,0,0.0280846,"e translation considerably but they are time consuming during decoding. Syntactic reordering approaches effectively improve the translation results by transforming the order of sourceside language into target-like order. Previous studies have proposed the syntactic reordering approach which is regarded as a preprocessing step on the source side language. Xia et al. (2004) automatically extract reordering rules on source side for English to French translation. Collins et al. (2005) improve the German to English translation by combining morphological and syntactical information into SMT system. Wang et al. (2007) extract the syntactic reordering rules manually on Chinese to English translation. Isozaki et al. (2010) propose a simple head finalized approach to reorder English into Japanese order based on Enju parser. We apply the head finalized method to our system and achieve improvement on the translation from English to Japanese. Dan et al. (2012) carry out a similar head finalized approach for Chinese to Japanese translation using a Chinese Enju parser (Yu et al., 2011). We propose a simple reordering methods for Chinese based on the accurate Berkeley Parser (Petrov et al., 2006). 3 We use the open"
W14-7009,W10-1736,0,0.0329172,"Missing"
W14-7009,W12-4207,0,0.0308343,"Missing"
W14-7009,J07-2003,0,0.207892,"Missing"
W14-7009,P06-1055,0,0.392227,"Missing"
W14-7009,N03-1017,0,\N,Missing
W18-6419,N12-1047,0,0.0542077,"r training data. Consequently, for Tr-En and Zh-En we simply trained regular phrase-based models using MSLR (monotone, swap, discontinuous-left, discontinuous-right) lexicalized reordering models and used the default distortion limit of 6. We trained two 4-gram language models: one on the entire monolingual data concatenated to the target side of the parallel data, and another one on the in-domain “News Crawl” corpora only, using LMPLZ (Heafield et al., 2013). For English, all singletons were pruned due to the large size of the monolingual data. To tune the SMT model weights, we used KB-MIRA (Cherry and Foster, 2012) and selected the weights giving the best BLEU score on the development data after 15 decoding runs. 4 target monolingual data i=i+1 source to target NMT target to source NMT synthetic parallel data synthetic parallel data ki=rki-1 sentences ki=rki-1 sentences ki back-translated sentences ki back-translated sentences Figure 1: Our incremental training framework. systems are trained, from scratch, on their respective new training data comprising the mixture of the original parallel data and the synthetic parallel data whose source side is back-translated from the target side. At this stage, we"
W18-6419,P13-2121,0,0.05331,"Missing"
W18-6419,W18-2703,0,0.069259,"the mixture of the synthetic and original parallel data, we back-translate a larger number of monolingual sentences, including the same sentences backtranslated at the first iteration. Since we have better NMT systems than those at the first iteration, we can expect the back-translation to be of a better quality. We mix this new synthetic parallel data to the original one and train again from scratch a source-to-target and a target-to-source NMT systems to obtain further improved translation models. Note that this procedure is partially similar to the work proposed by Zhang et al. (2018) and Hoang et al. (2018), but differs in the sense that we increase incrementally our back-translated data. Given the number of sentences used in the first iteration, k1 , and an expansion factor, r, we determine ki , the number of monolingual sentences back-translated at iteration i, as follows: Back-translation of Monolingual Data 4.1 bilingual parallel data Incremental Back-Translation with Et-En, Fi-En, and Tr-En We introduced an incremental training framework for NMT aiming to iteratively increase the quality and quantity of the synthetic parallel data used for training. In this framework, we first simultaneousl"
W18-6419,P18-4020,0,0.0575771,"Missing"
W18-6419,I17-1016,1,0.84595,"the scores given by right-to-left NMT models that we trained for each translation direction with the same parameters as left-to-right NMT models. The two right-to-left NMT models, each achieving the best BLEU and the best perplexity scores on the development data, were selected, giving us two other features for each translation direction. Since the Tr-En training parallel data are much smaller, we were able to perform one more right-to-left train15 In practice, adding one more right-to-left model for reranking did not significantly improve the BLEU score on the development data. 453 7 score (Zhang et al., 2017) thanks to the small size of the phrase table learned for this language pair. Also only for this language pair, we computed the scores for each hypothesis given by the so-called minimum Bayes risk (MBR) decoding for n-best list using two metrics: sBLEU and chrF++ (Popovi´c, 2017). The reranking framework was trained on n-best lists produced by the decoding of the same development data that we used to validate NMT system’s training and to tune SMT’s model weights. 6 Conclusion We participated in eight translation directions and for all of them we did experiments to compare SMT and NMT performan"
W18-6419,P07-2045,0,0.0179829,"9M (Fi) 4.4M (Tr) 509.9M (Zh) 36.0M (En) 72.8M (En) 5.1M (En) 576.2M (En) Table 1: Statistics of our preprocessed parallel data. Language En Et Fi Tr Zh Table 2: data. #lines #tokens 338.7M 146.1M 177.1M 105.0M 130.5M 7.5B 3.6B 3.2B 1.8B 2.3B Statistics of our preprocessed monolingual used only 100 millions sentence pairs randomly extracted from “Common Crawl.” To tune/validate and evaluate our systems, we used Newstest2016 and Newstest2017 for Fi-En and Tr-En, Newsdev2017 and Newstest2017 for Zh-En, and Newsdev2018 for Et-En. 2.2 Tokenization, Truecasing and Cleaning We used Moses tokenizer (Koehn et al., 2007) and truecaser for English, Estonian, Finnish, and Turkish. The truecaser was trained on one million tokenized lines extracted randomly from the monolingual data. Truecasing was then performed on all the tokenized data. For Chinese, we used Jieba3 for tokenization but did not perform truecasing. For cleaning, we only applied the Moses script clean-n-corpus.perl to remove lines in the parallel data containing more than 80 tokens and replaced characters forbidden by Moses. Note that we did not perform any punctuation normalization. Tables 1 and 2 present the statistics of the parallel and monoli"
W18-6419,W17-3204,0,0.0263216,"rie and Fujita (2018), and despite the simplicity of the method used, combining NMT and SMT makes MT more robust and can significantly improve translation quality, even when SMT greatly underperforms NMT. Following Marie and Fujita (2018), our combination of NMT and SMT works as follows. 5.1 Generation of n-best Lists We first produced the 100-best translation hypotheses with our NMT and SMT systems, independently.11 Unlike Moses, Marian must use a beam of size k to produce a k-best list during decoding. However, using a larger beam size during decoding for NMT may worsen translation quality (Koehn and Knowles, 2017).12 Consequently, we also produced with Marian the 10-best lists, for Zh-En, and 12-best lists for the other language pairs, and merged them with Marian’s 100-best lists to obtain lists containing up to 110 or 112 hypotheses.13 In this way, we make sure that we still have hypotheses of good quality in the lists despite using a larger beam size.14 Then, we merged the lists produced by Marian and Moses. We rescored all the hypotheses in the resulting lists with a reranking framework using features to better model the fluency and the adequacy of each hySetting for Zh-En For the Zh-En language pai"
W18-6419,W18-1811,1,0.930099,"1M 1M 200k 2 2 2 2 2 4 #lines back-translated 10M 20M 40M Table 3: Parameters used for our incremental training. For each language pair, the same parameters were used for both translation directions. In our preliminary experiments, we found that setting r = 2 and k1 very close to, or smaller than, the size of the original parallel data consistently gives good results across language pairs. Fine-tuning r and k1 would result in a better translation quality but at a greater cost. our primary submissions for WMT18 are the results of a simple combination of NMT and SMT. Indeed, as demonstrated by Marie and Fujita (2018), and despite the simplicity of the method used, combining NMT and SMT makes MT more robust and can significantly improve translation quality, even when SMT greatly underperforms NMT. Following Marie and Fujita (2018), our combination of NMT and SMT works as follows. 5.1 Generation of n-best Lists We first produced the 100-best translation hypotheses with our NMT and SMT systems, independently.11 Unlike Moses, Marian must use a beam of size k to produce a k-best list during decoding. However, using a larger beam size during decoding for NMT may worsen translation quality (Koehn and Knowles, 20"
W18-6419,P02-1040,0,0.102926,"ords --valid-metrics ce-mean-words perplexity translation --keep-best --enc-depth 6 --dec-depth 6 --transformer-dropout 0.1 --learn-rate 0.0003 --dropout-src 0.1 --dropout-trg 0.1 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --label-smoothing 0.1 --devices 0 1 2 3 --dim-vocabs 50000 50000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --sync-sgd --tied-embeddings --exponential-smoothing. For ZhEn, we did not use --dropout-src 0.1 --dropout-trg 0.1 since the training data is much larger. We performed NMT decoding with an ensemble of a total of six models according to the best BLEU (Papineni et al., 2002) and the best perplexity scores,7 produced by three independent training runs. #tokens 29.4M (Et) 52.9M (Fi) 4.4M (Tr) 509.9M (Zh) 36.0M (En) 72.8M (En) 5.1M (En) 576.2M (En) Table 1: Statistics of our preprocessed parallel data. Language En Et Fi Tr Zh Table 2: data. #lines #tokens 338.7M 146.1M 177.1M 105.0M 130.5M 7.5B 3.6B 3.2B 1.8B 2.3B Statistics of our preprocessed monolingual used only 100 millions sentence pairs randomly extracted from “Common Crawl.” To tune/validate and evaluate our systems, we used Newstest2016 and Newstest2017 for Fi-En and Tr-En, Newsdev2017 and Newstest2017 for"
W18-6419,W17-4770,0,0.0236645,"Missing"
W18-6419,P16-1009,0,0.397624,"(WMT), Volume 2: Shared Task Papers, pages 449–455 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64046 Language pair Et-En Fi-En Tr-En Zh-En #sent. pairs 1.9M 3.1M 207.4k 24.8M Marian4 (Junczys-Dowmunt et al., 2018) to train and evaluate our NMT systems since it supports state-of-the-art features and is one of the fastest NMT framework publicly available.5 In order to limit the size of the vocabulary of the NMT models, we segmented tokens in the parallel data into subword units via byte pair encoding (BPE) (Sennrich et al., 2016b) using 50k operations. BPE segmentations were jointly learned on the training parallel data for source and target languages, except for Zh-En for which Chinese and English segmentations were trained separately. All our NMT systems for Et-En, Fi-En, and Tr-En were consistently trained on 4 GPUs,6 with the following parameters for Marian: --type transformer --max-length 80 --mini-batch-fit --valid-freq 5000 --save-freq 5000 --workspace 8000 --disp-freq 500 --beam-size 12 --normalize 1 --valid-mini-batch 16 --overwrite --early-stopping 5 --cost-type ce-mean-words --valid-metrics ce-mean-words p"
W18-6419,P16-1162,0,0.730129,"(WMT), Volume 2: Shared Task Papers, pages 449–455 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64046 Language pair Et-En Fi-En Tr-En Zh-En #sent. pairs 1.9M 3.1M 207.4k 24.8M Marian4 (Junczys-Dowmunt et al., 2018) to train and evaluate our NMT systems since it supports state-of-the-art features and is one of the fastest NMT framework publicly available.5 In order to limit the size of the vocabulary of the NMT models, we segmented tokens in the parallel data into subword units via byte pair encoding (BPE) (Sennrich et al., 2016b) using 50k operations. BPE segmentations were jointly learned on the training parallel data for source and target languages, except for Zh-En for which Chinese and English segmentations were trained separately. All our NMT systems for Et-En, Fi-En, and Tr-En were consistently trained on 4 GPUs,6 with the following parameters for Marian: --type transformer --max-length 80 --mini-batch-fit --valid-freq 5000 --save-freq 5000 --workspace 8000 --disp-freq 500 --beam-size 12 --normalize 1 --valid-mini-batch 16 --overwrite --early-stopping 5 --cost-type ce-mean-words --valid-metrics ce-mean-words p"
W18-6489,P18-4020,0,0.08313,"Missing"
W18-6489,W18-2709,0,0.0269454,"00 million and 10 million words and built corresponding NMT systems. Empirical results show that our NMT systems trained on sampled data achieve promising performance. 1 Introduction This paper describes the corpus filtering system built for the participation of the National Institute of Information and Communications Technology (NICT) to the WMT18 shared parallel corpus filtering task. NMT has shown large gains in quality over Statistical machine translation (SMT) and set several new benchmarks (Bojar et al., 2017). However, NMT is much more sensitive to domain (Wang et al., 2017) and noise (Khayrallah and Koehn, 2018). The reason is that NMT is a single neural network structure, which would be affected by each instance during the training procedure (Wang et al., 2017). In comparison, SMT is a combination of distributed models, such as a phrase-table and a language model. Even if some instances in the phrase-table or the language model are noisy, they can only affect part of the models and would not affect the entire system so much. To the best of our knowledge, there are only few works investigating the impact of the noise problem in NMT (Xu and Koehn, 2017; Belinkov and Bisk, 2017). ∗ 2 Task Description W"
W18-6489,W18-6453,0,0.0251529,"etwork structure, which would be affected by each instance during the training procedure (Wang et al., 2017). In comparison, SMT is a combination of distributed models, such as a phrase-table and a language model. Even if some instances in the phrase-table or the language model are noisy, they can only affect part of the models and would not affect the entire system so much. To the best of our knowledge, there are only few works investigating the impact of the noise problem in NMT (Xu and Koehn, 2017; Belinkov and Bisk, 2017). ∗ 2 Task Description WMT18 shared parallel corpus filtering task1 (Koehn et al., 2018) provides a very noisy 1 billion words (English word count) German-English (De-En) corpus crawled from the web as a part of the Paracrawl project. Participants are asked to provide a quality score for each sentence pair in the corpus. Computed scores are then evaluated given the performance of SMT and NMT systems trained on 100M and 10M words sampled from data using the quality scores computed by the participants. newstest2016 is used as the development data and the test data include newstest2018, iwslt2017, Acquis, EMEA, Global Voices, and KDE.2 The statistics of the noisy data to filter are"
W18-6489,P12-3005,0,0.0214345,"Tesla P100 GPUs. Our settings were the same for all of the NMT systems. For each method, we use their score to select the top 100M and 10M sentences to train the corresponding NMT systems. In Table 4, “Original” means the original corpus without any filtering. “Aggressive Filtering” is the method which we introduced in Section 3.1. “Hunalign” indicates the baseline corpus filtering method (Varga et al., 2007)8 given by the organizers. “Classifier” indicates the classifier that we proposed in Section 3.3. “Classifier + LangID” indicates that we also use a language identification tool, LangID (Lui and Baldwin, 2012)9 , to filter the sentence pairs containing sentences that are not German or English. The results were evaluated on the development data newstest2016. Classifier We chose a logistic regression classifier to compute a score for each sentence pair using the features presented in Section 3.2. We trained our classifier on Newstest2014, that we used as positive examples of good sentence pairs, and created the same number of negative examples using the following procedure. We created three-type of negative examples, each of which contains one third of the sentence number of Newstest2014: • Misaligne"
W18-6489,P17-2062,1,0.829142,"3.2 We scored each of the remaining sentence pairs with four NMT transformer models, trained with Marian (Junczys-Dowmunt et al., 2018)4 , on all the parallel data provided for the shared news translation task (excluding the “paracrawl” corpus). We trained left-to-right and right-to-left models for German-to-English and English-toGerman translation directions. We used these four model scores as features in our classifier. We also trained lexical translation probability with Moses and used them to compute a sentencelevel translation probability, for both translation directions, as proposed by Marie and Fujita (2017). To evaluate the semantic similarity between the source and target sentence, we compute a feature based on bilingual word embeddings as follows. First, we trained monolingual word embeddings with FastText (Bojanowski et al., 2017)5 on the monolingual English and German data provided by the WMT organizers. Then, we aligned English and German monolingual word embedding spaces in a bilingual space using the unsupervised method proposed by Artetxe et al. (2018).6 Given the bilingual word embeddings, we computed embeddings for the source and target sentence by doing the element-wise addition of th"
W18-6489,P18-1073,0,0.0238372,"robability with Moses and used them to compute a sentencelevel translation probability, for both translation directions, as proposed by Marie and Fujita (2017). To evaluate the semantic similarity between the source and target sentence, we compute a feature based on bilingual word embeddings as follows. First, we trained monolingual word embeddings with FastText (Bojanowski et al., 2017)5 on the monolingual English and German data provided by the WMT organizers. Then, we aligned English and German monolingual word embedding spaces in a bilingual space using the unsupervised method proposed by Artetxe et al. (2018).6 Given the bilingual word embeddings, we computed embeddings for the source and target sentence by doing the element-wise addition of the bilingual embedding of the words they contain. Finally, we computed the cosine similarity between the embeddings of source and target sentence for each sentence pair, and used it as a feature. Other features are computed to take into account the sentence length: the number of tokens in the source and target sentences, and the difference, and its absolute value, between them. We summarize the features that we used in Table 2. Sentence Pairs Scoring The task"
W18-6489,D17-1155,1,0.758833,"sy data. Finally, we sampled 100 million and 10 million words and built corresponding NMT systems. Empirical results show that our NMT systems trained on sampled data achieve promising performance. 1 Introduction This paper describes the corpus filtering system built for the participation of the National Institute of Information and Communications Technology (NICT) to the WMT18 shared parallel corpus filtering task. NMT has shown large gains in quality over Statistical machine translation (SMT) and set several new benchmarks (Bojar et al., 2017). However, NMT is much more sensitive to domain (Wang et al., 2017) and noise (Khayrallah and Koehn, 2018). The reason is that NMT is a single neural network structure, which would be affected by each instance during the training procedure (Wang et al., 2017). In comparison, SMT is a combination of distributed models, such as a phrase-table and a language model. Even if some instances in the phrase-table or the language model are noisy, they can only affect part of the models and would not affect the entire system so much. To the best of our knowledge, there are only few works investigating the impact of the noise problem in NMT (Xu and Koehn, 2017; Belinkov"
W18-6489,D17-1319,0,0.0246581,"to domain (Wang et al., 2017) and noise (Khayrallah and Koehn, 2018). The reason is that NMT is a single neural network structure, which would be affected by each instance during the training procedure (Wang et al., 2017). In comparison, SMT is a combination of distributed models, such as a phrase-table and a language model. Even if some instances in the phrase-table or the language model are noisy, they can only affect part of the models and would not affect the entire system so much. To the best of our knowledge, there are only few works investigating the impact of the noise problem in NMT (Xu and Koehn, 2017; Belinkov and Bisk, 2017). ∗ 2 Task Description WMT18 shared parallel corpus filtering task1 (Koehn et al., 2018) provides a very noisy 1 billion words (English word count) German-English (De-En) corpus crawled from the web as a part of the Paracrawl project. Participants are asked to provide a quality score for each sentence pair in the corpus. Computed scores are then evaluated given the performance of SMT and NMT systems trained on 100M and 10M words sampled from data using the quality scores computed by the participants. newstest2016 is used as the development data and the test data inclu"
W18-6489,Q17-1010,0,0.0149258,"acrawl” corpus). We trained left-to-right and right-to-left models for German-to-English and English-toGerman translation directions. We used these four model scores as features in our classifier. We also trained lexical translation probability with Moses and used them to compute a sentencelevel translation probability, for both translation directions, as proposed by Marie and Fujita (2017). To evaluate the semantic similarity between the source and target sentence, we compute a feature based on bilingual word embeddings as follows. First, we trained monolingual word embeddings with FastText (Bojanowski et al., 2017)5 on the monolingual English and German data provided by the WMT organizers. Then, we aligned English and German monolingual word embedding spaces in a bilingual space using the unsupervised method proposed by Artetxe et al. (2018).6 Given the bilingual word embeddings, we computed embeddings for the source and target sentence by doing the element-wise addition of the bilingual embedding of the words they contain. Finally, we computed the cosine similarity between the embeddings of source and target sentence for each sentence pair, and used it as a feature. Other features are computed to take"
W19-5313,P18-4020,0,0.0305142,"Missing"
W19-5313,P07-2045,0,0.0106799,"Missing"
W19-5313,W18-6419,1,0.860496,"algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a equal contribution 168 Proceedings of the Fourth Conference on Machine Translation (WMT), Volu"
W19-5313,P02-1040,0,0.103864,"was set to 2048. The number of attention heads in each encoder and decoder layer was set to eight. During training, the value of label smoothing was set to 0.1, and the attention dropout and residual dropout were set to 0.1. The Adam optimizer (Kingma and Ba, 2014) was used to tune the parameters of the model. The learning rate was varied under a warm-up strategy with warm-up steps of 16,000. All NMT models for ZH↔EN tasks were consistently trained on four P100 GPUs. We validated the model with an interval of 5,000 batches on the development set and selected the best model according to BLEU (Papineni et al., 2002) score on the newsdev2018 data set. We performed the following training run independently for five times to obtain the models for ensembling. First, an initial model was trained on the provided parallel data and used to generate pseudo-parallel data through back-translation. A new model was then trained from scratch on the mixture of the original parallel data and the pseudo-parallel data. The new model was further 12 13 Results Table 2 shows the results of ZH↔EN tasks. It is obvious that the back-translation, fine-tuning, and ensemble methods are greatly effective for the ZH↔EN tasks. In part"
W19-5313,Y17-1038,1,0.807473,"ansfer Learning In addition to the approaches in Section 3.1, we also use fine-tuning for transfer learning. Zoph et al. (2016) proposed to train a robust L3→L1 parent model using a large L3–L1 parallel corpus and then fine-tune it on a small L2–L1 corpus to obtain a robust L2→L1 child model. The underlying assumption is that the pre-trained L3→L1 model contains prior probabilities for translation into L1. The prior information is divided into two parts: language modeling information (strong prior) and cross-lingual information (weak or strong depending on the relationship between L3 and L2). Dabre et al. (2017) have shown that linguistically similar L3 and L2 allow for better transfer learning. As such, we transliterate L3 to L2 before pre-training a parent model. This could help in faster convergence, ensure cognate overlap, and potentially lead to a better translation quality. In this participation, we used Hindi as the helping language, L3. Results Refer to rows 1 and 2 of Table 1 for the various automatic evaluation scores. For Kazakh→English our submitted system achieved a cased BLEU score of 26.2 placing our system at 3rd rank out of 9 primary systems. On the other hand, our English→Kazakh per"
W19-5313,P16-1009,0,0.456958,"al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transformer (Vaswani et al., 2"
W19-5313,N16-1101,0,0.279052,", 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transfo"
W19-5313,P16-1162,0,0.806805,"al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transformer (Vaswani et al., 2"
W19-5313,D16-1163,0,0.143248,"English corpus. Chinese↔English translation can benefit from back-translation, model ensembling, and fine-tuning based on the development data. Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which l"
W19-5330,P18-1073,0,0.354971,"sesdecoder 295 we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual word embeddings (BWE), and the 300 target phrases with the highest scores were kept in the phrase table for each source phrase. In total, the induced phrase table contains 90M phrase pairs. BWE of 512 dimensions were obtained using word embeddings trained with fastText9 and aligned in the same space using unsupervised Vecmap (Artetxe et al., 2018b)10 for this induction. In total four scores, to be used as features in the phrase table, for each of these phrase pairs were computed to mimic phrasebased SMT: forward and backward phrase and lexical translation probabilities. Then, the phrase table was plugged into a Moses system that was tuned on the development data using KB-MIRA. We performed four refinement steps to improve the system using at each step 3M synthetic parallel sentences generated by the forward and backward translation systems, instead of using only either forward (Marie and Fujita, 2018b) or backward translations (Artetx"
W19-5330,D18-1399,0,0.473732,"Missing"
W19-5330,P07-2045,0,0.00885169,"Missing"
W19-5330,W17-3204,0,0.021935,"work. To account for hypotheses length, we added the difference, and its absolute value, between the number of tokens in the translation hypothesis and the source sentence. The reranking framework was trained on n-best lists generated by decoding the first 3k sentence pairs of the development data that we also used to validate the training of UNMT and PNMT systems and to tune the weights of USMT models. Table 3: Parameters for training Marian. 4 Generation of n-best Lists 12 We generated n-best with different beam size for decoding since translation quality can decrease with larger beam size (Koehn and Knowles, 2017). https://marian-nmt.github.io/ 297 # Methods de-cs 1 2 Single UNMT system Single USMT system 15.5 11.1 3 4 5 6 7 Single NMT system pseudo-supervised by UNMT Single NMT system pseudo-supervised by USMT Single Pseudo-supervised MT system Ensemble Pseudo-supervised MT system Re-ranking Pseudo-supervised MT system 15.9 15.3 16.2 16.5 17.0 8 9 10 Fine-tuning Pseudo-supervised MT system Fine-tuning Pseudo-supervised MT system + fixed quotes Fine-tuning + re-ranking Pseudo-supervised MT system + fixed quotes 18.7 19.6 20.1 Table 4: BLEU scores of UMT. #10 is our primary system submitted to the organ"
W19-5330,N12-1047,0,0.0485966,"on 4 GPUs for 300,000 iterations, with the parameters listed by Table 3. Combination of PNMT and USMT Our primary submission for the task was the result of a simple combination of PNMT and USMT similarly to what we did last year in our participation to the supervised News Translation Task of WMT18 (Marie et al., 2018). As demonstrated by Marie and Fujita (2018a), and despite the simplicity of the method used, combining NMT and SMT makes MT more robust and can significantly improve translation quality, even though SMT greatly underperforms 11 Reranking Framework and Features We chose KB-MIRA (Cherry and Foster, 2012) as a rescoring framework and used a subset of the features proposed in Marie and Fujita (2018a). All the following features we used are described in details by Marie and Fujita (2018a). It includes the scores given by N PNMT models independently trained. We computed sentencelevel translation probabilities using the lexical translation probabilities learned by mgiza during the training of our USMT system. We also used two 4-gram language models to compute two features for each hypothesis. One is the same language model used by our USMT system while the other is a small model trained on all the"
W19-5330,P17-2061,0,0.0185166,"pus for Kazakh. Statistics of the data preprocessed with Moses are presented in Table 5. Our results are presented in Table 6. In contrast to what we observed for de-cs, unsupervised BWE are too noisy to be used in phrase table induction for USMT. For both en-gu and en-kk, we obtained unexploitable results confirming the conclusions of Søgaard et al. (2018). Switching to supervised BWE improved significantly the translation quality of USMT but Fine-tuning (Luong and Manning, 2015; Sennrich et al., 2016a) is a conventional method for NMT on low-resource language pairs and domainspecific tasks (Chu et al., 2017; Chu and Wang, 2018; Wang et al., 2017a,b). The PNMT model only relying on monolingual corpora was further trained on the parallel development data to improve translation performance. Finally, fixed quotes method was applied to the final Czech translation. 6 Results on the German-to-Czech Task The results of our systems computed for the Newstest2019 test set are presented in Table 4. As Table 4 shows, UNMT systems significantly outperformed our best USMT system according to BLEU. However, compared with pseudosupervised MT model trained only on pseudoparallel corpora generated by either UNMT ("
W19-5330,2015.iwslt-evaluation.11,0,0.0294481,"ge in-domain corpora. For Gujarati and Kazakh, we used Common Crawl and News Crawl corpora, in addition to the provided News Commentary corpus for Kazakh. Statistics of the data preprocessed with Moses are presented in Table 5. Our results are presented in Table 6. In contrast to what we observed for de-cs, unsupervised BWE are too noisy to be used in phrase table induction for USMT. For both en-gu and en-kk, we obtained unexploitable results confirming the conclusions of Søgaard et al. (2018). Switching to supervised BWE improved significantly the translation quality of USMT but Fine-tuning (Luong and Manning, 2015; Sennrich et al., 2016a) is a conventional method for NMT on low-resource language pairs and domainspecific tasks (Chu et al., 2017; Chu and Wang, 2018; Wang et al., 2017a,b). The PNMT model only relying on monolingual corpora was further trained on the parallel development data to improve translation performance. Finally, fixed quotes method was applied to the final Czech translation. 6 Results on the German-to-Czech Task The results of our systems computed for the Newstest2019 test set are presented in Table 4. As Table 4 shows, UNMT systems significantly outperformed our best USMT system a"
W19-5330,C18-1111,1,0.806407,"atistics of the data preprocessed with Moses are presented in Table 5. Our results are presented in Table 6. In contrast to what we observed for de-cs, unsupervised BWE are too noisy to be used in phrase table induction for USMT. For both en-gu and en-kk, we obtained unexploitable results confirming the conclusions of Søgaard et al. (2018). Switching to supervised BWE improved significantly the translation quality of USMT but Fine-tuning (Luong and Manning, 2015; Sennrich et al., 2016a) is a conventional method for NMT on low-resource language pairs and domainspecific tasks (Chu et al., 2017; Chu and Wang, 2018; Wang et al., 2017a,b). The PNMT model only relying on monolingual corpora was further trained on the parallel development data to improve translation performance. Finally, fixed quotes method was applied to the final Czech translation. 6 Results on the German-to-Czech Task The results of our systems computed for the Newstest2019 test set are presented in Table 4. As Table 4 shows, UNMT systems significantly outperformed our best USMT system according to BLEU. However, compared with pseudosupervised MT model trained only on pseudoparallel corpora generated by either UNMT (#3) or USMT (#4), me"
W19-5330,W18-1811,1,0.86005,"result of a simple combination of our unsupervised neural and statistical machine translation systems. Our system is ranked first for the German-to-Czech translation task, using only the data provided by the organizers (“constraint”), according to both BLEU-cased and human evaluation. We also performed contrastive experiments with other language pairs, namely, English-Gujarati and EnglishKazakh, to better assess the effectiveness of unsupervised machine translation in for distant language pairs and in truly low-resource conditions. reranking using different informative features as proposed by Marie and Fujita (2018a). This simple combination method performed the best among unsupervised MT systems at WMT19 by BLEU 1 and human evaluation (Bojar et al., 2019). In addition to the official track, we also present the unsupervised systems for English-Gujariti and English-Kazakh for contrastive experiments with much more distant language pairs. The remainder of this paper is organized as follows. In Section 2, we introduce the data preprocessing. In Section 3, we describe the details of our UNMT, USMT, and pseudosupervised MT systems. Then, the combination of pseudo-supervised NMT and USMT is described in Secti"
W19-5330,N16-1162,0,0.0471887,"The other parameters for training the language model were set as listed in Table 1. Then we trained a Transformer-based UNMT model with the pre-trained cross-lingual language model using XLM toolkit. The auto-encoder of UNMT architecture cannot learn useful knowledge without some constraints; it would merely become a copying task that learns to copy the input words one by one (Lample et al., 2018). To alleviate this issue, we utilized a denoising auto-encoder (Vincent et al., 2010), and added noise in the form of random token swapping in input sentences to improve the model learning ability (Hill et al., 2016; He et al., 2016). The denoising auto-encoder acts as a language model that has been trained in one language and Systems Our entire system is illustrated in Figure 1. 3.1 Unsupervised NMT To build competitive UNMT systems, we chose to rely on the Transformer-based UNMT initialized by a pre-trained cross-lingual language model (Lample and Conneau, 2019) since it had been shown to outperform UNMT initialized with word embeddings, in quality and efficiency. In order to limit the size of the vocabulary of the UNMT model, we segmented tokens in the training data into sub-word units via byte pair e"
W19-5330,W18-6419,1,0.803583,"pairs generated by UNMT. To train this pseudo-supervised NMT (PNMT) system, we chose Marian (Junczys-Dowmunt et al., 2018)11 since it supports state-of-the-art features and is one of the fastest NMT frameworks publicly available. Specifically, the pseudo-supervised NMT system for de-cs was trained on 4 GPUs for 300,000 iterations, with the parameters listed by Table 3. Combination of PNMT and USMT Our primary submission for the task was the result of a simple combination of PNMT and USMT similarly to what we did last year in our participation to the supervised News Translation Task of WMT18 (Marie et al., 2018). As demonstrated by Marie and Fujita (2018a), and despite the simplicity of the method used, combining NMT and SMT makes MT more robust and can significantly improve translation quality, even though SMT greatly underperforms 11 Reranking Framework and Features We chose KB-MIRA (Cherry and Foster, 2012) as a rescoring framework and used a subset of the features proposed in Marie and Fujita (2018a). All the following features we used are described in details by Marie and Fujita (2018a). It includes the scores given by N PNMT models independently trained. We computed sentencelevel translation pr"
W19-5330,P18-4020,0,0.0370204,"Missing"
W19-5330,P16-1009,0,0.474495,"16). The denoising auto-encoder acts as a language model that has been trained in one language and Systems Our entire system is illustrated in Figure 1. 3.1 Unsupervised NMT To build competitive UNMT systems, we chose to rely on the Transformer-based UNMT initialized by a pre-trained cross-lingual language model (Lample and Conneau, 2019) since it had been shown to outperform UNMT initialized with word embeddings, in quality and efficiency. In order to limit the size of the vocabulary of the UNMT model, we segmented tokens in the training data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b). We determined 60k BPE operations jointly on the training monolingual data for German and Czech, and used a shared vocabulary for both languages with 60k tokens based on BPE. We used 50M monolingual corpora to train a 6 https://github.com/facebookresearch/ XLM 7 NVIDIA @ Tesla @ P100 16Gb. 5 https://github.com/moses-smt/ mosesdecoder 295 we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual"
W19-5330,P16-1162,0,0.857696,"16). The denoising auto-encoder acts as a language model that has been trained in one language and Systems Our entire system is illustrated in Figure 1. 3.1 Unsupervised NMT To build competitive UNMT systems, we chose to rely on the Transformer-based UNMT initialized by a pre-trained cross-lingual language model (Lample and Conneau, 2019) since it had been shown to outperform UNMT initialized with word embeddings, in quality and efficiency. In order to limit the size of the vocabulary of the UNMT model, we segmented tokens in the training data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b). We determined 60k BPE operations jointly on the training monolingual data for German and Czech, and used a shared vocabulary for both languages with 60k tokens based on BPE. We used 50M monolingual corpora to train a 6 https://github.com/facebookresearch/ XLM 7 NVIDIA @ Tesla @ P100 16Gb. 5 https://github.com/moses-smt/ mosesdecoder 295 we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual"
W19-5330,P18-1072,0,0.0953411,"Missing"
W19-5330,P17-2089,1,0.888859,"Missing"
W19-5330,D17-1155,1,0.833261,"preprocessed with Moses are presented in Table 5. Our results are presented in Table 6. In contrast to what we observed for de-cs, unsupervised BWE are too noisy to be used in phrase table induction for USMT. For both en-gu and en-kk, we obtained unexploitable results confirming the conclusions of Søgaard et al. (2018). Switching to supervised BWE improved significantly the translation quality of USMT but Fine-tuning (Luong and Manning, 2015; Sennrich et al., 2016a) is a conventional method for NMT on low-resource language pairs and domainspecific tasks (Chu et al., 2017; Chu and Wang, 2018; Wang et al., 2017a,b). The PNMT model only relying on monolingual corpora was further trained on the parallel development data to improve translation performance. Finally, fixed quotes method was applied to the final Czech translation. 6 Results on the German-to-Czech Task The results of our systems computed for the Newstest2019 test set are presented in Table 4. As Table 4 shows, UNMT systems significantly outperformed our best USMT system according to BLEU. However, compared with pseudosupervised MT model trained only on pseudoparallel corpora generated by either UNMT (#3) or USMT (#4), merging pseudo-parall"
wang-li-2012-constructing,N10-1145,0,\N,Missing
wang-li-2012-constructing,wang-sporleder-2010-constructing,1,\N,Missing
wang-li-2012-constructing,D09-1082,1,\N,Missing
wang-li-2012-constructing,P09-1053,0,\N,Missing
wang-li-2012-constructing,P06-1114,0,\N,Missing
wang-li-2012-constructing,W07-1401,0,\N,Missing
wang-li-2012-constructing,P08-1118,0,\N,Missing
wang-li-2012-constructing,D10-1010,0,\N,Missing
wang-li-2012-constructing,W08-0906,0,\N,Missing
wang-sporleder-2010-constructing,D09-1082,1,\N,Missing
wang-sporleder-2010-constructing,W07-1431,0,\N,Missing
wang-sporleder-2010-constructing,W05-1209,0,\N,Missing
wang-sporleder-2010-constructing,W09-3027,0,\N,Missing
wang-sporleder-2010-constructing,N03-1003,0,\N,Missing
wang-sporleder-2010-constructing,W07-1401,0,\N,Missing
wang-zhang-2010-hybrid,N07-1051,0,\N,Missing
wang-zhang-2010-hybrid,W09-1204,1,\N,Missing
wang-zhang-2010-hybrid,C96-1058,0,\N,Missing
wang-zhang-2010-hybrid,H05-1066,0,\N,Missing
wang-zhang-2010-hybrid,W00-1205,0,\N,Missing
wang-zhang-2010-hybrid,W04-3224,0,\N,Missing
Y15-1031,D13-1106,0,0.0236413,", the first pass uses a BNLM in decoding to produce an n-best list. Then, a CSLM is used to rerank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Nearly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this paper. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other studies try to implement neural network LM or translation model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013). 271 The remainder is organized as follows: In Section 2, we will review the background of English to Chinese SMT. The character based SMT will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5. 2 Background The ancient Chinese (or Classical Chinese, 文言文) can be conveniently split into characters, for most characters in ancient Chinese still keep understood by o"
Y15-1031,N09-4002,0,0.0321488,"aseline is designed for Chinese-English translation. In this paper, we follow the same setting of the baseline system, only convert the source language and the target language. 273 BLEU (dev) 42.24 40.64 42.70 42.80 BLEU (dev) 42.80 42.78 42.85 BLEU (test) 39.33 38.08 39.78 40.31 BLEU (test) 40.31 40.04 40.30 Table 3: Different Heuristics Used for Word Alignment 4.2 The N -gram Language Model In this part, we will investigate how the factors in the n-gram LM influence the whole system. The scale of the training corpus is one of the most important factors to LM. And “more data is better data” (Brants and Xu, 2009) has been proved to be one of the most important rules for constructing a LMs. First we randomly divide the whole training sets into 4 parts equally. We build the LM with 1, 2 and 4 parts (i.e. for 1/4, 1/2 and the whole corpus respectively), with other setting as default. Then, we add the dictionary information to the LM. The pr stands for the size of the dictionary and the pf stands for the characters’ frequency in the dictionary. The results in Table 4 show that using the whole corpus 3 It is available at http://www.itl.nist.gov/iad/ mig/tests/mt/2009/ PACLIC 29 Segmentation Methods FMM Seg"
Y15-1031,J93-2003,0,0.0685388,"nd they must combine together into words to make sense. If we split modern Chinese sentences into characters, the semantic meaning in the words will partially lose. Whether or not this semantic function of Chinese word can be partly replaced by the alignment model and Language Model (LM) of character-based SMT will be shown in this paper. Ancient Chinese 三 人 行 ， 则 必 有 我 师 焉 。 Modern Chinese 三个 人 走路 , 那么 一定 存在 我的 老师 在其中 。 English Meaning three people walk , so must be my teacher/tutor there . Table 1: Ancient Chinese and Modern Chinese SMT as a research domain started in the late 1980s at IBM (Brown et al., 1993), which maps individual words to words and allows for deletion and insertion of words. Lately, various researchPACLIC 29 es have shown better translation quality with phrase translation. Phrase-based SMT can be traced back to Och’s alignment template model (Och and Ney, 2004), which can be re-framed as a phrase translation system. Other researchers augmented their systems with phrase translation, such as Yamada and Knight (Yamada and Knight, 2001), who used phrase translation in a syntax-based model. The phrase translation model is based on the noisy channel model. Bayes rule is mostly used to"
Y15-1031,W08-0336,0,0.0950037,"Missing"
Y15-1031,P96-1041,0,0.126614,"systems. Although there are also a lot of other works on automatic evaluation of SMT, such as METEOR (Lavie and Agarwal, 2007), GTM (Melamed et al., 2003) and TER (Snover et al., 2006), whether word or character is more suitable for automatic evaluation of Chinese translation output has not been systematically investigated (Li et al., 2011). Recently, different kinds of characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation method, recently, ContinuousSpace Language Models (CSLM), especially Neural Network Language Models (NNLM) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Wang et al., 2013). These works have shown that CSLMs can improve the BLEU scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are in the same size. However, i"
Y15-1031,D14-1179,0,0.095692,"Missing"
Y15-1031,P14-1129,0,0.0444373,"Missing"
Y15-1031,P14-1066,0,0.0130467,"the two pass approach, or nbest reranking. In this approach, the first pass uses a BNLM in decoding to produce an n-best list. Then, a CSLM is used to rerank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Nearly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this paper. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other studies try to implement neural network LM or translation model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013). 271 The remainder is organized as follows: In Section 2, we will review the background of English to Chinese SMT. The character based SMT will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5. 2 Background The ancient Chinese (or Classical Chinese, 文言文) can be conveniently split into characters, for m"
Y15-1031,P14-1142,1,0.844158,"s are shown in Table 8. n-gram LM 7 7 9 9 9 13 n-gram MERT 4 7 4 7 10 7 BLEU (dev) 42.95 25.54 42.84 25.93 15.82 25.41 4-gram BLEU (test) 40.30 39.91 40.55 40.75 40.37 40.47 Table 8: Parameter Combinations of n-gram LM and ngram MERT At last, the length of n-gram MERT and the smoothing methods are tuned together. The LM is set as 9-gram, the best BLEU score in n-gram LM experiments, and other factors set as default in the toolkits. The results are shown in Table 9. Among different parameters-combined setting, 275 Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), are actively used in SMT (Schwenk et al., 2006; Schwenk et al., 2006; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These models have demonstrated that CSLMs can improve BLEU scores of SMT over n-gram LMs with the same sized corpus for LM training. An attractive feature of CSLMs is that they can predict the probabilities of ngrams outside the training corp"
Y15-1031,D13-1176,0,0.0316504,"tions in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Nearly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this paper. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other studies try to implement neural network LM or translation model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013). 271 The remainder is organized as follows: In Section 2, we will review the background of English to Chinese SMT. The character based SMT will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5. 2 Background The ancient Chinese (or Classical Chinese, 文言文) can be conveniently split into characters, for most characters in ancient Chinese still keep understood by one who only knows modern Chinese (or Written Vernacular Chinese, 白话文) words. For example, “三人行，则必有我师焉。” is one of the popular sentenc"
Y15-1031,P07-2045,0,0.00507061,"not increase as the maximum length of phrases increases. Alignment Parameters union intersect grow-diag-final grow-diag-final-and Maximum Length 7 10 13 Comparison Experiment We use the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data consists of 1 million (M), 2,000, and 2,000 sentences, respectively1 . The basic settings of the NTCIR-9 English to Chinese translation baseline system (Goto et al., 2011) was followed2 . The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. 14 standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores and one LM score. The 1 Since we are the participants of NTCIR-9, so we have the bilingual sides of the evaluation data. 2 We are aware that the original NTCIR patentMT baseline is designed for Chinese-English translation. In this paper, we follow the same setting of the baseline system, only convert the source language and the target language. 273 BLEU (dev) 42.24 40."
Y15-1031,W07-0734,0,0.0366977,"test sets. To evaluate the quality of Chinese translation output, the International Workshop on Spoken Language Translation in 2005 (IWSLT’2005) used the word-level BLEU metric (Papineni et al.,2002). However, IWSLT’08 and NIST’08 adopted character-level evaluation metrics 270 29th Pacific Asia Conference on Language, Information and Computation pages 270 - 280 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Rui Wang, Hai Zhao and Bao-Liang Lu PACLIC 29 to rank the submitted systems. Although there are also a lot of other works on automatic evaluation of SMT, such as METEOR (Lavie and Agarwal, 2007), GTM (Melamed et al., 2003) and TER (Snover et al., 2006), whether word or character is more suitable for automatic evaluation of Chinese translation output has not been systematically investigated (Li et al., 2011). Recently, different kinds of characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better"
Y15-1031,P11-2028,0,0.0198058,"NIST’08 adopted character-level evaluation metrics 270 29th Pacific Asia Conference on Language, Information and Computation pages 270 - 280 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Rui Wang, Hai Zhao and Bao-Liang Lu PACLIC 29 to rank the submitted systems. Although there are also a lot of other works on automatic evaluation of SMT, such as METEOR (Lavie and Agarwal, 2007), GTM (Melamed et al., 2003) and TER (Snover et al., 2006), whether word or character is more suitable for automatic evaluation of Chinese translation output has not been systematically investigated (Li et al., 2011). Recently, different kinds of characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation method, recently, ContinuousSpace Language Models (CSLM), especially Neural Network Language Models (NNLM) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in SMT (Schwenk e"
Y15-1031,P12-1097,0,0.016235,"15 by Rui Wang, Hai Zhao and Bao-Liang Lu PACLIC 29 to rank the submitted systems. Although there are also a lot of other works on automatic evaluation of SMT, such as METEOR (Lavie and Agarwal, 2007), GTM (Melamed et al., 2003) and TER (Snover et al., 2006), whether word or character is more suitable for automatic evaluation of Chinese translation output has not been systematically investigated (Li et al., 2011). Recently, different kinds of characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation method, recently, ContinuousSpace Language Models (CSLM), especially Neural Network Language Models (NNLM) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Wang et al., 2013). These works have shown that CSLMs can improve the BLEU scores of SMT when compared with BNLMs, on the conditio"
Y15-1031,P13-1078,0,0.0159361,"es a BNLM in decoding to produce an n-best list. Then, a CSLM is used to rerank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Nearly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this paper. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other studies try to implement neural network LM or translation model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013). 271 The remainder is organized as follows: In Section 2, we will review the background of English to Chinese SMT. The character based SMT will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5. 2 Background The ancient Chinese (or Classical Chinese, 文言文) can be conveniently split into characters, for most characters in ancient Chinese still keep understood by one who only knows"
Y15-1031,I05-3025,0,0.0484949,"rds of segmentation between word-based and character-based English to Chinese translation are different, as well as the standard of the evaluation of them. That is, the test data contains words as the smallest unit for word-based SMT, and characters for character-based SMT. So the translated sentences of word-based translation will be converted into character-based sentence, and evaluated together with character-based translation BLEU score for fair comparison. We select two popular segmentation segmenters, one of which is based on Forward Maximum Matching (FMM) algorithm with the lexicon of (Low et al., 2005), and the other is based on Conditional Random Fields (CRF) with the same implementation of (Zhao et al., 2006). Because most Chinese words contains 1 to 4 characters, so we set the word-based LM as default trigram in SRILM, and character-based LM for 5-gram. All the different methods share the same other default parameters in the toolkits which will be further introduced in Section 4. There seems to be no ambiguity in different character segmentations, however English characters, numbers and other symbols are also contained in the corpus. If they are split into “characters” like “年 增 长 百 分 之"
Y15-1031,N03-2021,0,0.0628757,"lity of Chinese translation output, the International Workshop on Spoken Language Translation in 2005 (IWSLT’2005) used the word-level BLEU metric (Papineni et al.,2002). However, IWSLT’08 and NIST’08 adopted character-level evaluation metrics 270 29th Pacific Asia Conference on Language, Information and Computation pages 270 - 280 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Rui Wang, Hai Zhao and Bao-Liang Lu PACLIC 29 to rank the submitted systems. Although there are also a lot of other works on automatic evaluation of SMT, such as METEOR (Lavie and Agarwal, 2007), GTM (Melamed et al., 2003) and TER (Snover et al., 2006), whether word or character is more suitable for automatic evaluation of Chinese translation output has not been systematically investigated (Li et al., 2011). Recently, different kinds of characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation metho"
Y15-1031,2012.iwslt-papers.3,0,0.0209289,"BLEU score in n-gram LM experiments, and other factors set as default in the toolkits. The results are shown in Table 9. Among different parameters-combined setting, 275 Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), are actively used in SMT (Schwenk et al., 2006; Schwenk et al., 2006; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These models have demonstrated that CSLMs can improve BLEU scores of SMT over n-gram LMs with the same sized corpus for LM training. An attractive feature of CSLMs is that they can predict the probabilities of ngrams outside the training corpus more accurately. A CSLM implemented in a multi-layer neural network contains four layers: the input layer projects all words in the context hi onto the projection layer (the first hidden layer); the second hidden layer and the output layer achieve the non-liner probability estimation and calculate the LM probability P (wi |hi ) for the given context ("
Y15-1031,J03-1002,0,0.00839469,"ses increases. Alignment Parameters union intersect grow-diag-final grow-diag-final-and Maximum Length 7 10 13 Comparison Experiment We use the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data consists of 1 million (M), 2,000, and 2,000 sentences, respectively1 . The basic settings of the NTCIR-9 English to Chinese translation baseline system (Goto et al., 2011) was followed2 . The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. 14 standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores and one LM score. The 1 Since we are the participants of NTCIR-9, so we have the bilingual sides of the evaluation data. 2 We are aware that the original NTCIR patentMT baseline is designed for Chinese-English translation. In this paper, we follow the same setting of the baseline system, only convert the source language and the target language. 273 BLEU (dev) 42.24 40.64 42.70 42.80 BLEU (dev) 42.80 42.78 42.8"
Y15-1031,J04-4002,0,0.12999,"Model (LM) of character-based SMT will be shown in this paper. Ancient Chinese 三 人 行 ， 则 必 有 我 师 焉 。 Modern Chinese 三个 人 走路 , 那么 一定 存在 我的 老师 在其中 。 English Meaning three people walk , so must be my teacher/tutor there . Table 1: Ancient Chinese and Modern Chinese SMT as a research domain started in the late 1980s at IBM (Brown et al., 1993), which maps individual words to words and allows for deletion and insertion of words. Lately, various researchPACLIC 29 es have shown better translation quality with phrase translation. Phrase-based SMT can be traced back to Och’s alignment template model (Och and Ney, 2004), which can be re-framed as a phrase translation system. Other researchers augmented their systems with phrase translation, such as Yamada and Knight (Yamada and Knight, 2001), who used phrase translation in a syntax-based model. The phrase translation model is based on the noisy channel model. Bayes rule is mostly used to reformulate the translation probability for translating a foreign sentence f into target e as: argmaxe p(e|f ) = argmaxe p(f |e)p(e) (1) This allows for the probabilities of an LM p(e) and a separated translation model p(f |e). During decoding, the foreign input sentence f i"
Y15-1031,P03-1021,0,0.0628172,"ntersect grow-diag-final grow-diag-final-and Maximum Length 7 10 13 Comparison Experiment We use the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data consists of 1 million (M), 2,000, and 2,000 sentences, respectively1 . The basic settings of the NTCIR-9 English to Chinese translation baseline system (Goto et al., 2011) was followed2 . The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. 14 standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores and one LM score. The 1 Since we are the participants of NTCIR-9, so we have the bilingual sides of the evaluation data. 2 We are aware that the original NTCIR patentMT baseline is designed for Chinese-English translation. In this paper, we follow the same setting of the baseline system, only convert the source language and the target language. 273 BLEU (dev) 42.24 40.64 42.70 42.80 BLEU (dev) 42.80 42.78 42.85 BLEU (test) 39.33 38.08 39.78 40."
Y15-1031,P06-2093,0,0.382877,"l., 2011). Recently, different kinds of characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation method, recently, ContinuousSpace Language Models (CSLM), especially Neural Network Language Models (NNLM) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Wang et al., 2013). These works have shown that CSLMs can improve the BLEU scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are in the same size. However, in practice, CSLMs have not been widely used in SMT mainly due to high computational costs of training and using CSLMs. Since the using costs of CSLMs are very high, it is difficult to use CSLMs in decoding directly. A common approach in SMT using CSLMs is the two pass approach, or nbest reranking. In this approach, the first pass"
Y15-1031,W12-2702,0,0.239921,"characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation method, recently, ContinuousSpace Language Models (CSLM), especially Neural Network Language Models (NNLM) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Wang et al., 2013). These works have shown that CSLMs can improve the BLEU scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are in the same size. However, in practice, CSLMs have not been widely used in SMT mainly due to high computational costs of training and using CSLMs. Since the using costs of CSLMs are very high, it is difficult to use CSLMs in decoding directly. A common approach in SMT using CSLMs is the two pass approach, or nbest reranking. In this approach, the first pass uses a BNLM in decoding to produce an n"
Y15-1031,2006.amta-papers.25,0,0.0277773,"put, the International Workshop on Spoken Language Translation in 2005 (IWSLT’2005) used the word-level BLEU metric (Papineni et al.,2002). However, IWSLT’08 and NIST’08 adopted character-level evaluation metrics 270 29th Pacific Asia Conference on Language, Information and Computation pages 270 - 280 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Rui Wang, Hai Zhao and Bao-Liang Lu PACLIC 29 to rank the submitted systems. Although there are also a lot of other works on automatic evaluation of SMT, such as METEOR (Lavie and Agarwal, 2007), GTM (Melamed et al., 2003) and TER (Snover et al., 2006), whether word or character is more suitable for automatic evaluation of Chinese translation output has not been systematically investigated (Li et al., 2011). Recently, different kinds of characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation method, recently, ContinuousSpace L"
Y15-1031,D10-1076,0,0.101292,"ifferent kinds of characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation method, recently, ContinuousSpace Language Models (CSLM), especially Neural Network Language Models (NNLM) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Wang et al., 2013). These works have shown that CSLMs can improve the BLEU scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are in the same size. However, in practice, CSLMs have not been widely used in SMT mainly due to high computational costs of training and using CSLMs. Since the using costs of CSLMs are very high, it is difficult to use CSLMs in decoding directly. A common approach in SMT using CSLMs is the two pass approach, or nbest reranking. In this approach, the first pass uses a BNLM in de"
Y15-1031,N12-1005,0,0.303114,"luation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation method, recently, ContinuousSpace Language Models (CSLM), especially Neural Network Language Models (NNLM) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Wang et al., 2013). These works have shown that CSLMs can improve the BLEU scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are in the same size. However, in practice, CSLMs have not been widely used in SMT mainly due to high computational costs of training and using CSLMs. Since the using costs of CSLMs are very high, it is difficult to use CSLMs in decoding directly. A common approach in SMT using CSLMs is the two pass approach, or nbest reranking. In this approach, the first pass uses a BNLM in decoding to produce an n-best list. Then,"
Y15-1031,D14-1003,0,0.0156578,"ing to produce an n-best list. Then, a CSLM is used to rerank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Nearly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this paper. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other studies try to implement neural network LM or translation model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013). 271 The remainder is organized as follows: In Section 2, we will review the background of English to Chinese SMT. The character based SMT will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5. 2 Background The ancient Chinese (or Classical Chinese, 文言文) can be conveniently split into characters, for most characters in ancient Chinese still keep understood by one who only knows modern Chinese (or Written"
Y15-1031,D13-1140,0,0.0157676,"are very high, it is difficult to use CSLMs in decoding directly. A common approach in SMT using CSLMs is the two pass approach, or nbest reranking. In this approach, the first pass uses a BNLM in decoding to produce an n-best list. Then, a CSLM is used to rerank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Nearly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this paper. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other studies try to implement neural network LM or translation model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013). 271 The remainder is organized as follows: In Section 2, we will review the background of English to Chinese SMT. The character based SMT will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5. 2 Backg"
Y15-1031,D14-1023,1,0.748641,"approach in SMT using CSLMs is a two-pass procedure, or nbest re-ranking. In this approach, the first pass uses a BNLM in decoding to produce an n-best list. Then, a CSLM is used to re-rank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Because CSLM outperforms BNLM in probability estimation accuracy and BNLM outperforms CSLM in computational time. To integrate CSLM more efficiently into decoding, some existing approaches calculate the probabilities of the n-grams before decoding and store them (Wang et al., 2013; Wang et al., 2014; Arsoy et al., 2013; Arsoy et al., 2014) in n-gram format. That is, n-grams from BNLM are used as the input of CSLM, and the output probabilities of CSLM together with the corresponding n-grams of BNLM constitute converted CSLM. The converted CSLM is directly used in SMT, and its decoding speed is as fast as the n-gram LM. From the above tables, we find the most important parameter for character-based English to Chinese translation is the LM, and other parameters just have a minor influence. To verify this observation, we use 9-gram character based CSLM (Schwenk et al., 2006), with 4096 chara"
Y15-1031,P12-2056,0,0.0167352,"onal Basic Research Program of China (No. 2013CB329401), the Science and Technology Commission of Shanghai Municipality (No. 13511500200), the European Union Seventh Framework Program (No. 247619), the Cai Yuanpei Program (CSC fund 201304490199 and 201304490171), and the art and science interdiscipline funds of Shanghai Jiao Tong University, No. 14X190040031, and the Key Project of National Society Science Foundation of China, No. 15-ZDA041. † Word segmentation is necessary in most Chinese language processing doubtlessly, because there are no natural spaces between characters in Chinese text (Xi et al., 2012). It is defined in this paper as character-based segmentation if Chinese sentence is segmented into characters, otherwise as word segmentation. In Statistical Machine Translation (SMT) in which Chinese is target language, few work have shown that better word segmentation will lead to better result in SMT (Zhao et al., 2013; Chang et al., 2008; Zhang et al., 2008). Recently Xi et al. (2012) demonstrate that Chinese character alignment can improve both of alignment quality and translation performance, which also motivates us the hypothesis whether word segmentation is not even necessary for SMT"
Y15-1031,C12-2131,1,0.82672,"9 9 13 n-gram MERT 4 7 4 7 10 7 BLEU (dev) 42.95 25.54 42.84 25.93 15.82 25.41 4-gram BLEU (test) 40.30 39.91 40.55 40.75 40.37 40.47 Table 8: Parameter Combinations of n-gram LM and ngram MERT At last, the length of n-gram MERT and the smoothing methods are tuned together. The LM is set as 9-gram, the best BLEU score in n-gram LM experiments, and other factors set as default in the toolkits. The results are shown in Table 9. Among different parameters-combined setting, 275 Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), are actively used in SMT (Schwenk et al., 2006; Schwenk et al., 2006; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These models have demonstrated that CSLMs can improve BLEU scores of SMT over n-gram LMs with the same sized corpus for LM training. An attractive feature of CSLMs is that they can predict the probabilities of ngrams outside the training corpus more accurately. A CSLM implemented i"
Y15-1031,P01-1067,0,0.127187,"people walk , so must be my teacher/tutor there . Table 1: Ancient Chinese and Modern Chinese SMT as a research domain started in the late 1980s at IBM (Brown et al., 1993), which maps individual words to words and allows for deletion and insertion of words. Lately, various researchPACLIC 29 es have shown better translation quality with phrase translation. Phrase-based SMT can be traced back to Och’s alignment template model (Och and Ney, 2004), which can be re-framed as a phrase translation system. Other researchers augmented their systems with phrase translation, such as Yamada and Knight (Yamada and Knight, 2001), who used phrase translation in a syntax-based model. The phrase translation model is based on the noisy channel model. Bayes rule is mostly used to reformulate the translation probability for translating a foreign sentence f into target e as: argmaxe p(e|f ) = argmaxe p(f |e)p(e) (1) This allows for the probabilities of an LM p(e) and a separated translation model p(f |e). During decoding, the foreign input sentence f is segmented into a sequence of phrases f1i . It is assumed a uniform probability distribution over all possible segmentations. Each foreign phrase fi in f1i is translated into"
Y15-1031,W08-0335,0,0.0307259,"and the Key Project of National Society Science Foundation of China, No. 15-ZDA041. † Word segmentation is necessary in most Chinese language processing doubtlessly, because there are no natural spaces between characters in Chinese text (Xi et al., 2012). It is defined in this paper as character-based segmentation if Chinese sentence is segmented into characters, otherwise as word segmentation. In Statistical Machine Translation (SMT) in which Chinese is target language, few work have shown that better word segmentation will lead to better result in SMT (Zhao et al., 2013; Chang et al., 2008; Zhang et al., 2008). Recently Xi et al. (2012) demonstrate that Chinese character alignment can improve both of alignment quality and translation performance, which also motivates us the hypothesis whether word segmentation is not even necessary for SMT where Chinese as target language. From the view of evaluation, the difference between the word-based segmentation methods will also makes the evaluation of SMT where Chinese as target language confusing. The automatic evaluation methods (such as BLEU and NIST BLEU score) in SMT are mostly based on n-gram precision. If the segmentation of test sets are different,"
Y15-1031,C12-3067,1,0.809099,"8. n-gram LM 7 7 9 9 9 13 n-gram MERT 4 7 4 7 10 7 BLEU (dev) 42.95 25.54 42.84 25.93 15.82 25.41 4-gram BLEU (test) 40.30 39.91 40.55 40.75 40.37 40.47 Table 8: Parameter Combinations of n-gram LM and ngram MERT At last, the length of n-gram MERT and the smoothing methods are tuned together. The LM is set as 9-gram, the best BLEU score in n-gram LM experiments, and other factors set as default in the toolkits. The results are shown in Table 9. Among different parameters-combined setting, 275 Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), are actively used in SMT (Schwenk et al., 2006; Schwenk et al., 2006; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These models have demonstrated that CSLMs can improve BLEU scores of SMT over n-gram LMs with the same sized corpus for LM training. An attractive feature of CSLMs is that they can predict the probabilities of ngrams outside the training corpus more accurately."
Y15-1031,D14-1022,1,0.841286,"ng. In this approach, the first pass uses a BNLM in decoding to produce an n-best list. Then, a CSLM is used to rerank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Nearly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this paper. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other studies try to implement neural network LM or translation model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013). 271 The remainder is organized as follows: In Section 2, we will review the background of English to Chinese SMT. The character based SMT will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5. 2 Background The ancient Chinese (or Classical Chinese, 文言文) can be conveniently split into characters, for most characters in ancient Chinese still k"
Y15-1031,W06-0127,1,0.85061,"Missing"
Y15-1031,D13-1141,0,0.0157977,"is used to rerank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Nearly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this paper. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other studies try to implement neural network LM or translation model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013). 271 The remainder is organized as follows: In Section 2, we will review the background of English to Chinese SMT. The character based SMT will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5. 2 Background The ancient Chinese (or Classical Chinese, 文言文) can be conveniently split into characters, for most characters in ancient Chinese still keep understood by one who only knows modern Chinese (or Written Vernacular Chinese, 白话文) words. For"
Y15-1031,D13-1082,1,\N,Missing
Y15-1052,P96-1041,0,0.0239708,"more Chinese users. However, there are only under 500 pinyin syllables in standard Chinese, mandarin, but over 6,000 common Chinese characters, bringing huge ambiguities for pinyin-to-characters mapping (Jia and Zhao, 2014; Xu and Zhao, 2012; Zhang et al., 2014). Modern pinyin IMEs commonly use sentences-based decoding method (Chen and Lee, 2000; Zhang et al., 2012), that is, generating a Chinese sentence according to a pinyin sequence for disambiguation. The decoding method usually works with the help of statistic language model or other techniques. Back-off N -gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002a) have been broadly used for pinyin IME because of their simplicity and efficiency. Recently, continuous-space language models (CLSMs), especially neural network language models (NNLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2010) are used in more and more natural language processing tasks, and practically outperform BNLMs in prediction accuracy. However, NNLMs are still out of consideration for IMEs according to our best knowledge. The main obstacle about using NNLMs in IME is that it is too timeconsuming to meet the requirement from IME that n"
Y15-1052,P00-1031,0,0.876614,"aracters into computer or other information processing and communication devices, such as mobile phone. People working with computer cannot live without IMEs. With the development and improvement of IMEs, people are paying more and more attention to its efficiency and humanization. Since there are thousands of Chinese characters and only 26 English letters on standard keyboard, we cannot directly input the Chinese characters to the computer by simply hitting keys. A mapping or encoding from Chinese characters to English letters is required, and IME is such a system software to do the mapping (Chen and Lee, 2000; Wu et al., 2009; Zhao et al., 2006). Among various encoding schemes, most IMEs adopt Chinese ∗ Corresponding author pinyin, which is the phonetic representation of Chinese characters through Latin (English) letters. The advantage of pinyin IMEs is that it only requires for knowledge of pinyin rules, which are known by most educated Chinese users. Being easy to learn and use, pinyin IME is becoming the first choice of more and more Chinese users. However, there are only under 500 pinyin syllables in standard Chinese, mandarin, but over 6,000 common Chinese characters, bringing huge ambiguitie"
Y15-1052,P14-1129,0,0.0956997,"Missing"
Y15-1052,O05-4005,0,0.0232479,"at W ∗ = arg max P (W |S) W = arg max W P (W )P (S|W ) P (S) = arg max P (W )P (S|W ) W ∏ ∏ = arg max P (wi |wi−1 ) P (si |wi ) w1 ,w2 ,...,wM w i Basically the core engine of IME is a pipeline of three parts: pinyin segmentation, candidate words fetching and candidate sentence generation. Pinyin segmentation is a word segmentation task which may not be as typical as Chinese word segmentation. Since pinyin has a very small vocabulary that contains about 500 legal pinyin syllables, rule-based segmentation methods are widely used, i.e., backward maximum matching algorithm with additional rules (Goh et al., 2005). Carefully written rules can deal with most of the ambiguous conditions. Pinyin segmentation breaks continuous user input into separated pinyin syllables and passes them on to the next stage, candidate words fetching. It is a table look-up task finding Chinese words corresponding to pinyin syllables. A table of candidate words is built according to pinyin syllables. Each column of 456 wi where P (si |wi ) is the conditional probability mapping a word to its pinyin syllable, and P (wi |wi−1 ) is the transition probability. Here P (si |wi ) is 1 while the word wi is corresponding to the pinyin"
Y15-1052,P14-1142,1,0.611945,"). Among various encoding schemes, most IMEs adopt Chinese ∗ Corresponding author pinyin, which is the phonetic representation of Chinese characters through Latin (English) letters. The advantage of pinyin IMEs is that it only requires for knowledge of pinyin rules, which are known by most educated Chinese users. Being easy to learn and use, pinyin IME is becoming the first choice of more and more Chinese users. However, there are only under 500 pinyin syllables in standard Chinese, mandarin, but over 6,000 common Chinese characters, bringing huge ambiguities for pinyin-to-characters mapping (Jia and Zhao, 2014; Xu and Zhao, 2012; Zhang et al., 2014). Modern pinyin IMEs commonly use sentences-based decoding method (Chen and Lee, 2000; Zhang et al., 2012), that is, generating a Chinese sentence according to a pinyin sequence for disambiguation. The decoding method usually works with the help of statistic language model or other techniques. Back-off N -gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002a) have been broadly used for pinyin IME because of their simplicity and efficiency. Recently, continuous-space language models (CLSMs), especially neural network"
Y15-1052,D10-1076,0,0.0158711,"ommonly use sentences-based decoding method (Chen and Lee, 2000; Zhang et al., 2012), that is, generating a Chinese sentence according to a pinyin sequence for disambiguation. The decoding method usually works with the help of statistic language model or other techniques. Back-off N -gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002a) have been broadly used for pinyin IME because of their simplicity and efficiency. Recently, continuous-space language models (CLSMs), especially neural network language models (NNLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2010) are used in more and more natural language processing tasks, and practically outperform BNLMs in prediction accuracy. However, NNLMs are still out of consideration for IMEs according to our best knowledge. The main obstacle about using NNLMs in IME is that it is too timeconsuming to meet the requirement from IME that needs a real-time response as human-computer interface. 455 29th Pacific Asia Conference on Language, Information and Computation pages 455 - 461 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Shenyuan Chen, Hai Zhao and Rui Wang PACLIC 29 Actually, too long com"
Y15-1052,W10-3815,0,0.027243,"s of neurons, and the non-linear (commonly softmax) output layers, with the same size as the full vocabulary, jointly calculate the probability of each word in the vocabulary (Schwenk, 2007; Wang et al., 2013b; Wang et al., 2013c). Since NNLMs calculate the probabilities of ngrams on the continuous space, it can estimate probabilities for any possible n-grams, without worrying about the zero-probability problem, in comparison with BNLM. Hence, there is no need for backingoff to smaller history. Experiments have shown that the NNLM has lower perplexity than the BNLM trained on the same corpus (Schwenk, 2010; Huang et al., 2013). However, the computational complexity of NNLMs is quite high. To reduce the computational costs, NNLMs are only used to compute the probabilities for the subset containing the most PACLIC 29 frequent words in the vocabulary, called short-list (Schwenk, 2007; Schwenk, 2010); the probabilities of the rest words are given by BNLMs. The probability P (wi |hi ) using short-list is calculated as follows: { Pc (wi |hi )Ps (hi ) if wi ∈ short-list 1−Pc (o|hi ) P (wi |hi ) = Pb (wi |hi ) otherwise where Pc (·) is the probability calculated by NNLM, Pc (o|hi ) is given by the neur"
Y15-1052,D13-1140,0,0.0264383,"eds a real-time response as human-computer interface. 455 29th Pacific Asia Conference on Language, Information and Computation pages 455 - 461 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Shenyuan Chen, Hai Zhao and Rui Wang PACLIC 29 Actually, too long computational time makes the direct integration of NNLMs infeasible for pinyin IMEs. We can hardly image that users have to wait for over 10 seconds to get the result after typing the pinyin sequence. So we have to find another way. Although some work have reduced the decoding time of NNLMs, such as (Arisoy et al., 2014), (Vaswani et al., 2013) and (Devlin et al., 2014), these methods are mainly designed for speech recognition or machine translation and can not be integrated into IME directly. Instead of replacing BNLMs with NNLMs in IME, we propose to use NNLMs to enhance BNLMs, which means recalculating the probabilities of ngrams in the BNLMs with NNLMs. Thus we take advantage of the probabilities provided by NNLMs without increasing its on-site computational cost. Furthermore, we will also demonstrate that our method may indeed improve the prediction performance of pinyin IMEs. In Section 2, we introduce the typical pinyin IME m"
Y15-1052,D13-1082,1,0.860681,"Missing"
Y15-1052,D14-1023,1,0.794921,"Missing"
Y15-1052,C12-2131,1,0.835052,"oding schemes, most IMEs adopt Chinese ∗ Corresponding author pinyin, which is the phonetic representation of Chinese characters through Latin (English) letters. The advantage of pinyin IMEs is that it only requires for knowledge of pinyin rules, which are known by most educated Chinese users. Being easy to learn and use, pinyin IME is becoming the first choice of more and more Chinese users. However, there are only under 500 pinyin syllables in standard Chinese, mandarin, but over 6,000 common Chinese characters, bringing huge ambiguities for pinyin-to-characters mapping (Jia and Zhao, 2014; Xu and Zhao, 2012; Zhang et al., 2014). Modern pinyin IMEs commonly use sentences-based decoding method (Chen and Lee, 2000; Zhang et al., 2012), that is, generating a Chinese sentence according to a pinyin sequence for disambiguation. The decoding method usually works with the help of statistic language model or other techniques. Back-off N -gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002a) have been broadly used for pinyin IME because of their simplicity and efficiency. Recently, continuous-space language models (CLSMs), especially neural network language models (N"
Y15-1052,C12-3067,1,0.843238,"ers through Latin (English) letters. The advantage of pinyin IMEs is that it only requires for knowledge of pinyin rules, which are known by most educated Chinese users. Being easy to learn and use, pinyin IME is becoming the first choice of more and more Chinese users. However, there are only under 500 pinyin syllables in standard Chinese, mandarin, but over 6,000 common Chinese characters, bringing huge ambiguities for pinyin-to-characters mapping (Jia and Zhao, 2014; Xu and Zhao, 2012; Zhang et al., 2014). Modern pinyin IMEs commonly use sentences-based decoding method (Chen and Lee, 2000; Zhang et al., 2012), that is, generating a Chinese sentence according to a pinyin sequence for disambiguation. The decoding method usually works with the help of statistic language model or other techniques. Back-off N -gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002a) have been broadly used for pinyin IME because of their simplicity and efficiency. Recently, continuous-space language models (CLSMs), especially neural network language models (NNLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2010) are used in more and more natural language processing tasks, and pr"
Y15-1052,D14-1022,1,0.836285,"IMEs adopt Chinese ∗ Corresponding author pinyin, which is the phonetic representation of Chinese characters through Latin (English) letters. The advantage of pinyin IMEs is that it only requires for knowledge of pinyin rules, which are known by most educated Chinese users. Being easy to learn and use, pinyin IME is becoming the first choice of more and more Chinese users. However, there are only under 500 pinyin syllables in standard Chinese, mandarin, but over 6,000 common Chinese characters, bringing huge ambiguities for pinyin-to-characters mapping (Jia and Zhao, 2014; Xu and Zhao, 2012; Zhang et al., 2014). Modern pinyin IMEs commonly use sentences-based decoding method (Chen and Lee, 2000; Zhang et al., 2012), that is, generating a Chinese sentence according to a pinyin sequence for disambiguation. The decoding method usually works with the help of statistic language model or other techniques. Back-off N -gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002a) have been broadly used for pinyin IME because of their simplicity and efficiency. Recently, continuous-space language models (CLSMs), especially neural network language models (NNLMs) (Bengio et al.,"
Y15-1052,W06-0127,1,0.867883,"mation processing and communication devices, such as mobile phone. People working with computer cannot live without IMEs. With the development and improvement of IMEs, people are paying more and more attention to its efficiency and humanization. Since there are thousands of Chinese characters and only 26 English letters on standard keyboard, we cannot directly input the Chinese characters to the computer by simply hitting keys. A mapping or encoding from Chinese characters to English letters is required, and IME is such a system software to do the mapping (Chen and Lee, 2000; Wu et al., 2009; Zhao et al., 2006). Among various encoding schemes, most IMEs adopt Chinese ∗ Corresponding author pinyin, which is the phonetic representation of Chinese characters through Latin (English) letters. The advantage of pinyin IMEs is that it only requires for knowledge of pinyin rules, which are known by most educated Chinese users. Being easy to learn and use, pinyin IME is becoming the first choice of more and more Chinese users. However, there are only under 500 pinyin syllables in standard Chinese, mandarin, but over 6,000 common Chinese characters, bringing huge ambiguities for pinyin-to-characters mapping (J"
Y15-2041,P14-1062,0,0.0264218,"ave been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directly. This leads to a nbest reranking method which is available for our paper (Schwenk et al., 2006; Son et al., 2012). 3 The Proposed Approach In this Section, we present a machine learning method to distinguish poor translated sentences from good ones. 3.1 Support Vector Machine For text classification tasks, Many approaches have been proposed (Sebastiani, 2002). Among these approaches, SVM has shown widely applications (Joachims, 1998; Joachims, 1999; Joachims, 2002; Tong and Koller, 2002). And in fol"
Y15-2041,P96-1041,0,0.26339,"to evaluating MT. Their work shows a high accuracy in the classification task. However, 355 the generation of their training and test data should limit to the same SMT system. In this paper, we devote to developing a model that is capable of distinguishing texts generated by multiple sourced SMT systems from human texts. To achieve such an aim, we will introduce quite different types of features such as emotion agreement inside a sentence. In the statistical machine translation systems part, the performance is depended on the LM and translation model. Traditional Back-off n-gram LMs (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a succes"
Y15-2041,P01-1020,0,0.257784,"those poorly SMT-translated sentences, we train an SVM-classifier on a feature space. Most features are linguistically motivated only from the target language side. As only target language is concerned, our model will be facilitated of some direct applications. Among all features, a major part is related to the syntactic parser. The parsing structure of the output sentence is very sensible to the quality of SMT outputs. We therefore especially select these features related to the branching properties of the parse tree. One of the reason is that it had become apparent from failure analysis in (Corston-Oliver et al., 2001) that SMT system output tended to favor rightbranching structures over noun compounding. The remainder of this paper is organized as follows: In Section 2, we will give a quick review on SMT and revelent classification tasks. The SVM approaches and all the features used in our method will be presented in Section 3. Section 4 will give a description on the experiments and an analysis of corresponding results. Last, we will conclude our work in Section 5. 2 Related Work In the classification task part, as our goal is to distinguish sentences with different quality, we are actually working on con"
Y15-2041,de-marneffe-etal-2006-generating,0,0.0148243,"Missing"
Y15-2041,P14-1129,0,0.0225388,"Missing"
Y15-2041,P12-1100,0,0.021064,"to train the prediction model and these features are independent of the source language. Our prediction model presents an indicator to measure how much a sentence generated by a machine translation system looks like a real human translation. Furthermore, the indicator can directly and effectively enhance statistical machine translation systems, which can be proved as BLEU score improvements. 1 Introduction The translation performance of Statistical Machine Translation (SMT) systems has been improved significantly within this decade. However, it is still incomparable to the human translation (Feng et al., 2012; Li et al., 2012). Most translation text generated by SMT systems can be understood in some ∗ Correspondence author. Thank all the reviewers for valuable comments and suggestions on our paper. This work was partially supported by the National Natural Science Foundation of China (No. 61170114, and No. 61272248), the National Basic Research Program of China (No. 2013CB329401), the Science and Technology Commission of Shanghai Municipality (No. 13511500200), the European Union Seventh Framework Program (No. 247619), the Cai Yuanpei Program (CSC fund 201304490199 and 201304490171), and the art an"
Y15-2041,W03-0413,0,0.0579879,"The SVM approaches and all the features used in our method will be presented in Section 3. Section 4 will give a description on the experiments and an analysis of corresponding results. Last, we will conclude our work in Section 5. 2 Related Work In the classification task part, as our goal is to distinguish sentences with different quality, we are actually working on confidence estimation or automatic evaluation of SMT systems (Doddington, 2002; Papineni et al., 2002; Zhang et al., 2014). Early work on automatic evaluation of machine translation text estimates the quality at the word level (Gandrabur and Foster, 2003; Ueffing and Ney, 2005) . Namely, n-gram features played an important role in translation quality differentiation. However, this paper considers deep level of linguistic features such as those derived from parsing tree instead of n-gram features. Liu and Gildea (2005) also used features related to the syntactic parser. Compared with our work, they cared more about detailed syntax properties of the sentences on the parse trees. In this paper, we use less properties but more syntactic structure features. Corston-Oliver et al. (2001) adopted parse tree related features to evaluating MT. Their wo"
Y15-2041,P14-1142,1,0.768995,"same SMT system. In this paper, we devote to developing a model that is capable of distinguishing texts generated by multiple sourced SMT systems from human texts. To achieve such an aim, we will introduce quite different types of features such as emotion agreement inside a sentence. In the statistical machine translation systems part, the performance is depended on the LM and translation model. Traditional Back-off n-gram LMs (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directl"
Y15-2041,D13-1176,0,0.0257905,"Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directly. This leads to a nbest reranking method which is available for our paper (Schwenk et al., 2006; Son et al., 2012). 3 The Proposed Approach In this Section, we present a machine learning method to distinguish poor translated sentences from good ones. 3.1 Support Vector Machine For text classification tasks, Many approaches have been proposed (Sebastiani, 2002). Among these approaches, SVM has shown widely applications (Joachims, 1998; Joachims, 1999; Joachims, 2002; Tong and Kol"
Y15-2041,P07-2045,0,0.00668973,"sentence, the average scoring and standard deviation will be considered: • µemotion (S) • σemotion (S) where S is a sentence with length len. Finally, sizes of the following constituents are measured: • sentence length 357 • parse tree depth • maximal and average NP length • maximal and average Adjective Phrase (ADJP) length • maximal and average Prepositional Phrase (PP) length • maximal and average Adverb Phrase (ADVP) length 4 Experiment 4.1 Classification In this subsection, we will give experiment details of the prediction model. In all of our experiments, the default settings2 of Moses (Koehn et al., 2007) and GIZA++ (Och and Ney, 2003) are used for system building. For each SMT system, a 5-gram LM (Chen and Goodman, 1996) is trained on the target side of training set using IRST LM Toolkit. We use four language pairs from version 7 of the Europarl corpus3 (Koehn, 2005) as our experiment data and train four SMT systems, respec2 In this paper, we build only phrase-based SMT for experiment implementation. However, we believe this method is feasible for other SMT systems, such as syntax-based SMT. 3 http://www.statmt.org/europarl/ PACLIC 29 tively: French-English, German-English, ItalianEnglish and"
Y15-2041,2005.mtsummit-papers.11,0,0.0151579,"maximal and average Adjective Phrase (ADJP) length • maximal and average Prepositional Phrase (PP) length • maximal and average Adverb Phrase (ADVP) length 4 Experiment 4.1 Classification In this subsection, we will give experiment details of the prediction model. In all of our experiments, the default settings2 of Moses (Koehn et al., 2007) and GIZA++ (Och and Ney, 2003) are used for system building. For each SMT system, a 5-gram LM (Chen and Goodman, 1996) is trained on the target side of training set using IRST LM Toolkit. We use four language pairs from version 7 of the Europarl corpus3 (Koehn, 2005) as our experiment data and train four SMT systems, respec2 In this paper, we build only phrase-based SMT for experiment implementation. However, we believe this method is feasible for other SMT systems, such as syntax-based SMT. 3 http://www.statmt.org/europarl/ PACLIC 29 tively: French-English, German-English, ItalianEnglish and Danish-English. Considering the consistency of system and convenience of analysis, all these four systems use English as target language. We use these four systems to generate translation text. We randomly pick 5K sentences from the French corpus, noted as F 1(5K), a"
Y15-2041,P12-2007,0,0.0636712,"Missing"
Y15-2041,W05-0904,0,0.0462891,"part, as our goal is to distinguish sentences with different quality, we are actually working on confidence estimation or automatic evaluation of SMT systems (Doddington, 2002; Papineni et al., 2002; Zhang et al., 2014). Early work on automatic evaluation of machine translation text estimates the quality at the word level (Gandrabur and Foster, 2003; Ueffing and Ney, 2005) . Namely, n-gram features played an important role in translation quality differentiation. However, this paper considers deep level of linguistic features such as those derived from parsing tree instead of n-gram features. Liu and Gildea (2005) also used features related to the syntactic parser. Compared with our work, they cared more about detailed syntax properties of the sentences on the parse trees. In this paper, we use less properties but more syntactic structure features. Corston-Oliver et al. (2001) adopted parse tree related features to evaluating MT. Their work shows a high accuracy in the classification task. However, 355 the generation of their training and test data should limit to the same SMT system. In this paper, we devote to developing a model that is capable of distinguishing texts generated by multiple sourced SM"
Y15-2041,J03-1002,0,0.0113537,"i Yuanpei Program (CSC fund 201304490199 and 201304490171), and the art and science interdiscipline funds of Shanghai Jiao Tong University, No. 14X190040031, and the Key Project of National Society Science Foundation of China, No. 15-ZDA041. † degree but still not good enough. However, a significant proportion of text that exists serious mistakes and even does not make sense, and these text can be easily recognized by human. It is not difficult to understand the reason why SMT systems generate ill-formed or non-sense sentences. SMT systems combine probability models in a log-linear framework (Och and Ney, 2003), where the systems always attempt to find a sentence with the highest probability from the candidates. However, Language Model (LM), such as n-gram LM, and reordering model only have limited capacity to represent context, where sentences with local optimum could often be output. Meanwhile, it can be a very different thing for the entire translation sentence due to complicated semantic and pragmatic issues. Therefore, to improve SMT performance, if poorly translated sentences can be distinguished automatically, it is possible for us to refine these sentences by some extra efforts. In this pape"
Y15-2041,P02-1040,0,0.0939758,"ompounding. The remainder of this paper is organized as follows: In Section 2, we will give a quick review on SMT and revelent classification tasks. The SVM approaches and all the features used in our method will be presented in Section 3. Section 4 will give a description on the experiments and an analysis of corresponding results. Last, we will conclude our work in Section 5. 2 Related Work In the classification task part, as our goal is to distinguish sentences with different quality, we are actually working on confidence estimation or automatic evaluation of SMT systems (Doddington, 2002; Papineni et al., 2002; Zhang et al., 2014). Early work on automatic evaluation of machine translation text estimates the quality at the word level (Gandrabur and Foster, 2003; Ueffing and Ney, 2005) . Namely, n-gram features played an important role in translation quality differentiation. However, this paper considers deep level of linguistic features such as those derived from parsing tree instead of n-gram features. Liu and Gildea (2005) also used features related to the syntactic parser. Compared with our work, they cared more about detailed syntax properties of the sentences on the parse trees. In this paper,"
Y15-2041,P06-2093,0,0.0320453,"pes of features such as emotion agreement inside a sentence. In the statistical machine translation systems part, the performance is depended on the LM and translation model. Traditional Back-off n-gram LMs (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directly. This leads to a nbest reranking method which is available for our paper (Schwenk et al., 2006; Son et al., 2012). 3 The Proposed Approach In this Section, we present a machine learning method to distinguish poor translated"
Y15-2041,D10-1076,0,0.0138226,"hine translation systems part, the performance is depended on the LM and translation model. Traditional Back-off n-gram LMs (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directly. This leads to a nbest reranking method which is available for our paper (Schwenk et al., 2006; Son et al., 2012). 3 The Proposed Approach In this Section, we present a machine learning method to distinguish poor translated sentences from good ones. 3.1 Support Vector Machine For text classification ta"
Y15-2041,N12-1005,0,0.0176925,"ystems part, the performance is depended on the LM and translation model. Traditional Back-off n-gram LMs (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directly. This leads to a nbest reranking method which is available for our paper (Schwenk et al., 2006; Son et al., 2012). 3 The Proposed Approach In this Section, we present a machine learning method to distinguish poor translated sentences from good ones. 3.1 Support Vector Machine For text classification tasks, Many approach"
Y15-2041,2005.eamt-1.35,0,0.0338552,"the features used in our method will be presented in Section 3. Section 4 will give a description on the experiments and an analysis of corresponding results. Last, we will conclude our work in Section 5. 2 Related Work In the classification task part, as our goal is to distinguish sentences with different quality, we are actually working on confidence estimation or automatic evaluation of SMT systems (Doddington, 2002; Papineni et al., 2002; Zhang et al., 2014). Early work on automatic evaluation of machine translation text estimates the quality at the word level (Gandrabur and Foster, 2003; Ueffing and Ney, 2005) . Namely, n-gram features played an important role in translation quality differentiation. However, this paper considers deep level of linguistic features such as those derived from parsing tree instead of n-gram features. Liu and Gildea (2005) also used features related to the syntactic parser. Compared with our work, they cared more about detailed syntax properties of the sentences on the parse trees. In this paper, we use less properties but more syntactic structure features. Corston-Oliver et al. (2001) adopted parse tree related features to evaluating MT. Their work shows a high accuracy"
Y15-2041,D14-1023,1,0.803115,"ranslation model. Traditional Back-off n-gram LMs (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directly. This leads to a nbest reranking method which is available for our paper (Schwenk et al., 2006; Son et al., 2012). 3 The Proposed Approach In this Section, we present a machine learning method to distinguish poor translated sentences from good ones. 3.1 Support Vector Machine For text classification tasks, Many approaches have been proposed (Sebastiani, 2002). Among these appr"
Y15-2041,C12-2131,1,0.851305,"e to developing a model that is capable of distinguishing texts generated by multiple sourced SMT systems from human texts. To achieve such an aim, we will introduce quite different types of features such as emotion agreement inside a sentence. In the statistical machine translation systems part, the performance is depended on the LM and translation model. Traditional Back-off n-gram LMs (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directly. This leads to a nbest reranking metho"
Y15-2041,C12-3067,1,0.807346,"this paper, we devote to developing a model that is capable of distinguishing texts generated by multiple sourced SMT systems from human texts. To achieve such an aim, we will introduce quite different types of features such as emotion agreement inside a sentence. In the statistical machine translation systems part, the performance is depended on the LM and translation model. Traditional Back-off n-gram LMs (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directly. This leads to a n"
Y15-2041,D14-1022,1,0.839959,"er of this paper is organized as follows: In Section 2, we will give a quick review on SMT and revelent classification tasks. The SVM approaches and all the features used in our method will be presented in Section 3. Section 4 will give a description on the experiments and an analysis of corresponding results. Last, we will conclude our work in Section 5. 2 Related Work In the classification task part, as our goal is to distinguish sentences with different quality, we are actually working on confidence estimation or automatic evaluation of SMT systems (Doddington, 2002; Papineni et al., 2002; Zhang et al., 2014). Early work on automatic evaluation of machine translation text estimates the quality at the word level (Gandrabur and Foster, 2003; Ueffing and Ney, 2005) . Namely, n-gram features played an important role in translation quality differentiation. However, this paper considers deep level of linguistic features such as those derived from parsing tree instead of n-gram features. Liu and Gildea (2005) also used features related to the syntactic parser. Compared with our work, they cared more about detailed syntax properties of the sentences on the parse trees. In this paper, we use less propertie"
Y15-2041,D13-1082,1,\N,Missing
Y18-3006,P03-1021,0,0.0360105,"neni et al., 2002) and the perplexity scores, produced by 4 independent training runs. 3.2 SMT We also trained phrase-based SMT systems using Moses. Word alignments and phrase tables were trained on the tokenized parallel data using mgiza. Source-to-target and target-to-source word alignments were symmetrized with the grow-diag -final-and heuristic. We simply trained regular phrase-based models and used the default distortion limit of 6. We trained two 5gram language models on the entire target side of the parallel data, with SRILM (Stolcke, 2002). To tune the SMT model weights, we used MERT (Och, 2003) and selected the weights giving the best BLEU score on the development data. 3.3 Pre-ordering We also tried a classic pre-ordering method for English-to-Myanmar translation task. Specifically, the dependency-based head finalization in Ding et al. (2014) is exactly reproduced in our experiment. The source English part is pre-ordered before being input into NMT and SMT systems. 973 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 4 Results Our systems are ev"
Y18-3006,P02-1040,0,0.109271,"ni-batch 16 --valid-freq 5000 --learn-rate 0.0003 --lr-decay-inv-sqrt 16000 --lr-warmup 16000 --lr-report --sync-sgd --devices 0 1 2 3 --dim-vocabs 50000 50000 --exponential-smoothing 2 https://marian-nmt.github.io/, version 1.4.0 It is fully implemented in pure C++ and supports multiGPU training. 4 NVIDIA® Tesla® P100 16Gb. 3 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings --mini-batch-fit --early-stopping 5 --label-smoothing 0.1 --valid-metrics ce-mean-words perplexity translation We performed NMT decoding with an ensemble of a total of 4 models according to the best BLEU (Papineni et al., 2002) and the perplexity scores, produced by 4 independent training runs. 3.2 SMT We also trained phrase-based SMT systems using Moses. Word alignments and phrase tables were trained on the tokenized parallel data using mgiza. Source-to-target and target-to-source word alignments were symmetrized with the grow-diag -final-and heuristic. We simply trained regular phrase-based models and used the default distortion limit of 6. We trained two 5gram language models on the entire target side of the parallel data, with SRILM (Stolcke, 2002). To tune the SMT model weights, we used MERT (Och, 2003) and sel"
Y18-3006,2014.iwslt-papers.5,1,0.84444,"-to-target and target-to-source word alignments were symmetrized with the grow-diag -final-and heuristic. We simply trained regular phrase-based models and used the default distortion limit of 6. We trained two 5gram language models on the entire target side of the parallel data, with SRILM (Stolcke, 2002). To tune the SMT model weights, we used MERT (Och, 2003) and selected the weights giving the best BLEU score on the development data. 3.3 Pre-ordering We also tried a classic pre-ordering method for English-to-Myanmar translation task. Specifically, the dependency-based head finalization in Ding et al. (2014) is exactly reproduced in our experiment. The source English part is pre-ordered before being input into NMT and SMT systems. 973 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 4 Results Our systems are evaluated on the ALT test set and the results5 are shown in Table 2. Our observations from are as follows: 1) Our NMT (Marian) system performed much better than SMT (Moses) system in My-to-En. That is, nearly 7 BLEU scores. However, there is no significant"
Y18-3006,W18-6419,1,0.806536,"slation task (Nakazawa et al., 2018), specifically Myanmar (My) - English (En) for both translation directions. All of our systems are constrained, i.e., we used only the parallel adata provided by the organizers to train and tune our systems. The remainder of this paper is organized as follows. In Section 2, we present the data preprocessing. In Section 3, we introduce the details 1 of our NMT and SMT systems with pre-ordering technology. Empirical results obtained with our systems are analyzed in Section 4 and we conclude this paper in Section 5. This system is based on our WMT-2018 system (Marie et al., 2018). Corpus train(ALT) train(UCSY) dev(ALT) test(ALT) #lines #tokens (My/En) 17.9K 208.6K 0.9K 1.0K 1.0M / 410.2K 5.8M / 2.6M 57.4K / 22.1K 58.3K / 22.7K We used Moses tokenizer and truecaser for English. The truecaser was trained on the English data, after tokenization. For Myanmar, we used the original tokens. For cleaning, we only applied the Moses script clean-n-corpus.perl to remove lines in the parallel data containing more than 80 tokens and replaced characters forbidden by Moses. 972 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translatio"
zhang-etal-2012-joint,C10-2166,1,\N,Missing
zhang-etal-2012-joint,C10-1122,0,\N,Missing
zhang-etal-2012-joint,W97-1502,0,\N,Missing
zhang-etal-2012-joint,C10-2162,0,\N,Missing
zhang-etal-2012-joint,W09-3032,1,\N,Missing
zhang-etal-2012-joint,W09-2605,1,\N,Missing
zhang-etal-2012-joint,C02-2025,0,\N,Missing
zhang-etal-2012-joint,W02-1502,0,\N,Missing
zhang-etal-2012-joint,Y09-2048,0,\N,Missing
zhang-etal-2012-joint,P05-1041,0,\N,Missing
zhang-etal-2012-joint,W11-3404,1,\N,Missing
