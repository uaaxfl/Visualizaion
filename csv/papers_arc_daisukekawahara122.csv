2021.naacl-srw.15,Multi-Task Learning of Generation and Classification for Emotion-Aware Dialogue Response Generation,2021,-1,-1,2,0,3201,tatsuya ide,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"For a computer to naturally interact with a human, it needs to be human-like. In this paper, we propose a neural response generation model with multi-task learning of generation and classification, focusing on emotion. Our model based on BART (Lewis et al., 2020), a pre-trained transformer encoder-decoder model, is trained to generate responses and recognize emotions simultaneously. Furthermore, we weight the losses for the tasks to control the update of parameters. Automatic evaluations and crowdsourced manual evaluations show that the proposed model makes generated responses more emotionally aware."
2020.nlpcovid19-2.13,A System for Worldwide {COVID}-19 Information Aggregation,2020,-1,-1,8,0,5182,akiko aizawa,Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020,0,"The global pandemic of COVID-19 has made the public pay close attention to related news, covering various domains, such as sanitation, treatment, and effects on education. Meanwhile, the COVID-19 condition is very different among the countries (e.g., policies and development of the epidemic), and thus citizens would be interested in news in foreign countries. We build a system for worldwide COVID-19 information aggregation containing reliable articles from 10 regions in 7 languages sorted by topics. Our reliable COVID-19 related website dataset collected through crowdsourcing ensures the quality of the articles. A neural machine translation module translates articles in other languages into Japanese and English. A BERT-based topic-classifier trained on our article-topic pair dataset helps users find their interested information efficiently by putting articles into different categories."
2020.lrec-1.281,Acquiring Social Knowledge about Personality and Driving-related Behavior,2020,-1,-1,2,1,17224,ritsuko iwai,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we introduce our psychological approach to collect human-specific social knowledge from a text corpus, using NLP techniques. It is often not explicitly described but shared among people, which we call social knowledge. We focus on the social knowledge, especially personality and driving. We used the language resources that were developed based on psychological research methods; a Japanese personality dictionary (317 words) and a driving experience corpus (8,080 sentences) annotated with behavior and subjectivity. Using them, we automatically extracted collocations between personality descriptors and driving-related behavior from a driving behavior and subjectivity corpus (1,803,328 sentences after filtering) and obtained unique 5,334 collocations. To evaluate the collocations as social knowledge, we designed four step-by-step crowdsourcing tasks. They resulted in 266 pieces of social knowledge. They include the knowledge that might be difficult to recall by themselves but easy to agree with. We discuss the acquired social knowledge and the contribution to implementations into systems."
2020.lrec-1.379,Development of a {J}apanese Personality Dictionary based on Psychological Methods,2020,-1,-1,2,1,17224,ritsuko iwai,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We propose a new approach to constructing a personality dictionary with psychological evidence. In this study, we collect personality words, using word embeddings, and construct a personality dictionary with weights for Big Five traits. The weights are calculated based on the responses of the large sample (N=1,938, female = 1,004, M=49.8years old:20-78, SD=16.3). All the respondents answered a 20-item personality questionnaire and 537 personality items derived from word embeddings. We present the procedures to examine the qualities of responses with psychological methods and to calculate the weights. These result in a personality dictionary with two sub-dictionaries. We also discuss an application of the acquired resources."
2020.findings-emnlp.23,Minimize Exposure Bias of {S}eq2{S}eq Models in Joint Entity and Relation Extraction,2020,-1,-1,7,0,4848,ranran zhang,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Joint entity and relation extraction aims to extract relation triplets from plain text directly. Prior work leverages Sequence-to-Sequence (Seq2Seq) models for triplet sequence generation. However, Seq2Seq enforces an unnecessary order on the unordered triplets and involves a large decoding length associated with error accumulation. These methods introduce exposure bias, which may cause the models overfit to the frequent label combination, thus limiting the generalization ability. We propose a novel Sequence-to-Unordered-Multi-Tree (Seq2UMTree) model to minimize the effects of exposure bias by limiting the decoding length to three within a triplet and removing the order among triplets. We evaluate our model on two datasets, DuIE and NYT, and systematically study how exposure bias alters the performance of Seq2Seq models. Experiments show that the state-of-the-art Seq2Seq model overfits to both datasets while Seq2UMTree shows significantly better generalization. Our code is available at \url{https://github.com/WindChimeRan/OpenJERE}."
2020.emnlp-main.192,A Method for Building a Commonsense Inference Dataset based on Basic Events,2020,-1,-1,2,0,16256,kazumasa omura,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"We present a scalable, low-bias, and low-cost method for building a commonsense inference dataset that combines automatic extraction from a corpus and crowdsourcing. Each problem is a multiple-choice question that asks contingency between basic events. We applied the proposed method to a Japanese corpus and acquired 104k problems. While humans can solve the resulting problems with high accuracy (88.9{\%}), the accuracy of a high-performance transfer learning model is reasonably low (76.0{\%}). We also confirmed through dataset analysis that the resulting dataset contains low bias. We released the datatset to facilitate language understanding research."
2020.coling-main.114,{BERT}-based Cohesion Analysis of {J}apanese Texts,2020,-1,-1,2,0,16260,nobuhiro ueda,Proceedings of the 28th International Conference on Computational Linguistics,0,"The meaning of natural language text is supported by cohesion among various kinds of entities, including coreference relations, predicate-argument structures, and bridging anaphora relations. However, predicate-argument structures for nominal predicates and bridging anaphora relations have not been studied well, and their analyses have been still very difficult. Recent advances in neural networks, in particular, self training-based language models including BERT (Devlin et al., 2019), have significantly improved many natural language processing tasks, making it possible to dive into the study on analysis of cohesion in the whole text. In this study, we tackle an integrated analysis of cohesion in Japanese texts. Our results significantly outperformed existing studies in each task, especially about 10 to 20 point improvement both for zero anaphora and coreference resolution. Furthermore, we also showed that coreference resolution is different in nature from the other tasks and should be treated specially."
2020.acl-srw.31,Building a {J}apanese Typo Dataset from {W}ikipedia{'}s Revision History,2020,-1,-1,3,0,16259,yu tanaka,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"User generated texts contain many typos for which correction is necessary for NLP systems to work. Although a large number of typo{--}correction pairs are needed to develop a data-driven typo correction system, no such dataset is available for Japanese. In this paper, we extract over half a million Japanese typo{--}correction pairs from Wikipedia{'}s revision history. Unlike other languages, Japanese poses unique challenges: (1) Japanese texts are unsegmented so that we cannot simply apply a spelling checker, and (2) the way people inputting kanji logographs results in typos with drastically different surface forms from correct ones. We address them by combining character-based extraction rules, morphological analyzers to guess readings, and various filtering methods. We evaluate the dataset using crowdsourcing and run a baseline seq2seq model for typo correction."
W19-6704,Applying Machine Translation to Psychology: Automatic Translation of Personality Adjectives,2019,0,0,2,1,17224,ritsuko iwai,"Proceedings of Machine Translation Summit XVII: Translator, Project and User Tracks",0,None
N19-1281,Shrinking {J}apanese Morphological Analyzers With Neural Networks and Semi-supervised Learning,2019,0,0,2,1,26214,arseny tolmachev,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"For languages without natural word boundaries, like Japanese and Chinese, word segmentation is a prerequisite for downstream analysis. For Japanese, segmentation is often done jointly with part of speech tagging, and this process is usually referred to as morphological analysis. Morphological analyzers are trained on data hand-annotated with segmentation boundaries and part of speech tags. A segmentation dictionary or character n-gram information is also provided as additional inputs to the model. Incorporating this extra information makes models large. Modern neural morphological analyzers can consume gigabytes of memory. We propose a compact alternative to these cumbersome approaches which do not rely on any externally provided n-gram or word representations. The model uses only unigram character embeddings, encodes them using either stacked bi-LSTM or a self-attention network, and independently infers both segmentation and part of speech information. The model is trained in an end-to-end and semi-supervised fashion, on labels produced by a state-of-the-art analyzer. We demonstrate that the proposed technique rivals performance of a previous dictionary-based state-of-the-art approach and can even surpass it when training with the combination of human-annotated and automatically-annotated data. Our model itself is significantly smaller than the dictionary-based one: it uses less than 15 megabytes of space."
D19-6014,Diversity-aware Event Prediction based on a Conditional Variational Autoencoder with Reconstruction,2019,0,0,4,1,4549,hirokazu kiyomaru,Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing,0,"Typical event sequences are an important class of commonsense knowledge. Formalizing the task as the generation of a next event conditioned on a current event, previous work in event prediction employs sequence-to-sequence (seq2seq) models. However, what can happen after a given event is usually diverse, a fact that can hardly be captured by deterministic models. In this paper, we propose to incorporate a conditional variational autoencoder (CVAE) into seq2seq for its ability to represent diverse next events as a probabilistic distribution. We further extend the CVAE-based seq2seq with a reconstruction mechanism to prevent the model from concentrating on highly typical events. To facilitate fair and systematic evaluation of the diversity-aware models, we also extend existing evaluation datasets by tying each current event to multiple next events. Experiments show that the CVAE-based models drastically outperform deterministic models in terms of precision and that the reconstruction mechanism improves the recall of CVAE-based models without sacrificing precision."
D19-5814,Machine Comprehension Improves Domain-Specific {J}apanese Predicate-Argument Structure Analysis,2019,0,0,3,0,26490,norio takahashi,Proceedings of the 2nd Workshop on Machine Reading for Question Answering,0,"To improve the accuracy of predicate-argument structure (PAS) analysis, large-scale training data and knowledge for PAS analysis are indispensable. We focus on a specific domain, specifically Japanese blogs on driving, and construct two wide-coverage datasets as a form of QA using crowdsourcing: a PAS-QA dataset and a reading comprehension QA (RC-QA) dataset. We train a machine comprehension (MC) model based on these datasets to perform PAS analysis. Our experiments show that a stepwise training method is the most effective, which pre-trains an MC model based on the RC-QA dataset to acquire domain knowledge and then fine-tunes based on the PAS-QA dataset."
D19-1241,Tree-structured Decoding for Solving Math Word Problems,2019,0,0,4,1,16253,qianying liu,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Automatically solving math word problems is an interesting research topic that needs to bridge natural language descriptions and formal math equations. Previous studies introduced end-to-end neural network methods, but these approaches did not efficiently consider an important characteristic of the equation, i.e., an abstract syntax tree. To address this problem, we propose a tree-structured decoding method that generates the abstract syntax tree of the equation in a top-down manner. In addition, our approach can automatically stop during decoding without a redundant stop token. The experimental results show that our method achieves single model state-of-the-art performance on Math23K, which is the largest dataset on this task."
Y18-1026,Annotating a Driving Experience Corpus with Behavior and Subjectivity,2018,-1,-1,2,1,17224,ritsuko iwai,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
P18-1044,Neural Adversarial Training for Semi-supervised {J}apanese Predicate-argument Structure Analysis,2018,0,0,2,1,7366,shuhei kurita,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Japanese predicate-argument structure (PAS) analysis involves zero anaphora resolution, which is notoriously difficult. To improve the performance of Japanese PAS analysis, it is straightforward to increase the size of corpora annotated with PAS. However, since it is prohibitively expensive, it is promising to take advantage of a large amount of raw corpora. In this paper, we propose a novel Japanese PAS analysis model based on semi-supervised adversarial training with a raw corpus. In our experiments, our model outperforms existing state-of-the-art models for Japanese PAS analysis."
N18-2041,Knowledge-Enriched Two-Layered Attention Network for Sentiment Analysis,2018,14,0,2,0,24889,abhishek kumar,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,We propose a novel two-layered attention network based on Bidirectional Long Short-Term Memory for sentiment analysis. The novel two-layered attention network takes advantage of the external knowledge bases to improve the sentiment prediction. It uses the Knowledge Graph Embedding generated using the WordNet. We build our model by combining the two-layered attention network with the supervised model based on Support Vector Regression using a Multilayer Perceptron network for sentiment analysis. We evaluate our model on the benchmark dataset of SemEval 2017 Task 5. Experimental results show that the proposed model surpasses the top system of SemEval 2017 Task 5. The model performs significantly better by improving the state-of-the-art system at SemEval 2017 Task 5 by 1.7 and 3.7 points for sub-tracks 1 and 2 respectively.
L18-1050,Comprehensive Annotation of Various Types of Temporal Information on the Time Axis,2018,0,0,2,0,29559,tomohiro sakaguchi,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1223,{JFCKB}: {J}apanese Feature Change Knowledge Base,2018,0,0,2,1,29745,tetsuaki nakamura,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1461,{JDCFC}: A {J}apanese Dialogue Corpus with Feature Changes,2018,0,0,2,1,29745,tetsuaki nakamura,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1637,Improving Crowdsourcing-Based Annotation of {J}apanese Discourse Relations,2018,0,0,4,1,16897,yudai kishimoto,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-2010,{J}uman++: A Morphological Analysis Toolkit for Scriptio Continua,2018,0,2,2,1,26214,arseny tolmachev,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,We present a three-part toolkit for developing morphological analyzers for languages without natural word boundaries. The first part is a C++11/14 lattice-based morphological analysis library that uses a combination of linear and recurrent neural net language models for analysis. The other parts are a tool for exposing problems in the trained model and a partial annotation tool. Our morphological analyzer of Japanese achieves new SOTA on Jumandic-based corpora while being 250 times faster than the previous one. We also perform a small experiment and quantitive analysis and experience of using development tools. All components of the toolkit is open source and available under a permissive Apache 2 License.
C18-1128,Cross-lingual Knowledge Projection Using Machine Translation and Target-side Knowledge Base Completion,2018,0,3,3,1,20394,naoki otani,Proceedings of the 27th International Conference on Computational Linguistics,0,"Considerable effort has been devoted to building commonsense knowledge bases. However, they are not available in many languages because the construction of KBs is expensive. To bridge the gap between languages, this paper addresses the problem of projecting the knowledge in English, a resource-rich language, into other languages, where the main challenge lies in projection ambiguity. This ambiguity is partially solved by machine translation and target-side knowledge base completion, but neither of them is adequately reliable by itself. We show their combination can project English commonsense knowledge into Japanese and Chinese with high precision. Our method also achieves a top-10 accuracy of 90{\%} on the crowdsourced English{--}Japanese benchmark. Furthermore, we use our method to obtain 18,747 facts of accurate Japanese commonsense within a very short period."
W17-6301,Automatically Acquired Lexical Knowledge Improves {J}apanese Joint Morphological and Dependency Analysis,2017,24,1,1,1,3202,daisuke kawahara,Proceedings of the 15th International Conference on Parsing Technologies,0,"This paper presents a joint model for morphological and dependency analysis based on automatically acquired lexical knowledge. This model takes advantage of rich lexical knowledge to simultaneously resolve word segmentation, POS, and dependency ambiguities. In our experiments on Japanese, we show the effectiveness of our joint model over conventional pipeline models."
P17-1111,Neural Joint Model for Transition-based {C}hinese Syntactic Analysis,2017,25,12,2,1,7366,shuhei kurita,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We present neural network-based joint models for Chinese word segmentation, POS tagging and dependency parsing. Our models are the first neural approaches for fully joint Chinese analysis that is known to prevent the error propagation problem of pipeline models. Although word embeddings play a key role in dependency parsing, they cannot be applied directly to the joint task in the previous work. To address this problem, we propose embeddings of character strings, in addition to words. Experiments show that our models outperform existing systems in Chinese word segmentation and POS tagging, and perform preferable accuracies in dependency parsing. We also explore bi-LSTM models with fewer features."
E17-1054,Improving {C}hinese Semantic Role Labeling using High-quality Surface and Deep Case Frames,2017,11,0,2,1,33036,gongye jin,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"This paper presents a method for applying automatically acquired knowledge to semantic role labeling (SRL). We use a large amount of automatically extracted knowledge to improve the performance of SRL. We present two varieties of knowledge, which we call surface case frames and deep case frames. Although the surface case frames are compiled from syntactic parses and can be used as rich syntactic knowledge, they have limited capability for resolving semantic ambiguity. To compensate the deficiency of the surface case frames, we compile deep case frames from automatic semantic roles. We also consider quality management for both types of knowledge in order to get rid of the noise brought from the automatic analyses. The experimental results show that Chinese SRL can be improved using automatically acquired knowledge and the quality management shows a positive effect on this task."
W16-5407,{SCTB}: A {C}hinese Treebank in Scientific Domain,2016,0,0,3,0.25,293,chenhui chu,Proceedings of the 12th Workshop on {A}sian Language Resources ({ALR}12),0,"Treebanks are curial for natural language processing (NLP). In this paper, we present our work for annotating a Chinese treebank in scientific domain (SCTB), to address the problem of the lack of Chinese treebanks in this domain. Chinese analysis and machine translation experiments conducted using this treebank indicate that the annotated treebank can significantly improve the performance on both tasks. This treebank is released to promote Chinese NLP research in scientific domain."
W16-4402,Large-Scale Acquisition of Commonsense Knowledge via a Quiz Game on a Dialogue System,2016,15,3,2,1,20394,naoki otani,Proceedings of the Open Knowledge Base and Question Answering Workshop ({OKBQA} 2016),0,"Commonsense knowledge is essential for fully understanding language in many situations. We acquire large-scale commonsense knowledge from humans using a game with a purpose (GWAP) developed on a smartphone spoken dialogue system. We transform the manual knowledge acquisition process into an enjoyable quiz game and have collected over 150,000 unique commonsense facts by gathering the data of more than 70,000 players over eight months. In this paper, we present a simple method for maintaining the quality of acquired knowledge and an empirical analysis of the knowledge acquisition process. To the best of our knowledge, this is the first work to collect large-scale knowledge via a GWAP on a widely-used spoken dialogue system."
W16-1316,Design of Word Association Games using Dialog Systems for Acquisition of Word Association Knowledge,2016,10,2,2,0,34034,yuichiro machida,Proceedings of the 5th Workshop on Automated Knowledge Base Construction,0,None
W16-1006,Constructing a Dictionary Describing Feature Changes of Arguments in Event Sentences,2016,13,0,2,1,29745,tetsuaki nakamura,Proceedings of the Fourth Workshop on Events,0,None
S16-2012,Leveraging {V}erb{N}et to build Corpus-Specific Verb Clusters,2016,13,0,4,0,34180,daniel peterson,Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics,0,"In this paper, we aim to close the gap from extensive, human-built semantic resources and corpus-driven unsupervised models. The particular resource explored here is VerbNet, whose organizing principle is that semantics and syntax are linked. To capture patterns of usage that can augment knowledge resources like VerbNet, we expand a Dirichlet process mixture model to predict a VerbNet class for each sense of each verb, allowing us to incorporate annotated VerbNet data to guide the clustering process. The resulting clusters align more closely to hand-curated syntactic/semantic groupings than any previous models, and can be adapted to new domains since they require only corpus counts."
S16-1178,{M}2{L} at {S}em{E}val-2016 Task 8: {AMR} Parsing with Neural Networks,2016,16,6,2,0,23339,yevgeniy puzikov,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"This paper describes our contribution to the SemEval 2016 Workshop. We participated in the Shared Task 8 on Meaning Representation parsing using a transition-based approach, which builds upon the system of Wang et al. (2015a) and Wang et al. (2015b), with additions that utilize a Feedforward Neural Network classifier and an enriched feature set. We observed that exploiting Neural Networks in"
P16-1117,Neural Network-Based Model for {J}apanese Predicate Argument Structure Analysis,2016,16,6,2,0.438327,21565,tomohide shibata,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
D16-1049,{IRT}-based Aggregation Model of Crowdsourced Pairwise Comparison for Evaluating Machine Translations,2016,13,0,3,1,20394,naoki otani,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1029,"Consistent Word Segmentation, Part-of-Speech Tagging and Dependency Labelling Annotation for {C}hinese Language",2016,10,0,5,1,35693,mo shen,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we propose a new annotation approach to Chinese word segmentation, part-of-speech (POS) tagging and dependency labelling that aims to overcome the two major issues in traditional morphology-based annotation: Inconsistency and data sparsity. We re-annotate the Penn Chinese Treebank 5.0 (CTB5) and demonstrate the advantages of this approach compared to the original CTB5 annotation through word segmentation, POS tagging and machine translation experiments."
W15-3119,{C}hinese Semantic Role Labeling using High-quality Syntactic Knowledge,2015,19,0,2,1,33036,gongye jin,Proceedings of the Eighth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper presents an application of Chinese syntactic knowledge for semantic role labeling (SRL). Besides basic morphological information, syntactic structures are crucial in SRL. However, it is difficult to learn such information from limited, small-scale, manually annotated training data. Instead of manually increasing the size of annotated data, we use a large amount of automatically extracted syntactic knowledge to improve the performance of SRL."
W15-1701,Location Name Disambiguation Exploiting Spatial Proximity and Temporal Consistency,2015,17,3,2,0,37002,takashi awamura,Proceedings of the third International Workshop on Natural Language Processing for Social Media,0,"As the volume of documents on the Web increases, technologies to extract useful information from them become increasingly essential. For instance, information extracted from social network services such as Twitter and Facebook is useful because it contains a lot of location-specific information. To extract such information, it is necessary to identify the location of each location-relevant expression within a document. Previous studies on location disambiguation have tackled this problem on the basis of word sense disambiguation, and did not make use of location-specific clues. In this paper, we propose a method for location disambiguation that takes advantage of the following two clues: spatial proximity and temporal consistency. We confirm the effectiveness of these clues through experiments on Twitter tweets with GPS information."
W15-0813,Classification and Acquisition of Contradictory Event Pairs using Crowdsourcing,2015,7,1,3,0,37105,yu takabatake,"Proceedings of the The 3rd Workshop on {EVENTS}: Definition, Detection, Coreference, and Representation",0,"We propose a taxonomy of contradictory event pairs and a method for building a database of such pairs. When a dialog system participates in an open-domain conversation with a human, it is important to avoid the generation of utterances that conflict with the context of the dialog. Here, we refer to a pair of events that are not able to co-occur or that are not inconsistent with each other as a contradictory event pair. In this study, we collected contradictory event pairs using crowdsourcing and constructed a taxonomy of such pairs. We also built a large-scale database of Japanese contradictory event pairs for each class using crowdsourcing. This database will be used for consistent utterance generation in dialog systems."
P15-5004,Corpus Patterns for Semantic Processing,2015,12,1,4,0,21558,octavian popescu,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing: Tutorial Abstracts,0,"This tutorial presents a corpus-driven, pattern-based empirical approach to meaning representation and computation. Patterns in text are everywhere, but techniques for identifying and processing them are still rudimentary. Patterns are not merely syntactic but syntagmatic: each pattern identifies a lexico-semantic clause structure consisting of a predicator (verb or predicative adjective) together with open-ended lexical sets of collocates in different clause roles (subject, object, prepositional argument, etc.). If NLP is to make progress in identifying and processing text meaning, pattern recognition and collocational analysis will play an essential role, because:"
D15-1276,Morphological Analysis for Unsegmented Languages using Recurrent Neural Network Language Model,2015,16,23,2,0,25335,hajime morita,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We present a new morphological analysis model that considers semantic plausibility of word sequences by using a recurrent neural network language model (RNNLM). In unsegmented languages, since language models are learned from automatically segmented texts and inevitably contain errors, it is not apparent that conventional language models contribute to morphological analysis. To solve this problem, we do not use language models based on raw word sequences but use a semantically generalized language model, RNNLM, in morphological analysis. In our experiments on two Japanese corpora, our proposed model significantly outperformed baseline models. This result indicates the effectiveness of RNNLM in morphological analysis."
P14-2042,{C}hinese Morphological Analysis with Character-level {POS} Tagging,2014,17,11,3,1,35693,mo shen,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"The focus of recent studies on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words to characters. However, existing methods have not yet fully utilized the potentials of Chinese characters. In this paper, we investigate the usefulness of character-level part-of-speech in the task of Chinese morphological analysis. We propose the first tagset designed for the task of character-level POS tagging. We propose a method that performs character-level POS tagging jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved."
P14-1097,A Step-wise Usage-based Method for Inducing Polysemy-aware Verb Classes,2014,43,7,1,1,3202,daisuke kawahara,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We present an unsupervised method for inducing verb classes from verb uses in gigaword corpora. Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. By taking this step-wise approach, we can not only generate verb classes based on a massive amount of verb uses in a scalable manner, but also deal with verb polysemy, which is bypassed by most of the previous studies on verb clustering. In our experiments, we acquire semantic frames and verb classes from two giga-word corpora, the larger comprising 20 billion words. The effectiveness of our approach is verified through quantitative evaluations based on polysemy-aware gold-standard data."
jin-etal-2014-framework,A Framework for Compiling High Quality Knowledge Resources From Raw Corpora,2014,13,1,2,1,33036,gongye jin,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The identification of various types of relations is a necessary step to allow computers to understand natural language text. In particular, the clarification of relations between predicates and their arguments is essential because predicate-argument structures convey most of the information in natural languages. To precisely capture these relations, wide-coverage knowledge resources are indispensable. Such knowledge resources can be derived from automatic parses of raw corpora, but unfortunately parsing still has not achieved a high enough performance for precise knowledge acquisition. We present a framework for compiling high quality knowledge resources from raw corpora. Our proposed framework selects high quality dependency relations from automatic parses and makes use of them for not only the calculation of fundamental distributional similarity but also the acquisition of knowledge such as case frames."
kawahara-palmer-2014-single,Single Classifier Approach for Verb Sense Disambiguation based on Generalized Features,2014,19,3,1,1,3202,daisuke kawahara,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present a supervised method for verb sense disambiguation based on VerbNet. Most previous supervised approaches to verb sense disambiguation create a classifier for each verb that reaches a frequency threshold. These methods, however, have a significant practical problem that they cannot be applied to rare or unseen verbs. In order to overcome this problem, we create a single classifier to be applied to rare or unseen verbs in a new text. This single classifier also exploits generalized semantic features of a verb and its modifiers in order to better deal with rare or unseen verbs. Our experimental results show that the proposed method achieves equivalent performance to per-verb classifiers, which cannot be applied to unseen verbs. Our classifier could be utilized to improve the classifications in lexical resources of verbs, such as VerbNet, in a semi-automatic manner and to possibly extend the coverage of these resources to new verbs."
E14-1007,Inducing Example-based Semantic Frames from a Massive Amount of Verb Uses,2014,40,16,1,1,3202,daisuke kawahara,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present an unsupervised method for inducing semantic frames from verb uses in giga-word corpora. Our semantic frames are verb-specific example-based frames that are distinguished according to their senses. We use the Chinese Restaurant Process to automatically induce these frames from a massive amount of verb instances. In our experiments, we acquire broad-coverage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach."
C14-1027,Rapid Development of a Corpus with Discourse Annotations using Two-stage Crowdsourcing,2014,31,8,1,1,3202,daisuke kawahara,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We present a novel approach for rapidly developing a corpus with discourse annotations using crowdsourcing. Although discourse annotations typically require much time and cost owing to their complex nature, we realize discourse annotations in an extremely short time while retaining good quality of the annotations by crowdsourcing two annotation subtasks. In fact, our experiment to create a corpus comprising 30,000 Japanese sentences took less than eight hours to run. Based on this corpus, we also develop a supervised discourse parser and evaluate its performance to verify the usefulness of the acquired corpus."
2014.amta-wptp.15,Post-editing user interface using visualization of a sentence structure,2014,-1,-1,3,1,16897,yudai kishimoto,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas,0,"Translation has become increasingly important by virtue of globalization. To reduce the cost of translation, it is necessary to use machine translation and further to take advantage of post-editing based on the result of a machine translation for accurate information dissemination. Such post-editing (e.g., PET [Aziz et al., 2012]) can be used practically for translation between European languages, which has a high performance in statistical machine translation. However, due to the low accuracy of machine translation between languages with different word order, such as Japanese-English and Japanese-Chinese, post-editing has not been used actively."
W13-5714,Towards Fully Lexicalized Dependency Parsing for {K}orean,2013,0,3,2,0,24158,jungyeul park,Proceedings of the 13th International Conference on Parsing Technologies ({IWPT} 2013),0,None
I13-1005,Precise Information Retrieval Exploiting Predicate-Argument Structures,2013,25,5,1,1,3202,daisuke kawahara,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,A concept can be linguistically expressed in various syntactic constructions. Such syntactic variations spoil the effectiveness of incorporating dependencies between words into information retrieval systems. This paper presents an information retrieval method for normalizing syntactic variations via predicate-argument structures. We conduct experiments on standard test collections and show the effectiveness of our approach. Our proposed method significantly outperforms a baseline method based on word dependencies.
I13-1020,{C}hinese Word Segmentation by Mining Maximized Substrings,2013,21,1,2,1,35693,mo shen,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"A major problem in the field of Chinese word segmentation is the identification of out-ofvocabulary words. We propose a simple yet effective approach for extracting maximized substrings, which provide good estimations of unknown word boundaries. We also develop a new semi-supervised segmentation technique that incorporates retrieved substrings using discriminative learning. The effectiveness of this novel approach is demonstrated through experiments using both in-domain and out-ofdomain data."
I13-1124,High Quality Dependency Selection from Automatic Parses,2013,13,2,2,1,33036,gongye jin,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Many NLP tasks such as question answering and knowledge acquisition are tightly dependent on dependency parsing. Dependency parsing accuracy is always decisive for the performance of subsequent tasks. Therefore, reducing dependency parsing errors or selecting high quality dependencies is a primary issue. In this paper, we present a supervised approach for automatically selecting high quality dependencies from automatic parses. Experimental results on three different languages show that our approach can effectively select high quality dependencies from the result analyzed by a dependency parser."
D13-1095,{J}apanese Zero Reference Resolution Considering Exophora and Author/Reader Mentions,2013,12,10,2,1,10699,masatsugu hangyo,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"In Japanese, zero references often occur and many of them are categorized into zero exophora, in which a referent is not mentioned in the document. However, previous studies have focused on only zero endophora, in which a referent explicitly appears. We present a zero reference resolution model considering zero exophora and author/reader of a document. To deal with zero exophora, our model adds pseudo entities corresponding to zero exophora to candidate referents of zero pronouns. In addition, we automatically detect mentions that refer to the author and reader of a document by using lexico-syntactic patterns. We represent their particular behavior in a discourse as a feature vector of a machine learning model. The experimental results demonstrate the effectiveness of our model for not only zero exophora but also zero endophora."
D13-1121,Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in {J}apanese,2013,25,5,2,0.701754,5794,ryohei sasano,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"We present a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese. By leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness."
Y12-1033,A Reranking Approach for Dependency Parsing with Variable-sized Subtree Features,2012,23,7,2,1,35693,mo shen,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"Employing higher-order subtree structures in graph-based dependency parsing has shown substantial improvement over the accuracy, however suffers from the inefficiency increasing with the order of subtrees. We present a new reranking approach for dependency parsing that can utilize complex subtree representation by applying efficient subtree selection heuristics. We demonstrate the effectiveness of the approach in experiments conducted on the Penn Treebank and the Chinese Treebank. Our system improves the baseline accuracy from 91.88% to 93.37% for English, and in the case of Chinese from 87.39% to 89.16%."
Y12-1058,Building a Diverse Document Leads Corpus Annotated with Semantic Relations,2012,3,12,2,1,10699,masatsugu hangyo,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"In these days, semantic analysis has been actively studied in natural language processing. For the study of semantic analysis, corpora with semantic annotations are essential. Although there are such corpora annotated on newspaper articles, there are various genres and styles, including linguistic expressions that are not found in newspaper articles. In this paper, we build a diverse document leads corpus annotated with semantic relations. To reduce the workload of annotators and annotate as many various documents as possible, we restrict the annotation target of each document to only the first three sentences. We have completed building a corpus of 1,000 documents and report the statistics of this corpus."
2012.eamt-1.7,Exploiting Shared {C}hinese Characters in {C}hinese Word Segmentation Optimization for {C}hinese-{J}apanese Machine Translation,2012,14,15,3,0.25,293,chenhui chu,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"Unknown words and word segmentation granularity are two main problems in Chinese word segmentation for ChineseJapanese Machine Translation (MT). In this paper, we propose an approach of exploiting common Chinese characters shared between Chinese and Japanese in Chinese word segmentation optimization for MT aiming to solve these problems. We augment the system dictionary of a Chinese segmenter by extracting Chinese lexicons from a parallel training corpus. In addition, we adjust the granularity of the training data for the Chinese segmenter to that of Japanese. Experimental results of Chinese-Japanese MT on a phrase-based SMT system show that our approach improves MT performance significantly."
I11-1051,Generative Modeling of Coordination by Factoring Parallelism and Selectional Preferences,2011,20,4,1,1,3202,daisuke kawahara,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We present a unified generative model of coordination that considers parallelism of conjuncts and selectional preferences. Parallelism of conjuncts, which frequently characterizes coordinate structures, is modeled as a synchronized generation process in the generative parser. Selectional preferences learned from a large web corpus provide an important clue for resolving the ambiguities of coordinate structures. Our experiments of Japanese dependency parsing indicate the effectiveness of our approach, particularly in the domains of newspapers and patents."
kawahara-kurohashi-2010-acquiring,Acquiring Reliable Predicate-argument Structures from Raw Corpora for Case Frame Compilation,2010,25,16,1,1,3202,daisuke kawahara,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present a method for acquiring reliable predicate-argument structures from raw corpora for automatic compilation of case frames. Such lexicon compilation requires highly reliable predicate-argument structures to practically contribute to Natural Language Processing (NLP) applications, such as paraphrasing, text entailment, and machine translation. However, to precisely identify predicate-argument structures, case frames are required. This issue is similar to the question ''``what came first: the chicken or the egg?'''' In this paper, we propose the first step in the extraction of reliable predicate-argument structures without using case frames. We first apply chunking to raw corpora and then extract reliable chunks to ensure that high-quality predicate-argument structures are obtained from the chunks. We conducted experiments to confirm the effectiveness of our approach. We successfully extracted reliable chunks of an accuracy of 98{\%} and high-quality predicate-argument structures of an accuracy of 97{\%}. Our experiments confirmed that we succeeded in acquiring highly reliable predicate-argument structures that can be used to compile case frames."
C10-2061,Identifying Contradictory and Contrastive Relations between Statements to Outline Web Information on a Given Topic,2010,23,13,1,1,3202,daisuke kawahara,Coling 2010: Posters,0,"We present a method for producing a bird's-eye view of statements that are expressed on Web pages on a given topic. This method aggregates statements that are relevant to the topic, and shows contradictory and contrastive relations among them. This view of contradictions and contrasts helps users acquire a top-down understanding of the topic. To realize this, we extract such statements and relations, including cross-document implicit contrastive relations between statements, in an unsupervised manner. Our experimental results indicate the effectiveness of our approach."
W09-3817,Capturing Consistency between Intra-clause and Inter-clause Relations in Knowledge-rich Dependency and Case Structure Analysis,2009,30,1,1,1,3202,daisuke kawahara,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"We present a method for dependency and case structure analysis that captures the consistency between intra-clause relations (i.e., case structures or predicate-argument structures) and inter-clause relations. We assess intra-clause relations on the basis of case frames and inter-clause relations on the basis of transition knowledge between case frames. Both knowledge bases are automatically acquired from a massive amount of parses of a Web corpus. The significance of this study is that the proposed method selects the best dependency and case structure that are consistent within each clause and between clauses. We confirm that this method contributes to the improvement of dependency parsing of Japanese."
W09-1201,The {C}o{NLL}-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages,2009,26,269,4,0,17503,jan hajivc,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL} 2009): Shared Task,0,"For the 11th straight year, the Conference on Computational Natural Language Learning has been accompanied by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting. In 2009, the shared task was dedicated to the joint parsing of syntactic and semantic dependencies in multiple languages. This shared task combines the shared tasks of the previous five years under a unique dependency-based formalism similar to the 2008 task. In this paper, we define the shared task, describe how the data sets were created and show their quantitative properties, report the results and summarize the approaches of the participating systems."
P09-4001,{WISDOM}: A Web Information Credibility Analysis Systematic,2009,6,23,2,0,47164,susumu akamine,Proceedings of the {ACL}-{IJCNLP} 2009 Software Demonstrations,0,"We demonstrate an information credibility analysis system called WISDOM. The purpose of WISDOM is to evaluate the credibility of information available on the Web from multiple viewpoints. WISDOM considers the following to be the source of information credibility: information contents, information senders, and information appearances. We aim at analyzing and organizing these measures on the basis of semantics-oriented natural language processing (NLP) techniques."
N09-1059,The Effect of Corpus Size on Case Frame Acquisition for Discourse Analysis,2009,21,11,2,0.909091,5794,ryohei sasano,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper reports the effect of corpus size on case frame acquisition for discourse analysis in Japanese. For this study, we collected a Japanese corpus consisting of up to 100 billion words, and constructed case frames from corpora of six different sizes. Then, we applied these case frames to syntactic and case structure analysis, and zero anaphora resolution. We obtained better results by using case frames constructed from larger corpora; the performance was not saturated even with a corpus size of 100 billion words."
2009.mtsummit-papers.18,Mining Parallel Texts from Mixed-Language Web Pages,2009,-1,-1,2,0,127,masao utiyama,Proceedings of Machine Translation Summit XII: Papers,0,None
kawahara-uchimoto-2008-method,A Method for Automatically Constructing Case Frames for {E}nglish,2008,22,3,1,1,3202,daisuke kawahara,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Case frames are an important knowledge base for a variety of natural language processing (NLP) systems. For the practical use of these systems in the real world, wide-coverage case frames are required. In order to acquire such large-scale case frames, in this paper, we automatically compile case frames from a large corpus. The resultant case frames that are compiled from the English Gigaword corpus contain 9,300 verb entries. The case frames include most examples of normal usage, and are ready to be used in numerous NLP analyzers and applications."
shinzato-etal-2008-large,A Large-Scale Web Data Collection as a Natural Language Processing Infrastructure,2008,5,12,2,0,15889,keiji shinzato,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In recent years, language resources acquired from theWeb are released, and these data improve the performance of applications in several NLP tasks. Although the language resources based on the web page unit are useful in NLP tasks and applications such as knowledge acquisition, document retrieval and document summarization, such language resources are not released so far. In this paper, we propose a data format for results of web page processing, and a search engine infrastructure which makes it possible to share approximately 100 million Japanese web data. By obtaining the web data, NLP researchers are enabled to begin their own processing immediately without analyzing web pages by themselves."
I08-2097,Learning Reliability of Parses for Domain Adaptation of Dependency Parsing,2008,25,32,1,1,3202,daisuke kawahara,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"The accuracy of parsing has exceeded 90% recently, but this is not high enough to use parsing results practically in natural language processing (NLP) applications such as paraphrase acquisition and relation extraction. We present a method for detecting reliable parses out of the outputs of a single dependency parser. This technique is also applied to domain adaptation of dependency parsing. Our goal was to improve the performance of a state-of-the-art dependency parser on the data set of the domain adaptation track of the CoNLL 2007 shared task, a formidable challenge."
I08-1012,Dependency Parsing with Short Dependency Relations in Unlabeled Data,2008,18,27,2,0,21231,wenliang chen,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"This paper presents an effective dependency parsing approach of incorporating short dependency information from unlabeled data. The unlabeled data is automatically parsed by a deterministic dependency parser, which can provide relatively high performance for short dependencies between words. We then train another parser which uses the information on short dependency relations extracted from the output of the first parser. Our proposed approach achieves an unlabeled attachment score of 86.52, an absolute 1.24% improvement over the baseline system on the data set of Chinese Treebank."
I08-1025,{TSUBAKI}: An Open Search Engine Infrastructure for Developing New Information Access Methodology,2008,0,48,3,0,15889,keiji shinzato,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,None
D08-1104,Construction of an Idiom Corpus and its Application to Idiom Identification based on {WSD} Incorporating Idiom-Specific Features,2008,18,17,2,0,26950,chikara hashimoto,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"Some phrases can be interpreted either idiomatically (figuratively) or literally in context, and the precise identification of idioms is indispensable for full-fledged natural language processing (NLP). To this end, we have constructed an idiom corpus for Japanese. This paper reports on the corpus and the results of an idiom identification experiment using the corpus. The corpus targets 146 ambiguous idioms, and consists of 102, 846 sentences, each of which is annotated with a literal/idiom label. For idiom identification, we targeted 90 out of the 146 idioms and adopted a word sense disambiguation (WSD) method using both common WSD features and idiom-specific features. The corpus and the experiment are the largest of their kind, as far as we know. As a result, we found that a standard supervised WSD method works well for the idiom identification and achieved an accuracy of 89.25% and 88.86% with/without idiom-specific features and that the most effective idiom-specific feature is the one involving the adjacency of idiom constituents."
C08-1054,Coordination Disambiguation without Any Similarities,2008,20,12,1,1,3202,daisuke kawahara,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"The use of similarities has been one of the main approaches to resolve the ambiguities of coordinate structures. In this paper, we present an alternative method for coordination disambiguation, which does not use similarities. Our hypothesis is that coordinate structures are supported by surrounding dependency relations, and that such dependency relations rather yield similarity between conjuncts, which humans feel. Based on this hypothesis, we built a Japanese fully-lexicalized generative parser that includes coordination disambiguation. Experimental results on web sentences indicated the effectiveness of our approach, and endorsed our hypothesis."
C08-1097,A Fully-Lexicalized Probabilistic Model for {J}apanese Zero Anaphora Resolution,2008,13,21,2,0.952381,5794,ryohei sasano,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper presents a probabilistic model for Japanese zero anaphora resolution. First, this model recognizes discourse entities and links all mentions to them. Zero pronouns are then detected by case structure analysis based on automatically constructed case frames. Their appropriate antecedents are selected from the entities with high salience scores, based on the case frames and several preferences on the relation between a zero pronoun and an antecedent. Case structure and zero anaphora relation are simultaneously determined based on probabilistic evaluation metrics."
C08-1132,{C}hinese Dependency Parsing with Large Scale Automatically Constructed Case Structures,2008,27,22,2,0.714286,44132,kun yu,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper proposes an approach using large scale case structures, which are automatically constructed from both a small tagged corpus and a large raw corpus, to improve Chinese dependency parsing. The case structure proposed in this paper has two characteristics: (1) it relaxes the predicate of a case structure to be all types of words which behaves as a head; (2) it is not categorized by semantic roles but marked by the neighboring modifiers attached to a head. Experimental results based on Penn Chinese Treebank show the proposed approach achieved 87.26% on unlabeled attachment score, which significantly outperformed the baseline parser without using case structures."
P07-2052,Minimally Lexicalized Dependency Parsing,2007,17,3,1,1,3202,daisuke kawahara,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"Dependency structures do not have the information of phrase categories in phrase structure grammar. Thus, dependency parsing relies heavily on the lexical information of words. This paper discusses our investigation into the effectiveness of lexicalization in dependency parsing. Specifically, by restricting the degree of lexicalization in the training phase of a parser, we examine the change in the accuracy of dependency relations. Experimental results indicate that minimal or low lexicalization is sufficient for parsing accuracy."
D07-1032,Probabilistic Coordination Disambiguation in a Fully-Lexicalized {J}apanese Parser,2007,13,6,1,1,3202,daisuke kawahara,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,This paper describes a probabilistic model for coordination disambiguation integrated into syntactic and case structure analysis. Our model probabilistically assesses the parallelism of a candidate coordinate structure using syntactic/semantic similarities and cooccurrence statistics. We integrate these probabilities into the framework of fully-lexicalized parsing based on largescale case frames. This approach simultaneously addresses two tasks of coordination disambiguation: the detection of coordinate conjunctions and the scope disambiguation of coordinate structures. Experimental results on web sentences indicate the effectiveness of our approach.
N06-1023,A Fully-Lexicalized Probabilistic Model for {J}apanese Syntactic and Case Structure Analysis,2006,15,85,1,1,3202,daisuke kawahara,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"We present an integrated probabilistic model for Japanese syntactic and case structure analysis. Syntactic and case structure are simultaneously analyzed based on wide-coverage case frames that are constructed from a huge raw corpus in an unsupervised manner. This model selects the syntactic and case structure that has the highest generative probability. We evaluate both syntactic structure and case structure. In particular, the experimental results for syntactic analysis on web sentences show that the proposed model significantly outperforms known syntactic analyzers."
kawahara-kurohashi-2006-case,Case Frame Compilation from the Web using High-Performance Computing,2006,9,84,1,1,3202,daisuke kawahara,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Case frames are important knowledge for a variety of NLP systems, especially when wide-coverage case frames are available. To acquire such large-scale case frames, it is necessary to automatically compile them from an enormous amount of corpus. In this paper, we consider the web as a corpus. We first build a huge text corpus from the web, and then construct case frames from the corpus. It is infeasible to do these processes by one CPU, and thus we employ a high-performance computing environment composed of 350 CPUs. The acquired corpus consists of 470M sentences, and the case frames compiled from them have 90,000 verb entries. The case frames contain most examples of usual use, and are ready to be applied to lots of NLP analyses and applications."
2006.iwslt-evaluation.9,Example-based machine translation based on deeper {NLP},2006,11,15,3,1,283,toshiaki nakazawa,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,This paper describes our Kyoto-U system that attended the IWSLT06 Japanese-English machine translation task. Example-based machine translation is applied in this system to integrate our study on both structural NLP and machine translation.
I05-1017,{PP}-Attachment Disambiguation Boosted by a Gigantic Volume of Unambiguous Examples,2005,23,7,1,1,3202,daisuke kawahara,Second International Joint Conference on Natural Language Processing: Full Papers,0,We present a PP-attachment disambiguation method based on a gigantic volume of unambiguous examples extracted from raw corpus. The unambiguous examples are utilized to acquire precise lexical preferences for PP-attachment disambiguation. Attachment decisions are made by a machine learning method that optimizes the use of the lexical preferences. Our experiments indicate that the precise lexical preferences work effectively.
I05-1060,Automatic Acquisition of Basic Katakana Lexicon from a Given Corpus,2005,7,8,2,1,283,toshiaki nakazawa,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Katakana, Japanese phonogram mainly used for loan words, is a troublemaker in Japanese word segmentation. Since Katakana words are heavily domain-dependent and there are many Katakana neologisms, it is almost impossible to construct and maintain Katakana word dictionary by hand. This paper proposes an automatic segmentation method of Japanese Katakana compounds, which makes it possible to construct precise and concise Katakana word dictionary automatically, given only a medium or large size of Japanese corpus of some domain."
2005.iwslt-1.27,Example-based Machine Translation Pursuing Fully Structural {NLP},2005,7,6,4,0,297,sadao kurohashi,Proceedings of the Second International Workshop on Spoken Language Translation,0,We are conducting Example-Based Machine Translation research aiming at the improvement both of structural NLP and machine translation. This paper describes UTokyo system challenged IWSLT05 Japanese-English translation tasks.
N04-4039,Converting Text into Agent Animations: Assigning Gestures to Text,2004,15,28,3,0,49159,yukiko nakano,Proceedings of {HLT}-{NAACL} 2004: Short Papers,0,"This paper proposes a method for assigning gestures to text based on lexical and syntactic information. First, our empirical study identified lexical and syntactic information strongly correlated with gesture occurrence and suggested that syntactic structure is more useful for judging gesture occurrence than local syntactic cues. Based on the empirical results, we have implemented a system that converts text into an animated agent that gestures and speaks synchronously."
kawahara-etal-2004-toward,Toward Text Understanding: Integrating Relevance-tagged Corpus and Automatically Constructed Case Frames,2004,7,4,1,1,3202,daisuke kawahara,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper proposes a wide-range anaphora resolution system toward text understanding. This system resolves zero, direct and indirect anaphors in Japanese texts by integrating two sorts of linguistic resources: a hand-annotated corpus with various relations and automatically constructed case frames. The corpus has relevance tags which consist of predicate-argument relations, relations between nouns and coreferences, and is utilized for learning parameters of the system and testing it. The case frames are indispensable knowledge both for detecting zero/indirect anaphors and estimating appropriate antecedents. Our preliminary experiments showed promising results."
C04-1050,Improving {J}apanese Zero Pronoun Resolution by Global Word Sense Disambiguation,2004,10,5,1,1,3202,daisuke kawahara,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper proposes unsupervised word sense disambiguation based on automatically constructed case frames and its incorporation into our zero pronoun resolution system. The word sense disambiguation is applied to verbs and nouns. We consider that case frames define verb senses and semantic features in a thesaurus define noun senses, respectively, and perform sense disambiguation by selecting them based on case analysis. In addition, according to the one sense per discourse heuristic, the word sense disambiguation results are cached and applied globally to the subsequent words. We integrated this global word sense disambiguation into our zero pronoun resolution system, and conducted experiments of zero pronoun resolution on two different domain corpora. Both of the experimental results indicated the effectiveness of our approach."
C04-1174,Automatic Construction of Nominal Case Frames and its Application to Indirect Anaphora Resolution,2004,11,8,2,0.952381,5794,ryohei sasano,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper proposes a method to automatically construct Japanese nominal case frames. The point of our method is the integrated use of a dictionary and example phrases from large corpora. To examine the practical usefulness of the constructed nominal case frames, we also built a system of indirect anaphora resolution based on the case frames. The constructed case frames were evaluated by hand, and were confirmed to be good quality. Experimental results of indirect anaphora resolution also indicated the effectiveness of our approach."
P02-1028,Verb Paraphrase based on Case Frame Alignment,2002,5,34,2,0,26283,nobuhiro kaji,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes a method of translating a predicate-argument structure of a verb into that of an equivalent verb, which is a core component of the dictionary-based paraphrasing. Our method grasps several usages of a headword and those of the def-heads as a form of their case frames and aligns those case frames, which means the acquisition of word sense disambiguation rules and the detection of the appropriate equivalent and case marker transformation."
kawahara-etal-2002-construction,Construction of a {J}apanese Relevance-tagged Corpus,2002,5,56,1,1,3202,daisuke kawahara,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper describes our corpus annotation project. The annotated corpus has relevance tags which consist of predicate-argument relations, relations between nouns, and coreferences. To construct this relevance-tagged corpus, we investigated a large corpus and established the specification of the annotation. This paper shows the specification and dicult tagging problems which have emerged through the annotation so far."
C02-1122,Fertilization of Case Frame Dictionary for Robust {J}apanese Case Analysis,2002,4,28,1,1,3202,daisuke kawahara,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper proposes a method of fertilizing a Japanese case frame dictionary to handle complicated expressions: double nominative sentences, non-gapping relation of relative clauses, and case change. Our method is divided into two stages. In the first stage, we parse a large corpus and construct a Japanese case frame dictionary automatically from the parse results. In the second stage, we apply case analysis to the large corpus utilizing the constructed case frame dictionary, and upgrade the case frame dictionary by incorporating newly acquired information."
H01-1043,{J}apanese Case Frame Construction by Coupling the Verb and its Closest Case Component,2001,6,27,1,1,3202,daisuke kawahara,Proceedings of the First International Conference on Human Language Technology Research,0,"This paper describes a method to construct a case frame dictionary automatically from a raw corpus. The main problem is how to handle the diversity of verb usages. We collect predicate-argument examples, which are distinguished by the verb and its closest case component in order to deal with verb usages, from parsed results of a corpus. Since these couples multiply to millions of combinations, it is difficult to make a wide-coverage case frame dictionary from a small corpus like an analyzed corpus. We, however, use a raw corpus, so that this problem can be addressed. Furthermore, we cluster and merge predicate-argument examples which does not have different usages but belong to different case frames because of different closest case components. We also report on an experimental result of case structure analysis using the constructed case frame dictionary."
C00-1063,{J}apanese Case Structure Analysis,2000,4,0,1,1,3202,daisuke kawahara,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"In Japanese, case structure analysis is very imt)ortant to handle several t roublesome characteristics of Japanese snch as scrambling, onfission of ease components, mid disappearance of case markers. However, fi)r lack of a widecoverage ease frame dictionary, it has been difficult to perfornl case structure analysis accurat;ely. Although several methods to construct a ease fl'mne dictionary from analyzed corpora have been proposed, they cannot avoid data sparseness 1)rol)lem. This paper proposes an unsupervised method of constructing a case frame dictionary from an enormous raw corpus by using a robust and accurate parser. It also prorides a case structure analysis method based on the constructed dictionary. 1 I n t r o d u c t i o n Syntactic analysis, or parsing has been a main objective in Natural Language Processing. In case of Jat)anese , however, syntactic analysis cannot clarify relations between words ill sentences because of several t roublesome characteristics of Japanese such as scrambling, omission of case components, and disappearance of case markers. Therefore, in Japanese sentence analysis, case s tructure analysis is an important issue, and a case frame dictionary is necessary for the analysis. Some research institutes have constructed Japanese case frmne dictiouaries manually (Ikehara et al., 1997; Infbrmation-Technology Promotion Agency, Japan, 1987). However, it is quite expensive, or almost impossible to construct a wide-coverage ease fl'anm dictionary by hand. Others have tried to construct a case fl'mne dictionary automatical ly from analyzed corpora (Utsuro et al., 1998). However, existing syntactically analyzed corpora are too small to learn a dictionary, since case fl'ame iuformation consists of relations between nouns and verbs, which rnultiplies to millions of combinations. Based on such a consideration, we took the fbllowing unsupervised learning strategy to the .Japanese case structure analysis: 1. At first, a robust and accurate parser is developed, which does not utilize a case fl'mne dictionary, 2. a very large corI)us is parsed by the parser, 3. reliable noun-verb relations are extracted from the parse results, and a case frmne dict ionary is constructed from them, and 4. the dictionary is utilized for case structure analysis. 2 Characteristics of Japanese language and necessity of case s t r u c t u r e a n a l y s i s In Japanese, postposit ions function as case markers ( ( M s ) mid a verb is final in a sentence. The basic s tructure of a Japanese sentence is as fbllows: (1) k a t e 9a coat wo ki~'u. he nominative-CM coat accusative-CM wear (lie wears a coat) A clause modifier is left to the modified noun as follows: (2) k a t e 9 a k i t e i r u coat lie nom-CM wear coat (the coat he wears) The modified noun followed by a postposit ion then becomes a case component of a matrix verb. The typical s tructure of a Japanese complex sentence is as fbllows:"
