2006.amta-papers.15,J93-2003,0,0.00490333,"be synchronizable, to allow for multilinguality. Finally, in order to support automated induction, it should allow for a probabilistic variant, and a reasonably efficient parsing algorithm. The more expressive and flexible a formalism is, the less efficient parsing of it will be. Therefore, the primary trade-off to be made is between parsing efficiency on one hand and the rest of the desired characteristics on the other. However, even among formalisms with the same parse complexity, some formalisms better satisfy the desiderata than others. Finite-state word-based models, such as IBM Model 5 (Brown et al., 1993), use a base formalism that allows for synchronization, probabilistic variants, very efficient processing, and good ability to capture lexical and bilexical relationships. However, they are limited by the inability to use hierarchical information in the interlingual mapping. That bilingual dictionaries describe the mappings between languages in terms of constructions, not individual words, suggests that this information would be useful. For instance, the HarperCollins Italian College Dictionary (HCICD) translates the English “to take advantage of” as “sfruttare”, although that word is a direct"
2006.amta-papers.15,P05-1033,0,0.00464896,"are well known to perform poorly as language models compared to finite-state models; they gain the ability to substitute according to abstract categories at the expense of stating lexical relationships directly. Although arbitrary CFGs can be weakly lexicalized by other CFGs, this can require changing the shape of the derived trees produced, and more critically, changes the structure of the derivation (Schabes and Waters, 1993a; Schabes and Waters, 1995). Because synchronization requires substantial isomorphism of the derivation trees, synchronization of lexicalized CFGs becomes problematic. Chiang (2005) overcomes this short1 For example, “The US took advantage yesterday of the political and military momentum in its Afghan campaign. . . ” is one of many Google hits on the phrase “took advantage yesterday of”. 129 VP V NP take advantage VP PP P of V N P↓ NP NP take advantage Jean PP P of PP N P↓ Adv P P∗ yesterday Figure 1: An example TAG/TIG substitution Figure 2: An example TAG/TIG adjunction coming in his synchronous CFG-based system by making it both hierachical and phrase-based so that n-grams used in phrasal mappings could still capture some of the lexical dependencies. His system outper"
2006.amta-papers.15,P05-1067,0,0.0161726,"more parsing efficiency for greater expressivity (Melamed, 2004; Melamed et al., 2004). Formalisms such as Generalized Multitext Grammars (GMTG) do in principle satisfy all of the desiderata if the higher time and space complexity of the parsing algorithms for them does not make training prohibitively expensive (Melamed et al., 2004). It is an empirical question whether systems with a high degree of parse complexity can be induced in practice. Burbank et al. (2005) implemented a framework in which base formalisms such as GMTG can be tested though no substantial results have yet been reported. Ding and Palmer (2005) also employ a more expressive formalism but use heuristic approaches to limit the complexity of the processing. All of these considerations led us to seek a more expressive formalism that could still be parsed efficiently. As we will argue, probabilistic synchronous tree-insertion grammar substantially satisfies each of the desiderata without increasing parse complexity. We present an MT system based on it in the remainder of this paper. tution of sub-parts. Due to space limitations, for a detailed description of the TAG formalism we refer readers to the introduction by Joshi (1985). Importan"
2006.amta-papers.15,P99-1059,0,0.0716925,"Missing"
2006.amta-papers.15,J99-4004,0,0.121599,"Missing"
2006.amta-papers.15,W05-0833,0,0.164752,"antially higher quality translations can be produced by unsupervised induction systems that can easily be hybridized with 128 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 128-137, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas linguistically-motivated elementary structures generated manually or though a supervised process. In particular, we propose a method of hybridizing our system by adding elementary structures generated using the methods of Groves et al. (2004) in a manner similar to that used by Groves and Way (2005). 2 Motivation and Related Work Recent work in statistical machine translation by parsing has identified a set of characteristics an ideal base formalism should have for the translation task (Melamed, 2003; Melamed, 2004; Melamed et al., 2004). What is desired is a formalism that has the substitution-based hierarchical structure of contextfree grammars and the lexical relationship potential of n-gram models. Further, it should allow for discontinuity in phrases and be synchronizable, to allow for multilinguality. Finally, in order to support automated induction, it should allow for a probabili"
2006.amta-papers.15,C04-1154,0,0.381389,"ce a particular formalism, probabilistic synchronous treeinsertion grammar (PSTIG) that we argue satisfies the desiderata optimally within the class of formalisms that can be parsed no less efficiently than context-free grammars and demonstrate that it outperforms state-of-the-art word-based and phrasebased finite-state translation models on training and test data taken from the EuroParl corpus (Koehn, 2005). We then argue that a higher level of translation quality can be achieved by hybridizing our induced model with elementary structures produced using supervised techniques such as those of Groves et al. (2004). 1 Introduction In this paper we identify a base formalism, probabilistic synchronous tree-insertion grammar (PSTIG), for a statistical machine translation system that we propose: 1. maximizes, within its efficiency class, the quality of the MT system induced unsupervised from aligned sentence pairs; and We begin with an argument from first principles for the choice of PSTIG as a base formalism for syntax-aware statistical machine translation (SMT). We then present our implementation of a system that induces a PSTIG unsupervised from data and show that it outperforms a state-of-the-art phrase"
2006.amta-papers.15,N03-1017,0,0.00282311,"joint probability distributions over frequently co-occuring n-grams to find multiword translations, thereby improving on the performance of IBM Model 5. Such an approach does allow multiword relationships to be induced, but does not in any sense incorporate syntactic structure to do so. Indeed the natural way to augment the multiword approach to incorporate syntactic constraints is to restrict the multiword sequences to syntactic constituents (as determined by a statistical parser for instance) (Yamada and Knight, 2002). Yet this augmentation turns out to underperform the syntax-free variant (Koehn et al., 2003). The reason is not hard to understand: the word sequences that map well in translation—such as the German-English example of Koehn et al. (2003) “es gibt”/“there is”—are not themselves syntactic constituents, but rather syntactic templates (“es gibt. . . ”/“there is. . . ”) with “holes” (marked here by ellipses) that might be substituted for in some uniform manner. Bilingual dictionaries even make the mapping between such constructions explicit through the use of place fillers like “sb” (“somebody”) or “qn” (“qualcuno”), as in the HCICD entry “to drive sb mad”/“far impazzire qn”. Secondarily,"
2006.amta-papers.15,N03-1021,0,0.0173718,"ricas, pages 128-137, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas linguistically-motivated elementary structures generated manually or though a supervised process. In particular, we propose a method of hybridizing our system by adding elementary structures generated using the methods of Groves et al. (2004) in a manner similar to that used by Groves and Way (2005). 2 Motivation and Related Work Recent work in statistical machine translation by parsing has identified a set of characteristics an ideal base formalism should have for the translation task (Melamed, 2003; Melamed, 2004; Melamed et al., 2004). What is desired is a formalism that has the substitution-based hierarchical structure of contextfree grammars and the lexical relationship potential of n-gram models. Further, it should allow for discontinuity in phrases and be synchronizable, to allow for multilinguality. Finally, in order to support automated induction, it should allow for a probabilistic variant, and a reasonably efficient parsing algorithm. The more expressive and flexible a formalism is, the less efficient parsing of it will be. Therefore, the primary trade-off to be made is between"
2006.amta-papers.15,P04-1083,0,0.420773,"8-137, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas linguistically-motivated elementary structures generated manually or though a supervised process. In particular, we propose a method of hybridizing our system by adding elementary structures generated using the methods of Groves et al. (2004) in a manner similar to that used by Groves and Way (2005). 2 Motivation and Related Work Recent work in statistical machine translation by parsing has identified a set of characteristics an ideal base formalism should have for the translation task (Melamed, 2003; Melamed, 2004; Melamed et al., 2004). What is desired is a formalism that has the substitution-based hierarchical structure of contextfree grammars and the lexical relationship potential of n-gram models. Further, it should allow for discontinuity in phrases and be synchronizable, to allow for multilinguality. Finally, in order to support automated induction, it should allow for a probabilistic variant, and a reasonably efficient parsing algorithm. The more expressive and flexible a formalism is, the less efficient parsing of it will be. Therefore, the primary trade-off to be made is between parsing effici"
2006.amta-papers.15,J03-1002,0,0.00221192,"Model 2 only in that it does not allow for one-to-many mappings. We evaluated our system on a set of 15,277 sentences extracted from the over 600,000 sentence pairs in the EuroParl German-English corpus (Koehn, 2005). We selected sentence pairs in which the German sentence contained only words within the 1000 most frequent German words in the corpus, in order to limit the size of the vocabulary. We then further limited the set by using only sentence pairs with combined length less than 25.7 We held out 100 sentence pairs for evaluation of the resulting system. As baselines we trained GIZA++ (Och and Ney, 2003) using the CMU-Cambridge Statistical Language Modeling Toolkit (CMU Toolkit) and the ISI ReWrite Decoder for testing, and Pharaoh using alignments generated according to the algorithm given in Koehn (2003) based on GIZA++ word alignments and using the CMU Toolkit for language modeling.8 We then evaluated all three systems on the test set automatically using BLEU score. We also ran a human evaluation in which three subjects evaluated all of the 100 translations produced by each system, in random order and with no indication of which system generated the translation, against the “gold standard”"
2006.amta-papers.15,1993.iwpt-1.20,0,0.589423,"ext-free grammars. A natural approach, then is to incorporate synchronization of contextfree structures to allow for these kinds of mappings. However, probabilistic context-free grammars (PCFG) are well known to perform poorly as language models compared to finite-state models; they gain the ability to substitute according to abstract categories at the expense of stating lexical relationships directly. Although arbitrary CFGs can be weakly lexicalized by other CFGs, this can require changing the shape of the derived trees produced, and more critically, changes the structure of the derivation (Schabes and Waters, 1993a; Schabes and Waters, 1995). Because synchronization requires substantial isomorphism of the derivation trees, synchronization of lexicalized CFGs becomes problematic. Chiang (2005) overcomes this short1 For example, “The US took advantage yesterday of the political and military momentum in its Afghan campaign. . . ” is one of many Google hits on the phrase “took advantage yesterday of”. 129 VP V NP take advantage VP PP P of V N P↓ NP NP take advantage Jean PP P of PP N P↓ Adv P P∗ yesterday Figure 1: An example TAG/TIG substitution Figure 2: An example TAG/TIG adjunction coming in his synchr"
2006.amta-papers.15,J95-4002,0,0.850329,"l approach, then is to incorporate synchronization of contextfree structures to allow for these kinds of mappings. However, probabilistic context-free grammars (PCFG) are well known to perform poorly as language models compared to finite-state models; they gain the ability to substitute according to abstract categories at the expense of stating lexical relationships directly. Although arbitrary CFGs can be weakly lexicalized by other CFGs, this can require changing the shape of the derived trees produced, and more critically, changes the structure of the derivation (Schabes and Waters, 1993a; Schabes and Waters, 1995). Because synchronization requires substantial isomorphism of the derivation trees, synchronization of lexicalized CFGs becomes problematic. Chiang (2005) overcomes this short1 For example, “The US took advantage yesterday of the political and military momentum in its Afghan campaign. . . ” is one of many Google hits on the phrase “took advantage yesterday of”. 129 VP V NP take advantage VP PP P of V N P↓ NP NP take advantage Jean PP P of PP N P↓ Adv P P∗ yesterday Figure 1: An example TAG/TIG substitution Figure 2: An example TAG/TIG adjunction coming in his synchronous CFG-based system by ma"
2006.amta-papers.15,koen-2004-pharaoh,0,0.01532,"e yesterday of the political and military momentum in its Afghan campaign. . . ” is one of many Google hits on the phrase “took advantage yesterday of”. 129 VP V NP take advantage VP PP P of V N P↓ NP NP take advantage Jean PP P of PP N P↓ Adv P P∗ yesterday Figure 1: An example TAG/TIG substitution Figure 2: An example TAG/TIG adjunction coming in his synchronous CFG-based system by making it both hierachical and phrase-based so that n-grams used in phrasal mappings could still capture some of the lexical dependencies. His system outperformed Pharaoh, a state-of-the-art phrase-based decoder (Koehn, 2004), on several translation tasks. Although systems such as Chiang’s are the current state-of-the-art, because of the limitations of CFGs as a base formalism Melamed and others continue to explore the possibility of trading off more parsing efficiency for greater expressivity (Melamed, 2004; Melamed et al., 2004). Formalisms such as Generalized Multitext Grammars (GMTG) do in principle satisfy all of the desiderata if the higher time and space complexity of the parsing algorithms for them does not make training prohibitively expensive (Melamed et al., 2004). It is an empirical question whether sy"
2006.amta-papers.15,C88-2121,0,0.293951,"employ a more expressive formalism but use heuristic approaches to limit the complexity of the processing. All of these considerations led us to seek a more expressive formalism that could still be parsed efficiently. As we will argue, probabilistic synchronous tree-insertion grammar substantially satisfies each of the desiderata without increasing parse complexity. We present an MT system based on it in the remainder of this paper. tution of sub-parts. Due to space limitations, for a detailed description of the TAG formalism we refer readers to the introduction by Joshi (1985). Importantly, Schabes et al. (1988) show that TAG can lexicalize CFG without changing the trees produced. That is, given a CFG a lexicalized TAG can be constructed that will produce the same set of derived structures produced by the CFG. Because each elementary tree contains a lexical item, the operations of substitution and adjunction implicitly manifest a lexical relationship. In addition, the two operations of TAG, substitution and adjunction, are exactly what is needed to handle noncontiguity, as shown in Figures 1 and 2. However, the TAG formalism’s additional expressivity leads to additional processing complexity. TAG par"
2006.amta-papers.15,2005.mtsummit-papers.11,0,0.0828704,"rsing of it will be. However, even among formalisms with the same parse complexity, some formalisms better realize the desired characteristics for machine translation formalisms than others. We introduce a particular formalism, probabilistic synchronous treeinsertion grammar (PSTIG) that we argue satisfies the desiderata optimally within the class of formalisms that can be parsed no less efficiently than context-free grammars and demonstrate that it outperforms state-of-the-art word-based and phrasebased finite-state translation models on training and test data taken from the EuroParl corpus (Koehn, 2005). We then argue that a higher level of translation quality can be achieved by hybridizing our induced model with elementary structures produced using supervised techniques such as those of Groves et al. (2004). 1 Introduction In this paper we identify a base formalism, probabilistic synchronous tree-insertion grammar (PSTIG), for a statistical machine translation system that we propose: 1. maximizes, within its efficiency class, the quality of the MT system induced unsupervised from aligned sentence pairs; and We begin with an argument from first principles for the choice of PSTIG as a base fo"
2006.amta-papers.15,C90-3045,1,0.484228,"as determined by the location of the non-foot material. To maintain the invariant that textual material falls only on a single side of the spine, adjunction is restricted so that left auxiliary trees may not adjoin into a node on the spine of a right auxiliary tree and vice versa. This prevents the formation of “wrapping” trees in which there are terminal symbols on both sides of the foot node. This restriction, coupled with the requirement 3 Synchronous Tree-Insertion Grammar Tree-adjoining grammars (TAG), introduced in monolingual form by Joshi (1985), and in a synchronous variant (STAG) by Shieber and Schabes (1990), are natural choices to capture lexicallybased dependencies while also allowing the substi130 WordAx AuxAx ![ηS , (i, i + 1)], [ηT , (l, l + 1)], ∅, 1# ![ηS , (i, i)], [ηT , (l, l)], ∅, 1# EmptyAx ![η! , (i, i)], [η! , (j, j)], {(x, y)}, 1# wi+1 = Label(ηS ) vl+1 = Label(ηT ) F oot(ηS ) F oot(ηT ) x, y ∈ {L, R} EmptyT ree(η! ) Figure 3: Axioms for CKY-style PSTIG parsing that all elementary auxiliary trees be non-wrapping, is sufficient to limit the formalism to context-free expressivity and O(n3 ) parsability. In addition, Schabes and Waters (Schabes and Waters, 1995) demonstrate that TIG, l"
2006.amta-papers.15,W02-1018,0,0.0201298,"ey are limited by the inability to use hierarchical information in the interlingual mapping. That bilingual dictionaries describe the mappings between languages in terms of constructions, not individual words, suggests that this information would be useful. For instance, the HarperCollins Italian College Dictionary (HCICD) translates the English “to take advantage of” as “sfruttare”, although that word is a direct translation of neither “take” nor “advantage”. Retaining the same finite-state base formalism, these models can be augmented to allow multiword (in addition to single word) mapping. Marcu and Wong (2002), among others, use joint probability distributions over frequently co-occuring n-grams to find multiword translations, thereby improving on the performance of IBM Model 5. Such an approach does allow multiword relationships to be induced, but does not in any sense incorporate syntactic structure to do so. Indeed the natural way to augment the multiword approach to incorporate syntactic constraints is to restrict the multiword sequences to syntactic constituents (as determined by a statistical parser for instance) (Yamada and Knight, 2002). Yet this augmentation turns out to underperform the s"
2006.amta-papers.15,P04-1084,0,0.0905368,"e, August 2006. ©2006 The Association for Machine Translation in the Americas linguistically-motivated elementary structures generated manually or though a supervised process. In particular, we propose a method of hybridizing our system by adding elementary structures generated using the methods of Groves et al. (2004) in a manner similar to that used by Groves and Way (2005). 2 Motivation and Related Work Recent work in statistical machine translation by parsing has identified a set of characteristics an ideal base formalism should have for the translation task (Melamed, 2003; Melamed, 2004; Melamed et al., 2004). What is desired is a formalism that has the substitution-based hierarchical structure of contextfree grammars and the lexical relationship potential of n-gram models. Further, it should allow for discontinuity in phrases and be synchronizable, to allow for multilinguality. Finally, in order to support automated induction, it should allow for a probabilistic variant, and a reasonably efficient parsing algorithm. The more expressive and flexible a formalism is, the less efficient parsing of it will be. Therefore, the primary trade-off to be made is between parsing efficiency on one hand and th"
2006.amta-papers.15,P02-1039,0,0.0109959,"to allow multiword (in addition to single word) mapping. Marcu and Wong (2002), among others, use joint probability distributions over frequently co-occuring n-grams to find multiword translations, thereby improving on the performance of IBM Model 5. Such an approach does allow multiword relationships to be induced, but does not in any sense incorporate syntactic structure to do so. Indeed the natural way to augment the multiword approach to incorporate syntactic constraints is to restrict the multiword sequences to syntactic constituents (as determined by a statistical parser for instance) (Yamada and Knight, 2002). Yet this augmentation turns out to underperform the syntax-free variant (Koehn et al., 2003). The reason is not hard to understand: the word sequences that map well in translation—such as the German-English example of Koehn et al. (2003) “es gibt”/“there is”—are not themselves syntactic constituents, but rather syntactic templates (“es gibt. . . ”/“there is. . . ”) with “holes” (marked here by ellipses) that might be substituted for in some uniform manner. Bilingual dictionaries even make the mapping between such constructions explicit through the use of place fillers like “sb” (“somebody”)"
2006.amta-papers.15,P93-1017,0,\N,Missing
2006.amta-papers.15,W90-0102,1,\N,Missing
2020.acl-demos.38,W16-5901,0,0.353807,"ted in Torch-Struct. Backprop/Gradients gives overridden backpropagation computation and value computed by this combination. (Bot) Example of gradients from different semirings on sequence alignment with dynamic time warping. Derivatives of the log-partition again provide useful distributional properties. For instance, the marginal probabilities of parts are given by, P exp z:zp =1 z · ` ∂ p(zp = 1) = P = A(`) 0·` exp z ∂` p z0 ∈ Similarly derivatives of A∗ correspond to whether a part appears in the argmax structure, I(zp∗ = 1) = ∂ ∗ ∂`p A (`). While these gradient identities are well-known (Eisner, 2016), they are not commonly deployed in practice. Computing CRF properties is typically done through two-step specialized algorithms, such as forward-backward, inside-outside, or similar variants such as viterbi-backpointers (Jurafsky and Martin, 2014). Common wisdom is that these approaches are more efficient implementations. However, we observe that recent engineering of faster gradient computation for deep learning has made gradient-based calculations competitive with hand-written calculations. In our experiments, we found that using these identities with autodiffer338 entiation was often faste"
2020.acl-demos.38,P08-1109,0,0.0315628,"presentations of spaces with combinatorial structure, as well as algorithms for inference and parameter estimation over these structures. Core methods include both tractable exact approaches like dynamic programming and spanning tree algorithms as well as heuristic techniques such linear programming relaxations and greedy search. Structured prediction has played a key role in the history of natural language processing. Example methods include techniques for sequence labeling and segmentation (Lafferty et al., 2001; Sarawagi and Cohen, 2005), discriminative dependency and constituency parsing (Finkel et al., 2008; McDonald et al., 2005), unsupervised learning for labeling and alignment (Vogel et al., 1996; Goldwater and Griffiths, 2007), approximate translation decoding with beam search (Tillmann and Ney, 2003), among many others. In recent years, research into deep structured prediction has studied how these approaches can be integrated with neural networks and pretrained models. One line of work has utilized structured prediction as the final layer for deep models (Collobert et al., 2011; Durrett and Klein, 2015). Another has incorporated structured prediction within deep learning models, exploring"
2020.acl-demos.38,N19-1115,0,0.0504242,", 2017; Yogatama et al., 2016, inter alia). Torch-Struct 336 aims to encourage this general use case. To illustrate, we consider a latent tree model. ListOps (Nangia and Bowman, 2018) is a dataset of mathematical functions. Each input/output pair consists of a prefix expression x and its result y, e.g. x = [ MAX 2 9 [ MIN 4 7 ] 0 ] y = 9 Models such as a flat RNN will fail to capture the hierarchical structure of this task. However, if a model can induce an explicit latent z, the parse tree of the expression, then the task is easy to learn by a tree-RNN model p(y|x, z) (Yogatama et al., 2016; Havrylov et al., 2019). Let us briefly summarize a latent-tree RL model for this task. The objective is to maximize the probability of the correct prediction under the expectation of a prior tree model, p(z|x; φ), Obj = Ez∼p(z|x;φ) [log p(y |z, x)] Computing the expectation is intractable so policy gradient is used. First a tree is sampled z˜ ∼ p(z|x; φ), then the gradient with respect to φ is approximated as, ∂ ∂ Obj ≈ (log p(y |˜ z , x) − b)( p(z|x; φ)) ∂φ ∂φ where b is a variance reduction baseline. A common choice is the self-critical baseline (Rennie et al., 2017), b = log p(y |z ∗ , x) with z ∗ = arg max p(z|"
2020.acl-demos.38,D07-1015,0,0.259887,"Missing"
2020.acl-demos.38,D09-1005,0,0.0316472,"Missing"
2020.acl-demos.38,D10-1004,0,0.081246,"Missing"
2020.acl-demos.38,H05-1066,0,0.379666,"Missing"
2020.acl-demos.38,N18-4013,0,0.0250787,"by the library, then present a technical description of the methods used, and finally present several example use cases. 2 Related Work Several software libraries target structured prediction. Optimization tools, such as SVM3 Motivating Case Study While structured prediction is traditionally presented at the output layer, recent applications have deployed structured models broadly within neural networks (Johnson et al., 2016; Kim et al., 2017; Yogatama et al., 2016, inter alia). Torch-Struct 336 aims to encourage this general use case. To illustrate, we consider a latent tree model. ListOps (Nangia and Bowman, 2018) is a dataset of mathematical functions. Each input/output pair consists of a prefix expression x and its result y, e.g. x = [ MAX 2 9 [ MIN 4 7 ] 0 ] y = 9 Models such as a flat RNN will fail to capture the hierarchical structure of this task. However, if a model can induce an explicit latent z, the parse tree of the expression, then the task is easy to learn by a tree-RNN model p(y|x, z) (Yogatama et al., 2016; Havrylov et al., 2019). Let us briefly summarize a latent-tree RL model for this task. The objective is to maximize the probability of the correct prediction under the expectation of"
2020.acl-demos.38,J03-1005,0,0.0464295,"namic programming and spanning tree algorithms as well as heuristic techniques such linear programming relaxations and greedy search. Structured prediction has played a key role in the history of natural language processing. Example methods include techniques for sequence labeling and segmentation (Lafferty et al., 2001; Sarawagi and Cohen, 2005), discriminative dependency and constituency parsing (Finkel et al., 2008; McDonald et al., 2005), unsupervised learning for labeling and alignment (Vogel et al., 1996; Goldwater and Griffiths, 2007), approximate translation decoding with beam search (Tillmann and Ney, 2003), among many others. In recent years, research into deep structured prediction has studied how these approaches can be integrated with neural networks and pretrained models. One line of work has utilized structured prediction as the final layer for deep models (Collobert et al., 2011; Durrett and Klein, 2015). Another has incorporated structured prediction within deep learning models, exploring novel models for latentstructure learning, unsupervised learning, or model control (Johnson et al., 2016; Yogatama et al., 2016; Wiseman et al., 2018). We aspire to make both of these use-cases as easy"
2020.acl-demos.38,C96-2141,0,0.0971854,"arameter estimation over these structures. Core methods include both tractable exact approaches like dynamic programming and spanning tree algorithms as well as heuristic techniques such linear programming relaxations and greedy search. Structured prediction has played a key role in the history of natural language processing. Example methods include techniques for sequence labeling and segmentation (Lafferty et al., 2001; Sarawagi and Cohen, 2005), discriminative dependency and constituency parsing (Finkel et al., 2008; McDonald et al., 2005), unsupervised learning for labeling and alignment (Vogel et al., 1996; Goldwater and Griffiths, 2007), approximate translation decoding with beam search (Tillmann and Ney, 2003), among many others. In recent years, research into deep structured prediction has studied how these approaches can be integrated with neural networks and pretrained models. One line of work has utilized structured prediction as the final layer for deep models (Collobert et al., 2011; Durrett and Klein, 2015). Another has incorporated structured prediction within deep learning models, exploring novel models for latentstructure learning, unsupervised learning, or model control (Johnson et"
2020.acl-demos.38,D18-1356,1,0.808242,"pproximate translation decoding with beam search (Tillmann and Ney, 2003), among many others. In recent years, research into deep structured prediction has studied how these approaches can be integrated with neural networks and pretrained models. One line of work has utilized structured prediction as the final layer for deep models (Collobert et al., 2011; Durrett and Klein, 2015). Another has incorporated structured prediction within deep learning models, exploring novel models for latentstructure learning, unsupervised learning, or model control (Johnson et al., 2016; Yogatama et al., 2016; Wiseman et al., 2018). We aspire to make both of these use-cases as easy to use as standard neural networks. The practical challenge of employing structured 335 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 335–342 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics Name Structure (Z) Parts (P) Algorithm (A(`)) LoC T/S Sample Reference Linear-Chain, HMM Factorial-HMM Labeled Chain Edges (N C 2 ) Labeled Chains Alignment Alignment Semi-Markov Seg. Labels Trans. (LC 2 ) Obs. (N C L ) Match (N M ) Skips (2N M ) Edge(N KC 2 ) ForwardBackward Factori"
2020.acl-demos.38,P18-4013,0,0.03183,"many required algorithms are difficult to implement efficiently and correctly. Most projects reimplement custom versions of standard algorithms or focus particularly on a single welldefined model class. This research style makes it difficult to combine and try out new approaches, a problem that has compounded with the complexity of research in deep structured prediction. With this challenge in mind, we introduce TorchStruct with three specific contributions: struct (Joachims, 2008), focus on parameter estimation. Model libraries, such as CRFSuite (Okazaki, 2007), CRF++ (Kudo, 2005), or NCRF++(Yang and Zhang, 2018), implement inference for a fixed set of popular models, usually linear-chain CRFs. General-purpose inference libraries, such as PyStruct (M¨uller and Behnke, 2014) or TurboParser (Martins et al., 2010), utilize external solvers for (primarily MAP) inference such as integer linear programming solvers and ADMM. Probabilistic programming languages, for example languages that integrate with deep learning such as Pyro (Bingham et al., 2019), allow for specification and inference over some discrete domains. Most ambitiously, inference libraries such as Dyna (Eisner et al., 2004) allow for declarati"
2020.acl-demos.38,P07-1094,0,\N,Missing
2020.acl-demos.38,P15-1030,0,\N,Missing
2020.acl-demos.38,J99-4004,0,\N,Missing
2020.acl-main.234,D16-1203,0,0.0227802,"the recent inference procedure analysis of Dyer et al. (2019). While they study what biases a specific inference algorithm introduces to the unsupervised parsing problem, we focus on the representation induced in a grounded version of the task. Our empirical analysis is related to Htut et al. (2018), who methodologically, and successfully replicate the results of Shen et al. (2018a) to study their performance. The issues we study generalize beyond the parsing task. The question of what is captured by vision and language models has been studied before, including for visual question answering (Agrawal et al., 2016, 2017; Goyal et al., 2017), referring expression resolution (Cirik et al., 2018), and visual navigation (Jain et al., 2019). We ask this question in the setting of syntactic parsing, which allows to ground the analysis in the underlying formalism. Our conclusions are similar: multi-modal models often rely on simple signals, and do not exhibit the complex reasoning we would like them to acquire. Acknowledgements Special thanks to Freda Shi for code release and prompt help in re-producing the experiments of Shi et al. (2019). This work was supported by the NSF (CRII-1656998, IIS-1901030), a Goo"
2020.acl-main.234,D11-1063,0,0.0273772,"1d embeddings and a non-parameterized scoring function, are still competitive (1, sM , cME ) or even outperform (1, sMHI , cMX ) the original VG-NSL. Our simplified model variations largely learn the 2617 Training Setting Basic Setting +HI +HI+FastText +HI+FastText-IN 1, sWS , cME 2, sWS , cME 72.0 78.2 80.5 85.6 77.5 80.3 83.1 86.4 U Model 87.5 91.8 92.3 92.8 Table 2: Self-F1 agreement between two of our variations and the original VG-NSL model. We also report the upper bound scores (U ) calculated by directly comparing two separately trained sets of five original VG-NSL models. 1, sWS , cME Turney et al. (2011) Brysbaert et al. (2014) Hessel et al. (2018) 0.73 0.75 0.89 Shi2019∗ 0.94 Table 3: Pearson correlation coefficient of concreteness estimates between our 1, sWS , cME variant and existing concreteness estimates, including reproduced estimates derived from VG-NSL by Shi et al. (2019). d=2 Figure 2: Noun distribution using the 1d representation from the 1, sWS , cME variant. The nouns are sorted by their representation value in increasing order from left. d=1 Figure 1: Token embedding visualization for 2, sWS , cME (top) and 1, sWS , cME (bottom) colored by universal POS tags (Petrov et al., 201"
2020.acl-main.234,Q18-1019,0,0.0297571,"ion, as well as for our analysis and visualization in Section 5. We generate binary gold-trees using Benepar (Kitaev and Klein, 2018), an off-the-shelf supervised constituency parser. We notate model variations as d, score, combine. For example, 1, sWS , cME refers to dimensionality d = 1, weighted sum scoring function (sWS ), and mean pooling combine (cME ). We train five models for each variation, and select the best checkpoint for each model by maximizing the parse prediction agreement on the validation captions between five models. The agreement is measured by the self-F1 agreement score (Williams et al., 2018). This procedure is directly adopted from Shi et al. (2019). We use the hyper-parameters from the original implementation without further tuning. 1 The authors of Shi et al. (2019) suggested this ablation as particularly impactful on the learning outcome. PP ADJP Avg. F1 NP VP Shi2019 Shi2019∗ 1, sWS , cME 2, sWS , cME 79.6 80.5 77.2 80.8 26.2 26.9 17.0 19.1 42.0 45.0 53.4 52.3 22.0 21.3 18.2 17.1 50.4 ± 0.3 51.4 ± 1.1 49.7 ± 5.9 51.6 ± 0.6 +HI Shi2019 Shi2019∗ 1, sWS , cME 2, sWS , cME 74.6 73.1 74.0 73.8 32.5 33.9 35.2 30.2 66.5 64.5 62.0 63.7 21.7 22.5 24.2 21.9 53.3 ± 0.2 51.8 ± 0.3 51.8 ±"
2020.acl-main.234,P96-1025,0,\N,Missing
2020.acl-main.234,petrov-etal-2012-universal,0,\N,Missing
2020.acl-main.234,W18-5452,0,\N,Missing
2020.acl-main.234,D18-1544,0,\N,Missing
2020.acl-main.234,D18-1287,1,\N,Missing
2020.acl-main.234,N19-1115,0,\N,Missing
2020.acl-main.234,N19-1114,1,\N,Missing
2020.acl-main.234,N18-2123,0,\N,Missing
2020.acl-main.234,P19-1181,0,\N,Missing
2020.acl-main.234,N19-1116,0,\N,Missing
2020.acl-main.234,D17-1106,1,\N,Missing
2020.acl-main.234,D16-1044,0,\N,Missing
2020.acl-main.243,E06-1040,0,0.0668248,"latent tree variable in a variational autoencoding model with a CRF as the inference network, and on Yin et al. (2018) who use an encoder-decoder model as the inference network. 7 Experimental Setup Data and Metrics We consider two standard neural generation benchmarks: E2E (Novikova et al., 2017) and WikiBio (Lebret et al., 2016a) datasets, with examples shown in Figure 1. The E2E dataset contains approximately 50K examples with 8 distinct fields and 945 distinct word types; it contains multiple test references for one source table. We evaluate in terms of BLEU (Papineni et al., 2002), NIST (Belz and Reiter, 2006), ROUGE-L 2735 Table (x): name[Clowns] eatType[coffee shop] food[Chinese] customer-rating[1 out of 5] area[riverside] near[Clare Hall] Ref.1: Frederick ParkerRhodes (21 March 1914 Ref.1: Clowns is a coffee shop in the riverside area near Clare Hall that has a rating 1 out of 5 . They serve Chinese food . Ref.2: The Chinese coffee shop by the riverside near Clare Hall that only has a customer rating of 1 out of 5 is called Clowns . Ref.3: There is a Chinese coffee shop near Clare Hall in the riverside area called Clowns its not got a good rating though . – 21 November 1987) was an English lingu"
2020.acl-main.243,P19-1599,0,0.0202446,"al., 2018; Shen et al., 2017). Closest to this work is that of Wiseman et al. (2018) who control phrase-level content by using a neuralized hidden semi-Markov model for generation itself. Our work differs in that it makes no independence assumption on the decoder model, uses a faster training algorithm, and proposes a specific method for adding constraints. Finally, there is a line of work that manipulates the syntactic structure of generated texts, by using some labeled syntactic attribute (e.g., parses) or an exemplar (Deriu and Cieliebak, 2018; Colin and Gardent, 2018; Iyyer et al., 2018; Chen et al., 2019). While our work uses control states, there is no inherent assumption of compositional syntax or grammar. Posterior regularization (PR) is mostly used in standard EM settings to impose constraints on the posterior distribution that would otherwise be intractable (or computationally hard) in the prior. Ganchev et al. (2010) applies posterior regularization to word alignment, dependency parsing, and part-of-speech tagging. Combining powerful deep neural networks with structured knowledge has been a popular area of study: Xu et al. (2019) applies PR to multi-object generation to limit object over"
2020.acl-main.243,D18-1113,0,0.0215413,"Hu et al., 2017; Oraby et al., 2018; Zhang et al., 2018; Shen et al., 2017). Closest to this work is that of Wiseman et al. (2018) who control phrase-level content by using a neuralized hidden semi-Markov model for generation itself. Our work differs in that it makes no independence assumption on the decoder model, uses a faster training algorithm, and proposes a specific method for adding constraints. Finally, there is a line of work that manipulates the syntactic structure of generated texts, by using some labeled syntactic attribute (e.g., parses) or an exemplar (Deriu and Cieliebak, 2018; Colin and Gardent, 2018; Iyyer et al., 2018; Chen et al., 2019). While our work uses control states, there is no inherent assumption of compositional syntax or grammar. Posterior regularization (PR) is mostly used in standard EM settings to impose constraints on the posterior distribution that would otherwise be intractable (or computationally hard) in the prior. Ganchev et al. (2010) applies posterior regularization to word alignment, dependency parsing, and part-of-speech tagging. Combining powerful deep neural networks with structured knowledge has been a popular area of study: Xu et al. (2019) applies PR to mult"
2020.acl-main.243,W18-6503,0,0.012332,"generative neural models (Hu et al., 2017; Oraby et al., 2018; Zhang et al., 2018; Shen et al., 2017). Closest to this work is that of Wiseman et al. (2018) who control phrase-level content by using a neuralized hidden semi-Markov model for generation itself. Our work differs in that it makes no independence assumption on the decoder model, uses a faster training algorithm, and proposes a specific method for adding constraints. Finally, there is a line of work that manipulates the syntactic structure of generated texts, by using some labeled syntactic attribute (e.g., parses) or an exemplar (Deriu and Cieliebak, 2018; Colin and Gardent, 2018; Iyyer et al., 2018; Chen et al., 2019). While our work uses control states, there is no inherent assumption of compositional syntax or grammar. Posterior regularization (PR) is mostly used in standard EM settings to impose constraints on the posterior distribution that would otherwise be intractable (or computationally hard) in the prior. Ganchev et al. (2010) applies posterior regularization to word alignment, dependency parsing, and part-of-speech tagging. Combining powerful deep neural networks with structured knowledge has been a popular area of study: Xu et al."
2020.acl-main.243,P16-2008,0,0.0256064,"es over standard benchmarks, while also providing fine-grained control. 1 Introduction A core challenge in using deep learning for NLP is developing methods that allow for controlled output while maintaining the broad coverage of data-driven methods. While this issue is less problematic in classification tasks, it has hampered the deployment of systems for conditional natural language generation (NLG), where users often need to control output through task-specific knowledge or plans. While there have been significant improvements in generation quality from automatic systems (Mei et al., 2016; Dusek and Jurcicek, 2016; Lebret et al., 2016b), these methods are still far from being able to produce controlled output (Wiseman et al., 2017). Recent state-of-the-art system have even begun to utilize manual control through rulebased planning modules (Moryossef et al., 2019; Puduppully et al., 2019). Consider the case of encoder-decoder models for generation, built with RNNs or transformers. These models generate fluent output and provide flexible representations of their conditioning. Unfortunately, auto-regressive decoders are also globally dependent, which makes it challenging to incorporate domain constraints."
2020.acl-main.243,J99-4004,0,0.0249703,"en, 2005). We store two tables β and β , both of size T × |C|. βt (c) denotes the event that there is a transition at time t from state c. βt0 (c) denotes the event that there is a emission starting from time t at state c. Then we have the recursion for βt0 (c) by “summing” over different span length, and we have the recursion for βt (c) that sums over all different state transitions. The algorithm is generic in the sense that different (⊗, ⊕) operators allow us to compute different needed terms. For example, computing the parP tition function Z = φ(x, y, z 0 ) requires the z0 (+,×) semiring (Goodman, 1999; Li and Eisner, 2009), other distributional terms can be computed by using the same algorithm with alternative semirings and backpropagation 3 . 5 Posterior Constraints from Data Alignment To make the PR model concrete, we consider the problem of incorporating weak supervision from heuristic alignment in a data-to-text generation task. Assume that we are tasked with describing a table x consisting of global field names F each with a text value v, e.g. xf = v. Not all global fields may be used in a given x, we use f ∈ x to indicate an 3 We need four terms: (a) log-partition term P log z0 φ(x,"
2020.acl-main.243,P16-1154,0,0.197839,"ng15 (1, 2, name), (4, 6, eatType), (7, 9, near), (11, 15, rating) Table 2: Example of data alignment notation. Here x is a table of data, and f are its fields. For a given output y we enforce a soft alignment A. active field. We would like control states to indicate when each field is used in generation. Our alignment heuristic is that often these fields will be expressed using the identical text as in the table. While this heuristic obviously does not account for all cases, it is very common in natural language generation tasks as evidence by the wide use of copy attention based approaches (Gu et al., 2016; Gulcehre et al., 2016). To utilize these alignments, we use the notation (i, j, f ) ∈ A(x, y) to indicate that a span i : j in the training text y overlaps directly with a field f ∈ x. Table 2 gives an example of the notation. One-to-One Constraints We first consider oneto-one constraints where we assume that we have a static, mapping from fields to states σ : F 7→ C. Given this mapping, we need to add penalties to encourage the semi-Markov model to overlap with the given weak supervision. To enforce soft alignments, we define three posterior constraint types and their computation as shown i"
2020.acl-main.243,P16-1014,0,0.18045,", (4, 6, eatType), (7, 9, near), (11, 15, rating) Table 2: Example of data alignment notation. Here x is a table of data, and f are its fields. For a given output y we enforce a soft alignment A. active field. We would like control states to indicate when each field is used in generation. Our alignment heuristic is that often these fields will be expressed using the identical text as in the table. While this heuristic obviously does not account for all cases, it is very common in natural language generation tasks as evidence by the wide use of copy attention based approaches (Gu et al., 2016; Gulcehre et al., 2016). To utilize these alignments, we use the notation (i, j, f ) ∈ A(x, y) to indicate that a span i : j in the training text y overlaps directly with a field f ∈ x. Table 2 gives an example of the notation. One-to-One Constraints We first consider oneto-one constraints where we assume that we have a static, mapping from fields to states σ : F 7→ C. Given this mapping, we need to add penalties to encourage the semi-Markov model to overlap with the given weak supervision. To enforce soft alignments, we define three posterior constraint types and their computation as shown in Table 1 (Left). The th"
2020.acl-main.243,P16-1228,0,0.0357547,"Missing"
2020.acl-main.243,D16-1173,0,0.03259,"Missing"
2020.acl-main.243,N18-1170,0,0.0270139,"al., 2018; Zhang et al., 2018; Shen et al., 2017). Closest to this work is that of Wiseman et al. (2018) who control phrase-level content by using a neuralized hidden semi-Markov model for generation itself. Our work differs in that it makes no independence assumption on the decoder model, uses a faster training algorithm, and proposes a specific method for adding constraints. Finally, there is a line of work that manipulates the syntactic structure of generated texts, by using some labeled syntactic attribute (e.g., parses) or an exemplar (Deriu and Cieliebak, 2018; Colin and Gardent, 2018; Iyyer et al., 2018; Chen et al., 2019). While our work uses control states, there is no inherent assumption of compositional syntax or grammar. Posterior regularization (PR) is mostly used in standard EM settings to impose constraints on the posterior distribution that would otherwise be intractable (or computationally hard) in the prior. Ganchev et al. (2010) applies posterior regularization to word alignment, dependency parsing, and part-of-speech tagging. Combining powerful deep neural networks with structured knowledge has been a popular area of study: Xu et al. (2019) applies PR to multi-object generation"
2020.acl-main.243,N19-1114,1,0.902226,"θ (z |x, y), i.e. the probability of over state sequences for a known output. The decoder parameterization makes this distribution intractable to compute in general. We instead use variational inference to define a parameterized variational posterior distribution, qφ (z |x, y), from a preselected family of possible distributions Q.1 To fit the model parameters θ, we utilize the evidence lower bound (for any variational parameters φ), Several recent works have shown methods for effectively fitting neural models with structured variational inference (Johnson et al., 2016; Krishnan et al., 2017; Kim et al., 2019). We therefore use these techniques as a backbone for enforcing problem-specific control. See §4 for a full description of the variational family used. 3 Posterior Regularization of Control States Posterior regularization (PR) is an approach for enforcing soft constraints on the posterior distribution of generative models (Ganchev et al., 2010). Our goal is to utilize these soft constraints to enforce problem specific weak supervision. Traditionally PR uses linear constraints which in the special case of expectation maximization for exponential families leads to convenient closed-form training"
2020.acl-main.243,P18-1249,0,0.0158366,"erence for one source table. We follow the metrics from (Lebret et al., 2016a) and evaluate the BLEU, NIST, and ROUGE4 scores. Architecture and Hyperparameters For all tasks, we use an encoder-decoder LSTM for the generative model. We follow recent state-of-the-art works in parametrizing our encoder, and we use copy attention and dual attention (Gu et al., 2016; Gulcehre et al., 2016; Liu et al., 2018): full model architectures are given in the supplement. The inference network scores are computed using a BiLSTM. We compute the emission scores φ(e) using span embeddings (Wang and Chang, 2016; Kitaev and Klein, 2018; Stern et al., 2017); transition scores φ(t) by dot product between embedding vectors for the class labels; lengths φ(l) is kept uniform, as in Wiseman et al. (2018). Additional details are in the supplement. At training time, we use a rate for alleviating posterior collapse in the ELBO: warm-up the ELBO objective by annealing the coeffiPlinearly T cient on the term t=1 log pθ (zt |z&lt;t , y&lt;t ) and H[qφ (z |x, y)] from 0 to 1, as implemented in Kim et al. (2019). We use the REINFORCE algorithm to do Monte Carlo estimation of the stochastic gradient. We choose the control variate to be the mean"
2020.acl-main.243,W07-0734,0,0.0179128,"rside near Clare Hall that only has a customer rating of 1 out of 5 is called Clowns . Ref.3: There is a Chinese coffee shop near Clare Hall in the riverside area called Clowns its not got a good rating though . – 21 November 1987) was an English linguist, plant pathologist, computer scientist, mathematician, mystic, and mycologist. Figure 2: Generation benchmarks. Model is given a table x consisting of semantic fields and is tasked with generating a description y1:T of this data. Two example datasets are shown. Left: E2E, Right: WikiBio. (Lin, 2004), CIDEr (Vedantam et al., 2015) and METEOR (Lavie and Agarwal, 2007), using the official scoring scripts4 . The WikiBio dataset contains approximately 700K examples, 6K distinct table field types, and 400K word types approximately; it contains one reference for one source table. We follow the metrics from (Lebret et al., 2016a) and evaluate the BLEU, NIST, and ROUGE4 scores. Architecture and Hyperparameters For all tasks, we use an encoder-decoder LSTM for the generative model. We follow recent state-of-the-art works in parametrizing our encoder, and we use copy attention and dual attention (Gu et al., 2016; Gulcehre et al., 2016; Liu et al., 2018): full model"
2020.acl-main.243,D16-1128,0,0.0322208,"Missing"
2020.acl-main.243,D09-1005,0,0.0462944,"tore two tables β and β , both of size T × |C|. βt (c) denotes the event that there is a transition at time t from state c. βt0 (c) denotes the event that there is a emission starting from time t at state c. Then we have the recursion for βt0 (c) by “summing” over different span length, and we have the recursion for βt (c) that sums over all different state transitions. The algorithm is generic in the sense that different (⊗, ⊕) operators allow us to compute different needed terms. For example, computing the parP tition function Z = φ(x, y, z 0 ) requires the z0 (+,×) semiring (Goodman, 1999; Li and Eisner, 2009), other distributional terms can be computed by using the same algorithm with alternative semirings and backpropagation 3 . 5 Posterior Constraints from Data Alignment To make the PR model concrete, we consider the problem of incorporating weak supervision from heuristic alignment in a data-to-text generation task. Assume that we are tasked with describing a table x consisting of global field names F each with a text value v, e.g. xf = v. Not all global fields may be used in a given x, we use f ∈ x to indicate an 3 We need four terms: (a) log-partition term P log z0 φ(x, y, z 0 ) requires the"
2020.acl-main.243,W04-1013,0,0.010023,"nese food . Ref.2: The Chinese coffee shop by the riverside near Clare Hall that only has a customer rating of 1 out of 5 is called Clowns . Ref.3: There is a Chinese coffee shop near Clare Hall in the riverside area called Clowns its not got a good rating though . – 21 November 1987) was an English linguist, plant pathologist, computer scientist, mathematician, mystic, and mycologist. Figure 2: Generation benchmarks. Model is given a table x consisting of semantic fields and is tasked with generating a description y1:T of this data. Two example datasets are shown. Left: E2E, Right: WikiBio. (Lin, 2004), CIDEr (Vedantam et al., 2015) and METEOR (Lavie and Agarwal, 2007), using the official scoring scripts4 . The WikiBio dataset contains approximately 700K examples, 6K distinct table field types, and 400K word types approximately; it contains one reference for one source table. We follow the metrics from (Lebret et al., 2016a) and evaluate the BLEU, NIST, and ROUGE4 scores. Architecture and Hyperparameters For all tasks, we use an encoder-decoder LSTM for the generative model. We follow recent state-of-the-art works in parametrizing our encoder, and we use copy attention and dual attention (G"
2020.acl-main.243,P19-1603,0,0.0190473,"cabulary entry in σ should have low entropy; ii) Fit: The global σ should represent the class name distribution posterior of each table field by minimizing the cross entropy between types σ(c |f ) and tokens q(zi:j |x, y) for all (i, j, f ) ∈ A(x, y); iii) Diversity: the aggregate class label distribution over all the token in a sentence should have high entropy. 6 Related Work In addition to previously mentioned work, other researchers have noted the lack of control of deep neural networks and proposed methods at sentencelevel, word-level, and phrase-level. For example Peng et al. (2018) and Luo et al. (2019) control the sentiment in longer-form story generation. Others aim for sentence-level properties such as sentiment, style, tense, and specificity in generative neural models (Hu et al., 2017; Oraby et al., 2018; Zhang et al., 2018; Shen et al., 2017). Closest to this work is that of Wiseman et al. (2018) who control phrase-level content by using a neuralized hidden semi-Markov model for generation itself. Our work differs in that it makes no independence assumption on the decoder model, uses a faster training algorithm, and proposes a specific method for adding constraints. Finally, there is a"
2020.acl-main.243,N16-1086,0,0.0243437,"this method improves over standard benchmarks, while also providing fine-grained control. 1 Introduction A core challenge in using deep learning for NLP is developing methods that allow for controlled output while maintaining the broad coverage of data-driven methods. While this issue is less problematic in classification tasks, it has hampered the deployment of systems for conditional natural language generation (NLG), where users often need to control output through task-specific knowledge or plans. While there have been significant improvements in generation quality from automatic systems (Mei et al., 2016; Dusek and Jurcicek, 2016; Lebret et al., 2016b), these methods are still far from being able to produce controlled output (Wiseman et al., 2017). Recent state-of-the-art system have even begun to utilize manual control through rulebased planning modules (Moryossef et al., 2019; Puduppully et al., 2019). Consider the case of encoder-decoder models for generation, built with RNNs or transformers. These models generate fluent output and provide flexible representations of their conditioning. Unfortunately, auto-regressive decoders are also globally dependent, which makes it challenging to incor"
2020.acl-main.243,N19-1236,0,0.0330157,"Missing"
2020.acl-main.243,W17-5525,0,0.0700309,"blem-specific knowledge. At test time, the control states can be ignored or utilized as grounding for test-time constraints. Technically, the approach builds on recent advances in structured amortized variational inference to enforce additional constraints on the learned distribution. These constraints are enforced through efficient structured posterior calculations and do not hamper modeling power. We demonstrate that the method can improve accuracy and control, while utilizing a range of different posterior constraints. In particular on two large-scale data-to-text generation datasets, E2E (Novikova et al., 2017) and WikiBio (Lebret et al., 2016a), our method increases the performance of benchmark systems while also producing outputs that respect the grounded control states. Our code is available at https://github.com/XiangLi1999/ 2731 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2731–2743 c July 5 - 10, 2020. 2020 Association for Computational Linguistics PosteriorControl-NLG. 2 Control States for Blackbox Generation Consider a conditional generation setting where the input consists of an arbitrary context x and the output y1:T is a sequence of target"
2020.acl-main.243,W18-5019,0,0.0187267,"(zi:j |x, y) for all (i, j, f ) ∈ A(x, y); iii) Diversity: the aggregate class label distribution over all the token in a sentence should have high entropy. 6 Related Work In addition to previously mentioned work, other researchers have noted the lack of control of deep neural networks and proposed methods at sentencelevel, word-level, and phrase-level. For example Peng et al. (2018) and Luo et al. (2019) control the sentiment in longer-form story generation. Others aim for sentence-level properties such as sentiment, style, tense, and specificity in generative neural models (Hu et al., 2017; Oraby et al., 2018; Zhang et al., 2018; Shen et al., 2017). Closest to this work is that of Wiseman et al. (2018) who control phrase-level content by using a neuralized hidden semi-Markov model for generation itself. Our work differs in that it makes no independence assumption on the decoder model, uses a faster training algorithm, and proposes a specific method for adding constraints. Finally, there is a line of work that manipulates the syntactic structure of generated texts, by using some labeled syntactic attribute (e.g., parses) or an exemplar (Deriu and Cieliebak, 2018; Colin and Gardent, 2018; Iyyer et a"
2020.acl-main.243,P02-1040,0,0.106944,"t al. (2019), who introduce a latent tree variable in a variational autoencoding model with a CRF as the inference network, and on Yin et al. (2018) who use an encoder-decoder model as the inference network. 7 Experimental Setup Data and Metrics We consider two standard neural generation benchmarks: E2E (Novikova et al., 2017) and WikiBio (Lebret et al., 2016a) datasets, with examples shown in Figure 1. The E2E dataset contains approximately 50K examples with 8 distinct fields and 945 distinct word types; it contains multiple test references for one source table. We evaluate in terms of BLEU (Papineni et al., 2002), NIST (Belz and Reiter, 2006), ROUGE-L 2735 Table (x): name[Clowns] eatType[coffee shop] food[Chinese] customer-rating[1 out of 5] area[riverside] near[Clare Hall] Ref.1: Frederick ParkerRhodes (21 March 1914 Ref.1: Clowns is a coffee shop in the riverside area near Clare Hall that has a rating 1 out of 5 . They serve Chinese food . Ref.2: The Chinese coffee shop by the riverside near Clare Hall that only has a customer rating of 1 out of 5 is called Clowns . Ref.3: There is a Chinese coffee shop near Clare Hall in the riverside area called Clowns its not got a good rating though . – 21 Novem"
2020.acl-main.243,W18-1505,0,0.0235654,"σ: i) Sparsity: Each vocabulary entry in σ should have low entropy; ii) Fit: The global σ should represent the class name distribution posterior of each table field by minimizing the cross entropy between types σ(c |f ) and tokens q(zi:j |x, y) for all (i, j, f ) ∈ A(x, y); iii) Diversity: the aggregate class label distribution over all the token in a sentence should have high entropy. 6 Related Work In addition to previously mentioned work, other researchers have noted the lack of control of deep neural networks and proposed methods at sentencelevel, word-level, and phrase-level. For example Peng et al. (2018) and Luo et al. (2019) control the sentiment in longer-form story generation. Others aim for sentence-level properties such as sentiment, style, tense, and specificity in generative neural models (Hu et al., 2017; Oraby et al., 2018; Zhang et al., 2018; Shen et al., 2017). Closest to this work is that of Wiseman et al. (2018) who control phrase-level content by using a neuralized hidden semi-Markov model for generation itself. Our work differs in that it makes no independence assumption on the decoder model, uses a faster training algorithm, and proposes a specific method for adding constraint"
2020.acl-main.243,P19-1195,0,0.0374688,"Missing"
2020.acl-main.243,N19-1410,0,0.0748696,"m/tuetschek/e2e-metrics tion to jointly generate both the control states and the sentences. To obtain controlled generation, we observe the control states, and apply constrained beam search to p(y |x, z). Baselines For generation on E2E, we compare externally against 4 systems: E2E-B ENCHMARK (Duˇsek and Jurˇc´ıcˇ ek, 2016) is an encoder-decoder network followed by a reranker used as the shared task benchmark; NT EMP, a controllable neuralized hidden semi-Markov model; NT EMP +AR, the product of experts of both a NTemp model and an autoregressive LSTM network (Wiseman et al., 2018); S HEN 19 (Shen et al., 2019) is an pragmatically informed model, which is the current state-ofthe-art system on E2E dataset. We also compare internally with ablations of our system: E NC D EC is a conditional model p(y |x) trained without control states. PC0 is posterior control model with no constraints. It uses structured encoder with the PR coefficient set to 0. PC∞ is our model with hard constraints, which assumes fully-observed control states. These control states are obtained by mapping tokens with lexical overlap to their designated state; otherwise we map to a generic state. We train a seq2seq model p(y, z |x) wi"
2020.acl-main.243,P17-1076,0,0.0248875,"ble. We follow the metrics from (Lebret et al., 2016a) and evaluate the BLEU, NIST, and ROUGE4 scores. Architecture and Hyperparameters For all tasks, we use an encoder-decoder LSTM for the generative model. We follow recent state-of-the-art works in parametrizing our encoder, and we use copy attention and dual attention (Gu et al., 2016; Gulcehre et al., 2016; Liu et al., 2018): full model architectures are given in the supplement. The inference network scores are computed using a BiLSTM. We compute the emission scores φ(e) using span embeddings (Wang and Chang, 2016; Kitaev and Klein, 2018; Stern et al., 2017); transition scores φ(t) by dot product between embedding vectors for the class labels; lengths φ(l) is kept uniform, as in Wiseman et al. (2018). Additional details are in the supplement. At training time, we use a rate for alleviating posterior collapse in the ELBO: warm-up the ELBO objective by annealing the coeffiPlinearly T cient on the term t=1 log pθ (zt |z&lt;t , y&lt;t ) and H[qφ (z |x, y)] from 0 to 1, as implemented in Kim et al. (2019). We use the REINFORCE algorithm to do Monte Carlo estimation of the stochastic gradient. We choose the control variate to be the mean of the samples (Mnih"
2020.acl-main.243,P16-1218,0,0.025415,"y; it contains one reference for one source table. We follow the metrics from (Lebret et al., 2016a) and evaluate the BLEU, NIST, and ROUGE4 scores. Architecture and Hyperparameters For all tasks, we use an encoder-decoder LSTM for the generative model. We follow recent state-of-the-art works in parametrizing our encoder, and we use copy attention and dual attention (Gu et al., 2016; Gulcehre et al., 2016; Liu et al., 2018): full model architectures are given in the supplement. The inference network scores are computed using a BiLSTM. We compute the emission scores φ(e) using span embeddings (Wang and Chang, 2016; Kitaev and Klein, 2018; Stern et al., 2017); transition scores φ(t) by dot product between embedding vectors for the class labels; lengths φ(l) is kept uniform, as in Wiseman et al. (2018). Additional details are in the supplement. At training time, we use a rate for alleviating posterior collapse in the ELBO: warm-up the ELBO objective by annealing the coeffiPlinearly T cient on the term t=1 log pθ (zt |z&lt;t , y&lt;t ) and H[qφ (z |x, y)] from 0 to 1, as implemented in Kim et al. (2019). We use the REINFORCE algorithm to do Monte Carlo estimation of the stochastic gradient. We choose the contro"
2020.acl-main.243,D18-1356,1,0.686352,"corporate domain constraints. Research into controllable deep models aims to circumvent the all-or-nothing dependency tradeoff of encoder-decoder systems and expose explicit higher-level decisions. One line of research has looked at global control states that represent sentence-level properties for the full decoder. For example, Hu et al. (2017) uses generative adversarial networks where the attributes of the text (e.g., sentiment, tense) are exposed. Another line of research exposes fine-level properties, such as phrase type, but requires factoring the decoder to expose local decisions, e.g. Wiseman et al. (2018). This work proposes a method for augmenting any neural decoder architecture to incorporate finegrained control states. The approach first modifies training to incorporate structured latent control variables. Then, training constraints are added to anchor the state values to problem-specific knowledge. At test time, the control states can be ignored or utilized as grounding for test-time constraints. Technically, the approach builds on recent advances in structured amortized variational inference to enforce additional constraints on the learned distribution. These constraints are enforced thro"
2020.acl-main.243,P18-1070,0,0.0193307,"ove accuracy and interpretability. Finally, the core of this work is the use of amortized inference/variation autoencoder to approximate variational posterior (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014). We rely heavily on a structure distribution, either linear chain or semi-Markov, which was introduced as a structured VAEs (Johnson et al., 2016; Krishnan et al., 2017; Ammar et al., 2014). Our setting and optimization are based on Kim et al. (2019), who introduce a latent tree variable in a variational autoencoding model with a CRF as the inference network, and on Yin et al. (2018) who use an encoder-decoder model as the inference network. 7 Experimental Setup Data and Metrics We consider two standard neural generation benchmarks: E2E (Novikova et al., 2017) and WikiBio (Lebret et al., 2016a) datasets, with examples shown in Figure 1. The E2E dataset contains approximately 50K examples with 8 distinct fields and 945 distinct word types; it contains multiple test references for one source table. We evaluate in terms of BLEU (Papineni et al., 2002), NIST (Belz and Reiter, 2006), ROUGE-L 2735 Table (x): name[Clowns] eatType[coffee shop] food[Chinese] customer-rating[1 out"
2020.acl-main.243,P18-1102,0,0.0186753,"(i, j, f ) ∈ A(x, y); iii) Diversity: the aggregate class label distribution over all the token in a sentence should have high entropy. 6 Related Work In addition to previously mentioned work, other researchers have noted the lack of control of deep neural networks and proposed methods at sentencelevel, word-level, and phrase-level. For example Peng et al. (2018) and Luo et al. (2019) control the sentiment in longer-form story generation. Others aim for sentence-level properties such as sentiment, style, tense, and specificity in generative neural models (Hu et al., 2017; Oraby et al., 2018; Zhang et al., 2018; Shen et al., 2017). Closest to this work is that of Wiseman et al. (2018) who control phrase-level content by using a neuralized hidden semi-Markov model for generation itself. Our work differs in that it makes no independence assumption on the decoder model, uses a faster training algorithm, and proposes a specific method for adding constraints. Finally, there is a line of work that manipulates the syntactic structure of generated texts, by using some labeled syntactic attribute (e.g., parses) or an exemplar (Deriu and Cieliebak, 2018; Colin and Gardent, 2018; Iyyer et al., 2018; Chen et al"
2020.emnlp-demos.6,N19-4010,0,0.0647027,"Missing"
2020.emnlp-demos.6,P18-1031,0,0.0256687,"industrial deployments. The library is available at https://github.com/ huggingface/transformers. 1 Introduction The Transformer (Vaswani et al., 2017) has rapidly become the dominant architecture for natural language processing, surpassing alternative neural models such as convolutional and recurrent neural networks in performance for tasks in both natural language understanding and natural language generation. The architecture scales with training data and model size, facilitates efficient parallel training, and captures long-range sequence features. Model pretraining (McCann et al., 2017; Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2018) allows models to be trained on generic corpora and subsequently be easily adapted to specific tasks with strong performance. The Transformer architecture is particularly conducive to pretraining on large text corpora, leading to major gains in accuracy on downstream tasks including text classification (Yang et al., 2019), language understanding 1 https://github.com/huggingface/ transformers 2 https://huggingface.co/transformers/ 38 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 38–45 c November 16-20, 2020. 2020 Association for Computa"
2020.emnlp-demos.6,D19-1371,0,0.133995,"raining run facilitates fine-tuning on many specific tasks. The Model Hub makes it simple for any end-user to access a model for use with their own data. This hub now contains 2,097 user models, both pretrained and fine-tuned, from across the community. Figure 1 shows the increase and distribution of popular transformers over time. While core models like BERT and GPT-2 continue to be popular, other specialized models including DistilBERT (Sanh et al., 2019), which was developed for the library, are 41 Figure 3: Transformers Model Hub. (Left) Example of a model page and model card for SciBERT (Beltagy et al., 2019), a pretrained model targeting extraction from scientific literature submitted by a community contributor. (Right) Example of an automatic inference widget for the pretrained BART (Lewis et al., 2019) model for summarization. Users can enter arbitrary text and a full version of the model is deployed on the fly to produce a summary. be difficult for more general model collections. For example, because each uploaded model includes metadata concerning its structure, the model page can include live inference that allows users to experiment with output of models on a real data. Figure 3 (Right) sho"
2020.emnlp-demos.6,P17-4012,1,0.694521,"for model analysis, usage, deployment, benchmarking, and easy replicability. The NLP and ML communities have a strong culture of building open-source research tools. The structure of Transformers is inspired by the pioneering tensor2tensor library (Vaswani et al., 2018) and the original source code for BERT (Devlin et al., 2018), both from Google Research. The concept of providing easy caching for pretrained models stemmed from AllenNLP (Gardner et al., 2018). The library is also closely related to neural translation and language modeling systems, such as Fairseq (Ott et al., 2019), OpenNMT (Klein et al., 2017), Texar (Hu et al., 2018), Megatron-LM (Shoeybi et al., 2019), and Marian NMT (Junczys-Dowmunt et al., 2018). Building on these elements, Transformers adds extra user-facing features to allow for easy downloading, caching, and fine-tuning of the models as well as seamless transition to production. Transformers maintains some compatibility with these libraries, most directly including a tool for performing inference using models from Marian NMT and Google’s BERT. There is a long history of easy-to-use, userfacing libraries for general-purpose NLP. Two core libraries are NLTK (Loper and Bird, 20"
2020.emnlp-demos.6,P19-1285,0,0.0488469,"Missing"
2020.emnlp-demos.6,2020.lrec-1.302,0,0.064745,"Missing"
2020.emnlp-demos.6,2021.ccl-1.108,0,0.438837,"Missing"
2020.emnlp-demos.6,W02-0109,0,0.335056,"lein et al., 2017), Texar (Hu et al., 2018), Megatron-LM (Shoeybi et al., 2019), and Marian NMT (Junczys-Dowmunt et al., 2018). Building on these elements, Transformers adds extra user-facing features to allow for easy downloading, caching, and fine-tuning of the models as well as seamless transition to production. Transformers maintains some compatibility with these libraries, most directly including a tool for performing inference using models from Marian NMT and Google’s BERT. There is a long history of easy-to-use, userfacing libraries for general-purpose NLP. Two core libraries are NLTK (Loper and Bird, 2002) and Stanford CoreNLP (Manning et al., 2014), which collect a variety of different approaches to NLP in a single package. More recently, general-purpose, open-source libraries have focused primarily on machine learning for a variety of NLP tasks, these include Spacy (Honnibal and Montani, 2017), AllenNLP (Gardner et al., 2018), flair (Akbik et al., 2019), and Stanza (Qi et al., 2020). Transformers provides similar functionality as these libraries. Additionally, each of these libraries now uses the 3 Library Design Transformers is designed to mirror the standard NLP machine learning model pipel"
2020.emnlp-demos.6,P14-5010,0,0.00288817,"Megatron-LM (Shoeybi et al., 2019), and Marian NMT (Junczys-Dowmunt et al., 2018). Building on these elements, Transformers adds extra user-facing features to allow for easy downloading, caching, and fine-tuning of the models as well as seamless transition to production. Transformers maintains some compatibility with these libraries, most directly including a tool for performing inference using models from Marian NMT and Google’s BERT. There is a long history of easy-to-use, userfacing libraries for general-purpose NLP. Two core libraries are NLTK (Loper and Bird, 2002) and Stanford CoreNLP (Manning et al., 2014), which collect a variety of different approaches to NLP in a single package. More recently, general-purpose, open-source libraries have focused primarily on machine learning for a variety of NLP tasks, these include Spacy (Honnibal and Montani, 2017), AllenNLP (Gardner et al., 2018), flair (Akbik et al., 2019), and Stanza (Qi et al., 2020). Transformers provides similar functionality as these libraries. Additionally, each of these libraries now uses the 3 Library Design Transformers is designed to mirror the standard NLP machine learning model pipeline: process data, apply a model, and make p"
2020.emnlp-demos.6,P19-1452,0,0.0213299,"its predictions. An example model card is shown in Figure 3 (Left). Since the Model Hub is specific to transformerbased models, we can target use cases that would 42 interested in developing a test bed for the performance of Transformers on a variety of different semantic recognition tasks. Their framework Jiant (Pruksachatkun et al., 2020) allows them to experiment with different ways of pretraining models and comparing their outputs. They used the Transformers API as a generic front-end and performed fine-tuning on a variety of different models, leading to research on the structure of BERT (Tenney et al., 2019). Case 3: Application Users Plot.ly, a company focused on user dashboards and analytics, was interested in deploying a model for automatic document summarization. They wanted an approach that scaled well and was simple to deploy, but had no need to train or fine-tune the model. They were able to search the Model Hub and find DistilBART, a pretrained and fine-tuned summarization model designed for accurate, fast inference. They were able to run and deploy the model directly from the hub with no required research or ML expertise. 5 Figure 4: Experiments with Transformers inference in collaborati"
2020.emnlp-demos.6,W18-1819,0,0.0296895,"mers library and model hub as a low-level framework. Since Transformers provides a hub for NLP models, it is also related to popular model hubs including Torch Hub and TensorFlow Hub which collect framework-specific model parameters for easy use. Unlike these hubs, Transformers is domain-specific which allows the system to provide automatic support for model analysis, usage, deployment, benchmarking, and easy replicability. The NLP and ML communities have a strong culture of building open-source research tools. The structure of Transformers is inspired by the pioneering tensor2tensor library (Vaswani et al., 2018) and the original source code for BERT (Devlin et al., 2018), both from Google Research. The concept of providing easy caching for pretrained models stemmed from AllenNLP (Gardner et al., 2018). The library is also closely related to neural translation and language modeling systems, such as Fairseq (Ott et al., 2019), OpenNMT (Klein et al., 2017), Texar (Hu et al., 2018), Megatron-LM (Shoeybi et al., 2019), and Marian NMT (Junczys-Dowmunt et al., 2018). Building on these elements, Transformers adds extra user-facing features to allow for easy downloading, caching, and fine-tuning of the models"
2020.emnlp-demos.6,N19-4009,0,0.0207668,"to provide automatic support for model analysis, usage, deployment, benchmarking, and easy replicability. The NLP and ML communities have a strong culture of building open-source research tools. The structure of Transformers is inspired by the pioneering tensor2tensor library (Vaswani et al., 2018) and the original source code for BERT (Devlin et al., 2018), both from Google Research. The concept of providing easy caching for pretrained models stemmed from AllenNLP (Gardner et al., 2018). The library is also closely related to neural translation and language modeling systems, such as Fairseq (Ott et al., 2019), OpenNMT (Klein et al., 2017), Texar (Hu et al., 2018), Megatron-LM (Shoeybi et al., 2019), and Marian NMT (Junczys-Dowmunt et al., 2018). Building on these elements, Transformers adds extra user-facing features to allow for easy downloading, caching, and fine-tuning of the models as well as seamless transition to production. Transformers maintains some compatibility with these libraries, most directly including a tool for performing inference using models from Marian NMT and Google’s BERT. There is a long history of easy-to-use, userfacing libraries for general-purpose NLP. Two core librarie"
2020.emnlp-demos.6,N18-1202,0,0.0526299,"The library is available at https://github.com/ huggingface/transformers. 1 Introduction The Transformer (Vaswani et al., 2017) has rapidly become the dominant architecture for natural language processing, surpassing alternative neural models such as convolutional and recurrent neural networks in performance for tasks in both natural language understanding and natural language generation. The architecture scales with training data and model size, facilitates efficient parallel training, and captures long-range sequence features. Model pretraining (McCann et al., 2017; Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2018) allows models to be trained on generic corpora and subsequently be easily adapted to specific tasks with strong performance. The Transformer architecture is particularly conducive to pretraining on large text corpora, leading to major gains in accuracy on downstream tasks including text classification (Yang et al., 2019), language understanding 1 https://github.com/huggingface/ transformers 2 https://huggingface.co/transformers/ 38 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 38–45 c November 16-20, 2020. 2020 Association for Computational Linguistics Fi"
2020.emnlp-demos.6,W18-5446,0,0.119282,"Missing"
2020.emnlp-demos.6,2020.acl-demos.15,0,0.0219522,"roperties, architecture, and use cases. Additional model-specific metadata can be provided via a model card (Mitchell et al., 2018) that describes properties of its training, a citation to the work, datasets used during pretraining, and any caveats about known biases in the model and its predictions. An example model card is shown in Figure 3 (Left). Since the Model Hub is specific to transformerbased models, we can target use cases that would 42 interested in developing a test bed for the performance of Transformers on a variety of different semantic recognition tasks. Their framework Jiant (Pruksachatkun et al., 2020) allows them to experiment with different ways of pretraining models and comparing their outputs. They used the Transformers API as a generic front-end and performed fine-tuning on a variety of different models, leading to research on the structure of BERT (Tenney et al., 2019). Case 3: Application Users Plot.ly, a company focused on user dashboards and analytics, was interested in deploying a model for automatic document summarization. They wanted an approach that scaled well and was simple to deploy, but had no need to train or fine-tune the model. They were able to search the Model Hub and"
2020.emnlp-demos.6,2020.acl-demos.14,0,0.0233382,"tly including a tool for performing inference using models from Marian NMT and Google’s BERT. There is a long history of easy-to-use, userfacing libraries for general-purpose NLP. Two core libraries are NLTK (Loper and Bird, 2002) and Stanford CoreNLP (Manning et al., 2014), which collect a variety of different approaches to NLP in a single package. More recently, general-purpose, open-source libraries have focused primarily on machine learning for a variety of NLP tasks, these include Spacy (Honnibal and Montani, 2017), AllenNLP (Gardner et al., 2018), flair (Akbik et al., 2019), and Stanza (Qi et al., 2020). Transformers provides similar functionality as these libraries. Additionally, each of these libraries now uses the 3 Library Design Transformers is designed to mirror the standard NLP machine learning model pipeline: process data, apply a model, and make predictions. Although the library includes tools facilitating training and development, in this technical report we focus on the core modeling specifications. For complete details about the features of the library refer to the documentation available on https: //huggingface.co/transformers/. Every model in the library is fully defined by thr"
2020.emnlp-main.103,P19-1228,1,0.861964,"Missing"
2020.emnlp-main.103,J92-4003,0,0.365156,"|zt ) ∝ bzt 1(z ∈ Zxt )Ozt xt (5) An example of the HMM lattice after state dropout is show in Figure 2. In addition to accuracy improvements, state dropout gives a large practical speed up for both parameter computation and inference. For λ = 0.5 we get a 4× speed improvement for both, due to the reduction in possible transitions. This structured dropout is also easy to exploit on GPU, as it maintains block structure. 5 Experimental Setup Emission Blocks The model requires partitioning token types into blocks Xm . While there are many partitioning methods, a natural choice is Brown clusters (Brown et al., 1992; Liang, 2005) which are also based on HMMs. Brown clusters are obtained by assigning every token type in X a state in an HMM, then merging states until a desired number of partitions M is reached. We construct the Brown clusters on the training portions of the datasets and assume the vocabulary remains identical at test time (with OOV words mapped to unk). We include more background on Brown Clusters in the appendix. State Dropout We use a dropout rate of λ = 0.5 at training time. For each block of size |Xm |, we sample λ|Xm |states to use in that block each batch. We draw states from each bl"
2020.emnlp-main.103,D17-1176,0,0.095108,"ices as the distributional parameters of the HMM. Specifically, let A ∈ [0, 1]|Z|×|Z |be the transition probabilities and O ∈ [0, 1]|Z|×|X |the emission probabilities, p(zt |zt−1 ) = Azt−1 zt p(xt |zt ) = Ozt xt . (2) We distinguish between two types of model parameterizations: scalar and neural, where the model parameters are given by θ. A scalar parameterization sets the model parameters equal to the distributional parameters, so that θ = {A, O}, resulting in O(|Z|2 + |Z||X |) model parameters. A 1 Other work has used neural parameterization for structured models, such as dependency models (Han et al., 2017), hidden semi-Markov models (Wiseman et al., 2018), and context free grammars (Kim et al., 2019). 4 Scaling HMMs We propose three extensions to scale HMMs for better language modeling performance: blocked emissions, which allow for very large models; neural parameterization, which makes it easy for states to share model parameters; and state dropout, which encourages broader state usage. Blocked Emissions Our main goal is to apply a HMM with a large number of hidden states to learn the underlying dynamics of language data. However, the O(T |Z|2 ) complexity of marginal inference practically li"
2020.emnlp-main.103,P13-2121,0,0.0994102,"Missing"
2020.emnlp-main.103,2020.acl-main.437,0,0.0292899,"m the observed. This often leads to improved accuracy, but precludes posterior inference which is useful for interpretability. A further benefit of HMMs over RNNs is that their associative structure allows for parallel inference via the prefix-sum algorithm (Ladner and Fischer, 1980).2 Finally, HMMs bottleneck information from every timestep through a discrete hidden state. NLP has a long history of utilizing discrete representations, and discrete representations may yield interesting results. For example, recent work has found that discrete latent variables work well in low-resource regimes (Jin et al., 2020). t=1 We refer to the transition and emission matrices as the distributional parameters of the HMM. Specifically, let A ∈ [0, 1]|Z|×|Z |be the transition probabilities and O ∈ [0, 1]|Z|×|X |the emission probabilities, p(zt |zt−1 ) = Azt−1 zt p(xt |zt ) = Ozt xt . (2) We distinguish between two types of model parameterizations: scalar and neural, where the model parameters are given by θ. A scalar parameterization sets the model parameters equal to the distributional parameters, so that θ = {A, O}, resulting in O(|Z|2 + |Z||X |) model parameters. A 1 Other work has used neural parameterization"
2020.emnlp-main.103,D18-1356,1,0.89613,"Missing"
2020.emnlp-main.344,P18-1079,0,0.0420531,"modify or perturb inputs while changing the model’s output. Hosseini et al. (2017) showed that perturbations, such as inserting dots or spaces between characters, can deceive a toxic comment classifier. HotFlip used gradients to find such perturbations given white-box access to the target model (Ebrahimi et al., 2018). Wallace et al. (2019) extended HotFlip by inserting a short crafted “trigger” text to any input as perturbation; the trigger words are often highly associated with the target class label. Other approaches are based on rules, heuristics or generative models (Mahler et al., 2017; Ribeiro et al., 2018; Iyyer et al., 2018; Zhao et al., 2018). As explained in Section 1, our goal is the inverse of adversarial examples: we aim to generate inputs with drastically different semantics that are perceived as similar by the model. Several works studied attacks that change the semantics of inputs. Jia and Liang (2017) showed that inserting a heuristically crafted sentence into a paragraph can trick a question answering (QA) system into picking the answer from the inserted sentence. Aggressively perturbed texts based on HotFlip are nonsensical and can be translated into meaningful and malicious output"
2020.emnlp-main.344,D19-1221,0,0.374337,"ring and discuss other potential mitigations. Our code is available at https://github.com/ csong27/collision-bert. 1 Vitaly Shmatikov Cornell Tech shmat@cs.cornell.edu Introduction Deep neural networks are vulnerable to adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015), i.e., imperceptibly perturbed inputs that cause models to make wrong predictions. Adversarial examples based on inserting or modifying characters and words have been demonstrated for text classification (Liang et al., 2018; Ebrahimi et al., 2018; Pal and Tople, 2020), question answering (Jia and Liang, 2017; Wallace et al., 2019), and machine translation (Belinkov and Bisk, 2018; Wallace et al., 2020). These attacks aim to minimally perturb the input so as it to preserve its semantics while changing the output of the model. In this work, we introduce and study a different class of vulnerabilities in NLP models for analyzing the meaning and similarity of texts. Given an input (query), we demonstrate how to generate a semantic collision: an unrelated text that is judged semantically equivalent by the target model. Semantic collisions are the “inverse” of adversarial examples. Whereas adversarial examples are similar inp"
2020.emnlp-main.344,2020.emnlp-main.446,0,0.263171,"tps://github.com/ csong27/collision-bert. 1 Vitaly Shmatikov Cornell Tech shmat@cs.cornell.edu Introduction Deep neural networks are vulnerable to adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015), i.e., imperceptibly perturbed inputs that cause models to make wrong predictions. Adversarial examples based on inserting or modifying characters and words have been demonstrated for text classification (Liang et al., 2018; Ebrahimi et al., 2018; Pal and Tople, 2020), question answering (Jia and Liang, 2017; Wallace et al., 2019), and machine translation (Belinkov and Bisk, 2018; Wallace et al., 2020). These attacks aim to minimally perturb the input so as it to preserve its semantics while changing the output of the model. In this work, we introduce and study a different class of vulnerabilities in NLP models for analyzing the meaning and similarity of texts. Given an input (query), we demonstrate how to generate a semantic collision: an unrelated text that is judged semantically equivalent by the target model. Semantic collisions are the “inverse” of adversarial examples. Whereas adversarial examples are similar inputs that produce dissimilar model outputs, We develop gradient-based appr"
2020.emnlp-main.344,D19-3004,0,0.0375282,"Missing"
2020.emnlp-main.344,D19-1352,0,0.037889,"Missing"
2020.emnlp-main.344,P18-1205,0,0.0636884,"Missing"
2020.emnlp-main.447,2020.acl-main.676,0,0.36604,"lds approximately 1.0 BLEU improvement on five different translation datasets over strong Transformer baselines. On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements. 1 Introduction Natural language is thought to be characterized by systematic compositionality (Fodor and Pylyshyn, 1988). A computational model that is able to exploit such systematic compositionality should understand sentences by appropriately recombining subparts that have not been seen together during training. Consider the following example from Andreas (2020): (1a) She picks the wug up in Fresno. (1b) He puts the cup down in Tempe. Given the above sentences, a model which has learned compositional structure should be able to generalize and understand sentences such as: (2a) She puts the wug down in Fresno. (2b) She picks the wug up in Tempe. (2c) He picks the wug up in Fresno. (2d) She picks the wug up in Tempe. (2e) He picks the cup up in Fresno. (2f) He puts the cup up in Fresno. Instead of enumerating over all possible combinations of two sentences, SeqMix crafts a new example by softly mixing the two sentences via a convex combination of the o"
2020.emnlp-main.447,P16-1139,0,0.0161143,"significant improvements across a wide range of NLP tasks, training them to generalize by learning the compositional structure of language remains a challenging open problem. Notably, Lake and Baroni (2018) propose an influential dataset (SCAN) to evaluate the systematic compositionality of neural models and find that they often fail to generalize compositionally. One approach to encouraging compositional behavior in neural models is by incorporating compositional structures such as parse trees or programs directly into a network’s computational graph (Socher et al., 2013; Dyer et al., 2016; Bowman et al., 2016; Andreas et al., 2016; Johnson et al., 2017). While effective on certain domains such as visual question answering, these approaches usually rely on intermediate structures predicted from pipelined models, which limits their applicability in general. Further, it is an open question as to whether such putatively compositional models result in significant empirical improvements on many NLP tasks (Shi et al., 2018). Expressive parameterizations over high dimensional input afforded by neural networks contribute to their excellent performance in high resource settings; however, such flexible param"
2020.emnlp-main.447,2020.acl-main.194,0,0.0471329,"rce settings; however, such flexible parameterizations can easily lead to a model’s memorizing—i.e., overfitting to—long segments of text, instead of relying on the appropriate subparts of segments. Another approach to encouraging compositionality in richly-parameterized neural models, then, is to augment the training data with more examples. Existing work in this vein include SwitchOut (Wang et al., 2018), which replaces a word in a sentence with a random word from the vocabulary, GECA (Andreas, 2020), which creates new examples by switching subparts that occur in similar contexts, and TMix (Chen et al., 2020), which interpolates between hidden states of neural models for text classification. We compare to these approaches to our proposed approach in this paper. 3 for image classification tasks (DeVries and Taylor, 2017; Yun et al., 2019). We first describe the generative data augmentation process behind this model for text generation, and show how SeqMix approximates the resulting latent variable objective with a relaxed version. Let X ∈ Rs×V represent a source sequence of length s with vocabulary size V and Y ∈ Rt×V represent a target sequence to generate of length t. Assume that we sample a pair"
2020.emnlp-main.447,N16-1024,0,0.0233484,"atasets have led to significant improvements across a wide range of NLP tasks, training them to generalize by learning the compositional structure of language remains a challenging open problem. Notably, Lake and Baroni (2018) propose an influential dataset (SCAN) to evaluate the systematic compositionality of neural models and find that they often fail to generalize compositionally. One approach to encouraging compositional behavior in neural models is by incorporating compositional structures such as parse trees or programs directly into a network’s computational graph (Socher et al., 2013; Dyer et al., 2016; Bowman et al., 2016; Andreas et al., 2016; Johnson et al., 2017). While effective on certain domains such as visual question answering, these approaches usually rely on intermediate structures predicted from pipelined models, which limits their applicability in general. Further, it is an open question as to whether such putatively compositional models result in significant empirical improvements on many NLP tasks (Shi et al., 2018). Expressive parameterizations over high dimensional input afforded by neural networks contribute to their excellent performance in high resource settings; however"
2020.emnlp-main.447,P18-1033,0,0.0453849,"Missing"
2020.emnlp-main.447,P19-1555,0,0.0625544,"tences, SeqMix crafts a new example by softly mixing the two sentences via a convex combination of the original examples. This approach can be seen as a sequence-level variant of a broader family of techniques called mixed sample data augmentation (MSDA), which was originally proposed by Zhang et al. (2018) and has been shown to be particularly effective for classification tasks (DeVries and Taylor, 2017; Yun et al., 2019; Verma et al., 2019). We also show that SeqMix shares similarities with word replacement/dropout strategies in machine translation (Sennrich et al., 2016; Wang et al., 2018; Gao et al., 2019), SeqMix targets a crude but simple approach to data augmentation for language applications. We apply SeqMix to a variety of sequence-to-sequence tasks including neural machine translation, semantic parsing, and SCAN (a dataset designed to test for compositionality of data-driven models), and find that SeqMix improves results on top of (and 5547 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5547–5552, c November 16–20, 2020. 2020 Association for Computational Linguistics when combined with) existing data augmentation methods. 2 Motivation and Rel"
2020.emnlp-main.447,N19-4009,0,0.0204006,"y of sequence-to-sequence tasks: machine translation, SCAN, and semantic parsing. For all datasets, we tune the α hyperparameter in the range of [0.1, 1.5] on the validation set.3 Exact details regarding the training setup (including descriptions of the various datasets) can be found in the supplementary materials. Machine Translation Our machine translation experiments consider five translation datasets: (1) IWSLT ’14 German-English (de-en) (2) IWSLT ’14 English-{German, Italian, Spanish} (en-{de, it, es}) (3) WMT ’14 EnglishGerman (en-de). We use the Transformer implementation from fairseq (Ott et al., 2019) with the default configuration. SCAN SCAN is a command execution dataset designed to test for systematic compositionality of data-driven models. SCAN consists of simple English commands and corresponding action sequences. We consider three different splits that have been widely utilized in the existing literature: jump, around-right, turn-left. For the splits (jump, turn-left), the primitive commands (i.e. “jump”, “turn left”) are only seen in isolation during training, and the test set consists commands that compose the isolated primitive command with the other commands seen during training."
2020.emnlp-main.447,W16-2323,0,0.201242,"t. To motivate our approach, consider some example sentences that can be created by combining (1a) and (1b) : Despite their empirical success, neural networks still have difficulty capturing compositional aspects of natural language. This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems. Our approach, SeqMix, creates new synthetic examples by softly combining input/output sequences from the training set. We connect this approach to existing techniques such as SwitchOut (Wang et al., 2018) and word dropout (Sennrich et al., 2016), and show that these techniques are all approximating variants of a single objective. SeqMix consistently yields approximately 1.0 BLEU improvement on five different translation datasets over strong Transformer baselines. On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements. 1 Introduction Natural language is thought to be characterized by systematic compositionality (Fodor and Pylyshyn, 1988). A computational model that is able to exploit such systematic compositionality should understand sentences by appropriate"
2020.emnlp-main.447,D18-1065,0,0.0239168,"Beta(α, α) and train on these expected samples.1 Relationship to Existing Methods Table 1 shows that we can recover existing data augmentation methods such as SwitchOut and word dropout under the above framework. In particular, these methods approximate a version of the “hard” latent variable objective in Eq. 2 by considering different swap distributions p(m) and sampling distributions D0 .2 Compared to other approaches, SeqMix is essentially a relaxed variant of the same objective, similar to the difference between soft vs. hard attention (Xu et al., 2015; Deng et al., 2018; Wu et al., 2018; Shankar et al., 2018). SeqMix is also more efficient than more sophisticated augmentation strategies such as GECA which requires a computationally expensive validation check for swaps. 1 Our implementation can be found at https:// github.com/dguo98/seqmix, and pseudocode can be found in supplementary materials. 2 Wang et al. (2018) also offer an alternative formulation which unifies various data augmentation strategies as training on a distribution that better approximates the underlying data distribution. While the hard version of SeqMix can also be unified under SwitchOut’s resulting objective, we chose our alte"
2020.emnlp-main.447,D18-1492,0,0.0161222,"in neural models is by incorporating compositional structures such as parse trees or programs directly into a network’s computational graph (Socher et al., 2013; Dyer et al., 2016; Bowman et al., 2016; Andreas et al., 2016; Johnson et al., 2017). While effective on certain domains such as visual question answering, these approaches usually rely on intermediate structures predicted from pipelined models, which limits their applicability in general. Further, it is an open question as to whether such putatively compositional models result in significant empirical improvements on many NLP tasks (Shi et al., 2018). Expressive parameterizations over high dimensional input afforded by neural networks contribute to their excellent performance in high resource settings; however, such flexible parameterizations can easily lead to a model’s memorizing—i.e., overfitting to—long segments of text, instead of relying on the appropriate subparts of segments. Another approach to encouraging compositionality in richly-parameterized neural models, then, is to augment the training data with more examples. Existing work in this vein include SwitchOut (Wang et al., 2018), which replaces a word in a sentence with a rand"
2020.emnlp-main.447,D13-1170,0,0.00823954,"ks trained on large datasets have led to significant improvements across a wide range of NLP tasks, training them to generalize by learning the compositional structure of language remains a challenging open problem. Notably, Lake and Baroni (2018) propose an influential dataset (SCAN) to evaluate the systematic compositionality of neural models and find that they often fail to generalize compositionally. One approach to encouraging compositional behavior in neural models is by incorporating compositional structures such as parse trees or programs directly into a network’s computational graph (Socher et al., 2013; Dyer et al., 2016; Bowman et al., 2016; Andreas et al., 2016; Johnson et al., 2017). While effective on certain domains such as visual question answering, these approaches usually rely on intermediate structures predicted from pipelined models, which limits their applicability in general. Further, it is an open question as to whether such putatively compositional models result in significant empirical improvements on many NLP tasks (Shi et al., 2018). Expressive parameterizations over high dimensional input afforded by neural networks contribute to their excellent performance in high resourc"
2020.emnlp-main.447,D18-1100,0,0.457277,"ions of subparts to predict the output. To motivate our approach, consider some example sentences that can be created by combining (1a) and (1b) : Despite their empirical success, neural networks still have difficulty capturing compositional aspects of natural language. This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems. Our approach, SeqMix, creates new synthetic examples by softly combining input/output sequences from the training set. We connect this approach to existing techniques such as SwitchOut (Wang et al., 2018) and word dropout (Sennrich et al., 2016), and show that these techniques are all approximating variants of a single objective. SeqMix consistently yields approximately 1.0 BLEU improvement on five different translation datasets over strong Transformer baselines. On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements. 1 Introduction Natural language is thought to be characterized by systematic compositionality (Fodor and Pylyshyn, 1988). A computational model that is able to exploit such systematic compositionality s"
2020.emnlp-main.447,D18-1473,0,0.0283801,"re we sample λ ∼ Beta(α, α) and train on these expected samples.1 Relationship to Existing Methods Table 1 shows that we can recover existing data augmentation methods such as SwitchOut and word dropout under the above framework. In particular, these methods approximate a version of the “hard” latent variable objective in Eq. 2 by considering different swap distributions p(m) and sampling distributions D0 .2 Compared to other approaches, SeqMix is essentially a relaxed variant of the same objective, similar to the difference between soft vs. hard attention (Xu et al., 2015; Deng et al., 2018; Wu et al., 2018; Shankar et al., 2018). SeqMix is also more efficient than more sophisticated augmentation strategies such as GECA which requires a computationally expensive validation check for swaps. 1 Our implementation can be found at https:// github.com/dguo98/seqmix, and pseudocode can be found in supplementary materials. 2 Wang et al. (2018) also offer an alternative formulation which unifies various data augmentation strategies as training on a distribution that better approximates the underlying data distribution. While the hard version of SeqMix can also be unified under SwitchOut’s resulting objec"
2020.findings-emnlp.302,2020.acl-main.357,0,0.0214891,"temporal unit (e.g., second, minute, hour, etc.). To transform the sentences into the input format of our models. We insert duration pattern (“, lasting [MASK] [MASK], ”) after event word and use the new sentence as the input sequence. For example, one sentence in TimeBank is “Philip Morris Cos, adopted a defense measure ...”. Our method will convert it to “Philip Morris Cos, adopted, lasting [MASK] [MASK], a defense measure ...”. Our strategy of directly adding duration pattern is possible to help pre-trained model to utilize learned intrinsic textual representation for duration prediction (Tamborrino et al., 2020). McTACO is a multi-choice question answering dataset. McTACO-duration3 is a subset of Mc2 We use Gusev et al. (2011)’s split and obtain 1663/469/147 events in Train/Test/TestWSJ set respectively. 3 In practice we collect context-question-answer triples that questions are about event duration and answers can be transformed to a duration value. We get 1060/2827 triples for dev/test set respectively (out of 1112/3032). 3371 Coarsed-Grained (Test) &lt;day F1 &gt;day F1 Acc. Model Coarsed-Grained (TestWSJ) &lt;day F1 &gt;day F1 Acc. Fine-Grained Acc. (Test) Acc. (TestWSJ) Supervised Setting Majority class Max"
2020.findings-emnlp.302,N18-2026,0,0.296014,", 2015). It is challenging to make accurate prediction mainly due to two reasons: (1) duration is not only associated with event word but also the context. For example, “watch a movie” takes around 2 hours, while “watch a bird fly” only takes about 10 seconds; (2) the compositional nature of events makes it difficult to train a learning-based system only based on hand annotated data (since it’s hard to cover all the possible events). Thus, external knowledge and commonsense are needed to make further progress on the task. However, most current approaches (Pan et al., 2011; Gusev et al., 2011; Vempala et al., 2018) focus on developing features and cannot utilize external textual knowledge. The only exception is the web count based method proposed by Gusev et al. (2011), which queries search engine with event word (e.g., “watch”) and temporal units, and make predictions based on hitting times. However, this method achieves better performance when query only with the event word in the sentence, which means it does not enable contextualized understanding. To benefit from the generalizability of learningbased methods and utilizing external temporal knowledge, we introduce a framework, which includes (1) a p"
2020.findings-emnlp.302,P12-2044,0,0.0300227,", 2016; Vempala et al., 2018). In particular, aspectual (Vendler, 1957; Smith, 2013) features have been proved to be useful. Concurrent to our work, Zhou et al. (2020) also utilize unlabeled data. Different from our work, they focus on temporal commonsense acquisition in a more general setting (for frequency, typical time, duration, etc.) and the models predict the discrete temporal unit, while we propose two models (classification and regression-based). In addition, they focus on providing better representation instead of directly generating duration prediction. For the unsupervised setting, Williams and Katz (2012); Elazar et al. (2019) use rule-based method on web data and generate collections of mapping from verb/event pattern to numeric duration value. Kozareva and Hovy (2011); Gusev et al. (2011) develop queries for search engines and utilize the returned snippets / hitting times to make prediction. 5 Conclusion We propose a framework for leveraging free-form textual knowledge into neural models for duration prediction. Our best model (E- PRED) achieves state-of-the-art performance in various tasks. In addition, our model trained only with externallyobtained weakly supervised news data outperforms s"
2020.findings-emnlp.302,D19-1332,0,0.290336,"by reading temporal-related news sentences (time-aware pre-training). Specifically, one model predicts the range/unit where the duration value falls in (R- PRED); and the other predicts the exact duration value (EPRED ). Our best model – E- PRED , substantially outperforms previous work, and captures duration information more accurately than RPRED . We also demonstrate our models are capable of duration prediction in the unsupervised setting, outperforming the baselines. 1 Introduction Understanding duration of event expressed in text is a crucial task in NLP (Pustejovsky and Verhagen, 2009; Zhou et al., 2019). It facilitates downstream tasks such as story timeline construction (Ning et al., 2018; Leeuwenberg and Moens, 2019) and temporal question answering (Llorens et al., 2015). It is challenging to make accurate prediction mainly due to two reasons: (1) duration is not only associated with event word but also the context. For example, “watch a movie” takes around 2 hours, while “watch a bird fly” only takes about 10 seconds; (2) the compositional nature of events makes it difficult to train a learning-based system only based on hand annotated data (since it’s hard to cover all the possible event"
2020.findings-emnlp.302,2020.acl-main.678,0,0.301328,"s split and obtain 1663/469/147 events in Train/Test/TestWSJ set respectively. 3 In practice we collect context-question-answer triples that questions are about event duration and answers can be transformed to a duration value. We get 1060/2827 triples for dev/test set respectively (out of 1112/3032). 3371 Coarsed-Grained (Test) &lt;day F1 &gt;day F1 Acc. Model Coarsed-Grained (TestWSJ) &lt;day F1 &gt;day F1 Acc. Fine-Grained Acc. (Test) Acc. (TestWSJ) Supervised Setting Majority class Maximum Entropy (Pan et al., 2011)† Maximum Entropy++ (Gusev et al., 2011)† LSTM ensemble (Vempala et al., 2018) TACOLM (Zhou et al., 2020) 64.29 80.58 76.90 82.69 88.88 62.47 73.30 73.00 76.69 85.86 73.20 76.01 76.99 87.78 88.14 62.58 73.50 74.80 83.21 84.12 59.28 62.20 62.40 - 52.38 61.90 66.00 - R-PRED w/o pre-training E-PRED w/o pre-training 82.08 80.94 80.63 78.73 87.72 86.19 89.46 88.16 85.43 84.01 86.35 84.79 70.15 73.46 70.67 73.50 81.12 79.93 85.39 86.21 76.87 77.32 80.50 81.86 82.09 80.38 82.52 80.34 76.19 78.46 78.46 77.02 Unsupervised Setting Majority Web count, yesterday (Gusev et al., 2011)† Web count, bucket (Gusev et al., 2011)† R-PRED E-PRED - 76.90 - 62.47 70.70 72.40 - 76.99 - 62.58 74.80 73.50 59.28 66.50 52.3"
2021.acl-long.378,D18-2029,0,0.0589922,"Missing"
2021.acl-long.378,P19-1595,0,0.0368419,"Missing"
2021.acl-long.378,D17-1070,0,0.0298115,", 2018), which learn to read and write to layers of a shared model, have been applied to obtain parameter-efficient BERT models (Houlsby et al., 2019; Pfeiffer et al., 2020a,b,c). In recent work, Li & Liang (2021) and Qin & Eisner (2021) explore the use of learned prompts on top of pretrained models to obtain task-specific models. Yet another line of work targets extreme parameterefficiency through task-agnostic sentence representations that can be used without finetuning for downstream tasks (Le & Mikolov, 2014; Kiros et al., 2015; Wieting et al., 2016; Hill et al., 2016; Arora et al., 2017; Conneau et al., 2017; Cer et al., 2018; Zhang et al., 2018; Subramanian et al., 2018; Reimers & Gurevych, 2019; Zhang et al., 2020b). These feature-based transfer learning methods are however generally outperformed by fully finetuned models (Howard & Ruder, 2018). Model compression There has been much recent work on compressing pretrained trained with selfsupervision (see (Ganesh et al., 2020) for a recent survey). A particularly promising line of work focuses on obtaining smaller pretrained models (for subsequent finetuning) through weight pruning (Gordon et al., 2020; Sajjad et al., 2020; Chen et al., 2020) and"
2021.acl-long.378,N19-1423,0,0.0984249,"Missing"
2021.acl-long.378,2020.repl4nlp-1.18,0,0.0411669,"016; Hill et al., 2016; Arora et al., 2017; Conneau et al., 2017; Cer et al., 2018; Zhang et al., 2018; Subramanian et al., 2018; Reimers & Gurevych, 2019; Zhang et al., 2020b). These feature-based transfer learning methods are however generally outperformed by fully finetuned models (Howard & Ruder, 2018). Model compression There has been much recent work on compressing pretrained trained with selfsupervision (see (Ganesh et al., 2020) for a recent survey). A particularly promising line of work focuses on obtaining smaller pretrained models (for subsequent finetuning) through weight pruning (Gordon et al., 2020; Sajjad et al., 2020; Chen et al., 2020) and/or knowledge distillation (Sanh et al., 2019; Sun et al., 2019; Turc et al., 2019; Jiao et al., 2020; Sun et al., 2020b). It would be interesting to see whether our approach can be applied on top of these smaller pretrained models to for even greater parameter-efficiency. Learning to mask Our work is closely related to the line of work on learning to mask parts of deep networks with differentiable relaxations of binary masks for model pruning and parameter sharing (Wang et al., 2019b; Zhao et al., 2020; Sanh et al., 2020; Radiya-Dixit & Wang, 2020;"
2021.acl-long.378,N16-1162,0,0.0781133,"Missing"
2021.acl-long.378,P18-1031,0,0.0215327,"ore the use of learned prompts on top of pretrained models to obtain task-specific models. Yet another line of work targets extreme parameterefficiency through task-agnostic sentence representations that can be used without finetuning for downstream tasks (Le & Mikolov, 2014; Kiros et al., 2015; Wieting et al., 2016; Hill et al., 2016; Arora et al., 2017; Conneau et al., 2017; Cer et al., 2018; Zhang et al., 2018; Subramanian et al., 2018; Reimers & Gurevych, 2019; Zhang et al., 2020b). These feature-based transfer learning methods are however generally outperformed by fully finetuned models (Howard & Ruder, 2018). Model compression There has been much recent work on compressing pretrained trained with selfsupervision (see (Ganesh et al., 2020) for a recent survey). A particularly promising line of work focuses on obtaining smaller pretrained models (for subsequent finetuning) through weight pruning (Gordon et al., 2020; Sajjad et al., 2020; Chen et al., 2020) and/or knowledge distillation (Sanh et al., 2019; Sun et al., 2019; Turc et al., 2019; Jiao et al., 2020; Sun et al., 2020b). It would be interesting to see whether our approach can be applied on top of these smaller pretrained models to for even"
2021.acl-long.378,2020.findings-emnlp.372,0,0.217092,"oject onto the L0 ball was crucial in achieving exact sparsity targets. As shown in Table 4, we observed little loss in performance through this approach. We reiterate that it was crucial to finetune with a fixed mask, even for the approach which does not apply magnitude pruning.16 6.5 Comparison against BERT compression Direct BERT compression methods also provide a straightforward approach to parameter-efficient transfer learning. Here we compare diff pruning against existing BERT compression methods, in particular DistilBERT (Sanh et al., 2019), MobileBERT (Sun et al., 2020b) and TinyBERT (Jiao et al., 2020). In these experiments we apply diff pruning on the smaller BERTBASE model as these works typically utilize BERTBASE as the baseline. As shown in Table 5, we observe that diff pruning is more parameter-efficient when considering all GLUE tasks while maintaining better performance. Of course, BERT compression methods typically have faster inference time (e.g. TinyBERT4 is 9.4× faster that BERTBASE ). However we note that diff 16 Without fixed-mask finetuning, GLUE performance decreases from 84.9 to 81.4. pruning can be applied on these methods, which may further improve parameter-efficiency whi"
2021.acl-long.378,N19-1112,0,0.0547354,"Missing"
2021.acl-long.378,P19-1441,0,0.039226,"Missing"
2021.acl-long.378,N18-1202,0,0.01859,"ng as learning a diff vector δτ that is added to the pretrained model parameters θ, which remain fixed. We first reparameterize the task-specific model parameters, Background: Transfer Learning Transfer learning in NLP mostly uses a pretrainand-finetune paradigm, which initializes a subset of the model parameters for all tasks from a pretrained model and then finetunes on a task-specific objective. Pretraining objectives include context prediction (Mikolov et al., 2013), autoencoding (Dai & Le, 2015), machine translation (McCann et al., 2017), and more recently, variants of language modeling (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019) objectives. Here we consider applying transfer learning to multiple tasks. We consider a setting with a potentially unknown set of tasks (which may arrive in stream), where each task τ ∈ T has an asso(n) (n) ciated training set Dτ = {xτ , yτ }N n=1 . For all tasks, the goal is to produce (possibly tied) model parameters θτ to minimize the empirical risk, N  1 X  (n) min C fτ (x(n) + λR(θτ ) τ ; θτ ), yτ θτ N n=1 where fτ (·; θτ ) is a parameterized function over the input (e.g. a neural network), C(·, ·) is a loss function (e.g. cross-entropy),1 a"
2021.acl-long.378,2021.eacl-main.39,0,0.177583,"Missing"
2021.acl-long.378,2020.emnlp-main.617,0,0.0513401,"Missing"
2021.acl-long.378,2021.naacl-main.410,0,0.0264898,"multiple tasks when jointly trained (Liu et al., 2019b; Clark et al., 2019; Stickland & Murray, 2019). An alternative approach to multi-task learning that does not require access to all tasks during training involve training smaller task-specific layers that interact with a fixed pretrained model (Rebuffi et al., 2018; Zhang et al., 2020a). In particular, Adapters (Rebuffi et al., 2018), which learn to read and write to layers of a shared model, have been applied to obtain parameter-efficient BERT models (Houlsby et al., 2019; Pfeiffer et al., 2020a,b,c). In recent work, Li & Liang (2021) and Qin & Eisner (2021) explore the use of learned prompts on top of pretrained models to obtain task-specific models. Yet another line of work targets extreme parameterefficiency through task-agnostic sentence representations that can be used without finetuning for downstream tasks (Le & Mikolov, 2014; Kiros et al., 2015; Wieting et al., 2016; Hill et al., 2016; Arora et al., 2017; Conneau et al., 2017; Cer et al., 2018; Zhang et al., 2018; Subramanian et al., 2018; Reimers & Gurevych, 2019; Zhang et al., 2020b). These feature-based transfer learning methods are however generally outperformed by fully finetuned mod"
2021.acl-long.378,D16-1264,0,0.0315168,"Missing"
2021.acl-long.378,D19-1441,0,0.0301038,"nian et al., 2018; Reimers & Gurevych, 2019; Zhang et al., 2020b). These feature-based transfer learning methods are however generally outperformed by fully finetuned models (Howard & Ruder, 2018). Model compression There has been much recent work on compressing pretrained trained with selfsupervision (see (Ganesh et al., 2020) for a recent survey). A particularly promising line of work focuses on obtaining smaller pretrained models (for subsequent finetuning) through weight pruning (Gordon et al., 2020; Sajjad et al., 2020; Chen et al., 2020) and/or knowledge distillation (Sanh et al., 2019; Sun et al., 2019; Turc et al., 2019; Jiao et al., 2020; Sun et al., 2020b). It would be interesting to see whether our approach can be applied on top of these smaller pretrained models to for even greater parameter-efficiency. Learning to mask Our work is closely related to the line of work on learning to mask parts of deep networks with differentiable relaxations of binary masks for model pruning and parameter sharing (Wang et al., 2019b; Zhao et al., 2020; Sanh et al., 2020; Radiya-Dixit & Wang, 2020; Mallya et al., 2018; Guo et al., 2019; Sun et al., 2020a; Cao et al., 2021). While these works also enable"
2021.acl-long.378,D19-1410,0,0.0187079,"ence is linguistically acceptable or not; Semantic Textual Similarity Benchmark (STSB), which must predict a similarity rating between two sentences; Microsoft Research Paraphrase Corpus (MRPC), where the goal is to predict whether two sentences are semantically equivalent; Recognizing Textual Entailment (RTE), which must predict whether a second sentence is entailed by the first. The benchmark uses Matthew’s correlation for CoLA, Spearman for STS-B, F1 score for MRPC/QQP, and accuracy for MNLI/QNLI/SST2/RTE. For the main experiments and analysis, we use the BERTLARGE model from Devlin et al. (2019) to compare against the adapter-based approach of Houlsby et al. (2019). Our implementation is based on the Hugging Face Transformer library (Wolf et al., 2019).     −l −l 4.2 Baselines j = σ ατ,i − log · σ ατ − log . r r We compare both structured and non-structured j=1 i∈g(j) variants of diff pruning against the following baselines: Full finetuning, which fully finetunes We can train with gradient-based optimization as BERTLARGE as usual; Last layer finetuning, before. Parameters in a group are encouraged by which only finetunes the penultimate layer (along the regularizer to be removed"
2021.acl-long.378,2020.acl-main.195,0,0.127416,"Applying magnitude pruning to project onto the L0 ball was crucial in achieving exact sparsity targets. As shown in Table 4, we observed little loss in performance through this approach. We reiterate that it was crucial to finetune with a fixed mask, even for the approach which does not apply magnitude pruning.16 6.5 Comparison against BERT compression Direct BERT compression methods also provide a straightforward approach to parameter-efficient transfer learning. Here we compare diff pruning against existing BERT compression methods, in particular DistilBERT (Sanh et al., 2019), MobileBERT (Sun et al., 2020b) and TinyBERT (Jiao et al., 2020). In these experiments we apply diff pruning on the smaller BERTBASE model as these works typically utilize BERTBASE as the baseline. As shown in Table 5, we observe that diff pruning is more parameter-efficient when considering all GLUE tasks while maintaining better performance. Of course, BERT compression methods typically have faster inference time (e.g. TinyBERT4 is 9.4× faster that BERTBASE ). However we note that diff 16 Without fixed-mask finetuning, GLUE performance decreases from 84.9 to 81.4. pruning can be applied on these methods, which may furth"
2021.acl-long.378,P19-1452,0,0.0116929,"h force all parameters in a group to share the same mask (Louizos et al., 2018; Wang et al., 2019b). However we still expect entire groups to be pruned out more often, which might bias the learning process towards either eliminating completely or clustering together nonzero diffs. In Table 3, we indeed find that structured diff pruning leads to finetuned models that are much more likely to leave entire groups unchanged from their pretrained values (zero diffs). 6.3 Task-specific Sparsity Different layers of pretrained models have been argued to encode different information (Liu et al., 2019a; Tenney et al., 2019). Given that each task will likely recruit different kinds of language phenomena embedded in the hidden layers, we hypothesize that diff pruning will modify different parts of the pretrained model through task-specific finetuning. Figure 2 shows the percentage of nonzero diff parameters attributable to the different layers for each task. We find that different tasks indeed modify different parts of the network, although there are some qualitative similarities between some tasks, for example between QNLI & QQP (both must encode questions), and MRPC & STS-B (both must predict similarity between"
2021.acl-long.378,W18-5446,0,0.0495103,"Missing"
2021.acl-long.378,K17-1029,0,0.0414382,"Missing"
2021.acl-long.378,2020.acl-main.422,0,0.0210866,"Missing"
2021.acl-long.378,D18-1481,0,0.0184259,"o layers of a shared model, have been applied to obtain parameter-efficient BERT models (Houlsby et al., 2019; Pfeiffer et al., 2020a,b,c). In recent work, Li & Liang (2021) and Qin & Eisner (2021) explore the use of learned prompts on top of pretrained models to obtain task-specific models. Yet another line of work targets extreme parameterefficiency through task-agnostic sentence representations that can be used without finetuning for downstream tasks (Le & Mikolov, 2014; Kiros et al., 2015; Wieting et al., 2016; Hill et al., 2016; Arora et al., 2017; Conneau et al., 2017; Cer et al., 2018; Zhang et al., 2018; Subramanian et al., 2018; Reimers & Gurevych, 2019; Zhang et al., 2020b). These feature-based transfer learning methods are however generally outperformed by fully finetuned models (Howard & Ruder, 2018). Model compression There has been much recent work on compressing pretrained trained with selfsupervision (see (Ganesh et al., 2020) for a recent survey). A particularly promising line of work focuses on obtaining smaller pretrained models (for subsequent finetuning) through weight pruning (Gordon et al., 2020; Sajjad et al., 2020; Chen et al., 2020) and/or knowledge distillation (Sanh et al"
2021.acl-long.378,2020.emnlp-main.124,0,0.0684078,"Missing"
2021.eacl-main.52,D12-1091,0,0.115898,"e tagging, which results in a slightly better F1 score; (4) for the roles TARGET and W EAPON, our model is more conservative (lower recall) and achieves lower F1. One possibility is that for role like TAR - Models CohesionExtract (Huang and Riloff, 2012) NST (Du and Cardie, 2020) DY GIE++ (Wadden et al., 2019) GRIT P R F1 58.38 39.53 47.14 56.82 48.92 52.58 57.04 46.77 51.40 64.19∗∗ 47.36 54.50∗ Table 4: Micro-average results (the highest number of each column is boldfaced). Significance is indicated with ∗∗ (p &lt; 0.01),∗ (p &lt; 0.1) – all tests are computed using the paired bootstrap procedure (Berg-Kirkpatrick et al., 2012). GET , on average there are more entities (though with only one mention each), and it’s harder for our model to decode as many TARGET entities correct in a generative way. 8 Discussion How well do the models capture coreference relations between mentions? We also conduct targeted evaluations on subsets of test documents whose gold extractions come with coreferent mentions. From left to right in Table 3, we report results on the subsets of documents with increasing number (k) of possible (coreferent) mentions per role-filler entity. We find that: (1) On the subset of documents with only one me"
2021.eacl-main.52,D13-1185,0,0.0680623,"red in recent work, using hand-designed features for both local and additional context (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012), and with end-to-end sequence tagging based models with contextualized pre-trained representations (Du and Cardie, 2020). These efforts are the most related to our work. The key difference is that our work focuses on a more challenging, and more realistic, setting: extracting role-filler entities rather than lists of role-filler mentions that are not grouped according to their associated entity. Also on a related note, Chambers and Jurafsky (2011), Chambers (2013), and Liu et al. (2019) work on unsupervised event schema induction and open-domain event extraction from documents. The main idea is to group entities corresponding to the same role into an event template. Recently, there has also been increasing interest in cross-sentence/document-level relation extraction (RE). In the scientific domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representations between entity"
2021.eacl-main.52,P11-1098,0,0.131494,"tion extraction has been explored in recent work, using hand-designed features for both local and additional context (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012), and with end-to-end sequence tagging based models with contextualized pre-trained representations (Du and Cardie, 2020). These efforts are the most related to our work. The key difference is that our work focuses on a more challenging, and more realistic, setting: extracting role-filler entities rather than lists of role-filler mentions that are not grouped according to their associated entity. Also on a related note, Chambers and Jurafsky (2011), Chambers (2013), and Liu et al. (2019) work on unsupervised event schema induction and open-domain event extraction from documents. The main idea is to group entities corresponding to the same role into an event template. Recently, there has also been increasing interest in cross-sentence/document-level relation extraction (RE). In the scientific domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representation"
2021.eacl-main.52,P15-1017,0,0.349754,"ar in a sentence that does not explicitly mention the explosion of the bomb. In addition, REE is ultimately an entity-based task — exactly one descriptive mention for each role-filler should be extracted even when the entity is referenced multiple times in connection with the event. The final output for the bombing example should, therefore, include just one of the “water pipes” references, and one of the three alternative descriptions of the P ERP I ND and the second TARGET, the telephone company building. As a result of these complications, end-to-end sentence-level event extraction models (Chen et al., 2015; Lample et al., 2016), which dominate the literature, are ill-suited for the REE task, which calls for models that encode information and track entities across a longer context. Fortunately, neural models for event extraction that have the ability to model longer contexts have been developed. Du and Cardie (2020), for example, extend standard contextualized representations (Devlin et al., 2019) to produce a documentlevel sequence tagging model for event argument extraction. Both approaches show improvements in performance over sentence-level models on event extraction. Regrettably, these appr"
2021.eacl-main.52,N13-1104,0,0.0218939,"nt; (2) concatenates the representations of the span’s beginning & end token and use it as its representation, and pass it through a classifier layer to predict whether the span represents certain role-filler entity and what the role is. Both the NST and DYGIE++ are end-to-end and fine-tuned BERT (Devlin et al., 2019) contextualized representations with task-specific data. We train them to identify the first mention for each role-filler entity (to ensure fair comparison with our proposed model). Unsupervised event schema induction based approaches (Chambers and Jurafsky, 2011; Chambers, 2013; Cheung et al., 2013) are also able 639 5 Instead of using feature-engineering based sentence classification to identify event-relevant sentences, we re-implement the sentence classifier with BiLSTM-based neural sequence model. NST (Du and Cardie, 2020) DY GIE++ (Wadden et al., 2019) GRIT P ERP I ND P ERP O RG TARGET V ICTIM W EAPON 48.39 / 32.61 / 38.96 60.00 / 43.90 / 50.70 54.96 / 52.94 / 53.93 62.50 / 63.16 / 62.83 61.67 / 61.67 / 61.67 59.49 / 34.06 / 43.32 56.00 / 34.15 / 42.42 53.49 / 50.74 / 52.08 60.00 / 66.32 / 63.00 57.14 / 53.33 / 55.17 65.48 / 39.86 / 49.55 66.04 / 42.68 / 51.85 55.05 / 44.12 / 48.98"
2021.eacl-main.52,N19-1423,0,0.525565,"nd one of the three alternative descriptions of the P ERP I ND and the second TARGET, the telephone company building. As a result of these complications, end-to-end sentence-level event extraction models (Chen et al., 2015; Lample et al., 2016), which dominate the literature, are ill-suited for the REE task, which calls for models that encode information and track entities across a longer context. Fortunately, neural models for event extraction that have the ability to model longer contexts have been developed. Du and Cardie (2020), for example, extend standard contextualized representations (Devlin et al., 2019) to produce a documentlevel sequence tagging model for event argument extraction. Both approaches show improvements in performance over sentence-level models on event extraction. Regrettably, these approaches (as well as most sentence-level methods) handle each candidate role-filler prediction in isolation. Consequently, they cannot easily model the coreference structure required to limit spurious role-filler mention extractions. Nor can they easily exploit semantic dependencies between closely related roles like the P ERP I ND and the P ERP O RG, which can share a portion of the same entity s"
2021.eacl-main.52,2020.acl-main.714,1,0.147426,"bing example should, therefore, include just one of the “water pipes” references, and one of the three alternative descriptions of the P ERP I ND and the second TARGET, the telephone company building. As a result of these complications, end-to-end sentence-level event extraction models (Chen et al., 2015; Lample et al., 2016), which dominate the literature, are ill-suited for the REE task, which calls for models that encode information and track entities across a longer context. Fortunately, neural models for event extraction that have the ability to model longer contexts have been developed. Du and Cardie (2020), for example, extend standard contextualized representations (Devlin et al., 2019) to produce a documentlevel sequence tagging model for event argument extraction. Both approaches show improvements in performance over sentence-level models on event extraction. Regrettably, these approaches (as well as most sentence-level methods) handle each candidate role-filler prediction in isolation. Consequently, they cannot easily model the coreference structure required to limit spurious role-filler mention extractions. Nor can they easily exploit semantic dependencies between closely related roles lik"
2021.eacl-main.52,I17-1036,0,0.0150541,"n (2008) and Liao and Grishman (2010) utilize event type co-occurrence patterns to propagate 635 2 Our code for the evaluation script and models is at https://github.com/xinyadu/grit_doc_ event_entity for reproduction purposes. event classification decisions. Yang and Mitchell (2016) propose to learn within-event (sentence) structures for jointly extracting events and entities within a document context. Similarly, from a methodological perspective, our GRIT model also learns structured information, but it learns the dependencies between role-filler entity mentions and between different roles. Duan et al. (2017) and Zhao et al. (2018) leverage document embeddings as additional features to aid event detection. Although the approaches above make decisions with cross-sentence information, their extractions are still done the sentence level. Document-level IE Document-level event rolefiller mention extraction has been explored in recent work, using hand-designed features for both local and additional context (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012), and with end-to-end sequence tagging based models with contextualized pre-trained representations (Du and Cardie, 2020). These efforts are"
2021.eacl-main.52,2020.acl-main.718,0,0.0908325,"Missing"
2021.eacl-main.52,X96-1047,0,0.464851,"cording to unofficial reports, the bomb contained [125 to 150 grams of TnT] and was placed in the back of the [Pilmai [telephone company building]]. The explosion occurred at 2350 on 16 January, causing panic but no casualties. The explosion caused damages to the [telephone company offices]. It also destroyed a [public telephone booth] and [water pipes]. Witnesses reported that the bomb was planted by [[two men] wearing sports clothes], who escaped into the night. … They were later identified as [[Shining Path] members]. Gold extractions: Document-level template filling (Sundheim, 1991, 1993; Grishman and Sundheim, 1996) is a classic problem in information extraction (IE) and NLP (Jurafsky and Martin, 2014). It is of great importance for automating many real-world tasks, such as event extraction from newswire (Sundheim, 1991). The complete task is generally tackled in two steps. The first step detects events in the article and assigns templates to each of them (template recognition); the second step performs role-filler entity extraction (REE) for filling in the templates. In this work we focus on the role-filler entity extraction (REE) sub-task of template filling (Figure 1).1 The input text describes a bomb"
2021.eacl-main.52,P11-1114,0,0.877778,"two steps. The first step detects events in the article and assigns templates to each of them (template recognition); the second step performs role-filler entity extraction (REE) for filling in the templates. In this work we focus on the role-filler entity extraction (REE) sub-task of template filling (Figure 1).1 The input text describes a bombing event; the goal is to identify the entities that fill any of the roles associated with the event (e.g., the perpetrator, their organization, the weapon) by extracting 1 In this work, we assume there is one generic template for the entire document (Huang and Riloff, 2011, 2012). water pipes, water pipes Physical Target Pilmai telephone company building, telephone company building, telephone company offices public telephone booth Weapon 125 to 150 grams of TnT Victim - Figure 1: Role-filler entity extraction (REE). The first mention of each role-filler entity is bold in the table and document. The arrows denote coreferent mentions. a descriptive “mention” of it – a string from the document. In contrast to sentence-level event extraction (see, e.g., the ACE evaluation (Linguistic Data Consortium, 2005)), document-level REE introduces 634 Proceedings of the 16th"
2021.eacl-main.52,P08-1030,0,0.203698,"2015), which explored a variety of hand-designed features. More recently, neural network based models such as recurrent neural networks (Nguyen et al., 2016; Feng et al., 2018), convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) and attention mechanisms (Liu et al., 2017, 2018) have also been shown to help improve performance. Beyond the taskspecific features learned by the deep neural models, Zhang et al. (2019) and Wadden et al. (2019) also utilize pre-trained contextualized representations. Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) and Liao and Grishman (2010) utilize event type co-occurrence patterns to propagate 635 2 Our code for the evaluation script and models is at https://github.com/xinyadu/grit_doc_ event_entity for reproduction purposes. event classification decisions. Yang and Mitchell (2016) propose to learn within-event (sentence) structures for jointly extracting events and entities within a document context. Similarly, from a methodological perspective, our GRIT model also learns structured information, but it learns the dependencies between role-filler entity mentions and between different roles. Duan et"
2021.eacl-main.52,N19-1370,0,0.11043,"Missing"
2021.eacl-main.52,N16-1030,0,0.0580695,"at does not explicitly mention the explosion of the bomb. In addition, REE is ultimately an entity-based task — exactly one descriptive mention for each role-filler should be extracted even when the entity is referenced multiple times in connection with the event. The final output for the bombing example should, therefore, include just one of the “water pipes” references, and one of the three alternative descriptions of the P ERP I ND and the second TARGET, the telephone company building. As a result of these complications, end-to-end sentence-level event extraction models (Chen et al., 2015; Lample et al., 2016), which dominate the literature, are ill-suited for the REE task, which calls for models that encode information and track entities across a longer context. Fortunately, neural models for event extraction that have the ability to model longer contexts have been developed. Du and Cardie (2020), for example, extend standard contextualized representations (Devlin et al., 2019) to produce a documentlevel sequence tagging model for event argument extraction. Both approaches show improvements in performance over sentence-level models on event extraction. Regrettably, these approaches (as well as mos"
2021.eacl-main.52,P13-1008,0,0.0991986,"model outperforms substantially strong baseline models. We also demonstrate that GRIT is better than existing document-level event extraction approaches at capturing linguistic properties critical for the task, including coreference between entity mentions and cross-role extraction dependencies.2 2 Related Work Sentence-level Event Extraction Most work in event extraction has focused on the ACE sentencelevel event task (Walker et al., 2006), which requires the detection of an event trigger and extraction of its arguments from within a single sentence. Previous state-of-the-art methods include Li et al. (2013) and Li et al. (2015), which explored a variety of hand-designed features. More recently, neural network based models such as recurrent neural networks (Nguyen et al., 2016; Feng et al., 2018), convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) and attention mechanisms (Liu et al., 2017, 2018) have also been shown to help improve performance. Beyond the taskspecific features learned by the deep neural models, Zhang et al. (2019) and Wadden et al. (2019) also utilize pre-trained contextualized representations. Only a few models have gone beyond individual sentences to"
2021.eacl-main.52,W15-4502,0,0.0215223,"stantially strong baseline models. We also demonstrate that GRIT is better than existing document-level event extraction approaches at capturing linguistic properties critical for the task, including coreference between entity mentions and cross-role extraction dependencies.2 2 Related Work Sentence-level Event Extraction Most work in event extraction has focused on the ACE sentencelevel event task (Walker et al., 2006), which requires the detection of an event trigger and extraction of its arguments from within a single sentence. Previous state-of-the-art methods include Li et al. (2013) and Li et al. (2015), which explored a variety of hand-designed features. More recently, neural network based models such as recurrent neural networks (Nguyen et al., 2016; Feng et al., 2018), convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) and attention mechanisms (Liu et al., 2017, 2018) have also been shown to help improve performance. Beyond the taskspecific features learned by the deep neural models, Zhang et al. (2019) and Wadden et al. (2019) also utilize pre-trained contextualized representations. Only a few models have gone beyond individual sentences to make decisions. Ji an"
2021.eacl-main.52,P10-1081,0,0.182524,"iety of hand-designed features. More recently, neural network based models such as recurrent neural networks (Nguyen et al., 2016; Feng et al., 2018), convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) and attention mechanisms (Liu et al., 2017, 2018) have also been shown to help improve performance. Beyond the taskspecific features learned by the deep neural models, Zhang et al. (2019) and Wadden et al. (2019) also utilize pre-trained contextualized representations. Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) and Liao and Grishman (2010) utilize event type co-occurrence patterns to propagate 635 2 Our code for the evaluation script and models is at https://github.com/xinyadu/grit_doc_ event_entity for reproduction purposes. event classification decisions. Yang and Mitchell (2016) propose to learn within-event (sentence) structures for jointly extracting events and entities within a document context. Similarly, from a methodological perspective, our GRIT model also learns structured information, but it learns the dependencies between role-filler entity mentions and between different roles. Duan et al. (2017) and Zhao et al. (2"
2021.eacl-main.52,P17-1164,0,0.0374087,"Missing"
2021.eacl-main.52,P19-1276,0,0.0131219,"sing hand-designed features for both local and additional context (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012), and with end-to-end sequence tagging based models with contextualized pre-trained representations (Du and Cardie, 2020). These efforts are the most related to our work. The key difference is that our work focuses on a more challenging, and more realistic, setting: extracting role-filler entities rather than lists of role-filler mentions that are not grouped according to their associated entity. Also on a related note, Chambers and Jurafsky (2011), Chambers (2013), and Liu et al. (2019) work on unsupervised event schema induction and open-domain event extraction from documents. The main idea is to group entities corresponding to the same role into an event template. Recently, there has also been increasing interest in cross-sentence/document-level relation extraction (RE). In the scientific domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representations between entity span extraction and rel"
2021.eacl-main.52,D18-1156,0,0.142723,"Missing"
2021.eacl-main.52,D18-1360,0,0.0198759,"that are not grouped according to their associated entity. Also on a related note, Chambers and Jurafsky (2011), Chambers (2013), and Liu et al. (2019) work on unsupervised event schema induction and open-domain event extraction from documents. The main idea is to group entities corresponding to the same role into an event template. Recently, there has also been increasing interest in cross-sentence/document-level relation extraction (RE). In the scientific domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representations between entity span extraction and relations. Yao et al. (2019) construct an RE dataset of cross-sentence relations on Wikipedia paragraphs. Ebner et al. (2020) introduce RAMS dataset for multi-sentence argument mention linking, while we focus on entity-level extraction in our work. Different from work on joint modeling (Miwa and Bansal, 2016) and multi-task learning (Luan et al., 2019) setting for extracting entities and relations, through the generative modeling setup, our GRIT model implicitly cap"
2021.eacl-main.52,N19-1308,0,0.0178595,"017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representations between entity span extraction and relations. Yao et al. (2019) construct an RE dataset of cross-sentence relations on Wikipedia paragraphs. Ebner et al. (2020) introduce RAMS dataset for multi-sentence argument mention linking, while we focus on entity-level extraction in our work. Different from work on joint modeling (Miwa and Bansal, 2016) and multi-task learning (Luan et al., 2019) setting for extracting entities and relations, through the generative modeling setup, our GRIT model implicitly captures (non-)coreference relations between noun phrases, without relying on the cross-sentence coreference and relation annotations during training. Neural Generative Models with a Shared Module for Encoder and Decoder Our GRIT model uses one shared transformer module for both the encoder and decoder, which is simple and effective. For the machine translation task, He et al. (2018) propose a model which shares the parameters of each layer between the encoder and decoder to regular"
2021.eacl-main.52,H05-1004,0,0.464167,"IT is built upon the pre-trained transformer model (BERT): we add a pointer selection module in the decoder to permit access to the entire input document, and a generative head to model document-level extraction decisions. In spite of the added extraction capability, GRIT requires no additional parameters beyond those in the pre-trained BERT. • To measure the model’s ability to both extract entities for each role, and implicitly recognize coreferent relations between entity mentions, we design a metric (CEAF-REE) based on a maximum bipartite matching algorithm, drawing insights from the CEAF (Luo, 2005) coreference resolution measure. • We evaluate GRIT on the MUC-4 (1992) REE task (Section 3). Empirically, our model outperforms substantially strong baseline models. We also demonstrate that GRIT is better than existing document-level event extraction approaches at capturing linguistic properties critical for the task, including coreference between entity mentions and cross-role extraction dependencies.2 2 Related Work Sentence-level Event Extraction Most work in event extraction has focused on the ACE sentencelevel event task (Walker et al., 2006), which requires the detection of an event tr"
2021.eacl-main.52,M91-1001,0,0.142289,"zation Shining Path According to unofficial reports, the bomb contained [125 to 150 grams of TnT] and was placed in the back of the [Pilmai [telephone company building]]. The explosion occurred at 2350 on 16 January, causing panic but no casualties. The explosion caused damages to the [telephone company offices]. It also destroyed a [public telephone booth] and [water pipes]. Witnesses reported that the bomb was planted by [[two men] wearing sports clothes], who escaped into the night. … They were later identified as [[Shining Path] members]. Gold extractions: Document-level template filling (Sundheim, 1991, 1993; Grishman and Sundheim, 1996) is a classic problem in information extraction (IE) and NLP (Jurafsky and Martin, 2014). It is of great importance for automating many real-world tasks, such as event extraction from newswire (Sundheim, 1991). The complete task is generally tackled in two steps. The first step detects events in the article and assigns templates to each of them (template recognition); the second step performs role-filler entity extraction (REE) for filling in the templates. In this work we focus on the role-filler entity extraction (REE) sub-task of template filling (Figure"
2021.eacl-main.52,P16-1105,0,0.0315253,"n (RE). In the scientific domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representations between entity span extraction and relations. Yao et al. (2019) construct an RE dataset of cross-sentence relations on Wikipedia paragraphs. Ebner et al. (2020) introduce RAMS dataset for multi-sentence argument mention linking, while we focus on entity-level extraction in our work. Different from work on joint modeling (Miwa and Bansal, 2016) and multi-task learning (Luan et al., 2019) setting for extracting entities and relations, through the generative modeling setup, our GRIT model implicitly captures (non-)coreference relations between noun phrases, without relying on the cross-sentence coreference and relation annotations during training. Neural Generative Models with a Shared Module for Encoder and Decoder Our GRIT model uses one shared transformer module for both the encoder and decoder, which is simple and effective. For the machine translation task, He et al. (2018) propose a model which shares the parameters of each laye"
2021.eacl-main.52,D19-1585,0,0.558781,"n event trigger and extraction of its arguments from within a single sentence. Previous state-of-the-art methods include Li et al. (2013) and Li et al. (2015), which explored a variety of hand-designed features. More recently, neural network based models such as recurrent neural networks (Nguyen et al., 2016; Feng et al., 2018), convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) and attention mechanisms (Liu et al., 2017, 2018) have also been shown to help improve performance. Beyond the taskspecific features learned by the deep neural models, Zhang et al. (2019) and Wadden et al. (2019) also utilize pre-trained contextualized representations. Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) and Liao and Grishman (2010) utilize event type co-occurrence patterns to propagate 635 2 Our code for the evaluation script and models is at https://github.com/xinyadu/grit_doc_ event_entity for reproduction purposes. event classification decisions. Yang and Mitchell (2016) propose to learn within-event (sentence) structures for jointly extracting events and entities within a document context. Similarly, from a methodological perspective,"
2021.eacl-main.52,D18-1215,0,0.0170595,"nging, and more realistic, setting: extracting role-filler entities rather than lists of role-filler mentions that are not grouped according to their associated entity. Also on a related note, Chambers and Jurafsky (2011), Chambers (2013), and Liu et al. (2019) work on unsupervised event schema induction and open-domain event extraction from documents. The main idea is to group entities corresponding to the same role into an event template. Recently, there has also been increasing interest in cross-sentence/document-level relation extraction (RE). In the scientific domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representations between entity span extraction and relations. Yao et al. (2019) construct an RE dataset of cross-sentence relations on Wikipedia paragraphs. Ebner et al. (2020) introduce RAMS dataset for multi-sentence argument mention linking, while we focus on entity-level extraction in our work. Different from work on joint modeling (Miwa and Bansal, 2016) and multi-task learning (Luan et al., 2019) setting"
2021.eacl-main.52,N16-1034,0,0.0574356,"guistic properties critical for the task, including coreference between entity mentions and cross-role extraction dependencies.2 2 Related Work Sentence-level Event Extraction Most work in event extraction has focused on the ACE sentencelevel event task (Walker et al., 2006), which requires the detection of an event trigger and extraction of its arguments from within a single sentence. Previous state-of-the-art methods include Li et al. (2013) and Li et al. (2015), which explored a variety of hand-designed features. More recently, neural network based models such as recurrent neural networks (Nguyen et al., 2016; Feng et al., 2018), convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) and attention mechanisms (Liu et al., 2017, 2018) have also been shown to help improve performance. Beyond the taskspecific features learned by the deep neural models, Zhang et al. (2019) and Wadden et al. (2019) also utilize pre-trained contextualized representations. Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) and Liao and Grishman (2010) utilize event type co-occurrence patterns to propagate 635 2 Our code for the evaluation script and mode"
2021.eacl-main.52,P15-2060,0,0.0156441,"entity mentions and cross-role extraction dependencies.2 2 Related Work Sentence-level Event Extraction Most work in event extraction has focused on the ACE sentencelevel event task (Walker et al., 2006), which requires the detection of an event trigger and extraction of its arguments from within a single sentence. Previous state-of-the-art methods include Li et al. (2013) and Li et al. (2015), which explored a variety of hand-designed features. More recently, neural network based models such as recurrent neural networks (Nguyen et al., 2016; Feng et al., 2018), convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) and attention mechanisms (Liu et al., 2017, 2018) have also been shown to help improve performance. Beyond the taskspecific features learned by the deep neural models, Zhang et al. (2019) and Wadden et al. (2019) also utilize pre-trained contextualized representations. Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) and Liao and Grishman (2010) utilize event type co-occurrence patterns to propagate 635 2 Our code for the evaluation script and models is at https://github.com/xinyadu/grit_doc_ event_entity for reproduction pu"
2021.eacl-main.52,D09-1016,0,0.470518,"om a methodological perspective, our GRIT model also learns structured information, but it learns the dependencies between role-filler entity mentions and between different roles. Duan et al. (2017) and Zhao et al. (2018) leverage document embeddings as additional features to aid event detection. Although the approaches above make decisions with cross-sentence information, their extractions are still done the sentence level. Document-level IE Document-level event rolefiller mention extraction has been explored in recent work, using hand-designed features for both local and additional context (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012), and with end-to-end sequence tagging based models with contextualized pre-trained representations (Du and Cardie, 2020). These efforts are the most related to our work. The key difference is that our work focuses on a more challenging, and more realistic, setting: extracting role-filler entities rather than lists of role-filler mentions that are not grouped according to their associated entity. Also on a related note, Chambers and Jurafsky (2011), Chambers (2013), and Liu et al. (2019) work on unsupervised event schema induction and open-domain event extraction"
2021.eacl-main.52,Q17-1008,0,0.0250602,"ses on a more challenging, and more realistic, setting: extracting role-filler entities rather than lists of role-filler mentions that are not grouped according to their associated entity. Also on a related note, Chambers and Jurafsky (2011), Chambers (2013), and Liu et al. (2019) work on unsupervised event schema induction and open-domain event extraction from documents. The main idea is to group entities corresponding to the same role into an event template. Recently, there has also been increasing interest in cross-sentence/document-level relation extraction (RE). In the scientific domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representations between entity span extraction and relations. Yao et al. (2019) construct an RE dataset of cross-sentence relations on Wikipedia paragraphs. Ebner et al. (2020) introduce RAMS dataset for multi-sentence argument mention linking, while we focus on entity-level extraction in our work. Different from work on joint modeling (Miwa and Bansal, 2016) and multi-task learning (Luan"
2021.eacl-main.52,N16-1033,0,0.0470744,"nisms (Liu et al., 2017, 2018) have also been shown to help improve performance. Beyond the taskspecific features learned by the deep neural models, Zhang et al. (2019) and Wadden et al. (2019) also utilize pre-trained contextualized representations. Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) and Liao and Grishman (2010) utilize event type co-occurrence patterns to propagate 635 2 Our code for the evaluation script and models is at https://github.com/xinyadu/grit_doc_ event_entity for reproduction purposes. event classification decisions. Yang and Mitchell (2016) propose to learn within-event (sentence) structures for jointly extracting events and entities within a document context. Similarly, from a methodological perspective, our GRIT model also learns structured information, but it learns the dependencies between role-filler entity mentions and between different roles. Duan et al. (2017) and Zhao et al. (2018) leverage document embeddings as additional features to aid event detection. Although the approaches above make decisions with cross-sentence information, their extractions are still done the sentence level. Document-level IE Document-level ev"
2021.eacl-main.52,P19-1074,0,0.021912,"unsupervised event schema induction and open-domain event extraction from documents. The main idea is to group entities corresponding to the same role into an event template. Recently, there has also been increasing interest in cross-sentence/document-level relation extraction (RE). In the scientific domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representations between entity span extraction and relations. Yao et al. (2019) construct an RE dataset of cross-sentence relations on Wikipedia paragraphs. Ebner et al. (2020) introduce RAMS dataset for multi-sentence argument mention linking, while we focus on entity-level extraction in our work. Different from work on joint modeling (Miwa and Bansal, 2016) and multi-task learning (Luan et al., 2019) setting for extracting entities and relations, through the generative modeling setup, our GRIT model implicitly captures (non-)coreference relations between noun phrases, without relying on the cross-sentence coreference and relation annotations during training. Neural Gen"
2021.eacl-main.52,P18-2066,0,0.0123888,"rishman (2010) utilize event type co-occurrence patterns to propagate 635 2 Our code for the evaluation script and models is at https://github.com/xinyadu/grit_doc_ event_entity for reproduction purposes. event classification decisions. Yang and Mitchell (2016) propose to learn within-event (sentence) structures for jointly extracting events and entities within a document context. Similarly, from a methodological perspective, our GRIT model also learns structured information, but it learns the dependencies between role-filler entity mentions and between different roles. Duan et al. (2017) and Zhao et al. (2018) leverage document embeddings as additional features to aid event detection. Although the approaches above make decisions with cross-sentence information, their extractions are still done the sentence level. Document-level IE Document-level event rolefiller mention extraction has been explored in recent work, using hand-designed features for both local and additional context (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012), and with end-to-end sequence tagging based models with contextualized pre-trained representations (Du and Cardie, 2020). These efforts are the most related to ou"
2021.emnlp-demo.21,N19-1423,0,0.059334,"Missing"
2021.emnlp-demo.21,P19-1346,1,0.899407,"Missing"
2021.emnlp-demo.21,2021.naacl-demos.6,0,0.094252,"Missing"
2021.emnlp-demo.21,N18-2017,0,0.0367789,"Missing"
2021.emnlp-demo.21,L16-1262,0,0.0892661,"Missing"
2021.emnlp-demo.21,2020.acl-main.487,0,0.0227156,"Missing"
2021.emnlp-demo.21,W17-3204,0,0.0370409,"Missing"
2021.emnlp-demo.21,2021.naacl-main.393,0,0.0696758,"Missing"
2021.emnlp-demo.21,J93-2004,0,0.0747961,"Missing"
2021.emnlp-demo.21,N18-1202,0,0.0511123,"Missing"
2021.emnlp-demo.21,S18-2023,0,0.0586604,"Missing"
2021.emnlp-demo.21,N06-2015,0,0.127189,"Missing"
2021.emnlp-demo.21,D16-1264,0,0.124118,"Missing"
2021.emnlp-demo.21,2021.gem-1.11,1,0.771689,"Missing"
2021.emnlp-demo.21,W00-0726,0,0.715212,"Missing"
2021.emnlp-demo.21,W18-5446,0,0.0603795,"Missing"
2021.emnlp-main.807,2021.acl-long.91,0,0.0344617,"ise feature selection — where the goal is selecting features per-example — is a newer research area (Chen et al., 2018). We review local explanation methods used for NLP. Gradient saliency. Gradient-based saliency methods have long been used as a measure of feature importance in machine learning (Baehrens et al., 2010; Simonyan et al., 2013; Li et al., 2016a). Some variations involve word embeddings (Denil et al., 2014); integrated gradients, to improve sensitivity (Sundararajan et al., 2017); and relevance-propagation to track each input’s contribution through the network (Bach et al., 2015; Voita et al., 2021). But there are drawbacks to using gradient-based methods as explanatory tools. Sundararajan et al. (2017) show that in practice, gradients are saturated: they may all be close to zero for a wellfitted function, and thus not reflect importance. Adversarial methods can also distort gradient-based saliences while keeping a model’s prediction the same (Ghorbani et al., 2019; Wang et al., 2020). We compare greedy rationalization to gradient saliency methods in Section 8. Attention. Recently, NLP practitioners have focused on using attention weights as explanatory tools. The literature has made a d"
2021.emnlp-main.807,2020.findings-emnlp.24,0,0.0199944,"beddings (Denil et al., 2014); integrated gradients, to improve sensitivity (Sundararajan et al., 2017); and relevance-propagation to track each input’s contribution through the network (Bach et al., 2015; Voita et al., 2021). But there are drawbacks to using gradient-based methods as explanatory tools. Sundararajan et al. (2017) show that in practice, gradients are saturated: they may all be close to zero for a wellfitted function, and thus not reflect importance. Adversarial methods can also distort gradient-based saliences while keeping a model’s prediction the same (Ghorbani et al., 2019; Wang et al., 2020). We compare greedy rationalization to gradient saliency methods in Section 8. Attention. Recently, NLP practitioners have focused on using attention weights as explanatory tools. The literature has made a distinction between faithfulness and plausibility. An explanation is faithful if it accurately depicts how a model makes a decision (Jacovi and Goldberg, 2020); an explanation is plausible if it can be understood and interpreted by humans (Wiegreffe and Pinter, 2019). Practitioners have shown that attention-based explanations are generally not faithful (Jain and Wallace, (8) 2019; Serrano an"
2021.emnlp-main.807,P16-1144,0,0.392655,"ess, 1989). We show that compatibility can be learned by conditioning on randomly sampled context subsets while training a model. For large pretrained models like GPT-2 (Radford et al., 2019), fine-tuning is sufficient. In an empirical study, we compare greedy rationalization to various gradient- and attention-based explanation methods on language modeling and machine translation. Greedy rationalization best optimizes the objective, and its rationales are most faithful to the inner workings of the model. We additionally create a new dataset of annotated rationales based on the Lambada corpus (Paperno et al., 2016). We find that greedy rationales are most similar to human annotations, both on our dataset and on a labeled dataset of translation alignments. Our code and annotated dataset are available.1 2 Sequential Rationales Consider a sequence of tokens, y1:T , generated by some unknown process y1:T ∼ F . The goal of sequence modeling is to learn a probabilistic model pθ that approximates F from samples. Maximumlikelihood estimation is an effective way to train these models, where θ is fit according to arg max Ey1:T ∼F [log pθ (y1:T )]. Sequence models are typically factored into conditional distributi"
2021.emnlp-main.807,D19-1002,0,0.162015,"ance. Adversarial methods can also distort gradient-based saliences while keeping a model’s prediction the same (Ghorbani et al., 2019; Wang et al., 2020). We compare greedy rationalization to gradient saliency methods in Section 8. Attention. Recently, NLP practitioners have focused on using attention weights as explanatory tools. The literature has made a distinction between faithfulness and plausibility. An explanation is faithful if it accurately depicts how a model makes a decision (Jacovi and Goldberg, 2020); an explanation is plausible if it can be understood and interpreted by humans (Wiegreffe and Pinter, 2019). Practitioners have shown that attention-based explanations are generally not faithful (Jain and Wallace, (8) 2019; Serrano and Smith, 2019), but that they may 10318 be plausible (Wiegreffe and Pinter, 2019; Mohankumar et al., 2020; Vashishth et al., 2019). Others show that attention weights should not be interpreted as belonging to single tokens since they mix information across tokens (Brunner et al., 2019; Kobayashi et al., 2020). Bastings and Filippova (2020) argue that general input saliency measures, such as gradients, are better suited for explainability than attention. We compare gree"
2021.emnlp-main.829,2020.sustainlp-1.20,0,0.0319402,"- version of magnitude pruning, the mask would just thors have worked on combining matrix factoriza- zero-out parameters with low absolute values. Movement pruning (Sanh et al., 2020) is a scoretion and weight pruning. While Mao et al. (2020) combine SVD-based matrix factorization with un- based pruning approach that encourages the model structured pruning, Wang et al. (2019) use struc- to optimize these score parameters. Specifically, we focus on the soft-movement variant of movetured pruning in order to reduce the rank. Related ment pruning that sets M (S) = 1(S > τ ) for a to our approach, Kim and Awadalla (2020) and threshold parameter τ , and optimizes a regularized McCarley (2019) both apply structured pruning on objective, the heads of the multi-head attention (MHA) and on the inner-layer nodes of the feed-forward netarg min L(θ0 ) + λkσ(S)k work (FFN). The former uses predefined pruning θ0 ,S ratios, shared across all layers, in order to select P the modules to prune after sorting them given an where λ is a hyper-parameter, kAk = i,j Ai,j importance score. McCarley (2019) compares dif- and σ is the sigmoid function. 10620 This pruning objective encourages the model to fine-tune the parameters whi"
2021.emnlp-main.829,N19-1423,0,0.0257017,"l structure, our method only requires the size and shapes of the blocks, i.e. the set of (M 0 , N 0 ) for each parameter matrix in the model. If blocks are too large, then they are difficult to prune, but if they are too small they do not support efficient inference. CNN/DailyMail (“CNN”) is formulated as a conditional generation task. We report the performance on the development set as measured by the accuracy for MNLI and SST-2, F1 for QQP, the exact match (EM) and F1 for SQuAD and ROUGE for CNN/DailyMail. We experiment with task-specific pruning of transformer language models. We use BERT (Devlin et al., 2019) (an encoder-only Transformer language model with 110M parameters, among which 85M are part of the linear layers present in the Transformer layers) for sentence classification and question answering (340M and 227M respectively for BERT-large), and BART (Lewis et al., 2020) (an encoder-decoder language model with 139M parameters, among which 99M are part of the linear layers present in the Transformer layers) for summarization (406M and 353M for BART-large). We compare against several baselines. Movement pruning is a fully unstructured approach and gives an upper bound on the sparsity trade-off"
2021.emnlp-main.829,2020.repl4nlp-1.18,0,0.0212664,"of former is task-agnostic, the one used to parameters consist of 4 projection matrices (Wq , obtain the latter is task-specific. dmodel ×dmodel (query, Other previous work has focused on unstructured Wk , Wv and Wo ) of size R pruning (LeCun et al., 1989; Han et al., 2015; Fran- key, value, out). These are used to project the hidden vector to and from the component attenkle and Carbin, 2018). When targeting transformer tion parts. In implementations, this projection is models, it is typical to select the weights to prune with the matrices in their folded tensor form based on their magnitude (Gordon et al., 2020), made d nheads × nmodel ×dmodel heads where nheads is the number or by computing an importance score using a first- R order method (Sanh et al., 2020). While these meth- of attention heads. In standard fine-tuning, starting from θ, we opods allow for a significant reduction in model size, timize the loss L (for instance, cross-entropy for specialized hardware is required to make use of the classification): resulting unstructured sparse matrices in order to speed up inference. arg min L(θ0 ) In contrast, structured pruning removes coherent θ0 groups of weights (Murray and Chiang, 2015; See et"
2021.emnlp-main.829,2020.acl-main.703,0,0.0173568,"ilyMail (“CNN”) is formulated as a conditional generation task. We report the performance on the development set as measured by the accuracy for MNLI and SST-2, F1 for QQP, the exact match (EM) and F1 for SQuAD and ROUGE for CNN/DailyMail. We experiment with task-specific pruning of transformer language models. We use BERT (Devlin et al., 2019) (an encoder-only Transformer language model with 110M parameters, among which 85M are part of the linear layers present in the Transformer layers) for sentence classification and question answering (340M and 227M respectively for BERT-large), and BART (Lewis et al., 2020) (an encoder-decoder language model with 139M parameters, among which 99M are part of the linear layers present in the Transformer layers) for summarization (406M and 353M for BART-large). We compare against several baselines. Movement pruning is a fully unstructured approach and gives an upper bound on the sparsity trade-offs we hope to achieve, even if it provides little speed benefit. We also compare our results against state-ofthe-art approaches developed for fast inference of transformer-based language models. DistilBERT (Sanh et al., 2019) is obtained by distilling through pre-training a"
2021.emnlp-main.829,2021.acl-long.378,1,0.847303,"Missing"
2021.emnlp-main.829,2020.coling-main.287,0,0.0182968,"modify the model by introducing score parameters S for each parameter i and replace the 2019; Voita et al., 2019) show that some heads original parameter matrices with a masked version can be removed without significant degradation in 0 = W M (S). For instance, in the simplest W performance, leading to the conclusion that most heads provide redundant information. Other au- version of magnitude pruning, the mask would just thors have worked on combining matrix factoriza- zero-out parameters with low absolute values. Movement pruning (Sanh et al., 2020) is a scoretion and weight pruning. While Mao et al. (2020) combine SVD-based matrix factorization with un- based pruning approach that encourages the model structured pruning, Wang et al. (2019) use struc- to optimize these score parameters. Specifically, we focus on the soft-movement variant of movetured pruning in order to reduce the rank. Related ment pruning that sets M (S) = 1(S > τ ) for a to our approach, Kim and Awadalla (2020) and threshold parameter τ , and optimizes a regularized McCarley (2019) both apply structured pruning on objective, the heads of the multi-head attention (MHA) and on the inner-layer nodes of the feed-forward netarg mi"
2021.emnlp-main.829,A94-1016,0,0.297183,"Missing"
2021.emnlp-main.829,D15-1107,0,0.0253688,"2021 Conference on Empirical Methods in Natural Language Processing, pages 10619–10629 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Related Work ferent methods to compute the prunable module masks and find L0 regularization to perform the best. There has been a growing interest in the compression of pre-trained language models. We consider three varieties of methods: distillation, pruning, 3 Background and structured pruning. Starting with a transformer model with parameKnowledge distillation, introduced by Hinton θ, our goal is to produce a set of parameters et al. (2015), is a popular compression technique. ters 0 that are both fine-tuned for a specific end-task θ Researchers have applied this method to a variety of NLP models (Tang et al., 2019; Sun et al., 2019; and smaller in such a way that inference can be efficiently computed on parallel hardware. Turc et al., 2019). Distillation has been used to The two largest lines in the transformer paobtain significantly smaller BERT models achieving competitive performances. Sanh et al. (2019) rameter budget are the feed-forward network sublayer (FFN) and the multi-head attention sub-layer distills BERT into shall"
2021.emnlp-main.829,P18-2124,0,0.0314822,"Missing"
2021.emnlp-main.829,K16-1029,0,0.0606949,"Missing"
2021.emnlp-main.829,D19-1441,0,0.287329,"sformer model with parameKnowledge distillation, introduced by Hinton θ, our goal is to produce a set of parameters et al. (2015), is a popular compression technique. ters 0 that are both fine-tuned for a specific end-task θ Researchers have applied this method to a variety of NLP models (Tang et al., 2019; Sun et al., 2019; and smaller in such a way that inference can be efficiently computed on parallel hardware. Turc et al., 2019). Distillation has been used to The two largest lines in the transformer paobtain significantly smaller BERT models achieving competitive performances. Sanh et al. (2019) rameter budget are the feed-forward network sublayer (FFN) and the multi-head attention sub-layer distills BERT into shallower students during the pre-training stage and optionally during the fine- (MHA). The FFN parameters consist of two matrices (W1 and W2 ) of transposed shape Rdmodel ×dff tuning stage. MobileBERT (Sun et al., 2020) and and Rdff ×dmodel where dmodel is the hidden size and TinyBERT (Jiao et al., 2019) are obtained thanks to a layer-wise distillation strategy. While the dis- dff  dmodel is the inner size. These are used in the standard fashion by the network. The MHA tillat"
2021.emnlp-main.829,2020.acl-main.195,0,0.0727415,"and smaller in such a way that inference can be efficiently computed on parallel hardware. Turc et al., 2019). Distillation has been used to The two largest lines in the transformer paobtain significantly smaller BERT models achieving competitive performances. Sanh et al. (2019) rameter budget are the feed-forward network sublayer (FFN) and the multi-head attention sub-layer distills BERT into shallower students during the pre-training stage and optionally during the fine- (MHA). The FFN parameters consist of two matrices (W1 and W2 ) of transposed shape Rdmodel ×dff tuning stage. MobileBERT (Sun et al., 2020) and and Rdff ×dmodel where dmodel is the hidden size and TinyBERT (Jiao et al., 2019) are obtained thanks to a layer-wise distillation strategy. While the dis- dff  dmodel is the inner size. These are used in the standard fashion by the network. The MHA tillation of former is task-agnostic, the one used to parameters consist of 4 projection matrices (Wq , obtain the latter is task-specific. dmodel ×dmodel (query, Other previous work has focused on unstructured Wk , Wv and Wo ) of size R pruning (LeCun et al., 1989; Han et al., 2015; Fran- key, value, out). These are used to project the hidde"
2021.emnlp-main.829,P19-1580,0,0.142268,"for a significant reduction in model size, timize the loss L (for instance, cross-entropy for specialized hardware is required to make use of the classification): resulting unstructured sparse matrices in order to speed up inference. arg min L(θ0 ) In contrast, structured pruning removes coherent θ0 groups of weights (Murray and Chiang, 2015; See et al., 2016; Joulin et al., 2016; Fan et al., 2020; Score-based pruning methods (Ramanujan et al., Sajjad et al., 2020). Recent works (Michel et al., 2019) modify the model by introducing score parameters S for each parameter i and replace the 2019; Voita et al., 2019) show that some heads original parameter matrices with a masked version can be removed without significant degradation in 0 = W M (S). For instance, in the simplest W performance, leading to the conclusion that most heads provide redundant information. Other au- version of magnitude pruning, the mask would just thors have worked on combining matrix factoriza- zero-out parameters with low absolute values. Movement pruning (Sanh et al., 2020) is a scoretion and weight pruning. While Mao et al. (2020) combine SVD-based matrix factorization with un- based pruning approach that encourages the model"
2021.emnlp-main.829,2020.emnlp-main.496,0,0.0631616,"Missing"
2021.emnlp-main.829,N18-1101,0,0.014555,"e. We also include two additional baseline block types used to verify the approach: • (2n , 2n ), n ∈ [2, 5] : smaller power of two square block sizes to study the impact of size on performance (Block) • ( dnmodel , dmodel ) : for attention heads (Heads) heads The first considers small blocks, and the second considers very large functional blocks. 5 Experimental Setup We conduct experiments on five (English) tasks commonly used to evaluate pre-trained language models: question answering (SQuAD v1.1 Rajpurkar et al., 2016) and (SQuAD v2 Rajpurkar et al., 2018), natural language inference (MNLI Williams et al., 2018), sentence similarity (QQP Chen et al., 2018), sentiment classification (SST2 Socher et al., 2013) and abstractive summarization (CNN/DailyMail Hermann et al., 2015). These datasets respectively contain 87k, 130k, 392k, 2 Linear algebra libraries perform matrix multiplication us363k, 67k and 287k training examples, and are ing large blocks, typically 128*64. At a micro level those downloaded from the Hugging Face datasets hub. machines are typically 32 ways SIMD, and memory is loaded by large contiguous chunks to maximize bandwidth. UnSQuAD is formulated as a span-extraction task, structured s"
2021.emnlp-main.829,D13-1170,0,0.00403496,"∈ [2, 5] : smaller power of two square block sizes to study the impact of size on performance (Block) • ( dnmodel , dmodel ) : for attention heads (Heads) heads The first considers small blocks, and the second considers very large functional blocks. 5 Experimental Setup We conduct experiments on five (English) tasks commonly used to evaluate pre-trained language models: question answering (SQuAD v1.1 Rajpurkar et al., 2016) and (SQuAD v2 Rajpurkar et al., 2018), natural language inference (MNLI Williams et al., 2018), sentence similarity (QQP Chen et al., 2018), sentiment classification (SST2 Socher et al., 2013) and abstractive summarization (CNN/DailyMail Hermann et al., 2015). These datasets respectively contain 87k, 130k, 392k, 2 Linear algebra libraries perform matrix multiplication us363k, 67k and 287k training examples, and are ing large blocks, typically 128*64. At a micro level those downloaded from the Hugging Face datasets hub. machines are typically 32 ways SIMD, and memory is loaded by large contiguous chunks to maximize bandwidth. UnSQuAD is formulated as a span-extraction task, structured sparsity is hard to implement with dense algebra MNLI and QQP are sentence pairs classification per"
2021.findings-emnlp.318,D18-1149,0,0.0688619,"Missing"
2021.naacl-main.70,D13-1185,0,0.0231802,"er layer to predict whether the span represents certain role-filler entity and what the role is. SEQ TAGGING is a BERT-based sequence tagging model for extracting the role-fillers entities. A role-filler entity can appear in templates of different event types (e.g., “Zarate armed force” appear in both attack and bombing event). For both baselines, the prediction goal is multi-class classification. More specially, we adapt the DYGIE++ output layer implementation to first predict the role-filler entity’s role class, and then predicts its event classes conditioned on the entity’s role. Note that Chambers (2013) and Cheung et al. (2013) propose to do event schema induction with unsupervised learning. Given their unsupervised nature, empirically the performance is worse than supervised models (Patwardhan and Riloff, 2009). Thus we do not add these as comparisons. Models P R F1 G RIT- PIPELINE DY GIE++ (Wadden et al., 2019) SEQ TAGGING (Du and Cardie, 2020) 63.88 37.56 47.31 61.90 36.33 45.79 46.80 38.30 42.13 G TT 61.69 42.36 50.23∗ Table 2: Micro-average results on the full test set. 5 Results and Analysis Results on the full test set are shown in Table 2. We report the micro-average performance (pre"
2021.naacl-main.70,N13-1104,0,0.0319767,"whether the span represents certain role-filler entity and what the role is. SEQ TAGGING is a BERT-based sequence tagging model for extracting the role-fillers entities. A role-filler entity can appear in templates of different event types (e.g., “Zarate armed force” appear in both attack and bombing event). For both baselines, the prediction goal is multi-class classification. More specially, we adapt the DYGIE++ output layer implementation to first predict the role-filler entity’s role class, and then predicts its event classes conditioned on the entity’s role. Note that Chambers (2013) and Cheung et al. (2013) propose to do event schema induction with unsupervised learning. Given their unsupervised nature, empirically the performance is worse than supervised models (Patwardhan and Riloff, 2009). Thus we do not add these as comparisons. Models P R F1 G RIT- PIPELINE DY GIE++ (Wadden et al., 2019) SEQ TAGGING (Du and Cardie, 2020) 63.88 37.56 47.31 61.90 36.33 45.79 46.80 38.30 42.13 G TT 61.69 42.36 50.23∗ Table 2: Micro-average results on the full test set. 5 Results and Analysis Results on the full test set are shown in Table 2. We report the micro-average performance (precision, recall and F1). W"
2021.naacl-main.70,N19-1423,0,0.0226918,". For the < Role-filler Entities > of template i, following Du et al. (2020), we use the concatenation of target entity extractions for each role, separated by the separator token ([SEP]). Each entity is represented with its first mention’s beginning (b) and end (e) tokens: e11b , e11e , .. [SEP] e21b , e21e , .. [SEP] e31b , e31e , .. 3.2 Base Model and Decoding Constraints Next we describe the base model as well as special decoding constraints for template filling. BERT as Encoder and Decoder Our model extends upon the G RIT model for REE (Du et al., 2020). The base setup utilizes one BERT (Devlin et al., 2019) model for processing both the source and target tokens embeddings. To distinguish the encoder / decoder representations, it uses partial causal attention mask on the decoder side (Du et al., 2020). The joint sequence of source tokens’ embeddings (a0 , a1 , ..., am ) and target tokens’ embeddings (b0 , b1 , ..., bn ) are passed through BERT to obtain their contextualized representations, ˆ 0 ..., b ˆl ˆ a0 , ˆ a1 , ..., ˆ alsrc , b tgt = BERT(a0 , b1 , ..., alsrc , b0 , ..., bltgt ) Pointer Decoding For the final decoder layer, we replace word prediction with a simple pointer selection mechani"
2021.naacl-main.70,doddington-etal-2004-automatic,0,0.049614,"with a single end-to-end model. Introduction The classic template-filling task in information ex- shared perpetrator organization). Alternative endto-end event extraction models, even those incorpotraction involves extracting event-based templates from documents (Grishman and Sundheim, 1996; rating pretrained LM representations, only model events in isolation (Wadden et al., 2019; Du and Jurafsky and Martin, 2009; Grishman, 2019). It is usually tackled by a pipeline of two separate sys- Cardie, 2020), and are mainly evaluated on ACEtems, one for role-filler entity extraction – extract- style (Doddington et al., 2004) event extraction from single sentences (Yang and Mitchell, 2016; ing event-relevant entities (e.g., noun phrases) from the document; another for template/event recogni- Lin et al., 2020). tion – assigning each of the candidate role-fillers to To naturally model between-event dependencies the event(s)/template(s) that it participates in and across a document for template filling, we proidentifying the type of each event/template. pose a framework called “G TT” based on generSimplifications of the task (Patwardhan and ative transformers (Figure 2). To our best knowlRiloff, 2009; Huang and Rilof"
2021.naacl-main.70,2020.acl-main.714,1,0.884155,"Missing"
2021.naacl-main.70,X96-1047,0,0.302174,"eapon bomb Victim - Event 3 Template Arson Perpetrator Indiv. - Perpetrator Org Zarate armed forces Physical Target old shack Weapon - Victim - Figure 1: The template-filling task. Role-filler entity extraction is shown on the left, and template recognition is shown on the right. Our system performs both of these document-level tasks with a single end-to-end model. Introduction The classic template-filling task in information ex- shared perpetrator organization). Alternative endto-end event extraction models, even those incorpotraction involves extracting event-based templates from documents (Grishman and Sundheim, 1996; rating pretrained LM representations, only model events in isolation (Wadden et al., 2019; Du and Jurafsky and Martin, 2009; Grishman, 2019). It is usually tackled by a pipeline of two separate sys- Cardie, 2020), and are mainly evaluated on ACEtems, one for role-filler entity extraction – extract- style (Doddington et al., 2004) event extraction from single sentences (Yang and Mitchell, 2016; ing event-relevant entities (e.g., noun phrases) from the document; another for template/event recogni- Lin et al., 2020). tion – assigning each of the candidate role-fillers to To naturally model betw"
2021.naacl-main.70,P11-1114,0,0.0287961,"n et al., 2004) event extraction from single sentences (Yang and Mitchell, 2016; ing event-relevant entities (e.g., noun phrases) from the document; another for template/event recogni- Lin et al., 2020). tion – assigning each of the candidate role-fillers to To naturally model between-event dependencies the event(s)/template(s) that it participates in and across a document for template filling, we proidentifying the type of each event/template. pose a framework called “G TT” based on generSimplifications of the task (Patwardhan and ative transformers (Figure 2). To our best knowlRiloff, 2009; Huang and Riloff, 2011, 2012; Du edge, this is the first attempt to build an end-to-end et al., 2020) assume that there is one generic tem- learning framework for this task. We build our plate and focus only on role-filler entity extraction. framework upon G RIT (Du et al., 2020), which However, real documents often describe multiple tackles role-filler entity extraction (REE), but not events (Figure 1). From the example, we can ob- template/event recognition. G RIT performs REE serve that between-event dependencies are impor- by “generating” a sequence of role-filler entities, tant (e.g., a single organization can"
2021.naacl-main.70,2020.acl-main.713,0,0.0315927,"rpotraction involves extracting event-based templates from documents (Grishman and Sundheim, 1996; rating pretrained LM representations, only model events in isolation (Wadden et al., 2019; Du and Jurafsky and Martin, 2009; Grishman, 2019). It is usually tackled by a pipeline of two separate sys- Cardie, 2020), and are mainly evaluated on ACEtems, one for role-filler entity extraction – extract- style (Doddington et al., 2004) event extraction from single sentences (Yang and Mitchell, 2016; ing event-relevant entities (e.g., noun phrases) from the document; another for template/event recogni- Lin et al., 2020). tion – assigning each of the candidate role-fillers to To naturally model between-event dependencies the event(s)/template(s) that it participates in and across a document for template filling, we proidentifying the type of each event/template. pose a framework called “G TT” based on generSimplifications of the task (Patwardhan and ative transformers (Figure 2). To our best knowlRiloff, 2009; Huang and Riloff, 2011, 2012; Du edge, this is the first attempt to build an end-to-end et al., 2020) assume that there is one generic tem- learning framework for this task. We build our plate and focus"
2021.naacl-main.70,D09-1016,0,0.10042,"Missing"
2021.naacl-main.70,D19-1585,0,0.102639,"Physical Target old shack Weapon - Victim - Figure 1: The template-filling task. Role-filler entity extraction is shown on the left, and template recognition is shown on the right. Our system performs both of these document-level tasks with a single end-to-end model. Introduction The classic template-filling task in information ex- shared perpetrator organization). Alternative endto-end event extraction models, even those incorpotraction involves extracting event-based templates from documents (Grishman and Sundheim, 1996; rating pretrained LM representations, only model events in isolation (Wadden et al., 2019; Du and Jurafsky and Martin, 2009; Grishman, 2019). It is usually tackled by a pipeline of two separate sys- Cardie, 2020), and are mainly evaluated on ACEtems, one for role-filler entity extraction – extract- style (Doddington et al., 2004) event extraction from single sentences (Yang and Mitchell, 2016; ing event-relevant entities (e.g., noun phrases) from the document; another for template/event recogni- Lin et al., 2020). tion – assigning each of the candidate role-fillers to To naturally model between-event dependencies the event(s)/template(s) that it participates in and across a docume"
2021.naacl-main.70,N16-1033,0,0.012825,"-filling task in information ex- shared perpetrator organization). Alternative endto-end event extraction models, even those incorpotraction involves extracting event-based templates from documents (Grishman and Sundheim, 1996; rating pretrained LM representations, only model events in isolation (Wadden et al., 2019; Du and Jurafsky and Martin, 2009; Grishman, 2019). It is usually tackled by a pipeline of two separate sys- Cardie, 2020), and are mainly evaluated on ACEtems, one for role-filler entity extraction – extract- style (Doddington et al., 2004) event extraction from single sentences (Yang and Mitchell, 2016; ing event-relevant entities (e.g., noun phrases) from the document; another for template/event recogni- Lin et al., 2020). tion – assigning each of the candidate role-fillers to To naturally model between-event dependencies the event(s)/template(s) that it participates in and across a document for template filling, we proidentifying the type of each event/template. pose a framework called “G TT” based on generSimplifications of the task (Patwardhan and ative transformers (Figure 2). To our best knowlRiloff, 2009; Huang and Riloff, 2011, 2012; Du edge, this is the first attempt to build an en"
2021.naacl-main.74,2020.emnlp-main.14,0,0.349961,"ral language tasks, it is unclear what a model learns during pre-training. Research in probing investigates this question by training a shallow classifier on top of the pre-trained model’s internal representations to predict some linguistic property (Adi et al., 2016; Shi et al., 2016; Tenney et al., 2019, inter alia). The resulting accuracy is then roughly indicative of the model encoding that property. However, it is unclear how much is learned by the probe versus already captured in the model representations. This question has been the subject of much recent debate (Hewitt and Liang, 2019; Voita and Titov, 2020; Pimentel et al., 2020b, inter alia). The code is available at https://github.com/ stevenxcao/subnetwork-probing. We would like the probe to find only and all properties captured by a model, leading to a tradeoff between accuracy and complexity: a linear probe is insufficient to find the non-linear patterns in neural models, but a deeper multi-layer perceptron (MLP) is complex enough to learn the task on its own. Motivated by this tradeoff and the goal of lowcomplexity probes, we consider a different approach based on pruning. Specifically, we search for a subnetwork — a version of the model"
2021.naacl-main.74,2020.emnlp-demos.6,1,0.817209,"Missing"
2021.naacl-main.74,W18-5448,0,0.0269045,"lark et al. (2019), Hewitt and Manning (2019), and Manning et al. (2020) found that BERT captures various properties of syntax. Tenney et al. (2019) probed the layers of BERT for an array of tasks, and they found that their localization mirrored the classical NLP pipeline (partof-speech, parsing, named entity recognition, semantic roles, coreference) in that lower-level tasks were captured in the lower layers. However, these results are difficult to interpret due to the use of a learned classifier. One line of work suggests comparing the probe accuracy to random baselines, e.g. random models (Zhang and Bowman, 2018) or random control tasks (Hewitt and Liang, 2019). Other works take an informationtheoretic view: Voita and Titov (2020) measure the complexity of the probe in terms of the bits needed to transmit its parameters, while Pimentel et al. (2020b) argue that probing should measure mutual information between the representation and the property. Pimentel et al. (2020a) propose a Pareto approach where they plot accuracy versus probe complexity, unifying several of these goals. We use these proposed metrics to compare our probing method to standard probing approaches. Subnetworks. While pruning is wide"
2021.naacl-main.74,2020.emnlp-main.174,0,0.0226034,"al information between the representation and the property. Pimentel et al. (2020a) propose a Pareto approach where they plot accuracy versus probe complexity, unifying several of these goals. We use these proposed metrics to compare our probing method to standard probing approaches. Subnetworks. While pruning is widely used for model compression, some works have explored pruning as a technique for learning as well. Mallya et al. (2018) found that a model trained on ImageNet could be used for new tasks by learning a binary mask over the weights. More recently, Radiya-Dixit and Wang (2020) and Zhao et al. (2020) showed the analogous result in NLP that weight pruning can be used as an alternative to finetuning for pre-trained models. Our paper seeks to use pruning to reveal what the model already captures, rather than learn new tasks. 3 weights masked, i.e. set to zero. We search for this subnetwork via supervised gradient descent on the head and a continuous relaxation of the mask. We also mask at several levels of granularity, including pruning weights, neurons, or layers. To learn the masks, we follow Louizos et al. (2017). Letting φ ∈ Rd denote the model weights, we associate the ith weight φi wit"
D10-1001,W08-2102,1,0.637361,"Missing"
D10-1001,D07-1101,0,0.0662954,", and simple additive updates to the Lagrange multipliers enforcing agreement between the two models. 4.2 Integrating Two Lexicalized Parsers Our second example problem is the integration of a phrase-structure parser with a higher-order dependency parser. The goal is to add higher-order features to phrase-structure parsing without greatly increasing the complexity of inference. First, we define an index set for second-order unlabeled projective dependency parsing. The secondorder parser considers first-order dependencies, as well as grandparent and sibling second-order dependencies (e.g., see Carreras (2007)). We assume that Idep is an index set containing all such dependencies (for brevity we omit the details of this index set). For convenience we define an extended index set that makes explicit use of first-order dependencies, I 0 dep = Idep ∪ Ifirst , where 5.1 Marginal Polytopes For a finite set Y, define the set of all distributions Ifirst = {(i, j) : i ∈ {0 . . . n}, j ∈ {1 . . . n}, i 6= j} |Y |: α ≥ over y P elements in Y as ∆ = {α ∈ R Here (i, j) represents a dependency with head wi 0, y∈Y αy = 1}. Each α ∈ ∆ gives a vector of P and modifier wj (i = 0 corresponds to the root sym- margina"
D10-1001,W02-1001,1,0.467871,"POS experiments. Each column gives the percentage of sentences whose exact solutions were found in a given range of subgradient iterations. ** is the percentage of sentences that did not converge by the iteration limit (K=50). 1),11 and the 2nd order discriminative dependency parser of Koo et al. (2008). The inference problem for a sentence x is to find Model 1 Koo08 Baseline DD Combination Precision 88.4 89.9 91.0 Recall 87.8 89.6 90.4 F1 88.1 89.7 90.7 Dep 91.4 93.3 93.8 Table 2: Performance results for Section 23 of the WSJ Treebank. Model 1: a reimplementation of the generative parser of (Collins, 2002). Koo08 Baseline: Model 1 with a hard restriction to dependencies predicted by the discriminative dependency parser of (Koo et al., 2008). DD Combination: a model that maximizes the joint score of the two parsers. Dep shows the unlabeled dependency accuracy of each system. 100 y = arg max (f1 (y) + γf2 (y)) y∈Y (11) where Y is the set of all lexicalized phrase-structure trees for the sentence x; f1 (y) is the score (log probability) under Model 1; f2 (y) is the score under Koo et al. (2008) for the dependency structure implied by y; and γ &gt; 0 is a parameter dictating the relative weight of the"
D10-1001,J03-4003,1,0.175967,"e; we use it in this paper. In our experiments we found that in the vast majority of cases, case 1 applies, after a small number of iterations; see the next section for more details. 7 Proposition 6.1 The algorithm in figure 1 is an instantiation of the algorithm in figure 4,8 with X1 = conv(Y), X2 = conv(Z), and the matrices E and F defined to be binary matrices specifying the constraints µ(i, t) = ν(i, t) for all (i, t) ∈ Iuni . 8 Recovering the LP Solution Experiments 7.1 Integrated Phrase-Structure and Dependency Parsing Our first set of experiments considers the integration of Model 1 of Collins (2003) (a lexicalized phrasestructure parser, from here on referred to as Model (k) (k) (k) (k) We have that θ1 · x1 + θ2 · x2 = L(u(k) , x1 , x2 ) = (k) (k) (k) L(u ), where the last equality is because x1 and x2 are de(k) (k) fined by the respective arg max’s. Thus, (x1 , x2 ) and u(k) are primal and dual optimal. 10 The resulting fractional solution can be projected back to the set Q, see (Smith and Eisner, 2008; Martins et al., 2009). 9 Itn. Dep POS 1 43.5 58.7 2 20.1 15.4 3 10.2 6.3 4 4.9 3.6 5-10 14.0 10.3 11-20 5.7 3.8 20-50 1.4 0.8 ** 0.4 1.1 Table 1: Convergence results for Section 23 of th"
D10-1001,J85-1006,0,0.942677,"ion of the original inference problem. • Empirically, the LP relaxation often leads to an exact solution to the original problem. Introduction Dynamic programming algorithms have been remarkably useful for inference in many NLP problems. Unfortunately, as models become more complex, for example through the addition of new features or components, dynamic programming algorithms can quickly explode in terms of computational or implementational complexity.1 As a result, efficiency of inference is a critical bottleneck for many problems in statistical NLP. This paper introduces dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) as a framework for deriving inference algorithms in NLP. Dual decomposition leverages the observation that complex inference problems can often be decomposed into efficiently solvable sub-problems. The approach leads to inference algorithms with the following properties: 1 The same is true for NLP inference algorithms based on other exact combinatorial methods, for example methods based on minimum-weight spanning trees (McDonald et al., 2005), or graph cuts (Pang and Lee, 2004). The approach is very general, and should be applicable to a wide range of problems in NLP."
D10-1001,P08-1068,1,0.250264,"tional solution can be projected back to the set Q, see (Smith and Eisner, 2008; Martins et al., 2009). 9 Itn. Dep POS 1 43.5 58.7 2 20.1 15.4 3 10.2 6.3 4 4.9 3.6 5-10 14.0 10.3 11-20 5.7 3.8 20-50 1.4 0.8 ** 0.4 1.1 Table 1: Convergence results for Section 23 of the WSJ Treebank for the dependency parsing and POS experiments. Each column gives the percentage of sentences whose exact solutions were found in a given range of subgradient iterations. ** is the percentage of sentences that did not converge by the iteration limit (K=50). 1),11 and the 2nd order discriminative dependency parser of Koo et al. (2008). The inference problem for a sentence x is to find Model 1 Koo08 Baseline DD Combination Precision 88.4 89.9 91.0 Recall 87.8 89.6 90.4 F1 88.1 89.7 90.7 Dep 91.4 93.3 93.8 Table 2: Performance results for Section 23 of the WSJ Treebank. Model 1: a reimplementation of the generative parser of (Collins, 2002). Koo08 Baseline: Model 1 with a hard restriction to dependencies predicted by the discriminative dependency parser of (Koo et al., 2008). DD Combination: a model that maximizes the joint score of the two parsers. Dep shows the unlabeled dependency accuracy of each system. 100 y = arg max"
D10-1001,D10-1125,1,0.627932,"Missing"
D10-1001,P09-1039,0,0.450406,"n loopy belief propagation (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008). Our approach similarly makes use of combinatorial algorithms to efficiently solve subproblems of the global inference problem. However, unlike LBP, our algorithms have strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. These guarantees are possible because our algorithms directly solve an LP relaxation. Other work has considered LP or integer linear programming (ILP) formulations of inference in NLP (Martins et al., 2009; Riedel and Clarke, 2006; Roth and Yih, 2005). These approaches typically use general-purpose LP or ILP solvers. Our method has the advantage that it leverages underlying structure arising in LP formulations of NLP problems. We will see that dynamic programming algorithms such as CKY can be considered to be very efficient solvers for particular LPs. In dual decomposition, these LPs—and their efficient solvers—can be embedded within larger LPs corresponding to more complex inference problems. 3 Background: Structured Models for NLP We now describe the type of models used throughout the paper."
D10-1001,H05-1066,0,0.312236,"ficiency of inference is a critical bottleneck for many problems in statistical NLP. This paper introduces dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) as a framework for deriving inference algorithms in NLP. Dual decomposition leverages the observation that complex inference problems can often be decomposed into efficiently solvable sub-problems. The approach leads to inference algorithms with the following properties: 1 The same is true for NLP inference algorithms based on other exact combinatorial methods, for example methods based on minimum-weight spanning trees (McDonald et al., 2005), or graph cuts (Pang and Lee, 2004). The approach is very general, and should be applicable to a wide range of problems in NLP. The connection to linear programming ensures that the algorithms provide a certificate of optimality when they recover the exact solution, and also opens up the possibility of methods that incrementally tighten the LP relaxation until it is exact (Sherali and Adams, 1994; Sontag et al., 2008). The structure of this paper is as follows. We first give two examples as an illustration of the approach: 1) integrated parsing and trigram part-ofspeech (POS) tagging; and 2)"
D10-1001,P04-1035,0,0.00645042,"tleneck for many problems in statistical NLP. This paper introduces dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) as a framework for deriving inference algorithms in NLP. Dual decomposition leverages the observation that complex inference problems can often be decomposed into efficiently solvable sub-problems. The approach leads to inference algorithms with the following properties: 1 The same is true for NLP inference algorithms based on other exact combinatorial methods, for example methods based on minimum-weight spanning trees (McDonald et al., 2005), or graph cuts (Pang and Lee, 2004). The approach is very general, and should be applicable to a wide range of problems in NLP. The connection to linear programming ensures that the algorithms provide a certificate of optimality when they recover the exact solution, and also opens up the possibility of methods that incrementally tighten the LP relaxation until it is exact (Sherali and Adams, 1994; Sontag et al., 2008). The structure of this paper is as follows. We first give two examples as an illustration of the approach: 1) integrated parsing and trigram part-ofspeech (POS) tagging; and 2) combined phrasestructure and depende"
D10-1001,W06-1616,0,0.42633,"tion (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008). Our approach similarly makes use of combinatorial algorithms to efficiently solve subproblems of the global inference problem. However, unlike LBP, our algorithms have strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. These guarantees are possible because our algorithms directly solve an LP relaxation. Other work has considered LP or integer linear programming (ILP) formulations of inference in NLP (Martins et al., 2009; Riedel and Clarke, 2006; Roth and Yih, 2005). These approaches typically use general-purpose LP or ILP solvers. Our method has the advantage that it leverages underlying structure arising in LP formulations of NLP problems. We will see that dynamic programming algorithms such as CKY can be considered to be very efficient solvers for particular LPs. In dual decomposition, these LPs—and their efficient solvers—can be embedded within larger LPs corresponding to more complex inference problems. 3 Background: Structured Models for NLP We now describe the type of models used throughout the paper. We take some care to set"
D10-1001,D08-1016,0,0.6547,"(Wainwright 2 et al., 2005a; Komodakis et al., 2007; Globerson and Jaakkola, 2007). In this approach, the MRF is decomposed into sub-problems corresponding to treestructured subgraphs that together cover all edges of the original graph. The resulting inference algorithms provably solve an LP relaxation of the MRF inference problem, often significantly faster than commercial LP solvers (Yanover et al., 2006). Our work is also related to methods that incorporate combinatorial solvers within loopy belief propagation (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008). Our approach similarly makes use of combinatorial algorithms to efficiently solve subproblems of the global inference problem. However, unlike LBP, our algorithms have strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. These guarantees are possible because our algorithms directly solve an LP relaxation. Other work has considered LP or integer linear programming (ILP) formulations of inference in NLP (Martins et al., 2009; Riedel and Clarke, 2006; Roth and Yih, 2005). These approaches typically use general-purpose LP or ILP solver"
D10-1001,W04-3201,0,0.0365589,"raints To make the connection to linear programming, in Eq. 6 specify that for each production of the form we first introduce the idea of marginal polytopes in 6 For any finite set Y, conv(Y) can be expressed as {µ ∈ section 5.1. In section 5.2, we give a precise statem R : Aµ ≤ b} where A is a matrix of dimension p × m, and ment of the LP relaxations that are being solved b ∈ Rp (see, e.g., Korte and Vygen (2008), pg. 65). The value by the example algorithms, making direct use of for p depends on the set Y, and can be exponential in size. 7 marginal polytopes. In section 6 we will prove that Taskar et al. (2004) describe the same set of constraints, but without proof of correctness or reference to Martin et al. (1990). the example algorithms solve these LP relaxations. 5 X ∀r ∈ I 0 , µr ≥ 0 ; X 0 ∀r ∈ Itag , νr ≥ 0 ; µ(X → Y Z, 1, k, n) = 1 (5) ν((X, Y ) → Z, 3) = 1 X,Y,Z∈T X,Y,Z∈N k=1...(n−1) ∀X ∈ N , ∀(i, j) such that 1 ≤ i &lt; j ≤ n and (i, j) 6= (1, n): X X µ(X → Y Z, i, k, j) = µ(Y → Z X, k, i − 1, j) Y,Z∈N Y,Z∈N k=i...(j−1) k=1...(i−1) X + µ(Y → X Z, i, j, k) (6) Y,Z∈N k=(j+1)...n X,Z∈N X,Z∈N k=1...(i−1) Y,Z∈T Y,Z∈T ∀X ∈ T , ∀i ∈ {3 . . . n − 2}: X X ν((Y, Z) → X, i) = ν((X, Y ) → Z, i + 2) Y,Z∈T"
D10-1001,W00-1308,0,0.0345805,"Missing"
D10-1001,J93-2004,0,\N,Missing
D10-1125,P96-1023,0,0.143953,"Missing"
D10-1125,W06-2920,0,0.536662,"lso related to recent work on training using outer bounds (see, e.g., (Taskar et al., 2003; Finley and Joachims, 2008; Kulesza and Pereira, 2008; Martins et al., 2009)). Note, however, that the LP relaxation optimized by dual decomposition is significantly tighter than Z. Thus, an alternative approach would be to use the dual decomposition algorithm for inference during training. 7 Experiments We report results on a number of data sets. For comparison to Martins et al. (2009), we perform experiments for Danish, Dutch, Portuguese, Slovene, Swedish and Turkish data from the CoNLL-X shared task (Buchholz and Marsi, 2006), and English data from the CoNLL-2008 shared task (Surdeanu et al., 2008). We use the official training/test splits for these data sets, and the same evaluation methodology as Martins et al. (2009). For comparison to Smith and Eisner (2008), we also report results on Danish and Dutch using their alternate training/test split. Finally, we report results on the English WSJ treebank, and the Prague treebank. We use feature sets that are very similar to those described in Carreras (2007). We use marginalbased pruning, using marginals calculated from an arc-factored spanning tree model using the m"
D10-1125,D07-1101,0,0.397968,"ments for Danish, Dutch, Portuguese, Slovene, Swedish and Turkish data from the CoNLL-X shared task (Buchholz and Marsi, 2006), and English data from the CoNLL-2008 shared task (Surdeanu et al., 2008). We use the official training/test splits for these data sets, and the same evaluation methodology as Martins et al. (2009). For comparison to Smith and Eisner (2008), we also report results on Danish and Dutch using their alternate training/test split. Finally, we report results on the English WSJ treebank, and the Prague treebank. We use feature sets that are very similar to those described in Carreras (2007). We use marginalbased pruning, using marginals calculated from an arc-factored spanning tree model using the matrixtree theorem (McDonald and Satta, 2007; Smith and Smith, 2007; Koo et al., 2007). In all of our experiments we set the value K, the maximum number of iterations of dual decomposition in Figures 1 and 2, to be 5,000. If the algorithm does not terminate—i.e., it does not return (y (k) , z (k) ) within 5,000 iterations—we simply take the parse y (k) with the maximum value of f (y (k) ) as the output from the algorithm. At first sight 5,000 might appear to be a large number, but deco"
D10-1125,W02-1001,1,0.273285,"some feature vector definition φ(x, y|i ). In the bigram sibling models in our experiments, we assume that φ(x, y|i ) = p+1 X k=1 φL (x, i, lk−1 , lk ) + q+1 X φR (x, i, rk−1 , rk ) k=1 where as before l1 . . . lp and r1 . . . rq are left and right modifiers under y|i , and where φL and φR are feature vector definitions. In the grandparent models in our experiments, we use a similar definition with feature vectors φL (x, i, k ∗ , lk−1 , lk ) and φR (x, i, k ∗ , rk−1 , rk ), where k ∗ is the parent for word i under y|i . We train the model using the averaged perceptron for structured problems (Collins, 2002). Given the i’th example in the training set, (x(i) , y (i) ), the perceptron updates are as follows: • z ∗ = argmaxy∈Z w · φ(x(i) , y) • If z ∗ 6= y (i) , w = w +φ(x(i) , y (i) )−φ(x(i) , z ∗ ) The first step involves inference over the set Z, rather than Y as would be standard in the perceptron. Thus, decoding during training can be achieved by dynamic programming over head automata alone, which is very efficient. Our training approach is closely related to local training methods (Punyakanok et al., 2005). We have found this method to be effective, very likely because Z is a superset of Y. O"
D10-1125,D07-1015,1,0.681145,"2008). We use the official training/test splits for these data sets, and the same evaluation methodology as Martins et al. (2009). For comparison to Smith and Eisner (2008), we also report results on Danish and Dutch using their alternate training/test split. Finally, we report results on the English WSJ treebank, and the Prague treebank. We use feature sets that are very similar to those described in Carreras (2007). We use marginalbased pruning, using marginals calculated from an arc-factored spanning tree model using the matrixtree theorem (McDonald and Satta, 2007; Smith and Smith, 2007; Koo et al., 2007). In all of our experiments we set the value K, the maximum number of iterations of dual decomposition in Figures 1 and 2, to be 5,000. If the algorithm does not terminate—i.e., it does not return (y (k) , z (k) ) within 5,000 iterations—we simply take the parse y (k) with the maximum value of f (y (k) ) as the output from the algorithm. At first sight 5,000 might appear to be a large number, but decoding is still fast—see Sections 7.3 and 7.4 for discussion.2 2 Note also that the feature vectors φ and inner products w ·φ 1294 The strategy for choosing step sizes αk is described in Appendix A,"
D10-1125,D08-1017,0,0.0390796,". Sib/G+S: Non-projective head automata with sibling or grandparent/sibling interactions, decoded via dual decomposition. Ma09: The best UAS of the LP/ILP-based parsers introduced in Martins et al. (2009). Sm08: The best UAS of any LBP-based parser in Smith and Eisner (2008). Mc06: The best UAS reported by McDonald and Pereira (2006). Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task (Buchholz and Marsi, 2006) or in any column of Martins et al. (2009, Table 1); note that the latter includes McDonald and Pereira (2006), Nivre and McDonald (2008), and Martins et al. (2008). CertS/CertG: Percent of test examples for which dual decomposition produced a certificate of optimality, for Sib/G+S. TimeS/TimeG: Seconds/sentence for test decoding, for Sib/G+S. TrainS/TrainG: Seconds/sentence during training, for Sib/G+S. For consistency of timing, test decoding was carried out on identical machines with zero additional load; however, training was conducted on machines with varying hardware and load. We ran two tests on the CoNLL-08 corpus. Eng1 : UAS when testing on the CoNLL-08 validation set, following Martins et al. (2009). Eng2 : UAS when testing on the CoNLL-08 test"
D10-1125,P09-1039,0,0.787813,"d formalisms such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 2000), and projective head automata. 2 Related Work McDonald et al. (2005) describe MST-based parsing for non-projective dependency parsing models with arc-factored decompositions; McDonald and Pereira (2006) make use of an approximate (hill-climbing) algorithm for parsing with more complex models. McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for nonprojective parsing, showing that parsing for a variety of models is NP-hard. Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al. (2009) recently introduced alternative LP and ILP formulations. Our algorithm differs in that we do not use general-purpose LP or ILP solvers, instead using an MST solver in combination with dynamic programming; thus we leverage the underlying structure of the problem, thereby deriving more efficient decoding algorithms. Both dual decomposition and Lagrangian relaxation have a long history in combinatorial optimization. Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007). However, the non-projective"
D10-1125,E06-1011,0,0.82283,"s most complex settings. The method compares favorably to previous work using LP/ILP formulations, both in terms of efficiency, and also in terms of the percentage of exact solutions returned. While the focus of the current paper is on nonprojective dependency parsing, the approach opens up new ways of thinking about parsing algorithms for lexicalized formalisms such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 2000), and projective head automata. 2 Related Work McDonald et al. (2005) describe MST-based parsing for non-projective dependency parsing models with arc-factored decompositions; McDonald and Pereira (2006) make use of an approximate (hill-climbing) algorithm for parsing with more complex models. McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for nonprojective parsing, showing that parsing for a variety of models is NP-hard. Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al. (2009) recently introduced alternative LP and ILP formulations. Our algorithm differs in that we do not use general-purpose LP or ILP solvers, instead using an MST solver in combination with dynamic programming; thus we leverage the underlying structure of the"
D10-1125,W07-2216,0,0.11176,"for arc-factored models can be accomplished using directed minimum-weight spanning tree (MST) algorithms. The resulting parsing algorithms have the following properties: • They are efficient and easy to implement, relying on standard dynamic programming and MST algorithms. • They provably solve a linear programming (LP) relaxation of the original decoding problem. Introduction Non-projective dependency parsing is useful for many languages that exhibit non-projective syntactic structures. Unfortunately, the non-projective parsing problem is known to be NP-hard for all but the simplest models (McDonald and Satta, 2007). There has been a long history in combinatorial optimization of methods that exploit structure in complex problems, using methods such as dual decomposition or Lagrangian relaxation (Lemar´echal, 2001). Thus far, however, these methods are not widely used in NLP. This paper introduces algorithms for nonprojective parsing based on dual decomposition. We focus on parsing algorithms for non-projective head automata, a generalization of the head-automata models of Eisner (2000) and Alshawi (1996) to nonprojective structures. These models include nonprojective dependency parsing models with higher"
D10-1125,H05-1066,0,0.845802,"Missing"
D10-1125,P08-1108,0,0.0583026,"k. MST: Our firstorder baseline. Sib/G+S: Non-projective head automata with sibling or grandparent/sibling interactions, decoded via dual decomposition. Ma09: The best UAS of the LP/ILP-based parsers introduced in Martins et al. (2009). Sm08: The best UAS of any LBP-based parser in Smith and Eisner (2008). Mc06: The best UAS reported by McDonald and Pereira (2006). Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task (Buchholz and Marsi, 2006) or in any column of Martins et al. (2009, Table 1); note that the latter includes McDonald and Pereira (2006), Nivre and McDonald (2008), and Martins et al. (2008). CertS/CertG: Percent of test examples for which dual decomposition produced a certificate of optimality, for Sib/G+S. TimeS/TimeG: Seconds/sentence for test decoding, for Sib/G+S. TrainS/TrainG: Seconds/sentence during training, for Sib/G+S. For consistency of timing, test decoding was carried out on identical machines with zero additional load; however, training was conducted on machines with varying hardware and load. We ran two tests on the CoNLL-08 corpus. Eng1 : UAS when testing on the CoNLL-08 validation set, following Martins et al. (2009). Eng2 : UAS when t"
D10-1125,W06-1616,0,0.411692,"up new ways of thinking about parsing algorithms for lexicalized formalisms such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 2000), and projective head automata. 2 Related Work McDonald et al. (2005) describe MST-based parsing for non-projective dependency parsing models with arc-factored decompositions; McDonald and Pereira (2006) make use of an approximate (hill-climbing) algorithm for parsing with more complex models. McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for nonprojective parsing, showing that parsing for a variety of models is NP-hard. Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al. (2009) recently introduced alternative LP and ILP formulations. Our algorithm differs in that we do not use general-purpose LP or ILP solvers, instead using an MST solver in combination with dynamic programming; thus we leverage the underlying structure of the problem, thereby deriving more efficient decoding algorithms. Both dual decomposition and Lagrangian relaxation have a long history in combinatorial optimization. Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al."
D10-1125,D10-1001,1,0.539821,"els, and the decomposition we use is very different in nature from those used in graphical models. Other work has made extensive use of decomposition approaches for efficiently solving LP relaxations for graphical models (e.g., Sontag et al. (2008)). Methods that incorporate combinatorial solvers within loopy belief propagation (LBP) (Duchi et al., 2007; Smith and Eisner, 2008) are also closely related to our approach. Unlike LBP, our method has strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. 1289 Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems. 3 Sibling Models This section describes a particular class of models, sibling models; the next section describes a dualdecomposition algorithm for decoding these models. Consider the dependency parsing problem for a sentence with n words. We define the index set for dependency parsing to be I = {(i, j) : i ∈ {0 . . . n}, j ∈ {1 . . . n}, i 6= j}. A dependency parse is a vector y = {y(i, j) : (i, j) ∈ I}, where y(i, j) = 1 if a dependency with head word i and modifier j is in the parse, 0 otherwise. We use i = 0 for the root symbol"
D10-1125,D08-1016,0,0.904744,"ur work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007). However, the non-projective parsing problem has a very different structure from these models, and the decomposition we use is very different in nature from those used in graphical models. Other work has made extensive use of decomposition approaches for efficiently solving LP relaxations for graphical models (e.g., Sontag et al. (2008)). Methods that incorporate combinatorial solvers within loopy belief propagation (LBP) (Duchi et al., 2007; Smith and Eisner, 2008) are also closely related to our approach. Unlike LBP, our method has strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. 1289 Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems. 3 Sibling Models This section describes a particular class of models, sibling models; the next section describes a dualdecomposition algorithm for decoding these models. Consider the dependency parsing problem for a sentence with n words. We define the index set for dependency parsing to be I = {("
D10-1125,D07-1014,0,0.0291793,"task (Surdeanu et al., 2008). We use the official training/test splits for these data sets, and the same evaluation methodology as Martins et al. (2009). For comparison to Smith and Eisner (2008), we also report results on Danish and Dutch using their alternate training/test split. Finally, we report results on the English WSJ treebank, and the Prague treebank. We use feature sets that are very similar to those described in Carreras (2007). We use marginalbased pruning, using marginals calculated from an arc-factored spanning tree model using the matrixtree theorem (McDonald and Satta, 2007; Smith and Smith, 2007; Koo et al., 2007). In all of our experiments we set the value K, the maximum number of iterations of dual decomposition in Figures 1 and 2, to be 5,000. If the algorithm does not terminate—i.e., it does not return (y (k) , z (k) ) within 5,000 iterations—we simply take the parse y (k) with the maximum value of f (y (k) ) as the output from the algorithm. At first sight 5,000 might appear to be a large number, but decoding is still fast—see Sections 7.3 and 7.4 for discussion.2 2 Note also that the feature vectors φ and inner products w ·φ 1294 The strategy for choosing step sizes αk is descr"
D10-1125,W08-2121,0,\N,Missing
D12-1131,W06-2920,0,0.155602,"e WSJ PennTreebank (Marcus et al., 1993) and the QuestionBank (QTB) (Judge et al., 2006). In the WSJ → QTB scenario, we train on sections 2-21 of the WSJ and test on the entire QTB (4000 questions). In the QTB → WSJ scenario, we train on the entire QTB and test on section 23 of the WSJ. Data for Lightly Supervised Training For all English experiments, our data was taken from the WSJ PennTreebank: training sentences from Section 0, development sentences from Section 22, and test sentences from Section 23. For experiments in Bulgarian, German, Japanese, and Spanish, we use the CONLL-X data set (Buchholz and Marsi, 2006) with training data taken from the official training files. We trained the sentence-level models with 50-500 sentences. To verify the robustness of our results, our test sets consist of the official test sets augmented with additional sentences from the official training files such that each test file consists of 25,000 words. Our results on the official test sets are very similar to the results we report and are omitted for brevity. Parameters The model parameters, δ1 , δ2 , and δ3 of the scoring function (Section 4) and α of the Lagrange multipliers update rule (Section 6), were tuned on the"
D12-1131,P04-1056,0,0.0717296,"formation with sentence-level algorithms have been applied to a number of NLP tasks. The most similar models to our work are skip-chain CRFs (Sutton and Mccallum, 2004), relational markov networks (Taskar et al., 2002), and collective inference with symmetric clique potentials (Gupta et al., 2010). These models use a linear-chain CRF or MRF objective modified by potentials defined over pairs of nodes or clique templates. The latter model makes use of Lagrangian relaxation. Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). Finkel et al. (2005) also integrated non-local information into entity annotation algorithms using Gibbs sampling. Our model can be applied to a variety of off-theshelf structured prediction models. In particular, we focus on dependency parsing which is characterized by a more complicated structure compared to the IE tasks addressed by previous work. Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (Ganchev et al., 2010; Gillenwater et al., 2010), generalized expectation (Mann and McCallum, 2007; Mann a"
D12-1131,D10-1003,0,0.0232755,"ere used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic structures – have been demonstrated in human language corpora (e.g. WSJ and Brown) (Dubey et al., 2009) and were shown useful in generative and discriminative parsers (e.g. (Cheung and Penn, 2010)). We complement these works, which focus on consistency between consecutive sentences, and explore corpus level consistency. 3 where u is a vector in R|I(x) |. In practice, u will be a vector of Lagrange multipliers associated with the dependencies of y in our dual decomposition algorithm given in Section 6. We can construct a very similar setting for POS tagging where the goal is to find the best tagging y for a sentence x = (w1 , . . . , wn ). We skip the formal details here. We next introduce notation for Markov random fields (MRFs) (Koller and Friedman, 2009). An MRF consists of an undire"
D12-1131,W03-0407,0,0.0205689,"Missing"
D12-1131,N09-1068,0,0.00939211,"in adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic st"
D12-1131,P05-1045,0,0.0245567,"el algorithms have been applied to a number of NLP tasks. The most similar models to our work are skip-chain CRFs (Sutton and Mccallum, 2004), relational markov networks (Taskar et al., 2002), and collective inference with symmetric clique potentials (Gupta et al., 2010). These models use a linear-chain CRF or MRF objective modified by potentials defined over pairs of nodes or clique templates. The latter model makes use of Lagrangian relaxation. Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). Finkel et al. (2005) also integrated non-local information into entity annotation algorithms using Gibbs sampling. Our model can be applied to a variety of off-theshelf structured prediction models. In particular, we focus on dependency parsing which is characterized by a more complicated structure compared to the IE tasks addressed by previous work. Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (Ganchev et al., 2010; Gillenwater et al., 2010), generalized expectation (Mann and McCallum, 2007; Mann and McCallum, ), and Ba"
D12-1131,P10-2036,0,0.0269225,"Missing"
D12-1131,J04-3001,0,0.022584,"ised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, how"
D12-1131,P06-1063,0,0.110247,"Missing"
D12-1131,D10-1125,1,0.938178,"S tagging and parsing. The constraints used by these works differ from ours in that they encourage the posterior label distribution to have desired properties such as sparsity (e.g. a given word can take a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. (2005), Komodakis et al. (2007), and Globerson and Jaakkola (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Bel"
D12-1131,I05-1006,0,0.05248,"(Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistic"
D12-1131,J93-2004,0,0.053186,"Missing"
D12-1131,P08-2026,0,0.0106814,"manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper."
D12-1131,P06-1043,0,0.055918,"and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a majo"
D12-1131,N10-1004,0,0.0836437,"s (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic pr"
D12-1131,H05-1066,0,0.802359,"ncy parse is a vector y = {y(m, h) : (m, h) ∈ I(x)} where y(m, h) = 1 if m is a modifier of the head word h. We define the set Y(x) ⊂ {0, 1}|I(x)| to be the set of all valid dependency parses for a sentence x. In this work, we use projective dependency parses, but the method also applies to the set of nonprojective parse trees. Additionally, we have a scoring function f : Y(x) → R. The optimal parse y ∗ for a sentence x is given by, y ∗ = arg maxy∈Y(x) f (y). This sentencelevel decoding problem can often be solved efficiently. For example in commonly used projective dependency parsing models (McDonald et al., 2005), we can compute y ∗ efficiently using variants of the Viterbi algorithm. For this work, we make the assumption that we have an efficient algorithm to find the argmax of X f (y) + u(m, h)y(m, h) = f (y) + u · y (m,h)∈I(x) 1436 {((i, j), li , lj ) : (i, j) ∈ E, li ∈ Li , lj ∈ Lj } A label assignment in the MRF is a binary vector z with z(i, l) = 1 if the label l is selected at node i and z((i, j), li , lj ) = 1 if the labels li , lj are selected for the nodes i, j. In applications such as parsing and POS tagging, some of the label assignments are not allowed. For example, in dependency parsing"
D12-1131,P07-1078,1,0.381082,"ce-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used b"
D12-1131,D08-1050,0,0.0198208,"06) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phe"
D12-1131,D10-1001,1,0.955163,"and semi-supervised POS tagging and parsing. The constraints used by these works differ from ours in that they encourage the posterior label distribution to have desired properties such as sparsity (e.g. a given word can take a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. (2005), Komodakis et al. (2007), and Globerson and Jaakkola (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifo"
D12-1131,D07-1111,0,0.0147533,"aper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic structures – have been demonstrated in huma"
D12-1131,D08-1016,0,0.0219949,"ake a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. (2005), Komodakis et al. (2007), and Globerson and Jaakkola (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the"
D12-1131,E03-1008,0,0.364995,"on. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS ta"
D12-1131,D10-1017,0,0.0145414,"co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic structures – have been demonstrated in human language corpora (e.g. WSJ and Brown) (Dubey et al., 2009) and were shown useful in generative and discriminative parsers (e.g. (Cheung and Penn, 2010)). We compleme"
D12-1131,N03-1033,0,0.0205525,"Missing"
D13-1022,D11-1003,1,0.428041,"us the language model score of the target sentence f (p) = n X i=1 |u|+1 ω(q(pi ), r(pi )) + X σ(ui−1 , ui ) i=0 where u is the sequence of words in Σ formed by concatenating the phrases r(p1 ) . . . r(pn ), with boundary cases u0 = &lt;s&gt; and u|u|+1 = &lt;/s&gt;. 212 Crucially for a derivation to be valid it must satisfy an additional condition: it must translate every source word exactly once. The decoding problem for phrase-based translation is to find the highestscoring derivation satisfying this property. We can represent this decoding problem as a constrained hypergraph using the construction of Chang and Collins (2011). The hypergraph weights encode the translation and language model scores, and its structure ensures that the count of source words translated is |w|, i.e. the length of the source sentence. Each vertex will remember the preceding target-language word and the count of source words translated so far. The hypergraph, which for this problem is also a directed graph, takes the following form. • Vertices v ∈ V are labeled (c, u) where c ∈ {1 . . . |w|} is the count of source words translated and u ∈ Σ is the last target-language word produced by a partial hypothesis at this vertex. Additionally the"
D13-1022,J10-3008,0,0.0412784,"Missing"
D13-1022,P10-4002,0,0.063735,"Missing"
D13-1022,P01-1030,0,0.0773916,"dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with limited reordering (Kumar and Byrne, 2005). Exact decoding based on FST is also feasible for certain hierarchical grammars (de Gispert et al., 2010). Chang 217 procedure O PT B EAM S TAGED(α, β) λ, ub, opt ←L AGRANGIAN R ELAXATION (α) if opt then return ub θ"
D13-1022,W05-1506,0,0.0400969,"m invokes the black-box function, P RUNE, on line 13, passing it a pruning parameter β and a vertex-signature pair. The parameter β controls a threshold for pruning. For instance for phrase-based translation, it specifies a hard-limit on the number of hypotheses to retain. The function returns true if it prunes from the chart. Note that pruning may remove optimal hypotheses, so we set the certificate flag opt to false if the chart is modified. 2 For simplicity we write this loop over the entire set. In practice it is important to use data structures to optimize lookup. See Tillmann (2006) and Huang and Chiang (2005). 214 1: procedure B EAM S EARCH(θ, τ, lb, β) 2: ubs ← O UTSIDE(θ, τ ) 3: opt ← true 4: π[v, sig] ← −∞ for all v ∈ V, sig ∈ R|b| 5: π[v, 0] ← 0 for all v ∈ T 6: for e ∈ E in topological order do 7: hhv2 , . . . , v|v |i, v1 i ← e 8: for sig (2) . . . sig (|v|) ∈ S IGS(v2 , . . . , v|v |) do 9: sig ← Aδ(e) + |v| X sig (i) i=2 |v| X s ← θ(e) + π[vi , sig (i) ] i=2   s &gt; π[v1 , sig] ∧ 11: if  C HECK(sig) ∧  then s + ubs[v1 ] ≥ lb 12: π[v1 , sig] ← s 13: if P RUNE(π, v1 , sig, β) then opt ← false 0 14: lb ← π[1, c] + τ 0 15: return  lb , opt (V, E, θ, τ ) hypergraph with weights  (A, b) matr"
D13-1022,P07-1019,0,0.0433886,"6. In each round, the algorithm alternates between computing subgradients to tighten ubs and running beam search to maximize lb. In early rounds we set β for aggressive beam pruning, and as the upper bounds get tighter, we loosen pruning to try to get a certificate. If at any point either a primal or dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) ha"
D13-1022,D10-1027,0,0.12585,"ween computing subgradients to tighten ubs and running beam search to maximize lb. In early rounds we set β for aggressive beam pruning, and as the upper bounds get tighter, we loosen pruning to try to get a certificate. If at any point either a primal or dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with"
D13-1022,E09-1044,0,0.0470736,"Missing"
D13-1022,J99-4005,0,0.0468421,"peredge constraints. Define the set of constrained hyperpaths as X 0 = {x ∈ X : Ax = b} 1 The purpose of the offset will be clear in later sections. For this section, the value of τ can be taken as 0. where we have a constraint matrix A ∈ R|b|×|E| and vector b ∈ R|b |encoding |b |constraints. The optimal constrained hyperpath is x∗ = arg maxx∈X 0 θ&gt; x + τ . Note that the constrained hypergraph search problem may be NP-Hard. Crucially this is true even when the corresponding unconstrained search problem is solvable in polynomial time. For instance, phrase-based decoding is known to be NP-Hard (Knight, 1999), but we will see that it can be expressed as a polynomial-sized hypergraph with constraints. Example: Phrase-Based Machine Translation Consider translating a source sentence w1 . . . w|w |to a target sentence in a language with vocabulary Σ. A simple phrase-based translation model consists of a tuple (P, ω, σ) with • P; a set of pairs (q, r) where q1 . . . q|q |is a sequence of source-language words and r1 . . . r|r| is a sequence of target-language words drawn from the target vocabulary Σ. • ω : R|P |; parameters for the translation model mapping each pair in P to a real-valued score. • σ :"
D13-1022,N03-1017,0,0.0690447,"exact solution on the majority of translation examples in our test data. The algorithm is 3.5 times faster than an optimized incremental constraint-based decoder for phrase-based translation and 4 times faster for syntax-based translation. 1 Michael Collins Department of Computer Science, Columbia University, New York, NY 10027, USA mcollins@cs.columbia.edu • It utilizes well-studied algorithms and extends off-the-shelf beam search decoders. • Empirically it is very fast, results show that it is 3.5 times faster than an optimized incremental constraint-based solver. Introduction Beam search (Koehn et al., 2003) and cube pruning (Chiang, 2007) have become the de facto decoding algorithms for phrase- and syntax-based translation. The algorithms are central to large-scale machine translation systems due to their efficiency and tendency to produce high-quality translations (Koehn, 2004; Koehn et al., 2007; Dyer et al., 2010). However despite practical effectiveness, neither algorithm provides any bound on possible decoding error. In this work we present a variant of beam search decoding for phrase- and syntax-based translation. The motivation is to exploit the effectiveness and efficiency of beam search"
D13-1022,koen-2004-pharaoh,0,0.0241844,"Missing"
D13-1022,H05-1021,0,0.0704157,"work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with limited reordering (Kumar and Byrne, 2005). Exact decoding based on FST is also feasible for certain hierarchical grammars (de Gispert et al., 2010). Chang 217 procedure O PT B EAM S TAGED(α, β) λ, ub, opt ←L AGRANGIAN R ELAXATION (α) if opt then return ub θ 0 ← θ − A&gt; λ τ 0 ← τ + λ&gt; b lb(0) ← −∞ for k in 1 . . . K do lb(k) , opt ← B EAM S EARCH(θ0 , τ 0 , lb(k−1) , βk ) if opt then return lb(k) return maxk∈{1...K} lb(k) procedure O PT B EAM(α, β) λ(0) ← 0 lb(0) ← −∞ for k in 1 . . . K do λ(k) , ub(k) , opt ← LRROUND(αk , λ(k−1) ) if opt then return ub(k) θ0 ← θ − A&gt; λ(k) τ 0 ← τ + λ(k)&gt; b lb(k) , opt ← B EAM S EARCH(θ0 , τ 0 , lb(k−1"
D13-1022,W01-1408,0,0.0463385,"oximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with limited reordering (Kumar and Byrne, 2005). Exact decoding based on FST is also feasible for certain hierarchical grammars (de Gispert et al., 2010). Chang 217 procedure O PT B EAM S TAGED(α, β) λ, ub, opt ←L AGRANGIAN R ELAXATION (α) if opt then return ub θ 0 ← θ − A&gt; λ τ 0 ← τ + λ&gt; b lb(0) ← −∞ for k in 1 . . . K do lb(k) , opt ← B EAM S EA"
D13-1022,N09-2002,0,0.0231604,"hm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with limited reordering (Kumar and Byrne, 2005). Exact decoding based on FST is also feasible for certain hierarchical grammars (de Gispert et al., 2010). Chang 217 procedure O PT B EAM S TAGED(α, β) λ, ub, opt ←L AGRANGIAN R ELAXATION (α) if opt then return ub θ 0 ← θ − A&gt; λ τ 0 ← τ + λ&gt; b lb(0) ← −∞ f"
D13-1022,D12-1067,0,0.114093,"l beam search: staged and alternating. Staged runs Lagrangian relaxation to find the optimal λ, uses λ to compute upper bounds, and then repeatedly runs beam search with pruning sequence β1 . . . βk . Alternating switches between running a round of Lagrangian relaxation and a round of beam search with the updated λ. If either produces a certificate it returns the result. and Collins (2011) and Rush and Collins (2011) develop Lagrangian relaxation-based approaches for exact machine translation. Apart from translation decoding, this paper is closely related to work on column generation for NLP. Riedel et al. (2012) and Belanger et al. (2012) relate column generation to beam search and produce exact solutions for parsing and tagging problems. The latter work also gives conditions for when beam search-style decoding is optimal. 7 Results To evaluate the effectiveness of optimal beam search for translation decoding, we implemented decoders for phrase- and syntax-based models. In this section we compare the speed and optimality of these decoders to several baseline methods. 7.1 Setup and Implementation For phrase-based translation we used a German-toEnglish data set taken from Europarl (Koehn, 2005). We tes"
D13-1022,P11-1008,1,0.32464,"{x ∈ X : Ax = 1 }, and the best derivation under this phrase-based translation model has score maxx∈X 0 θ&gt; x + τ . Figure 2.2 shows an example hypergraph with constraints for translating the sentence les pauvres sont demunis into English using a simple set of phrases. Even in this small example, many of the possible hyperpaths violate the constraints and correspond to invalid derivations. Example: Syntax-Based Machine Translation Syntax-based machine translation with a language model can also be expressed as a constrained hypergraph problem. For the sake of space, we omit the definition. See Rush and Collins (2011) for an indepth description of the constraint matrix used for syntax-based translation. 213 3 A Variant of Beam Search This section describes a variant of the beam search algorithm for finding the highest-scoring constrained hyperpath. The algorithm uses three main techniques: (1) dynamic programming with additional signature information to satisfy the constraints, (2) beam pruning where some, possibly optimal, hypotheses are discarded, and (3) branch-andbound-style application of upper and lower bounds to discard provably non-optimal hypotheses. Any solution returned by the algorithm will be"
D13-1022,J03-1005,0,0.0103242,"improve the lower bound lb. This motivates the alternating algorithm O PTB EAM shown Figure 6. In each round, the algorithm alternates between computing subgradients to tighten ubs and running beam search to maximize lb. In early rounds we set β for aggressive beam pruning, and as the upper bounds get tighter, we loosen pruning to try to get a certificate. If at any point either a primal or dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. E"
D13-1022,W06-3602,0,0.072549,"checks. The algorithm invokes the black-box function, P RUNE, on line 13, passing it a pruning parameter β and a vertex-signature pair. The parameter β controls a threshold for pruning. For instance for phrase-based translation, it specifies a hard-limit on the number of hypotheses to retain. The function returns true if it prunes from the chart. Note that pruning may remove optimal hypotheses, so we set the certificate flag opt to false if the chart is modified. 2 For simplicity we write this loop over the entire set. In practice it is important to use data structures to optimize lookup. See Tillmann (2006) and Huang and Chiang (2005). 214 1: procedure B EAM S EARCH(θ, τ, lb, β) 2: ubs ← O UTSIDE(θ, τ ) 3: opt ← true 4: π[v, sig] ← −∞ for all v ∈ V, sig ∈ R|b| 5: π[v, 0] ← 0 for all v ∈ T 6: for e ∈ E in topological order do 7: hhv2 , . . . , v|v |i, v1 i ← e 8: for sig (2) . . . sig (|v|) ∈ S IGS(v2 , . . . , v|v |) do 9: sig ← Aδ(e) + |v| X sig (i) i=2 |v| X s ← θ(e) + π[vi , sig (i) ] i=2   s &gt; π[v1 , sig] ∧ 11: if  C HECK(sig) ∧  then s + ubs[v1 ] ≥ lb 12: π[v1 , sig] ← s 13: if P RUNE(π, v1 , sig, β) then opt ← false 0 14: lb ← π[1, c] + τ 0 15: return  lb , opt (V, E, θ, τ ) hypergrap"
D13-1022,P06-1098,0,0.0269831,"lgorithm alternates between computing subgradients to tighten ubs and running beam search to maximize lb. In early rounds we set β for aggressive beam pruning, and as the upper bounds get tighter, we loosen pruning to try to get a certificate. If at any point either a primal or dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phra"
D13-1022,P09-1038,0,0.0179732,"ed on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with limited reordering (Kumar and Byrne, 2005). Exact decoding based on FST is also feasible for certain hierarchical grammars (de Gispert et al., 2010). Chang 217 procedure O PT B EAM S TAGED(α, β) λ, ub, opt ←L AGRANGIAN R ELAXATION (α) if opt then return ub θ 0 ← θ − A&gt; λ τ 0 ← τ + λ&gt; b lb(0) ← −∞ for k in 1 . . . K do lb(k) , opt ← B EAM S EARCH(θ0 , τ 0 , lb(k−1) , β"
D13-1022,P07-2045,0,\N,Missing
D13-1022,J07-2003,0,\N,Missing
D15-1044,D13-1176,0,0.429435,"eural model, we fix θ and tune the α parameters. We follow the statistical machine translation setup and use minimumerror rate training (MERT) to tune for the summarization metric on tuning data (Och, 2003). This tuning step is also identical to the one used for the phrase-based machine translation baseline. 6 Neural MT This work is closely related to recent work on neural network language models (NNLM) and to work on neural machine translation. The core of our model is a NNLM based on that of Bengio et al. (2003). Recently, there have been several papers about models for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). Of these our model is most closely related to the attention-based model of Bahdanau et al. (2014), which explicitly finds a soft alignment between the current position and the input source. Most of these models utilize recurrent neural networks (RNNs) for generation as opposed to feedforward models. We hope to incorporate an RNNLM in future work. Related Work Abstractive sentence summarization has been traditionally connected to the task of headline generation. Our work is similar to early work of Banko et al. (2000) who developed a statistical mach"
D15-1044,P00-1041,0,0.8743,"beddings, and h is a hidden layer of size H. The black-box function enc is a contextual encoder term that returns a vector of size H representing the input and current context; we consider several possible variants, described subsequently. Figure 3a gives a schematic representation of the decoder architecture. Model The distribution of interest, p(yi+1 |x, yc ; θ), is a conditional language model based on the input sentence x. Past work on summarization and compression has used a noisy-channel approach to split and independently estimate a language model and a conditional summarization model (Banko et al., 2000; Knight and Marcu, 2002; Daum´e III and Marcu, 2002), i.e., 3.2 Encoders Note that without the encoder term this represents a standard language model. By incorporating in enc and training the two elements jointly we crucially can incorporate the input text into generation. We discuss next several possible instantiations of the encoder. arg max log p(y|x) = arg max log p(y)p(x|y) y where p(y) and p(x|y) are estimated separately. Here we instead follow work in neural machine translation and directly parameterize the original distribution as a neural network. The network contains both a neural p"
D15-1044,P07-2045,0,0.0480613,"e original sentence along with a language model trained on the headline data to produce a compressed output. The syntax and language model are combined with a set of linguistic constraints and decoding is performed with an ILP solver. To control for memorizing titles from training, we implement an information retrieval baseline, IR. This baseline indexes the training set, and gives the title for the article with highest BM-25 match to the input (see Manning et al. (2008)). Finally, we use a phrase-based statistical machine translation system trained on Gigaword to produce summaries, M OSES + (Koehn et al., 2007). To improve the baseline for this task, we augment the phrase table with “deletion” rules mapping each article word to , include an additional deletion feature for these rules, and allow for an infinite distortion limit. We also explicitly tune the model using MERT to target the 75byte capped ROUGE score as opposed to standard In addition to the standard DUC-2014 evaluation, we also report evaluation on single reference headline-generation using a randomly heldout subset of Gigaword. This evaluation is closer to the task the model is trained for, and it allows us to use a bigger evaluation s"
D15-1044,W04-1013,0,0.302425,"es and Associated Press Wire services each paired with 4 different human-generated reference summaries (not actually headlines), capped at 75 bytes. This data set is evaluation-only, although the similarly sized DUC-2003 data set was made available for the task. The expectation is for a summary of roughly 14 words, based on the text of a complete article (although we only make use of the first sentence). The full data set is available by request at http://duc.nist.gov/data.html. For this shared task, systems were entered and evaluated using several variants of the recalloriented ROUGE metric (Lin, 2004). To make recall-only evaluation unbiased to length, output of all systems is cut-off after 75-characters and no bonus is given for shorter summaries. 384 headline and the input; although only 2.6 in the first 75-characters of the input. Unlike BLEU which interpolates various n-gram matches, there are several versions of ROUGE for different match lengths. The DUC evaluation uses ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest-common substring), all of which we report. 7.2 Baselines Due to the variety of approaches to the sentence summarization problem, we report a broad set of head"
D15-1044,C08-1018,0,0.43679,"not appear as part of the original. We focus on the task of sentence-level summarization. While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002). Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and Lapata, 2008; Woodsend et al., 2010). These approaches are described in more detail in Section 6. We instead explore a fully data-driven approach for generating abstractive summaries. Inspired by the recent success of neural machine translation, we combine a neural language model with a contextual input encoder. Our encoder is modeled off of the attention-based encoder of Bahdanau et al. (2014) in that it learns a latent soft alignment over the input text to help inform the summary (as shown in Figure 1). Crucially both the encoder and the generation model are trained jointly on the sentence summarization"
D15-1044,P14-5010,0,0.0191357,"n addition to the standard DUC-2014 evaluation, we also report evaluation on single reference headline-generation using a randomly heldout subset of Gigaword. This evaluation is closer to the task the model is trained for, and it allows us to use a bigger evaluation set, which we will include in our code release. For this evaluation, we tune systems to generate output of the average title length. For training data for both tasks, we utilize the annotated Gigaword data set (Graff et al., 2003; Napoles et al., 2012), which consists of standard Gigaword, preprocessed with Stanford CoreNLP tools (Manning et al., 2014). Our model only uses annotations for tokenization and sentence separation, although several of the baselines use parsing and tagging as well. Gigaword contains around 9.5 million news articles sourced from various domestic and international news services over the last two decades. For our training set, we pair the headline of each article with its first sentence to create an inputsummary pair. While the model could in theory be trained on any pair, Gigaword contains many spurious headline-article pairs. We therefore prune training based on the following heuristic filters: (1) Are there no non"
D15-1044,P02-1057,0,0.0280446,"Missing"
D15-1044,W03-0501,0,0.894234,"st, abstractive summarization attempts to produce a bottom-up summary, aspects of which may not appear as part of the original. We focus on the task of sentence-level summarization. While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002). Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and Lapata, 2008; Woodsend et al., 2010). These approaches are described in more detail in Section 6. We instead explore a fully data-driven approach for generating abstractive summaries. Inspired by the recent success of neural machine translation, we combine a neural language model with a contextual input encoder. Our encoder is modeled off of the attention-based encoder of Bahdanau et al. (2014) in that it learns a latent soft alignment over the input text to help inform the summary (as shown in Figure 1). Cruci"
D15-1044,W12-3018,0,0.146461,"Missing"
D15-1044,D13-1155,0,0.197713,"generation and allows it to fit with a wider range of training data. In this work we focus on factored scoring functions, s, that take into account a fixed window of previous words: s(x, y) ≈ N −1 X g(yi+1 , x, yc ), (4) i=0 2 For the DUC-2004 evaluation, it is actually the number of bytes of the output that is capped. More detail is given in Section 7. 3 Unfortunately the literature is inconsistent on the formal definition of this distinction. Some systems self-described as abstractive would be extractive under our definition. 1 In contrast to a large-scale sentence compression systems like Filippova and Altun (2013) which require monotonic aligned compressions. 380 where we define yc , y[i−C+1,...,i] for a window of size C. In particular consider the conditional logprobability of a summary given the input, s(x, y) = log p(y|x; θ). We can write this as: log p(y|x; θ) ≈ N −1 X log p(yi+1 |x, yc ; θ), x P U y ˜c x ˜ y ˜c0 E yc F x G yc (b) (a) Figure 3: (a) A network diagram for the NNLM decoder with additional encoder element. (b) A network diagram for the attention-based encoder enc3 . The parameters are θ = (E, U, V, W) where E ∈ RD×V is a word embedding matrix, U ∈ R(CD)×H , V ∈ RV ×H , W ∈ RV ×H are we"
D15-1044,P03-1021,0,0.0445649,"ors of unigram, bigram, and trigram match with the input as well as reordering of input words. Note that setting α = h1, 0, . . . , 0i gives a model identical to standard A BS. 383 pression, the sentences are transformed by a series of heuristics such that the words are in monotonic alignment. Our system does not require this alignment step but instead uses the text directly. After training the main neural model, we fix θ and tune the α parameters. We follow the statistical machine translation setup and use minimumerror rate training (MERT) to tune for the summarization metric on tuning data (Och, 2003). This tuning step is also identical to the one used for the phrase-based machine translation baseline. 6 Neural MT This work is closely related to recent work on neural network language models (NNLM) and to work on neural machine translation. The core of our model is a NNLM based on that of Bengio et al. (2003). Recently, there have been several papers about models for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). Of these our model is most closely related to the attention-based model of Bahdanau et al. (2014), which explicitly finds a soft al"
D15-1044,J02-4006,0,0.236151,"ilize extractive approaches that crop out and stitch together portions of the text to produce a condensed version. In contrast, abstractive summarization attempts to produce a bottom-up summary, aspects of which may not appear as part of the original. We focus on the task of sentence-level summarization. While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002). Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and Lapata, 2008; Woodsend et al., 2010). These approaches are described in more detail in Section 6. We instead explore a fully data-driven approach for generating abstractive summaries. Inspired by the recent success of neural machine translation, we combine a neural language model with a contextual input encoder. Our encoder is modeled off of the attention-based encoder of Bahdanau et"
D15-1044,D10-1050,0,0.771207,"he original. We focus on the task of sentence-level summarization. While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002). Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and Lapata, 2008; Woodsend et al., 2010). These approaches are described in more detail in Section 6. We instead explore a fully data-driven approach for generating abstractive summaries. Inspired by the recent success of neural machine translation, we combine a neural language model with a contextual input encoder. Our encoder is modeled off of the attention-based encoder of Bahdanau et al. (2014) in that it learns a latent soft alignment over the input text to help inform the summary (as shown in Figure 1). Crucially both the encoder and the generation model are trained jointly on the sentence summarization task. The model is desc"
D15-1044,P12-1107,0,0.00783747,"Missing"
D16-1137,P16-1231,0,0.179913,"Missing"
D16-1137,P14-1005,0,0.0118363,"Missing"
D16-1137,2014.iwslt-evaluation.1,0,0.257344,"Missing"
D16-1137,W14-3346,0,0.0134211,"Missing"
D16-1137,D14-1082,0,0.0253085,"ly benefits from search. We treat dependency parsing with arc-standard transitions as a seq2seq task by attempting to map from a source sentence to a target sequence of source sentence words interleaved with the arc-standard, reduce-actions in its parse. For example, we attempt to map the source sentence But it was the Quotron problems that ... to the target sequence But it was @L SBJ @L DEP the Quotron problems @L NMOD @L NMOD that ... We use the standard Penn Treebank dataset splits with Stanford dependency labels, and the standard UAS/LAS evaluation metric (excluding punctuation) following Chen and Manning (2014). All models thus see only the words in the source and, when decoding, the actions it has emitted so far; no other features are used. We use 2-layer encoder and decoder LSTMs with 300 hidden units per layer 1303 93.17/91.18 88.53/84.16 91.00/87.18 91.25/86.92 88.66/84.33 91.17/87.41 91.57/87.26 - - Table 3: Dependency parsing. UAS/LAS of seq2seq, BSO, ConBSO and baselines on PTB test set. Andor is the current state-of-the-art model for this data set (Andor et al. 2016), and we note that with a beam of size 32 they obtain 94.41/92.55. All experiments above have Ktr = 6. and dropout with a rate"
D16-1137,W14-4012,0,0.0285647,"Missing"
D16-1137,P04-1015,0,0.0281997,"ed and aggregated, and there have additionally been impor1297 tant refinements to this style of training over the past several years (Chang et al., 2015). When it comes to training RNNs, SEARN/DAgger has been applied under the name “scheduled sampling” (Bengio et al., 2015), which involves training an RNN to generate the t + 1’st token in a target sequence after consuming either the true t’th token, or, with probability that increases throughout training, the predicted t’th token. Though technically possible, it is uncommon to use beam search when training with SEARN/DAgger. The early-update (Collins and Roark, 2004) and LaSO (Daum´e III and Marcu, 2005) training strategies, however, explicitly account for beam search, and describe strategies for updating parameters when the gold structure becomes unreachable during search. Early update and LaSO differ primarily in that the former discards a training example after the first search error, whereas LaSO resumes searching after an error from a state that includes the gold partial structure. In the context of feed-forward neural network training, early update training has been recently explored in a feedforward setting by Zhou et al. (2015) and Andor et al. (2"
D16-1137,D15-1042,0,0.0239562,"Missing"
D16-1137,N12-1015,0,0.0114059,"Missing"
D16-1137,N15-1012,0,0.0124451,"partial sequence LSTMs. 7 Our code is based on Yoon Kim’s seq2seq code, https: //github.com/harvardnlp/seq2seq-attn. 1302 ferent problems: word ordering, dependency parsing, and machine translation. While we do not include all the features and extensions necessary to reach state-of-the-art performance, even the baseline seq2seq model is generally quite performant. Word Ordering The task of correctly ordering the words in a shuffled sentence has recently gained some attention as a way to test the (syntactic) capabilities of text-generation systems (Zhang and Clark, 2011; Zhang and Clark, 2015; Liu et al., 2015; Schmaltz et al., 2016). We cast this task as seq2seq problem by viewing a shuffled sentence as a source sentence, and the correctly ordered sentence as the target. While word ordering is a somewhat synthetic task, it has two interesting properties for our purposes. First, it is a task which plausibly requires search (due to the exponentially many possible orderings), and, second, there is a clear hard constraint on output sequences, namely, that they be a permutation of the source sequence. For both the baseline and BSO models we enforce this constraint at testtime. However, we also experime"
D16-1137,D15-1166,0,0.21769,"on to each of the three aforementioned issues, while largely maintaining the model architecture and training efficiency of standard seq2seq learning. Moreover, by scoring sequences rather than words, our approach also allows for enforcing hard-constraints on sequence generation at training time. To test out the effectiveness of the proposed approach, we develop a general-purpose seq2seq system with beam search optimization. We run experiments on three very different problems: word ordering, syntactic parsing, and machine translation, and compare to a highlytuned seq2seq system with attention (Luong et al., 2015). The version with beam search optimization shows significant improvements on all three tasks, and particular improvements on tasks that require difficult search. 2 Related Work The issues of exposure bias and label bias have received much attention from authors in the structured prediction community, and we briefly review some of this work here. One prominent approach to combating exposure bias is that of SEARN (Daum´e III et al., 2009), a meta-training algorithm that learns a search policy in the form of a cost-sensitive classifier trained on examples generated from an interpolation of an or"
D16-1137,P02-1040,0,0.0987653,"ts for machine translation (Bahdanau et al., 2015), roughly the same model and training have also proven to be useful for sentence compression (Filippova et al., 2015), parsing (Vinyals et al., 2015), and dialogue systems (Serban et al., 2016), and they additionally underlie other 1. Exposure Bias: the model is never exposed to its own errors during training, and so the inferred histories at test-time do not resemble the gold training histories. 2. Loss-Evaluation Mismatch: training uses a word-level loss, while at test-time we target improving sequence-level evaluation metrics, such as BLEU (Papineni et al., 2002). We might additionally add the concern of label bias (Lafferty et al., 2001) to the list, since wordprobabilities at each time-step are locally normalized, guaranteeing that successors of incorrect his1296 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1296–1306, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics tories receive the same mass as do the successors of the true history. In this work we develop a non-probabilistic variant of the seq2seq model that can assign a score to any possible target sequence, and"
D16-1137,D16-1255,1,0.89294,"STMs. 7 Our code is based on Yoon Kim’s seq2seq code, https: //github.com/harvardnlp/seq2seq-attn. 1302 ferent problems: word ordering, dependency parsing, and machine translation. While we do not include all the features and extensions necessary to reach state-of-the-art performance, even the baseline seq2seq model is generally quite performant. Word Ordering The task of correctly ordering the words in a shuffled sentence has recently gained some attention as a way to test the (syntactic) capabilities of text-generation systems (Zhang and Clark, 2011; Zhang and Clark, 2015; Liu et al., 2015; Schmaltz et al., 2016). We cast this task as seq2seq problem by viewing a shuffled sentence as a source sentence, and the correctly ordered sentence as the target. While word ordering is a somewhat synthetic task, it has two interesting properties for our purposes. First, it is a task which plausibly requires search (due to the exponentially many possible orderings), and, second, there is a clear hard constraint on output sequences, namely, that they be a permutation of the source sequence. For both the baseline and BSO models we enforce this constraint at testtime. However, we also experiment with constraining the"
D16-1137,P16-1159,0,0.138987,"ith recurrent components. Recently authors have also proposed alleviating exposure bias using techniques from reinforcement learning. Ranzato et al. (2016) follow this approach to train RNN decoders in a seq2seq model, and they obtain consistent improvements in performance, even over models trained with scheduled sampling. As Daum´e III and Marcu (2005) note, LaSO is similar to reinforcement learning, except it does not require “exploration” in the same way. Such exploration may be unnecessary in supervised text-generation, since we typically know the gold partial sequences at each time-step. Shen et al. (2016) use minimum risk training (approximated by sampling) to address the issues of exposure bias and loss-evaluation mismatch for seq2seq MT, and show impressive performance gains. Whereas exposure bias results from training in a certain way, label bias results from properties of the model itself. In particular, label bias is likely to affect structured models that make sub-structure predictions using locally-normalized scores. Because the neural and non-neural literature on this point has recently been reviewed by Andor et al. (2016), we simply note here that RNN models are typically locally norm"
D16-1137,P15-1113,0,0.0117787,"le during search. Early update and LaSO differ primarily in that the former discards a training example after the first search error, whereas LaSO resumes searching after an error from a state that includes the gold partial structure. In the context of feed-forward neural network training, early update training has been recently explored in a feedforward setting by Zhou et al. (2015) and Andor et al. (2016). Our work differs in that we adopt a LaSO-like paradigm (with some minor modifications), and apply it to the training of seq2seq RNNs (rather than feed-forward networks). We also note that Watanabe and Sumita (2015) apply maximumviolation training (Huang et al., 2012), which is similar to early-update, to a parsing model with recurrent components, and that Yazdani and Henderson (2015) use beam-search in training a discriminative, locally normalized dependency parser with recurrent components. Recently authors have also proposed alleviating exposure bias using techniques from reinforcement learning. Ranzato et al. (2016) follow this approach to train RNN decoders in a seq2seq model, and they obtain consistent improvements in performance, even over models trained with scheduled sampling. As Daum´e III and"
D16-1137,K15-1015,0,0.0282107,"an error from a state that includes the gold partial structure. In the context of feed-forward neural network training, early update training has been recently explored in a feedforward setting by Zhou et al. (2015) and Andor et al. (2016). Our work differs in that we adopt a LaSO-like paradigm (with some minor modifications), and apply it to the training of seq2seq RNNs (rather than feed-forward networks). We also note that Watanabe and Sumita (2015) apply maximumviolation training (Huang et al., 2012), which is similar to early-update, to a parsing model with recurrent components, and that Yazdani and Henderson (2015) use beam-search in training a discriminative, locally normalized dependency parser with recurrent components. Recently authors have also proposed alleviating exposure bias using techniques from reinforcement learning. Ranzato et al. (2016) follow this approach to train RNN decoders in a seq2seq model, and they obtain consistent improvements in performance, even over models trained with scheduled sampling. As Daum´e III and Marcu (2005) note, LaSO is similar to reinforcement learning, except it does not require “exploration” in the same way. Such exploration may be unnecessary in supervised te"
D16-1137,D11-1106,0,0.0180353,"each time-step, and sharing them between the partial sequence LSTMs. 7 Our code is based on Yoon Kim’s seq2seq code, https: //github.com/harvardnlp/seq2seq-attn. 1302 ferent problems: word ordering, dependency parsing, and machine translation. While we do not include all the features and extensions necessary to reach state-of-the-art performance, even the baseline seq2seq model is generally quite performant. Word Ordering The task of correctly ordering the words in a shuffled sentence has recently gained some attention as a way to test the (syntactic) capabilities of text-generation systems (Zhang and Clark, 2011; Zhang and Clark, 2015; Liu et al., 2015; Schmaltz et al., 2016). We cast this task as seq2seq problem by viewing a shuffled sentence as a source sentence, and the correctly ordered sentence as the target. While word ordering is a somewhat synthetic task, it has two interesting properties for our purposes. First, it is a task which plausibly requires search (due to the exponentially many possible orderings), and, second, there is a clear hard constraint on output sequences, namely, that they be a permutation of the source sequence. For both the baseline and BSO models we enforce this constrai"
D16-1137,J15-3005,0,0.0226865,"aring them between the partial sequence LSTMs. 7 Our code is based on Yoon Kim’s seq2seq code, https: //github.com/harvardnlp/seq2seq-attn. 1302 ferent problems: word ordering, dependency parsing, and machine translation. While we do not include all the features and extensions necessary to reach state-of-the-art performance, even the baseline seq2seq model is generally quite performant. Word Ordering The task of correctly ordering the words in a shuffled sentence has recently gained some attention as a way to test the (syntactic) capabilities of text-generation systems (Zhang and Clark, 2011; Zhang and Clark, 2015; Liu et al., 2015; Schmaltz et al., 2016). We cast this task as seq2seq problem by viewing a shuffled sentence as a source sentence, and the correctly ordered sentence as the target. While word ordering is a somewhat synthetic task, it has two interesting properties for our purposes. First, it is a task which plausibly requires search (due to the exponentially many possible orderings), and, second, there is a clear hard constraint on output sequences, namely, that they be a permutation of the source sequence. For both the baseline and BSO models we enforce this constraint at testtime. However"
D16-1137,P15-1117,0,0.0316188,"early-update (Collins and Roark, 2004) and LaSO (Daum´e III and Marcu, 2005) training strategies, however, explicitly account for beam search, and describe strategies for updating parameters when the gold structure becomes unreachable during search. Early update and LaSO differ primarily in that the former discards a training example after the first search error, whereas LaSO resumes searching after an error from a state that includes the gold partial structure. In the context of feed-forward neural network training, early update training has been recently explored in a feedforward setting by Zhou et al. (2015) and Andor et al. (2016). Our work differs in that we adopt a LaSO-like paradigm (with some minor modifications), and apply it to the training of seq2seq RNNs (rather than feed-forward networks). We also note that Watanabe and Sumita (2015) apply maximumviolation training (Huang et al., 2012), which is similar to early-update, to a parsing model with recurrent components, and that Yazdani and Henderson (2015) use beam-search in training a discriminative, locally normalized dependency parser with recurrent components. Recently authors have also proposed alleviating exposure bias using technique"
D16-1139,W14-3346,0,0.015345,"Missing"
D16-1139,N16-1155,0,0.0356037,"Missing"
D16-1139,D13-1176,0,0.0331541,"Missing"
D16-1139,D16-1180,0,0.0629092,"Missing"
D16-1139,N16-1014,0,0.0147708,"Missing"
D16-1139,P06-1096,0,0.0192123,"Missing"
D16-1139,D15-1176,0,0.0184985,"Missing"
D16-1139,D15-1166,0,0.0191485,"Missing"
D16-1139,D15-1107,0,0.0626349,"Missing"
D16-1139,P03-1021,0,0.0472041,"Missing"
D16-1139,P02-1040,0,0.111644,"Missing"
D16-1139,D15-1044,1,0.747861,"Missing"
D16-1139,K16-1029,0,0.0603618,"Missing"
D16-1139,P16-1159,0,0.0267632,"Missing"
D16-1139,Q16-1027,0,0.0629439,"Missing"
D16-1221,N13-1090,0,0.0334483,"essperson c and the set B of unique words in a bill. Our output y is whether that the congressperson voted yea or nay on the bill. We train on the full set of congressional votes on a number of bills. At test time, we supply entirely new bills and predict how each congressperson will vote on each new bill. We propose a simple bilinear model that uses low-dimensional embeddings to model each word in our dictionary and each congressperson. We represent each bill using its word embeddings in order to capture the multivariate relationships between words and their meanings (Collobert et al., 2011; Mikolov et al., 2013). The model is trained to synthesize information about each congressperson’s voting record into a multidimensional ideal vector. At test time, the model combines the embedding representation of a new bill with the trained ideal vector of a congressperson and generates a prediction for how the congressperson will vote on the bill. Let ew ∈ Rdword be the pretrained embedding for a word w. We initialize to the GloVe embeddings with dword = 50 (Pennington et al., 2014), then jointly train them with the model. To represent a bill, we average over the embeddings of the set B of words in the bill. To"
D16-1221,D14-1162,0,0.0973962,"sing its word embeddings in order to capture the multivariate relationships between words and their meanings (Collobert et al., 2011; Mikolov et al., 2013). The model is trained to synthesize information about each congressperson’s voting record into a multidimensional ideal vector. At test time, the model combines the embedding representation of a new bill with the trained ideal vector of a congressperson and generates a prediction for how the congressperson will vote on the bill. Let ew ∈ Rdword be the pretrained embedding for a word w. We initialize to the GloVe embeddings with dword = 50 (Pennington et al., 2014), then jointly train them with the model. To represent a bill, we average over the embeddings of the set B of words in the bill. To represent a congressperson, we introduce another set of embeddings vc ∈ Rdemb for each congressperson c. The embeddings act as the ideal vector for each legislator. Unlike the word embeddings, we initialize these randomly. The full model takes in a bill and a congressper2067 Congress # Bills House Senate Pres 106 107 108 109 110 111 557 505 607 579 854 965 R R R R D D R D2 R R D D Clinton Bush Bush Bush Bush Obama Table 1: Dataset details for 106-111th Congress. s"
D16-1255,J90-2002,0,0.547167,"Missing"
D16-1255,E14-1028,0,0.382265,"Missing"
D16-1255,E14-3010,0,0.158376,"Missing"
D16-1255,J10-4005,0,0.0688143,"Missing"
D16-1255,D15-1043,0,0.242476,"Missing"
D16-1255,N15-1012,0,0.251442,"Missing"
D16-1255,J93-2004,0,0.0549901,"Missing"
D16-1255,W12-3018,0,0.0346934,"Missing"
D16-1255,P02-1040,0,0.0966085,"Missing"
D16-1255,D11-1106,0,0.339341,"Missing"
D16-1255,J15-3005,0,0.10736,"Missing"
D16-1255,E12-1075,0,0.350454,"Missing"
D16-1255,C92-2092,0,\N,Missing
D17-1239,D10-1049,0,0.028878,"2005). Historically, research has focused on both content selection (“what to say”) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (“how to say it”) (Goldberg et al., 1994; Reiter et al., 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). Within the world of neural text generation, some recent work has focused on conditioning language models on tables (Yang et al., 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017). Mei et al. (2016) use a neural encoderdecoder approach on standard record-based generation datasets, obtaining impressive results, and motivating the need for more challenging NLG problems. 8 Conclusion and Future Work This work explores the challenges facing neural data-to-document generation"
D17-1239,H05-1042,0,0.899964,"enial to a standard encoder-decoder approach, and, more importantly, that it is reasonable to evaluate generations in terms of their fidelity to the database. One task that meets these criteria is that of generating summaries of sports games from associated box-score data, and there is indeed a long history of NLG work that generates sports game 2253 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253–2263 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics summaries (Robin, 1994; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005). To this end, we make the following contributions: • We introduce a new large-scale corpus consisting of textual descriptions of basketball games paired with extensive statistical tables. This dataset is sufficiently large that fully data-driven approaches might be sufficient. • We introduce a series of extractive evaluation models to automatically evaluate output generation performance, exploiting the fact that post-hoc information extraction is significantly easier than generation itself. • We apply a series of state-of-the-art neural methods, as well as a simple templated generation system"
D17-1239,K16-1002,0,0.0220758,"Missing"
D17-1239,P00-1037,0,0.25372,"Missing"
D17-1239,P17-4012,1,0.161462,"ets 49 49 , giving them just enough of an advantage to secure the victory in front of their home crowd . The Jazz were led by the duo of Derrick Favors and James Harden . Favors went 2 - for - 6 from the field and 0 - for - 1 from the three - point line to score a game - high of 15 points , while also adding four rebounds and four assists .... The <team1>’ next game will be at home against the Dallas Mavericks, while the <team2> will travel to play the Bulls. Code implementing all models can be found at https://github.com/harvardnlp/ data2text. Our encoder-decoder models are based on OpenNMT (Klein et al., 2017). 6 Results We found that all models performed quite poorly on the SBNATION data, with the best model achieving a validation perplexity of 33.34 and a BLEU score of 1.78. This poor performance is presumably attributable to the noisy quality of the SBNATION data, and the fact that many documents in the dataset focus on information not in the box- and line-scores. Accordingly, we focus on ROTOW IRE in what follows. The main results for the ROTOW IRE dataset are shown in Table 2, which shows the performance of the models in Section 4 in terms of the metrics defined in Section 3.2, as well as in t"
D17-1239,E17-1060,0,0.0174651,"ng (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). Within the world of neural text generation, some recent work has focused on conditioning language models on tables (Yang et al., 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017). Mei et al. (2016) use a neural encoderdecoder approach on standard record-based generation datasets, obtaining impressive results, and motivating the need for more challenging NLG problems. 8 Conclusion and Future Work This work explores the challenges facing neural data-to-document generation by introducing a new dataset, and proposing various metrics for automatically evaluating content selection, generation, and ordering. We see that recent ideas in copying and reconstruction lead to improvements on this task, but that there is a significant gap even between these neural models and templa"
D17-1239,P83-1022,0,0.289198,"ard generating longer outputs in response to longer and more complicated inputs, however, the generated texts begin to display reference errors, intersentence incoherence, and a lack of fidelity to the source material. The goal of this paper is to suggest a particular, long-form generation task in which these challenges may be fruitfully explored, to provide a publically available dataset for this task, to suggest some automatic evaluation metrics, and finally to establish how current, neural text generation methods perform on this task. A classic problem in natural-language generation (NLG) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997) involves taking structured data, such as a table, as input, and producing text that adequately and fluently describes this data as output. Unlike machine translation, which aims for a complete transduction of the sentence to be translated, this form of NLG is typically taken to require addressing (at least) two separate challenges: what to say, the selection of an appropriate subset of the input data to discuss, and how to say it, the surface realization of a generation (Reiter and Dale, 1997; Jurafsky and Martin, 2014). Traditionally, these two challeng"
D17-1239,W14-4012,0,0.111212,"Missing"
D17-1239,D16-1128,0,0.120481,"Missing"
D17-1239,W03-1016,0,0.0314109,"Rockets’ rebounds could manifest in a lower CO score, and incorrectly indicating the win/loss records is a CS error. 7 Related Work In this section we note additional related work not noted throughout. Natural language generation has been studied for decades (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997), and generating summaries of sports games has been a topic of interest for almost as long (Robin, 1994; TanakaIshii et al., 1998; Barzilay and Lapata, 2005). Historically, research has focused on both content selection (“what to say”) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (“how to say it”) (Goldberg et al., 1994; Reiter et al., 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). Within the world of neural text generation, some recent work has focused on"
D17-1239,P16-1154,0,0.518854,"are defined with respect to the 2 DLD is a variant of Levenshtein distance that allows transpositions of elements; it is useful in comparing the ordering of sequences that may not be permutations of the same set (which is a requirement for measures like Kendall’s Tau). 2256 predictions of an information extraction system. Accordingly, our metrics are quite interpretable, since by construction it is always possible to determine which fact (i.e., entity-value pair) in the generation is determined by the extractor to not match the database or the gold generation. 4 Joint Copy Model The models of Gu et al. (2016) and Yang et al. (2016) parameterize the joint distribution table over yˆt and zt directly: p(ˆ yt , zt |yˆ1:t−1 , s) ∝   yt , yˆ1:t−1 , s) copy(ˆ 0   gen(ˆ yt , yˆ1:t−1 , s) Neural Data-to-Document Models In this section we briefly describe the neural generation methods we apply to the proposed task. As a base model we utilize the now standard attentionbased encoder-decoder model (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015). We also experiment with several recent extensions to this model, including copy-based generation, and training with a source reconstruction term"
D17-1239,P16-1014,0,0.0223001,"Missing"
D17-1239,P16-1002,0,0.0168363,"n represented as s r j }Jj=1 . ˜, we use an LSTM decoder with attenGiven s tion and input-feeding, in the style of Luong et al. (2015), to compute the probability of each target word, conditioned on the previous words and on s. The model is trained end-to-end to minimize the negative log-likelihood of the words in the gold text y1:T given corresponding source material s. Copying There has been a surge of recent work involving augmenting encoder-decoder models to copy words directly from the source material on which they condition (Gu et al., 2016; G¨ulc¸ehre et al., 2016; Merity et al., 2016; Jia and Liang, 2016; Yang et al., 2016). These models typically introduce an additional binary variable zt into the per-timestep target word distribution, which indicates whether the target word yˆt is copied from the source or generated: p(ˆ yt |yˆ1:t−1 , s) = X p(ˆ yt , zt = z |yˆ1:t−1 , s). z∈{0,1} In our case, we assume that target words are copied from the value portion of a record r; that is, a copy implies yˆt = r.m for some r and t. 3 We also include an additional feature for whether the player is on the home- or away-team. zt = 1, yˆt ∈ s zt = 1, yˆt 6∈ s zt = 0, where copy and gen are functions paramet"
D17-1239,C10-2062,0,0.0918002,"research has focused on both content selection (“what to say”) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (“how to say it”) (Goldberg et al., 1994; Reiter et al., 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). Within the world of neural text generation, some recent work has focused on conditioning language models on tables (Yang et al., 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017). Mei et al. (2016) use a neural encoderdecoder approach on standard record-based generation datasets, obtaining impressive results, and motivating the need for more challenging NLG problems. 8 Conclusion and Future Work This work explores the challenges facing neural data-to-document generation by introducing a new d"
D17-1239,D14-1181,0,0.00296209,"he scores used in copy or pcopy . We train the generation models using SGD and truncated BPTT (Elman, 1990; Mikolov et al., 2010), as in language modeling. That is, we split each y1:T into contiguous blocks of length 100, and backprop both the gradients with respect to the current block as well as with respect to the encoder parameters for each block. Our extractive evaluator consists of an ensemble of 3 single-layer convolutional and 3 singlelayer bidirectional LSTM models. The convolutional models concatenate convolutions with kernel widths 2, 3, and 5, and 200 feature maps in the style of (Kim, 2014). Both models are trained with SGD. Templatized Generator In addition to neural baselines, we also use a problem-specific, template-based generator. The template-based generator first emits a sentence about the teams playing in the game, using a templatized sentence taken from the training set: The <team1> (<wins1>-<losses1>) delog pk (r.x |bi ; θ), feated the <team2> (<wins2>-<losses2>) x∈{e,m,t} <pts1>-<pts2>. where pk is the k’th predicted distribution over records, and where we have modeled each component of r independently. This loss attempts to make the most probable record in s given bi"
D17-1239,D17-1230,0,0.0221655,"Damerau-Levenshtein Distance (Brill and Moore, 2000)2 between the sequences of records extracted from y1:T and that extracted from yˆ1:T . This measures how well the system orders the records it chooses to discuss. We note that CS primarily targets the “what to say” aspect of evaluation, CO targets the “how to say it” aspect, and RG targets both. We conclude this section by contrasting the automatic evaluation we have proposed with recently proposed adversarial evaluation approaches, which also advocate automatic metrics backed by classification (Bowman et al., 2016; Kannan and Vinyals, 2016; Li et al., 2017). Unlike adversarial evaluation, which uses a blackbox classifier to determine the quality of a generation, our metrics are defined with respect to the 2 DLD is a variant of Levenshtein distance that allows transpositions of elements; it is useful in comparing the ordering of sequences that may not be permutations of the same set (which is a requirement for measures like Kendall’s Tau). 2256 predictions of an information extraction system. Accordingly, our metrics are quite interpretable, since by construction it is always possible to determine which fact (i.e., entity-value pair) in the gener"
D17-1239,P09-1011,0,0.703039,"improvements in BLEU and in our proposed extractive evaluations, current models are still quite far from producing human-level output, and are significantly worse than templated systems in terms of content selection and realization. Overall, we believe this problem of data-to-document generation highlights important remaining challenges in neural generation systems, and the use of extractive evaluation reveals significant issues hidden by standard automatic metrics. 2 Data-to-Text Datasets We consider the problem of generating descriptive text from database records. Following the notation in Liang et al. (2009), let s = {rj }Jj=1 be a set of records, where for each r ∈ s we define r.t ∈ T to be the type of r, and we assume each r to be a binarized relation, where r.e and r.m are a record’s entity and value, respectively. For example, a database recording statistics for a basketball game might have a record r such that r.t = POINTS, r.e = RUSSELL W ESTBROOK, and r.m = 50. In this case, r.e gives the player in question, and r.m gives the number of points the player scored. From these records, we are interested in generating descriptive text, yˆ1:T = yˆ1 , . . . , yˆT of T words such that yˆ1:T is an a"
D17-1239,D16-1230,0,0.0225098,"are typically evaluated using a combination of automatic measures, such as BLEU (Papineni et al., 2002), and human evaluation. While BLEU is perhaps a reasonably effective way of evaluating short-form text generation, we found it to be unsatisfactory for document generation. In particular, we note that it primarily rewards fluent text generation, rather than generations that capture the most important information in the database, or that report the information in a particularly coherent way. While human evaluation, on the other hand, is likely ultimately necessary for evaluating generations (Liu et al., 2016; Wu et al., 2016), it is much less convenient than using automatic metrics. Furthermore, we believe that current text generations are sufficiently bad in sufficiently obvious ways that automatic metrics can still be of use in evaluation, and we are not yet at the point of needing to rely solely on human evaluators. 3.1 Extractive Evaluation To address this evaluation challenge, we begin with the intuition that assessing document quality is easier than document generation. In particular, it is much easier to automatically extract information from documents than to generate documents that accur"
D17-1239,D11-1149,0,0.0176997,"on both content selection (“what to say”) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (“how to say it”) (Goldberg et al., 1994; Reiter et al., 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). Within the world of neural text generation, some recent work has focused on conditioning language models on tables (Yang et al., 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017). Mei et al. (2016) use a neural encoderdecoder approach on standard record-based generation datasets, obtaining impressive results, and motivating the need for more challenging NLG problems. 8 Conclusion and Future Work This work explores the challenges facing neural data-to-document generation by introducing a new dataset, and propo"
D17-1239,D15-1166,0,0.0514046,"2015). We also experiment with several recent extensions to this model, including copy-based generation, and training with a source reconstruction term in the loss (in addition to the standard per-target-word loss). Base Model For our base model, we map each record r ∈ s into a vector r˜ by first embedding r.t (e.g., POINTS), r.e (e.g., RUSSELL W ESTBROOK), and r.m (e.g., 50), and then applying a 1-layer MLP (similar to Yang et al. (2016)).3 Our source ˜ = {˜ data-records are then represented as s r j }Jj=1 . ˜, we use an LSTM decoder with attenGiven s tion and input-feeding, in the style of Luong et al. (2015), to compute the probability of each target word, conditioned on the previous words and on s. The model is trained end-to-end to minimize the negative log-likelihood of the words in the gold text y1:T given corresponding source material s. Copying There has been a surge of recent work involving augmenting encoder-decoder models to copy words directly from the source material on which they condition (Gu et al., 2016; G¨ulc¸ehre et al., 2016; Merity et al., 2016; Jia and Liang, 2016; Yang et al., 2016). These models typically introduce an additional binary variable zt into the per-timestep targe"
D17-1239,N16-1086,0,0.603298,"d in generating descriptive text, yˆ1:T = yˆ1 , . . . , yˆT of T words such that yˆ1:T is an adequate and fluent summary of s. A dataset for training data-to-document systems typically consists of (s, y1:T ) pairs, where y1:T is a document consisting of a gold (i.e., human generated) summary for database s. Several benchmark datasets have been used in recent years for the text generation task, the most popular of these being W EATHER G OV (Liang et al., 2009) and ROBOCUP (Chen and Mooney, 2008). Recently, neural generation systems have show strong results on these datasets, with the system of Mei et al. (2016) achieving BLEU scores in the 60s and 70s on W EATHER G OV, and BLEU scores of almost 30 even on the smaller ROBOCUP dataset. These results are quite promising, and suggest that neural models are a good fit for text generation. However, the statistics of these datasets, shown in Table 1, indicate that these datasets use relatively simple language and record structure. Furthermore, there is reason to believe that W EATHER G OV is at least partially machinegenerated (Reiter, 2017). More recently, Lebret et al. (2016) introduced the W IKI B IO dataset, which is at least an order of magnitude larg"
D17-1239,P02-1040,0,0.110284,"aset is significantly larger, but also much more challenging, as the language is very informal, and often tangential to the statistics themselves. We show some sample text from ROTOW IRE in Figure 1. Our primary focus will be on the RO TOW IRE data. 3 Evaluating Document Generation We begin by discussing the evaluation of generated documents, since both the task we introduce and the evaluation methods we propose are motivated by some of the shortcomings of current approaches to evaluation. Text generation systems are typically evaluated using a combination of automatic measures, such as BLEU (Papineni et al., 2002), and human evaluation. While BLEU is perhaps a reasonably effective way of evaluating short-form text generation, we found it to be unsatisfactory for document generation. In particular, we note that it primarily rewards fluent text generation, rather than generations that capture the most important information in the database, or that report the information in a particularly coherent way. While human evaluation, on the other hand, is likely ultimately necessary for evaluating generations (Liu et al., 2016; Wu et al., 2016), it is much less convenient than using automatic metrics. Furthermore"
D17-1239,P15-1061,0,0.0110588,"ity (player, team, and city) and value (number and certain string) pairs r.e, r.m that appear in the text, and then predict the type r.t (or none) of each candidate pair. For example, we might extract the entity-value pair (“Miami Heat”, “95”) from the first sentence in Figure 1, and then predict that the type of this pair is POINTS, giving us an extracted record r such that (r.e, r.m, r.t) = (M IAMI H EAT, 95, POINTS). Indeed, many relation extraction systems reduce relation extraction to multi-class classification precisely in this way (Zhang, 2004; Zhou et al., 2008; Zeng et al., 2014; dos Santos et al., 2015). More concretely, given a document yˆ1:T , we consider all pairs of word-spans in each sentence that represent possible entities e and values m. We then model p(r.t |e, m; θ) for each pair, using r.t =  to indicate unrelated pairs. We use architectures similar to those discussed in Collobert et al. (2011) and dos Santos et al. (2015) to parameterize this probability; full details are given in the Appendix. Importantly, we note that the (s, y1:T ) pairs typically used for training data-to-document systems are also sufficient for training the information extraction model presented above, since"
D17-1239,P06-1139,0,0.104479,"and generating summaries of sports games has been a topic of interest for almost as long (Robin, 1994; TanakaIshii et al., 1998; Barzilay and Lapata, 2005). Historically, research has focused on both content selection (“what to say”) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (“how to say it”) (Goldberg et al., 1994; Reiter et al., 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). Within the world of neural text generation, some recent work has focused on conditioning language models on tables (Yang et al., 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017). Mei et al. (2016) use a neural encoderdecoder approach on standard record-based generation datasets, obtaining impressive results, and motivating the"
D17-1239,N07-1022,0,0.0118997,"has been studied for decades (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997), and generating summaries of sports games has been a topic of interest for almost as long (Robin, 1994; TanakaIshii et al., 1998; Barzilay and Lapata, 2005). Historically, research has focused on both content selection (“what to say”) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (“how to say it”) (Goldberg et al., 1994; Reiter et al., 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). Within the world of neural text generation, some recent work has focused on conditioning language models on tables (Yang et al., 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017). Mei et al. (2016) use a neural encoderdecoder approach on standard"
D17-1239,1983.tc-1.13,0,0.327159,"Missing"
D17-1239,C14-1220,0,0.011714,"t extract candidate entity (player, team, and city) and value (number and certain string) pairs r.e, r.m that appear in the text, and then predict the type r.t (or none) of each candidate pair. For example, we might extract the entity-value pair (“Miami Heat”, “95”) from the first sentence in Figure 1, and then predict that the type of this pair is POINTS, giving us an extracted record r such that (r.e, r.m, r.t) = (M IAMI H EAT, 95, POINTS). Indeed, many relation extraction systems reduce relation extraction to multi-class classification precisely in this way (Zhang, 2004; Zhou et al., 2008; Zeng et al., 2014; dos Santos et al., 2015). More concretely, given a document yˆ1:T , we consider all pairs of word-spans in each sentence that represent possible entities e and values m. We then model p(r.t |e, m; θ) for each pair, using r.t =  to indicate unrelated pairs. We use architectures similar to those discussed in Collobert et al. (2011) and dos Santos et al. (2015) to parameterize this probability; full details are given in the Appendix. Importantly, we note that the (s, y1:T ) pairs typically used for training data-to-document systems are also sufficient for training the information extraction mo"
D17-1239,I08-1005,0,0.00919431,"gure 1. We may first extract candidate entity (player, team, and city) and value (number and certain string) pairs r.e, r.m that appear in the text, and then predict the type r.t (or none) of each candidate pair. For example, we might extract the entity-value pair (“Miami Heat”, “95”) from the first sentence in Figure 1, and then predict that the type of this pair is POINTS, giving us an extracted record r such that (r.e, r.m, r.t) = (M IAMI H EAT, 95, POINTS). Indeed, many relation extraction systems reduce relation extraction to multi-class classification precisely in this way (Zhang, 2004; Zhou et al., 2008; Zeng et al., 2014; dos Santos et al., 2015). More concretely, given a document yˆ1:T , we consider all pairs of word-spans in each sentence that represent possible entities e and values m. We then model p(r.t |e, m; θ) for each pair, using r.t =  to indicate unrelated pairs. We use architectures similar to those discussed in Collobert et al. (2011) and dos Santos et al. (2015) to parameterize this probability; full details are given in the Appendix. Importantly, we note that the (s, y1:T ) pairs typically used for training data-to-document systems are also sufficient for training the inform"
D17-1239,P98-2209,0,0.262177,"t the task is somewhat congenial to a standard encoder-decoder approach, and, more importantly, that it is reasonable to evaluate generations in terms of their fidelity to the database. One task that meets these criteria is that of generating summaries of sports games from associated box-score data, and there is indeed a long history of NLG work that generates sports game 2253 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253–2263 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics summaries (Robin, 1994; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005). To this end, we make the following contributions: • We introduce a new large-scale corpus consisting of textual descriptions of basketball games paired with extensive statistical tables. This dataset is sufficiently large that fully data-driven approaches might be sufficient. • We introduce a series of extractive evaluation models to automatically evaluate output generation performance, exploiting the fact that post-hoc information extraction is significantly easier than generation itself. • We apply a series of state-of-the-art neural methods, as well as a simple"
D17-1239,2007.mtsummit-ucnlg.4,0,0.039326,"of sports games has been a topic of interest for almost as long (Robin, 1994; TanakaIshii et al., 1998; Barzilay and Lapata, 2005). Historically, research has focused on both content selection (“what to say”) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (“how to say it”) (Goldberg et al., 1994; Reiter et al., 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). Within the world of neural text generation, some recent work has focused on conditioning language models on tables (Yang et al., 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017). Mei et al. (2016) use a neural encoderdecoder approach on standard record-based generation datasets, obtaining impressive results, and motivating the need for more challe"
D17-1298,D16-1195,0,0.0113331,"only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the task (Junczys-Dowmunt and Grundkiewicz, 2016), outperforming wordand character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017), phrase-based systems with neural features (Chollampatt et al., 2016b,a), re-ranking output from phrase-based systems (Hoang et al., 2016), and combining phrase-based systems with classifiers trained for hand-picked subsets of errors (Rozovskaya and Roth, 2016). We revisit the comparison across translation approaches for the correction task in light of the Automated Evaluation of Scientific Writing (AESW) 2016 dataset, a correction dataset containing over 1 million sentences, holding constant the training data across approaches. The dataset was previously proposed for the distinct binary classification task of grammatical error identification. Experiments demo"
D17-1298,N12-1067,0,0.434547,"Missing"
D17-1298,W13-1703,0,0.20543,"ntences as the dev set3 (of which 53,502 contain edits). The test set contains 146,478 sentences. The primary focus of the present study is conducting controlled experiments on the AESW dataset, but we also investigate results on the CoNLL-2014 shared task data in light of recent neural results (Ji et al., 2017) and to serve as a baseline of comparison against existing sequenceto-sequence approaches (Yuan and Briscoe, 2016; Xie et al., 2016). We use the common sets of public data appearing in past work for training: the National University of Singapore (NUS) Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) and the publicly available Lang-8 so document-level context is not available. 2 Characteristics of the dataset preclude experiments with additional paragraph context features. (See Appendix A.) 3 The dev set contains 13,562 unique deletion types, 29,952 insertion types, and 39,930 replacement types. data (Tajiri et al., 2012; Mizumoto et al., 2012). The Lang-8 dataset of corrections is large4 but is crowd-sourced5 and is thus of a different nature than the professionally annotated AESW and NUCLE datasets. We use the revised CoNLL2013 test set as a tuning/dev set and the CoNLL2014 test set (wi"
D17-1298,W12-2006,0,0.0185518,"ints. Additionally, in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M 2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches. 1 Introduction The task of sentence correction is to convert a natural language sentence that may or may not have errors into a corrected version. The task is envisioned as a component of a learning tool or writing-assistant, and has seen increased interest since 2011 driven by a series of shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014). Most recent work on language correction has focused on the data provided by the CoNLL-2014 shared task (Ng et al., 2014), a set of corrected essays by second-language learners. The CoNLL2014 data consists of only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-"
D17-1298,W11-2838,0,0.0450534,"ata, by 6 M 2 (0.5 GLEU) points. Additionally, in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M 2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches. 1 Introduction The task of sentence correction is to convert a natural language sentence that may or may not have errors into a corrected version. The task is envisioned as a component of a learning tool or writing-assistant, and has seen increased interest since 2011 driven by a series of shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014). Most recent work on language correction has focused on the data provided by the CoNLL-2014 shared task (Ng et al., 2014), a set of corrected essays by second-language learners. The CoNLL2014 data consists of only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific featur"
D17-1298,W16-0506,0,0.0201803,"42.78 41.21 44.62 W ORD + DOM W ORD +B I + DOM C HAR CNN+B I + DOM C HAR CNN+ DOM C HAR +B I + DOM 91.25 91.45 91.15 91.35 91.64 − − − − 91.39 43.12 44.33 40.79 43.94 47.25 − − − − 46.72 Table 1: AESW development/test set correction results. GLEU and M 2 differences on test are statistically significant via paired bootstrap resampling (Koehn, 2004; Graham et al., 2014) at the 0.05 level, resampling the full set 50 times. models we propose modeling the input and output sequences with a special initial token representing the journal domain (+ DOM).2 3 Experiments Data AESW (Daudaravicius, 2016; Daudaravicius et al., 2016) consists of sentences taken from academic articles annotated with corrections by professional editors used for the AESW shared task. The training set contains 1,182,491 sentences, of which 460,901 sentences have edits. We set aside a 9,947 sentence sample from the original development set for tuning (of which 3,797 contain edits), and use the remaining 137,446 sentences as the dev set3 (of which 53,502 contain edits). The test set contains 146,478 sentences. The primary focus of the present study is conducting controlled experiments on the AESW dataset, but we also investigate results on the"
D17-1298,W14-3333,0,0.019636,"DIFFS +M 2 SMT– DIFFS +BLEU W ORD +B I – DIFFS C HAR +B I – DIFFS 90.44 90.90 91.18 91.28 − − − − 38.55 37.66 38.88 40.11 − − − − SMT+BLEU W ORD +B I C HAR CNN C HAR +B I 90.95 91.34 91.23 91.46 90.70 91.05 90.96 91.22 38.99 43.61 42.02 44.67 38.31 42.78 41.21 44.62 W ORD + DOM W ORD +B I + DOM C HAR CNN+B I + DOM C HAR CNN+ DOM C HAR +B I + DOM 91.25 91.45 91.15 91.35 91.64 − − − − 91.39 43.12 44.33 40.79 43.94 47.25 − − − − 46.72 Table 1: AESW development/test set correction results. GLEU and M 2 differences on test are statistically significant via paired bootstrap resampling (Koehn, 2004; Graham et al., 2014) at the 0.05 level, resampling the full set 50 times. models we propose modeling the input and output sequences with a special initial token representing the journal domain (+ DOM).2 3 Experiments Data AESW (Daudaravicius, 2016; Daudaravicius et al., 2016) consists of sentences taken from academic articles annotated with corrections by professional editors used for the AESW shared task. The training set contains 1,182,491 sentences, of which 460,901 sentences have edits. We set aside a 9,947 sentence sample from the original development set for tuning (of which 3,797 contain edits), and use th"
D17-1298,P17-1070,0,0.41752,"s by second-language learners. The CoNLL2014 data consists of only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the task (Junczys-Dowmunt and Grundkiewicz, 2016), outperforming wordand character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017), phrase-based systems with neural features (Chollampatt et al., 2016b,a), re-ranking output from phrase-based systems (Hoang et al., 2016), and combining phrase-based systems with classifiers trained for hand-picked subsets of errors (Rozovskaya and Roth, 2016). We revisit the comparison across translation approaches for the correction task in light of the Automated Evaluation of Scientific Writing (AESW) 2016 dataset, a correction dataset containing over 1 million sentences, holding constant the training data across approaches. The dataset was previously proposed for the distinct binary clas"
D17-1298,D16-1161,0,0.625423,"4). Most recent work on language correction has focused on the data provided by the CoNLL-2014 shared task (Ng et al., 2014), a set of corrected essays by second-language learners. The CoNLL2014 data consists of only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the task (Junczys-Dowmunt and Grundkiewicz, 2016), outperforming wordand character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017), phrase-based systems with neural features (Chollampatt et al., 2016b,a), re-ranking output from phrase-based systems (Hoang et al., 2016), and combining phrase-based systems with classifiers trained for hand-picked subsets of errors (Rozovskaya and Roth, 2016). We revisit the comparison across translation approaches for the correction task in light of the Automated Evaluation of Scientific Writing (AESW) 2016 dataset, a correction dataset containing over 1 million se"
D17-1298,W04-3250,0,0.0652137,"0 00.00 SMT– DIFFS +M 2 SMT– DIFFS +BLEU W ORD +B I – DIFFS C HAR +B I – DIFFS 90.44 90.90 91.18 91.28 − − − − 38.55 37.66 38.88 40.11 − − − − SMT+BLEU W ORD +B I C HAR CNN C HAR +B I 90.95 91.34 91.23 91.46 90.70 91.05 90.96 91.22 38.99 43.61 42.02 44.67 38.31 42.78 41.21 44.62 W ORD + DOM W ORD +B I + DOM C HAR CNN+B I + DOM C HAR CNN+ DOM C HAR +B I + DOM 91.25 91.45 91.15 91.35 91.64 − − − − 91.39 43.12 44.33 40.79 43.94 47.25 − − − − 46.72 Table 1: AESW development/test set correction results. GLEU and M 2 differences on test are statistically significant via paired bootstrap resampling (Koehn, 2004; Graham et al., 2014) at the 0.05 level, resampling the full set 50 times. models we propose modeling the input and output sequences with a special initial token representing the journal domain (+ DOM).2 3 Experiments Data AESW (Daudaravicius, 2016; Daudaravicius et al., 2016) consists of sentences taken from academic articles annotated with corrections by professional editors used for the AESW shared task. The training set contains 1,182,491 sentences, of which 460,901 sentences have edits. We set aside a 9,947 sentence sample from the original development set for tuning (of which 3,797 cont"
D17-1298,D15-1166,0,0.0927899,"Missing"
D17-1298,C12-2084,0,0.46323,"Missing"
D17-1298,W14-1701,0,0.28744,"r or better M 2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches. 1 Introduction The task of sentence correction is to convert a natural language sentence that may or may not have errors into a corrected version. The task is envisioned as a component of a learning tool or writing-assistant, and has seen increased interest since 2011 driven by a series of shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014). Most recent work on language correction has focused on the data provided by the CoNLL-2014 shared task (Ng et al., 2014), a set of corrected essays by second-language learners. The CoNLL2014 data consists of only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the task (Junczys-Dowmunt and Grundkiewicz, 2016), outperforming wordand character-based sequence-to-sequence models (Yuan and Briscoe"
D17-1298,W13-3601,0,0.200026,"in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M 2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches. 1 Introduction The task of sentence correction is to convert a natural language sentence that may or may not have errors into a corrected version. The task is envisioned as a component of a learning tool or writing-assistant, and has seen increased interest since 2011 driven by a series of shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014). Most recent work on language correction has focused on the data provided by the CoNLL-2014 shared task (Ng et al., 2014), a set of corrected essays by second-language learners. The CoNLL2014 data consists of only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the t"
D17-1298,P16-1208,0,0.121171,"addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the task (Junczys-Dowmunt and Grundkiewicz, 2016), outperforming wordand character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017), phrase-based systems with neural features (Chollampatt et al., 2016b,a), re-ranking output from phrase-based systems (Hoang et al., 2016), and combining phrase-based systems with classifiers trained for hand-picked subsets of errors (Rozovskaya and Roth, 2016). We revisit the comparison across translation approaches for the correction task in light of the Automated Evaluation of Scientific Writing (AESW) 2016 dataset, a correction dataset containing over 1 million sentences, holding constant the training data across approaches. The dataset was previously proposed for the distinct binary classification task of grammatical error identification. Experiments demonstrate that pure characterlevel sequence-to-sequence models are more effective on AESW than word-based models and models that encode subword information via convolutions over characters, and t"
D17-1298,W16-0528,1,0.83144,"rate that on a large, professionally annotated dataset, the most effective sequence-to-sequence approach can significantly outperform a state-of-the-art SMT system without augmenting the sequence-to-sequence model with a secondary model to handle lowfrequency words (Yuan and Briscoe, 2016) or an additional model to improve precision or intersecting a large language model (Xie et al., 2016). We also demonstrate improvements over these previous sequence-to-sequence approaches on the CoNLL-2014 data and competitive results with Ji et al. (2017), despite using significantly less data. The work of Schmaltz et al. (2016) applies W ORD and C HAR CNN models to the distinct binary classification task of error identification. Additional Approaches The standard formulation of the correction task is to model the output sequence as t above. Here, we also propose modeling the diffs between s and t. The diffs are provided in-line within t and are described via tags marking the starts and ends of insertions and deletions, with replacements represented as deletioninsertion pairs, as in the following example selected from the training set: “Some key points are worth &lt;del&gt; emphasiz &lt;/del&gt; &lt;ins&gt; emphasizing &lt;/ins&gt; .”. Here"
D17-1298,P12-2039,0,0.145359,"baseline of comparison against existing sequenceto-sequence approaches (Yuan and Briscoe, 2016; Xie et al., 2016). We use the common sets of public data appearing in past work for training: the National University of Singapore (NUS) Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) and the publicly available Lang-8 so document-level context is not available. 2 Characteristics of the dataset preclude experiments with additional paragraph context features. (See Appendix A.) 3 The dev set contains 13,562 unique deletion types, 29,952 insertion types, and 39,930 replacement types. data (Tajiri et al., 2012; Mizumoto et al., 2012). The Lang-8 dataset of corrections is large4 but is crowd-sourced5 and is thus of a different nature than the professionally annotated AESW and NUCLE datasets. We use the revised CoNLL2013 test set as a tuning/dev set and the CoNLL2014 test set (without alternatives) for testing. We do not make use of the non-public Cambridge Learner Corpus (CLC) (Nicholls, 2003), which contains over 1.5 million sentence pairs. Evaluation We follow past work and use the Generalized Language Understanding Evaluation (GLEU) (Napoles et al., 2016) and MaxMatch (M 2 ) metrics (Dahlmeier an"
D17-1298,N16-1042,0,0.214811,"Ng et al., 2014), a set of corrected essays by second-language learners. The CoNLL2014 data consists of only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the task (Junczys-Dowmunt and Grundkiewicz, 2016), outperforming wordand character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017), phrase-based systems with neural features (Chollampatt et al., 2016b,a), re-ranking output from phrase-based systems (Hoang et al., 2016), and combining phrase-based systems with classifiers trained for hand-picked subsets of errors (Rozovskaya and Roth, 2016). We revisit the comparison across translation approaches for the correction task in light of the Automated Evaluation of Scientific Writing (AESW) 2016 dataset, a correction dataset containing over 1 million sentences, holding constant the training data across approaches. The dataset was previously p"
D18-1084,1983.tc-1.13,0,0.42716,"Missing"
D18-1130,P06-1005,0,0.0606058,"ame size as the original word embedding, adding the vectors as well as a bias, and applying a tanh nonlinearity. Method 2: Multitasking We additionally encourage the neural model to keep track of entities by multitasking with simple auxiliary entitytracking tasks. Examples such as Figure 1 suggest that keeping track of which entities are currently in 1 POS tags are produced with the NLTK library (Bird et al., 2009), and NER tags with the Stanford NER tagger (Finkel et al., 2005). We additionally found it useful to tag animate words as PERSONs on the CBT-NE data, using the animate word list of Bergsma and Lin (2006). → ui = h i , and the query q = h j−1 . (Note that unlike above, both of these only use the forward states of the GRU). We use a bilinear similarity score si = q T Q ui , for this prediction where Q is a learned transformation in R2d×2d . This task is inspired by the antecedent ranking task in coreference (Wiseman et al., 2015, 2016). For Task 2 (L2 ) we train to predict the order index in which a named entity has been introduced. For example, in Figure 1, julie would be 1, amy would be 2, marsh would be 3, etc. The hope here is that learning to predict when entities reappear will help the mo"
D18-1130,E17-2009,0,0.127711,"redictions before and after adding multi-task objective are shown. Introduction There has been tremendous interest over the past several years in Cloze-style (Taylor, 1953) reading comprehension tasks, datasets, and models (Hermann et al., 2015; Hill et al., 2016; Kadlec et al., 2016; Dhingra et al., 2016; Cui et al., 2016). Many of these systems apply neural models to learn to predict answers based on contextual matching, and have inspired other work in long-form generation and question answering. The extent and limits of these successes have also been a topic of interest (Chen et al., 2016; Chu et al., 2017). Recent analysis by Chu et al. (2017) suggests that a significant portion of the errors made by standard models, especially on the LAMBADA dataset (Paperno et al., 2016), derive from the inability to correctly track entities or speakers, or a failure to handle various forms of reference. This work targets these shortcomings by designing a model and training scheme targeted towards entity tracking. Specifically we introduce two simple changes to a stripped down model: (1) simple, entity-focused features, and (2) two multi-task objectives that target entity tracking. Our ablation analysis shows"
D18-1130,N18-2007,0,0.0130979,"nd define ||as the concatenation operator. The → ← context vectors are constructed as ui = h i,↑ ||h i,↑ . For datasets using the last word, the query is con→ ← structed as q = h n,↓ ||h 1,↓ . When the masked word can be anywhere, the query is constructed → ← as q = h j−1,↓ ||h j+1,↓ . Our main contribution is the extension of this simple model to incorporate entity tracking. Other authors have explored extending neural reading comprehension models with linguistic features, particularly Dhingra et al. (2017) who use a modified GRU with knowledge such as coreference relations and hypernymy. In Dhingra et al. (2018), the most recent coreferent antecedent for each token is incorporated into the update equations of the GRU unit to bias the reader towards coreferent recency. In this work, we instead use a much sim1050 1 2 3 4 5 6 7 Sentence Index, POS Tag, NER Tag Is among last 3 PERSON words in x Is a PERSON word in the last sentence Is a PERSON word identical to previous PERSON word Is a PERSON word identical to next PERSON word Quoted-speech Index Speaker Table 1: Word-level features used in AttSum-Feat model. pler set of features and compare to this and several other models as baseline approaches. scope"
D18-1130,P05-1045,0,0.0437302,"Wang et al., 2017). All features are incorporated into a word’s representation by embedding each discrete feature into a vector of the same size as the original word embedding, adding the vectors as well as a bias, and applying a tanh nonlinearity. Method 2: Multitasking We additionally encourage the neural model to keep track of entities by multitasking with simple auxiliary entitytracking tasks. Examples such as Figure 1 suggest that keeping track of which entities are currently in 1 POS tags are produced with the NLTK library (Bird et al., 2009), and NER tags with the Stanford NER tagger (Finkel et al., 2005). We additionally found it useful to tag animate words as PERSONs on the CBT-NE data, using the animate word list of Bergsma and Lin (2006). → ui = h i , and the query q = h j−1 . (Note that unlike above, both of these only use the forward states of the GRU). We use a bilinear similarity score si = q T Q ui , for this prediction where Q is a learned transformation in R2d×2d . This task is inspired by the antecedent ranking task in coreference (Wiseman et al., 2015, 2016). For Task 2 (L2 ) we train to predict the order index in which a named entity has been introduced. For example, in Figure 1,"
D18-1130,P16-1086,0,0.204686,"set, particularly on difficult entity examples. 1 Figure 1: A LAMBADA example where the final word “julie” (with reference chain in brackets) is the answer, y, to be predicted from the preceding context x. A system must know the two speakers and the current dialogue turn, simple context matching is not sufficient. Here, our model’s predictions before and after adding multi-task objective are shown. Introduction There has been tremendous interest over the past several years in Cloze-style (Taylor, 1953) reading comprehension tasks, datasets, and models (Hermann et al., 2015; Hill et al., 2016; Kadlec et al., 2016; Dhingra et al., 2016; Cui et al., 2016). Many of these systems apply neural models to learn to predict answers based on contextual matching, and have inspired other work in long-form generation and question answering. The extent and limits of these successes have also been a topic of interest (Chen et al., 2016; Chu et al., 2017). Recent analysis by Chu et al. (2017) suggests that a significant portion of the errors made by standard models, especially on the LAMBADA dataset (Paperno et al., 2016), derive from the inability to correctly track entities or speakers, or a failure to handle vario"
D18-1130,D16-1241,0,0.0442023,"This task is inspired by the antecedent ranking task in coreference (Wiseman et al., 2015, 2016). For Task 2 (L2 ) we train to predict the order index in which a named entity has been introduced. For example, in Figure 1, julie would be 1, amy would be 2, marsh would be 3, etc. The hope here is that learning to predict when entities reappear will help the model track their reoccurences. For the blue labeled julie, the model would aim to predict 1, even though it appears later in the context. This task is inspired by the One-Hot Pointer Reader of Wang et al. (2017) on the Who-didWhat dataset (Onishi et al., 2016). Formally, letc j ) be the predicted index for xj , we minting idx(x imize: c j ) = idx(xj ) |x1:j−1 ) L2 (θ) = − ln p(idx(x → = − ln softmax(W h j )idx(xj ) , where W ∈ R|E|×2d and E is the set of entity word types in the document. Note that this is a simpler computation, requiring only O(|E |× n) predictions per x, whereas L1 requires O(n2 ). The full model minimizes a multi-task loss: L0 (θ) + γ1 L1 (θ) + γ2 L2 (θ). Using L1 and L2 simultaneously did not lead to improved performance however, and so either γ1 , γ2 is always 0. We believe that this is because, while the learning objectives f"
D18-1130,P16-1144,0,0.0573718,"Missing"
D18-1130,D14-1162,0,0.101876,"edict, as well as the number of tokens of each type considered. 4 LAMBADA Experiments Methods This section highlights several aspects of our methodology; full hyperparameters are given in the Supplementary Material. For the training sets, we exclude examples where the answer is not in the context. The validation and test sets are not modified however and the model with the highest accuracy on the validation set is chosen for testing. For both tasks, the context words are mapped to learned embeddings; importantly, we initialize the first 100 dimensions with the 100dimensional GLOVE embeddings (Pennington et al., 2014). Named entity words are anonymized, as is done in the CNN/Daily Mail corpus (Hermann et al., 2015) and in some of the experiments of Wang et al. (2017). The model is regularized with dropout (Srivastava et al., 2014) and optimized with ADAM (Kingma and Ba, 2014). For all experiments we performed a random search over hyperparameter values (Bergstra and Bengio, 2012), and report the results of the models that performed best on the validation set. Our implementation is available at https://github.com/ harvardnlp/readcomp. Results and Discussion Table 2 shows the full results of our best models o"
D18-1130,D16-1013,0,0.0981622,"Missing"
D18-1130,P15-2115,0,0.0136332,"of features in Table 1 to augment the representation of each word in x. These features are meant to help the system to identify and use the relationships between words in the passage.1 Features 2-5 apply only to words tagged PERSON by the NER tagger. Features 6-7 apply only to words between opening and closing quotation marks. Feature 6 indicates the index of the quote in the document, and Feature 7 gives the assumed speaker of the quote using some simple rules; we provide the rules in the Supplementary Material. Though most of these features are novel, they are motivated by recent analysis (Wang et al., 2015; Chen et al., 2016; Wang et al., 2017). All features are incorporated into a word’s representation by embedding each discrete feature into a vector of the same size as the original word embedding, adding the vectors as well as a bias, and applying a tanh nonlinearity. Method 2: Multitasking We additionally encourage the neural model to keep track of entities by multitasking with simple auxiliary entitytracking tasks. Examples such as Figure 1 suggest that keeping track of which entities are currently in 1 POS tags are produced with the NLTK library (Bird et al., 2009), and NER tags with the S"
D18-1130,W17-2604,0,0.104134,"showed, however, that training only on examples where y is in x leads to improved overall performance, and we adopt this approach as well. Related Work The first popular neural network reading comprehension models were the Attentive Reader and its variant Impatient Reader (Hermann et al., 2015). Both were the first to use bidirectional LSTMs to encode the context paragraph and the query separately. The Stanford Reader (Chen et al., 2016) is a simpler version with fewer layers for inference. These models use an encoder to map each context token xi to a vector ui . Following the terminology of Wang et al. (2017), explicit reference models calculate a similarity measure si = s(ui , q) between each context vector ui and a query vector q derived for the masked word. These similarity scores are projected to an attention distribution α = softmax({si }) over the context positions in 1, . . . , n, which are taken to be candidate answers. The Attention Sum Reader (Kadlec et al., 2016) is a further simplified version. It computes ui and q with separate bidirectional GRU (Chung et al., 2014) networks, and si with a dot-product. It is trained to minimize: L0 (θ) = − ln p(y |x, q) X X = − ln p(xi |q) = − ln αi ,"
D18-1130,N16-1114,1,0.858676,"Missing"
D18-1130,P15-1137,1,0.837374,"ties are currently in 1 POS tags are produced with the NLTK library (Bird et al., 2009), and NER tags with the Stanford NER tagger (Finkel et al., 2005). We additionally found it useful to tag animate words as PERSONs on the CBT-NE data, using the animate word list of Bergsma and Lin (2006). → ui = h i , and the query q = h j−1 . (Note that unlike above, both of these only use the forward states of the GRU). We use a bilinear similarity score si = q T Q ui , for this prediction where Q is a learned transformation in R2d×2d . This task is inspired by the antecedent ranking task in coreference (Wiseman et al., 2015, 2016). For Task 2 (L2 ) we train to predict the order index in which a named entity has been introduced. For example, in Figure 1, julie would be 1, amy would be 2, marsh would be 3, etc. The hope here is that learning to predict when entities reappear will help the model track their reoccurences. For the blue labeled julie, the model would aim to predict 1, even though it appears later in the context. This task is inspired by the One-Hot Pointer Reader of Wang et al. (2017) on the Who-didWhat dataset (Onishi et al., 2016). Formally, letc j ) be the predicted index for xj , we minting idx(x"
D18-1356,P16-1014,0,0.0593699,"Missing"
D18-1356,W13-0113,0,0.0286323,"elieve the proposed methodology represents a compelling approach to learning discrete, latent-variable representations of conditional text. 2 Related Work A core task of NLG is to generate textual descriptions of knowledge base records. A common approach is to use hand-engineered templates (Kukich, 1983; McKeown, 1992; McRoy et al., 2000), but there has also been interest in creating templates in an automated manner. For instance, many authors induce templates by clustering sentences and then abstracting templated fields with hand-engineered rules (Angeli et al., 2010; Kondadadi et al., 2013; Howald et al., 2013), or with a pipeline of other automatic approaches (Wang and Cardie, 2013). There has also been work in incorporating probabilistic notions of templates into generation models (Liang et al., 2009; Konstas and Lapata, 2013), which is similar to our approach. However, these approaches have always been conjoined with discriminative classifiers or rerankers in order to actually accomplish the generation (Angeli et al., 2010; Konstas and Lapata, 2013). In addition, these models explicitly model knowledge base field selection, whereas the model we present is fundamentally an end-to-end model over ge"
D18-1356,P09-1011,0,0.750449,"descriptions of knowledge base records. A common approach is to use hand-engineered templates (Kukich, 1983; McKeown, 1992; McRoy et al., 2000), but there has also been interest in creating templates in an automated manner. For instance, many authors induce templates by clustering sentences and then abstracting templated fields with hand-engineered rules (Angeli et al., 2010; Kondadadi et al., 2013; Howald et al., 2013), or with a pipeline of other automatic approaches (Wang and Cardie, 2013). There has also been work in incorporating probabilistic notions of templates into generation models (Liang et al., 2009; Konstas and Lapata, 2013), which is similar to our approach. However, these approaches have always been conjoined with discriminative classifiers or rerankers in order to actually accomplish the generation (Angeli et al., 2010; Konstas and Lapata, 2013). In addition, these models explicitly model knowledge base field selection, whereas the model we present is fundamentally an end-to-end model over generation segments. Recently, a new paradigm has emerged around neural text generation systems based on machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015). Most"
D18-1356,W09-0613,0,0.0640977,"nd ability the records in Figure 2: ”frederick parker-rhodes (21 november 1914 - 2 march 1987) was an english mycology and plant pathology, mathematics at the university of uk.” In addition to not being fluent, it is unclear what the end of this sentence is even attempting to convey: it may be attempting to convey a fact not actually in the knowledge base (e.g., where Parker-Rhodes studied), or perhaps it is simply failing to fluently realize information that is in the knowledge base (e.g., ParkerRhodes’s country of residence). Traditional NLG systems (Kukich, 1983; McKeown, 1992; Belz, 2008; Gatt and Reiter, 2009), in contrast, largely avoid these problems. Since they typically employ an explicit planning component, which decides which knowledge base records to Parker-Rhodes (21ofMarch 1914 -Parker-Rhodes. 21 November FigureFrederick 1: Wikipedia infobox Frederick The focus on, and a surface realization component, 1987) was an English linguist, plant pathologist, computer introduction of his article reads: “Frederick Parker-Rhodes (21 which realizes the chosen records, the intent of the scientist, mathematician, mystic, and mycologist. system is always explicit, and it may be modified March 1914 – 21 N"
D18-1356,P16-1154,0,0.0590413,"rd-algorithm familiar from HMMs (Rabiner, 1989). It is actually more convenient to use the backward-algorithm formulation when using RNNs to parameterize the emission distributions, and we briefly review the backward recurrences here, again following Murphy (2002). We have: βt (j) = p(yt+1:T |zt = j, ft = 1, x) with parameters g k1 ∈ R2d and W ∈ RV ×2d . Note that there is a g k1 vector for each of K discrete states. To additionally implement a kind of slot filling, we allow emissions to be directly copied from the value portion of the records rj using copy attention (G¨ulc¸ehre et al., 2016; Gu et al., 2016; Yang et al., 2016). Define copy scores, = K X βt∗ (k) p(zt+1 = k |zt = j) k=1 βt∗ (k) = p(yt+1:T |zt+1 = k, ft = 1, x) L h X βt+l (k) p(lt+1 = l |zt+1 = k) = l=1 ρj = k rT j tanh(g 2 ◦ hki−1 ), i p(yt+1:t+l |zt+1 = k, lt+1 = l) , where g k2 ∈ Rd . We then normalize the outputvocabulary and copy scores together, to arrive at ei−1 = softmax([v i−1 , ρ1 , . . . , ρJ ]), v and thus with base case βT (j) = 1. We can now obtain theP marginal probability of y as ∗ where we p(y |x) = K k=1 β0 (k) p(z1 = k), have used the fact that f0 must be 1, and we therefore train to maximize the log-marginal lik"
D18-1356,D15-1166,0,0.0917572,"ead to states that specialize to specific emission lengths. 3177 where &lt;/seg&gt; is an end of segment token. The RNN decoder uses attention and copy-attention over the embedded records r j , and is conditioned on zt = k by concatenating an embedding corresponding to the k’th latent state to the RNN’s input; the RNN is also conditioned on the entire x by initializing its hidden state with xa . More concretely, let hki−1 ∈ Rd be the state of an RNN conditioned on x and zt = k (as above) run over the sequence yt−lt +1:t−lt +i−1 . We let the model attend over records r j using hki−1 (in the style of Luong et al. (2015)), producing a context vector cki−1 . We may then obtain scores v i−1 for each word in the output vocabulary, v i−1 = W tanh(g k1 ◦ [hki−1 , cki−1 ]), 5.2 Learning The model requires fitting a large set of neural network parameters. Since we assume z, l, and f are unobserved, we marginalize over these variables to maximize the log marginal-likelihood of the observed tokens y given x. The HSMM marginal-likelihood calculation can be carried out efficiently with a dynamic program analogous to either the forward- or backward-algorithm familiar from HMMs (Rabiner, 1989). It is actually more conveni"
D18-1356,W00-1437,0,0.518905,"iments indicate that we can induce explicit templates (as shown in Figure 1) while achieving competitive automatic scores, and that we can control and interpret our generations by manipulating these templates. Finally, while our experiments focus on the data-to-text regime, we believe the proposed methodology represents a compelling approach to learning discrete, latent-variable representations of conditional text. 2 Related Work A core task of NLG is to generate textual descriptions of knowledge base records. A common approach is to use hand-engineered templates (Kukich, 1983; McKeown, 1992; McRoy et al., 2000), but there has also been interest in creating templates in an automated manner. For instance, many authors induce templates by clustering sentences and then abstracting templated fields with hand-engineered rules (Angeli et al., 2010; Kondadadi et al., 2013; Howald et al., 2013), or with a pipeline of other automatic approaches (Wang and Cardie, 2013). There has also been work in incorporating probabilistic notions of templates into generation models (Liang et al., 2009; Konstas and Lapata, 2013), which is similar to our approach. However, these approaches have always been conjoined with disc"
D18-1443,N18-1150,0,0.49744,"equires a larger beam size and set it to 10. The coverage penalty parameter β is set to 10, and the copy attention normalization parameter λ to 2 for both approaches. We use AllenNLP (Gardner et al., 2018) for the content selector, and the abstractive models are implemented in OpenNMT-py (Klein et al., 2017).3 . 3 Code and reproduction instructions can be found at https://github.com/sebastianGehrmann/ bottom-up-summary 3 These results compare on the non-anonymized version of this corpus used by (See et al., 2017). The best results on the anonymized version are R1:41.69 R2:19.47 RL:37.92 from (Celikyilmaz et al., 2018). We compare to their DCA model on the NYT corpus. 7 Results Table 1 shows our main results on the CNN-DM corpus, with abstractive models shown in the top, and bottom-up attention methods at the bottom. We first observe that using a coverage inference penalty scores the same as a full coverage mechanism, without requiring any additional model parameters. We found that none of our end-to-end models lead to improvements, indicating that it is difficult to apply the masking during training without hurting the training process. The Mask Only model with increased supervision on the copy mechanism p"
D18-1443,P18-1063,0,0.229299,"ive model, biased with a powerful content selector. Other recent work explores alternative approaches to content selection. For example, Cohan et al. (2018) use a hierarchical attention to detect relevant sections in a document, Li et al. (2018a) generate a set of keywords that is used to guide the summarization process, and Pasunuru and Bansal (2018) develop a loss-function based on whether salient keywords are included in a summary. Other approaches investigate the content-selection at the sentence-level. Tan et al. (2017) describe a graphbased attention to attend to one sentence at a time, Chen and Bansal (2018) first extract full sentences from a document and then compress them, and Hsu et al. (2018) modulate the attention based on how likely a sentence is included in a summary. 3 Background: Neural Summarization Throughout this paper, we consider a set of pairs of texts (X , Y) where x ∈ X corresponds to source tokens x1 , . . . , xn and y ∈ Y to a summary y1 , . . . , ym with m  n. Abstractive summaries are generated one word at a time. At every time-step, a model is aware of the previously generated words. The problem is to learn a function f (x) parametrized by θ that maximizes the probability"
D18-1443,P18-1013,0,0.170041,"aches to content selection. For example, Cohan et al. (2018) use a hierarchical attention to detect relevant sections in a document, Li et al. (2018a) generate a set of keywords that is used to guide the summarization process, and Pasunuru and Bansal (2018) develop a loss-function based on whether salient keywords are included in a summary. Other approaches investigate the content-selection at the sentence-level. Tan et al. (2017) describe a graphbased attention to attend to one sentence at a time, Chen and Bansal (2018) first extract full sentences from a document and then compress them, and Hsu et al. (2018) modulate the attention based on how likely a sentence is included in a summary. 3 Background: Neural Summarization Throughout this paper, we consider a set of pairs of texts (X , Y) where x ∈ X corresponds to source tokens x1 , . . . , xn and y ∈ Y to a summary y1 , . . . , ym with m  n. Abstractive summaries are generated one word at a time. At every time-step, a model is aware of the previously generated words. The problem is to learn a function f (x) parametrized by θ that maximizes the probability of generating the correct sequences. Following previous work, we model the abstractive summ"
D18-1443,D16-1053,0,0.0339308,"Missing"
D18-1443,P16-1046,0,0.290748,"Missing"
D18-1443,N16-1012,1,0.890482,"Missing"
D18-1443,P17-4012,1,0.859162,"Missing"
D18-1443,N18-2097,0,0.0806982,"eate a new source document comprised of the important sentences from the source and then train an abstractive system. Liu et al. (2018) describe an extractive phase that extracts full paragraphs and an abstractive one that determines their order. Finally Zeng et al. (2016) introduce a mechanism that reads a source document in two passes and uses the information from the first pass to bias the second. Our method differs in that we utilize a completely abstractive model, biased with a powerful content selector. Other recent work explores alternative approaches to content selection. For example, Cohan et al. (2018) use a hierarchical attention to detect relevant sections in a document, Li et al. (2018a) generate a set of keywords that is used to guide the summarization process, and Pasunuru and Bansal (2018) develop a loss-function based on whether salient keywords are included in a summary. Other approaches investigate the content-selection at the sentence-level. Tan et al. (2017) describe a graphbased attention to attend to one sentence at a time, Chen and Bansal (2018) first extract full sentences from a document and then compress them, and Hsu et al. (2018) modulate the attention based on how likely"
D18-1443,W03-0501,0,0.239412,"Missing"
D18-1443,N18-2009,0,0.164765,"Missing"
D18-1443,P16-1188,0,0.176963,"Missing"
D18-1443,K16-1028,0,0.419013,"Missing"
D18-1443,W18-2501,0,0.0184561,"Missing"
D18-1443,P16-1154,0,0.0740725,"s little as 1,000 sentences, making it easy to transfer a trained summarizer to a new domain. 1 Introduction Text summarization systems aim to generate natural language summaries that compress the information in a longer text. Approaches using neural networks have shown promising results on this task with end-to-end models that encode a source document and then decode it into an abstractive summary. Current state-of-the-art neural abstractive summarization models combine extractive and abstractive techniques by using pointergenerator style models which can copy words from the source document (Gu et al., 2016; See et al., 2017). These end-to-end models produce fluent abstractive summaries but have had mixed success in content selection, i.e. deciding what to summarize, compared to fully extractive models. There is an appeal to end-to-end models from a modeling perspective; however, there is evidence that when summarizing people follow a two-step Figure 1: Example of two sentence summaries with and without bottom-up attention. The model does not allow copying of words in [gray], although it can generate words. With bottom-up attention, we see more explicit sentence compression, while without it who"
D18-1443,N18-2102,0,0.0613687,"aphs and an abstractive one that determines their order. Finally Zeng et al. (2016) introduce a mechanism that reads a source document in two passes and uses the information from the first pass to bias the second. Our method differs in that we utilize a completely abstractive model, biased with a powerful content selector. Other recent work explores alternative approaches to content selection. For example, Cohan et al. (2018) use a hierarchical attention to detect relevant sections in a document, Li et al. (2018a) generate a set of keywords that is used to guide the summarization process, and Pasunuru and Bansal (2018) develop a loss-function based on whether salient keywords are included in a summary. Other approaches investigate the content-selection at the sentence-level. Tan et al. (2017) describe a graphbased attention to attend to one sentence at a time, Chen and Bansal (2018) first extract full sentences from a document and then compress them, and Hsu et al. (2018) modulate the attention based on how likely a sentence is included in a summary. 3 Background: Neural Summarization Throughout this paper, we consider a set of pairs of texts (X , Y) where x ∈ X corresponds to source tokens x1 , . . . , xn"
D18-1443,D14-1162,0,0.0817648,"possible subsequence of tokens s = xi−j:i:i+k , for integers j ≤ i; k ≤ (n − i), if s ∈ x and s ∈ y, and (2) there exists no earlier sequence u with s = u. We use a standard bidirectional LSTM model trained with maximum likelihood for the sequence labeling problem. Recent results have shown that better word representations can lead to significantly improved performance in sequence tagging tasks (Peters et al., 2017). Therefore, we first map each token wi into two embedding channels (w) embedding represents a e(w) and e(c) i i . The e static channel of pre-trained word embeddings, e.g. GLoVE (Pennington et al., 2014). The e(c) are contextual embeddings from a pretrained language model, e.g. ELMo (Peters et al., 2018) which uses a character-aware token embedding (Kim et al., 2016) followed by two bidirectional LSTM lay(1) (2) ers hi and hi . The contextual embeddings are fine-tuned to learn a task-specific embedding e(c) i as a linear combination of the states of each LSTM layer and the token embedding, e(c) i =γ× 2 X (`) sj × hi , ( p(aij |x, y1:j−1 ) qi &gt;  p(˜ aij |x, y1:j−1 ) = 0 ow. To ensure that Eq. 1 still yields a correct probability distribution, we first multiply p(˜ aj |x, y1:j−1 ) by a normali"
D18-1443,P17-1161,0,0.00965293,"otherwise. While there is no supervised data for this task, we can generate training data by aligning the summaries to the document. We define a word xi as 4100 copied if (1) it is part of the longest possible subsequence of tokens s = xi−j:i:i+k , for integers j ≤ i; k ≤ (n − i), if s ∈ x and s ∈ y, and (2) there exists no earlier sequence u with s = u. We use a standard bidirectional LSTM model trained with maximum likelihood for the sequence labeling problem. Recent results have shown that better word representations can lead to significantly improved performance in sequence tagging tasks (Peters et al., 2017). Therefore, we first map each token wi into two embedding channels (w) embedding represents a e(w) and e(c) i i . The e static channel of pre-trained word embeddings, e.g. GLoVE (Pennington et al., 2014). The e(c) are contextual embeddings from a pretrained language model, e.g. ELMo (Peters et al., 2018) which uses a character-aware token embedding (Kim et al., 2016) followed by two bidirectional LSTM lay(1) (2) ers hi and hi . The contextual embeddings are fine-tuned to learn a task-specific embedding e(c) i as a linear combination of the states of each LSTM layer and the token embedding, e("
D18-1443,N18-1202,0,0.0145239,"(2) there exists no earlier sequence u with s = u. We use a standard bidirectional LSTM model trained with maximum likelihood for the sequence labeling problem. Recent results have shown that better word representations can lead to significantly improved performance in sequence tagging tasks (Peters et al., 2017). Therefore, we first map each token wi into two embedding channels (w) embedding represents a e(w) and e(c) i i . The e static channel of pre-trained word embeddings, e.g. GLoVE (Pennington et al., 2014). The e(c) are contextual embeddings from a pretrained language model, e.g. ELMo (Peters et al., 2018) which uses a character-aware token embedding (Kim et al., 2016) followed by two bidirectional LSTM lay(1) (2) ers hi and hi . The contextual embeddings are fine-tuned to learn a task-specific embedding e(c) i as a linear combination of the states of each LSTM layer and the token embedding, e(c) i =γ× 2 X (`) sj × hi , ( p(aij |x, y1:j−1 ) qi &gt;  p(˜ aij |x, y1:j−1 ) = 0 ow. To ensure that Eq. 1 still yields a correct probability distribution, we first multiply p(˜ aj |x, y1:j−1 ) by a normalization parameter λ and then renormalize the distribution. The resulting normalized distribution can be"
D18-1443,1983.tc-1.13,0,0.18547,"Missing"
D18-1443,P18-1061,0,0.160482,"Missing"
D18-1443,D15-1044,1,0.871736,"Missing"
D18-1443,P17-1099,0,0.253567,"sentences, making it easy to transfer a trained summarizer to a new domain. 1 Introduction Text summarization systems aim to generate natural language summaries that compress the information in a longer text. Approaches using neural networks have shown promising results on this task with end-to-end models that encode a source document and then decode it into an abstractive summary. Current state-of-the-art neural abstractive summarization models combine extractive and abstractive techniques by using pointergenerator style models which can copy words from the source document (Gu et al., 2016; See et al., 2017). These end-to-end models produce fluent abstractive summaries but have had mixed success in content selection, i.e. deciding what to summarize, compared to fully extractive models. There is an appeal to end-to-end models from a modeling perspective; however, there is evidence that when summarizing people follow a two-step Figure 1: Example of two sentence summaries with and without bottom-up attention. The model does not allow copying of words in [gray], although it can generate words. With bottom-up attention, we see more explicit sentence compression, while without it whole sentences are co"
D18-1443,P17-1108,0,0.193339,"Missing"
D18-1443,P16-1008,0,0.0653314,"Missing"
D18-3004,N15-1144,0,\N,Missing
D18-3004,W10-2902,0,\N,Missing
D18-3004,N10-1083,0,\N,Missing
D18-3004,J93-2003,0,\N,Missing
D18-3004,N06-1041,0,\N,Missing
D18-3004,N09-1012,0,\N,Missing
D18-3004,P11-1087,0,\N,Missing
D18-3004,D10-1056,0,\N,Missing
D18-3004,J94-2001,0,\N,Missing
D18-3004,J92-4003,0,\N,Missing
D18-3004,P05-1044,0,\N,Missing
D18-3004,P04-1061,0,\N,Missing
D18-3004,W15-1509,0,\N,Missing
D18-3004,D11-1118,0,\N,Missing
D18-3004,N16-1024,0,\N,Missing
D18-3004,D16-1137,1,\N,Missing
D18-3004,Q16-1018,0,\N,Missing
D18-3004,W16-5907,0,\N,Missing
D18-3004,W16-5901,0,\N,Missing
D18-3004,D16-1073,0,\N,Missing
D18-3004,C16-1133,0,\N,Missing
D18-3004,D17-1230,0,\N,Missing
D18-3004,N18-1122,0,\N,Missing
D18-3004,N18-1049,0,\N,Missing
D18-3004,D17-1070,0,\N,Missing
D18-3004,W17-2629,0,\N,Missing
D18-3004,D17-1065,0,\N,Missing
D18-3004,N18-1202,0,\N,Missing
D18-3004,D18-1480,0,\N,Missing
D18-3004,D18-1065,0,\N,Missing
D18-3004,P18-1070,0,\N,Missing
D18-3004,K16-1002,0,\N,Missing
D18-3004,W16-0106,0,\N,Missing
D18-3004,D16-1138,0,\N,Missing
D19-1109,W18-1002,0,0.424074,"efined edges representing the nature of the relations between concepts (IsA, UsedFor, CapableOf, etc.). Commonsense knowledge base completion (CKBC) is a machine learning task motivated by the need to improve the coverage of these resources. In this formulation of the problem, one is supplied with a list of candidate entityrelation-entity triples, and the task is to distinguish which of the triples express valid commonsense knowledge and which are fictitious (Li et al., 2016). Several approaches have been proposed for training models for commonsense knowledge base completion (Li et al., 2016; Jastrzebski et al., 2018). Each of these approaches uses some sort of supervised training on a particular knowledge base, evaluating the model’s performance on a held-out test set from the same database. These works use relations from ConceptNet, a crowd-sourced database of structured commonsense knowledge, to train and validate their models (Liu and Singh, 2004). However, it has been shown that these methods generalize poorly to novel data (Li et al., 2016; Jastrzebski et al., 2018). Jastrzebski et al. (2018) demonstrated that much of the data in the ConceptNet test set were simply rephrased relations from the traini"
D19-1109,K18-1014,0,0.0343323,") use unidirectional language models for CKBC, but their approach requires a supervised training step. Our approach differs in that we intentionally avoid training on any particular database, relying instead on the language model’s general world knowledge. Additionally, we use a bidirectional masked model which provides a more flexible framework for likelihood estimation and allows us to estimate point-wise mutual information. Although it is beyond the scope of this paper, it would be interesting to adapt the methods presented here for the related task of generating new commonsense knowledge (Saito et al., 2018). 2 Method Given a commonsense head-relation-tail triple x = (h, r, t), we are interested in determining the validity of that tuple as a representation of a commonsense fact. Specifically, we would like to determine a numeric score y ∈ R reflecting our confidence that a given tuple represents true knowledge. We assume that heads and tails are arbitrarylength sequences of words in a vocabulary V so that h = {h1 , h2 , . . . , hn } and t = {t1 , t2 , . . . , tm }. We further assume that we have a known set of possible relations R so that r ∈ R. The goal is to determine a function f that maps rel"
D19-1109,K17-1004,0,0.0311911,"cular, we use a masked language model to estimate point-wise mutual information between entities in a possible relation, an approach that differs significantly from fine-tuning approaches used for other language modeling tasks. Since the weights of the model are fixed, our approach is not biased by the coverage of any one dataset. As we might expect, our method underperforms when compared to previous benchmarks on the ConceptNet common sense triples dataset (Li et al., 2016), but demonstrates a superior ability to generalize when mining novel commonsense knowledge from Wikipedia. Related Work Schwartz et al. (2017) and Trinh and Le (2018) demonstrate a similar approach to using language models for tasks requiring commonsense, such as the Story Cloze Task and the Winograd Schema Challenge, respectively (Mostafazadeh et al., 2016; Levesque et al., 2012). Bosselut et al. (2019) and Trinh and Le (2019) use unidirectional language models for CKBC, but their approach requires a supervised training step. Our approach differs in that we intentionally avoid training on any particular database, relying instead on the language model’s general world knowledge. Additionally, we use a bidirectional masked model which"
D19-1109,speer-havasi-2012-representing,0,0.0573924,". (2016). Jastrzebski et al. (2018) introduce FAC TORIZED and P ROTOTYPICAL models. The Factorized model embeds the head, relation, and tail in a vector space and then produces a score by taking a linear combination of the inner products between each pair of embeddings. The PrototypiTask 1: Commonsense Knowledge Base Completion Our experimental setup follows Li et al. (2016), evaluating our model with their test set (n = 2400) containing an equal number of valid and invalid triples. The valid triples are from the crowd-sourced Open Mind Common Sense (OMCS) entries in the ConceptNet 5 dataset (Speer and Havasi, 2012). Invalid triples are generated by replacing an element of a valid tuple with another randomly selected element. We use our scoring method to classify each tuple as valid or invalid. To this end, we use our method to assign a score to each tuple and then group the resulting scores into two clusters. Instances in the cluster with the higher mean PMI are labeled as valid, and the remainder are labeled as invalid. We use expectation-maximization with a mixture of Gaussians to cluster. We also tune the PMI weight via grid search over 90 points from λ ∈ [0.5, 5.], using the Akaike information crite"
D19-1109,D15-1174,0,0.0202182,"ese works use relations from ConceptNet, a crowd-sourced database of structured commonsense knowledge, to train and validate their models (Liu and Singh, 2004). However, it has been shown that these methods generalize poorly to novel data (Li et al., 2016; Jastrzebski et al., 2018). Jastrzebski et al. (2018) demonstrated that much of the data in the ConceptNet test set were simply rephrased relations from the training set, and that this train-test set leakage led to artificially inflated test performance metrics. This problem of traintest leakage is typical in knowledge base completion tasks (Toutanova et al., 2015; Dettmers et al., 2018). Instead of training a predictive model on any specific database, we attempt to utilize the world knowledge of large language models to identify commonsense facts directly. By constructing a candidate piece of knowledge as a sentence, we can use a language model to approximate the likelihood of this text as a proxy for its truthfulness. 1173 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1173–1178, c Hong Kong, China, November 3–7, 2019. 2019 Associa"
D19-1109,P16-1137,0,0.352082,"dge bases are represented as a graph, with nodes consisting of conceptual entities (i.e. dog, running away, excited, etc.) and the pre-defined edges representing the nature of the relations between concepts (IsA, UsedFor, CapableOf, etc.). Commonsense knowledge base completion (CKBC) is a machine learning task motivated by the need to improve the coverage of these resources. In this formulation of the problem, one is supplied with a list of candidate entityrelation-entity triples, and the task is to distinguish which of the triples express valid commonsense knowledge and which are fictitious (Li et al., 2016). Several approaches have been proposed for training models for commonsense knowledge base completion (Li et al., 2016; Jastrzebski et al., 2018). Each of these approaches uses some sort of supervised training on a particular knowledge base, evaluating the model’s performance on a held-out test set from the same database. These works use relations from ConceptNet, a crowd-sourced database of structured commonsense knowledge, to train and validate their models (Liu and Singh, 2004). However, it has been shown that these methods generalize poorly to novel data (Li et al., 2016; Jastrzebski et al"
D19-1109,N16-1098,0,0.0499637,"deling tasks. Since the weights of the model are fixed, our approach is not biased by the coverage of any one dataset. As we might expect, our method underperforms when compared to previous benchmarks on the ConceptNet common sense triples dataset (Li et al., 2016), but demonstrates a superior ability to generalize when mining novel commonsense knowledge from Wikipedia. Related Work Schwartz et al. (2017) and Trinh and Le (2018) demonstrate a similar approach to using language models for tasks requiring commonsense, such as the Story Cloze Task and the Winograd Schema Challenge, respectively (Mostafazadeh et al., 2016; Levesque et al., 2012). Bosselut et al. (2019) and Trinh and Le (2019) use unidirectional language models for CKBC, but their approach requires a supervised training step. Our approach differs in that we intentionally avoid training on any particular database, relying instead on the language model’s general world knowledge. Additionally, we use a bidirectional masked model which provides a more flexible framework for likelihood estimation and allows us to estimate point-wise mutual information. Although it is beyond the scope of this paper, it would be interesting to adapt the methods presen"
D19-1115,P19-1422,0,0.31975,"with, without their knowledge or consent. though it’s been claimed by others that kim il sang fled north korea in 1940 to escape the reprisals, even that is disputed by k Table 1: Steganography example. Two different encoded messages are produced given the same introductory context. The messages are first converted into bit strings and then mapped to cover text using the arithmetic steganography approach described in Section 4.4. is most important; but are susceptible to machinebased eavesdroppers which can in principle identify statistical patterns in generations. Concurrent with this work, Dai and Cai (2019) conduct a related analysis in terms of KL with a modified Huffman algorithm. Experiments consider the distribution of KL values, although no human evaluation is performed. 3 Arithmetic coding Arithmetic coding is a data compression method designed specifically to code strings of elements with a known probability distribution (Rissanen and Langdon, 1979; Zoph et al., 2015). For long strings the coding is optimal; it compresses information to its entropy (Rissanen and Langdon, 1979). In practice, it is often more efficient than Huffman coding because it does not require blocking. Arithmetic cod"
D19-1115,P17-3017,0,0.21,"Missing"
D19-1115,D15-1105,0,0.0254969,"arithmetic steganography approach described in Section 4.4. is most important; but are susceptible to machinebased eavesdroppers which can in principle identify statistical patterns in generations. Concurrent with this work, Dai and Cai (2019) conduct a related analysis in terms of KL with a modified Huffman algorithm. Experiments consider the distribution of KL values, although no human evaluation is performed. 3 Arithmetic coding Arithmetic coding is a data compression method designed specifically to code strings of elements with a known probability distribution (Rissanen and Langdon, 1979; Zoph et al., 2015). For long strings the coding is optimal; it compresses information to its entropy (Rissanen and Langdon, 1979). In practice, it is often more efficient than Huffman coding because it does not require blocking. Arithmetic coding traditionally maps a string of elements to a uniformly distributed binary string. To use such a coding for steganography we reverse the order: first a (uniformly sampled) message is selected, then the message is mapped to a sequence (words). The coding scheme is demonstrated in Figure 2. In this work the probability distribution comes from the conditional distributions"
D19-1115,K16-1028,0,0.0427648,"ion: we modulate the LM distribution by a temperature τ , and we truncate the distribution to a top-k tokens at each position to ensure generations do not include tokens from the long tail. In practice, a naive implementation of arithmetic coding quickly runs into precision limitations. We use a common equivalent variant based on fixed precision binary fractions (Rubin, 1979). 4 4.1 Experiments and results Experimental Setup We use the 345M parameter GPT-2 model as our language model pLM (Radford et al., 2019). We run our experiments on the CNN/Dailymail (CNNDM) dataset (Hermann et al., 2015; Nallapati et al., 2016), which is used to provide context for generation1 . We take the first three sentences of each news article as the context on which to condition our language model, and use the indicated steganography algorithm to generate an entire sentence for estimation or evaluation given a uniform random message. We compare the proposed arithmetic coding-based algorithm with the Block (Fang et al., 2017) and Huffman (Yang et al., 2019) methods as baselines. The baselines are re1 Conditional generation makes it more challenging to fool humans and is thus better for comparisons. CNNDM presents an additional"
N12-1054,C10-1007,0,0.491007,"ting the relationship to greedy transition-based dependency parsers that are also linear-time (Nivre et al., 2004) or quadratic-time (Yamada and Matsumoto, 2003). It is their success that motivates building explicitly trained, linear-time pruning models. However, while a greedy solution for arc-standard transition-based parsers can be computed in linear-time, Kuhlmann et al. (2011) recently showed that computing exact solutions or (max-)marginals has time complexity O(n4 ), making these models inappropriate for coarse-to-fine style pruning. As an alternative, Roark and Hollingshead (2008) and Bergsma and Cherry (2010) present approaches where individual classifiers are used to prune chart cells. Such approaches have the drawback that pruning decisions are made locally and therefore can rule out all valid structures, despite explicitly evaluating O(n2 ) chart cells. In contrast, we make pruning decisions based on global parse max-marginals using a vine pruning pass, which is linear in the sentence length, but nonetheless guarantees to preserve a valid parse structure. 0.4 0.3 0.2 0.1 1 2 3 4 5 6 7 8 9 modifier index (a) 0.0 1 2 3 4 5 6 dependency length (b) Figure 1: (a) Heat map indicating how likely a par"
N12-1054,W06-2920,0,0.155287,"ing cascade with a wide range of common pruning methods on the Penn WSJ Treebank (PTB) (Marcus et al., 1993). We then also show that vine pruning is effective across a variety of different languages. For English, we convert the PTB constituency trees to dependencies using the Stanford dependency framework (De Marneffe et al., 2006). We then train on the standard PTB split with sections 2-21 as training, section 22 as validation, and section 23 as test. Results are similar using the Yamada and Matsumoto (2003) conversion. We additionally selected six languages from the CoNLL-X shared task 504 (Buchholz and Marsi, 2006) that cover a number of different language families: Bulgarian, Chinese, Japanese, German, Portuguese, and Swedish. We use the standard CoNLL-X training/test split and tune parameters with cross-validation. All experiments use unlabeled dependencies for training and test. Accuracy is reported as unlabeled attachment score (UAS), the percentage of tokens with the correct head word. For English, UAS ignores punctuation tokens and the test set uses predicted POS tags. For the other languages we follow the CoNLL-X setup and include punctuation in UAS and use gold POS tags on the set set. Speedups"
N12-1054,W08-2102,0,0.0452562,"work, we present a multipass coarse-to-fine architecture for graph-based dependency parsing. We start with a linear-time vine pruning pass and build up to higher-order models, achieving speed-ups of two orders of magnitude while maintaining state-of-the-art accuracies. In constituency parsing, exhaustive inference for all but the simplest grammars tends to be prohibitively slow. Consequently, most high-accuracy constituency parsers routinely employ a coarse grammar to prune dynamic programming chart cells ∗ Research conducted at Google. of the final grammar of interest (Charniak et al., 2006; Carreras et al., 2008; Petrov, 2009). While there are no strong theoretical guarantees for these approaches,1 in practice one can obtain significant speed improvements with minimal loss in accuracy. This benefit comes primarily from reducing the large grammar constant |G |that can dominate the runtime of the cubic-time CKY inference algorithm. Dependency parsers on the other hand do not have a multiplicative grammar factor |G|, and until recently were considered efficient enough for exhaustive inference. However, the increased model complexity of a third-order parser forced Koo and Collins (2010) to prune with a f"
N12-1054,D07-1101,0,0.227812,"unary head or modifier token, as well as features for the POS tag bordering the cutoff and the direction of the arc. 5.2 5.3 Z HANG N IVRE an unlabeled reimplementation of the linear-time, k-best, transition-based parser of Zhang and Nivre (2011). This parser uses composite features up to third-order with a greedy decoding algorithm. The reimplementation is about twice as fast as their reported speed, but scores slightly lower. Features For the non-pruning models, we use a standard set of features proposed in the discriminative graphbased dependency parsing literature (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). 3 For the first-order parser, we found it beneficial to employ a reduced feature first-order pruner before the final model, i.e. the cascade has four rounds: dictionary, vine, first-order pruning, and first-order 1-best. 505 Results A comparison between the pruning methods is shown in Table 1. The table gives relative speedups, compared to the unpruned first-order baseline, as well as accuracy, pruning efficiency, and oracle scores. Note particularly that the third-order cascade is twice as fast as an unpruned first-order model and >200 times faster than the unpruned"
N12-1054,N06-1022,0,0.0323638,"omplex models. In this work, we present a multipass coarse-to-fine architecture for graph-based dependency parsing. We start with a linear-time vine pruning pass and build up to higher-order models, achieving speed-ups of two orders of magnitude while maintaining state-of-the-art accuracies. In constituency parsing, exhaustive inference for all but the simplest grammars tends to be prohibitively slow. Consequently, most high-accuracy constituency parsers routinely employ a coarse grammar to prune dynamic programming chart cells ∗ Research conducted at Google. of the final grammar of interest (Charniak et al., 2006; Carreras et al., 2008; Petrov, 2009). While there are no strong theoretical guarantees for these approaches,1 in practice one can obtain significant speed improvements with minimal loss in accuracy. This benefit comes primarily from reducing the large grammar constant |G |that can dominate the runtime of the cubic-time CKY inference algorithm. Dependency parsers on the other hand do not have a multiplicative grammar factor |G|, and until recently were considered efficient enough for exhaustive inference. However, the increased model complexity of a third-order parser forced Koo and Collins ("
N12-1054,W02-1001,0,0.0384352,"UAS 93.3 93.3 93.1 93.1 93.1 93.1 93.1 93.1 92.7 Table 1: Results comparing pruning methods on PTB Section 22. Oracle is the max achievable UAS after pruning. Pruning efficiency (PE) is the percentage of non-gold first-order dependency arcs pruned. Speed is parsing time relative to the unpruned first-order model (around 2000 tokens/sec). UAS is the unlabeled attachment score of the final parses. 4.3 1-Best Training For the final pass, we want to train the model for 1best output. Several different learning methods are available for structured prediction models including structured perceptron (Collins, 2002), max-margin models (Taskar et al., 2003), and log-linear models (Lafferty et al., 2001). In this work, we use the margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al., 2006) with a hamming-loss margin. MIRA is an online algorithm with similar benefits as structured perceptron in terms of simplicity and fast training time. In practice, we found that MIRA with hamming-loss margin gives a performance improvement over structured perceptron and structured SVM. 5 Parsing Experiments To empirically demonstrate the effectiveness of our approach, we compare our vine prunin"
N12-1054,de-marneffe-etal-2006-generating,0,0.00809156,"Missing"
N12-1054,W05-1504,0,0.681294,"r forced Koo and Collins (2010) to prune with a first-order model in order to make inference practical. While fairly effective, all these approaches are limited by the fact that inference in the coarse model remains cubic in the sentence length. The desire to parse vast amounts of text necessitates more efficient dependency parsing algorithms. We thus propose a multi-pass coarse-to-fine approach where the initial pass is a linear-time sweep, which tries to resolve local ambiguities, but leaves arcs beyond a fixed length b unspecified (Section 3). The dynamic program is a form of vine parsing (Eisner and Smith, 2005), which we use to compute parse max-marginals, rather than for finding the 1best parse tree. To reduce pruning errors, the parameters of the vine parser (and all subsequent pruning models) are trained using the structured prediction cascades of Weiss and Taskar (2010) to optimize for pruning efficiency, and not for 1-best prediction (Section 4). Despite a limited scope of b = 3, the 1 This is in contrast to optimality preserving methods such as A* search, which typically do not provide sufficient speed-ups (Pauls and Klein, 2009). 498 2012 Conference of the North American Chapter of the Associ"
N12-1054,P10-1110,0,0.0481801,"Missing"
N12-1054,P10-1001,0,0.784373,"niak et al., 2006; Carreras et al., 2008; Petrov, 2009). While there are no strong theoretical guarantees for these approaches,1 in practice one can obtain significant speed improvements with minimal loss in accuracy. This benefit comes primarily from reducing the large grammar constant |G |that can dominate the runtime of the cubic-time CKY inference algorithm. Dependency parsers on the other hand do not have a multiplicative grammar factor |G|, and until recently were considered efficient enough for exhaustive inference. However, the increased model complexity of a third-order parser forced Koo and Collins (2010) to prune with a first-order model in order to make inference practical. While fairly effective, all these approaches are limited by the fact that inference in the coarse model remains cubic in the sentence length. The desire to parse vast amounts of text necessitates more efficient dependency parsing algorithms. We thus propose a multi-pass coarse-to-fine approach where the initial pass is a linear-time sweep, which tries to resolve local ambiguities, but leaves arcs beyond a fixed length b unspecified (Section 3). The dynamic program is a form of vine parsing (Eisner and Smith, 2005), which"
N12-1054,D10-1125,1,0.534942,"Missing"
N12-1054,P11-1068,0,0.019053,"Missing"
N12-1054,J93-2004,0,0.042341,"1). In this work, we use the margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al., 2006) with a hamming-loss margin. MIRA is an online algorithm with similar benefits as structured perceptron in terms of simplicity and fast training time. In practice, we found that MIRA with hamming-loss margin gives a performance improvement over structured perceptron and structured SVM. 5 Parsing Experiments To empirically demonstrate the effectiveness of our approach, we compare our vine pruning cascade with a wide range of common pruning methods on the Penn WSJ Treebank (PTB) (Marcus et al., 1993). We then also show that vine pruning is effective across a variety of different languages. For English, we convert the PTB constituency trees to dependencies using the Stanford dependency framework (De Marneffe et al., 2006). We then train on the standard PTB split with sections 2-21 as training, section 22 as validation, and section 23 as test. Results are similar using the Yamada and Matsumoto (2003) conversion. We additionally selected six languages from the CoNLL-X shared task 504 (Buchholz and Marsi, 2006) that cover a number of different language families: Bulgarian, Chinese, Japanese,"
N12-1054,E06-1011,0,0.34821,"]l+1 0 , s ∈ [n] where k + 1 is the sibling order, l + 1 is the parent order, and k + l + 1 is the model order. The canonical second-order model uses I1,0 , which has a cardinality of O(n3 ). Although there are several possibilities for higher-order models, we use I1,1 as our third-order model. Generally, the parsing index set has cardinality |Ik,l |= O(n2+k+l ). Inference in higher-order models uses variants of the dynamic program for first-order parsing, and we refer to previous work for the full set of rules. For second-order models with index set I1,0 , parsing can be done in O(n3 ) time (McDonald and Pereira, 2006) and for third-order models in O(n4 ) time (Koo and Collins, 2010). Even though second-order parsing has the same asymptotic time complexity as first-order parsing, inference is significantly slower due to the cost of scoring the larger index set. We aim to prune the index set, by mapping each higher-order index down to a set of small set indices that can be pruned using a coarse pruning model. For example, to use a first-order model for pruning, we would map the higher-order index to the individual indices for its arc, grandparents, and siblings: * pk,l→1 (g, s) = {(g1 , sj ) : j ∈ [k + 1]} M"
N12-1054,P05-1012,0,0.251695,"a first-order model, since these two models tend to often agree. Included are lexical features, part-of-speech features, features on in-between tokens, as well as feature conjunctions, surrounding part-of-speech tags, and back-off features. In addition, we replicate each part-of-speech (POS) feature with an additional feature using coarse POS representations (Petrov et al., 2012). Our baseline parsing models replicate and, for some experiments, surpass previous best results. The first- and second-order pruning models have the same structure, but for efficiency use only the basic features from McDonald et al. (2005). As feature computation is quite costly, future work may investigate whether this set can be reduced further. V INE P RUNE and L OCAL S HORT use the same feature sets for short arcs. Outer arcs have features of the unary head or modifier token, as well as features for the POS tag bordering the cutoff and the direction of the arc. 5.2 5.3 Z HANG N IVRE an unlabeled reimplementation of the linear-time, k-best, transition-based parser of Zhang and Nivre (2011). This parser uses composite features up to third-order with a greedy decoding algorithm. The reimplementation is about twice as fast as t"
N12-1054,W04-2407,0,0.395442,"i-th order passes introduce larger scope features, while further constraining the search space. In Section 5 we present experiments in multiple languages. Our coarse-to-fine first-, second-, and third-order parsers preserve the accuracy of the unpruned models, but are faster by up to two orders of magnitude. Our pruned third-order model is faster than an unpruned first-order model, and compares favorably in speed to the state-of-the-art transitionbased parser of Zhang and Nivre (2011). It is worth noting the relationship to greedy transition-based dependency parsers that are also linear-time (Nivre et al., 2004) or quadratic-time (Yamada and Matsumoto, 2003). It is their success that motivates building explicitly trained, linear-time pruning models. However, while a greedy solution for arc-standard transition-based parsers can be computed in linear-time, Kuhlmann et al. (2011) recently showed that computing exact solutions or (max-)marginals has time complexity O(n4 ), making these models inappropriate for coarse-to-fine style pruning. As an alternative, Roark and Hollingshead (2008) and Bergsma and Cherry (2010) present approaches where individual classifiers are used to prune chart cells. Such appr"
N12-1054,N09-1063,0,0.0334304,"ied (Section 3). The dynamic program is a form of vine parsing (Eisner and Smith, 2005), which we use to compute parse max-marginals, rather than for finding the 1best parse tree. To reduce pruning errors, the parameters of the vine parser (and all subsequent pruning models) are trained using the structured prediction cascades of Weiss and Taskar (2010) to optimize for pruning efficiency, and not for 1-best prediction (Section 4). Despite a limited scope of b = 3, the 1 This is in contrast to optimality preserving methods such as A* search, which typically do not provide sufficient speed-ups (Pauls and Klein, 2009). 498 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 498–507, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics 2 Motivation & Overview The goal of this work is fast, high-order, graphbased dependency parsing. Previous work on constituency parsing demonstrates that performing several passes with increasingly more complex models results in faster inference (Charniak et al., 2006; Petrov and Klein, 2007). The same technique applies to dependency parsing with a cascade of models o"
N12-1054,N07-1051,1,0.120141,"Missing"
N12-1054,petrov-etal-2012-universal,1,0.647629,"and second-order pruning, and a final third-order 1best pass.3 We tune the pruning thresholds for each round and each cascade separately. This is because we might be willing to do a more aggressive vine pruning pass if the final model is a first-order model, since these two models tend to often agree. Included are lexical features, part-of-speech features, features on in-between tokens, as well as feature conjunctions, surrounding part-of-speech tags, and back-off features. In addition, we replicate each part-of-speech (POS) feature with an additional feature using coarse POS representations (Petrov et al., 2012). Our baseline parsing models replicate and, for some experiments, surpass previous best results. The first- and second-order pruning models have the same structure, but for efficiency use only the basic features from McDonald et al. (2005). As feature computation is quite costly, future work may investigate whether this set can be reduced further. V INE P RUNE and L OCAL S HORT use the same feature sets for short arcs. Outer arcs have features of the unary head or modifier token, as well as features for the POS tag bordering the cutoff and the direction of the arc. 5.2 5.3 Z HANG N IVRE an un"
N12-1054,C08-1094,0,0.22505,"Missing"
N12-1054,W03-3023,0,0.801784,"e features, while further constraining the search space. In Section 5 we present experiments in multiple languages. Our coarse-to-fine first-, second-, and third-order parsers preserve the accuracy of the unpruned models, but are faster by up to two orders of magnitude. Our pruned third-order model is faster than an unpruned first-order model, and compares favorably in speed to the state-of-the-art transitionbased parser of Zhang and Nivre (2011). It is worth noting the relationship to greedy transition-based dependency parsers that are also linear-time (Nivre et al., 2004) or quadratic-time (Yamada and Matsumoto, 2003). It is their success that motivates building explicitly trained, linear-time pruning models. However, while a greedy solution for arc-standard transition-based parsers can be computed in linear-time, Kuhlmann et al. (2011) recently showed that computing exact solutions or (max-)marginals has time complexity O(n4 ), making these models inappropriate for coarse-to-fine style pruning. As an alternative, Roark and Hollingshead (2008) and Bergsma and Cherry (2010) present approaches where individual classifiers are used to prune chart cells. Such approaches have the drawback that pruning decisions"
N12-1054,P11-2033,0,0.0369688,"ous best results. The first- and second-order pruning models have the same structure, but for efficiency use only the basic features from McDonald et al. (2005). As feature computation is quite costly, future work may investigate whether this set can be reduced further. V INE P RUNE and L OCAL S HORT use the same feature sets for short arcs. Outer arcs have features of the unary head or modifier token, as well as features for the POS tag bordering the cutoff and the direction of the arc. 5.2 5.3 Z HANG N IVRE an unlabeled reimplementation of the linear-time, k-best, transition-based parser of Zhang and Nivre (2011). This parser uses composite features up to third-order with a greedy decoding algorithm. The reimplementation is about twice as fast as their reported speed, but scores slightly lower. Features For the non-pruning models, we use a standard set of features proposed in the discriminative graphbased dependency parsing literature (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). 3 For the first-order parser, we found it beneficial to employ a reduced feature first-order pruner before the final model, i.e. the cascade has four rounds: dictionary, vine, first-order pruning, and first-"
N15-1080,W08-2102,0,0.41481,"araParser 9 http://nlp.cs.nyu.edu/evalb 7 7 794 Model PTB §23 F1 Sent./s. Charniak (2000) Stanford PCFG (2003) Petrov (2007) Zhu (2013) Carreras (2008) 89.5 85.5 90.1 90.3 91.1 – 5.3 8.6 39.0 – CJ Reranking (2005) Stanford RNN (2013) 91.5 90.0 4.3 2.8 PAD PAD (Pruned) 90.4 90.3 34.3 58.6 Model CTB F1 Charniak (2000) Bikel (2004) Petrov (2007) Zhu (2013) 80.8 80.6 83.3 83.2 PAD 82.4 Table 3: Accuracy and speed on PTB §23 and CTB 5.1 test split. Comparisons are to state-of-the-art non-reranking supervised phrase-structure parsers (Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007; Carreras et al., 2008; Zhu et al., 2013; Bikel, 2004), and semi-supervised and reranking parsers (Charniak and Johnson, 2005; Socher et al., 2013). F1 Sent./s. Oracle M ALT PARSER RS-K1 RS-K4 RS-K16 YARA -K1 YARA -K16 YARA -K32 YARA -K64 TP-BASIC TP-S TANDARD TP-F ULL 89.7 90.1 92.5 93.1 89.7 92.9 93.1 93.1 92.8 93.3 93.5 85.5 86.6 90.1 90.6 85.3 89.8 90.4 90.5 88.9 90.9 90.8 240.7 233.9 151.3 58.6 1265.8 157.5 48.3 47.3 132.8 27.2 13.2 87.8 87.6 91.5 92.5 86.7 91.7 92.0 92.2 90.8 92.6 92.9 dency constraints, the English results show that the parser is comparable in accuracy to many widelyused systems, and is sign"
N15-1080,P05-1022,0,0.605688,"sers are generally much faster than c-parsers, we consider an alternate pipeline (Section 3): d-parse first, then transform the dependency representation into a phrase-structure tree constrained to be consistent with the dependency parse. This idea was explored by Xia and Palmer (2001) and Xia et al. (2009) using hand-written rules. Instead, we present a data-driven algorithm using the structured prediction framework (Section 4). The approach can be understood as a specially-trained coarse-to-fine decoding algorithm where a d-parser provides “coarse” structure and the second stage refines it (Charniak and Johnson, 2005; Petrov and Klein, 2007). Our lexicalized phrase-structure parser, PAD, is asymptotically faster than parsing with a lexicalized context-free grammar: O(n2 ) plus d-parsing, vs. O(n5 ) worst case runtime in sentence length n, with the same grammar constant. Experiments show that our approach achieves linear observable runtime, and accuracy similar to state-of-the-art phrase-structure parsers without reranking or semisupervised training (Section 7). 2 Background We begin with the conventional development by first introducing c-parsing and then defining d-parses through a mechanical conversion"
N15-1080,A00-2018,0,0.543393,"Missing"
N15-1080,J05-1003,0,0.0608399,"Missing"
N15-1080,P99-1065,0,0.346961,"Missing"
N15-1080,J03-4003,0,0.892132,"otated with a terminal or nonterminal symbol and a derived head index. The blue and red vertices have the words automaker2 and sold3 as heads respectively. The vertex VP(3) implies that automaker2 is a left-dependent of sold3 , and that 2 ∈ L(3) in the d-parse. Dependency Parsing Dependency parses provide an alternative, and in some sense simpler, representation of sentence structure. These d-parses can be derived through mechanical transformation from context-free trees. There are several popular transformations in wide use; each provides a different representation of a sentence’s structure (Collins, 2003; De Marneffe and Manning, 2008; Yamada and Matsumoto, 2003; Johansson and Nugues, 2007). We consider the class of transformations that are defined through local head rules. For a binary CFG, define a collection of head rules as a mapping from each CFG rule to a head preference for its left or right child. We use the notation A → β1∗ β2 and A → β1 β2∗ to indicate a left- or right-headed rule, respectively. The head rules can be used to map a c-parse to a dependency tree (d-parse). In a d-parse, each word in the sentence is assigned as a dependent to a head word, h ∈ {0, . . . , n}, where 0 is"
N15-1080,W08-1301,0,0.115419,"Missing"
N15-1080,P05-1067,0,0.117591,"Missing"
N15-1080,P99-1059,0,0.23871,"to produce two vertices covering hi, ki and hk + 1, ji, and that the new head is index h has dependent index m. We say this production “completes” word m since it can no longer be the head of a larger span. Running the algorithm consists of bottom-up dynamic programming over these productions. However, applying this version of the CKY algorithm requires O(n5 |G|) time (linear in the number of productions), which is not practical to run without heavy pruning. Most lexicalized parsers therefore make further assumptions on the scoring function which can lead to asymptotically faster algorithms (Eisner and Satta, 1999). Instead, we consider the same objective, but constrain the c-parses to be consistent with a given dparse, d. By “consistent,” we mean that the cparse will be converted by the head rules to this exact d-parse.4 Define the set of consistent c-parses as Y(x, d) and the constrained search problem as arg maxy∈Y(x,d) s(y; x, d). Figure 3 (right) shows the algorithm for this new problem. The algorithm has several nice properties. All rules now must select words h and m that are consistent with the dependency parse (i.e., there is an arc (h, m)) so these variables are no longer free. Furthermore, si"
N15-1080,P15-1147,0,0.501488,"Missing"
N15-1080,W08-1007,0,0.511542,"Missing"
N15-1080,W07-2444,0,0.694585,"Missing"
N15-1080,P14-1022,0,0.183115,"− y 0 |where y is an indicator for production rules firing over pairs of adjacent spans (i.e., i, j, k). Xia et al. (2009) PAD (§19) PAD (§2–21) 88.1 95.9 97.5 PTB §22 Rec. F1 90.7 95.9 97.8 89.4 95.9 97.7 The objective is optimized using AdaGrad (Duchi et al., 2011). The gradient calculation requires computing a loss-augmented max-scoring c-parse for each training example which is done using the algorithm of Figure 3 (right). modifier word and part-of-speech, and head word and part-of-speech. The second set of features is modeled after the span features described in the X-bar-style parser of Hall et al. (2014). These include conjunctions of the rule with: first and last word of current span, preceding and following word of current span, adjacent words at split of current span, and binned length of the span. The full feature set is shown in Figure 4. After training, there are a total of around 2 million nonzero features. For efficiency, we use lossy feature hashing. We found this had no impact on parsing accuracy but made the parsing significantly faster. min Prec. Table 2: Comparison with the rule-based system of Xia et al. (2009). Results are shown using gold-standard tags and dependencies. Xia et"
N15-1080,W07-2416,0,0.0606049,"blue and red vertices have the words automaker2 and sold3 as heads respectively. The vertex VP(3) implies that automaker2 is a left-dependent of sold3 , and that 2 ∈ L(3) in the d-parse. Dependency Parsing Dependency parses provide an alternative, and in some sense simpler, representation of sentence structure. These d-parses can be derived through mechanical transformation from context-free trees. There are several popular transformations in wide use; each provides a different representation of a sentence’s structure (Collins, 2003; De Marneffe and Manning, 2008; Yamada and Matsumoto, 2003; Johansson and Nugues, 2007). We consider the class of transformations that are defined through local head rules. For a binary CFG, define a collection of head rules as a mapping from each CFG rule to a head preference for its left or right child. We use the notation A → β1∗ β2 and A → β1 β2∗ to indicate a left- or right-headed rule, respectively. The head rules can be used to map a c-parse to a dependency tree (d-parse). In a d-parse, each word in the sentence is assigned as a dependent to a head word, h ∈ {0, . . . , n}, where 0 is a special symbol indicating the pseudo-root of the sentence. For each h we define L(h) ⊂"
N15-1080,P03-1054,0,0.0928356,"performance (i.e., how well a perfect scoring function could do with a pruned Y(x, d)). Table 1 shows a comparison of these pruning methods on development data. The constrained parsing algorithm is much faster than standard lexicalized parsing, and Model L EX CKY∗ D EP CKY P RUNE 1 P RUNE 2 P RUNE 1+2 Complexity 5 n |G| P P h |L(h)||R(h)||G| h |L(h)||R(h)||GT | – – Sent./s. Ora. F1 0.25 71.2 336.0 96.6 425.1 100.0 92.6 92.5 92.5 92.5 We also explored binarization using horizontal and vertical markovization to include additional context of the tree, as found useful in unlexicalized approaches (Klein and Manning, 2003). Preliminary experiments showed that this increased the size of the grammar, and the runtime of the algorithm, without leading to improvements in accuracy. Phrase-structure trees also include unary rules of the form A → β1∗ . To handle unary rules we modify the parsing algorithms in Figure 3 to include a unary completion rule, Table 1: Comparison of three parsing setups: L EX CKY∗ is the complete lexicalized c-parser on Y(x), but limited to only sentences less than 20 words for tractability, D EP CKY is the constrained c-parser on Y(x, d), P RUNE 1, P RUNE 2, and P RUNE 1+2 are combinations o"
N15-1080,J93-2004,0,0.0507126,"have also been several papers that use ideas from dependency parsing to simplify and speed up phrase-structure prediction. Zhu et al. (2013) build a high-accuracy phrase-structure parser using a transition-based system. Hall et al. (2014) use a stripped down parser based on a simple X-bar grammar and a small set of lexicalized features. 6 Methods We ran a series of experiments to assess the accuracy, efficiency, and applicability of our parser, PAD, to several tasks. These experiments use the following setup. For English experiments we use the standard Penn Treebank (PTB) experimental setup (Marcus et al., 1993). Training is done on §2–21, development on §22, and testing on §23. We use the development set to tune the regularization parameter, λ = 1e−8, and the pruning threshold, γ = 0.95. For Chinese experiments, we use version 5.1 of the Penn Chinese Treebank 5.1 (CTB) (Xue et al., 2005). We followed previous work and used articles 001–270 and 440–1151 for training, 301–325 for development, and 271–300 for test. We also use the development set to tune the regularization parame6 https://github.com/syllog1sm/redshift http://stp.lingfil.uu.se/˜nivre/ research/chn_headrules.txt 8 https://github.com/yaho"
N15-1080,P13-2109,1,0.863476,"parser. For these experiments we treat our system and the Zhang-Nivre parser as an independently trained, but complete end-to-end c-parser. Runtime for these experiments includes both the time for dparsing and conversion. Despite the fixed depenUAS Table 4: The effect of d-parsing accuracy (PTB §22) on PAD and an oracle converter. Runtime includes d-parsing and cparsing. Inputs include MaltParser (Nivre et al., 2006), the RedShift and the Yara implementations of the parser of Zhang and Nivre (2011) with various beam size, and three versions of TurboParser trained with projective constraints (Martins et al., 2013). unlabeled accuracy score (UAS). We implemented the grammar binarization, head rules, and pruning tables in Python, and the parser, features, and training in C++. Experiments are performed on a Lenovo ThinkCentre desktop computer with 32GB of memory and Core i7-3770 3.4GHz 8M cache CPU. 7 Model Effect of Dependencies Table 4 shows experiments comparing the effect of different input dparses. For these experiments we used the same version of PAD with 11 different d-parsers of varying quality and speed. We measure for each parser: its UAS, speed, and labeled F1 when used with PAD and with an ora"
N15-1080,nivre-etal-2006-maltparser,0,0.0456621,"f dependency parsing accuracy, and the effect of the amount of annotated phrase-structure data. Parsing Accuracy Table 3 compares the accuracy and speed of the phrase-structure trees produced by the parser. For these experiments we treat our system and the Zhang-Nivre parser as an independently trained, but complete end-to-end c-parser. Runtime for these experiments includes both the time for dparsing and conversion. Despite the fixed depenUAS Table 4: The effect of d-parsing accuracy (PTB §22) on PAD and an oracle converter. Runtime includes d-parsing and cparsing. Inputs include MaltParser (Nivre et al., 2006), the RedShift and the Yara implementations of the parser of Zhang and Nivre (2011) with various beam size, and three versions of TurboParser trained with projective constraints (Martins et al., 2013). unlabeled accuracy score (UAS). We implemented the grammar binarization, head rules, and pruning tables in Python, and the parser, features, and training in C++. Experiments are performed on a Lenovo ThinkCentre desktop computer with 32GB of memory and Core i7-3770 3.4GHz 8M cache CPU. 7 Model Effect of Dependencies Table 4 shows experiments comparing the effect of different input dparses. For t"
N15-1080,N07-1051,0,0.655647,"er than c-parsers, we consider an alternate pipeline (Section 3): d-parse first, then transform the dependency representation into a phrase-structure tree constrained to be consistent with the dependency parse. This idea was explored by Xia and Palmer (2001) and Xia et al. (2009) using hand-written rules. Instead, we present a data-driven algorithm using the structured prediction framework (Section 4). The approach can be understood as a specially-trained coarse-to-fine decoding algorithm where a d-parser provides “coarse” structure and the second stage refines it (Charniak and Johnson, 2005; Petrov and Klein, 2007). Our lexicalized phrase-structure parser, PAD, is asymptotically faster than parsing with a lexicalized context-free grammar: O(n2 ) plus d-parsing, vs. O(n5 ) worst case runtime in sentence length n, with the same grammar constant. Experiments show that our approach achieves linear observable runtime, and accuracy similar to state-of-the-art phrase-structure parsers without reranking or semisupervised training (Section 7). 2 Background We begin with the conventional development by first introducing c-parsing and then defining d-parses through a mechanical conversion using head rules. In the"
N15-1080,D10-1001,1,0.934096,"Missing"
N15-1080,W13-2307,1,0.904871,"Missing"
N15-1080,P13-1045,0,0.0514711,"hu (2013) Carreras (2008) 89.5 85.5 90.1 90.3 91.1 – 5.3 8.6 39.0 – CJ Reranking (2005) Stanford RNN (2013) 91.5 90.0 4.3 2.8 PAD PAD (Pruned) 90.4 90.3 34.3 58.6 Model CTB F1 Charniak (2000) Bikel (2004) Petrov (2007) Zhu (2013) 80.8 80.6 83.3 83.2 PAD 82.4 Table 3: Accuracy and speed on PTB §23 and CTB 5.1 test split. Comparisons are to state-of-the-art non-reranking supervised phrase-structure parsers (Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007; Carreras et al., 2008; Zhu et al., 2013; Bikel, 2004), and semi-supervised and reranking parsers (Charniak and Johnson, 2005; Socher et al., 2013). F1 Sent./s. Oracle M ALT PARSER RS-K1 RS-K4 RS-K16 YARA -K1 YARA -K16 YARA -K32 YARA -K64 TP-BASIC TP-S TANDARD TP-F ULL 89.7 90.1 92.5 93.1 89.7 92.9 93.1 93.1 92.8 93.3 93.5 85.5 86.6 90.1 90.6 85.3 89.8 90.4 90.5 88.9 90.9 90.8 240.7 233.9 151.3 58.6 1265.8 157.5 48.3 47.3 132.8 27.2 13.2 87.8 87.6 91.5 92.5 86.7 91.7 92.0 92.2 90.8 92.6 92.9 dency constraints, the English results show that the parser is comparable in accuracy to many widelyused systems, and is significantly faster. The parser most competitive in both speed and accuracy is that of Zhu et al. (2013), a fast shift-reduce ph"
N15-1080,H01-1014,0,0.275729,"ite extensive work on directto-dependency parsing algorithms (which we call dparsing), the most accurate dependency parsers for English still involve phrase-structure parsing (which we call c-parsing) followed by rule-based extraction of dependencies (Kong and Smith, 2014). What if dependency annotations had come first? Because d-parsers are generally much faster than c-parsers, we consider an alternate pipeline (Section 3): d-parse first, then transform the dependency representation into a phrase-structure tree constrained to be consistent with the dependency parse. This idea was explored by Xia and Palmer (2001) and Xia et al. (2009) using hand-written rules. Instead, we present a data-driven algorithm using the structured prediction framework (Section 4). The approach can be understood as a specially-trained coarse-to-fine decoding algorithm where a d-parser provides “coarse” structure and the second stage refines it (Charniak and Johnson, 2005; Petrov and Klein, 2007). Our lexicalized phrase-structure parser, PAD, is asymptotically faster than parsing with a lexicalized context-free grammar: O(n2 ) plus d-parsing, vs. O(n5 ) worst case runtime in sentence length n, with the same grammar constant. E"
N15-1080,W03-3023,0,0.112004,"nd a derived head index. The blue and red vertices have the words automaker2 and sold3 as heads respectively. The vertex VP(3) implies that automaker2 is a left-dependent of sold3 , and that 2 ∈ L(3) in the d-parse. Dependency Parsing Dependency parses provide an alternative, and in some sense simpler, representation of sentence structure. These d-parses can be derived through mechanical transformation from context-free trees. There are several popular transformations in wide use; each provides a different representation of a sentence’s structure (Collins, 2003; De Marneffe and Manning, 2008; Yamada and Matsumoto, 2003; Johansson and Nugues, 2007). We consider the class of transformations that are defined through local head rules. For a binary CFG, define a collection of head rules as a mapping from each CFG rule to a head preference for its left or right child. We use the notation A → β1∗ β2 and A → β1 β2∗ to indicate a left- or right-headed rule, respectively. The head rules can be used to map a c-parse to a dependency tree (d-parse). In a d-parse, each word in the sentence is assigned as a dependent to a head word, h ∈ {0, . . . , n}, where 0 is a special symbol indicating the pseudo-root of the sentence"
N15-1080,P11-2033,0,0.0591488,"structure data. Parsing Accuracy Table 3 compares the accuracy and speed of the phrase-structure trees produced by the parser. For these experiments we treat our system and the Zhang-Nivre parser as an independently trained, but complete end-to-end c-parser. Runtime for these experiments includes both the time for dparsing and conversion. Despite the fixed depenUAS Table 4: The effect of d-parsing accuracy (PTB §22) on PAD and an oracle converter. Runtime includes d-parsing and cparsing. Inputs include MaltParser (Nivre et al., 2006), the RedShift and the Yara implementations of the parser of Zhang and Nivre (2011) with various beam size, and three versions of TurboParser trained with projective constraints (Martins et al., 2013). unlabeled accuracy score (UAS). We implemented the grammar binarization, head rules, and pruning tables in Python, and the parser, features, and training in C++. Experiments are performed on a Lenovo ThinkCentre desktop computer with 32GB of memory and Core i7-3770 3.4GHz 8M cache CPU. 7 Model Effect of Dependencies Table 4 shows experiments comparing the effect of different input dparses. For these experiments we used the same version of PAD with 11 different d-parsers of var"
N15-1080,P13-1043,0,0.405068,"oth for pruning and within a richer lexicalized parser. Similarly, Rush et al. (2010) use dual decomposition to combine a powerful dependency parser with a lexicalized phrase-structure model. This work differs in that we treat the dependency parse as a hard constraint, hence largely reduce the runtime of a fully lexicalized phrase structure parsing model while maintaining the ability, at least in principle, to generate highly accurate phrasestructure parses. Finally there have also been several papers that use ideas from dependency parsing to simplify and speed up phrase-structure prediction. Zhu et al. (2013) build a high-accuracy phrase-structure parser using a transition-based system. Hall et al. (2014) use a stripped down parser based on a simple X-bar grammar and a small set of lexicalized features. 6 Methods We ran a series of experiments to assess the accuracy, efficiency, and applicability of our parser, PAD, to several tasks. These experiments use the following setup. For English experiments we use the standard Penn Treebank (PTB) experimental setup (Marcus et al., 1993). Training is done on §2–21, development on §22, and testing on §23. We use the development set to tune the regularizatio"
N16-1012,P00-1041,0,0.682766,"ata set and is comparable on the DUC-2004 task. 93 Proceedings of NAACL-HLT 2016, pages 93–98, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2 Previous Work vidual conditional probabilities: While there is a large body of work for generating extractive summaries of sentences (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Altun, 2013; Filippova et al., 2015), there has been much less research on abstractive summarization. A count-based noisy-channel machine translation model was proposed for the problem in Banko et al. (2000). The task of abstractive sentence summarization was later formalized around the DUC-2003 and DUC-2004 competitions (Over et al., 2007), where the T OP IARY system (Zajic et al., 2004) was the state-ofthe-art. More recently Cohn and Lapata (2008) and later Woodsend et al. (2010) proposed systems which made heavy use of the syntactic features of the sentence-summary pairs. Later, along the lines of Banko et al. (2000), M OSES was used directly as a method for text simplification by Wubben et al. (2012). Other works which have recently been proposed for the problem of sentence summarization incl"
N16-1012,C08-1018,0,0.0286084,"ere is a large body of work for generating extractive summaries of sentences (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Altun, 2013; Filippova et al., 2015), there has been much less research on abstractive summarization. A count-based noisy-channel machine translation model was proposed for the problem in Banko et al. (2000). The task of abstractive sentence summarization was later formalized around the DUC-2003 and DUC-2004 competitions (Over et al., 2007), where the T OP IARY system (Zajic et al., 2004) was the state-ofthe-art. More recently Cohn and Lapata (2008) and later Woodsend et al. (2010) proposed systems which made heavy use of the syntactic features of the sentence-summary pairs. Later, along the lines of Banko et al. (2000), M OSES was used directly as a method for text simplification by Wubben et al. (2012). Other works which have recently been proposed for the problem of sentence summarization include (Galanis and Androutsopoulos, 2010; Napoles et al., 2011; Cohn and Lapata, 2013). Very recently Rush et al. (2015) proposed a neural attention model for this problem using a new data set for training and showing state-of-the-art performance o"
N16-1012,D13-1155,0,0.547969,"cularly notable is the fact that even with a simple generation module, which does not use any extractive feature tuning, our model manages to significantly outperform their ABS+ system on the Gigaword data set and is comparable on the DUC-2004 task. 93 Proceedings of NAACL-HLT 2016, pages 93–98, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2 Previous Work vidual conditional probabilities: While there is a large body of work for generating extractive summaries of sentences (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Altun, 2013; Filippova et al., 2015), there has been much less research on abstractive summarization. A count-based noisy-channel machine translation model was proposed for the problem in Banko et al. (2000). The task of abstractive sentence summarization was later formalized around the DUC-2003 and DUC-2004 competitions (Over et al., 2007), where the T OP IARY system (Zajic et al., 2004) was the state-ofthe-art. More recently Cohn and Lapata (2008) and later Woodsend et al. (2010) proposed systems which made heavy use of the syntactic features of the sentence-summary pairs. Later, along the lines of Ban"
N16-1012,N10-1131,0,0.0426305,"task of abstractive sentence summarization was later formalized around the DUC-2003 and DUC-2004 competitions (Over et al., 2007), where the T OP IARY system (Zajic et al., 2004) was the state-ofthe-art. More recently Cohn and Lapata (2008) and later Woodsend et al. (2010) proposed systems which made heavy use of the syntactic features of the sentence-summary pairs. Later, along the lines of Banko et al. (2000), M OSES was used directly as a method for text simplification by Wubben et al. (2012). Other works which have recently been proposed for the problem of sentence summarization include (Galanis and Androutsopoulos, 2010; Napoles et al., 2011; Cohn and Lapata, 2013). Very recently Rush et al. (2015) proposed a neural attention model for this problem using a new data set for training and showing state-of-the-art performance on the DUC tasks. Our model can be seen as an extension of their model. 3 Attentive Recurrent Architecture Let x denote the input sentence consisting of a sequence of M words x = [x1 , . . . , xM ], where each word xi is part of vocabulary V, of size |V |= V . Our task is to generate a target sequence y = [y1 , . . . , yN ], of N words, where N < M , such that the meaning of x is preserved:"
N16-1012,A00-1043,0,0.112081,"e state-of-the-art systems of Rush et al. (2015) on multiple data sets. Particularly notable is the fact that even with a simple generation module, which does not use any extractive feature tuning, our model manages to significantly outperform their ABS+ system on the Gigaword data set and is comparable on the DUC-2004 task. 93 Proceedings of NAACL-HLT 2016, pages 93–98, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2 Previous Work vidual conditional probabilities: While there is a large body of work for generating extractive summaries of sentences (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Altun, 2013; Filippova et al., 2015), there has been much less research on abstractive summarization. A count-based noisy-channel machine translation model was proposed for the problem in Banko et al. (2000). The task of abstractive sentence summarization was later formalized around the DUC-2003 and DUC-2004 competitions (Over et al., 2007), where the T OP IARY system (Zajic et al., 2004) was the state-ofthe-art. More recently Cohn and Lapata (2008) and later Woodsend et al. (2010) proposed systems which made heav"
N16-1012,W04-1013,0,0.382019,"nization and sentence separation while discarding other annotations such as tags and parses. We pair the first sentence of each article with its headline to form sentence-summary pairs. The data is pre-processed in the same way as Rush et al. (2015) and we use the same splits for training, validation, and testing. For Gigaword we report results on the same randomly held-out test set of 2000 sentence-summary pairs as (Rush et al., 2015).1 We also evaluate our models on the DUC-2004 evaluation data set comprising 500 pairs (Over et al., 2007). Our evaluation is based on three variants of ROUGE (Lin, 2004), namely, ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest-common substring). 4.2 Architectural Choices We implemented our models in the Torch library (http://torch.ch/)2 . To optimize our loss (Equation 5) we used stochastic gradient descent with mini-batches of size 32. During training we measure the perplexity of the summaries in the validation set and adjust our hyper-parameters, such as the learning rate, based on this number. 1 We remove pairs with empty titles resulting in slightly different accuracy compared to Rush et al. (2015) for their systems. 2 Our code can found at ww"
N16-1012,D15-1166,0,0.367153,"Missing"
N16-1012,E06-1038,0,0.0119889,"t al. (2015) on multiple data sets. Particularly notable is the fact that even with a simple generation module, which does not use any extractive feature tuning, our model manages to significantly outperform their ABS+ system on the Gigaword data set and is comparable on the DUC-2004 task. 93 Proceedings of NAACL-HLT 2016, pages 93–98, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2 Previous Work vidual conditional probabilities: While there is a large body of work for generating extractive summaries of sentences (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Altun, 2013; Filippova et al., 2015), there has been much less research on abstractive summarization. A count-based noisy-channel machine translation model was proposed for the problem in Banko et al. (2000). The task of abstractive sentence summarization was later formalized around the DUC-2003 and DUC-2004 competitions (Over et al., 2007), where the T OP IARY system (Zajic et al., 2004) was the state-ofthe-art. More recently Cohn and Lapata (2008) and later Woodsend et al. (2010) proposed systems which made heavy use of the syntactic features of the s"
N16-1012,K16-1028,0,0.805068,"Missing"
N16-1012,W11-1610,0,0.0643834,"Missing"
N16-1012,W12-3018,0,0.478482,"Missing"
N16-1012,P03-1021,0,0.0330929,"on the held-out set and use it to compute the F1-score of ROUGE-1, ROUGE-2, and ROUGE-L on the test sets, all of which we report. For the DUC corpus however, inline with the standard, we report the recall-only ROUGE. As baseline we use the state-of-the-art attention-based system (ABS) of Rush et al. (2015) which relies on a feed-forward network decoder. Additionally, we compare to an enhanced version of their system (ABS+), which relies on a range of separate extractive summarization features that are added as log-linear features in a secondary learning step with minimum error rate training (Och, 2003). Table 1 shows that both our RAS-Elman and RAS-LSTM models achieve lower perplexity than 96 ROUGE recall, while as we use the more balanced F-measure. RG-1 RG-2 RG-L ABS ABS+ RAS-Elman (k = 1) RAS-Elman (k = 10) RAS-LSTM (k = 1) RAS-LSTM (k = 10) 26.55 28.18 29.13 28.97 26.90 27.41 7.06 8.49 7.62 8.26 6.57 7.69 22.05 23.81 23.92 24.06 22.12 23.06 Luong-NMT 28.55 8.79 24.43 Table 3: ROUGE results (recall-only) on the DUC-2004 test sets. ABS and ABS+ are the systems of Rush et al. 2015. k refers to the size of the beam for generation; k = 1 implies greedy generation. RG refers to ROUGE. ABS as"
N16-1012,D15-1044,1,0.730926,"del. In addition, at every time step the decoder also takes a conditioning input which is the output of an encoder module. Depending on the current state of the RNN, the encoder computes scores over the words in the input sentence. These scores can be interpreted as a soft alignment over the input text, informing the decoder which part of the input sentence it should focus on to generate the next word. Both the decoder and encoder are jointly trained on a data set consisting of sentence-summary pairs. Our model can be seen as an extension of the recently proposed model for the same problem by Rush et al. (2015). While they use a feed-forward neural language model for generation, we use a recurrent neural network. Furthermore, our encoder is more sophisticated, in that it explicitly encodes the position information of the input words. Lastly, our encoder uses a convolutional network to encode input words. These extensions result in improved performance. Abstractive Sentence Summarization generates a shorter version of a given sentence while attempting to preserve its meaning. We introduce a conditional recurrent neural network (RNN) which generates a summary of an input sentence. The conditioning is"
N16-1012,D10-1050,0,0.0107957,"enerating extractive summaries of sentences (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Altun, 2013; Filippova et al., 2015), there has been much less research on abstractive summarization. A count-based noisy-channel machine translation model was proposed for the problem in Banko et al. (2000). The task of abstractive sentence summarization was later formalized around the DUC-2003 and DUC-2004 competitions (Over et al., 2007), where the T OP IARY system (Zajic et al., 2004) was the state-ofthe-art. More recently Cohn and Lapata (2008) and later Woodsend et al. (2010) proposed systems which made heavy use of the syntactic features of the sentence-summary pairs. Later, along the lines of Banko et al. (2000), M OSES was used directly as a method for text simplification by Wubben et al. (2012). Other works which have recently been proposed for the problem of sentence summarization include (Galanis and Androutsopoulos, 2010; Napoles et al., 2011; Cohn and Lapata, 2013). Very recently Rush et al. (2015) proposed a neural attention model for this problem using a new data set for training and showing state-of-the-art performance on the DUC tasks. Our model can be"
N16-1012,P12-1107,0,0.0118831,"Missing"
N16-1012,D15-1042,0,\N,Missing
N16-1114,D08-1031,0,0.543141,"scoring function clearly ignores much structural information, the mentionranking approach has been attractive for at least two reasons. First, inference is relatively simple and efficient, requiring only a left-to-right pass through a document’s mentions during which a mention’s antecedents (as well as ) are scored and the highest scoring antecedent is predicted. Second, from a linguistic modeling perspective, mention-ranking models learn a scoring function that requires a mention xn to be compatible with only one of its coreferent antecedents. This contrasts with mention-pair models (e.g., Bengtson and Roth (2008)), which score all pairs of mentions in a cluster, as well as with certain cluster-based models (see discussion in Culotta et al. (2007)). Modeling each mention as having a single antecedent is particularly advantageous for pronominal mentions, which we might like to model 1 We assume nested mentions are ordered by their syntactic heads. 995 as linking to a single nominal or proper antecedent, for example, but not necessarily to all other coreferent mentions. Accordingly, in this paper we attempt to maintain the inferential simplicity and modeling benefits of mention ranking, while allowing th"
N16-1114,P14-1005,0,0.11608,"Missing"
N16-1114,C10-1017,0,0.0193937,"into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention-ranking models, which select a single antecedent for each anaphoric mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013; Wiseman et al., 2015). Structured approaches typically divide between those that induce a clustering of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, 2010), and, more recently, those that learn a latent tree of mentions (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). There have also been structured approaches that merge the mention-ranking and mention-pair ideas in some way. For instance, Rahman and Ng (2011) rank clusters rather than mentions; Clark and Manning (2015) use the output of both mention-ranking and mention pair systems to learn a clustering. The application of RNNs to modeling (the trajectory of) the state of a cluster is apparently novel, though it bears some similarity to the recent work of Dyer e"
N16-1114,D13-1057,0,0.0610294,"Missing"
N16-1114,P15-1136,0,0.656832,"2015) Peng et al. (2015) Wiseman et al. (2015) This work P MUC R F1 74.30 76.72 76.12 76.23 77.49 67.46 68.13 69.38 69.31 69.75 70.72 72.17 72.59 72.22 72.60 73.42 P B3 R F1 P CEAFe R F1 CoNLL 62.71 66.12 65.64 66.07 66.83 54.96 54.22 56.01 55.83 56.95 58.58 59.58 60.44 60.50 60.52 61.50 59.40 59.47 59.44 59.41 62.14 52.27 52.33 52.98 54.88 53.85 55.61 55.67 56.02 56.37 57.05 57.70 61.63 62.47 63.02 63.03 63.39 64.21 Table 1: Results on CoNLL 2012 English test set. We compare against recent state of the art systems, including (in order) Bjorkelund and Kuhn (2014), Martschat and Strube (2015), Clark and Manning (2015), Peng et al. (2015), and Wiseman et al. (2015). F1 gains are significant (p < 0.05 under the bootstrap resample test (Koehn, 2004)) compared with Wiseman et al. (2015) for all metrics. genre (out of {bc,bn,mz,nw,pt,tc,wb}) indicator to φp and φa . • We add features indicating if a mention has a substring overlap with the current speaker (φp and φa ), and if an antecedent has a substring overlap with a speaker distinct from the current mention’s speaker (φp ). • We add a single centered, rescaled document position feature to each mention when learning hc . We calculate a mention xn ’s rescaled"
N16-1114,N07-1011,0,0.293996,"rst, inference is relatively simple and efficient, requiring only a left-to-right pass through a document’s mentions during which a mention’s antecedents (as well as ) are scored and the highest scoring antecedent is predicted. Second, from a linguistic modeling perspective, mention-ranking models learn a scoring function that requires a mention xn to be compatible with only one of its coreferent antecedents. This contrasts with mention-pair models (e.g., Bengtson and Roth (2008)), which score all pairs of mentions in a cluster, as well as with certain cluster-based models (see discussion in Culotta et al. (2007)). Modeling each mention as having a single antecedent is particularly advantageous for pronominal mentions, which we might like to model 1 We assume nested mentions are ordered by their syntactic heads. 995 as linking to a single nominal or proper antecedent, for example, but not necessarily to all other coreferent mentions. Accordingly, in this paper we attempt to maintain the inferential simplicity and modeling benefits of mention ranking, while allowing the model to utilize global, structural information relating to z in making its predictions. We therefore investigate objective functions"
N16-1114,D08-1069,0,0.183256,"and so we may represent a clustering with a vector z ∈ {1, . . . , M }N , where zn = m iff xn is a member of X (m) . Coreference systems attempt to find the best clustering z ∗ ∈ Z under some scoring function, with Z the set of valid clusterings. One strategy to avoid the computational intractability associated with predicting an entire clustering z is to instead predict a single antecedent for each mention xn ; because xn may not be anaphoric (and therefore have no antecedents), a “dummy” antecedent  may also be predicted. The aforementioned strategy is adopted by “mention-ranking” systems (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013), which, formally, predict an antecedent yˆ ∈ Y(xn ) for each mention xn , where Y(xn ) = {1, . . . , n − 1, }. Through transitivity, these decisions induce a clustering over the document. Mention-ranking systems make their antecedent predictions with a local scoring function f (xn , y) defined for any mention xn and any antecedent y ∈ Y(xn ). While such a scoring function clearly ignores much structural information, the mentionranking approach has been attractive for at least two reasons. First, inference is relatively simple and efficient, requ"
N16-1114,D13-1203,0,0.681581,"cal classifier with fixed context (that is, as a history-based model). As such, unlike several recent approaches, which may require complicated inference during training, we are able to train our model in much the same way as a vanilla mentionranking model. Experiments compare the use of learned global features to several strong baseline systems for coreference resolution. We demonstrate that the learned global representations capture important underlying information that can help resolve difficult pronominal mentions, which remain a persistent source of errors for modern coreference systems (Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Wiseman et al., 2015; Martschat and Strube, 2015). Our final system improves over 0.8 points in CoNLL score over the current state of the art, and the improvement is statistically significant on all three CoNLL metrics. 2 Background and Notation Coreference resolution is fundamentally a clustering task. Given a sequence (xn )N n=1 of (intra-document) mentions – that is, syntactic units that can refer or be referred to – coreference resolution involves partitioning (xn ) into a sequence of clusters (X (m) )M m=1 such that all the mentions in any particular cluster"
N16-1114,Q14-1037,0,0.0979,"Missing"
N16-1114,P15-1033,0,0.0249897,"Missing"
N16-1114,N10-1061,0,0.0687172,"structured approaches to coreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention-ranking models, which select a single antecedent for each anaphoric mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013; Wiseman et al., 2015). Structured approaches typically divide between those that induce a clustering of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, 2010), and, more recently, those that learn a latent tree of mentions (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). There have also been structured approaches that merge the mention-ranking and mention-pair ideas in some way. For instance, Rahman and Ng (2011) rank clusters rather than mentions; Clark and Manning (2015) use the output of both mention-ranking and mention pair systems to learn a clustering. The application of RNNs to modeling (the trajectory of) the state of a cluster is apparently novel, though it"
N16-1114,N06-2015,0,0.116337,"5: 6: 7: 8: 9: 10: 11: 12: y∈Y(xn ) m ← zy ∗ if y ∗ =  then M ←M +1 m←M append xn to X (m) zn ← m h(m) ← RNN(hc (xn ), h(m) ) return X (1) , . . . , X (M ) is shown in Algorithm 1. The greedy search algorithm is identical to a simple mention-ranking system, with the exception of line 11, which updates the current RNN representation based on the previous decision that was made, and line 4, which then uses this cluster representation as part of scoring. 6 Experiments 6.1 Methods We run experiments on the CoNLL 2012 English shared task (Pradhan et al., 2012). The task uses the OntoNotes corpus (Hovy et al., 2006), consisting of 3,493 documents in various domains and formats. We use the experimental split provided in the shared task. For all experiments, we use the Berkeley Coreference System (Durrett and Klein, 2013) for mention extraction and to compute features φa and φp . Features We use the raw BASIC + feature sets described by Wiseman et al. (2015), with the following modifications: • We remove all features from φp that concatenate a feature of the antecedent with a feature of the current mention, such as bi-head features. • We add true-cased head features, a current speaker indicator feature, an"
N16-1114,W04-3250,0,0.0132488,"59 72.22 72.60 73.42 P B3 R F1 P CEAFe R F1 CoNLL 62.71 66.12 65.64 66.07 66.83 54.96 54.22 56.01 55.83 56.95 58.58 59.58 60.44 60.50 60.52 61.50 59.40 59.47 59.44 59.41 62.14 52.27 52.33 52.98 54.88 53.85 55.61 55.67 56.02 56.37 57.05 57.70 61.63 62.47 63.02 63.03 63.39 64.21 Table 1: Results on CoNLL 2012 English test set. We compare against recent state of the art systems, including (in order) Bjorkelund and Kuhn (2014), Martschat and Strube (2015), Clark and Manning (2015), Peng et al. (2015), and Wiseman et al. (2015). F1 gains are significant (p < 0.05 under the bootstrap resample test (Koehn, 2004)) compared with Wiseman et al. (2015) for all metrics. genre (out of {bc,bn,mz,nw,pt,tc,wb}) indicator to φp and φa . • We add features indicating if a mention has a substring overlap with the current speaker (φp and φa ), and if an antecedent has a substring overlap with a speaker distinct from the current mention’s speaker (φp ). • We add a single centered, rescaled document position feature to each mention when learning hc . We calculate a mention xn ’s rescaled doc−1 ument position as 2n−N N −1 . These modifications result in there being approximately 14K distinct features in φa and approx"
N16-1114,D13-1027,0,0.0294159,"context (that is, as a history-based model). As such, unlike several recent approaches, which may require complicated inference during training, we are able to train our model in much the same way as a vanilla mentionranking model. Experiments compare the use of learned global features to several strong baseline systems for coreference resolution. We demonstrate that the learned global representations capture important underlying information that can help resolve difficult pronominal mentions, which remain a persistent source of errors for modern coreference systems (Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Wiseman et al., 2015; Martschat and Strube, 2015). Our final system improves over 0.8 points in CoNLL score over the current state of the art, and the improvement is statistically significant on all three CoNLL metrics. 2 Background and Notation Coreference resolution is fundamentally a clustering task. Given a sequence (xn )N n=1 of (intra-document) mentions – that is, syntactic units that can refer or be referred to – coreference resolution involves partitioning (xn ) into a sequence of clusters (X (m) )M m=1 such that all the mentions in any particular cluster 994 Proceedings of NAACL-HLT"
N16-1114,N16-1082,0,0.0151961,"since companies are generally not coreferent with pronouns like “his.” Figure 4 shows an example (consisting of a telephone conversation between “A” and “B”) in which the bracketed pronoun “It’s” is being used pleonastically. Whereas the baseline MR model predicts “It’s” to corefer with a previous “it” — thus making a FL error — the greedy RNN model does not. In Figure 4 the final mention in three preceding clusters is shaded so its intensity corresponds to the magnitude of the gradient of the NA term in g with respect to that mention. This visualization resembles the “saliency” technique of Li et al. (2016), and it attempts to gives a sense of the contribution of a (preceding) cluster in the calculation of the NA score. We see that the potential antecedent “S-Bahn” has a large gradient, but also that the initial, obviously pleonastic use of “it’s” has a large gradient, 1002 Related Work In addition to the related work noted throughout, we add supplementary references here. Unstructured approaches to coreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 20"
N16-1114,P14-2005,0,0.0223759,"forming the dot-product scores. 1000 MR Avg, OH RNN, GH RNN, OH MUC B3 CEAFe CoNLL 73.06 73.30 73.63 74.26 62.66 63.06 63.23 63.89 58.98 58.85 59.56 59.54 64.90 65.07 65.47 65.90 Table 2: F1 scores of models described in text on CoNLL 2012 development set. Rows in grey highlight models using oracle history. Following Wiseman et al. (2015) we use the costweights α = h0.5, 1.2, 1i in defining ∆, and we use their pre-training scheme as well. For final results, we train on both training and development portions of the CoNLL data. Scoring uses the official CoNLL 2012 script (Pradhan et al., 2014; Luo et al., 2014). Code for our system is available at https: //github.com/swiseman/nn_coref. The system makes use of a GPU for training, and trains in about two hours. 6.2 Results In Table 1 we present our main results on the CoNLL English test set, and compare with other recent stateof-the-art systems. We see a statistically significant improvement of over 0.8 CoNLL points over the previous state of the art, and the highest F1 scores to date on all three CoNLL metrics. We now consider in more detail the impact of global features and RNNs on performance. For these experiments, we report MUC, B3 , and CEAFe F1"
N16-1114,Q15-1029,0,0.81448,"uch, unlike several recent approaches, which may require complicated inference during training, we are able to train our model in much the same way as a vanilla mentionranking model. Experiments compare the use of learned global features to several strong baseline systems for coreference resolution. We demonstrate that the learned global representations capture important underlying information that can help resolve difficult pronominal mentions, which remain a persistent source of errors for modern coreference systems (Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Wiseman et al., 2015; Martschat and Strube, 2015). Our final system improves over 0.8 points in CoNLL score over the current state of the art, and the improvement is statistically significant on all three CoNLL metrics. 2 Background and Notation Coreference resolution is fundamentally a clustering task. Given a sequence (xn )N n=1 of (intra-document) mentions – that is, syntactic units that can refer or be referred to – coreference resolution involves partitioning (xn ) into a sequence of clusters (X (m) )M m=1 such that all the mentions in any particular cluster 994 Proceedings of NAACL-HLT 2016, pages 994–1004, c San Diego, California, Jun"
N16-1114,N15-3002,0,0.0202065,"Missing"
N16-1114,C02-1139,0,0.175186,"e “saliency” technique of Li et al. (2016), and it attempts to gives a sense of the contribution of a (preceding) cluster in the calculation of the NA score. We see that the potential antecedent “S-Bahn” has a large gradient, but also that the initial, obviously pleonastic use of “it’s” has a large gradient, 1002 Related Work In addition to the related work noted throughout, we add supplementary references here. Unstructured approaches to coreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention-ranking models, which select a single antecedent for each anaphoric mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013; Wiseman et al., 2015). Structured approaches typically divide between those that induce a clustering of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, 2010), and, more recently, those that learn a latent tree of mentions (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat"
N16-1114,K15-1002,0,0.291432,"vel features improved their results, whereas Martschat and Strube (2015) found that they did not. Clark and Manning (2015) found that incorporating cluster-level features beyond those involving the precomputed mention-pair and mention-ranking probabilities that form the basis of their agglomerative clustering coreference system did not improve performance. Furthermore, among recent, state-of-theart systems, mention-ranking systems (which are completely local) perform at least as well as their more structured counterparts (Durrett and Klein, 2014; Clark and Manning, 2015; Wiseman et al., 2015; Peng et al., 2015). 3.2 Issues with Global Features We believe a major reason for the relative ineffectiveness of global features in coreference problems is that, as noted by Clark and Manning (2015), cluster-level features can be hard to define. Specif996 ically, it is difficult to define discrete, fixed-length features on clusters, which can be of variable size (or shape). As a result, global coreference features tend to be either too coarse or too sparse. Thus, early attempts at defining cluster-level features simply applied the coarse quantifier predicates all, none, most to the mention-level features defin"
N16-1114,D08-1068,0,0.0359802,"ntary references here. Unstructured approaches to coreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention-ranking models, which select a single antecedent for each anaphoric mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013; Wiseman et al., 2015). Structured approaches typically divide between those that induce a clustering of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, 2010), and, more recently, those that learn a latent tree of mentions (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). There have also been structured approaches that merge the mention-ranking and mention-pair ideas in some way. For instance, Rahman and Ng (2011) rank clusters rather than mentions; Clark and Manning (2015) use the output of both mention-ranking and mention pair systems to learn a clustering. The application of RNNs to modeling (the trajectory of) the state of a cluster is ap"
N16-1114,W12-4501,0,0.271279,". . N do 4: y ∗ ← arg max f (xn , y) + g(xn , y, z 1:n−1 ) 5: 6: 7: 8: 9: 10: 11: 12: y∈Y(xn ) m ← zy ∗ if y ∗ =  then M ←M +1 m←M append xn to X (m) zn ← m h(m) ← RNN(hc (xn ), h(m) ) return X (1) , . . . , X (M ) is shown in Algorithm 1. The greedy search algorithm is identical to a simple mention-ranking system, with the exception of line 11, which updates the current RNN representation based on the previous decision that was made, and line 4, which then uses this cluster representation as part of scoring. 6 Experiments 6.1 Methods We run experiments on the CoNLL 2012 English shared task (Pradhan et al., 2012). The task uses the OntoNotes corpus (Hovy et al., 2006), consisting of 3,493 documents in various domains and formats. We use the experimental split provided in the shared task. For all experiments, we use the Berkeley Coreference System (Durrett and Klein, 2013) for mention extraction and to compute features φa and φp . Features We use the raw BASIC + feature sets described by Wiseman et al. (2015), with the following modifications: • We remove all features from φp that concatenate a feature of the antecedent with a feature of the current mention, such as bi-head features. • We add true-case"
N16-1114,P14-2006,0,0.0831968,"the LSTM states before forming the dot-product scores. 1000 MR Avg, OH RNN, GH RNN, OH MUC B3 CEAFe CoNLL 73.06 73.30 73.63 74.26 62.66 63.06 63.23 63.89 58.98 58.85 59.56 59.54 64.90 65.07 65.47 65.90 Table 2: F1 scores of models described in text on CoNLL 2012 development set. Rows in grey highlight models using oracle history. Following Wiseman et al. (2015) we use the costweights α = h0.5, 1.2, 1i in defining ∆, and we use their pre-training scheme as well. For final results, we train on both training and development portions of the CoNLL data. Scoring uses the official CoNLL 2012 script (Pradhan et al., 2014; Luo et al., 2014). Code for our system is available at https: //github.com/swiseman/nn_coref. The system makes use of a GPU for training, and trains in about two hours. 6.2 Results In Table 1 we present our main results on the CoNLL English test set, and compare with other recent stateof-the-art systems. We see a statistically significant improvement of over 0.8 CoNLL points over the previous state of the art, and the highest F1 scores to date on all three CoNLL metrics. We now consider in more detail the impact of global features and RNNs on performance. For these experiments, we report MUC"
N16-1114,D09-1101,0,0.730904,"lustering with a vector z ∈ {1, . . . , M }N , where zn = m iff xn is a member of X (m) . Coreference systems attempt to find the best clustering z ∗ ∈ Z under some scoring function, with Z the set of valid clusterings. One strategy to avoid the computational intractability associated with predicting an entire clustering z is to instead predict a single antecedent for each mention xn ; because xn may not be anaphoric (and therefore have no antecedents), a “dummy” antecedent  may also be predicted. The aforementioned strategy is adopted by “mention-ranking” systems (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013), which, formally, predict an antecedent yˆ ∈ Y(xn ) for each mention xn , where Y(xn ) = {1, . . . , n − 1, }. Through transitivity, these decisions induce a clustering over the document. Mention-ranking systems make their antecedent predictions with a local scoring function f (xn , y) defined for any mention xn and any antecedent y ∈ Y(xn ). While such a scoring function clearly ignores much structural information, the mentionranking approach has been attractive for at least two reasons. First, inference is relatively simple and efficient, requiring only a left-to-"
N16-1114,J01-4004,0,0.413935,"zation resembles the “saliency” technique of Li et al. (2016), and it attempts to gives a sense of the contribution of a (preceding) cluster in the calculation of the NA score. We see that the potential antecedent “S-Bahn” has a large gradient, but also that the initial, obviously pleonastic use of “it’s” has a large gradient, 1002 Related Work In addition to the related work noted throughout, we add supplementary references here. Unstructured approaches to coreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention-ranking models, which select a single antecedent for each anaphoric mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013; Wiseman et al., 2015). Structured approaches typically divide between those that induce a clustering of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, 2010), and, more recently, those that learn a latent tree of mentions (Fernandes et al., 2012; Bj¨orkelund and"
N16-1114,C12-1154,0,0.0479564,"oreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention-ranking models, which select a single antecedent for each anaphoric mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013; Wiseman et al., 2015). Structured approaches typically divide between those that induce a clustering of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, 2010), and, more recently, those that learn a latent tree of mentions (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). There have also been structured approaches that merge the mention-ranking and mention-pair ideas in some way. For instance, Rahman and Ng (2011) rank clusters rather than mentions; Clark and Manning (2015) use the output of both mention-ranking and mention pair systems to learn a clustering. The application of RNNs to modeling (the trajectory of) the state of a cluster is apparently novel, though it bears some similarity to th"
N16-1114,P15-1137,1,0.110816,"ory-based model). As such, unlike several recent approaches, which may require complicated inference during training, we are able to train our model in much the same way as a vanilla mentionranking model. Experiments compare the use of learned global features to several strong baseline systems for coreference resolution. We demonstrate that the learned global representations capture important underlying information that can help resolve difficult pronominal mentions, which remain a persistent source of errors for modern coreference systems (Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Wiseman et al., 2015; Martschat and Strube, 2015). Our final system improves over 0.8 points in CoNLL score over the current state of the art, and the improvement is statistically significant on all three CoNLL metrics. 2 Background and Notation Coreference resolution is fundamentally a clustering task. Given a sequence (xn )N n=1 of (intra-document) mentions – that is, syntactic units that can refer or be referred to – coreference resolution involves partitioning (xn ) into a sequence of clusters (X (m) )M m=1 such that all the mentions in any particular cluster 994 Proceedings of NAACL-HLT 2016, pages 994–1004,"
N16-1114,D08-1067,0,\N,Missing
N19-1114,P17-2021,0,0.0614883,"ork and the generative model) to avoid posterior collapse. Finally, the URNNG also seemed to rely heavily on punctuation to identify constituents and we were unable to improve upon a right-branching baseline when training the URNNG on a version of PTB where punctuation is removed.22 5 Related Work There has been much work on incorporating tree structures into deep models for syntax-aware language modeling, both for unconditional (Emami and Jelinek, 2005; Buys and Blunsom, 2015; Dyer et al., 2016) and conditional (Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; Gu et al., 2018) cases. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees directly from words often em"
N19-1114,N19-1116,0,0.263685,"on the full dataset.19 For RNNG/URNNG we obtain the highest scoring 17 We fine-tune for 10 epochs and use a smaller learning rate of 0.1 for the generative model. 18 To parse the training set we use the benepar en2 model from https://github.com/nikitakit/self-attentive-parser, which obtains an F1 score of 95.17 on the PTB test set. 19 Past work on grammar induction usually train/evaluate on short sentences and also assume access to gold POS tags (Klein and Manning, 2002; Smith and Eisner, 2004; Bod, 2006). However more recent works do train directly words (Jin et al., 2018; Shen et al., 2018; Drozdov et al., 2019). PPL 169.3 113.4 102.4 101.2 100.9 138.6 100.7 107.6 125.2 96.7 93.2 90.6 88.7 85.9 1M Sentences PPL † 77.7 77.4 71.8 72.9 72.0 PRPN (Shen et al., 2018) RNNLM URNNG RNNG‡ RNNG‡ → URNNG Table 2: (Top) Comparison of this work as a language model against prior works on sentence-level PTB with preprocessing from Dyer et al. (2016). Note that previous versions of RNNG differ from ours in terms of parameterization and model size. (Bottom) Results on a subset (1M sentences) of the one billion word corpus. PRPN† is the model from Shen et al. (2018), whose hyperparameters were tuned by us. RNNG‡ is tr"
N19-1114,P06-1109,0,0.234657,"lso shows the F1 scores for grammar induction. Note that we induce latent trees directly from words on the full dataset.19 For RNNG/URNNG we obtain the highest scoring 17 We fine-tune for 10 epochs and use a smaller learning rate of 0.1 for the generative model. 18 To parse the training set we use the benepar en2 model from https://github.com/nikitakit/self-attentive-parser, which obtains an F1 score of 95.17 on the PTB test set. 19 Past work on grammar induction usually train/evaluate on short sentences and also assume access to gold POS tags (Klein and Manning, 2002; Smith and Eisner, 2004; Bod, 2006). However more recent works do train directly words (Jin et al., 2018; Shen et al., 2018; Drozdov et al., 2019). PPL 169.3 113.4 102.4 101.2 100.9 138.6 100.7 107.6 125.2 96.7 93.2 90.6 88.7 85.9 1M Sentences PPL † 77.7 77.4 71.8 72.9 72.0 PRPN (Shen et al., 2018) RNNLM URNNG RNNG‡ RNNG‡ → URNNG Table 2: (Top) Comparison of this work as a language model against prior works on sentence-level PTB with preprocessing from Dyer et al. (2016). Note that previous versions of RNNG differ from ours in terms of parameterization and model size. (Bottom) Results on a subset (1M sentences) of the one billi"
N19-1114,P15-1030,0,0.197267,"cting inductive bias. Specifically we employ amortized variational inference (Kingma and Welling, 2014; Rezende et al., 2014; Mnih and Gregor, 2014) with a structured inference network. Variational inference lets us tractably optimize a lower bound on the log marginal likelihood, while employing a structured inference network encourages non-trivial structure. In particular, a con1105 Proceedings of NAACL-HLT 2019, pages 1105–1117 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ditional random field (CRF) constituency parser (Finkel et al., 2008; Durrett and Klein, 2015), which makes significant independence assumptions, acts as a guide on the generative model to learn meaningful trees through regularizing the posterior (Ganchev et al., 2010). We experiment with URNNGs on English and Chinese and observe that they perform well as language models compared to their supervised counterparts and standard neural LMs. In terms of grammar induction, they are competitive with recently-proposed neural architectures that discover tree-like structures through gated attention (Shen et al., 2018). Our results, along with other recent work on joint language modeling/structur"
N19-1114,K16-1002,0,0.280211,"pout of 0.5. We share word embeddings between the generative model and the inference network, and also tie weights between the input/output word embeddings (Press and Wolf, 2016). Optimization of the model itself required standard techniques for avoiding posterior collapse in VAEs.12 We warm-up the ELBO objective by linearly annealing (per batch) the weight on the conditional prior log pθ (z |x<z ) and the entropy H[qφ (z |x)] from 0 to 1 over the first two epochs (see equation (1) for definition of log pθ (z |x<z )). This is analogous to KL-annealing in VAEs with continuous latent variables (Bowman et al., 2016; Sønderby et al., 2016). We train for 18 epochs (enough for convergence for all models) with a batch size of 16 and K = 8 samples for the Monte Carlo gradient estimators. The generative model is optimized with SGD with learning rate equal to 1, 12 Posterior collapse in our context means that qφ (z |x) always produced trivial (always left or right branching) trees. 1109 except for the affine layer that produces a distribution over the actions, which has learning rate 0.1. Gradients of the generative model are clipped at 5. The inference network is optimized with Adam (Kingma and Ba, 2015) with"
N19-1114,P15-1033,1,0.8078,"te a sentence of length T , and z ∈ ZT to denote an unlabeled binary parse tree over a sequence of length T , represented as a a binary vector of length 2T − 1. Here 0 and 1 correspond to SHIFT and REDUCE actions, explained below.1 Figure 1 presents an overview of our approach. 2.1 Generative Model An RNNG defines a joint probability distribution pθ (x, z) over sentences x and parse trees z. We consider a simplified version of the original RNNG (Dyer et al., 2016) by ignoring constituent labels and only considering binary trees. The RNNG utilizes an RNN to parameterize a stack data structure (Dyer et al., 2015) of partiallycompleted constituents to incrementally build the parse tree while generating terminals. Using the current stack representation, the model samples an action (SHIFT or REDUCE): SHIFT generates a terminal symbol, i.e. word, and shifts it onto the stack,2 REDUCE pops the last two elements off the stack, composes them, and shifts the composed Figure 1: Overview of our approach. The inference network qφ (z |x) (left) is a CRF parser which produces a distribution over binary trees (shown in dotted box). Bij are random variables for existence of a constituent spanning i-th and j-th words"
N19-1114,W17-4303,0,0.0207434,"to rely on punctuation, similar to recent works such as depth-bounded PCFGs (Jin et al., 2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Liu and Lapata, 2018; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; Liu et al., 2018), policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoenco"
N19-1114,N16-1024,1,0.100545,"of RNNGs. Since directly marginalizing over the space of latent trees is intractable, we instead apply amortized variational inference. To maximize the evidence lower bound, we develop an inference network parameterized as a neural CRF constituency parser. On language modeling, unsupervised RNNGs perform as well their supervised counterparts on benchmarks in English and Chinese. On constituency grammar induction, they are competitive with recent neural language models that induce tree structures from words through attention mechanisms. 1 Introduction Recurrent neural network grammars (RNNGs) (Dyer et al., 2016) model sentences by first generating a nested, hierarchical syntactic structure which is used to construct a context representation to be conditioned upon for upcoming words. Supervised RNNGs have been shown to outperform standard sequential language models, achieve excellent results on parsing (Dyer et al., 2016; Kuncoro et al., 2017), better encode syntactic properties of language (Kuncoro et al., 2018), and correlate with electrophysiological responses in the human brain (Hale et al., 2018). However, these all require annotated syntactic trees for training. In this work, we explore unsuperv"
N19-1114,P15-2142,0,0.114578,"ten qφ (z |x) by dividing span scores sij by a temperature term 2.0 before feeding it to the CRF. 15 Using the code from https://github.com/yikangshen/ PRPN, we tuned model size, initialization, dropout, learning rate, and use of batch normalization. 16 RNNG is trained to maximize log pθ (x, z) while URNNG is trained to maximize (a lower bound on) the language modeling objective log pθ (x). 1110 PTB KN 5-gram (Dyer et al., 2016) RNNLM (Dyer et al., 2016) Original RNNG (Dyer et al., 2016) Stack-only RNNG (Kuncoro et al., 2017) Gated-Attention RNNG (Kuncoro et al., 2017) Generative Dep. Parser (Buys and Blunsom, 2015) RNNLM (Buys and Blunsom, 2018) Sup. Syntactic NLM (Buys and Blunsom, 2018) Unsup. Syntactic NLM (Buys and Blunsom, 2018) PRPN† (Shen et al., 2018) This work: RNNLM URNNG RNNG RNNG → URNNG Figure 2: Perplexity of the different models grouped by sentence length on PTB. modeling of syntax helps generalization even with richly-parameterized neural models. Encouraged by these observations, we also experiment with a hybrid approach where we train a supervised RNNG first and continue fine-tuning the model (including the inference network) on the URNNG objective (RNNG → URNNG in Table 1).17 This appr"
N19-1114,N18-1086,0,0.206657,"2 each epoch after the first epoch at which validation performance does not improve, but this learning rate decay is not triggered for the first eight epochs to ensure adequate training. We use the same hyperparameters/training setup for both PTB and CTB. For experiments on (the subset of) the one billion word corpus, we use a smaller dropout rate of 0.1. The baseline RNNLM also uses the smaller dropout rate. All models are trained with an end-of-sentence token, but for perplexity calculation these tokens are not counted to be comparable to prior work (Dyer et al., 2016; Kuncoro et al., 2017; Buys and Blunsom, 2018). To be more precise, the inference network does not make use of the end-of-sentence token to produce parse trees, but the generative model is trained to generate the end-of-sentence token after the final REDUCE operation. 3.3 Baselines We compare the unsupervised RNNG (URNNG) against several baselines: (1) RNNLM, a standard RNN language model whose size is the same as URNNG’s stack LSTM; (2) Parsing Reading Predict Network (PRPN) (Shen et al., 2018), a neural language model that uses gated attention layers to embed soft tree-like structures into a neural network (and among the current state-o"
N19-1114,P17-2012,0,0.0739657,") to avoid posterior collapse. Finally, the URNNG also seemed to rely heavily on punctuation to identify constituents and we were unable to improve upon a right-branching baseline when training the URNNG on a version of PTB where punctuation is removed.22 5 Related Work There has been much work on incorporating tree structures into deep models for syntax-aware language modeling, both for unconditional (Emami and Jelinek, 2005; Buys and Blunsom, 2015; Dyer et al., 2016) and conditional (Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; Gu et al., 2018) cases. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees directly from words often employ additional heurist"
N19-1114,D17-1171,0,0.057174,"tama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al., 2016; Krishnan et al., 2017), which have been used previously for unsupervised (Cai et al., 2017; Drozdov et al., 2019; Li et al., 2019) and semi-supervised (Yin et al., 2018; Corro and Titov, 2019) parsing. 6 Conclusion It is an open question as to whether explicit modeling of syntax significantly helps neural models. Strubell et al. (2018) find that supervising intermediate attention layers with syntactic heads improves semantic role labeling, while Shi et al. (2018) observe that for text classification, syntactic trees only have marginal impact. Our work suggests that at least for language modeling, incorporating syntax either via explicit supervision or as latent variables does provi"
N19-1114,D14-1082,0,0.0565262,"lso test our approach on a subset of the one billion word 10 https://github.com/clab/rnng Both versions of the PTB data can be obtained from http: //demo.clab.cs.cmu.edu/cdyer/ptb-lm.tar.gz. 11 corpus (Chelba et al., 2013). We randomly sample 1M sentences for training and 2K sentences for validation/test, and limit the vocabulary to 30K word types. While still a subset of the full corpus (which has 30M sentences), this dataset is two orders of magnitude larger than PTB. Experiments on Chinese utilize version 5.1 of the Chinese Penn Treebank (CTB) (Xue et al., 2005), with the same splits as in Chen and Manning (2014). Singleton words are replaced with a single hUNKi token, resulting in a vocabulary of 17,489 word types. 3.2 Training and Hyperparameters The stack LSTM has two layers with input/hidden size equal to 650 and dropout of 0.5. The tree LSTM also has 650 units. The inference network uses a one-layer bidirectional LSTM with 256 hidden units, and the MLP (to produce span scores sij for i ≤ j) has a single hidden layer with a ReLU nonlinearity followed by layer normalization (Ba et al., 2016) and dropout of 0.5. We share word embeddings between the generative model and the inference network, and als"
N19-1114,W14-4012,0,0.0576477,"Missing"
N19-1114,P08-1109,0,0.0709613,"Missing"
N19-1114,W06-1673,0,0.0185025,"018) observe a similar phenomenon in the context of learning latent trees for classification tasks. However Li et al. (2019) find that it is possible use a transition-based parser as the inference network for dependency grammar induction, if the inference network is constrained via posterior regularization (Ganchev et al., 2010) based on universal syntactic rules (Naseem et al., 2010). K 1 X ∇θ log pθ (x, z(k) ), K k=1 with samples z(1) , . . . , z(K) from qφ (z |x). Sampling uses the intermediate values calculated during the inside algorithm to sample split points recursively (Goodman, 1998; Finkel et al., 2006), as shown in Algorithm 2. The gradient with respect to φ involves two parts. The entropy term H[qφ (z |x)] can be calculated exactly in O(T 3 ), again using the intermediate values from the inside algorithm (see Algorithm 3).8 Since each step of this dynamic program is differentiable, we can obtain the gradient ∇φ H[qφ (z |x)] using automatic differentation.9 An estimator for the gradient with respect to Eqφ (z |x) [log pθ (x, z)] is obtained via the score function gradient estimator (Glynn, 1987; Williams, 1992), ∇φ Eqφ (z |x) [log pθ (x, z)] = Eqφ (z |x) [log pθ (x, z)∇φ log qφ (z |x)] ≈ K"
N19-1114,D18-1037,0,0.0314417,"e URNNG also seemed to rely heavily on punctuation to identify constituents and we were unable to improve upon a right-branching baseline when training the URNNG on a version of PTB where punctuation is removed.22 5 Related Work There has been much work on incorporating tree structures into deep models for syntax-aware language modeling, both for unconditional (Emami and Jelinek, 2005; Buys and Blunsom, 2015; Dyer et al., 2016) and conditional (Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; Gu et al., 2018) cases. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees directly from words often employ additional heuristics based on punctuation (Seginer, 20"
N19-1114,P18-1254,1,0.850713,"es from words through attention mechanisms. 1 Introduction Recurrent neural network grammars (RNNGs) (Dyer et al., 2016) model sentences by first generating a nested, hierarchical syntactic structure which is used to construct a context representation to be conditioned upon for upcoming words. Supervised RNNGs have been shown to outperform standard sequential language models, achieve excellent results on parsing (Dyer et al., 2016; Kuncoro et al., 2017), better encode syntactic properties of language (Kuncoro et al., 2018), and correlate with electrophysiological responses in the human brain (Hale et al., 2018). However, these all require annotated syntactic trees for training. In this work, we explore unsupervised learning of recurrent neural network grammars for language modeling and grammar induction. Work done while the first author was an intern at DeepMind. Code available at https://github.com/harvardnlp/urnng The standard setup for unsupervised structure learning is to define a generative model pθ (x, z) over observed data x (e.g. sentence) and unobserved structure z (e.g. parse tree, part-of-speech sequence), and maximizeP the log marginal likelihood log pθ (x) = log z pθ (x, z). Successful"
N19-1114,N19-1115,0,0.0703663,"duce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Liu and Lapata, 2018; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; Liu et al., 2018), policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al., 2016; Krishnan et al., 2017), which have been used previously for unsupervised (Cai et al., 2017; Drozdov et al., 2019; Li et al., 2019) and sem"
N19-1114,W18-5452,0,0.0592617,"a subset (1M sentences) of the one billion word corpus. PRPN† is the model from Shen et al. (2018), whose hyperparameters were tuned by us. RNNG‡ is trained on predicted parse trees from Kitaev and Klein (2018). tree from qφ (z |x) through the Viterbi inside (i.e. CKY) algorithm. We calculate unlabeled F1 using evalb, which ignores punctuation and discards trivial spans (width-one and sentence spans).20 Since we compare F1 against the original, nonbinarized trees (per convention), F1 scores of models using oracle binarized trees constitute the upper bounds. We confirm the replication study of Htut et al. (2018) and find that PRPN is a strong model for grammar induction. URNNG performs on par with PRPN on English but PRPN does better on Chinese; both outperform right branching baselines. Table 3 further analyzes the learned trees and shows the F1 score of URNNG trees against 20 Available at https://nlp.cs.nyu.edu/evalb/. We evaluate with COLLINS.prm parameter file and LABELED option equal to 0. We observe that the setup for grammar induction varies widely across different papers: lexicalized vs. unlexicalized; use of punctuation vs. not; separation of train/test sets; counting sentence-level spans fo"
N19-1114,P99-1010,0,0.350123,"are copied from Table 1 of Drozdov et al. (2019). other trees (left), and the recall of URNNG/PRPN trees against ground truth constituents (right). We find that trees induced by URNNG and PRPN are quite different; URNNG is more sensitive to SBAR and VP, while PRPN is better at identifying NP. While left as future work, this naturally suggests a hybrid approach wherein the intersection of constituents from URNNG and PRPN is used to create a corpus of partially annotated trees, which can be used to guide another model, e.g. via posterior regularization (Ganchev et al., 2010) or semisupervision (Hwa, 1999). Finally, Table 4 compares our results using the same evaluation setup as in Drozdov et al. (2019), which differs considerably from our setup. 4.3 Distributional Metrics Table 5 shows some standard metrics related to the learned generative model/inference network. The “reconstruction” perplexity based on Eqφ (z |x) [log pθ (x |z)] is much lower than regular perplexity, and further, the Kullback-Leibler divergence between the conditional prior and the variational posterior, given by   qφ (z |x) Eqφ (z |x) log , pθ (z |x<z ) PPL Recon. PPL KL Prior Entropy Post. Entropy Unif. Entropy RNNG PTB"
N19-1114,W00-1306,0,0.12894,"alculated exactly in O(T 3 ), again using the intermediate values from the inside algorithm (see Algorithm 3).8 Since each step of this dynamic program is differentiable, we can obtain the gradient ∇φ H[qφ (z |x)] using automatic differentation.9 An estimator for the gradient with respect to Eqφ (z |x) [log pθ (x, z)] is obtained via the score function gradient estimator (Glynn, 1987; Williams, 1992), ∇φ Eqφ (z |x) [log pθ (x, z)] = Eqφ (z |x) [log pθ (x, z)∇φ log qφ (z |x)] ≈ K 1 X log pθ (x, z(k) )∇φ log qφ (z(k) |x). K k=1 8 We adapt the algorithm for calculating tree entropy in PCFGs from Hwa (2000) to the CRF case. 9 ∇φ H[qφ (z |x)] can also be computed using the insideoutside algorithm and a second-order expectation semiring (Li and Eisner, 2009), which has the same asymptotic runtime complexity but generally better constants. 1108 Algorithm 2 Top-down sampling a tree from qφ (z |x) Algorithm 3 Calculating the tree entropy H[qφ (z |x)] 1: procedure S AMPLE(β) . β from running I NSIDE(s) 2: B=0 . binary matrix representation of tree 3: Q = [(1, T )] . queue of constituents 4: while Q is not empty do 5: (i, j) = pop(Q) P 6: τ = j−1 k=i β[i, k] · β[k + 1, j] 7: for k := i to j − 1 do . ge"
N19-1114,Q18-1016,0,0.212843,"uce latent trees directly from words on the full dataset.19 For RNNG/URNNG we obtain the highest scoring 17 We fine-tune for 10 epochs and use a smaller learning rate of 0.1 for the generative model. 18 To parse the training set we use the benepar en2 model from https://github.com/nikitakit/self-attentive-parser, which obtains an F1 score of 95.17 on the PTB test set. 19 Past work on grammar induction usually train/evaluate on short sentences and also assume access to gold POS tags (Klein and Manning, 2002; Smith and Eisner, 2004; Bod, 2006). However more recent works do train directly words (Jin et al., 2018; Shen et al., 2018; Drozdov et al., 2019). PPL 169.3 113.4 102.4 101.2 100.9 138.6 100.7 107.6 125.2 96.7 93.2 90.6 88.7 85.9 1M Sentences PPL † 77.7 77.4 71.8 72.9 72.0 PRPN (Shen et al., 2018) RNNLM URNNG RNNG‡ RNNG‡ → URNNG Table 2: (Top) Comparison of this work as a language model against prior works on sentence-level PTB with preprocessing from Dyer et al. (2016). Note that previous versions of RNNG differ from ours in terms of parameterization and model size. (Bottom) Results on a subset (1M sentences) of the one billion word corpus. PRPN† is the model from Shen et al. (2018), whose hyp"
N19-1114,N07-1018,0,0.163783,"ar induction. Work done while the first author was an intern at DeepMind. Code available at https://github.com/harvardnlp/urnng The standard setup for unsupervised structure learning is to define a generative model pθ (x, z) over observed data x (e.g. sentence) and unobserved structure z (e.g. parse tree, part-of-speech sequence), and maximizeP the log marginal likelihood log pθ (x) = log z pθ (x, z). Successful approaches to unsupervised parsing have made strong conditional independence assumptions (e.g. context-freeness) and employed auxiliary objectives (Klein and Manning, 2002) or priors (Johnson et al., 2007). These strategies imbue the learning process with inductive biases that guide the model to discover meaningful structures while allowing tractable algorithms for marginalization; however, they come at the expense of language modeling performance, particularly compared to sequential neural models that make no independence assumptions. Like RNN language models, RNNGs make no independence assumptions. Instead they encode structural bias through operations that compose linguistic constituents. The lack of independence assumptions contributes to the strong language modeling performance of RNNGs, b"
N19-1114,P18-1249,0,0.441047,"riational autoencoders) learn posterior distributions that are close to the variational family (Cremer et al., 2018). We can use this to our advantage with an inference network that injects inductive bias. We propose to do this by using a context-free model for the inference network, in particular, a neural CRF parser (Durrett and Klein, 2015). This choice 1107 can seen as a form of posterior regularization that limits posterior flexibility of the overly powerful RNNG generative model.6,7 The parameterization of span scores is similar to recent works (Wang and Chang, 2016; Stern et al., 2017; Kitaev and Klein, 2018): we add position embeddings to word embeddings and run a bidirectional LSTM over the input representations to → − → − obtain the forward [ h 1 , . . . , h T ] and backward ← − ← − [ h 1 , . . . , h T ] hidden states. The score sij ∈ R for a constituent spanning xi to xj is given by, → − → − ← − ← − sij = MLP([ h j+1 − h i ; h i−1 − h j ]). Letting B be the binary matrix representation of a tree (Bij = 1 means there is a constituent spanning xi and xj ), the CRF parser defines a distribution over binary trees via the Gibbs distribution, X  1 qφ (B |x) = exp Bij sij , ZT (x) i≤j where ZT (x)"
N19-1114,P02-1017,0,0.736041,"mars for language modeling and grammar induction. Work done while the first author was an intern at DeepMind. Code available at https://github.com/harvardnlp/urnng The standard setup for unsupervised structure learning is to define a generative model pθ (x, z) over observed data x (e.g. sentence) and unobserved structure z (e.g. parse tree, part-of-speech sequence), and maximizeP the log marginal likelihood log pθ (x) = log z pθ (x, z). Successful approaches to unsupervised parsing have made strong conditional independence assumptions (e.g. context-freeness) and employed auxiliary objectives (Klein and Manning, 2002) or priors (Johnson et al., 2007). These strategies imbue the learning process with inductive biases that guide the model to discover meaningful structures while allowing tractable algorithms for marginalization; however, they come at the expense of language modeling performance, particularly compared to sequential neural models that make no independence assumptions. Like RNN language models, RNNGs make no independence assumptions. Instead they encode structural bias through operations that compose linguistic constituents. The lack of independence assumptions contributes to the strong language"
N19-1114,E17-1117,1,0.883284,"Missing"
N19-1114,P18-1132,1,0.848696,"induction, they are competitive with recent neural language models that induce tree structures from words through attention mechanisms. 1 Introduction Recurrent neural network grammars (RNNGs) (Dyer et al., 2016) model sentences by first generating a nested, hierarchical syntactic structure which is used to construct a context representation to be conditioned upon for upcoming words. Supervised RNNGs have been shown to outperform standard sequential language models, achieve excellent results on parsing (Dyer et al., 2016; Kuncoro et al., 2017), better encode syntactic properties of language (Kuncoro et al., 2018), and correlate with electrophysiological responses in the human brain (Hale et al., 2018). However, these all require annotated syntactic trees for training. In this work, we explore unsupervised learning of recurrent neural network grammars for language modeling and grammar induction. Work done while the first author was an intern at DeepMind. Code available at https://github.com/harvardnlp/urnng The standard setup for unsupervised structure learning is to define a generative model pθ (x, z) over observed data x (e.g. sentence) and unobserved structure z (e.g. parse tree, part-of-speech sequ"
N19-1114,D09-1005,0,0.0454807,"c program is differentiable, we can obtain the gradient ∇φ H[qφ (z |x)] using automatic differentation.9 An estimator for the gradient with respect to Eqφ (z |x) [log pθ (x, z)] is obtained via the score function gradient estimator (Glynn, 1987; Williams, 1992), ∇φ Eqφ (z |x) [log pθ (x, z)] = Eqφ (z |x) [log pθ (x, z)∇φ log qφ (z |x)] ≈ K 1 X log pθ (x, z(k) )∇φ log qφ (z(k) |x). K k=1 8 We adapt the algorithm for calculating tree entropy in PCFGs from Hwa (2000) to the CRF case. 9 ∇φ H[qφ (z |x)] can also be computed using the insideoutside algorithm and a second-order expectation semiring (Li and Eisner, 2009), which has the same asymptotic runtime complexity but generally better constants. 1108 Algorithm 2 Top-down sampling a tree from qφ (z |x) Algorithm 3 Calculating the tree entropy H[qφ (z |x)] 1: procedure S AMPLE(β) . β from running I NSIDE(s) 2: B=0 . binary matrix representation of tree 3: Q = [(1, T )] . queue of constituents 4: while Q is not empty do 5: (i, j) = pop(Q) P 6: τ = j−1 k=i β[i, k] · β[k + 1, j] 7: for k := i to j − 1 do . get distribution over splits 8: wk = (β[i, k] · β[k + 1, j])/τ 9: k ∼ Cat([wi , . . . , wj−1 ]) . sample a split point 10: Bi,k = 1, Bk+1,j = 1 . update B"
N19-1114,D18-1184,0,0.0207635,"dov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Liu and Lapata, 2018; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; Liu et al., 2018), policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al., 2016; Krishnan et al., 2017), which have"
N19-1114,Q18-1005,0,0.0290815,"milar to recent works such as depth-bounded PCFGs (Jin et al., 2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Liu and Lapata, 2018; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; Liu et al., 2018), policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 20"
N19-1114,W18-2903,0,0.037053,"terate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Liu and Lapata, 2018; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; Liu et al., 2018), policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al., 2016; Krishnan et al., 2017), which have been used previously for unsupervised (Cai et al., 2017; Drozdov et al., 2019; Li et al., 2019) and semi-supervised (Yin et al., 2018; Corro and Titov, 2019) parsing. 6 Conclusion"
N19-1114,J93-2004,0,0.0658527,"estimator is unbiased but typically suffers from high variance. To reduce variance, we use a control variate derived from an average of the other samples’ joint likelihoods (Mnih and Rezende, 2016), yielding the following estimator, K 1 X (log pθ (x, z(k) ) − r(k) )∇φ log qφ (z(k) |x), K k=1 1 P (j) where r(k) = K−1 j6=k log pθ (x, z ). This control variate worked better than alternatives such as estimates of baselines from an auxiliary network (Mnih and Gregor, 2014; Deng et al., 2018) or a language model (Yin et al., 2018). 3 Experimental Setup 3.1 Data For English we use the Penn Treebank (Marcus et al., 1993, PTB) with splits and preprocessing from Dyer et al. (2016) which retains punctuation and replaces singleton words with Berkeley parser’s mapping rules, resulting in a vocabulary of 23,815 word types.10 Notably this is much larger than the standard PTB LM setup from Mikolov et al. (2010) which uses 10K types.11 Also different from the LM setup, we model each sentence separately instead of carrying information across sentence boundaries, as the RNNG is a generative model of sentences. Hence our perplexity numbers are not comparable to the PTB LM results (Melis et al., 2018; Merity et al., 2018"
N19-1114,D18-1151,0,0.0283266,"rior pθ (z |x<z ), and uniform entropy is the entropy of the uniform distribution over binary trees. is highly nonzero. (See equation (1) for definitions of log pθ (x |z) and log pθ (z |x<z )). This indicates that the latent space is being used in a meaningful way and that there is no posterior collapse (Bowman et al., 2016). As expected, the entropy of the variational posterior is much lower than the entropy of the conditional prior, but there is still some uncertainty in the posterior. 4.4 Syntactic Evaluation We perform a syntactic evaluation of the different models based on the setup from Marvin and Linzen (2018): the model is given two minimally different sentences, one grammatical and one ungrammatical, and must identify the grammatical sentence by assigning it higher probability.21 Table 6 shows the accuracy results. Overall the supervised RNNG significantly outperforms the other models, indicating opportunities for further work in unsupervised modeling. While the URNNG does slightly outperform an RNNLM, the distribution of errors made from both models are similar, and thus it is not clear whether the outperformance is simply due to better perplexity or learning different structural biases. 4.5 Lim"
N19-1114,P05-1010,0,0.0754416,"es gated attention layers to embed soft tree-like structures into a neural network (and among the current state-of-the-art in grammar induction from words on the full corpus); (3) RNNG with trivial trees (left branching, right branching, random); (4) supervised RNNG trained on unlabeled, binarized gold trees.13 Note that the supervised RNNG also trains a discriminative parser qφ (z |x) (alongside the generative model pθ (x, z)) in order to sample parse forests for perplexity evaluation (i.e. importance sampling). This discriminative parser has the same ar13 We use right branching binarization—Matsuzaki et al. (2005) find that differences between various binarization schemes have marginal impact. Our supervised RNNG therefore differs the original RNNG, which trains on nonbinarized trees and does not ignore constituent labels. PTB PPL F1 Model RNNLM PRPN (default) PRPN (tuned) Left Branching Trees Right Branching Trees Random Trees URNNG CTB PPL F1 93.2 126.2 96.7 100.9 93.3 113.2 90.6 – 32.9 41.2 10.3 34.8 17.0 40.7 201.3 290.9 216.0 223.6 203.5 209.1 195.7 – 32.9 36.1 12.4 20.6 17.4 29.1 RNNG RNNG → URNNG 88.7 85.9 68.1 67.7 193.1 181.1 52.3 51.9 Oracle Binary Trees – 82.5 – 88.6 Table 1: Language modeli"
N19-1114,D10-1120,0,0.0336572,"looks at the entire sentence. However we found that under this setup, the inference network degenerated into a local minimum whereby it always generated left-branching trees despite various optimization strategies. Williams et al. (2018) observe a similar phenomenon in the context of learning latent trees for classification tasks. However Li et al. (2019) find that it is possible use a transition-based parser as the inference network for dependency grammar induction, if the inference network is constrained via posterior regularization (Ganchev et al., 2010) based on universal syntactic rules (Naseem et al., 2010). K 1 X ∇θ log pθ (x, z(k) ), K k=1 with samples z(1) , . . . , z(K) from qφ (z |x). Sampling uses the intermediate values calculated during the inside algorithm to sample split points recursively (Goodman, 1998; Finkel et al., 2006), as shown in Algorithm 2. The gradient with respect to φ involves two parts. The entropy term H[qφ (z |x)] can be calculated exactly in O(T 3 ), again using the intermediate values from the inside algorithm (see Algorithm 3).8 Since each step of this dynamic program is differentiable, we can obtain the gradient ∇φ H[qφ (z |x)] using automatic differentation.9 An e"
N19-1114,D18-1108,0,0.0153213,"2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Liu and Lapata, 2018; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; Liu et al., 2018), policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al., 2016; Krishnan et al."
N19-1114,P14-1100,0,0.0571071,"d parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees directly from words often employ additional heuristics based on punctuation (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013; Parikh et al., 2014), as punctuation (e.g. comma) is usually a reliable signal for start/end of constituent spans. The URNNG still has to learn to rely on punctuation, similar to recent works such as depth-bounded PCFGs (Jin et al., 2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of"
N19-1114,P18-1173,0,0.0209789,"PCFGs (Jin et al., 2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Liu and Lapata, 2018; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; Liu et al., 2018), policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al.,"
N19-1114,P11-1108,0,0.0560271,"es. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees directly from words often employ additional heuristics based on punctuation (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013; Parikh et al., 2014), as punctuation (e.g. comma) is usually a reliable signal for start/end of constituent spans. The URNNG still has to learn to rely on punctuation, similar to recent works such as depth-bounded PCFGs (Jin et al., 2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent l"
N19-1114,P17-1105,0,0.0250359,"rs for the inference network and the generative model) to avoid posterior collapse. Finally, the URNNG also seemed to rely heavily on punctuation to identify constituents and we were unable to improve upon a right-branching baseline when training the URNNG on a version of PTB where punctuation is removed.22 5 Related Work There has been much work on incorporating tree structures into deep models for syntax-aware language modeling, both for unconditional (Emami and Jelinek, 2005; Buys and Blunsom, 2015; Dyer et al., 2016) and conditional (Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; Gu et al., 2018) cases. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees"
N19-1114,P07-1049,0,0.119971,"al., 2018) cases. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees directly from words often employ additional heuristics based on punctuation (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013; Parikh et al., 2014), as punctuation (e.g. comma) is usually a reliable signal for start/end of constituent spans. The URNNG still has to learn to rely on punctuation, similar to recent works such as depth-bounded PCFGs (Jin et al., 2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also r"
N19-1114,D18-1492,0,0.0535325,"inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al., 2016; Krishnan et al., 2017), which have been used previously for unsupervised (Cai et al., 2017; Drozdov et al., 2019; Li et al., 2019) and semi-supervised (Yin et al., 2018; Corro and Titov, 2019) parsing. 6 Conclusion It is an open question as to whether explicit modeling of syntax significantly helps neural models. Strubell et al. (2018) find that supervising intermediate attention layers with syntactic heads improves semantic role labeling, while Shi et al. (2018) observe that for text classification, syntactic trees only have marginal impact. Our work suggests that at least for language modeling, incorporating syntax either via explicit supervision or as latent variables does provide useful inductive biases and improves performance. Finally, in modeling child language acquisition, the complex interaction of the parser and the grammatical knowledge being acquired is the object of much investigation (Trueswell and Gleitman, 2007); our work shows that apparently grammatical constraints can emerge from the interaction of a constrained parser and a more ge"
N19-1114,P04-1062,0,0.193764,"mmar Induction Table 1 also shows the F1 scores for grammar induction. Note that we induce latent trees directly from words on the full dataset.19 For RNNG/URNNG we obtain the highest scoring 17 We fine-tune for 10 epochs and use a smaller learning rate of 0.1 for the generative model. 18 To parse the training set we use the benepar en2 model from https://github.com/nikitakit/self-attentive-parser, which obtains an F1 score of 95.17 on the PTB test set. 19 Past work on grammar induction usually train/evaluate on short sentences and also assume access to gold POS tags (Klein and Manning, 2002; Smith and Eisner, 2004; Bod, 2006). However more recent works do train directly words (Jin et al., 2018; Shen et al., 2018; Drozdov et al., 2019). PPL 169.3 113.4 102.4 101.2 100.9 138.6 100.7 107.6 125.2 96.7 93.2 90.6 88.7 85.9 1M Sentences PPL † 77.7 77.4 71.8 72.9 72.0 PRPN (Shen et al., 2018) RNNLM URNNG RNNG‡ RNNG‡ → URNNG Table 2: (Top) Comparison of this work as a language model against prior works on sentence-level PTB with preprocessing from Dyer et al. (2016). Note that previous versions of RNNG differ from ours in terms of parameterization and model size. (Bottom) Results on a subset (1M sentences) of t"
N19-1114,D13-1204,0,0.052756,"enerally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees directly from words often employ additional heuristics based on punctuation (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013; Parikh et al., 2014), as punctuation (e.g. comma) is usually a reliable signal for start/end of constituent spans. The URNNG still has to learn to rely on punctuation, similar to recent works such as depth-bounded PCFGs (Jin et al., 2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning l"
N19-1114,P17-1076,0,0.0846814,"l inference (i.e. variational autoencoders) learn posterior distributions that are close to the variational family (Cremer et al., 2018). We can use this to our advantage with an inference network that injects inductive bias. We propose to do this by using a context-free model for the inference network, in particular, a neural CRF parser (Durrett and Klein, 2015). This choice 1107 can seen as a form of posterior regularization that limits posterior flexibility of the overly powerful RNNG generative model.6,7 The parameterization of span scores is similar to recent works (Wang and Chang, 2016; Stern et al., 2017; Kitaev and Klein, 2018): we add position embeddings to word embeddings and run a bidirectional LSTM over the input representations to → − → − obtain the forward [ h 1 , . . . , h T ] and backward ← − ← − [ h 1 , . . . , h T ] hidden states. The score sij ∈ R for a constituent spanning xi to xj is given by, → − → − ← − ← − sij = MLP([ h j+1 − h i ; h i−1 − h j ]). Letting B be the binary matrix representation of a tree (Bij = 1 means there is a constituent spanning xi and xj ), the CRF parser defines a distribution over binary trees via the Gibbs distribution, X  1 qφ (B |x) = exp Bij sij ,"
N19-1114,D18-1548,0,0.0362,"2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al., 2016; Krishnan et al., 2017), which have been used previously for unsupervised (Cai et al., 2017; Drozdov et al., 2019; Li et al., 2019) and semi-supervised (Yin et al., 2018; Corro and Titov, 2019) parsing. 6 Conclusion It is an open question as to whether explicit modeling of syntax significantly helps neural models. Strubell et al. (2018) find that supervising intermediate attention layers with syntactic heads improves semantic role labeling, while Shi et al. (2018) observe that for text classification, syntactic trees only have marginal impact. Our work suggests that at least for language modeling, incorporating syntax either via explicit supervision or as latent variables does provide useful inductive biases and improves performance. Finally, in modeling child language acquisition, the complex interaction of the parser and the grammatical knowledge being acquired is the object of much investigation (Trueswell and Gleitman, 2"
N19-1114,P15-1150,0,0.244109,"Missing"
N19-1114,W18-2704,0,0.0354662,"such as depth-bounded PCFGs (Jin et al., 2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Liu and Lapata, 2018; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; Liu et al., 2018), policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VA"
N19-1114,P16-1218,0,0.034818,"h amortized variational inference (i.e. variational autoencoders) learn posterior distributions that are close to the variational family (Cremer et al., 2018). We can use this to our advantage with an inference network that injects inductive bias. We propose to do this by using a context-free model for the inference network, in particular, a neural CRF parser (Durrett and Klein, 2015). This choice 1107 can seen as a form of posterior regularization that limits posterior flexibility of the overly powerful RNNG generative model.6,7 The parameterization of span scores is similar to recent works (Wang and Chang, 2016; Stern et al., 2017; Kitaev and Klein, 2018): we add position embeddings to word embeddings and run a bidirectional LSTM over the input representations to → − → − obtain the forward [ h 1 , . . . , h T ] and backward ← − ← − [ h 1 , . . . , h T ] hidden states. The score sij ∈ R for a constituent spanning xi to xj is given by, → − → − ← − ← − sij = MLP([ h j+1 − h i ; h i−1 − h j ]). Letting B be the binary matrix representation of a tree (Bij = 1 means there is a constituent spanning xi and xj ), the CRF parser defines a distribution over binary trees via the Gibbs distribution, X  1 qφ (B"
N19-1114,D18-1509,0,0.0369447,"llapse. Finally, the URNNG also seemed to rely heavily on punctuation to identify constituents and we were unable to improve upon a right-branching baseline when training the URNNG on a version of PTB where punctuation is removed.22 5 Related Work There has been much work on incorporating tree structures into deep models for syntax-aware language modeling, both for unconditional (Emami and Jelinek, 2005; Buys and Blunsom, 2015; Dyer et al., 2016) and conditional (Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; Gu et al., 2018) cases. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees directly from words often employ additional heuristics based on punctu"
N19-1114,Q18-1019,0,0.0811044,"Missing"
N19-1114,D18-1356,1,0.894549,"Missing"
N19-1114,P17-1041,0,0.0319167,"various optimization strategies (e.g. separate optimizers for the inference network and the generative model) to avoid posterior collapse. Finally, the URNNG also seemed to rely heavily on punctuation to identify constituents and we were unable to improve upon a right-branching baseline when training the URNNG on a version of PTB where punctuation is removed.22 5 Related Work There has been much work on incorporating tree structures into deep models for syntax-aware language modeling, both for unconditional (Emami and Jelinek, 2005; Buys and Blunsom, 2015; Dyer et al., 2016) and conditional (Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; Gu et al., 2018) cases. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization ov"
N19-1114,P18-1070,0,0.078418,"] 11: − log wu ) · wu 12: return H[1, T ] . return tree entropy H[qφ (z |x)] The above estimator is unbiased but typically suffers from high variance. To reduce variance, we use a control variate derived from an average of the other samples’ joint likelihoods (Mnih and Rezende, 2016), yielding the following estimator, K 1 X (log pθ (x, z(k) ) − r(k) )∇φ log qφ (z(k) |x), K k=1 1 P (j) where r(k) = K−1 j6=k log pθ (x, z ). This control variate worked better than alternatives such as estimates of baselines from an auxiliary network (Mnih and Gregor, 2014; Deng et al., 2018) or a language model (Yin et al., 2018). 3 Experimental Setup 3.1 Data For English we use the Penn Treebank (Marcus et al., 1993, PTB) with splits and preprocessing from Dyer et al. (2016) which retains punctuation and replaces singleton words with Berkeley parser’s mapping rules, resulting in a vocabulary of 23,815 word types.10 Notably this is much larger than the standard PTB LM setup from Mikolov et al. (2010) which uses 10K types.11 Also different from the LM setup, we model each sentence separately instead of carrying information across sentence boundaries, as the RNNG is a generative model of sentences. Hence our perplexity"
P11-1008,P05-1033,0,0.0940162,"then minimized, for example using subgradient methods. In recent work, dual decomposition—a special case of Lagrangian relaxation, where the linear constraints enforce agreement between two or more models—has been applied to inference in Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008), and also to inference problems in NLP (Rush et al., 2010; Koo et al., 2010). There are close connections between dual decomposition and work on belief propagation (Smith and Eisner, 2008). 73 3 Background: Hypergraphs Translation with many syntax-based systems (e.g., (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008; Huang and Mi, 2010)) can be implemented as a two-step process. The first step is to take an input sentence in the source language, and from this to create a hypergraph (sometimes called a translation forest) that represents the set of possible translations (strings in the target language) and derivations under the grammar. The second step is to integrate an n-gram language model with this hypergraph. For example, in the system of (Chiang, 2005), the hypergraph is created as follows: first, the source side of the synchronous grammar is used to create a p"
P11-1008,J10-3008,0,0.0990845,"Missing"
P11-1008,P07-1019,0,0.0829747,"Missing"
P11-1008,D10-1027,0,0.0729844,". In recent work, dual decomposition—a special case of Lagrangian relaxation, where the linear constraints enforce agreement between two or more models—has been applied to inference in Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008), and also to inference problems in NLP (Rush et al., 2010; Koo et al., 2010). There are close connections between dual decomposition and work on belief propagation (Smith and Eisner, 2008). 73 3 Background: Hypergraphs Translation with many syntax-based systems (e.g., (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008; Huang and Mi, 2010)) can be implemented as a two-step process. The first step is to take an input sentence in the source language, and from this to create a hypergraph (sometimes called a translation forest) that represents the set of possible translations (strings in the target language) and derivations under the grammar. The second step is to integrate an n-gram language model with this hypergraph. For example, in the system of (Chiang, 2005), the hypergraph is created as follows: first, the source side of the synchronous grammar is used to create a parse forest over the source language string. Second, transdu"
P11-1008,E09-1044,0,0.107895,"Missing"
P11-1008,D10-1125,1,0.155243,"binatorial optimization (Korte and Vygen, 2008). Lagrange multipliers are used to add linear constraints to an existing problem that can be solved using a combinatorial algorithm; the resulting dual function is then minimized, for example using subgradient methods. In recent work, dual decomposition—a special case of Lagrangian relaxation, where the linear constraints enforce agreement between two or more models—has been applied to inference in Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008), and also to inference problems in NLP (Rush et al., 2010; Koo et al., 2010). There are close connections between dual decomposition and work on belief propagation (Smith and Eisner, 2008). 73 3 Background: Hypergraphs Translation with many syntax-based systems (e.g., (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008; Huang and Mi, 2010)) can be implemented as a two-step process. The first step is to take an input sentence in the source language, and from this to create a hypergraph (sometimes called a translation forest) that represents the set of possible translations (strings in the target language) and derivations under the grammar. The second step is to integr"
P11-1008,H05-1021,0,0.0385627,"ccurate estimates of the number of search errors for cube pruning. 2 Related Work A variety of approximate decoding algorithms have been explored for syntax-based translation systems, including cube-pruning (Chiang, 2007; Huang and Chiang, 2007), left-to-right decoding with beam search (Watanabe et al., 2006; Huang and Mi, 2010), and coarse-to-fine methods (Petrov et al., 2008). Recent work has developed decoding algorithms based on finite state transducers (FSTs). Iglesias et al. (2009) show that exact FST decoding is feasible for a phrase-based system with limited reordering (the MJ1 model (Kumar and Byrne, 2005)), and de Gispert et al. (2010) show that exact FST decoding is feasible for a specific class of hierarchical grammars (shallow-1 grammars). Approximate search methods are used for more complex reordering models or grammars. The FST algorithms are shown to produce higher scoring solutions than cube-pruning on a large proportion of examples. Lagrangian relaxation is a classical technique in combinatorial optimization (Korte and Vygen, 2008). Lagrange multipliers are used to add linear constraints to an existing problem that can be solved using a combinatorial algorithm; the resulting dual funct"
P11-1008,A00-2023,0,0.0413351,"ing, largely because of the cost of integrating an n-gram language model into the search process. Exact dynamic programming algorithms for the problem are well known (Bar-Hillel et al., 1964), but are too expensive to be used in practice.2 Previous work on decoding for syntax-based SMT has therefore been focused primarily on approximate search methods. This paper describes an efficient algorithm for exact decoding of synchronous grammar models for translation. We avoid the construction of (Bar-Hillel 1 This problem is also relevant to other areas of statistical NLP, for example NL generation (Langkilde, 2000). 2 E.g., with a trigram language model they run in O(|E|w6 ) time, where |E |is the number of edges in the hypergraph, and w is the number of distinct lexical items in the hypergraph. 2. Application of an all-pairs shortest path algorithm to a directed graph derived from the weighted hypergraph. The size of the derived directed graph is linear in the size of the hypergraph, hence this step is again efficient. Informally, the first decoding algorithm incorporates the weights and hard constraints on translations from the synchronous grammar, while the second decoding algorithm is used to integr"
P11-1008,W06-1606,0,0.0294161,", for example using subgradient methods. In recent work, dual decomposition—a special case of Lagrangian relaxation, where the linear constraints enforce agreement between two or more models—has been applied to inference in Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008), and also to inference problems in NLP (Rush et al., 2010; Koo et al., 2010). There are close connections between dual decomposition and work on belief propagation (Smith and Eisner, 2008). 73 3 Background: Hypergraphs Translation with many syntax-based systems (e.g., (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008; Huang and Mi, 2010)) can be implemented as a two-step process. The first step is to take an input sentence in the source language, and from this to create a hypergraph (sometimes called a translation forest) that represents the set of possible translations (strings in the target language) and derivations under the grammar. The second step is to integrate an n-gram language model with this hypergraph. For example, in the system of (Chiang, 2005), the hypergraph is created as follows: first, the source side of the synchronous grammar is used to create a parse forest over the"
P11-1008,D08-1012,0,0.0770549,"Missing"
P11-1008,W06-1616,0,0.257948,"Missing"
P11-1008,D10-1001,1,0.249468,"al technique in combinatorial optimization (Korte and Vygen, 2008). Lagrange multipliers are used to add linear constraints to an existing problem that can be solved using a combinatorial algorithm; the resulting dual function is then minimized, for example using subgradient methods. In recent work, dual decomposition—a special case of Lagrangian relaxation, where the linear constraints enforce agreement between two or more models—has been applied to inference in Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008), and also to inference problems in NLP (Rush et al., 2010; Koo et al., 2010). There are close connections between dual decomposition and work on belief propagation (Smith and Eisner, 2008). 73 3 Background: Hypergraphs Translation with many syntax-based systems (e.g., (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008; Huang and Mi, 2010)) can be implemented as a two-step process. The first step is to take an input sentence in the source language, and from this to create a hypergraph (sometimes called a translation forest) that represents the set of possible translations (strings in the target language) and derivations under the grammar. The secon"
P11-1008,P08-1066,0,0.0204753,"subgradient methods. In recent work, dual decomposition—a special case of Lagrangian relaxation, where the linear constraints enforce agreement between two or more models—has been applied to inference in Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008), and also to inference problems in NLP (Rush et al., 2010; Koo et al., 2010). There are close connections between dual decomposition and work on belief propagation (Smith and Eisner, 2008). 73 3 Background: Hypergraphs Translation with many syntax-based systems (e.g., (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008; Huang and Mi, 2010)) can be implemented as a two-step process. The first step is to take an input sentence in the source language, and from this to create a hypergraph (sometimes called a translation forest) that represents the set of possible translations (strings in the target language) and derivations under the grammar. The second step is to integrate an n-gram language model with this hypergraph. For example, in the system of (Chiang, 2005), the hypergraph is created as follows: first, the source side of the synchronous grammar is used to create a parse forest over the source language st"
P11-1008,D08-1016,0,0.0604628,"o an existing problem that can be solved using a combinatorial algorithm; the resulting dual function is then minimized, for example using subgradient methods. In recent work, dual decomposition—a special case of Lagrangian relaxation, where the linear constraints enforce agreement between two or more models—has been applied to inference in Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008), and also to inference problems in NLP (Rush et al., 2010; Koo et al., 2010). There are close connections between dual decomposition and work on belief propagation (Smith and Eisner, 2008). 73 3 Background: Hypergraphs Translation with many syntax-based systems (e.g., (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008; Huang and Mi, 2010)) can be implemented as a two-step process. The first step is to take an input sentence in the source language, and from this to create a hypergraph (sometimes called a translation forest) that represents the set of possible translations (strings in the target language) and derivations under the grammar. The second step is to integrate an n-gram language model with this hypergraph. For example, in the system of (Chiang, 2005), the hypergraph"
P11-1008,N06-1054,0,0.269276,"Missing"
P11-1008,P06-1098,0,0.145069,"Missing"
P11-1008,N09-2002,0,\N,Missing
P11-1008,P01-1030,0,\N,Missing
P11-1008,N09-1049,0,\N,Missing
P11-1008,J07-2003,0,\N,Missing
P11-5006,P11-1043,0,0.022125,"man man man man man g (z1 ) > g (z2 ) Identifying word tags notation: identify the tag labels selected by each model • Ys (i, t) = 1 when the tagger for sentence s at position i selects tag t • z(s, i, t) = 1 when the constraint assigns at sentence s position i the tag t example: a parse and tagging with Y1 (5, N) = 1 and z(1, 5, N) = 1 He saw an American man The smart man stood outside Y man man z man Combined optimization goal: arg max Y ∈Y,z∈Z F (Y ) + g (z) such that for all s = 1 . . . m, i = 1 . . . n, t ∈ T , Ys (i, t) = z(s, i, t) Algorithm step-by-step [Animation] Combined alignment (DeNero and Macherey, 2011) setup: assume separate models trained for English-to-French and French-to-English alignment problem: find an alignment that maximizes the score of both models with soft agreement example: • HMM models for both directional alignments (assume correct alignment is one-to-one for simplicity) English-to-French alignment define: • • • Y is set of all possible English-to-French alignments y ∈ Y is a valid alignment f (y ) scores of the alignment example: HMM alignment 1 3 2 4 6 5 The1 ugly2 dog3 has4 red5 fur6 French-to-English alignment define: • • • Z is set of all possible French-to-English align"
P11-5006,D10-1125,1,0.825095,"complexity focus: decoding problem for natural language tasks y ∗ = arg max f (y ) y motivation: • richer model structure often leads to improved accuracy • exact decoding for complex models tends to be intractable Decoding tasks many common problems are intractable to decode exactly high complexity • combined parsing and part-of-speech tagging (Rush et al., 2010) • “loopy” HMM part-of-speech tagging • syntactic machine translation (Rush and Collins, 2011) NP-Hard • symmetric HMM alignment (DeNero and Macherey, 2011) • phrase-based translation • higher-order non-projective dependency parsing (Koo et al., 2010) in practice: • approximate decoding methods (coarse-to-fine, beam search, cube pruning, gibbs sampling, belief propagation) • approximate models (mean field, variational models) Motivation cannot hope to find exact algorithms (particularly when NP-Hard) aim: develop decoding algorithms with formal guarantees method: • derive fast algorithms that provide certificates of optimality • show that for practical instances, these algorithms often yield exact solutions • provide strategies for improving solutions or finding approximate solutions when no certificate is found dual decomposition helps us"
P11-5006,D10-1001,1,0.891786,"Missing"
P11-5006,D08-1016,0,0.0817424,"Missing"
P11-5006,P11-1008,1,\N,Missing
P11-5006,D11-1003,1,\N,Missing
P14-1139,P06-1002,0,0.0974445,"e bidirectional models with soft-penalties to explicitly permit these violations. A Proof of NP-Hardness We can show that the bidirectional alignment problem is NP-hard by reduction from the trav50 0 50 1000 50 100 150 200 250 iteration 300 350 400 (a) The best dual and the best primal score, relative to the optimal score, averaged over all sentence pairs. The best primal curve uses a feasible greedy algorithm, whereas the intersection curve is calculated by taking the intersection of x and y. 1.0 relative search space size ble 2 also compares the models in terms of phraseextraction accuracy (Ayan and Dorr, 2006). We use the phrase extraction algorithm described by DeNero and Klein (2010), accounting for possible links and  alignments. CONS performs better than each of the directional models, but worse than the best D&M model. Finally we consider the impact of constraint addition, pruning, and use of a lower bound. Table 3 gives the average number of constraints added for sentence pairs for which Lagrangian relaxation alone does not produce a certificate. Figure 7(a) shows the average over all sentence pairs of the best dual and best primal scores. The graph compares the use of the greedy algorithm f"
P14-1139,J93-2003,0,0.121881,"Missing"
P14-1139,E09-1020,0,0.0164391,"e heuristic based on the intersection of x and y at the current round of Lagrangian relaxation. Experiments show that running this algorithm significantly improves the lower bound compared to just taking the intersection, and consequently helps pruning significantly. 7 Related Work The most common techniques for bidirectional alignment are post-hoc combinations, such as union or intersection, of directional models, (Och et al., 1999), or more complex heuristic combiners such as grow-diag-final (Koehn et al., 2003). Several authors have explored explicit bidirectional models in the literature. Cromieres and Kurohashi (2009) use belief propagation on a factor graph to train and decode a one-to-one word alignment problem. Qualitatively this method is similar to ours, although the model and decoding algorithm are different, and their method is not able to provide certificates of optimality. A series of papers by Ganchev et al. (2010), Graca et al. (2008), and Ganchev et al. (2008) use posterior regularization to constrain the posterior probability of the word alignment problem to be symmetric and bijective. This work acheives stateof-the-art performance for alignment. Instead of utilizing posteriors our model tries"
P14-1139,P10-1147,1,0.888202,"Missing"
P14-1139,P11-1043,1,0.881064,"for building statistical machine translation systems. In order to ensure accurate word alignments, most systems employ a post-hoc symmetrization step to combine directional word aligners, such as IBM Model 4 (Brown et al., 1993) or hidden Markov model (HMM) based aligners (Vogel et al., 1996). Several authors have proposed bidirectional models that incorporate this step directly, but decoding under many bidirectional models is NP-Hard and finding exact solutions has proven difficult. In this paper, we describe a novel Lagrangianrelaxation based decoder for the bidirectional model proposed by DeNero and Macherey (2011), with the goal of improving search accuracy. In that work, the authors implement a dual decomposition-based decoder for the problem, but We begin in Section 2 by formally describing the directional word alignment problem. Section 3 describes a preliminary bidirectional model using full agreement constraints and a Lagrangian relaxation-based solver. Section 4 modifies this model to include adjacency constraints. Section 5 describes an extension to the relaxed algorithm to explicitly enforce constraints, and Section 6 gives a pruning method for improving the efficiency of the algorithm. Experim"
P14-1139,P08-1112,0,0.187295,"Missing"
P14-1139,N06-1015,0,0.118406,"I + 1 where the hidden state at position i ∈ [I]0 is the aligned index j ∈ [J]0 , and the transition score takes into account the previously aligned index j 0 ∈ [J]0 .1 Formally, define the set of possible HMM alignments as X ⊂ {0, 1}([I]0 ×[J]0 )∪([I]×[J]0 ×[J]0 ) with where the vector θ ∈ R[I]×[J]0 ×[J]0 includes the transition and alignment scores. For a generative model of alignment, we might define θ(j 0 , i, j) = log(p(ei |fj )p(j|j 0 )). For a discriminative model of alignment, we might define θ(j 0 , i, j) = w · φ(i, j 0 , j, f , e) for a feature function φ and weights w (Moore, 2005; Lacoste-Julien et al., 2006). Now reverse the direction of the model and consider the f →e alignment problem. An f →e alignment is a binary vector y ∈ Y where for each j ∈ [J], y(i, j) = 1 for exactly one i ∈ [I]0 . Define the set of HMM alignments Y ⊂ {0, 1}([I]0 ×[J]0 )∪([I]0 ×[I]0 ×[J]) as Y=  y : y(0, 0) = 1,    I  X    y(i, j) = y(i0 , i, j)        y(i, j) = i0 =0 I X ∀i ∈ [I]0 , j ∈ [J], y(i, i0 , j + 1) Similarly define the objective function g(y; ω) = J X I X I X ω(i0 , i, j)y(i0 , i, j) j=1 i=0 i0 =0 1 Our definition differs slightly from other HMM-based aligners in that it does not track the last"
P14-1139,N06-1014,0,0.294672,"d decoding algorithm are different, and their method is not able to provide certificates of optimality. A series of papers by Ganchev et al. (2010), Graca et al. (2008), and Ganchev et al. (2008) use posterior regularization to constrain the posterior probability of the word alignment problem to be symmetric and bijective. This work acheives stateof-the-art performance for alignment. Instead of utilizing posteriors our model tries to decode a single best one-to-one word alignment. A different approach is to use constraints at training time to obtain models that favor bidirectional properties. Liang et al. (2006) propose agreement-based learning, which jointly learns probabilities by maximizing a combination of likelihood and agreement between two directional models. General linear programming approaches have also been applied to word alignment problems. Lacoste-Julien et al. (2006) formulate the word alignment problem as quadratic assignment problem and solve it using an integer linear programming solver. Our work is most similar to DeNero and Macherey (2011), which uses dual decomposition to encourage agreement between two directional HMM aligners during decoding time. 8 Experiments Our experimental"
P14-1139,H05-1011,0,0.0783886,"MM of length I + 1 where the hidden state at position i ∈ [I]0 is the aligned index j ∈ [J]0 , and the transition score takes into account the previously aligned index j 0 ∈ [J]0 .1 Formally, define the set of possible HMM alignments as X ⊂ {0, 1}([I]0 ×[J]0 )∪([I]×[J]0 ×[J]0 ) with where the vector θ ∈ R[I]×[J]0 ×[J]0 includes the transition and alignment scores. For a generative model of alignment, we might define θ(j 0 , i, j) = log(p(ei |fj )p(j|j 0 )). For a discriminative model of alignment, we might define θ(j 0 , i, j) = w · φ(i, j 0 , j, f , e) for a feature function φ and weights w (Moore, 2005; Lacoste-Julien et al., 2006). Now reverse the direction of the model and consider the f →e alignment problem. An f →e alignment is a binary vector y ∈ Y where for each j ∈ [J], y(i, j) = 1 for exactly one i ∈ [I]0 . Define the set of HMM alignments Y ⊂ {0, 1}([I]0 ×[J]0 )∪([I]0 ×[I]0 ×[J]) as Y=  y : y(0, 0) = 1,    I  X    y(i, j) = y(i0 , i, j)        y(i, j) = i0 =0 I X ∀i ∈ [I]0 , j ∈ [J], y(i, i0 , j + 1) Similarly define the objective function g(y; ω) = J X I X I X ω(i0 , i, j)y(i0 , i, j) j=1 i=0 i0 =0 1 Our definition differs slightly from other HMM-based aligners in t"
P14-1139,W99-0604,0,0.548881,"Note that unlike an alignment from Y multiple words may be aligned in a column and words may transition from nonaligned positions. Note that for both of these models we can solve the optimization problem exactly using the standard Viterbi algorithm for HMM decoding. The first can be solved in O(IJ 2 ) time and the second in O(I 2 J) time. 3 Bidirectional Alignment The directional bias of the e→f and f →e alignment models may cause them to produce differing alignments. To obtain the best single alignment, it is common practice to use a post-hoc algorithm to merge these directional alignments (Och et al., 1999). First, a directional alignment is found from each word in e to a word f . Next an alignment is produced in the reverse direction from f to e. Finally, these alignments are merged, either through intersection, union, or with an interpolation algorithm such as grow-diag-final (Koehn et al., 2003). In this work, we instead consider a bidirectional alignment model that jointly considers both directional models. We begin in this section by introducing a simple bidirectional model that enforces full agreement between directional models and giving a relaxation for decoding. Section 4 loosens this m"
P14-1139,C96-2141,0,0.757889,"ng problem. Given a sentence e of length |e |= I and a sentence f of length |f |= J, our goal is to find the best bidirectional alignment between the two sentences under a given objective function. Before turning to the model of interest, we first introduce directional word alignment. 2.1 Word Alignment In the e→f word alignment problem, each word in e is aligned to a word in f or to the null word . This alignment is a mapping from each index i ∈ [I] to an index j ∈ [J]0 (where j = 0 represents alignment to ). We refer to a single word alignment as a link. A first-order HMM alignment model (Vogel et al., 1996) is an HMM of length I + 1 where the hidden state at position i ∈ [I]0 is the aligned index j ∈ [J]0 , and the transition score takes into account the previously aligned index j 0 ∈ [J]0 .1 Formally, define the set of possible HMM alignments as X ⊂ {0, 1}([I]0 ×[J]0 )∪([I]×[J]0 ×[J]0 ) with where the vector θ ∈ R[I]×[J]0 ×[J]0 includes the transition and alignment scores. For a generative model of alignment, we might define θ(j 0 , i, j) = log(p(ei |fj )p(j|j 0 )). For a discriminative model of alignment, we might define θ(j 0 , i, j) = w · φ(i, j 0 , j, f , e) for a feature function φ and wei"
P14-1139,P09-1104,1,0.884523,"Missing"
P14-1139,N03-1017,0,0.289841,"O(IJ 2 ) time and the second in O(I 2 J) time. 3 Bidirectional Alignment The directional bias of the e→f and f →e alignment models may cause them to produce differing alignments. To obtain the best single alignment, it is common practice to use a post-hoc algorithm to merge these directional alignments (Och et al., 1999). First, a directional alignment is found from each word in e to a word f . Next an alignment is produced in the reverse direction from f to e. Finally, these alignments are merged, either through intersection, union, or with an interpolation algorithm such as grow-diag-final (Koehn et al., 2003). In this work, we instead consider a bidirectional alignment model that jointly considers both directional models. We begin in this section by introducing a simple bidirectional model that enforces full agreement between directional models and giving a relaxation for decoding. Section 4 loosens this model to adjacent agreement. 3.1 Y0 = ∗ x ,y = arg max f (x) + g(y) s.t. x∈X ,y∈Y x(i, j) = y(i, j) ∀i ∈ [I], j ∈ [J]  y : y(0, 0) = 1, P y(i, j) = Ii0 =0 y(i0 , i, j) ∀i ∈ [I]0 , j ∈ [J] Figure 2(b) shows a possible y ∈ Y 0 and a valid unchained structure. To form the Lagrangian dual with relaxe"
P15-1137,D13-1203,0,0.841381,"ge by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representations, and we report the best overall score on the CoNLL 2012 English test set to date. 1 Introduction One of the major challenges associated with resolving coreference is that in typical documents the number of mentions (syntactic units capable of referring or being referred to) that are nonanaphoric – that is, that are not coreferent with any previous mention – far exceeds the number of mentions that are anaphoric (Kummerfeld and Klein, 2013; Durrett and Klein, 2013). This preponderance of non-anaphoric mentions makes coreference resolution challenging, partly because many basic coreference features, such as those looking at head, number, or gender match fail to distinguish between truly coreferent pairs and the large number of matching but nonetheless non-coreferent pairs. Indeed, several authors have noted that it is difficult to obtain good performance on the coreference task using simple features (Lee et al., 2011; Fernandes et al., 2012; Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Bj¨orkelund and Kuhn, 2014) and, as a result, state-of-the-ar"
P15-1137,Q14-1037,0,0.108586,"Missing"
P15-1137,D08-1031,0,0.0330564,"1.5 points over the state-of-the-art coreference system. Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” setting, where it is not k"
P15-1137,W12-4503,0,0.0829107,"Missing"
P15-1137,P14-1005,0,0.290072,"Missing"
P15-1137,D13-1057,0,0.0221907,"inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” setting, where it is not known a priori which mentions in a document participate in coreference clusters, and so (all) mentions must be automatically extrac"
P15-1137,D08-1069,0,0.0483972,"et al., 2012), and of over 1.5 points over the state-of-the-art coreference system. Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” s"
P15-1137,N07-1030,0,0.0359765,"Missing"
P15-1137,N06-2015,0,0.277262,"Missing"
P15-1137,W04-3250,0,0.0267784,"Missing"
P15-1137,D13-1027,0,0.0563879,"nt ranking, which we encourage by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representations, and we report the best overall score on the CoNLL 2012 English test set to date. 1 Introduction One of the major challenges associated with resolving coreference is that in typical documents the number of mentions (syntactic units capable of referring or being referred to) that are nonanaphoric – that is, that are not coreferent with any previous mention – far exceeds the number of mentions that are anaphoric (Kummerfeld and Klein, 2013; Durrett and Klein, 2013). This preponderance of non-anaphoric mentions makes coreference resolution challenging, partly because many basic coreference features, such as those looking at head, number, or gender match fail to distinguish between truly coreferent pairs and the large number of matching but nonetheless non-coreferent pairs. Indeed, several authors have noted that it is difficult to obtain good performance on the coreference task using simple features (Lee et al., 2011; Fernandes et al., 2012; Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Bj¨orkelund and Kuhn, 2014) and, as"
P15-1137,P13-1049,0,0.0594376,"Missing"
P15-1137,J13-4004,0,0.205374,"Missing"
P15-1137,P14-2005,0,0.101775,"Missing"
P15-1137,H05-1004,0,0.414638,"Missing"
P15-1137,D14-1225,0,0.389411,"Missing"
P15-1137,C02-1139,0,0.844818,"Missing"
P15-1137,P04-1020,0,0.110193,"presentations for natural language tasks (Collobert et al., 2011), we explore neural network models which take only raw, unconjoined features as input, and attempt to learn intermediate representations automatically. In particular, the model we describe attempts to create independent feature representations useful for both detecting the anaphoricity of a mention (that is, whether or not a mention is anaphoric) and ranking the potential antecedents of an anaphoric mention. Adequately capturing anaphoricity information has long been thought to be an important aspect of the coreference task (see Ng (2004) and Section 7), since a strong non-anaphoric signal might, for instance, discourage the erroneous prediction of an antecedent for a non-anaphoric mention even in the presence of a misleading head match. We furthermore attempt to encourage the learning of the desired feature representations by pretraining the model’s weights on two corresponding subtasks, namely, anaphoricity detection and antecedent ranking of known anaphoric mentions. Overall our best model has an absolute gain of almost 2 points in CoNLL score over a similar but linear mention-ranking model on the CoNLL 2012 English test se"
P15-1137,W12-4501,0,0.684515,"d Section 7), since a strong non-anaphoric signal might, for instance, discourage the erroneous prediction of an antecedent for a non-anaphoric mention even in the presence of a misleading head match. We furthermore attempt to encourage the learning of the desired feature representations by pretraining the model’s weights on two corresponding subtasks, namely, anaphoricity detection and antecedent ranking of known anaphoric mentions. Overall our best model has an absolute gain of almost 2 points in CoNLL score over a similar but linear mention-ranking model on the CoNLL 2012 English test set (Pradhan et al., 2012), and of over 1.5 points over the state-of-the-art coreference system. Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resolution (Denis and Bald"
P15-1137,D09-1101,0,0.0965473,"e-of-the-art coreference system. Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” setting, where it is not known a priori which me"
P15-1137,N13-1071,0,0.160357,"Missing"
P15-1137,D11-1014,0,0.0258548,"Missing"
P15-1137,D12-1110,0,0.0274344,"Missing"
P15-1137,J01-4004,0,0.741286,"Missing"
P15-1137,C12-1154,0,0.0202981,"Missing"
P15-1137,M95-1005,0,0.672142,"Missing"
P15-1137,P12-1040,0,0.0193087,"Missing"
P15-1137,P14-2006,0,0.333577,"Missing"
P15-1137,N07-1011,0,\N,Missing
P15-1137,W12-4502,0,\N,Missing
P15-1137,N10-1061,0,\N,Missing
P15-1137,Q14-1043,0,\N,Missing
P15-1137,D08-1067,0,\N,Missing
P15-1137,W11-1902,0,\N,Missing
P17-4012,D15-1166,0,0.935439,"e current hidden state to produce a prediction p(wt |w1:t−1 , x) of the next word. This prediction is then fed back into the target RNN. Introduction Neural machine translation (NMT) is a new methodology for machine translation that has led to remarkable improvements, particularly in terms of human evaluation, compared to rule-based and statistical machine translation (SMT) systems (Wu et al., 2016; Crego et al., 2016). Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT community benefited greatly from toolkits like Moses (Koehn et al., 2007) for phrase-based SMT and CDec (Dyer et al., 2010) or travatar (Neubig, 2013) for syntax-based SMT, NMT toolkits can provide"
P17-4012,K16-1002,0,0.159928,"Missing"
P17-4012,P13-4016,0,0.0466914,"n-based variants (Bahdanau et al., 2014; Luong et al., 2015), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT community benefited greatly from toolkits like Moses (Koehn et al., 2007) for phrase-based SMT and CDec (Dyer et al., 2010) or travatar (Neubig, 2013) for syntax-based SMT, NMT toolkits can provide a foundation to build upon. A toolkit should aim to provide a shared framework for developing and comparing open-source systems, while at the same time being efficient and accurate enough to be used in production contexts. Currently there are several existing NMT implementations. Many systems such as those developed in industry by Google, Microsoft, and Baidu, are closed source, and are unlikely to be released with unrestricted licenses. Many other systems such as GroundHog, Blocks, neuralmonkey, tensorflow-seq2seq, lamtram, and our own seq2seq-a"
P17-4012,W16-2209,0,0.0203922,"Table 1: Translation speed in source tokens per second for the Torch CPU/GPU implementations and for the multithreaded CPU C implementation. (Run with Intel i7/GTX 1080) Figure 3: 3D Visualization of OpenNMT source embedding from the TensorBoard visualization system. tion within the code. To test whether this approach would allow novel feature development we experimented with two case studies. to basic seq2seq models. We next discuss a case study to demonstrate that OpenNMT is extensible to future variants. Case Study: Factored Neural Translation In feature-based factored neural translation (Sennrich and Haddow, 2016), instead of generating a word at each time step, the model generates both word and associated features. For instance, the system might include words and separate case features. This extension requires modifying both the inputs and the output of the decoder to generate multiple symbols. In OpenNMT both of these aspects are abstracted from the core translation code, and therefore factored translation simply modifies the input network to instead process the featurebased representation, and the output generator network to instead produce multiple conditionally independent predictions. Multiple Mo"
P17-4012,N16-1012,1,0.109534,"ion in the 4 other languages. Corpus was tokenized using shared Byte Pair Encoding of 32k. Comparative results between multi-way translation and each of the 20 independent training are presented in Table 2. The systematically large improvement shows that language pair benefits from training jointly with the other language pairs. Additionally we have found interest from the community in using OpenNMT for non-standard MT tasks like sentence document summarization dialogue response generation (chatbots), among others. Using OpenNMT, we were able to replicate the sentence summarization results of Chopra et al. (2016), reaching a ROUGE-1 score of 33.13 on the Gigaword data. We have also trained a model on 14 million sentences of the OpenSubtitles data set based on the work Vinyals and Le (2015), achieving comparable perplexity. Table 3: Performance Results for EN→DE on WMT15 tested on newstest2014. Both system 2x500 RNN, embedding size 300, 13 epochs, batch size 64, beam size 5. We compare on a 50k vocabulary and a 32k BPE setting. OpenNMT shows improvements in speed and accuracy compared to Nematus. kenization, (b) has extremely simple, languageindependent tokenization rules. The tokenizer can also perfor"
P17-4012,P16-5005,0,0.0397599,"urrent neural network (RNN). Upon seeing the heosi symbol, the final time step initializes a target blue RNN. At each target time step, attention is applied over the source RNN and combined with the current hidden state to produce a prediction p(wt |w1:t−1 , x) of the next word. This prediction is then fed back into the target RNN. Introduction Neural machine translation (NMT) is a new methodology for machine translation that has led to remarkable improvements, particularly in terms of human evaluation, compared to rule-based and statistical machine translation (SMT) systems (Wu et al., 2016; Crego et al., 2016). Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT"
P17-4012,1983.tc-1.13,0,0.689675,"Missing"
P17-4012,N16-1174,0,0.14961,"is a speech-to-text recognition system based on the work of Chan et al. (2015). This system has been implemented directly in OpenNMT by replacing the source encoder with a Pyrimidal source model. Case Study: Attention Networks The use of attention over the encoder at each step of translation is crucial for the model to perform well. The default method is to utilize the global attention mechanism. However there are many other types of attention that have recently proposed including local attention (Luong et al., 2015), sparse-max attention (Martins and Astudillo, 2016), hierarchical attention (Yang et al., 2016) among others. As this is simply a module in OpenNMT it can easily be substituted. Recently the Harvard group developed a structured attention approach, that utilizes graphical model inference to compute this attention. The method is quite computationally complex; however as it is modularized by the Torch interface, it can be used in OpenNMT to substitute for standard attention. 4.3 4.4 Additional Tools Finally we briefly summarize some of the additional tools that extend OpenNMT to make it more beneficial to the research community. Tokenization We aimed for OpenNMT to be a standalone project"
P17-4012,P10-4002,0,0.00659525,"and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT community benefited greatly from toolkits like Moses (Koehn et al., 2007) for phrase-based SMT and CDec (Dyer et al., 2010) or travatar (Neubig, 2013) for syntax-based SMT, NMT toolkits can provide a foundation to build upon. A toolkit should aim to provide a shared framework for developing and comparing open-source systems, while at the same time being efficient and accurate enough to be used in production contexts. Currently there are several existing NMT implementations. Many systems such as those developed in industry by Google, Microsoft, and Baidu, are closed source, and are unlikely to be released with unrestricted licenses. Many other systems such as GroundHog, Blocks, neuralmonkey, tensorflow-seq2seq, lam"
P17-4012,P07-2045,0,0.0670209,"models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT community benefited greatly from toolkits like Moses (Koehn et al., 2007) for phrase-based SMT and CDec (Dyer et al., 2010) or travatar (Neubig, 2013) for syntax-based SMT, NMT toolkits can provide a foundation to build upon. A toolkit should aim to provide a shared framework for developing and comparing open-source systems, while at the same time being efficient and accurate enough to be used in production contexts. Currently there are several existing NMT implementations. Many systems such as those developed in industry by Google, Microsoft, and Baidu, are closed source, and are unlikely to be released with unrestricted licenses. Many other systems such as Ground"
P17-4012,P16-1162,0,\N,Missing
P19-1084,D17-1070,0,0.0437902,"the baseline model learned to rely on the presence/absence of the bias term c, always predicting T RUE/FALSE respectively. Table 1 shows the results of our two proposed methods. As we increase the hyper-parameters α and β, our methods initially behave like the baseline, learning the training set but failing on the test set. However, with strong enough hyperparameters (moving towards the bottom in the tables), they perform perfectly on both the biased training set and the unbiased test set. For Method 1, stronger hyper-parameters work better. Baseline & Implementation Details We use InferSent (Conneau et al., 2017) as our baseline model because it has been shown to work well on popular NLI datasets and is representative of many NLI models. We use separate BiLSTM encoders to learn vector representations of P and H.11 The vector representations are combined following Mou et al. (2016),12 and passed to an MLP classifier with one hidden layer. Our proposed 9 Detailed descriptions of these datasets can be found in Poliak et al. (2018b). 10 We leave additional NLI datasets, such as the Diverse NLI Collection (Poliak et al., 2018a), for future work. 11 Many NLI models encode P and H separately (Rockt¨aschel et"
P19-1084,S19-1028,1,0.7647,"Missing"
P19-1084,D15-1075,0,0.462661,"taset B), a letter c is appended to the hypothesis side in the T RUE examples, but not in the FALSE examples. In order to transfer well to the test set, a model that is trained on this training set needs to learn the underlying relationship—that P entails H if and only if their first letter is identical—rather than relying on the presence of c in the hypothesis side. max L1 (θ) = (1 − α) log pθ (y |P, H) θ − α log pθ,φ (y |P 0 , H) max L2 (φ) = β log pθ,φ (y |P 0 , H) Common NLI datasets Moving to existing NLI datasets, we train models on the Stanford Natural Language Inference dataset (SNLI; Bowman et al., 2015), since it is known to contain significant annotation artifacts. We evaluate the robustness of our methods on other, target datasets. As target datasets, we use the 10 datasets investigated by Poliak et al. (2018b) in their hypothesisonly study, plus two test sets: GLUE’s diagnostic test set, which was carefully constructed to not contain hypothesis-biases (Wang et al., 2018), and SNLI-hard, a subset of the SNLI test set that is thought to have fewer biases (Gururangan et al., 2018). The target datasets include humanjudged datasets that used automatic methods to pair premises and hypotheses, a"
P19-1084,C18-1055,0,0.0314895,"ur methods with known biases in NLI datasets, the effects of stronger bias removal, and the possibility of fine-tuning on the target datasets. Our methodology can be extended to handle biases in other tasks where one is concerned with finding relationships between two objects, such as visual question answering, story cloze completion, and reading comprehension. We hope to encourage such investigation in the broader community. Improving model robustness Neural networks are sensitive to adversarial examples, primarily in machine vision, but also in NLP (Jia & Liang, 2017; Belinkov & Bisk, 2018; Ebrahimi et al., 2018; Heigold et al., 2018; Mudrakarta et al., 2018; Ribeiro et al., 2018; Belinkov & Glass, 2019). A common approach to improving robustness is to include adversarial examples in training (Szegedy et al., 2014; Goodfellow et al., 2015). However, this may not generalize well to new types of examples (Xiaoyong Yuan, 2017; Tramr et al., 2018). Domain-adversarial neural networks aim to increase robustness to domain change, by learning to be oblivious to the domain using gradient reversals (Ganin et al., 2016). Our methods rely similarly on gradient reversals when encouraging models to ignore dataset-"
P19-1084,P17-2097,0,0.232016,"ctic clues alone (Snow et al., 2006; Vanderwende & Dolan, 2006). Recent work also found artifacts in new NLI datasets (Tsuchiya, 2018; Gururangan et al., 2018; Poliak et al., 2018b). Other NLU datasets also exhibit biases. In ROC Stories (Mostafazadeh et al., 2016), a story cloze dataset, Schwartz et al. (2017b) obtained a high performance by only considering the candidate endings, without even looking at the story context. In this case, stylistic features of the candidate endings alone, such as the length or certain words, were strong indicators of the correct ending (Schwartz et al., 2017a; Cai et al., 2017). A similar phenomenon was observed in reading comprehension, where systems performed non-trivially well by using only the final sentence in the passage or ignoring the passage altogether (Kaushik & Lipton, 2018). Finally, multiple studies found non-trivial performance in visual question answering (VQA) by using only the question, without access to the image, due to question biases (Zhang et al., 2016; Kafle & Kanan, 2016, 2017; Goyal et al., 2017; Agrawal et al., 2017). Our main goal is to determine whether our methods help a model perform well across multiple datasets by ignoring dataset-spe"
P19-1084,P18-1225,0,0.0192681,"niques developed for textual entailment“ datasets, e.g., RTE-3, do not transfer well to other domains, specifically conversational entailment (Zhang & Chai, 2009, 2010). Bowman et al. (2015) and Williams et al. (2018) demonstrated (specifically in their respective Tables 7 and 4) how models trained on SNLI and MNLI may not transfer well across other NLI datasets like SICK. Talman & Chatzikyriakidis (2018) recently reported similar findings using many advanced deep-learning models. versarial examples that do not conform to logical rules and regularize models based on those examples. Similarly, Kang et al. (2018) incorporate external linguistic resources and use a GAN-style framework to adversarially train robust NLI models. In contrast, we do not use external resources and we are interested in mitigating hypothesisonly biases. Finally, a similar approach has recently been used to mitigate biases in VQA (Ramakrishnan et al., 2018; Grand & Belinkov, 2019). 8 Conclusion Biases in annotations are a major source of concern for the quality of NLI datasets and systems. We presented a solution for combating annotation biases by proposing two training methods to predict the probability of a premise given an e"
P19-1084,D18-1546,0,0.0793546,"exhibit biases. In ROC Stories (Mostafazadeh et al., 2016), a story cloze dataset, Schwartz et al. (2017b) obtained a high performance by only considering the candidate endings, without even looking at the story context. In this case, stylistic features of the candidate endings alone, such as the length or certain words, were strong indicators of the correct ending (Schwartz et al., 2017a; Cai et al., 2017). A similar phenomenon was observed in reading comprehension, where systems performed non-trivially well by using only the final sentence in the passage or ignoring the passage altogether (Kaushik & Lipton, 2018). Finally, multiple studies found non-trivial performance in visual question answering (VQA) by using only the question, without access to the image, due to question biases (Zhang et al., 2016; Kafle & Kanan, 2016, 2017; Goyal et al., 2017; Agrawal et al., 2017). Our main goal is to determine whether our methods help a model perform well across multiple datasets by ignoring dataset-specific artifacts. In turn, we did not update the models’ parameters on other datasets. But, what if we are given different amounts of training data for a new NLI dataset? To determine if our approach is still help"
P19-1084,W19-1801,1,0.837619,"ss other NLI datasets like SICK. Talman & Chatzikyriakidis (2018) recently reported similar findings using many advanced deep-learning models. versarial examples that do not conform to logical rules and regularize models based on those examples. Similarly, Kang et al. (2018) incorporate external linguistic resources and use a GAN-style framework to adversarially train robust NLI models. In contrast, we do not use external resources and we are interested in mitigating hypothesisonly biases. Finally, a similar approach has recently been used to mitigate biases in VQA (Ramakrishnan et al., 2018; Grand & Belinkov, 2019). 8 Conclusion Biases in annotations are a major source of concern for the quality of NLI datasets and systems. We presented a solution for combating annotation biases by proposing two training methods to predict the probability of a premise given an entailment label and a hypothesis. We demonstrated that this discourages the hypothesis encoder from learning the biases to instead obtain a less biased representation. When empirically evaluating our approaches, we found that in a synthetic setting, as well as on a wide-range of existing NLI datasets, our methods perform better than the tradition"
P19-1084,N18-2017,0,0.0783303,"Missing"
P19-1084,I17-1011,0,0.118985,"Missing"
P19-1084,P18-2005,0,0.0390164,"vious to the domain using gradient reversals (Ganin et al., 2016). Our methods rely similarly on gradient reversals when encouraging models to ignore dataset-specific artifacts. One distinction is that domain-adversarial networks require knowledge of the domain at training time, while our methods learn to ignore latent artifacts and do not require direct supervision in the form of a domain label. Others have attempted to remove biases from learned representations, e.g., gender biases in word embeddings (Bolukbasi et al., 2016) or sensitive information like sex and age in text representations (Li et al., 2018). However, removing such attributes from text representations may be difficult (Elazar & Goldberg, 2018). In contrast to this line of work, our final goal is not the removal of such attributes per se; instead, we strive for more robust representations that better transfer to other datasets, similar to Li et al. (2018). Recent work has applied adversarial learning to NLI. Minervini & Riedel (2018) generate adAcknowledgements We would like to thank Aviad Rubinstein and Cynthia Dwork for discussing an earlier version of this work and the anonymous reviewers for their useful comments. Y.B. was sup"
P19-1084,P16-1204,0,0.135588,"Missing"
P19-1084,marelli-etal-2014-sick,0,0.149192,"Missing"
P19-1084,P15-2067,1,0.883981,"Missing"
P19-1084,K18-1007,0,0.0984265,"in label. Others have attempted to remove biases from learned representations, e.g., gender biases in word embeddings (Bolukbasi et al., 2016) or sensitive information like sex and age in text representations (Li et al., 2018). However, removing such attributes from text representations may be difficult (Elazar & Goldberg, 2018). In contrast to this line of work, our final goal is not the removal of such attributes per se; instead, we strive for more robust representations that better transfer to other datasets, similar to Li et al. (2018). Recent work has applied adversarial learning to NLI. Minervini & Riedel (2018) generate adAcknowledgements We would like to thank Aviad Rubinstein and Cynthia Dwork for discussing an earlier version of this work and the anonymous reviewers for their useful comments. Y.B. was supported by the Harvard Mind, Brain, and Behavior Initiative. A.P. and B.V.D were supported by JHU-HLTCOE and DARPA LORELEI. A.M.R gratefully acknowledges the support of NSF 1845664. Views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U.S. Government. 885 References URL http://ww"
P19-1084,D14-1162,0,0.0849444,"Missing"
P19-1084,N16-1098,0,0.0286455,"ises will lead to a model’s degradation. 6.3 7 Fine-tuning on target datasets Related Work Biases and artifacts in NLU datasets Many natural language undersrtanding (NLU) datasets contain annotation artifacts. Early work on NLI, also known as recognizing textual entailment (RTE), found biases that allowed models to perform relatively well by focusing on syntactic clues alone (Snow et al., 2006; Vanderwende & Dolan, 2006). Recent work also found artifacts in new NLI datasets (Tsuchiya, 2018; Gururangan et al., 2018; Poliak et al., 2018b). Other NLU datasets also exhibit biases. In ROC Stories (Mostafazadeh et al., 2016), a story cloze dataset, Schwartz et al. (2017b) obtained a high performance by only considering the candidate endings, without even looking at the story context. In this case, stylistic features of the candidate endings alone, such as the length or certain words, were strong indicators of the correct ending (Schwartz et al., 2017a; Cai et al., 2017). A similar phenomenon was observed in reading comprehension, where systems performed non-trivially well by using only the final sentence in the passage or ignoring the passage altogether (Kaushik & Lipton, 2018). Finally, multiple studies found no"
P19-1084,W18-5441,1,0.895818,"Missing"
P19-1084,P16-2022,0,0.0707933,"ng the training set but failing on the test set. However, with strong enough hyperparameters (moving towards the bottom in the tables), they perform perfectly on both the biased training set and the unbiased test set. For Method 1, stronger hyper-parameters work better. Baseline & Implementation Details We use InferSent (Conneau et al., 2017) as our baseline model because it has been shown to work well on popular NLI datasets and is representative of many NLI models. We use separate BiLSTM encoders to learn vector representations of P and H.11 The vector representations are combined following Mou et al. (2016),12 and passed to an MLP classifier with one hidden layer. Our proposed 9 Detailed descriptions of these datasets can be found in Poliak et al. (2018b). 10 We leave additional NLI datasets, such as the Diverse NLI Collection (Poliak et al., 2018a), for future work. 11 Many NLI models encode P and H separately (Rockt¨aschel et al., 2016; Mou et al., 2016; Liu et al., 2016; Cheng et al., 2016; Chen et al., 2017), although some share information between the encoders via attention (Parikh et al., 2016; Duan et al., 2018). 12 Specifically, representations are concatenated, subtracted, and multiplie"
P19-1084,P18-1176,0,0.0561621,"Missing"
P19-1084,D12-1071,0,0.092754,"are provided in Appendix A.2. For both methods, we sweep hyper-parameters α, β over {0.05, 0.1, 0.2, 0.4, 0.8, 1.0}. For each target dataset, we choose the best-performing model on its development set and report results on the test set.13 sense Inference (JOCI; Zhang et al., 2017), Multiple Premise Entailment (MPE; Lai et al., 2017),and Sentences Involving Compositional Knowledge (SICK; Marelli et al., 2014). The target datasets also include datasets recast by White et al. (2017) to evaluate different semantic phenomena: FrameNet+ (FN+; Pavlick et al., 2015), Definite Pronoun Resolution (DPR; Rahman & Ng, 2012), and Semantic Proto-Roles (SPR; Reisinger et al., 2015).9 As many of these datasets have different label spaces than SNLI, we define a mapping (Appendix A.1) from our models’ predictions to each target dataset’s labels. Finally, we also test on the Multi-genre NLI dataset (MNLI; Williams et al., 2018), a successor to SNLI.10 5 Results 5.1 Synthetic Experiments To examine how well our methods work in a controlled setup, we train on the biased dataset (B), but evaluate on the unbiased test set (A). As expected, without a method to remove hypothesisonly biases, the baseline fails to generalize t"
P19-1084,D16-1244,0,0.170648,"Missing"
P19-1084,Q15-1034,1,0.916901,"Missing"
P19-1084,P18-1079,0,0.45999,"sentations of P and H, and a classification layer, gθ , which learns a distribution over y. Typically, this is done by maximizing this discriminative likelihood directly, which will act as our baseline (Figure 1a). However, many NLI datasets contain biases that allow models to perform non-trivially well when accessing just the hypotheses (Tsuchiya, 2018; Gururangan et al., 2018; Poliak et al., 2018b). This allows models to leverage hypothesis-only biases that may be present in a dataset. A model may perform well on a specific dataset, without identifying whether P entails H. Gururangan et al. (2018) argue that “the bulk” of many models’ “success [is] attribute[d] to the easy examples”. Consequently, this may limit how well a model trained on one dataset would perform on other datasets that may have different artifacts. Consider an example where P and H are strings from {a, b, c}, and an environment where P enIn summary, in this paper we make the follow878 tails H if and only if the first letters are the same, as in synthetic dataset A. In such a setting, a model should be able to learn the correct condition for P to entail H.4 3.1 Our first approach is to estimate the term p(y |H) direct"
P19-1084,L18-1239,0,0.442811,"se for Granted: Mitigating Artifacts in Natural Language Inference Yonatan Belinkov13∗ Adam Poliak2∗ Stuart M. Shieber1 Benjamin Van Durme2 Alexander M. Rush1 1 2 3 Harvard University Johns Hopkins University Massachusetts Institute of Technology {belinkov,shieber,srush}@seas.harvard.edu {azpoliak,vandurme}@cs.jhu.edu Abstract many NLI datasets contain biases, or annotation artifacts, i.e., features present in hypotheses that enable models to perform surprisingly well using only the hypothesis, without learning the relationship between two texts (Gururangan et al., 2018; Poliak et al., 2018b; Tsuchiya, 2018).3 For instance, in some datasets, negation words like “not” and “nobody” are often associated with a relationship of contradiction. As a ramification of such biases, models may not generalize well to other datasets that contain different or no such biases. Recent studies have tried to create new NLI datasets that do not contain such artifacts, but many approaches to dealing with this issue remain unsatisfactory: constructing new datasets (Sharma et al., 2018) is costly and may still result in other artifacts; filtering “easy” examples and defining a harder subset is useful for evaluation purp"
P19-1084,W18-5446,0,0.0389124,"= (1 − α) log pθ (y |P, H) θ − α log pθ,φ (y |P 0 , H) max L2 (φ) = β log pθ,φ (y |P 0 , H) Common NLI datasets Moving to existing NLI datasets, we train models on the Stanford Natural Language Inference dataset (SNLI; Bowman et al., 2015), since it is known to contain significant annotation artifacts. We evaluate the robustness of our methods on other, target datasets. As target datasets, we use the 10 datasets investigated by Poliak et al. (2018b) in their hypothesisonly study, plus two test sets: GLUE’s diagnostic test set, which was carefully constructed to not contain hypothesis-biases (Wang et al., 2018), and SNLI-hard, a subset of the SNLI test set that is thought to have fewer biases (Gururangan et al., 2018). The target datasets include humanjudged datasets that used automatic methods to pair premises and hypotheses, and then relied on humans to label the pairs: SCITAIL (Khot et al., 2018), ADD-ONE-RTE (Pavlick & CallisonBurch, 2016), Johns Hopkins Ordinal Commonφ Finally, we share the classifier weights between pθ (y |P, H) and pφ,θ (y |P 0 , H). In a sense this is counter-intuitive, since pθ is being trained to unlearn bias, while pφ,θ is being trained to learn it. However, if the models"
P19-1084,K17-1004,0,0.222659,"-tuning on target datasets Related Work Biases and artifacts in NLU datasets Many natural language undersrtanding (NLU) datasets contain annotation artifacts. Early work on NLI, also known as recognizing textual entailment (RTE), found biases that allowed models to perform relatively well by focusing on syntactic clues alone (Snow et al., 2006; Vanderwende & Dolan, 2006). Recent work also found artifacts in new NLI datasets (Tsuchiya, 2018; Gururangan et al., 2018; Poliak et al., 2018b). Other NLU datasets also exhibit biases. In ROC Stories (Mostafazadeh et al., 2016), a story cloze dataset, Schwartz et al. (2017b) obtained a high performance by only considering the candidate endings, without even looking at the story context. In this case, stylistic features of the candidate endings alone, such as the length or certain words, were strong indicators of the correct ending (Schwartz et al., 2017a; Cai et al., 2017). A similar phenomenon was observed in reading comprehension, where systems performed non-trivially well by using only the final sentence in the passage or ignoring the passage altogether (Kaushik & Lipton, 2018). Finally, multiple studies found non-trivial performance in visual question answe"
P19-1084,I17-1100,1,0.922317,"Missing"
P19-1084,W17-0907,0,0.0297645,"Missing"
P19-1084,N18-1101,0,0.316511,"Entailment (MPE; Lai et al., 2017),and Sentences Involving Compositional Knowledge (SICK; Marelli et al., 2014). The target datasets also include datasets recast by White et al. (2017) to evaluate different semantic phenomena: FrameNet+ (FN+; Pavlick et al., 2015), Definite Pronoun Resolution (DPR; Rahman & Ng, 2012), and Semantic Proto-Roles (SPR; Reisinger et al., 2015).9 As many of these datasets have different label spaces than SNLI, we define a mapping (Appendix A.1) from our models’ predictions to each target dataset’s labels. Finally, we also test on the Multi-genre NLI dataset (MNLI; Williams et al., 2018), a successor to SNLI.10 5 Results 5.1 Synthetic Experiments To examine how well our methods work in a controlled setup, we train on the biased dataset (B), but evaluate on the unbiased test set (A). As expected, without a method to remove hypothesisonly biases, the baseline fails to generalize to the test set. Examining its predictions, we found that the baseline model learned to rely on the presence/absence of the bias term c, always predicting T RUE/FALSE respectively. Table 1 shows the results of our two proposed methods. As we increase the hyper-parameters α and β, our methods initially b"
P19-1084,N06-1005,0,0.725739,"Missing"
P19-1084,D10-1074,0,0.0834512,"Missing"
P19-1084,W09-3930,0,0.0870273,"Missing"
P19-1084,Q17-1027,1,0.9051,"Missing"
P19-1084,D16-1053,0,\N,Missing
P19-1084,P17-1152,0,\N,Missing
P19-1084,D18-1007,1,\N,Missing
P19-1084,Q19-1004,1,\N,Missing
P19-1228,P06-1109,0,0.752129,"Spitkovsky et al., 2012). During training we perform early stopping based on validation perplexity.7 To mitigate against overfitting to PTB, experiments on CTB utilize the same hyperparameters from PTB. Baselines and Evaluation We observe that even on PTB, there is enough variation in setups across prior work on grammar induction to render a meaningful comparison difficult. Some important dimensions along which prior works vary include, (1) lexicalization: earlier work on grammar induction generally assumed gold (or induced) partof-speech tags (Klein and Manning, 2004; Smith and Eisner, 2004; Bod, 2006; Snyder et al., 2009), while more recent works induce grammar directly from words (Spitkovsky et al., 2013; Shen et al., 2018); (2) use of punctuation: even within papers that induce a grammar directly from words, some papers employ heuristics based on punctuation as punctuation is usually a strong signal for start/end of constituents (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013), some train with punctuation (Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019), while others discard punctuation altogether for training (Shen et al., 2018, 2019); (3) train/test data: some"
P19-1228,N19-1116,0,0.167779,"e latent vector, we employ standard amortized inference using reparameterized samples from a variational 2369 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2369–2385 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics posterior approximated from an inference network (Kingma and Welling, 2014; Rezende et al., 2014). On standard benchmarks for English and Chinese, the proposed approach is found to perform favorably against recent neural network-based approaches to grammar induction (Shen et al., 2018, 2019; Drozdov et al., 2019; Kim et al., 2019). 2 Probabilistic Context-Free Grammars We consider context-free grammars (CFG) consisting of a 5-tuple G = (S, N , P, Σ, R) where S is the distinguished start symbol, N is a finite set of nonterminals, P is a finite set of preterminals,1 Σ is a finite set of terminal symbols, and R is a finite set of rules of the form, S → A, A∈N A → B C, A ∈ N , B, C ∈ N ∪ P T → w, Charniak, 1992).2 Successful approaches to unsupervised parsing have therefore modified the model/learning objective by guiding potentially unrelated rules to behave similarly. Recognizing that sharing among rul"
P19-1228,P15-1030,0,0.0253383,"odel trained on induced trees on classification tasks. Figure 2: Alignment of induced nonterminals ordered from top based on predicted frequency (therefore NT-04 is the most frequently-predicted nonterminal). For each nonterminal we visualize the proportion of correctly-predicted constituents that correspond to particular gold labels. For reference we also show the precision (i.e. probability of correctly predicting unlabeled constituents) in the rightmost column. tween the original model and the URNNG’s structured inference network, which is parameterized as a neural CRF constituency parser (Durrett and Klein, 2015; Liu et al., 2018).18 Model Analysis We analyze our best compound PCFG model in more detail. Since we induce a full set of nonterminals in our grammar, we can analyze the learned nonterminals to see if they can be aligned with linguistic constituent labels. Figure 2 visualizes the alignment between induced and gold labels, where for each nonterminal we show the empirical probability that a predicted constituent of this type will correspond to a particular linguistic constituent in the test set, conditioned on its being a correct constituent (for reference we also show the precision). We obser"
P19-1228,N16-1024,1,0.888677,"Missing"
P19-1228,P12-2004,0,0.587187,"Carroll and Charniak, 1992). While the reasons for the failure are manifold and not completely understood, two major potential causes are the ill-behaved optimization landscape and the overly strict independence assumptions of PCFGs. More successful approaches to grammar induction have thus resorted to carefully-crafted auxiliary objectives (Klein and Manning, 2002), priors or Code: https://github.com/harvardnlp/compound-pcfg non-parametric models (Kurihara and Sato, 2006; Johnson et al., 2007; Liang et al., 2007; Wang and Blunsom, 2013), and manually-engineered features (Huang et al., 2012; Golland et al., 2012) to encourage the desired structures to emerge. We revisit these aforementioned issues in light of advances in model parameterization and inference. First, contrary to common wisdom, we find that parameterizing a PCFG’s rule probabilities with neural networks over distributed representations makes it possible to induce linguistically meaningful grammars by simply optimizing log likelihood. While the optimization problem remains non-convex, recent work suggests that there are optimization benefits afforded by over-parameterized models (Arora et al., 2018; Xu et al., 2018; Du et al., 2019), and"
P19-1228,N19-1254,0,0.0148113,"ver, the 2019), though comparison is confounded by various factors such as preprocessing (e.g. we drop punctuation). A neural PCFG/HMM obtains 68.2 and 63.4 respectively. model fails to identify ((T-40 w5 ) (T-22 w6 )) as a constituent in this case (as well as well in the bottom right example). See appendix A.5 for more examples. It is possible that the model is utilizing the subtrees to capture broad template-like structures and then using z to fill them in, similar to recent works that also train models to separate “what to say” from “how to say it” (Wiseman et al., 2018; Peng et al., 2019; Chen et al., 2019a,b). Limitations We report on some negative results as well as important limitations of our work. While distributed representations promote parameter sharing, we were unable to obtain improvements through more factorized parameterizations that promote even greater parameter sharing. In particular, for rules of the type A → BC, we tried having the output embeddings be a function of the input embeddings (e.g. uBC = g([wB ; wC ]) where g is an MLP), but obtained worse results. For rules of the type T → w, we tried using a character-level CNN (dos Santos and Zadrozny, 2014; Kim et al., 2016) to o"
P19-1228,J98-4004,0,0.424064,"more expressive than PCFGs as each sentence has its own set of rule probabilities. However, it still assumes a tree-based generative process, making it possible to learn latent tree structures. Our motivation for the compound PCFG is based on the observation that for grammar induction, first-order context-free assumptions are generally made not because they represent an adequate model of natural language, but because they allow for tractable training.4 Higher-order PCFGs can introduce dependencies between children and ancestors/siblings through, for example, vertical/horizontal Markovization (Johnson, 1998; Klein and Manning, 2003). However such dependencies complicate training due to the rapid increase in the number of rules. Under this view, we can interpret the compound PCFG as a restricted version of some higher-order PCFG where a child can depend on its ancestors and siblings through a shared latent vector. We hypothesize that this dependence among siblings is especially useful in grammar induction from words, where (for example) if we know that watched is used as a verb 4 A piece of evidence for the misspecification of first-order PCFGs as a statistical model of natural language is that i"
P19-1228,W01-0713,0,0.697458,"hest (PC +) principal component values. plementations, training was significantly more expensive (both in terms of time and memory) than NLM-based grammar induction systems due to the O(|R||x|3 ) dynamic program, which makes our approach potentially difficult to scale. 6 Related Work Grammar induction has a long and rich history in natural language processing. Early work on grammar induction with pure unsupervised learning was mostly negative (Lari and Young, 1990; Carroll and Charniak, 1992; Charniak, 1993), though Pereira and Schabes (1992) reported some success on partially bracketed data. Clark (2001) and Klein and Manning (2002) were some of the first successful statistical approaches to grammar induction. In particular, the constituent-context model (CCM) of Klein and Manning (2002), which explicitly models both constituents and distituents, was the basis for much subsequent work (Klein and Manning, 2004; Huang et al., 2012; Golland et al., 2012). Other works have explored imposing inductive biases through Bayesian priors (Johnson et al., 2007; Liang et al., 2007; Wang and Blunsom, 2013), modified objectives (Smith and Eisner, 2004), and additional constraints on recursion depth (Noji et"
P19-1228,N07-1018,0,0.78419,"language data through direct methods, such as optimizing the log likelihood with the EM algorithm (Lari and Young, 1990; Carroll and Charniak, 1992). While the reasons for the failure are manifold and not completely understood, two major potential causes are the ill-behaved optimization landscape and the overly strict independence assumptions of PCFGs. More successful approaches to grammar induction have thus resorted to carefully-crafted auxiliary objectives (Klein and Manning, 2002), priors or Code: https://github.com/harvardnlp/compound-pcfg non-parametric models (Kurihara and Sato, 2006; Johnson et al., 2007; Liang et al., 2007; Wang and Blunsom, 2013), and manually-engineered features (Huang et al., 2012; Golland et al., 2012) to encourage the desired structures to emerge. We revisit these aforementioned issues in light of advances in model parameterization and inference. First, contrary to common wisdom, we find that parameterizing a PCFG’s rule probabilities with neural networks over distributed representations makes it possible to induce linguistically meaningful grammars by simply optimizing log likelihood. While the optimization problem remains non-convex, recent work suggests that there ar"
P19-1228,N09-1009,0,0.078737,"supervised initial grammar while the log marginal likelihood improves (Johnson et al., 2007). Similar observations have been made for part-of-speech induction with Hidden Markov Models (Merialdo, 1994). 2371 then the noun phrase is likely to be a movie. In contrast to the usual Bayesian treatment of PCFGs which places priors on global rule probabilities (Kurihara and Sato, 2006; Johnson et al., 2007; Wang and Blunsom, 2013), the compound PCFG assumes a prior on local, sentence-level rule probabilities. It is therefore closely related to the Bayesian grammars studied by Cohen et al. (2009) and Cohen and Smith (2009), who also sample local rule probabilities from a logistic normal prior for training dependency models with valence (DMV) (Klein and Manning, 2004). Inference in Compound PCFGs The expressivity of compound PCFGs comes at a significant challenge in learning and inference. Letting θ = {EG , λ} be the parameters of the generative model, we would like to maximize the log marginal likelihood of the observed sentence log pθ (x). In the neural PCFG P the log marginal likelihood log pθ (x) = log t∈TG (x) pθ (t) can be obtained by summing out the latent tree structure using the inside algorithm (Baker,"
P19-1228,N19-1114,1,0.658695,"ploy standard amortized inference using reparameterized samples from a variational 2369 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2369–2385 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics posterior approximated from an inference network (Kingma and Welling, 2014; Rezende et al., 2014). On standard benchmarks for English and Chinese, the proposed approach is found to perform favorably against recent neural network-based approaches to grammar induction (Shen et al., 2018, 2019; Drozdov et al., 2019; Kim et al., 2019). 2 Probabilistic Context-Free Grammars We consider context-free grammars (CFG) consisting of a 5-tuple G = (S, N , P, Σ, R) where S is the distinguished start symbol, N is a finite set of nonterminals, P is a finite set of preterminals,1 Σ is a finite set of terminal symbols, and R is a finite set of rules of the form, S → A, A∈N A → B C, A ∈ N , B, C ∈ N ∪ P T → w, Charniak, 1992).2 Successful approaches to unsupervised parsing have therefore modified the model/learning objective by guiding potentially unrelated rules to behave similarly. Recognizing that sharing among rule types is benefici"
P19-1228,J94-2001,0,0.206064,"ful in grammar induction from words, where (for example) if we know that watched is used as a verb 4 A piece of evidence for the misspecification of first-order PCFGs as a statistical model of natural language is that if one pretrains a first-order PCFG on supervised data and continues training with the unsupervised objective (i.e. log marginal likelihood), the resulting grammar deviates significantly from the supervised initial grammar while the log marginal likelihood improves (Johnson et al., 2007). Similar observations have been made for part-of-speech induction with Hidden Markov Models (Merialdo, 1994). 2371 then the noun phrase is likely to be a movie. In contrast to the usual Bayesian treatment of PCFGs which places priors on global rule probabilities (Kurihara and Sato, 2006; Johnson et al., 2007; Wang and Blunsom, 2013), the compound PCFG assumes a prior on local, sentence-level rule probabilities. It is therefore closely related to the Bayesian grammars studied by Cohen et al. (2009) and Cohen and Smith (2009), who also sample local rule probabilities from a logistic normal prior for training dependency models with valence (DMV) (Klein and Manning, 2004). Inference in Compound PCFGs Th"
P19-1228,D16-1004,0,0.0551085,"(2001) and Klein and Manning (2002) were some of the first successful statistical approaches to grammar induction. In particular, the constituent-context model (CCM) of Klein and Manning (2002), which explicitly models both constituents and distituents, was the basis for much subsequent work (Klein and Manning, 2004; Huang et al., 2012; Golland et al., 2012). Other works have explored imposing inductive biases through Bayesian priors (Johnson et al., 2007; Liang et al., 2007; Wang and Blunsom, 2013), modified objectives (Smith and Eisner, 2004), and additional constraints on recursion depth (Noji et al., 2016; Jin et al., 2018). While the framework of specifying the structure of a grammar and learning the parameters is common, other methods exist. Bod (2006) consider a nonparametric-style approach to unsupervised parsing by using random subsets of training subtrees to parse new sentences. Seginer (2007) utilize an incremental algorithm to unsupervised parsing which makes local decisions to create constituents based on a complex set of heuristics. Ponvert et al. (2011) induce parse trees through cascaded applications of finite state models. More recently, neural network-based approaches to grammar"
P19-1228,P18-1249,0,0.111453,"Missing"
P19-1228,P14-1100,0,0.0148972,"unctuation: even within papers that induce a grammar directly from words, some papers employ heuristics based on punctuation as punctuation is usually a strong signal for start/end of constituents (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013), some train with punctuation (Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019), while others discard punctuation altogether for training (Shen et al., 2018, 2019); (3) train/test data: some works do not explicitly separate out train/test sets (Reichart and Rappoport, 2010; Golland et al., 2012) while some do (Huang et al., 2012; Parikh et al., 2014; Htut 7 However, we used F1 against validation trees on PTB to select some hyperparameters (e.g. grammar size), as is sometimes done in grammar induction. Hence our PTB results are arguably not fully unsupervised in the strictest sense of the term. The hyperparameters of the PRPN/ON baselines are also tuned using validation F1 for fair comparison. et al., 2018). Maintaining train/test splits is less of an issue for unsupervised structure learning, however in this work we follow the latter and separate train/test data. (4) evaluation: for unlabeled F1 , almost all works ignore punctuation (eve"
P19-1228,P02-1017,0,0.943053,"ameters through optimization. Early work found that it was difficult to induce probabilistic context-free grammars (PCFG) from natural language data through direct methods, such as optimizing the log likelihood with the EM algorithm (Lari and Young, 1990; Carroll and Charniak, 1992). While the reasons for the failure are manifold and not completely understood, two major potential causes are the ill-behaved optimization landscape and the overly strict independence assumptions of PCFGs. More successful approaches to grammar induction have thus resorted to carefully-crafted auxiliary objectives (Klein and Manning, 2002), priors or Code: https://github.com/harvardnlp/compound-pcfg non-parametric models (Kurihara and Sato, 2006; Johnson et al., 2007; Liang et al., 2007; Wang and Blunsom, 2013), and manually-engineered features (Huang et al., 2012; Golland et al., 2012) to encourage the desired structures to emerge. We revisit these aforementioned issues in light of advances in model parameterization and inference. First, contrary to common wisdom, we find that parameterizing a PCFG’s rule probabilities with neural networks over distributed representations makes it possible to induce linguistically meaningful g"
P19-1228,N19-1263,0,0.0161336,"eed mostly PP. However, the 2019), though comparison is confounded by various factors such as preprocessing (e.g. we drop punctuation). A neural PCFG/HMM obtains 68.2 and 63.4 respectively. model fails to identify ((T-40 w5 ) (T-22 w6 )) as a constituent in this case (as well as well in the bottom right example). See appendix A.5 for more examples. It is possible that the model is utilizing the subtrees to capture broad template-like structures and then using z to fill them in, similar to recent works that also train models to separate “what to say” from “how to say it” (Wiseman et al., 2018; Peng et al., 2019; Chen et al., 2019a,b). Limitations We report on some negative results as well as important limitations of our work. While distributed representations promote parameter sharing, we were unable to obtain improvements through more factorized parameterizations that promote even greater parameter sharing. In particular, for rules of the type A → BC, we tried having the output embeddings be a function of the input embeddings (e.g. uBC = g([wB ; wC ]) where g is an MLP), but obtained worse results. For rules of the type T → w, we tried using a character-level CNN (dos Santos and Zadrozny, 2014; Kim"
P19-1228,P04-1061,0,0.915052,"ech induction with Hidden Markov Models (Merialdo, 1994). 2371 then the noun phrase is likely to be a movie. In contrast to the usual Bayesian treatment of PCFGs which places priors on global rule probabilities (Kurihara and Sato, 2006; Johnson et al., 2007; Wang and Blunsom, 2013), the compound PCFG assumes a prior on local, sentence-level rule probabilities. It is therefore closely related to the Bayesian grammars studied by Cohen et al. (2009) and Cohen and Smith (2009), who also sample local rule probabilities from a logistic normal prior for training dependency models with valence (DMV) (Klein and Manning, 2004). Inference in Compound PCFGs The expressivity of compound PCFGs comes at a significant challenge in learning and inference. Letting θ = {EG , λ} be the parameters of the generative model, we would like to maximize the log marginal likelihood of the observed sentence log pθ (x). In the neural PCFG P the log marginal likelihood log pθ (x) = log t∈TG (x) pθ (t) can be obtained by summing out the latent tree structure using the inside algorithm (Baker, 1979), which is differentiable and thus amenable to gradientbased optimization. In the compound PCFG, the log marginal likelihood is given by Z X"
P19-1228,P07-1049,0,0.516964,"ingful comparison difficult. Some important dimensions along which prior works vary include, (1) lexicalization: earlier work on grammar induction generally assumed gold (or induced) partof-speech tags (Klein and Manning, 2004; Smith and Eisner, 2004; Bod, 2006; Snyder et al., 2009), while more recent works induce grammar directly from words (Spitkovsky et al., 2013; Shen et al., 2018); (2) use of punctuation: even within papers that induce a grammar directly from words, some papers employ heuristics based on punctuation as punctuation is usually a strong signal for start/end of constituents (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013), some train with punctuation (Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019), while others discard punctuation altogether for training (Shen et al., 2018, 2019); (3) train/test data: some works do not explicitly separate out train/test sets (Reichart and Rappoport, 2010; Golland et al., 2012) while some do (Huang et al., 2012; Parikh et al., 2014; Htut 7 However, we used F1 against validation trees on PTB to select some hyperparameters (e.g. grammar size), as is sometimes done in grammar induction. Hence our PTB results are arguably n"
P19-1228,P04-1062,0,0.0562063,"for grammar induction (Spitkovsky et al., 2012). During training we perform early stopping based on validation perplexity.7 To mitigate against overfitting to PTB, experiments on CTB utilize the same hyperparameters from PTB. Baselines and Evaluation We observe that even on PTB, there is enough variation in setups across prior work on grammar induction to render a meaningful comparison difficult. Some important dimensions along which prior works vary include, (1) lexicalization: earlier work on grammar induction generally assumed gold (or induced) partof-speech tags (Klein and Manning, 2004; Smith and Eisner, 2004; Bod, 2006; Snyder et al., 2009), while more recent works induce grammar directly from words (Spitkovsky et al., 2013; Shen et al., 2018); (2) use of punctuation: even within papers that induce a grammar directly from words, some papers employ heuristics based on punctuation as punctuation is usually a strong signal for start/end of constituents (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013), some train with punctuation (Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019), while others discard punctuation altogether for training (Shen et al., 2018, 2019); (3) train/test"
P19-1228,P09-1009,0,0.11015,"et al., 2012). During training we perform early stopping based on validation perplexity.7 To mitigate against overfitting to PTB, experiments on CTB utilize the same hyperparameters from PTB. Baselines and Evaluation We observe that even on PTB, there is enough variation in setups across prior work on grammar induction to render a meaningful comparison difficult. Some important dimensions along which prior works vary include, (1) lexicalization: earlier work on grammar induction generally assumed gold (or induced) partof-speech tags (Klein and Manning, 2004; Smith and Eisner, 2004; Bod, 2006; Snyder et al., 2009), while more recent works induce grammar directly from words (Spitkovsky et al., 2013; Shen et al., 2018); (2) use of punctuation: even within papers that induce a grammar directly from words, some papers employ heuristics based on punctuation as punctuation is usually a strong signal for start/end of constituents (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013), some train with punctuation (Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019), while others discard punctuation altogether for training (Shen et al., 2018, 2019); (3) train/test data: some works do not explicitl"
P19-1228,D12-1063,0,0.027687,"f the LSTM and passing it through an affine layer. Model parameters are initialized with Xavier uniform initialization. For training we use Adam (Kingma and Ba, 2015) with β1 = 0.75, β2 = 0.999 and learning rate of 0.001, with a maximum gradient norm limit of 3. We train for 10 epochs with batch size equal to 4. We employ a curriculum learning strategy (Bengio et al., 2009) where we train only on sentences of length up to 30 in the first epoch, and increase this length limit by 1 each epoch. This slightly improved performance and similar strategies have used in the past for grammar induction (Spitkovsky et al., 2012). During training we perform early stopping based on validation perplexity.7 To mitigate against overfitting to PTB, experiments on CTB utilize the same hyperparameters from PTB. Baselines and Evaluation We observe that even on PTB, there is enough variation in setups across prior work on grammar induction to render a meaningful comparison difficult. Some important dimensions along which prior works vary include, (1) lexicalization: earlier work on grammar induction generally assumed gold (or induced) partof-speech tags (Klein and Manning, 2004; Smith and Eisner, 2004; Bod, 2006; Snyder et al."
P19-1228,D18-1356,1,0.894462,"Missing"
P19-1503,P18-1015,0,0.0340563,"Missing"
P19-1503,N16-1012,1,0.886548,"while maintaining output fluency. Experiments on both abstractive and extractive sentence summarization data sets show promising results of our method without being exposed to any paired data. 1 Introduction Automatic text summarization is the process of formulating a shorter output text than the original while capturing its core meaning. We study the problem of unsupervised sentence summarization with no paired examples. While datadriven approaches have achieved great success based on various powerful learning frameworks such as sequence-to-sequence models with attention (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), variational auto-encoders (Miao and Blunsom, 2016), and reinforcement learning (Paulus et al., 2017), they usually require a large amount of parallel data for supervision to do well. In comparison, the unsupervised approach reduces the human effort for collecting and annotating large amount of paired training data. Recently researchers have begun to study the unsupervised sentence summarization tasks. These methods all use parameterized unsupervised learning methods to induce a latent variable model: for example Schumann (2018) uses a length controlled variational au"
P19-1503,K18-1040,0,0.226266,"et al., 2016), variational auto-encoders (Miao and Blunsom, 2016), and reinforcement learning (Paulus et al., 2017), they usually require a large amount of parallel data for supervision to do well. In comparison, the unsupervised approach reduces the human effort for collecting and annotating large amount of paired training data. Recently researchers have begun to study the unsupervised sentence summarization tasks. These methods all use parameterized unsupervised learning methods to induce a latent variable model: for example Schumann (2018) uses a length controlled variational autoencoder, Fevry and Phang (2018) use a denoising autoencoder but only for extractive summarization, and Wang and Lee (2018) apply a reinforcement learning procedure combined with GANs, which takes a further step to the goal of Miao and Blunsom (2016) using language as latent representations for semisupervised learning. This work instead proposes a simple approach to this task that does not require any joint training. We utilize a generic pretrained language model to enforce contextual matching between sentence prefixes. We then use a smoothed problem specific target language model to guide the fluency of the generation proce"
P19-1503,D15-1044,1,0.904748,"contextual matching while maintaining output fluency. Experiments on both abstractive and extractive sentence summarization data sets show promising results of our method without being exposed to any paired data. 1 Introduction Automatic text summarization is the process of formulating a shorter output text than the original while capturing its core meaning. We study the problem of unsupervised sentence summarization with no paired examples. While datadriven approaches have achieved great success based on various powerful learning frameworks such as sequence-to-sequence models with attention (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), variational auto-encoders (Miao and Blunsom, 2016), and reinforcement learning (Paulus et al., 2017), they usually require a large amount of parallel data for supervision to do well. In comparison, the unsupervised approach reduces the human effort for collecting and annotating large amount of paired training data. Recently researchers have begun to study the unsupervised sentence summarization tasks. These methods all use parameterized unsupervised learning methods to induce a latent variable model: for example Schumann (2018) uses a length cont"
P19-1503,D18-1451,0,0.608931,"(Paulus et al., 2017), they usually require a large amount of parallel data for supervision to do well. In comparison, the unsupervised approach reduces the human effort for collecting and annotating large amount of paired training data. Recently researchers have begun to study the unsupervised sentence summarization tasks. These methods all use parameterized unsupervised learning methods to induce a latent variable model: for example Schumann (2018) uses a length controlled variational autoencoder, Fevry and Phang (2018) use a denoising autoencoder but only for extractive summarization, and Wang and Lee (2018) apply a reinforcement learning procedure combined with GANs, which takes a further step to the goal of Miao and Blunsom (2016) using language as latent representations for semisupervised learning. This work instead proposes a simple approach to this task that does not require any joint training. We utilize a generic pretrained language model to enforce contextual matching between sentence prefixes. We then use a smoothed problem specific target language model to guide the fluency of the generation process. We combine these two models in a product-of-experts objective. This approach does not r"
P19-1503,D16-1138,0,0.0438985,"over the token sequence y, pcm (y|x) = N Y qcm (yn |y<n , x). n=1 The generative process aligns each target word to a source prefix. At the first step, n = 1, we compute a greedy alignment score for each possible word w ∈ C, sw = maxj≥1 S(x1:j , w) for all source prefixes up to length j. The probability qcm (y1 = w|x) is computed as softmax(s) over all target words. We also store the aligned context z1 = arg maxj≥1 S(x1:j , y1 ). For future words, we ensure that the alignment is strictly monotonic increasing, such that zn < zn+1 for all n. Monotonicity is a common assumption in summarization (Yu et al., 2016a,b; Raffel et al., 2017). For n &gt; 1 we compute the alignment score sw = maxj&gt;zn−1 S(x1:j , [y1:n−1 , w]) to only look at prefixes longer than zn−1 , the last greedy alignment. Since the distribution conditions on y the past alignments are deterministic to compute (and can be stored). The main computational cost is in extending the target language zn Calculate the similarity scores with best match Figure 1: Generative process of the contextual matching model. model context to compute S. This process is terminated when a sampled token in y is aligned to the end of the source sequence x, and the"
P19-1503,P18-2028,0,0.180589,"Missing"
P19-1503,D15-1042,0,0.0650347,"Missing"
P19-1503,D13-1155,0,0.253375,"nt lm is task specific, and pretrained on a corpus of summarizations. We use an LSTM model with 2 layers, both embedding size and hidden size set to 1024. It is trained using dropout rate 0.5 and SGD combined with gradient clipping. We test our method on both abstractive and extractive sentence summarization tasks. For abstractive summarization, we use the English Gigaword data set pre-processed by Rush et al. (2015). We train pfm using its 3.8 million headlines in the training set, and generate summaries for the input in test set. For extractive summarization, we use the Google data set from Filippova and Altun (2013). We train pfm on 200K compressed sentences in the training set and test on the first 1000 pairs of evaluation set consistent with previous works. For generation, we set λ = 0.11 in (1) and beam size to 10. Each source sentence is tokenized and lowercased, with periods deleted and a special end of sentence token appended. In abstractive summarization, we use K = 6 in the candidate list and use the fixed embeddings at the bottom layer of ELMo language model for similarity. Larger K has only small impact on performance but makes the generation more expensive. The hyper-parameter α for length pen"
P19-1503,D16-1031,0,0.0600802,"tractive sentence summarization data sets show promising results of our method without being exposed to any paired data. 1 Introduction Automatic text summarization is the process of formulating a shorter output text than the original while capturing its core meaning. We study the problem of unsupervised sentence summarization with no paired examples. While datadriven approaches have achieved great success based on various powerful learning frameworks such as sequence-to-sequence models with attention (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), variational auto-encoders (Miao and Blunsom, 2016), and reinforcement learning (Paulus et al., 2017), they usually require a large amount of parallel data for supervision to do well. In comparison, the unsupervised approach reduces the human effort for collecting and annotating large amount of paired training data. Recently researchers have begun to study the unsupervised sentence summarization tasks. These methods all use parameterized unsupervised learning methods to induce a latent variable model: for example Schumann (2018) uses a length controlled variational autoencoder, Fevry and Phang (2018) use a denoising autoencoder but only for ex"
P19-1503,K16-1028,0,0.0385241,"tput fluency. Experiments on both abstractive and extractive sentence summarization data sets show promising results of our method without being exposed to any paired data. 1 Introduction Automatic text summarization is the process of formulating a shorter output text than the original while capturing its core meaning. We study the problem of unsupervised sentence summarization with no paired examples. While datadriven approaches have achieved great success based on various powerful learning frameworks such as sequence-to-sequence models with attention (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), variational auto-encoders (Miao and Blunsom, 2016), and reinforcement learning (Paulus et al., 2017), they usually require a large amount of parallel data for supervision to do well. In comparison, the unsupervised approach reduces the human effort for collecting and annotating large amount of paired training data. Recently researchers have begun to study the unsupervised sentence summarization tasks. These methods all use parameterized unsupervised learning methods to induce a latent variable model: for example Schumann (2018) uses a length controlled variational autoencoder, Fevry and Phan"
P19-1503,N18-1202,0,0.021242,"balances integration with pcm . 2.3 Summary Generation To generate summaries we maximize the log probability (1) to approximate y∗ using beam search. We begin with a special start token. A sequence is moved out of beam if it has aligned to the end token appended to the source sequence. To discourage extremely short sequences, we apply length normalization to re-rank the finished hypotheses. We choose a simple length penalty as lp(y) = |y |+ α with α a tuning parameter. 3 Experimental Setup For the contextual matching model’s similarity function S, we adopt the forward language model of ELMo (Peters et al., 2018) to encode tokens to corresponding hidden states in the sequence, resulting in a three-layer representation each of dimension 512. The bottom layer is a fixed character embedding layer, and the above two layers are LSTMs associated with the generic unsupervised language model trained on a large amount of text data. We explicitly manage the ELMo hidden states to allow our model to generate contextual embeddings sequentially for efficient beam search.1 The fluency language model component lm is task specific, and pretrained on a corpus of summarizations. We use an LSTM model with 2 layers, both"
P19-3019,J95-2003,0,0.075656,"he demo and 21,000 for the blog. Numerous news websites and policy researchers reached out to discuss the ethical implications of language generation. The feedback from these discussions and in-person presentations helped us to refine our publicly released examples and explore the limits of our detection methods. Qualitative Findings The tool caused students to think about the properties of the fake text. While humans would vary expressions in real texts, models rarely generate synonyms or referring expressions for entities, which does not follow the theory of centering in discourse analysis (Grosz et al., 1995). An example of this is shown in the text in Figure 3b in which the model keeps generating the name P´erez and never refers to him as he. Another observation was that samples from Heliograf exhibit high parallelism in sentence structure. Since previous work has found that neural language models learn long linguistic structures as well, we imagine that sentence structure analysis can further be used for forensic analysis. We hope that automatic analysis and visualization like GLTR will help students better underFuture Work A core assumption of GLTR is that systems use biased sampling for genera"
P19-3019,D17-1210,0,0.0651581,"Missing"
P19-3019,P13-1157,0,0.0740656,"Missing"
P19-3019,P16-2057,0,0.0917082,"Missing"
P19-3019,I08-2115,0,0.421989,"Missing"
P19-3019,N18-1202,0,0.0218113,"ext follows a similar sampling assumption and is generated by a large language model. We develop a visual tool, GLTR, that highlights text passages based on these metrics, as shown in Figure 11 . We conduct experiments to empirically test these metrics on a set of widely-used language models and show that real text uses a wider subset of the distribution under a model. This is noticeable especially when the model distribution is low-entropy and concentrates most Introduction The success of pretrained language models for natural language understanding (McCann et al., 2017; Devlin et al., 2018; Peters et al., 2018) has led to a race to train unprecedentedly large language models (Radford et al., 2019). These large language models have the potential to generate textual output that is indistinguishable from human-written text to a non-expert reader. That means that the advances in the development of large language models also lower the barrier for abuse. Instances of malicious autonomously generated text at scale are rare but often high-profile, for instance when a simple generation system was used to create fake comments in opposition to net neutrality (Grimaldi, 2018). Other scenarios include the possib"
P19-3019,D17-1235,0,0.0273892,"Missing"
P19-3019,P18-1082,0,0.0387421,"to the language model distribution, p(Xi |X1:i−1 ), that was used in generation. In the general case, we assume access to a different learned model of the same form. This approach can be contextualized in the evaluation framework proposed by Hashimoto et al. (2019) who find that human-written and generated text can be discriminated based on the model likelihood if the human acceptability is high. The underlying assumption of our methods is that to generate natural looking text, most systems sample from the head of the distribution, e.g., through max sampling (Gu et al., 2017), k-max sampling (Fan et al., 2018), beam search (Chorowski and Jaitly, 2016; Shao et al., 3 GLTR: Visualizing Outliers We apply these tests within our tool GLTR (pronounced Glitter) – a Giant Language model Test Room. GLTR aims to both teach users what to be aware of when assessing whether a text is real, and to assist them in performing forensic analyses. It works on a per-instance basis for any textual input. The backend supports multiple detection models. Our publicly deployed version uses both BERT (Devlin et al., 2018) and GPT-2 117M (Radford et al., 2019). Since GPT-2 117M is a standard left-to-right language model, we c"
P19-3019,E14-1030,0,0.0194143,"ord et al., 2019). These large language models have the potential to generate textual output that is indistinguishable from human-written text to a non-expert reader. That means that the advances in the development of large language models also lower the barrier for abuse. Instances of malicious autonomously generated text at scale are rare but often high-profile, for instance when a simple generation system was used to create fake comments in opposition to net neutrality (Grimaldi, 2018). Other scenarios include the possibility of generating false articles (Wang, 2017) or misleading reviews (Fornaciari and Poesio, 2014). Forensic techniques will be necessary to detect this automatically generated text. These techniques should be accurate, but also easy to convey to non-experts and require little setup cost. 1 Our tool is available at http://gltr.io. The code is provided at https://github.com/ HendrikStrobelt/detecting-fake-text 111 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 111–116 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) statistics (b) config (c) tokens (d) details about ‘chuck’"
P19-3019,P17-2067,0,0.0399913,"entedly large language models (Radford et al., 2019). These large language models have the potential to generate textual output that is indistinguishable from human-written text to a non-expert reader. That means that the advances in the development of large language models also lower the barrier for abuse. Instances of malicious autonomously generated text at scale are rare but often high-profile, for instance when a simple generation system was used to create fake comments in opposition to net neutrality (Grimaldi, 2018). Other scenarios include the possibility of generating false articles (Wang, 2017) or misleading reviews (Fornaciari and Poesio, 2014). Forensic techniques will be necessary to detect this automatically generated text. These techniques should be accurate, but also easy to convey to non-experts and require little setup cost. 1 Our tool is available at http://gltr.io. The code is provided at https://github.com/ HendrikStrobelt/detecting-fake-text 111 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 111–116 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) statist"
P19-3019,N19-1169,0,\N,Missing
S19-1028,N18-2017,0,0.0824448,"Missing"
S19-1028,D15-1075,0,0.21254,"Missing"
S19-1028,N18-1111,0,0.0169731,"ils another (hypothesis) - contain hypothesis-only biases that allow models to perform the task surprisingly well by only considering hypotheses while ignoring the corresponding premises. For instance, such a method correctly predicted the examples in Table 1 as contradictions. As datasets may always contain biases, it is important to analyze whether, and to what extent, models are immune to or rely on known biases. Furthermore, it is important to build models that can overcome these biases. Recent work in NLP aims to build more robust systems using adversarial methods (Alzantot et al., 2018; Chen & Cardie, 2018; Belinkov & Bisk, 2018, i.a.). In particular, Elazar & Goldberg (2018) attempted to use adversarial training to remove demographic attributes from text data, with limited success. Inspired by this line of work, we use adversarial learning to add small components to an existing and popular NLI system that has been used to learn general sentence representations (Conneau et al., 2017). The adversarial ∗ techniques include (1) using an external adversarial classifier conditioned on hypotheses alone, and (2) creating noisy, perturbed training examples. In our analyses we ask whether hidden, hypoth"
S19-1028,P18-1225,0,0.0559285,"ude (1) using an external adversarial classifier conditioned on hypotheses alone, and (2) creating noisy, perturbed training examples. In our analyses we ask whether hidden, hypothesisonly biases are no longer present in the resulting sentence representations after adversarial learning. The goal is to build models with less bias, ideally while limiting the inevitable degradation in task performance. Our results suggest that progress on this goal may depend on which adversarial learning techniques are used. Although recent work has applied adversarial learning to NLI (Minervini & Riedel, 2018; Kang et al., 2018), this is the first work to our knowledge that explicitly studies NLI models designed to ignore hypothesis-only biases. 2 Methods We consider two types of adversarial methods. In the first method, we incorporate an external classifier to force the hypothesis-encoder to ignore hypothesis-only biases. In the second method, we randomly swap premises in the training set to create noisy examples. Equal contribution 256 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 256–262 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics"
S19-1028,D17-1070,0,0.235347,"e immune to or rely on known biases. Furthermore, it is important to build models that can overcome these biases. Recent work in NLP aims to build more robust systems using adversarial methods (Alzantot et al., 2018; Chen & Cardie, 2018; Belinkov & Bisk, 2018, i.a.). In particular, Elazar & Goldberg (2018) attempted to use adversarial training to remove demographic attributes from text data, with limited success. Inspired by this line of work, we use adversarial learning to add small components to an existing and popular NLI system that has been used to learn general sentence representations (Conneau et al., 2017). The adversarial ∗ techniques include (1) using an external adversarial classifier conditioned on hypotheses alone, and (2) creating noisy, perturbed training examples. In our analyses we ask whether hidden, hypothesisonly biases are no longer present in the resulting sentence representations after adversarial learning. The goal is to build models with less bias, ideally while limiting the inevitable degradation in task performance. Our results suggest that progress on this goal may depend on which adversarial learning techniques are used. Although recent work has applied adversarial learning"
S19-1028,K18-1007,0,0.120137,"ersarial ∗ techniques include (1) using an external adversarial classifier conditioned on hypotheses alone, and (2) creating noisy, perturbed training examples. In our analyses we ask whether hidden, hypothesisonly biases are no longer present in the resulting sentence representations after adversarial learning. The goal is to build models with less bias, ideally while limiting the inevitable degradation in task performance. Our results suggest that progress on this goal may depend on which adversarial learning techniques are used. Although recent work has applied adversarial learning to NLI (Minervini & Riedel, 2018; Kang et al., 2018), this is the first work to our knowledge that explicitly studies NLI models designed to ignore hypothesis-only biases. 2 Methods We consider two types of adversarial methods. In the first method, we incorporate an external classifier to force the hypothesis-encoder to ignore hypothesis-only biases. In the second method, we randomly swap premises in the training set to create noisy examples. Equal contribution 256 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 256–262 c Minneapolis, June 6–7, 2019. 2019 Association for Comput"
S19-1028,P16-2022,0,0.0833388,"filtered such that it may not contain unwanted artifacts. We apply both adversarial techniques to InferSent (Conneau et al., 2017), which serves as our general NLI architecture.2 Following the standard training details used in InferSent, we encode premises and hypotheses separately using bi-directional long short-term memory (BiLSTM) networks (Hochreiter & Schmidhuber, 1997). Premises and hypotheses are initially mapped (token-by-token) to Glove (Pennington et al., 2014) representations. We use max-pooling over the BiLSTM states to extract premise and hypothesis representations and, following Mou et al. (2016), combine the representations by concatenating their vectors, their difference, and their multiplication (element-wise). We use the default training hyper-parameters in the released InferSent codebase.3 These include setting the initial learning rate to 0.1 and the decay rate to 0.99, using SGD optimization and dividing the learning rate by 5 at every epoch when the accuracy deceases on the validation set. The default settings also include stopping training either when the learning rate drops below 10−5 or after 20 epochs. In both adversarial settings, the hyper-parameters are swept through {0"
S19-1028,D14-1162,0,0.0955863,"LI. We use the standard SNLI split and report validation and test results. We also test on SNLI-hard, a subset of SNLI that Gururangan et al. (2018) filtered such that it may not contain unwanted artifacts. We apply both adversarial techniques to InferSent (Conneau et al., 2017), which serves as our general NLI architecture.2 Following the standard training details used in InferSent, we encode premises and hypotheses separately using bi-directional long short-term memory (BiLSTM) networks (Hochreiter & Schmidhuber, 1997). Premises and hypotheses are initially mapped (token-by-token) to Glove (Pennington et al., 2014) representations. We use max-pooling over the BiLSTM states to extract premise and hypothesis representations and, following Mou et al. (2016), combine the representations by concatenating their vectors, their difference, and their multiplication (element-wise). We use the default training hyper-parameters in the released InferSent codebase.3 These include setting the initial learning rate to 0.1 and the decay rate to 0.99, using SGD optimization and dividing the learning rate by 5 at every epoch when the accuracy deceases on the validation set. The default settings also include stopping train"
S19-1028,W17-0907,0,0.145469,"Missing"
S19-1028,L18-1239,0,0.442817,"othesis-only biases. Adversarial learning may help models ignore sensitive biases and spurious correlations in data. We evaluate whether adversarial learning can be used in NLI to encourage models to learn representations free of hypothesis-only biases. Our analyses indicate that the representations learned via adversarial learning may be less biased, with only small drops in NLI accuracy. 1 A person writing something on a newspaper I A person is driving a fire truck A man is doing tricks on a skateboard I Nobody is doing tricks Table 1: Examples from SNLI’s development set that Poliak et al. (2018)’s hypothesis-only model correctly predicted as contradictions. The first line in each section is a premise and lines with I are corresponding hypotheses. The italicized words are correlated with the “contradiction” label in SNLI Introduction Popular datasets for Natural Language Inference (NLI) - the task of determining whether one sentence (premise) likely entails another (hypothesis) - contain hypothesis-only biases that allow models to perform the task surprisingly well by only considering hypotheses while ignoring the corresponding premises. For instance, such a method correctly predicted"
S19-1028,W18-5448,0,0.0525636,"Missing"
S19-1028,D18-1316,0,\N,Missing
S19-1028,D18-1002,0,\N,Missing
S19-1028,W19-1801,1,\N,Missing
W13-3507,P12-1024,1,0.909167,"g statistically consistent parameter estimates: even with very large amounts of data, EM is not guaranteed to estimate parameters which are close to the “correct” model parameters. In this paper, we derive a spectral algorithm for learning the parameters of R-HMMs. Unlike EM, this technique is guaranteed to find the true parameters of the underlying model under mild conditions on the singular values of the model. The algorithm we derive is simple and efficient, relying on singular value decomposition followed by standard matrix operations. We also describe the connection of R-HMMs to L-PCFGs. Cohen et al. (2012) present a spectral algorithm for L-PCFG estimation, but the na¨ıve transformation of the L-PCFG model and its spectral algorithm to R-HMMs is awkward and opaque. We therefore work through the non-trivial derivation the spectral algorithm for R-HMMs. We note that much of the prior work on spectral algorithms for discrete structures in NLP has shown limited experimental success for this family of algorithms (see, for example, Luque et al., 2012). Our experiments demonstrate empirical Introduction Consider the task of supervised sequence labeling. We are given a training set where the j’th train"
W13-3507,N13-1015,1,0.903398,"hidden state as well as the label. Unfortunately, estimating the parameters of an R-HMM is complicated by the unobserved hidden variables. A standard approach is to use the expectation-maximization (EM) algorithm which 56 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 56–64, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics 3.1 success for the R-HMM spectral algorithm. The spectral algorithm performs competitively with EM on a phoneme recognition task, and is more stable with respect to the number of hidden states. Cohen et al. (2013) present experiments with a parsing algorithm and also demonstrate it is competitive with EM. Our set of experiments comes as an additional piece of evidence that spectral algorithms can function as a viable, efficient and more principled alternative to the EM algorithm. 2 We distinguish row vectors from column vectors when such distinction is necessary. We use a superscript > to denote the transpose operation. We write [n] to denote the set {1, 2, . . . , n} for any integer n ≥ 1. For any vector v ∈ Rm , diag(v) ∈ Rm×m is a diagonal matrix with entries v1 . . . vm . For any statement S, we us"
W13-3507,E12-1042,0,0.0933967,"le and efficient, relying on singular value decomposition followed by standard matrix operations. We also describe the connection of R-HMMs to L-PCFGs. Cohen et al. (2012) present a spectral algorithm for L-PCFG estimation, but the na¨ıve transformation of the L-PCFG model and its spectral algorithm to R-HMMs is awkward and opaque. We therefore work through the non-trivial derivation the spectral algorithm for R-HMMs. We note that much of the prior work on spectral algorithms for discrete structures in NLP has shown limited experimental success for this family of algorithms (see, for example, Luque et al., 2012). Our experiments demonstrate empirical Introduction Consider the task of supervised sequence labeling. We are given a training set where the j’th training example consists of a sequence of ob(j) (j) servations x1 ...xN paired with a sequence of (j) (j) labels a1 ...aN and asked to predict the correct labels on a test set of observations. A common approach is to learn a joint distribution over sequences p(a1 . . . aN , x1 . . . xN ) as a hidden Markov model (HMM). The downside of HMMs is that they assume each label ai is independent of labels before the previous label ai−1 . This independence"
W13-3507,P05-1010,0,0.0362596,"or performance. Spectral learning has been applied to related models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b). Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing. 3 Notation 3.2 Definition of an R-HMM An R-HMM is a 7-tuple hl, m, n, π, o, t, f i for integers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations. • π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence. • o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] and h ∈ [m]. • t(b, h0 |a,"
W13-3507,P92-1017,0,0.411969,"ning has been applied to related models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b). Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing. 3 Notation 3.2 Definition of an R-HMM An R-HMM is a 7-tuple hl, m, n, π, o, t, f i for integers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations. • π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence. • o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] and h ∈ [m]. • t(b, h0 |a, h) is the probability of genera"
W13-3507,P06-1055,0,0.0566726,"his type are crucial for performance. Spectral learning has been applied to related models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b). Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing. 3 Notation 3.2 Definition of an R-HMM An R-HMM is a 7-tuple hl, m, n, π, o, t, f i for integers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations. • π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence. • o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] a"
W13-3507,D07-1094,0,0.0373788,"ated models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b). Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing. 3 Notation 3.2 Definition of an R-HMM An R-HMM is a 7-tuple hl, m, n, π, o, t, f i for integers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations. • π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence. • o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] and h ∈ [m]. • t(b, h0 |a, h) is the probability of generating b ∈ [l] and h0 ∈"
W16-0528,D14-1179,0,0.034134,"Missing"
W16-0528,W11-2838,0,0.0404598,"ets were not available, complicating comparisons and limiting the choice of methods used. Given the lack of a large hand-annotated corpus at the time, Park and Levy (2011) demonstrated the use of the EM algorithm for parameter learning of a noise model using error data without corrections, performing evaluation on a much smaller set of sentences hand-corrected by Amazon Mechanical Turk workers. More recent work has emerged as a result of a series of shared tasks, starting with the Helping Our Own (HOO) Pilot Shared Task run in 2011, which focused on a diverse set of errors in a small dataset (Dale and Kilgarriff, 2011), and the subsequent HOO 2012 Shared Task, which focused on the automated detection and correction of preposition and determiner errors (Dale et al., 2012). The CoNLL-2013 Shared Task (Ng et al., 2013)3 focused on the correction of a limited set of five error types in essays by second-language learners of English at the National University of Singapore. The follow-up CoNLL-2014 Shared Task (Ng et al., 2014)4 focused on the full generation task of correcting all errors in essays by second-language learners. As with machine translation (MT), evaluation of mation track. We leave for future work t"
W16-0528,W12-2006,0,0.0311,"y (2011) demonstrated the use of the EM algorithm for parameter learning of a noise model using error data without corrections, performing evaluation on a much smaller set of sentences hand-corrected by Amazon Mechanical Turk workers. More recent work has emerged as a result of a series of shared tasks, starting with the Helping Our Own (HOO) Pilot Shared Task run in 2011, which focused on a diverse set of errors in a small dataset (Dale and Kilgarriff, 2011), and the subsequent HOO 2012 Shared Task, which focused on the automated detection and correction of preposition and determiner errors (Dale et al., 2012). The CoNLL-2013 Shared Task (Ng et al., 2013)3 focused on the correction of a limited set of five error types in essays by second-language learners of English at the National University of Singapore. The follow-up CoNLL-2014 Shared Task (Ng et al., 2014)4 focused on the full generation task of correcting all errors in essays by second-language learners. As with machine translation (MT), evaluation of mation track. We leave for future work the adaptation of our approach to that task. 3 http://www.comp.nus.edu.sg/˜nlp/ conll13st.html 4 http://www.comp.nus.edu.sg/˜nlp/ conll14st.html the full ge"
W16-0528,W16-0506,0,0.0258345,"rposes here, we refer to all such edits as “grammatical” errors. 2 The 2016 Shared Task also included a probabilistic esti243 Evaluation is at the sentence level, but the paragraph-level context for each sentence is also provided. The paragraphs, themselves, are shuffled so that full article context is not available. A coarse academic field category is also provided for each paragraph. Our models described below do not make use of the paragraph context nor the field category, and they treat each sentence independently. Further information about the task is available in the Shared Task report (Daudaravicius et al., 2016). 3 Related Work While this is the first year for a shared task focusing on sentence-level binary error identification, previous work and shared tasks have focused on the related tasks of intra-sentence identification and correction of errors. Until recently, standard handannotated grammatical error datasets were not available, complicating comparisons and limiting the choice of methods used. Given the lack of a large hand-annotated corpus at the time, Park and Levy (2011) demonstrated the use of the EM algorithm for parameter learning of a noise model using error data without corrections, per"
W16-0528,W14-1702,0,0.0194439,"University of Singapore. The follow-up CoNLL-2014 Shared Task (Ng et al., 2014)4 focused on the full generation task of correcting all errors in essays by second-language learners. As with machine translation (MT), evaluation of mation track. We leave for future work the adaptation of our approach to that task. 3 http://www.comp.nus.edu.sg/˜nlp/ conll13st.html 4 http://www.comp.nus.edu.sg/˜nlp/ conll14st.html the full generation task is still an open research area, but a subsequent human evaluation ranked the output from the CoNLL-2014 Shared Task systems (Napoles et al., 2015). The system of Felice et al. (2014) ranked highest, utilizing a combination of a rule-based system and phrase-based MT, with re-ranking via a large web-scale language model. Of the non-MT based approaches, the IllinoisColumbia system was a strong performer, combining several classifiers trained for specific types of errors (Rozovskaya et al., 2014). 4 Models We use an end-to-end approach that does not have separate components for candidate generation or reranking that make use of hand-tuned rules or explicit syntax, nor do we employ separate classifiers for human-differentiated subsets of errors, unlike some previous work for t"
W16-0528,D14-1181,1,0.039554,"y separate classifiers for human-differentiated subsets of errors, unlike some previous work for the related task of grammatical error correction. We next introduce two approaches for the task of sentence-level grammatical error identification: A binary classifier and a sequence-to-sequence model that is trained for correction but can also be used for identification as a side-effect. 4.1 Baseline Convolutional Neural Net To establish a baseline, we follow past work that has shown strong performance with convolutional neural nets (CNNs) across various domains for sentence-level classification (Kim, 2014; Zhang and Wallace, 2015). We utilize the one-layer CNN architecture of Kim (2014) with the publicly available5 word vectors trained on the Google News dataset, which contains about 100 billion words (Mikolov et al., 2013). We experiment with keeping the word vectors static (CNN- STATIC) and fine-tuning the vectors (CNN- NONSTATIC). The CNN models only have access to sentence-level labels and are not given correction-level annotations. 4.2 Encoder-Decoder While it may seem more natural to utilize models trained for binary prediction, such as the aforementioned CNN, or for example, the recurre"
W16-0528,D15-1166,0,0.0472903,"Missing"
W16-0528,P15-2097,0,0.0078628,"learners of English at the National University of Singapore. The follow-up CoNLL-2014 Shared Task (Ng et al., 2014)4 focused on the full generation task of correcting all errors in essays by second-language learners. As with machine translation (MT), evaluation of mation track. We leave for future work the adaptation of our approach to that task. 3 http://www.comp.nus.edu.sg/˜nlp/ conll13st.html 4 http://www.comp.nus.edu.sg/˜nlp/ conll14st.html the full generation task is still an open research area, but a subsequent human evaluation ranked the output from the CoNLL-2014 Shared Task systems (Napoles et al., 2015). The system of Felice et al. (2014) ranked highest, utilizing a combination of a rule-based system and phrase-based MT, with re-ranking via a large web-scale language model. Of the non-MT based approaches, the IllinoisColumbia system was a strong performer, combining several classifiers trained for specific types of errors (Rozovskaya et al., 2014). 4 Models We use an end-to-end approach that does not have separate components for candidate generation or reranking that make use of hand-tuned rules or explicit syntax, nor do we employ separate classifiers for human-differentiated subsets of err"
W16-0528,W13-3601,0,0.0412932,"m for parameter learning of a noise model using error data without corrections, performing evaluation on a much smaller set of sentences hand-corrected by Amazon Mechanical Turk workers. More recent work has emerged as a result of a series of shared tasks, starting with the Helping Our Own (HOO) Pilot Shared Task run in 2011, which focused on a diverse set of errors in a small dataset (Dale and Kilgarriff, 2011), and the subsequent HOO 2012 Shared Task, which focused on the automated detection and correction of preposition and determiner errors (Dale et al., 2012). The CoNLL-2013 Shared Task (Ng et al., 2013)3 focused on the correction of a limited set of five error types in essays by second-language learners of English at the National University of Singapore. The follow-up CoNLL-2014 Shared Task (Ng et al., 2014)4 focused on the full generation task of correcting all errors in essays by second-language learners. As with machine translation (MT), evaluation of mation track. We leave for future work the adaptation of our approach to that task. 3 http://www.comp.nus.edu.sg/˜nlp/ conll13st.html 4 http://www.comp.nus.edu.sg/˜nlp/ conll14st.html the full generation task is still an open research area,"
W16-0528,W14-1701,0,0.22087,"ntains a “grammatical” error, broadly construed1 ). The dataset consists of sentences taken from academic articles annotated with corrections by professional editors. Annotations are described via insertions and deletions, which are marked with start and end tags. Tokens to be deleted are surrounded with the deletion start tag <del> and the deletion end tag </del> and tokens to be inserted are surrounded with the insertion start tag <ins> and the insertion end tag </ins>. Replacements (as shown in Figure 1) are represented as deletioninsertion pairs. Unlike the related CoNLL-2014 Shared Task (Ng et al., 2014) data, errors are not labeled with fine-grained types (article or determiner error, verb tense error, etc.). More formally, we assume a vocabulary V of natural language word types (some of which have orthographic errors) and a set Q = {<ins>, </ins>, <del>, </del>} of annotation tags. Given a sentence s = [s1 , . . . , sI ], where si ∈ V is the i-th token of the sentence of length I, we seek to predict whether or not the gold, annotated target sentence t = [t1 , . . . , tJ ], where tj ∈ Q ∪ V is the j-th token of the annotated sentence of length J, is identical to s. We are given both s and t"
W16-0528,P11-1094,0,0.031188,"they treat each sentence independently. Further information about the task is available in the Shared Task report (Daudaravicius et al., 2016). 3 Related Work While this is the first year for a shared task focusing on sentence-level binary error identification, previous work and shared tasks have focused on the related tasks of intra-sentence identification and correction of errors. Until recently, standard handannotated grammatical error datasets were not available, complicating comparisons and limiting the choice of methods used. Given the lack of a large hand-annotated corpus at the time, Park and Levy (2011) demonstrated the use of the EM algorithm for parameter learning of a noise model using error data without corrections, performing evaluation on a much smaller set of sentences hand-corrected by Amazon Mechanical Turk workers. More recent work has emerged as a result of a series of shared tasks, starting with the Helping Our Own (HOO) Pilot Shared Task run in 2011, which focused on a diverse set of errors in a small dataset (Dale and Kilgarriff, 2011), and the subsequent HOO 2012 Shared Task, which focused on the automated detection and correction of preposition and determiner errors (Dale et"
W16-0528,W14-1704,0,0.00810069,". 3 http://www.comp.nus.edu.sg/˜nlp/ conll13st.html 4 http://www.comp.nus.edu.sg/˜nlp/ conll14st.html the full generation task is still an open research area, but a subsequent human evaluation ranked the output from the CoNLL-2014 Shared Task systems (Napoles et al., 2015). The system of Felice et al. (2014) ranked highest, utilizing a combination of a rule-based system and phrase-based MT, with re-ranking via a large web-scale language model. Of the non-MT based approaches, the IllinoisColumbia system was a strong performer, combining several classifiers trained for specific types of errors (Rozovskaya et al., 2014). 4 Models We use an end-to-end approach that does not have separate components for candidate generation or reranking that make use of hand-tuned rules or explicit syntax, nor do we employ separate classifiers for human-differentiated subsets of errors, unlike some previous work for the related task of grammatical error correction. We next introduce two approaches for the task of sentence-level grammatical error identification: A binary classifier and a sequence-to-sequence model that is trained for correction but can also be used for identification as a side-effect. 4.1 Baseline Convolutional"
W16-0708,P14-1005,0,0.0437352,"Missing"
W16-0708,D13-1203,0,0.403498,"gest some directions for further improvement. 1 Introduction Most recent approaches to identity coreference resolution rely on a set of pipelined features generated by relatively accurate upstream systems. For instance, the CoNLL 2012 coreference datasets (Pradhan et al., 2012), which are based on the OntoNotes corpus (Hovy et al., 2006), make available both gold and predicted parse, part-of-speech, and namedentity information for each sentence in the corpus. While recent systems have managed to improve on the state of the art in coreference resolution by taking advantage of such information (Durrett and Klein, 2013; Wiseman et al., 2015; Bj¨orkelund and Kuhn, 2014; Fernandes et al., 2012; Martschat and Strube, 2015), we might be interested in systems that do not use pipelined features for several reasons: first, pipelined systems are known to accumulate errors throughout the stages of the pipeline. Second, unpipelined models do not need to contend with the intricacies of the various systems in the pipeline, Accordingly, in this paper we consider systems that attempt to move beyond OntoNotes by making coreference predictions without access to pipelined features, using only a document’s words and sentence"
W16-0708,N06-2015,0,0.0359464,"upstream models, and because we might expect them to generalize better to situations in which upstream features are unavailable or unreliable. Through quantitative and qualitative error analysis we identify what sorts of cases are particularly difficult for such models, and suggest some directions for further improvement. 1 Introduction Most recent approaches to identity coreference resolution rely on a set of pipelined features generated by relatively accurate upstream systems. For instance, the CoNLL 2012 coreference datasets (Pradhan et al., 2012), which are based on the OntoNotes corpus (Hovy et al., 2006), make available both gold and predicted parse, part-of-speech, and namedentity information for each sentence in the corpus. While recent systems have managed to improve on the state of the art in coreference resolution by taking advantage of such information (Durrett and Klein, 2013; Wiseman et al., 2015; Bj¨orkelund and Kuhn, 2014; Fernandes et al., 2012; Martschat and Strube, 2015), we might be interested in systems that do not use pipelined features for several reasons: first, pipelined systems are known to accumulate errors throughout the stages of the pipeline. Second, unpipelined models"
W16-0708,D14-1181,0,0.00466964,"Missing"
W16-0708,J13-4004,0,0.0210064,"ent antecedents. We will moreover require that in making these antecedent predictions no pipelined features are used. In particular, we will assume that “unpipelined” systems have access only to a document’s mention-boundaries, to the sets C(x) for each x ∈ X (when training), to the words in each document, and to the document’s sentence boundaries. Whereas recent coreference systems typically make use of syntactic information, named-entity tags, word-lists containing type information (e.g., number, gender, animacy), and speaker information (Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014; Lee et al., 2013), given the aforementioned restrictions, the only common coreference features that remain legal are word-based features and “distance” features. Distance features are typically defined in terms of the number of words, mentions, or sentences between a mention and a candidate antecedent (Durrett and Klein, 2013), and such features can presumably be defined accurately in many settings without the use of upstream systems. 54 2 Models We will use a very simple mention-ranking style model for our antecedent prediction. Mentionranking models make use of a scoring function s(x, y) that scores the comp"
W16-0708,Q15-1029,0,0.0136016,"eference resolution rely on a set of pipelined features generated by relatively accurate upstream systems. For instance, the CoNLL 2012 coreference datasets (Pradhan et al., 2012), which are based on the OntoNotes corpus (Hovy et al., 2006), make available both gold and predicted parse, part-of-speech, and namedentity information for each sentence in the corpus. While recent systems have managed to improve on the state of the art in coreference resolution by taking advantage of such information (Durrett and Klein, 2013; Wiseman et al., 2015; Bj¨orkelund and Kuhn, 2014; Fernandes et al., 2012; Martschat and Strube, 2015), we might be interested in systems that do not use pipelined features for several reasons: first, pipelined systems are known to accumulate errors throughout the stages of the pipeline. Second, unpipelined models do not need to contend with the intricacies of the various systems in the pipeline, Accordingly, in this paper we consider systems that attempt to move beyond OntoNotes by making coreference predictions without access to pipelined features, using only a document’s words and sentence boundaries. In the hopes of shedding light on whether this is a viable strategy, we consider, as a cas"
W16-0708,W12-4501,0,0.0254827,"eresting because they allow for side-stepping the intricacies of upstream models, and because we might expect them to generalize better to situations in which upstream features are unavailable or unreliable. Through quantitative and qualitative error analysis we identify what sorts of cases are particularly difficult for such models, and suggest some directions for further improvement. 1 Introduction Most recent approaches to identity coreference resolution rely on a set of pipelined features generated by relatively accurate upstream systems. For instance, the CoNLL 2012 coreference datasets (Pradhan et al., 2012), which are based on the OntoNotes corpus (Hovy et al., 2006), make available both gold and predicted parse, part-of-speech, and namedentity information for each sentence in the corpus. While recent systems have managed to improve on the state of the art in coreference resolution by taking advantage of such information (Durrett and Klein, 2013; Wiseman et al., 2015; Bj¨orkelund and Kuhn, 2014; Fernandes et al., 2012; Martschat and Strube, 2015), we might be interested in systems that do not use pipelined features for several reasons: first, pipelined systems are known to accumulate errors thro"
W16-0708,P15-1137,1,0.780594,"further improvement. 1 Introduction Most recent approaches to identity coreference resolution rely on a set of pipelined features generated by relatively accurate upstream systems. For instance, the CoNLL 2012 coreference datasets (Pradhan et al., 2012), which are based on the OntoNotes corpus (Hovy et al., 2006), make available both gold and predicted parse, part-of-speech, and namedentity information for each sentence in the corpus. While recent systems have managed to improve on the state of the art in coreference resolution by taking advantage of such information (Durrett and Klein, 2013; Wiseman et al., 2015; Bj¨orkelund and Kuhn, 2014; Fernandes et al., 2012; Martschat and Strube, 2015), we might be interested in systems that do not use pipelined features for several reasons: first, pipelined systems are known to accumulate errors throughout the stages of the pipeline. Second, unpipelined models do not need to contend with the intricacies of the various systems in the pipeline, Accordingly, in this paper we consider systems that attempt to move beyond OntoNotes by making coreference predictions without access to pipelined features, using only a document’s words and sentence boundaries. In the ho"
W16-0708,W12-4502,0,\N,Missing
W17-4505,D16-1147,0,0.0944574,"been proposed in the literature to efficiently handle the problem of large inputs to deep neural networks. One particular framework is that of “conditional computation”, as coined by Bengio et al. (2013) — the idea is to only compute a subset of a network’s units for a given input by gating different parts of the network. Several methods, some stochastic and some deterministic, have been explored in the vein of conditional computation. In this work, we will focus on stochastic methods, although deterministic methods are worth considering as future work (Rae et al., 2016; Shazeer et al., 2017; Miller et al., 2016; Martins and Astudillo, 2016). On the stochastic front, Xu et al. (2015) demonstrate the effectiveness of “hard” attention. While standard “soft” attention averages the representations of where the model attends to, hard attention discretely selects a single location. Hard attention has been successfully applied in various computer vision tasks (Mnih et al., 2014; Ba et al., 2015), but so far has limited usage in NLP. We will apply hard attention to the document summarization task by sparsifying our reading of the source text. 3 p(yt |yt−1 , . . . , y1 , [h1 , . . . , hn ]) = softmax(Wout ct"
W17-4505,P16-1188,0,0.0158265,"inal word generator layer to regularize (with dropout probability 0.3). At test time, we run beam search to produce the summary with a beam size of 5. Our models are implemented using Torch based on a past version of the OpenNMT system4 (Klein et al., 2017). We ran our experiments on a 12GB Geforce GTX Titan X GPU. The models take between 2-2.5 hours to train per epoch. Models Baselines We consider a few baseline models. A strong and simple baseline is the first sentence of the document, which we denote F IRST. We also consider the integer linear programming (ILP) based document summarizer of Durrett et al. (2016). We apply the code 2 directly on the test set without retraining the system. We provide the necessary preprocessing using the Berkeley coreference system3 . We call this baseline ILP. Our models We ran experiments with the models S TANDARD, H IER, and C2F as described above. For the coarse attention representations hsi of H IER and C2F, we experiment with convolutional and bag of words encodings. We use convolutions for the top-level representations by default, where we follow Kim (2014) and perform a convolution over each window of words in the chunk using 600 filters of kernel width 6. We u"
W17-4505,K16-1028,0,0.0538341,"Missing"
W17-4505,D14-1181,0,0.00443471,"note F IRST. We also consider the integer linear programming (ILP) based document summarizer of Durrett et al. (2016). We apply the code 2 directly on the test set without retraining the system. We provide the necessary preprocessing using the Berkeley coreference system3 . We call this baseline ILP. Our models We ran experiments with the models S TANDARD, H IER, and C2F as described above. For the coarse attention representations hsi of H IER and C2F, we experiment with convolutional and bag of words encodings. We use convolutions for the top-level representations by default, where we follow Kim (2014) and perform a convolution over each window of words in the chunk using 600 filters of kernel width 6. We use maxover-time pooling to obtain a fixed-dimensional top-level representation in Rdf where df = 600 is the number of filters. For bag of words, we simply take the top-level representation as the sum of 2 3 Training 5.5 Evaluation We report metrics for perplexity and ROUGE balanced F-scores (Lin, 2004) on the test set. https://github.com/gregdurrett/berkeley-doc-summarizer https://github.com/gregdurrett/berkeley-entity 4 37 http://opennmt.net With multiple gold summaries in the CNN/Dailym"
W17-4505,D15-1044,1,0.656794,"pirically, we find that while coarse-tofine attention models lag behind state-ofthe-art baselines, our method achieves the desired behavior of sparsely attending to subsets of the document for generation. 1 Introduction The sequence-to-sequence architecture of Sutskever et al. (2014), also known as the encoder-decoder architecture, is now the gold standard for many NLP tasks, including machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), question answering (Hermann et al., 2015), dialogue (Li et al., 2016), caption generation (Xu et al., 2015), and in particular summarization (Rush et al., 2015). A popular variant of sequence-to-sequence models are attention models (Bahdanau et al., 2015). By keeping an encoded representation of each part of the input, we “attend” to the relevant part each time we produce an output from the decoder. In practice, this means computing attention 2 Related Work In summarization, neural attention models were first applied by Rush et al. (2015) to do headline 33 Proceedings of the Workshop on New Frontiers in Summarization, pages 33–42 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics time step t with ht = fenc (xt ,"
W17-4505,P17-4012,1,0.0698497,"e word embeddings with 300dimensional word2vec embeddings (Mikolov and Dean, 2013). We initialize all other parameters as uniform in the interval [−0.1, 0.1]. For convolutional layers, we use a kernel width of 6 and 600 filters. Positional embeddings have dimension 25. We use dropout (Srivastava et al., 2014) between stacked LSTM hidden states and before the final word generator layer to regularize (with dropout probability 0.3). At test time, we run beam search to produce the summary with a beam size of 5. Our models are implemented using Torch based on a past version of the OpenNMT system4 (Klein et al., 2017). We ran our experiments on a 12GB Geforce GTX Titan X GPU. The models take between 2-2.5 hours to train per epoch. Models Baselines We consider a few baseline models. A strong and simple baseline is the first sentence of the document, which we denote F IRST. We also consider the integer linear programming (ILP) based document summarizer of Durrett et al. (2016). We apply the code 2 directly on the test set without retraining the system. We provide the necessary preprocessing using the Berkeley coreference system3 . We call this baseline ILP. Our models We ran experiments with the models S TAN"
W17-4505,P16-1094,0,0.0204647,"ethod scales with the number of top-level chunks and can handle much longer sequences. Empirically, we find that while coarse-tofine attention models lag behind state-ofthe-art baselines, our method achieves the desired behavior of sparsely attending to subsets of the document for generation. 1 Introduction The sequence-to-sequence architecture of Sutskever et al. (2014), also known as the encoder-decoder architecture, is now the gold standard for many NLP tasks, including machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), question answering (Hermann et al., 2015), dialogue (Li et al., 2016), caption generation (Xu et al., 2015), and in particular summarization (Rush et al., 2015). A popular variant of sequence-to-sequence models are attention models (Bahdanau et al., 2015). By keeping an encoded representation of each part of the input, we “attend” to the relevant part each time we produce an output from the decoder. In practice, this means computing attention 2 Related Work In summarization, neural attention models were first applied by Rush et al. (2015) to do headline 33 Proceedings of the Workshop on New Frontiers in Summarization, pages 33–42 c Copenhagen, Denmark, Septembe"
W17-4505,P17-1099,0,0.155313,"roduced at each time step using an attention function a that takes the encoded hidden states [h1 , . . . , hn ] and the current decoder hidden state hdec t and produces the d ctx context ct ∈ R : ct = a([h1 , . . . , hn ], hdec t ). As in Luong et al. (2015), we feed the context vector at time t−1 back into the decoder RNN at time t, i.e. hdec = fdec ([yt , ct−1 ], hdec t t−1 ). Finally, a linear projection and softmax (the generator) produces a distribution over output words yt ∈ V: generation, i.e. produce a title for a news article given only the first sentence. Nallapati et al. (2016) and See et al. (2017) apply attention models to summarize full documents, achieving stateof-the-art results on the CNN/Dailymail dataset. All of these models, however, suffer from the inherent complexity of attention over the full document. Indeed, See et al. (2017) report that a single model takes over 3 days to train. Many techniques have been proposed in the literature to efficiently handle the problem of large inputs to deep neural networks. One particular framework is that of “conditional computation”, as coined by Bengio et al. (2013) — the idea is to only compute a subset of a network’s units for a given in"
W17-4505,W04-1013,0,0.0197821,"Missing"
W17-4505,D15-1166,0,0.053662,"Missing"
W17-4505,N16-1086,0,0.085044,", giving total reward Ts=t γ s−t rs for the stochastic hard attention node at . We found Coarse-to-Fine Attention With the previous models S TANDARD and H IER, we are required to compute hidden states over all words and top-level chunks in the document, so that if we have M chunks and N words per chunk, the computational complexity is O(M N ) for each attention step. However, if we are able to perform conditional computation and only read M + of the chunks at a time, we can reduce the attention complexity to 1 The term coarse-to-fine attention has previously been introduced in the literature (Mei et al., 2016). However, their idea is different: they use coarse attention to reweight the fine attention computed over the entire input. This idea has also been called hierarchical attention (Nallapati et al., 2016). 35 + British worker contracts ... A British military health ... A British military health care worker in Sierra Leone ... ... ... &lt;s> British worker Figure 1: Model architecture for sequence-to-sequence with coarse-to-fine attention. The left side is the encoder that reads the document, and the right side is the decoder that produces the output sequence. On the encoder side, the top-level hid"
W18-1817,D17-1151,0,0.135168,"ston, March 17 - 21, 2018 |Page 180 System BLEU-cased uedin-nmt-ensemble LMU-nmt-reranked-wmt17-en-de SYSTRAN-single (OpenNMT) 28.3 27.1 26.7 Table 1: Top 3 on English-German newstest2017 WMT17. System Nematus ONMT Speed tok/sec Train Trans BLEU System newstest14 newstest17 3221 5254 18.25 19.34 seq2seq Sockeye ONMT 22.19 23.23 [19.34] 25.55 25.06 [22.69] 252 457 Table 2: Performance results for EN→DE on WMT15 tested on newstest2014. Both systems 2x500 RNN, embedding size 300, 13 epochs, batch size 64, beam size 5. We compare on a 32k BPE setting. Table 3: OpenNMT’s performance as reported by Britz et al. (2017) and Hieber et al. (2017) (bracketed) compared to our best results. ONMT used 32k BPE, 2-layers bi-RNN of 1024, embedding size 512, dropout 0.1 and max length 100. Extensible Data, Models, and Search In addition to plain text, OpenNMT also supports different input types including models with discrete features (Sennrich and Haddow, 2016), models with non-sequential input such as tables, continuous data such as speech signals, and multi-dimensional data such as images. To support these different input modalities the library implements image encoder (Xu et al., 2015; Deng et al., 2017) and audio"
W18-1817,D14-1179,0,0.0444157,"Missing"
W18-1817,N16-1012,1,0.881204,"Missing"
W18-1817,P16-5005,0,0.0299161,"ce and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques. OpenNMT has been used in several production MT systems, modiﬁed for numerous research papers, and is implemented across several deep learning frameworks. 1 Introduction Neural machine translation (NMT) is a new methodology for machine translation that has led to remarkable improvements, particularly in terms of human evaluation, compared to rule-based and statistical machine translation (SMT) systems (Wu et al., 2016; Crego et al., 2016). Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015a), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT"
W18-1817,P10-4002,0,0.0336078,"and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015a), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT community beneﬁted greatly from toolkits like Moses (Koehn et al., 2007) for phrase-based SMT and CDec (Dyer et al., 2010) for syntax-based SMT, NMT toolkits can provide a foundation to build upon. A toolkit should aim to provide a shared framework for developing and comparing open-source systems, while at the same time being efﬁcient and accurate enough to be used in production contexts. With these goals in mind, in this work we present an open-source toolkit for developing neural machine translation systems, known as OpenNMT (http://opennmt.net). Since its launch in December 2016, OpenNMT has become a collection of implementations targeting both academia and industry. The system is designed to be simple to use"
W18-1817,W17-3518,0,0.0185142,"7), data-toProceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 181 System GNMT 4 layers GNMT 8 layers WMT reference ONMT newstest14 newstest15 23.7 24.4 20.6 23.2 26.5 27.6 24.9 26.0 Table 4: Comparison with GNMT on EN→DE. ONMT used 2-layers bi-RNN of 1024, embedding size 512, dropout 0.1 and max length 100. System T2T ONMT T2T GNMT (rnn) ONMT (rnn) newstest14 newstest17 27.3 26.8 24.6 23.2 27.8 28.0 25.1 Table 5: Transformer Results on English-German newstest14 and newstest17. We use 6-layer transformer with model size of 512. document (Wiseman et al., 2017; Gardent et al., 2017), and transliteration (Ameur et al., 2017), to name a few of many applications. Additional Tools OpenNMT packages several additional tools, including: 1) reversible tokenizer, which can also perform Byte Pair Encoding (BPE) (Sennrich et al., 2015); 2) loading and exporting word embeddings; 3) translation server which enables showcase results remotely; and 4) visualization tools for debugging or understanding, such as beam search visualization, proﬁler and TensorBoard logging. 5 Experiments OpenNMT achieves competitive results against other systems, e.g. in the recent WMT 2017 translation task,"
W18-1817,P16-1154,0,0.0123343,"g models with discrete features (Sennrich and Haddow, 2016), models with non-sequential input such as tables, continuous data such as speech signals, and multi-dimensional data such as images. To support these different input modalities the library implements image encoder (Xu et al., 2015; Deng et al., 2017) and audio encoders (Chan et al., 2015). OpenNMT implements various attention types including general, dot product, and concatenation (Luong et al., 2015a; Britz et al., 2017). This also includes recent extensions to these standard modules such as the copy mechanism (Vinyals et al., 2015; Gu et al., 2016), which is widely used in summarization and generation applications. The newer implementations of OpenNMT have also been updated to include support for recent innovations in non-recurrent translation models. In particular recent support has been added for convolution translation (Gehring et al., 2017) and the attention-only transformer network (Vaswani et al., 2017). Finally, the translation code allows for user customization. In addition to out-of-vocabulary (OOV) handling (Luong et al., 2015b), OpenNMT also allows beam search with various normalizations including length and attention coverag"
W18-1817,E17-3017,0,0.116372,"8 |Page 180 System BLEU-cased uedin-nmt-ensemble LMU-nmt-reranked-wmt17-en-de SYSTRAN-single (OpenNMT) 28.3 27.1 26.7 Table 1: Top 3 on English-German newstest2017 WMT17. System Nematus ONMT Speed tok/sec Train Trans BLEU System newstest14 newstest17 3221 5254 18.25 19.34 seq2seq Sockeye ONMT 22.19 23.23 [19.34] 25.55 25.06 [22.69] 252 457 Table 2: Performance results for EN→DE on WMT15 tested on newstest2014. Both systems 2x500 RNN, embedding size 300, 13 epochs, batch size 64, beam size 5. We compare on a 32k BPE setting. Table 3: OpenNMT’s performance as reported by Britz et al. (2017) and Hieber et al. (2017) (bracketed) compared to our best results. ONMT used 32k BPE, 2-layers bi-RNN of 1024, embedding size 512, dropout 0.1 and max length 100. Extensible Data, Models, and Search In addition to plain text, OpenNMT also supports different input types including models with discrete features (Sennrich and Haddow, 2016), models with non-sequential input such as tables, continuous data such as speech signals, and multi-dimensional data such as images. To support these different input modalities the library implements image encoder (Xu et al., 2015; Deng et al., 2017) and audio encoders (Chan et al., 20"
W18-1817,P07-2045,0,0.0157885,"models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015a), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT community beneﬁted greatly from toolkits like Moses (Koehn et al., 2007) for phrase-based SMT and CDec (Dyer et al., 2010) for syntax-based SMT, NMT toolkits can provide a foundation to build upon. A toolkit should aim to provide a shared framework for developing and comparing open-source systems, while at the same time being efﬁcient and accurate enough to be used in production contexts. With these goals in mind, in this work we present an open-source toolkit for developing neural machine translation systems, known as OpenNMT (http://opennmt.net). Since its launch in December 2016, OpenNMT has become a collection of implementations targeting both academia and ind"
W18-1817,W17-4505,1,0.845813,"016) and structured attention (Kim et al., 2017) with minimal change of code. As another example, in order to get feature-based factored neural translation (Sennrich and Haddow, 2016) we simply need to modify the input network to process the feature-based representation, and the output network to produce multiple conditionally independent predictions. We have seen instances of this use in published research. In addition to machine translation (Levin et al., 2017; Ha et al., 2017; Ma et al., 2017), researchers have employed OpenNMT for parsing (van Noord and Bos, 2017), document summarization (Ling and Rush, 2017), data-toProceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 181 System GNMT 4 layers GNMT 8 layers WMT reference ONMT newstest14 newstest15 23.7 24.4 20.6 23.2 26.5 27.6 24.9 26.0 Table 4: Comparison with GNMT on EN→DE. ONMT used 2-layers bi-RNN of 1024, embedding size 512, dropout 0.1 and max length 100. System T2T ONMT T2T GNMT (rnn) ONMT (rnn) newstest14 newstest17 27.3 26.8 24.6 23.2 27.8 28.0 25.1 Table 5: Transformer Results on English-German newstest14 and newstest17. We use 6-layer transformer with model size of 512. document (Wiseman et al., 2017; Ga"
W18-1817,D15-1166,0,0.827632,"ed in several production MT systems, modiﬁed for numerous research papers, and is implemented across several deep learning frameworks. 1 Introduction Neural machine translation (NMT) is a new methodology for machine translation that has led to remarkable improvements, particularly in terms of human evaluation, compared to rule-based and statistical machine translation (SMT) systems (Wu et al., 2016; Crego et al., 2016). Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015a), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT community beneﬁted greatly from toolkits like Moses (Koehn et al., 2007) for phrase-based SMT and CDec (Dyer et al., 2010) for syntax-based SMT, NMT toolkits can provide a foundation to build upo"
W18-1817,W17-4751,0,0.016057,"tion module, we can implement local attention (Luong et al., 2015a), sparse-max attention (Martins and Astudillo, 2016) and structured attention (Kim et al., 2017) with minimal change of code. As another example, in order to get feature-based factored neural translation (Sennrich and Haddow, 2016) we simply need to modify the input network to process the feature-based representation, and the output network to produce multiple conditionally independent predictions. We have seen instances of this use in published research. In addition to machine translation (Levin et al., 2017; Ha et al., 2017; Ma et al., 2017), researchers have employed OpenNMT for parsing (van Noord and Bos, 2017), document summarization (Ling and Rush, 2017), data-toProceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 181 System GNMT 4 layers GNMT 8 layers WMT reference ONMT newstest14 newstest15 23.7 24.4 20.6 23.2 26.5 27.6 24.9 26.0 Table 4: Comparison with GNMT on EN→DE. ONMT used 2-layers bi-RNN of 1024, embedding size 512, dropout 0.1 and max length 100. System T2T ONMT T2T GNMT (rnn) ONMT (rnn) newstest14 newstest17 27.3 26.8 24.6 23.2 27.8 28.0 25.1 Table 5: Transformer Results on English-"
W18-1817,W16-2209,0,0.108277,"19.34] 25.55 25.06 [22.69] 252 457 Table 2: Performance results for EN→DE on WMT15 tested on newstest2014. Both systems 2x500 RNN, embedding size 300, 13 epochs, batch size 64, beam size 5. We compare on a 32k BPE setting. Table 3: OpenNMT’s performance as reported by Britz et al. (2017) and Hieber et al. (2017) (bracketed) compared to our best results. ONMT used 32k BPE, 2-layers bi-RNN of 1024, embedding size 512, dropout 0.1 and max length 100. Extensible Data, Models, and Search In addition to plain text, OpenNMT also supports different input types including models with discrete features (Sennrich and Haddow, 2016), models with non-sequential input such as tables, continuous data such as speech signals, and multi-dimensional data such as images. To support these different input modalities the library implements image encoder (Xu et al., 2015; Deng et al., 2017) and audio encoders (Chan et al., 2015). OpenNMT implements various attention types including general, dot product, and concatenation (Luong et al., 2015a; Britz et al., 2017). This also includes recent extensions to these standard modules such as the copy mechanism (Vinyals et al., 2015; Gu et al., 2016), which is widely used in summarization and"
W18-1817,D17-1239,1,0.846882,"on (Ling and Rush, 2017), data-toProceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 181 System GNMT 4 layers GNMT 8 layers WMT reference ONMT newstest14 newstest15 23.7 24.4 20.6 23.2 26.5 27.6 24.9 26.0 Table 4: Comparison with GNMT on EN→DE. ONMT used 2-layers bi-RNN of 1024, embedding size 512, dropout 0.1 and max length 100. System T2T ONMT T2T GNMT (rnn) ONMT (rnn) newstest14 newstest17 27.3 26.8 24.6 23.2 27.8 28.0 25.1 Table 5: Transformer Results on English-German newstest14 and newstest17. We use 6-layer transformer with model size of 512. document (Wiseman et al., 2017; Gardent et al., 2017), and transliteration (Ameur et al., 2017), to name a few of many applications. Additional Tools OpenNMT packages several additional tools, including: 1) reversible tokenizer, which can also perform Byte Pair Encoding (BPE) (Sennrich et al., 2015); 2) loading and exporting word embeddings; 3) translation server which enables showcase results remotely; and 4) visualization tools for debugging or understanding, such as beam search visualization, proﬁler and TensorBoard logging. 5 Experiments OpenNMT achieves competitive results against other systems, e.g. in the recent WMT"
W18-2509,D17-1151,0,0.017718,"nnected feed-forward network. def subsequent_mask(size): &quot;Mask out subsequent positions.&quot; attn_shape = (1, size, size) subsequent_mask = np.triu(np.ones(attn_shape), k=1) return torch.from_numpy( subsequent_mask.astype(&apos;uint8&apos;)) == 0 54 3.1.3 two are similar in theoretical complexity, dotproduct attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk (Britz et al., 2017). We suspect that for large values of dk , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients (To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q · k = d ∑i=k 1 qi k i , has mean 0 and variance dk .). To counteract this effect, we scale the dot products by √1d . Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and"
W18-2715,P17-2091,0,0.201752,"print, we applied basic optimization techniques to reduce the final size of our models. Our strategy for the shared task was to take advantage of four main optimization techniques: (a) sequence-level distillation, in particular cross-class distillation from a transformer model (Vaswani et al., 2017) to an RNN, (b) architecture search, changing the structure of the network by increasing the size of the most efficient modules, reducing the size of the most costly modules and replacing default gated units, (c) specialized precomputation such as reducing dynamically the runtime target vocabulary (Shi and Knight, 2017), and (d) quantization and faster matrix operations, based on the work of Devlin (2017) and gemmlowp2 . All of these methods are employed in a special-purpose C++-based decoder CTranslate3 . The complete training workflow including data preparation and distillation is described in Section 2. Inference techniques and quantization are described in Section 3. Our experiments compare the different approaches in terms of speed and accuracy. A meta Introduction As neural machine translation becomes more widely deployed in production environments, it becomes also increasingly important to serve trans"
W18-2715,W18-2701,0,0.0219747,"a preparation and distillation is described in Section 2. Inference techniques and quantization are described in Section 3. Our experiments compare the different approaches in terms of speed and accuracy. A meta Introduction As neural machine translation becomes more widely deployed in production environments, it becomes also increasingly important to serve translations models in a way as fast and as memory-efficient as possible, both on dedicated GPU and on standard CPU hardwares. The WNMT 2018 shared task1 focused on comparing different systems on both accuracy and computational efficiency (Birch et al., 2018). This paper describes the entry for the OpenNMT system to this competition. Our specific interest was to explore the different techniques for training and optimizing CPU models for very high throughput while preserving highest possible accuracy compared to state-of-the-art. While we did not put real focus on memory and docker size foot2 1 https://sites.google.com/site/wnmt18/ shared-task 3 https://github.com/google/gemmlowp https://github.com/OpenNMT/CTranslate 122 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 122–128 c Melbourne, Australia, July 20, 2018"
W18-2715,D17-1300,0,0.0327723,"ategy for the shared task was to take advantage of four main optimization techniques: (a) sequence-level distillation, in particular cross-class distillation from a transformer model (Vaswani et al., 2017) to an RNN, (b) architecture search, changing the structure of the network by increasing the size of the most efficient modules, reducing the size of the most costly modules and replacing default gated units, (c) specialized precomputation such as reducing dynamically the runtime target vocabulary (Shi and Knight, 2017), and (d) quantization and faster matrix operations, based on the work of Devlin (2017) and gemmlowp2 . All of these methods are employed in a special-purpose C++-based decoder CTranslate3 . The complete training workflow including data preparation and distillation is described in Section 2. Inference techniques and quantization are described in Section 3. Our experiments compare the different approaches in terms of speed and accuracy. A meta Introduction As neural machine translation becomes more widely deployed in production environments, it becomes also increasingly important to serve translations models in a way as fast and as memory-efficient as possible, both on dedicated"
W18-2715,D16-1139,1,0.873957,"uccessful for reducing the size of neural models. We considered the transformer network as our teacher network. We used OpenNMT-tf 6 to train two transformer based systems: base and large described in Table 2 with their evaluation in Table 3. For both, the learning rate is set to 2.0 and warmup steps 8000, we average the last 8 checkpoints to get the final model. Our baseline system outperforms the provided baseline Sockeye model by +0.37 BLEU on newstest2014. 2.2 Distillation to RNN To train our smaller student system, we follow the sequence-level knowledge distillation approach described by Kim and Rush (2016). First, we build the full transformer as above. Next, we use the teacher system to retranslate all the training source sentences to generate a set of simplified target sentences. Then, we use this simplified corpus (original source and newly generated target) to train a student system. The student system can be assigned with smaller network size, in our case a RNN-based sequence-to-sequence model similar to Bahdanau et al. (2014) Results from Crego and Senellart (2016) show that the distillation process not only improves the throughput of the student models and reduce their size, but can also"
W18-2715,P17-4012,1,0.866484,"Missing"
W18-2715,P07-2045,0,0.0250883,"Missing"
W18-2715,D15-1166,0,0.146064,"Missing"
W18-2715,N18-2074,0,0.0351052,"d Senellart (2016) who reported that student models could outperform their teacher for reference RNN-based model. (b) We compare quantitatively different quantizations, and (c) we give an improved algorithm to dynamically select target vocabulary for a given batch. Finally, we also report several complementary experiments that resulted in systems inside of the pareto convex border. For instance, we compare using 8-bit quantization to 16-bit quantization. 2 Teacher Model: Transformer Transformer networks (Vaswani et al., 2017) are the current state-of-the art in many machine translation tasks (Shaw et al., 2018). The network directly models the representations of each sentence with a self-attention mechanism. Hence much longer term dependencies than with standard sequence-to-sequence models can be learned, which is especially important for language pairs like English-German. In addition, transformer allows to easily parallelise the MLE training process across multiple GPUs. However, a large number of parameters are needed by the network to obtain its best performance. In order to reduce the model size, we applied knowledge distillation, a technique that has proven successful for reducing the size of"
W18-5451,P17-4012,1,0.770469,"Samples: S EQ 2S EQ V IS connects the encoder and decoder of a seq2seq model to relevant training examples by showing a neighborhood of examples with the most similar internal states. Test Alternative Decisions: S EQ 2S EQ V IS enables ”what if” explorations and causal relationship testing by manipulation of inputs, attention, and outputs. The full system is shown in Figure 1. It combines visualizations for the external components with internal representations from specific examples and nearest-neighbor lookups over a corpus of precomputed examples. The entire system integrates with OpenNMT (Klein et al., 2017), one of the largest open source seq2seq libraries. Neural attention-based sequence-to-sequence models (seq2seq) (Sutskever et al., 2014; Bahdanau et al., 2014) have proven to be accurate and robust for many sequence prediction tasks. They have become the standard approach for automatic translation of text, at the cost of increased model complexity and uncertainty. End-to-end trained neural models act as a black box, which makes it difficult to examine model decisions and attribute errors to a specific part of a model. The highly connected and high-dimensional internal representations pose a c"
W18-5451,2012.eamt-1.60,0,0.0121955,"h an encoder or decoder misrepresent a word within a given context (2) alignment errors, in which the attention focuses on the wrong word, and (3) decoding errors, in which the prediction assigns a wrong probability distribution over words, or the beam search fails to include the correct solution. We define three steps within an analysis that aim to understand the prediction process, understand 2 Debugging Use Case This case study follows the example in Figure 1 and involves a model trainer (Strobelt et al., 2018b) who is building a German-toEnglish translation model on the IWSLT ’14 dataset (Mauro et al., 2012)). The user observes that a specific example was mistranslated. She finds the source sentence: Die l¨angsten Reisen fangen an, wenn es auf den Straßen dunkel wird. The correct translation for this sentence is The longest journeys begin, when it gets dark in the streets. The model produces the mistranslation: The longest journey begins, when it gets to the streets. S EQ 2S EQ -V IS shows the tokenized input sentence in blue and the corresponding translation of the model in yellow (on the top). The user observes that the model does not translate the word dunkel into dark. This mistake exemplifie"
W18-5451,W18-5451,1,0.0512899,"errors within each translation stage into the following categories: (1) representation errors, in which an encoder or decoder misrepresent a word within a given context (2) alignment errors, in which the attention focuses on the wrong word, and (3) decoding errors, in which the prediction assigns a wrong probability distribution over words, or the beam search fails to include the correct solution. We define three steps within an analysis that aim to understand the prediction process, understand 2 Debugging Use Case This case study follows the example in Figure 1 and involves a model trainer (Strobelt et al., 2018b) who is building a German-toEnglish translation model on the IWSLT ’14 dataset (Mauro et al., 2012)). The user observes that a specific example was mistranslated. She finds the source sentence: Die l¨angsten Reisen fangen an, wenn es auf den Straßen dunkel wird. The correct translation for this sentence is The longest journeys begin, when it gets dark in the streets. The model produces the mistranslation: The longest journey begins, when it gets to the streets. S EQ 2S EQ -V IS shows the tokenized input sentence in blue and the corresponding translation of the model in yellow (on the top). T"
W18-6505,P16-2008,0,0.123215,"Missing"
W18-6505,W17-3501,0,0.187272,"odel the hierarchical structure of discourse relations (Walker et al., 2007). Early data-driven approach used phrase-based language models for generation (Oh and Rudnicky, 2000; Mairesse and Young, 2014), or aimed to predict the best fitting cluster of semantically similar templates (Kondadadi et al., 2013). More recent work combines both steps by learning plan and realization jointly using end-to-end trained models (e.g. Wen et al., 2015). Several approaches have looked at generation from abstract meaning representations (AMR), and Peng et al. (2017) apply S2S models to the problem. However, Ferreira et al. (2017) show that S2S models are outperformed by 3 Background: Sequence-to-Sequence Generation We start by introducing the standard a text-totext problem and discuss how to map structured data into a sequential form. Let (x(0) , y(0) ), . . . (x(N ) , y(N ) ) ∈ (X , Y) be a set of N aligned source and target sequence pairs, with (x(i) , y(i) ) denoting the ith element in (X , Y) pairs. Further, let x = x1 , . . . , xm be the sequence of m tokens in the source, and y = y1 , . . . , yn the target sequence of length n. Let V be the vocabulary of possible tokens, and [n] the list of integers up to n, [1,"
W18-6505,D18-1426,0,0.182739,"ng data during the process of training the model itself, thus leading to models that follow distinct sentence templates. We show that this approach improves the quality of generated text, but also the robustness of the training process to outliers in the training data. Experiments are run on the E2E NLG challenge1 . We show that the application of this technique increases the quality of generated text across five different automated metrics (BLEU, NIST, METEOR, ROUGE, and CIDEr) over the multiple strong S2S baseline models (Duˇsek and Jurˇc´ıcˇ ek, 2016; Vaswani et al., 2017; Su et al., 2018; Freitag and Roy, 2018). Among 60 submissions to the challenge, our approach ranked first in METEOR, ROUGE, and CIDEr scores, third in BLEU, and sixth in NIST. 2 phrase-based machine translation models in small datasets. To address this issue, Konstas et al. (2017) propose a semi-supervised training method that can utilize English sentences outside of the training set to train parts of the model. We address the issue by using copy-attention to enable the model to copy words from the source, which helps to generate out of vocabulary and rare words. We note that end-to-end trained models, including our approach, often"
W18-6505,W14-3348,0,0.0241101,"vating the sMCL loss. All models are implemented in OpenNMTpy (Klein et al., 2017)2 . The parameters were found by grid search starting from the parameters used in the TGEN model by Duˇsek and Jurˇc´ıcˇ ek (2016). Unless stated otherwise, models do not block repeat sentence beginnings, since it results in worse performance in automated met2 Code and documentation can be found https://github.com/sebastianGehrmann/ diverse_ensembling rics. We show results on the multi-reference validation and the blind test sets for the five metrics BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), and CIDEr (Vedantam et al., 2015). 7 7.1 Results Results on the Validation Set Table 2 shows the results of different models on the validation set. During inference, we set the length penalty parameter α to 0.4, the coverage penalty parameter β to 0.1, and use beam search with a beam size of 10. Our models outperform all shown baselines, which represent all published results on this dataset to date. Except for the copyonly condition, the data-efficient dot outperforms mlp. Both copy-attention and diverse ensembling increase performance, and combining the two methods yields"
W18-6505,N18-1014,0,0.132817,"Missing"
W18-6505,W17-5525,0,0.128135,"Missing"
W18-6505,P17-4012,1,0.793655,"edding sizes of 750. During training, we apply dropout with probability 0.2 and train models with Adam (Kingma and Ba, 2014) and an initial learning rate of 0.002. We evaluate both mlp and dot attention types. The Transformer model has 4 layers with hidden and embedding sizes 512. We use the training rate schedule described by Vaswani et al. (2017), using Adam and a maximum learning rate of 0.1 after 2,000 warmup steps. The diverse ensembling technique is applied to all approaches, pre-training all models for 4 epochs and then activating the sMCL loss. All models are implemented in OpenNMTpy (Klein et al., 2017)2 . The parameters were found by grid search starting from the parameters used in the TGEN model by Duˇsek and Jurˇc´ıcˇ ek (2016). Unless stated otherwise, models do not block repeat sentence beginnings, since it results in worse performance in automated met2 Code and documentation can be found https://github.com/sebastianGehrmann/ diverse_ensembling rics. We show results on the multi-reference validation and the blind test sets for the five metrics BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), and CIDEr (Vedantam et al., 2015)."
W18-6505,W00-0306,0,0.439956,"most natural sounding sentence plans. Related Work Traditional approaches to natural language generation separate the generation of a sentence plan from the surface realization. First, an input is mapped into a format that represents the layout of the output sentence, for example, an adequate pre-defined template. Then, the surface realization transforms the intermediary structure into text (Stent et al., 2004). These representations often model the hierarchical structure of discourse relations (Walker et al., 2007). Early data-driven approach used phrase-based language models for generation (Oh and Rudnicky, 2000; Mairesse and Young, 2014), or aimed to predict the best fitting cluster of semantically similar templates (Kondadadi et al., 2013). More recent work combines both steps by learning plan and realization jointly using end-to-end trained models (e.g. Wen et al., 2015). Several approaches have looked at generation from abstract meaning representations (AMR), and Peng et al. (2017) apply S2S models to the problem. However, Ferreira et al. (2017) show that S2S models are outperformed by 3 Background: Sequence-to-Sequence Generation We start by introducing the standard a text-totext problem and dis"
W18-6505,P13-1138,0,0.36595,"f a sentence plan from the surface realization. First, an input is mapped into a format that represents the layout of the output sentence, for example, an adequate pre-defined template. Then, the surface realization transforms the intermediary structure into text (Stent et al., 2004). These representations often model the hierarchical structure of discourse relations (Walker et al., 2007). Early data-driven approach used phrase-based language models for generation (Oh and Rudnicky, 2000; Mairesse and Young, 2014), or aimed to predict the best fitting cluster of semantically similar templates (Kondadadi et al., 2013). More recent work combines both steps by learning plan and realization jointly using end-to-end trained models (e.g. Wen et al., 2015). Several approaches have looked at generation from abstract meaning representations (AMR), and Peng et al. (2017) apply S2S models to the problem. However, Ferreira et al. (2017) show that S2S models are outperformed by 3 Background: Sequence-to-Sequence Generation We start by introducing the standard a text-totext problem and discuss how to map structured data into a sequential form. Let (x(0) , y(0) ), . . . (x(N ) , y(N ) ) ∈ (X , Y) be a set of N aligned s"
W18-6505,P02-1040,0,0.101297,"aches, pre-training all models for 4 epochs and then activating the sMCL loss. All models are implemented in OpenNMTpy (Klein et al., 2017)2 . The parameters were found by grid search starting from the parameters used in the TGEN model by Duˇsek and Jurˇc´ıcˇ ek (2016). Unless stated otherwise, models do not block repeat sentence beginnings, since it results in worse performance in automated met2 Code and documentation can be found https://github.com/sebastianGehrmann/ diverse_ensembling rics. We show results on the multi-reference validation and the blind test sets for the five metrics BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), and CIDEr (Vedantam et al., 2015). 7 7.1 Results Results on the Validation Set Table 2 shows the results of different models on the validation set. During inference, we set the length penalty parameter α to 0.4, the coverage penalty parameter β to 0.1, and use beam search with a beam size of 10. Our models outperform all shown baselines, which represent all published results on this dataset to date. Except for the copyonly condition, the data-efficient dot outperforms mlp. Both copy-attention and diverse ensembli"
W18-6505,P17-1014,0,0.10113,"iers in the training data. Experiments are run on the E2E NLG challenge1 . We show that the application of this technique increases the quality of generated text across five different automated metrics (BLEU, NIST, METEOR, ROUGE, and CIDEr) over the multiple strong S2S baseline models (Duˇsek and Jurˇc´ıcˇ ek, 2016; Vaswani et al., 2017; Su et al., 2018; Freitag and Roy, 2018). Among 60 submissions to the challenge, our approach ranked first in METEOR, ROUGE, and CIDEr scores, third in BLEU, and sixth in NIST. 2 phrase-based machine translation models in small datasets. To address this issue, Konstas et al. (2017) propose a semi-supervised training method that can utilize English sentences outside of the training set to train parts of the model. We address the issue by using copy-attention to enable the model to copy words from the source, which helps to generate out of vocabulary and rare words. We note that end-to-end trained models, including our approach, often do not explicitly model the sentence planning stage, and are thus not directly comparable to previous work on sentence planning. This is especially limiting for generation of complex argument structures that rely on hierarchical structure. F"
W18-6505,C16-1105,0,0.0849185,"Missing"
W18-6505,E17-1035,0,0.0305046,"into text (Stent et al., 2004). These representations often model the hierarchical structure of discourse relations (Walker et al., 2007). Early data-driven approach used phrase-based language models for generation (Oh and Rudnicky, 2000; Mairesse and Young, 2014), or aimed to predict the best fitting cluster of semantically similar templates (Kondadadi et al., 2013). More recent work combines both steps by learning plan and realization jointly using end-to-end trained models (e.g. Wen et al., 2015). Several approaches have looked at generation from abstract meaning representations (AMR), and Peng et al. (2017) apply S2S models to the problem. However, Ferreira et al. (2017) show that S2S models are outperformed by 3 Background: Sequence-to-Sequence Generation We start by introducing the standard a text-totext problem and discuss how to map structured data into a sequential form. Let (x(0) , y(0) ), . . . (x(N ) , y(N ) ) ∈ (X , Y) be a set of N aligned source and target sequence pairs, with (x(i) , y(i) ) denoting the ith element in (X , Y) pairs. Further, let x = x1 , . . . , xm be the sequence of m tokens in the source, and y = y1 , . . . , yn the target sequence of length n. Let V be the vocabul"
W18-6505,W14-3301,0,0.0612896,"Missing"
W18-6505,P17-1099,0,0.391437,". In this work, we focus on the generation of language from meaning representations (MR), as shown in Figure 1. This task requires learning a semantic alignment from MR to utterance, wherein the MR can comprise a variable number of attributes. Recently, end-to-end generation has been handled primarily by Sequence-to-sequence (S2S) models (Sutskever et al., 2014; Bahdanau et al., 2014) that encode some information and decode it into a desired format. Extensions for summarization and other tasks have developed a mechanism to copy words from the input into a generated text (Vinyals et al., 2015; See et al., 2017). We begin with a strong S2S model with copymechanism for the E2E NLG task and include methods that can help to control the length of a generated text and how many inputs a model uses (Tu et al., 2016; Wu et al., 2016). Finally, Introduction Recent developments in end-to-end learning with neural networks have enabled methods to generate textual output from complex structured inputs such as images and tables. These methods may also enable the creation of text-generation models that are conditioned on multiple key-value attribute pairs. The conditional generation of fluent text poses multiple ch"
W18-6505,W04-1013,0,0.0172164,"e implemented in OpenNMTpy (Klein et al., 2017)2 . The parameters were found by grid search starting from the parameters used in the TGEN model by Duˇsek and Jurˇc´ıcˇ ek (2016). Unless stated otherwise, models do not block repeat sentence beginnings, since it results in worse performance in automated met2 Code and documentation can be found https://github.com/sebastianGehrmann/ diverse_ensembling rics. We show results on the multi-reference validation and the blind test sets for the five metrics BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), and CIDEr (Vedantam et al., 2015). 7 7.1 Results Results on the Validation Set Table 2 shows the results of different models on the validation set. During inference, we set the length penalty parameter α to 0.4, the coverage penalty parameter β to 0.1, and use beam search with a beam size of 10. Our models outperform all shown baselines, which represent all published results on this dataset to date. Except for the copyonly condition, the data-efficient dot outperforms mlp. Both copy-attention and diverse ensembling increase performance, and combining the two methods yields the highest BLEU a"
W18-6505,P04-1011,0,0.147254,"ss in which they use heuristics to filter a dataset to the most natural sounding examples according to a set of rules. Our work aims at the unsupervised segmentation of data such that one model learns the most natural sounding sentence plans. Related Work Traditional approaches to natural language generation separate the generation of a sentence plan from the surface realization. First, an input is mapped into a format that represents the layout of the output sentence, for example, an adequate pre-defined template. Then, the surface realization transforms the intermediary structure into text (Stent et al., 2004). These representations often model the hierarchical structure of discourse relations (Walker et al., 2007). Early data-driven approach used phrase-based language models for generation (Oh and Rudnicky, 2000; Mairesse and Young, 2014), or aimed to predict the best fitting cluster of semantically similar templates (Kondadadi et al., 2013). More recent work combines both steps by learning plan and realization jointly using end-to-end trained models (e.g. Wen et al., 2015). Several approaches have looked at generation from abstract meaning representations (AMR), and Peng et al. (2017) apply S2S m"
W18-6505,D15-1166,0,0.0745603,"he conditional probability of pθ (y|x). We assume that the target is generated from left to right, such that pθ (y|x) = Q n t=1 pθ (yt |y[t−1] , x), and that pθ (yt |y[t−1] , x) takes the form of an encoder-decoder architecture with attention. The training aims to maximize the log-likelihood of the observed training data. We evaluate the performance of both the LSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) architecture. We additionally experiment with two attention formulations. The first uses a dot-product between the hidden states of the encoder and decoder (Luong et al., 2015). The second uses a multi-layer perceptron with the hidden states as inputs (Bahdanau et al., 2014). We refer to them as dot and MLP respectively. Since dot attention does not require additional parameters, we hypothesize that it performs well in a limited data environment. In order to apply S2S models, a list of attributes in an MR has to be linearized into a sequence of tokens (Konstas et al., 2017; Ferreira et al., 2017). Not all attributes have to appear for all inputs, and each attribute might have multi-token values, such as area: city centre. We use special start and stop tokens for eac"
W18-6505,J14-4003,0,0.0231139,"entence plans. Related Work Traditional approaches to natural language generation separate the generation of a sentence plan from the surface realization. First, an input is mapped into a format that represents the layout of the output sentence, for example, an adequate pre-defined template. Then, the surface realization transforms the intermediary structure into text (Stent et al., 2004). These representations often model the hierarchical structure of discourse relations (Walker et al., 2007). Early data-driven approach used phrase-based language models for generation (Oh and Rudnicky, 2000; Mairesse and Young, 2014), or aimed to predict the best fitting cluster of semantically similar templates (Kondadadi et al., 2013). More recent work combines both steps by learning plan and realization jointly using end-to-end trained models (e.g. Wen et al., 2015). Several approaches have looked at generation from abstract meaning representations (AMR), and Peng et al. (2017) apply S2S models to the problem. However, Ferreira et al. (2017) show that S2S models are outperformed by 3 Background: Sequence-to-Sequence Generation We start by introducing the standard a text-totext problem and discuss how to map structured"
W18-6505,N18-2010,0,0.233229,"tition the training data during the process of training the model itself, thus leading to models that follow distinct sentence templates. We show that this approach improves the quality of generated text, but also the robustness of the training process to outliers in the training data. Experiments are run on the E2E NLG challenge1 . We show that the application of this technique increases the quality of generated text across five different automated metrics (BLEU, NIST, METEOR, ROUGE, and CIDEr) over the multiple strong S2S baseline models (Duˇsek and Jurˇc´ıcˇ ek, 2016; Vaswani et al., 2017; Su et al., 2018; Freitag and Roy, 2018). Among 60 submissions to the challenge, our approach ranked first in METEOR, ROUGE, and CIDEr scores, third in BLEU, and sixth in NIST. 2 phrase-based machine translation models in small datasets. To address this issue, Konstas et al. (2017) propose a semi-supervised training method that can utilize English sentences outside of the training set to train parts of the model. We address the issue by using copy-attention to enable the model to copy words from the source, which helps to generate out of vocabulary and rare words. We note that end-to-end trained models, inclu"
W18-6505,P16-1008,0,0.0600134,"Missing"
W18-6505,D15-1199,0,0.27122,"ables. These methods may also enable the creation of text-generation models that are conditioned on multiple key-value attribute pairs. The conditional generation of fluent text poses multiple challenges since a model has to select content appropriate for an utterance, develop a sentence layout that fits all selected information, and finally generate fluent language that incorporates the content. End-to-end methods have already been applied to increasingly complex data to simultaneously learn sentence planning and surface realization but were often restricted by the limited data availability (Wen et al., 2015; Mei et al., 2015; Duˇsek and Jurˇc´ıcˇ ek, 2016; Lampouras and Vlachos, 2016). The re46 Proceedings of The 11th International Natural Language Generation Conference, pages 46–56, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics we also present results of the Transformer architecture (Vaswani et al., 2017) as an alternative S2S variant. We show that these extensions lead to improved text generation and content selection. We further propose a training approach based on the diverse ensembling technique (GuzmanRivera et al., 2012). In this technique,"
W19-8665,P18-1063,0,0.0540028,"Missing"
W19-8665,N19-1213,0,0.0449228,"Missing"
W19-8665,D15-1044,1,0.757091,"per, as part of the TL;DR challenge, we compare the abstractiveness of summaries from different summarization approaches and show that transfer-learning can be efficiently utilized without any changes to the model architecture. We demonstrate that the approach leads to a higher level of abstraction for a similar performance on the TL;DR challenge tasks, enabling true natural language compression. 1 Introduction Abstractive summarization, the challenge of generating text that captures the content of a longer document, has been successfully approached by many recent deep learning systems (e.g., Rush et al., 2015; Nallapati et al., 2016). However, the most common testbed for such methods, news summarization, provides mostly extractive reference summaries which reuse long phrases from the source document. This property gave rise to extensions of neural summarization models that extract text from a source document in addition to generating new words (Vinyals et al., 2015; Gu et al., 2016). As a side-effect, many of the abstractive summarization models have an inductive bias to almost always extract text from the source document verbatim instead of paraphrasing it. 516 Proceedings of The 12th Internation"
W19-8665,P17-1099,0,0.100236,"approach to strong baselines that rely on minor modifications of the Transformer (Gehrmann et al., 2018). 3.1 Methods Models We consider the following models for neural abstractive summarization. All models are sequenceto-sequence models with attention (Bahdanau et al., 2014), but differ in architecture, use of a copy mechanism, and language model pretraining. LSTM As a baseline we consider a bidirectional LSTM encoder and uni-directional LSTM decoder with attention from Luong et al. (2015). LSTM+Copy We additionally consider the same LSTM model equipped with the copy attention mechanism from See et al. (2017). At each time step the approach reuses the normal alignment distribution as a distribution over source words to copy. This copy distribution is combined with the standard target vocabulary distribution from the decoder via a binary switch zt that is predicted at each time step t. Transformer(+Copy) For the transformer baselines, we replace the LSTM architecture in the encoder and decoder with transformers (Vaswani et al., 2017). As in the LSTM case we consider version with and without the copy mechanism. Similarly to Gehrmann et al. (2018), we randomly select one of the attention heads as the"
W19-8665,D18-1443,1,0.937818,"al Transformer architecture (Vaswani et al., 2017) have shown promising results in language understanding tasks (Houlsby et al., 2019; Devlin et al., 2018; Chronopoulou et al., 2019), but so far have had limited success in generation tasks (Zhang et al., 2019). Most recently, the pseudo-self attention method for fine-tuning language models to generation tasks has been introduced which may allow the application of transfer-learning to abstractive summarization (Ziegler et al., 2019). In this work, we compare this approach to strong baselines that rely on minor modifications of the Transformer (Gehrmann et al., 2018). 3.1 Methods Models We consider the following models for neural abstractive summarization. All models are sequenceto-sequence models with attention (Bahdanau et al., 2014), but differ in architecture, use of a copy mechanism, and language model pretraining. LSTM As a baseline we consider a bidirectional LSTM encoder and uni-directional LSTM decoder with attention from Luong et al. (2015). LSTM+Copy We additionally consider the same LSTM model equipped with the copy attention mechanism from See et al. (2017). At each time step the approach reuses the normal alignment distribution as a distribu"
W19-8665,W19-2303,0,0.034271,"Missing"
W19-8665,P16-1154,0,0.0289922,"uage compression. 1 Introduction Abstractive summarization, the challenge of generating text that captures the content of a longer document, has been successfully approached by many recent deep learning systems (e.g., Rush et al., 2015; Nallapati et al., 2016). However, the most common testbed for such methods, news summarization, provides mostly extractive reference summaries which reuse long phrases from the source document. This property gave rise to extensions of neural summarization models that extract text from a source document in addition to generating new words (Vinyals et al., 2015; Gu et al., 2016). As a side-effect, many of the abstractive summarization models have an inductive bias to almost always extract text from the source document verbatim instead of paraphrasing it. 516 Proceedings of The 12th International Conference on Natural Language Generation, pages 516–522, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics 2 3 Problem and Related Work Throughout this study, we consider the supervised summarization problem, which aims to compress a source document of tokens x1 , . . . , xm of length m. The aligned summary y1 , . . . , yn has a length n"
W19-8665,W17-4508,0,0.355715,"Missing"
W19-8665,D15-1166,0,0.0559914,"low the application of transfer-learning to abstractive summarization (Ziegler et al., 2019). In this work, we compare this approach to strong baselines that rely on minor modifications of the Transformer (Gehrmann et al., 2018). 3.1 Methods Models We consider the following models for neural abstractive summarization. All models are sequenceto-sequence models with attention (Bahdanau et al., 2014), but differ in architecture, use of a copy mechanism, and language model pretraining. LSTM As a baseline we consider a bidirectional LSTM encoder and uni-directional LSTM decoder with attention from Luong et al. (2015). LSTM+Copy We additionally consider the same LSTM model equipped with the copy attention mechanism from See et al. (2017). At each time step the approach reuses the normal alignment distribution as a distribution over source words to copy. This copy distribution is combined with the standard target vocabulary distribution from the decoder via a binary switch zt that is predicted at each time step t. Transformer(+Copy) For the transformer baselines, we replace the LSTM architecture in the encoder and decoder with transformers (Vaswani et al., 2017). As in the LSTM case we consider version with"
W19-8665,K19-1074,0,0.027302,"Missing"
