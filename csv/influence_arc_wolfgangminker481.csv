2005.sigdial-1.21,H90-1020,0,0.0570142,"minker,artha}@it.e-technik.uni-ulm.de Abstract One-stage decoding as an integration of speech recognition and linguistic analysis into one probabilistic process is an interesting trend in speech research. In this paper, we present a simple one-stage decoding scheme that can be realised without the implementation of a specialized decoder, nor the use of complex language models. Instead, we reduce an HMMbased semantic analysis to the problem of deriving annotated versions of the conventional language model, while the acoustic model remains unchanged. We present experiments with the ATIS corpus (Price, 1990) in which the performance of the one-stage method is shown to be comparable with the traditional two-stage approach, while requiring a significantly smaller increase in language model size. 1 Introduction In a spoken dialogue system, speech recognition and linguistic analysis play a decisive role for the overall performance of the system. Traditionally, word hypotheses produced by the automatic speech recognition (ASR) component are fed into a separate natural language understanding (NLU) module for deriving a semantic meaning representation. These semantic representations are the system’s und"
2020.lrec-1.61,W09-3947,0,0.189202,"sending messages to friends and colleagues. While serving the users’ needs, PAs constantly collect personal data in order to personalize their services and adapt their behavior. Adaptation needs not only to be performed in terms of adapting to the user, but also in terms of adapting to the situation, in which these assistants are used. Especially, when the user drives or is busy with another task at home (e.g., cooking), the interaction with a PA is only the secondary task. Thus, user experience designers need to focus on the user’s cognitive load in such settings, too. (Gabaude et al., 2012; Villing, 2009b; Villing, 2009a) In order to investigate how users perceive proactive voice output while driving, we conducted a Wizard of Oz study in a driving simulator with 42 participants. We varied traffic density during a highway drive to induce different levels of cognitive load . Furthermore, we permuted six in-car specific use cases and added a non-proactive control condition with the same six use cases. By employing a subjective DALI questionnaire (Pauzi´e, 2008) we assessed the users’ cognitive load during the interaction with the two PA variants. Additionally, we let the participants rate both P"
2020.lrec-1.65,E17-1024,0,0.0163103,"t contain topic specific key words. The list of key words consists of sub strings of the topic (for example nuclear and energy) as well as a WordNet (Bird et al., 2009; Miller et al., 1990) synonym either of the complete search query or (if none was found) the sub strings. The final arguments are then sampled from this list with a fixed random seed, also 3 http://commoncrawl.org/ https://guncontrolfacts.org/category/gun-control-pros-andcons/ 4 516 to ensure reproducibility. In order to determine the stance of the retrieved argument, we train a classifier on the IBM stance classification data (Bar-Haim et al., 2017). The corpus consists of 2394 claims annotated with an overall sentiment label sc ∈ {1, −1}, the stance towards the related topic and a sentiment label for the topic st ∈ {1, −1}. Similar to the baseline approaches in the original work, we train a model to estimate the sentiment of each claim and assume that the target towards which the estimated sentiment is expressed is consistent with the target of the topic. The corresponding stance can then be derived as sc × st . Our approach utilizes a pre-trained BERT (Devlin et al., 2019) model5 to get sentence embeddings for all claims in the corpus"
2020.lrec-1.65,W15-4603,0,0.0157949,"guments, i.e. the dialogue management, and the non-verbal behaviour of the avatar. Implications for Dialogue Systems The results of our study also allow some general conclusions for the development of future argumentative dialogue systems that aim to exploit argument search in order to retrieve arguments. Firstly, the reported subjectivity of the argument perception stresses the need for a careful selection of arguments based on the target audience or user. Consequently, adaptation and user modelling approaches investigated for the use in dialogue systems (Ultes et al., 2019; Mo et al., 2018; Casanueva et al., 2015) are also required in the domain of argumentation, although it is not clear from the results which user traits are the most relevant. In addition to this, the reported high user comprehension of the system’s utterances (Figure 2) allows the conclusion that arguments retrieved by argument search engines can generally be understood. However, several participants reported that spelling or grammar errors in the arguments lead to an unnatural system output, which, although generally comprehensible, was not natural and intuitive. Consequently, more advanced approaches to NLG in combination with para"
2020.lrec-1.65,P19-3022,0,0.522077,"first requirement is necessary for the technical applicability of the search engine within argumentative applications whereas the second requirement is motivated by the need for stance information in the majority of the desired tasks of an argumentative system. The aim of an argument search engine is to retrieve a ranked list of arguments related to a given search query. Different systems introduced so far follow different paradigms in order to accomplish this goal (Ajjour et al., 2019) and include the IBM Project Debater (Levy et al., 2018), TARGER (Chernodub et al., 2019), PerspectroScope (Chen et al., 2019), args.me (Wachsmuth et al., 2017c) and ArgumenText (Stab et al., 2018a). Out of this list, only ArgumenText, args.me and TARGER provide an API to access retrieved arguments and only the first two also include information about their stance. Consequently, we focus our evaluation on these two and discuss the underlying approaches in detail in the following subsections. In addition, we propose a novel system utilizing a conventional web search approach in order to generate baseline results for the evaluation. 515 Information System Return Information System Return Premises If marriage’s main fun"
2020.lrec-1.65,P19-3031,0,0.244586,"the stance of the retrieved arguments. The first requirement is necessary for the technical applicability of the search engine within argumentative applications whereas the second requirement is motivated by the need for stance information in the majority of the desired tasks of an argumentative system. The aim of an argument search engine is to retrieve a ranked list of arguments related to a given search query. Different systems introduced so far follow different paradigms in order to accomplish this goal (Ajjour et al., 2019) and include the IBM Project Debater (Levy et al., 2018), TARGER (Chernodub et al., 2019), PerspectroScope (Chen et al., 2019), args.me (Wachsmuth et al., 2017c) and ArgumenText (Stab et al., 2018a). Out of this list, only ArgumenText, args.me and TARGER provide an API to access retrieved arguments and only the first two also include information about their stance. Consequently, we focus our evaluation on these two and discuss the underlying approaches in detail in the following subsections. In addition, we propose a novel system utilizing a conventional web search approach in order to generate baseline results for the evaluation. 515 Information System Return Information System R"
2020.lrec-1.65,N19-1423,0,0.0139232,"we train a classifier on the IBM stance classification data (Bar-Haim et al., 2017). The corpus consists of 2394 claims annotated with an overall sentiment label sc ∈ {1, −1}, the stance towards the related topic and a sentiment label for the topic st ∈ {1, −1}. Similar to the baseline approaches in the original work, we train a model to estimate the sentiment of each claim and assume that the target towards which the estimated sentiment is expressed is consistent with the target of the topic. The corresponding stance can then be derived as sc × st . Our approach utilizes a pre-trained BERT (Devlin et al., 2019) model5 to get sentence embeddings for all claims in the corpus which are then used as feature vectors for a support vector machine (SVM) classification. The parameters of the SVM are optimized in a systematic grid search in order to match the specific task. The performance of our model is evaluated by averaging the results for five different random train/test splittings of the data with the same characteristics provided in the original work (training: 25 topics and 1039 claims, test: 30 topics and 1355 claims). Since the overall system requires an estimate of the stance for all retrieved argu"
2020.lrec-1.65,P19-1093,0,0.0174145,"hey divide argument quality in the broad categories of logical, rhetorical and dialectical quality and introduce 15 fine-grained sub-dimensions as well as a corpus annotated with these dimensions. Habernal and Gurevych (2016a) introduced an approach to assess the convincingness of arguments in which arguments were rated in direct comparison to each other in a crowd-sourcing experiment. The correlations between the theoretical and the crowd-sourcing based approach were also investigated (Wachsmuth et al., 2017a) and a corpus for the comparison of the convincingness of evidences was introduced (Gleize et al., 2019). The overall quality of single arguments as well as argument pairs was discussed by Toledo et al. (2019) together with automatized approaches for argument ranking and argument-pair classification. Finally, Potthast et al. (2019) utilized expert ratings of the above mentioned categories logical, rhetorical and dialectical quality to assess different retrieval approaches for argument search in combination with the information retrieval notion of relevance. To the best of our knowledge, no studies have been carried out which explicitly focus on argument quality assessment in the context of dialo"
2020.lrec-1.65,D16-1129,0,0.115863,"eans of an argumentative dialogue system that evaluates arguments retrieved by different search approaches directly in the interaction with users. This is realized by allowing the user to give specific ratings in the categories Interesting, Convincing, Comprehensible and Related as direct feedback to each system utterance. In order to ensure a setting that is representative for dialogue system applications, the arguments are presented by means of a virtual avatar and synthetic speech. The approach is motivated by the difficulty of argument quality assessment from a purely logical perspective (Habernal and Gurevych, 2016b; Wachsmuth et al., 2017a) as well as the common approach to evaluate dialogue systems from the user perspective (Deriu et al., 2019). Especially the subjective nature of our addressed application scenarios (and argumentation itself) and the effects of system modalities (virtual avatar and synthetic speech) in the present scenario render approaches that do not explicitly consider the user perception impractical. We apply our system in a user study in order to compare two state of the art argument search engines, namely ArgumenText (Stab et al., 2018a) and args.me (Wachsmuth et al., 2017c), to"
2020.lrec-1.65,P16-1150,0,0.180899,"eans of an argumentative dialogue system that evaluates arguments retrieved by different search approaches directly in the interaction with users. This is realized by allowing the user to give specific ratings in the categories Interesting, Convincing, Comprehensible and Related as direct feedback to each system utterance. In order to ensure a setting that is representative for dialogue system applications, the arguments are presented by means of a virtual avatar and synthetic speech. The approach is motivated by the difficulty of argument quality assessment from a purely logical perspective (Habernal and Gurevych, 2016b; Wachsmuth et al., 2017a) as well as the common approach to evaluate dialogue systems from the user perspective (Deriu et al., 2019). Especially the subjective nature of our addressed application scenarios (and argumentation itself) and the effects of system modalities (virtual avatar and synthetic speech) in the present scenario render approaches that do not explicitly consider the user perception impractical. We apply our system in a user study in order to compare two state of the art argument search engines, namely ArgumenText (Stab et al., 2018a) and args.me (Wachsmuth et al., 2017c), to"
2020.lrec-1.65,W18-5215,0,0.109042,"006). The effect of different types of arguments presented by an argumentative chat bot were investigated in the behaviour change domain and also by means of a user study by Chalaguine et al. (2019), showing that arguments that address the concerns of the user were preferred over others. The assessment from a technical perspective was considered by Rakshit et al. (2019), where a comparison of response times for the different underlying techniques was utilized as evaluation criterion. Moreover, a retrieval-based approach and a generative approach to generate the system response were discussed (Le et al., 2018) and separately evaluated on established metrics for the underlying technological task. Finally, Sakai et al. (2018) and Rach et al. (2019) evaluated argument structures acquired specifically for the use in dialogue systems also by means of user studies. However, the evaluation in these cases is focused on specific systems and/or data and the effect of different acquisition techniques as desired in the present work was therefore not included. In order to provide a detailed evaluation that is not tailored to one specific task, we combine the generalized approach of task success rate as evaluati"
2020.lrec-1.65,C18-1176,0,0.200915,"d provide information about the stance of the retrieved arguments. The first requirement is necessary for the technical applicability of the search engine within argumentative applications whereas the second requirement is motivated by the need for stance information in the majority of the desired tasks of an argumentative system. The aim of an argument search engine is to retrieve a ranked list of arguments related to a given search query. Different systems introduced so far follow different paradigms in order to accomplish this goal (Ajjour et al., 2019) and include the IBM Project Debater (Levy et al., 2018), TARGER (Chernodub et al., 2019), PerspectroScope (Chen et al., 2019), args.me (Wachsmuth et al., 2017c) and ArgumenText (Stab et al., 2018a). Out of this list, only ArgumenText, args.me and TARGER provide an API to access retrieved arguments and only the first two also include information about their stance. Consequently, we focus our evaluation on these two and discuss the underlying approaches in detail in the following subsections. In addition, we propose a novel system utilizing a conventional web search approach in order to generate baseline results for the evaluation. 515 Information S"
2020.lrec-1.65,P19-1054,1,0.836778,"roaches to NLG in combination with paraphrasing and grammar correction are also of interest in order to improve the user experience (Kwon et al., 2015; Wen et al., 2015). Finally it should be noted that many dialogue systems require a more fine grained structure of arguments (Aicher et al., 2019; Sakai et al., 2018; Rosenfeld and Kraus, 2016; Rach et al., 2018) that include not just the general argument stance but also their explicit relations to each other. Hence, additional processing of the search results in order to structure the retrieved arguments as for example clustering of arguments (Reimers et al., 2019) may be required in order to allow systems of these kind to exploit argument search engines. 8. Conclusion We introduced an evaluation setup for argument search approaches in the context of argumentative dialogue systems. Our approach assesses the users’ opinions and perception regarding arguments presented by an avatar with synthetic speech. During the interaction with the system, users are able to rate the arguments presented by the avatar in the categories Interesting, Convincing, Comprehensible and Related as a direct response to the system utterance. The approach was applied in a user stu"
2020.lrec-1.65,L18-1627,0,0.185199,"behaviour change domain and also by means of a user study by Chalaguine et al. (2019), showing that arguments that address the concerns of the user were preferred over others. The assessment from a technical perspective was considered by Rakshit et al. (2019), where a comparison of response times for the different underlying techniques was utilized as evaluation criterion. Moreover, a retrieval-based approach and a generative approach to generate the system response were discussed (Le et al., 2018) and separately evaluated on established metrics for the underlying technological task. Finally, Sakai et al. (2018) and Rach et al. (2019) evaluated argument structures acquired specifically for the use in dialogue systems also by means of user studies. However, the evaluation in these cases is focused on specific systems and/or data and the effect of different acquisition techniques as desired in the present work was therefore not included. In order to provide a detailed evaluation that is not tailored to one specific task, we combine the generalized approach of task success rate as evaluation criterion with aspects related to the field of argument quality assessment. As for computational argumentation, W"
2020.lrec-1.65,N07-2038,0,0.20191,"ussion of the results is included in Section 7., followed by a conclusion and an outlook on future work in Section 8. 2. Related Work In this section, we discuss related work from the fields of dialogue system evaluation and argument quality assessment. For dialogue systems, different general evaluation approaches exist based on the different types of system (Deriu et al., 2019). For task oriented systems, which are the most relevant in view of argumentation, the task success rate, i.e. the rate at which a system successfully carries out the assigned function is a common evaluation criterion (Schatzmann et al., 2007; Laroche et al., 2011). It was combined with a measure of the dialogue cost and the subjective satisfaction of the user with the interaction in the PARADISE framework (Walker et al., 1997; Walker et al., 2000) in order to enable comparisons of different systems. In addition, Ultes et al. (2013) introduced the Interaction Quality as an expert rating based approach to model user satisfaction. Since argumentation is a comparatively new domain for dialogue systems, assessment of argumentative systems is currently done in a very system specific way: The Project Debater introduced by IBM was evalua"
2020.lrec-1.65,N18-5005,1,0.864142,"Missing"
2020.lrec-1.65,D18-1402,0,0.249833,"rom a purely logical perspective (Habernal and Gurevych, 2016b; Wachsmuth et al., 2017a) as well as the common approach to evaluate dialogue systems from the user perspective (Deriu et al., 2019). Especially the subjective nature of our addressed application scenarios (and argumentation itself) and the effects of system modalities (virtual avatar and synthetic speech) in the present scenario render approaches that do not explicitly consider the user perception impractical. We apply our system in a user study in order to compare two state of the art argument search engines, namely ArgumenText (Stab et al., 2018a) and args.me (Wachsmuth et al., 2017c), to each other. In addition, we introduce an argument retrieval system based on conventional web search to provide a suitable baseline. In order to exclude topic dependencies, the comparison is done over three different controversial topics. The results show significant differences between the investigated approaches for three of the four categories and both search engines outperform the baseline in one category. In addition, both search engines outperform each other in a different category, thereby reflecting the different strengths and drawbacks of th"
2020.lrec-1.65,D19-1564,0,0.0113043,"troduce 15 fine-grained sub-dimensions as well as a corpus annotated with these dimensions. Habernal and Gurevych (2016a) introduced an approach to assess the convincingness of arguments in which arguments were rated in direct comparison to each other in a crowd-sourcing experiment. The correlations between the theoretical and the crowd-sourcing based approach were also investigated (Wachsmuth et al., 2017a) and a corpus for the comparison of the convincingness of evidences was introduced (Gleize et al., 2019). The overall quality of single arguments as well as argument pairs was discussed by Toledo et al. (2019) together with automatized approaches for argument ranking and argument-pair classification. Finally, Potthast et al. (2019) utilized expert ratings of the above mentioned categories logical, rhetorical and dialectical quality to assess different retrieval approaches for argument search in combination with the information retrieval notion of relevance. To the best of our knowledge, no studies have been carried out which explicitly focus on argument quality assessment in the context of dialogue systems. 3. Evaluation Criteria As a first step in developing the actual evaluation setup, we introdu"
2020.lrec-1.65,N13-1064,1,0.699414,"tion approaches exist based on the different types of system (Deriu et al., 2019). For task oriented systems, which are the most relevant in view of argumentation, the task success rate, i.e. the rate at which a system successfully carries out the assigned function is a common evaluation criterion (Schatzmann et al., 2007; Laroche et al., 2011). It was combined with a measure of the dialogue cost and the subjective satisfaction of the user with the interaction in the PARADISE framework (Walker et al., 1997; Walker et al., 2000) in order to enable comparisons of different systems. In addition, Ultes et al. (2013) introduced the Interaction Quality as an expert rating based approach to model user satisfaction. Since argumentation is a comparatively new domain for dialogue systems, assessment of argumentative systems is currently done in a very system specific way: The Project Debater introduced by IBM was evaluated by engaging with a human in a live debate. The outcome was determined by a comparison of the audience’s stance before and after the debate, showing an advantage of the human debater over Project Debater. Rosenfeld and Kraus (2016) evaluated the persuasive effect of their introduced persuasiv"
2020.lrec-1.65,P17-2039,0,0.0963646,"gue system that evaluates arguments retrieved by different search approaches directly in the interaction with users. This is realized by allowing the user to give specific ratings in the categories Interesting, Convincing, Comprehensible and Related as direct feedback to each system utterance. In order to ensure a setting that is representative for dialogue system applications, the arguments are presented by means of a virtual avatar and synthetic speech. The approach is motivated by the difficulty of argument quality assessment from a purely logical perspective (Habernal and Gurevych, 2016b; Wachsmuth et al., 2017a) as well as the common approach to evaluate dialogue systems from the user perspective (Deriu et al., 2019). Especially the subjective nature of our addressed application scenarios (and argumentation itself) and the effects of system modalities (virtual avatar and synthetic speech) in the present scenario render approaches that do not explicitly consider the user perception impractical. We apply our system in a user study in order to compare two state of the art argument search engines, namely ArgumenText (Stab et al., 2018a) and args.me (Wachsmuth et al., 2017c), to each other. In addition,"
2020.lrec-1.65,E17-1017,0,0.134625,"gue system that evaluates arguments retrieved by different search approaches directly in the interaction with users. This is realized by allowing the user to give specific ratings in the categories Interesting, Convincing, Comprehensible and Related as direct feedback to each system utterance. In order to ensure a setting that is representative for dialogue system applications, the arguments are presented by means of a virtual avatar and synthetic speech. The approach is motivated by the difficulty of argument quality assessment from a purely logical perspective (Habernal and Gurevych, 2016b; Wachsmuth et al., 2017a) as well as the common approach to evaluate dialogue systems from the user perspective (Deriu et al., 2019). Especially the subjective nature of our addressed application scenarios (and argumentation itself) and the effects of system modalities (virtual avatar and synthetic speech) in the present scenario render approaches that do not explicitly consider the user perception impractical. We apply our system in a user study in order to compare two state of the art argument search engines, namely ArgumenText (Stab et al., 2018a) and args.me (Wachsmuth et al., 2017c), to each other. In addition,"
2020.lrec-1.65,W17-5106,0,0.22544,"gue system that evaluates arguments retrieved by different search approaches directly in the interaction with users. This is realized by allowing the user to give specific ratings in the categories Interesting, Convincing, Comprehensible and Related as direct feedback to each system utterance. In order to ensure a setting that is representative for dialogue system applications, the arguments are presented by means of a virtual avatar and synthetic speech. The approach is motivated by the difficulty of argument quality assessment from a purely logical perspective (Habernal and Gurevych, 2016b; Wachsmuth et al., 2017a) as well as the common approach to evaluate dialogue systems from the user perspective (Deriu et al., 2019). Especially the subjective nature of our addressed application scenarios (and argumentation itself) and the effects of system modalities (virtual avatar and synthetic speech) in the present scenario render approaches that do not explicitly consider the user perception impractical. We apply our system in a user study in order to compare two state of the art argument search engines, namely ArgumenText (Stab et al., 2018a) and args.me (Wachsmuth et al., 2017c), to each other. In addition,"
2020.lrec-1.65,P97-1035,0,0.683758,"f dialogue system evaluation and argument quality assessment. For dialogue systems, different general evaluation approaches exist based on the different types of system (Deriu et al., 2019). For task oriented systems, which are the most relevant in view of argumentation, the task success rate, i.e. the rate at which a system successfully carries out the assigned function is a common evaluation criterion (Schatzmann et al., 2007; Laroche et al., 2011). It was combined with a measure of the dialogue cost and the subjective satisfaction of the user with the interaction in the PARADISE framework (Walker et al., 1997; Walker et al., 2000) in order to enable comparisons of different systems. In addition, Ultes et al. (2013) introduced the Interaction Quality as an expert rating based approach to model user satisfaction. Since argumentation is a comparatively new domain for dialogue systems, assessment of argumentative systems is currently done in a very system specific way: The Project Debater introduced by IBM was evaluated by engaging with a human in a live debate. The outcome was determined by a comparison of the audience’s stance before and after the debate, showing an advantage of the human debater ov"
2020.lrec-1.65,W15-4639,0,0.0675677,"Missing"
2020.lrec-1.68,E17-1003,0,0.043442,"Missing"
2020.lrec-1.68,D16-1216,0,0.0188355,"n down to rather simple key word spotting (e.g. “sorry”, “just”, “and stuff”, “I think”). Neuliep (2011) describes the indirect style as “one where the speaker’s intentions are hidden or only hinted at during interaction”. For our corpus annotation, we used this definition and annotated the directness/indirectness in a global way and not based on fixed structures or key words. Other work in this field only focuses on a specific phenomena of indirect speech, like hedge detection (Prokofieva and Hirschberg, 2014; Ulinski et al., 2018), politeness detection (Danescu-Niculescu-Mizil et al., 2013; Aubakirova and Bansal, 2016) and uncertainty detection (Liscombe et al., 2005; Forbes-Riley and Litman, 2011; Adel and Sch¨utze, 2017). 3. Corpus Description Our data set is based on recordings on health care topics containing spontaneous interactions in dialogue format between two participants: one is taking the role of the system while the other one is taking the role of the user. Each dialogue turn contains one or more dialogue acts. These dialogue acts are chosen out of a set of 43 distinct dialogue acts which have been predefined. A list of all dialogue acts can be found in Table 9 in the appendix. Along with the di"
2020.lrec-1.68,W15-4603,0,0.0294542,"linguistic features on its estimation. Our classifiers use only features that can be automatically generated during an interaction with a spoken dialogue system (i.e. without any manual annotation). Even though intelligent assistants like Amazon Alexa, Apple Siri, Google Assistant or Microsoft Cortana are becoming increasingly popular, they do not consider different communication styles to adapt their behaviour. Instead, current research in the field of spoken dialogue systems focuses on general user adaptivity like satisfaction or general user groups (Honold et al., 2014; Ultes et al., 2015; Casanueva et al., 2015; Pragst et al., 2015; Miehle et al., 2019). However, various studies suggest that adapting the communication styles of spoken dialogue systems to the individual users in a similar way to what humans do will lead to more natural interactions (Cassell and Bickmore, 2003; Forbes-Riley et al., 2008; Stenchikova and Stent, 2007; Reitter et al., 2006; Mairesse and Walker, 2010). To adapt the behaviour of the system to individual users, we consider the communication styles elaborateness and directness in this work as Pragst et al. (2019) have shown that they influence the user’s perception of a dial"
2020.lrec-1.68,P13-1025,0,0.0330135,"and that indirectness cannot be broken down to rather simple key word spotting (e.g. “sorry”, “just”, “and stuff”, “I think”). Neuliep (2011) describes the indirect style as “one where the speaker’s intentions are hidden or only hinted at during interaction”. For our corpus annotation, we used this definition and annotated the directness/indirectness in a global way and not based on fixed structures or key words. Other work in this field only focuses on a specific phenomena of indirect speech, like hedge detection (Prokofieva and Hirschberg, 2014; Ulinski et al., 2018), politeness detection (Danescu-Niculescu-Mizil et al., 2013; Aubakirova and Bansal, 2016) and uncertainty detection (Liscombe et al., 2005; Forbes-Riley and Litman, 2011; Adel and Sch¨utze, 2017). 3. Corpus Description Our data set is based on recordings on health care topics containing spontaneous interactions in dialogue format between two participants: one is taking the role of the system while the other one is taking the role of the user. Each dialogue turn contains one or more dialogue acts. These dialogue acts are chosen out of a set of 43 distinct dialogue acts which have been predefined. A list of all dialogue acts can be found in Table 9 in t"
2020.lrec-1.68,L18-1550,0,0.0149334,"laries and the combination with word embeddings led to three different linguistic feature sets: • U: This feature set contains a BoW-U vector for each utterance, thus encoding the number of times each word (of the overall vocabulary) appears in the corresponding utterance. • UB: This feature set contains a BoW-UB vector for each utterance, thus encoding the number of times each word and each two-word-sequence (of the overall vocabulary) appear in the corresponding utterance. • WE: For this feature set, the BoW-U vocabulary has been combined with the German pre-trained fastText word vectors by Grave et al. (2018)2 . Matrix X of dimension u × w contains the BoW-U vectors (dimension 1 × w with w the amount of words in vocabulary BoW-U) for each utterance, where u is the total number of utterances. Matrix W of dimension w × p contains the fastText word vectors (dimension 1 × p with p the length of each word vector) for each word. By multiplying these matrices a new matrix Z = X ·W of dimension u×p is obtained, containing a vector representation for each utterance. These utterance vectors of dimension 1 × p can then be used as feature vectors for the classification task. The Contribution of Grammatical an"
2020.lrec-1.68,W16-3610,1,0.834984,"m behaviour to the user. spoken user interface are perceived by users and whether there exist global preferences in the communication styles elaborateness and directness. The authors could show that the system’s communication style influences the user’s satisfaction and the user’s perception of the dialogue and that there is no general preference in the system’s communication style. The authors conclude that spoken dialogue systems need to adapt their communication style to each user individually during every dialogue in order to achieve a high level of user satisfaction. A study presented by Miehle et al. (2016) investigated cultural differences between the Germans and the Japanese. The results revealed that communication idiosyncrasies in human-human interaction may also be observed during human-computer interaction in a spoken dialogue system context. Moreover, Miehle et al. (2018b) presented another study examining five European cultures whose communication styles are much more alike than the German and Japanese communication idiosyncrasies. The study explores not only the influence of the user’s culture but also of the gender, the frequency of use of speech based assistants as well as the system’"
2020.lrec-1.68,L18-1625,1,0.74689,"nswer provides some additional information: “Most of the time it is cloudy and in the afternoon it will rain.” The indirect and concise version of this utterance also contains few information, yet addresses the fact that it is raining in a less concrete way: “Today is a good day for cosy activities at home.” In this case, the interlocutor can infer that the weather won’t be nice as it is better to stay at home. The elaborate and indirect version provides some more details: “Today is a good day for cosy activities at home. In the afternoon you could get wet outside.” This example is taken from Miehle et al. (2018a) addressing the issues of how varying communication styles of a 540 Speech Recognition Linguistic Analysis Communication Style Classifier Speech Synthesis Dialogue Management Application Text Generation Figure 1: The estimated communication style, which is classified based on features from the speech recognition and the linguistic analysis, can be used in the dialogue management to adapt the system behaviour to the user. spoken user interface are perceived by users and whether there exist global preferences in the communication styles elaborateness and directness. The authors could show that"
2020.lrec-1.68,P08-2043,0,0.0559436,"e classification results. Afterwards, we run a comparison with a support vector machine and a recurrent neural network classifier. Keywords: Dialogue management, User adaptation, Supervised learning. 1. Introduction For humans, speech is the most natural form of interaction and it has been shown that people adapt their interaction styles to one another across many levels of utterance production when communicating, e.g. by matching each other’s behaviour or synchronising the timing of behaviour (Burgoon et al., 2007; Niederhoffer and Pennebaker, 2002; Brennan, 1996; Pickering and Garrod, 2004; Nenkova et al., 2008). However, this adaptive behaviour has rarely been addressed and implemented in a live spoken dialogue system. With the aim of designing such a spoken dialogue system which adapts to the user’s communication idiosyncrasies, we present a classification approach to automatically estimate the user’s communication style during an ongoing dialogue. The estimated communication style can then be used in the dialogue management to adapt the system behaviour to the user, as depicted in Figure 1. To the best of our knowledge, this is the first work on automatic estimation of the user’s communication sty"
2020.lrec-1.68,E14-2005,0,0.0195777,"containing a vector representation for each utterance. These utterance vectors of dimension 1 × p can then be used as feature vectors for the classification task. The Contribution of Grammatical and Linguistic Features To address the question of whether grammatical features improve the estimation of the communication style, a second feature set is used containing the dialogue act features (as have been used for the baseline) as well as grammatical features. The grammatical features (G) are represented by Part-of-speech (POS) tags which have been assigned to the utterances by the RDRPOSTagger (Nguyen et al., 2014). As the utterance is the output of the speech recognition and this tagger can be used online during an ongoing interaction, there is also no annotation necessary for this feature set. The results are shown in Table 5. It can be seen that there is no improvement in comparison to the baseline. In addition to grammatical features, linguistic features may majorly contribute to the overall classification performance. In order to encode the linguistic features, a Bag-of-Words (BoW) approach has been used in combination with unigrams (U), unigrams and bigrams (UB) and word embeddings (WE). Using BoW"
2020.lrec-1.68,N06-2031,0,0.0744697,"consider different communication styles to adapt their behaviour. Instead, current research in the field of spoken dialogue systems focuses on general user adaptivity like satisfaction or general user groups (Honold et al., 2014; Ultes et al., 2015; Casanueva et al., 2015; Pragst et al., 2015; Miehle et al., 2019). However, various studies suggest that adapting the communication styles of spoken dialogue systems to the individual users in a similar way to what humans do will lead to more natural interactions (Cassell and Bickmore, 2003; Forbes-Riley et al., 2008; Stenchikova and Stent, 2007; Reitter et al., 2006; Mairesse and Walker, 2010). To adapt the behaviour of the system to individual users, we consider the communication styles elaborateness and directness in this work as Pragst et al. (2019) have shown that they influence the user’s perception of a dialogue and are therefore valuable candidates for adaptive dialogue management. The elaborateness thereby refers to the amount of additional information provided to the user and the directness describes how concretely the information that is to be conveyed is addressed by the speaker. This means that a direct and concise answer to the question “Can"
2020.lrec-1.68,2007.sigdial-1.29,0,0.0558973,"easingly popular, they do not consider different communication styles to adapt their behaviour. Instead, current research in the field of spoken dialogue systems focuses on general user adaptivity like satisfaction or general user groups (Honold et al., 2014; Ultes et al., 2015; Casanueva et al., 2015; Pragst et al., 2015; Miehle et al., 2019). However, various studies suggest that adapting the communication styles of spoken dialogue systems to the individual users in a similar way to what humans do will lead to more natural interactions (Cassell and Bickmore, 2003; Forbes-Riley et al., 2008; Stenchikova and Stent, 2007; Reitter et al., 2006; Mairesse and Walker, 2010). To adapt the behaviour of the system to individual users, we consider the communication styles elaborateness and directness in this work as Pragst et al. (2019) have shown that they influence the user’s perception of a dialogue and are therefore valuable candidates for adaptive dialogue management. The elaborateness thereby refers to the amount of additional information provided to the user and the directness describes how concretely the information that is to be conveyed is addressed by the speaker. This means that a direct and concise answe"
2020.lrec-1.68,W18-1301,0,0.0249547,"6 0.643 Directness (3 classes) in this corpus and that indirectness cannot be broken down to rather simple key word spotting (e.g. “sorry”, “just”, “and stuff”, “I think”). Neuliep (2011) describes the indirect style as “one where the speaker’s intentions are hidden or only hinted at during interaction”. For our corpus annotation, we used this definition and annotated the directness/indirectness in a global way and not based on fixed structures or key words. Other work in this field only focuses on a specific phenomena of indirect speech, like hedge detection (Prokofieva and Hirschberg, 2014; Ulinski et al., 2018), politeness detection (Danescu-Niculescu-Mizil et al., 2013; Aubakirova and Bansal, 2016) and uncertainty detection (Liscombe et al., 2005; Forbes-Riley and Litman, 2011; Adel and Sch¨utze, 2017). 3. Corpus Description Our data set is based on recordings on health care topics containing spontaneous interactions in dialogue format between two participants: one is taking the role of the system while the other one is taking the role of the user. Each dialogue turn contains one or more dialogue acts. These dialogue acts are chosen out of a set of 43 distinct dialogue acts which have been predefin"
2020.lrec-1.68,W15-4649,1,0.821632,"of grammatical and linguistic features on its estimation. Our classifiers use only features that can be automatically generated during an interaction with a spoken dialogue system (i.e. without any manual annotation). Even though intelligent assistants like Amazon Alexa, Apple Siri, Google Assistant or Microsoft Cortana are becoming increasingly popular, they do not consider different communication styles to adapt their behaviour. Instead, current research in the field of spoken dialogue systems focuses on general user adaptivity like satisfaction or general user groups (Honold et al., 2014; Ultes et al., 2015; Casanueva et al., 2015; Pragst et al., 2015; Miehle et al., 2019). However, various studies suggest that adapting the communication styles of spoken dialogue systems to the individual users in a similar way to what humans do will lead to more natural interactions (Cassell and Bickmore, 2003; Forbes-Riley et al., 2008; Stenchikova and Stent, 2007; Reitter et al., 2006; Mairesse and Walker, 2010). To adapt the behaviour of the system to individual users, we consider the communication styles elaborateness and directness in this work as Pragst et al. (2019) have shown that they influence the use"
2020.lrec-1.845,W17-5310,0,0.0158255,"ingle work but must be gathered from numerous individual contributions. Furthermore, the same procedure is not necessarily used across evaluations, further impeding a thorough understanding of advantages and disadvantages of any given model. There have been some efforts to address this issue, e.g. (White et al., 2015). Here, the authors evaluate sentence embeddings with a semantic classification task in a generalised manner. Additionally, the RepEval 2017 Shared Task (Nangia et al., 2017) compares the performance of seven sentence embedding approaches (Chen et al., 2017; Nie and Bansal, 2017; Balazs et al., 2017; Vu et al., 2017; Yang et al., 2017) on a shared task. Those works operate on a strict definition of sentence paraphrases: each sentence must entail the other to be considered a paraphrase. The first task of SemEval-2014 (Marelli et al., 2014) expands on this definition by adding a semantic relatedness score to sentence pairs. The task was solved by 21 participating teams, with 17 submission for the semantic relatedness subtask and 18 for the entailment subtask. However, only 14 of those entries were accompanied by papers (Alves et al., 2014; Beltagy et al., 2014; Bestgen, 2014; Bic¸ici and W"
2020.lrec-1.845,S14-2141,0,0.0173973,"., 2017; Nie and Bansal, 2017; Balazs et al., 2017; Vu et al., 2017; Yang et al., 2017) on a shared task. Those works operate on a strict definition of sentence paraphrases: each sentence must entail the other to be considered a paraphrase. The first task of SemEval-2014 (Marelli et al., 2014) expands on this definition by adding a semantic relatedness score to sentence pairs. The task was solved by 21 participating teams, with 17 submission for the semantic relatedness subtask and 18 for the entailment subtask. However, only 14 of those entries were accompanied by papers (Alves et al., 2014; Beltagy et al., 2014; Bestgen, 2014; Bic¸ici and Way, 2014; Bjerva et al., 2014; Ferrone and Zanzotto, 2014; Gupta et al., 2014; Jimenez et al., 2014; Lai and Hockenmaier, 2014; Le´on et al., 2014; 6841 Lien and Kouylekov, 2014; Proisl et al., 2014; Vo et al., 2014; Zhao et al., 2014). Often, evaluations of the semantic meaning of sentence embeddings are carried out as classification task on paraphrase corpora such as the MSR paraphrase corpus (Dolan et al., 2004). Another, similar option is the natural language entailment task where, instead of labelling sentence pairs as either paraphrases or unrelated, they ar"
2020.lrec-1.845,S14-2024,0,0.0209715,"l, 2017; Balazs et al., 2017; Vu et al., 2017; Yang et al., 2017) on a shared task. Those works operate on a strict definition of sentence paraphrases: each sentence must entail the other to be considered a paraphrase. The first task of SemEval-2014 (Marelli et al., 2014) expands on this definition by adding a semantic relatedness score to sentence pairs. The task was solved by 21 participating teams, with 17 submission for the semantic relatedness subtask and 18 for the entailment subtask. However, only 14 of those entries were accompanied by papers (Alves et al., 2014; Beltagy et al., 2014; Bestgen, 2014; Bic¸ici and Way, 2014; Bjerva et al., 2014; Ferrone and Zanzotto, 2014; Gupta et al., 2014; Jimenez et al., 2014; Lai and Hockenmaier, 2014; Le´on et al., 2014; 6841 Lien and Kouylekov, 2014; Proisl et al., 2014; Vo et al., 2014; Zhao et al., 2014). Often, evaluations of the semantic meaning of sentence embeddings are carried out as classification task on paraphrase corpora such as the MSR paraphrase corpus (Dolan et al., 2004). Another, similar option is the natural language entailment task where, instead of labelling sentence pairs as either paraphrases or unrelated, they are labelled as e"
2020.lrec-1.845,S14-2085,0,0.0537419,"Missing"
2020.lrec-1.845,S14-2114,0,0.0662646,"Missing"
2020.lrec-1.845,D15-1075,0,0.0432129,"4; Lai and Hockenmaier, 2014; Le´on et al., 2014; 6841 Lien and Kouylekov, 2014; Proisl et al., 2014; Vo et al., 2014; Zhao et al., 2014). Often, evaluations of the semantic meaning of sentence embeddings are carried out as classification task on paraphrase corpora such as the MSR paraphrase corpus (Dolan et al., 2004). Another, similar option is the natural language entailment task where, instead of labelling sentence pairs as either paraphrases or unrelated, they are labelled as either entailment, neutral or contradiction. Such corpora include the Stanford Natural Language Inference corpus (Bowman et al., 2015) and the Multi-Genre NLI corpus (Williams et al., 2018), which has been used in the RepEval 2017 Shared Task. The SICK corpus (Marelli et al., 2014) developed for SemEval-2014 Task 1 expands entailment annotations by more fine-grained, human-annotated semantic relatedness scores. In our work, we include a greater number of sentence pairs in our understanding of paraphrases: two sentences are considered contextual paraphrases if they can fulfil the same function in the context of a conversation. 3. Sentence Similarity Models Several approaches to sentence encoding exist, however, not all of the"
2020.lrec-1.845,W17-5307,0,0.0138364,"f different models cannot be found in a single work but must be gathered from numerous individual contributions. Furthermore, the same procedure is not necessarily used across evaluations, further impeding a thorough understanding of advantages and disadvantages of any given model. There have been some efforts to address this issue, e.g. (White et al., 2015). Here, the authors evaluate sentence embeddings with a semantic classification task in a generalised manner. Additionally, the RepEval 2017 Shared Task (Nangia et al., 2017) compares the performance of seven sentence embedding approaches (Chen et al., 2017; Nie and Bansal, 2017; Balazs et al., 2017; Vu et al., 2017; Yang et al., 2017) on a shared task. Those works operate on a strict definition of sentence paraphrases: each sentence must entail the other to be considered a paraphrase. The first task of SemEval-2014 (Marelli et al., 2014) expands on this definition by adding a semantic relatedness score to sentence pairs. The task was solved by 21 participating teams, with 17 submission for the semantic relatedness subtask and 18 for the entailment subtask. However, only 14 of those entries were accompanied by papers (Alves et al., 2014; Beltagy"
2020.lrec-1.845,D17-1070,0,0.377036,"of great importance to the overall performance of the approach. However, the model used in that work is highly specialised to the evaluation corpus and unlikely to perform as well in different scenarios. This work examines existing approaches to measuring sentence similarity and determines how well they capture information relevant to contextual paraphrasing tasks in a more general setting. To this end, we evaluate four models: sentence similarity based on semantic nets and corpus statistics (Li et al., 2006), BERT (Devlin et al., 2018), skipthought vectors (Kiros et al., 2015) and InferSent (Conneau et al., 2017). We test their performance regarding paraphrase classification, dialogue act clustering and sentence swapping, and provide an in depth discussion of the implications of our findings. In the following, we first discuss related work in Section 2. Section 3 gives an overview of the chosen sentence embedding models, followed by a description of our evaluation approaches and discussion of our findings in Section 4. Finally, we summarise our contribution and outline future work. 2. Related Work The evaluation of sentence embeddings is often performed at the time of their introduction with regard to"
2020.lrec-1.845,L18-1218,0,0.18761,"r unrelated. The ground truth is usually given by a corpus of sentences pairs with corresponding labels. We perform the evaluation of the paraphrase classification task in two steps: first, a traditional paraphrase classification task is performed on the MSR paraphrase corpus (Dolan et al., 2004). This corpus contains 5,800 sentences pairs from news sources on the web, human-annotated as either paraphrases or unrelated. The results of this first part serve as comparison for the results obtained in the second part, the contextual paraphrase classification task. Here, we utilise the Opusparcus (Creutz, 2018) corpus. Opusparcus is a paraphrase corpus for six languages, including English. The sentence pairs are extracted from the Opensubtitles2016 corpus (Lison and Tiedemann, 2016), a corpus of movie and TV subtitles. Opusparcus consists of manually annotated development and test sets, as well as a larger automatically ranked training set. The annotations consist of four categories that rate the degree to which two sentences are paraphrases, ‘good’, ‘mostly good’, ‘mostly bad’ and ‘bad’. As opposed to a binary classification, this annotation allows for sentences that are contextual paraphrases to b"
2020.lrec-1.845,C04-1051,0,0.508643,"the semantic relatedness subtask and 18 for the entailment subtask. However, only 14 of those entries were accompanied by papers (Alves et al., 2014; Beltagy et al., 2014; Bestgen, 2014; Bic¸ici and Way, 2014; Bjerva et al., 2014; Ferrone and Zanzotto, 2014; Gupta et al., 2014; Jimenez et al., 2014; Lai and Hockenmaier, 2014; Le´on et al., 2014; 6841 Lien and Kouylekov, 2014; Proisl et al., 2014; Vo et al., 2014; Zhao et al., 2014). Often, evaluations of the semantic meaning of sentence embeddings are carried out as classification task on paraphrase corpora such as the MSR paraphrase corpus (Dolan et al., 2004). Another, similar option is the natural language entailment task where, instead of labelling sentence pairs as either paraphrases or unrelated, they are labelled as either entailment, neutral or contradiction. Such corpora include the Stanford Natural Language Inference corpus (Bowman et al., 2015) and the Multi-Genre NLI corpus (Williams et al., 2018), which has been used in the RepEval 2017 Shared Task. The SICK corpus (Marelli et al., 2014) developed for SemEval-2014 Task 1 expands entailment annotations by more fine-grained, human-annotated semantic relatedness scores. In our work, we inc"
2020.lrec-1.845,S14-2049,0,0.0118311,"2017) on a shared task. Those works operate on a strict definition of sentence paraphrases: each sentence must entail the other to be considered a paraphrase. The first task of SemEval-2014 (Marelli et al., 2014) expands on this definition by adding a semantic relatedness score to sentence pairs. The task was solved by 21 participating teams, with 17 submission for the semantic relatedness subtask and 18 for the entailment subtask. However, only 14 of those entries were accompanied by papers (Alves et al., 2014; Beltagy et al., 2014; Bestgen, 2014; Bic¸ici and Way, 2014; Bjerva et al., 2014; Ferrone and Zanzotto, 2014; Gupta et al., 2014; Jimenez et al., 2014; Lai and Hockenmaier, 2014; Le´on et al., 2014; 6841 Lien and Kouylekov, 2014; Proisl et al., 2014; Vo et al., 2014; Zhao et al., 2014). Often, evaluations of the semantic meaning of sentence embeddings are carried out as classification task on paraphrase corpora such as the MSR paraphrase corpus (Dolan et al., 2004). Another, similar option is the natural language entailment task where, instead of labelling sentence pairs as either paraphrases or unrelated, they are labelled as either entailment, neutral or contradiction. Such corpora include the Sta"
2020.lrec-1.845,S14-2139,0,0.0410817,"Missing"
2020.lrec-1.845,D13-1090,0,0.0302725,"Presumably, BERT lacks fine-tuning, while STV puts too little focus on the word level to perform well on this task. We decided to use the UAR for our evaluation as it is robust with regard to unbalanced class sizes. However, many previous works have used Accuracy and F-score as performance metrics. To put our results in context, we provide those values for our chosen approaches in Table 1, as well as the worst and best approaches for Paraphrase Identification according to https://aclweb.org/aclwiki/State of the art (as of 28.02.2020): Vector Based Similarity (Mihalcea et al., 2006) and TF-KLD(Ji and Eisenstein, 2013). Our chosen approaches show moderate success for this task compared to others available. The evaluation of the contextual paraphrase task results in a different ranking. As Figure 3 shows, the best result is again achieved by InfS. With an average UAR of 0.42, it performs significantly better than the next best model, STV, achieving an UAR of 0.36 (t(108.24) = 6.36, p &lt; .001). While InfS can maintain its status, SNCS does not seem to be as well suited for this task, performing significantly worse than STV with an UAR of 0.29 (t(105.40) = 7.36, p &lt; .001). A majority class predictor would achie"
2020.lrec-1.845,S14-2131,0,0.0354951,"Missing"
2020.lrec-1.845,P14-1062,0,0.0184407,"ed semantic relatedness scores. In our work, we include a greater number of sentence pairs in our understanding of paraphrases: two sentences are considered contextual paraphrases if they can fulfil the same function in the context of a conversation. 3. Sentence Similarity Models Several approaches to sentence encoding exist, however, not all of them are equally promising to perform well on contextual paraphrasing tasks. Many focus mainly on word-level features such as lexical similarity or word order (e.g. (Sutskever et al., 2014; Palangi et al., 2016; Tsunoo et al., 2017; Shen et al., 2014; Kalchbrenner et al., 2014; Hu et al., 2014; Socher et al., 2011)). The identification of contextual paraphrases is likely to be dependent mainly on the ability of a model to capture functional similarity. Therefore, we choose the following four approaches for our comparative study: a similarity measure based on semantic nets and corpus statistics (Li et al., 2006), BERT (Devlin et al., 2018), skip-thought vectors (Kiros et al., 2015) and InferSent (Conneau et al., 2017). Those models do not rely solely on word similarities for their encoding, but take context into account. In the following, a short overview of the cho"
2020.lrec-1.845,W03-1601,0,0.17371,"Those variants can be considered word-level paraphrases. However, the sentence ‘I just got on the bus.’ can fulfil the same function of providing the expected time of arrival, while differing greatly with regard to syntactic structure and the words used. We refer to such sentences that can fulfil the same function in the context of a conversation despite being dissimilar in their surface realisation as contextual paraphrases. In the area of dialogue systems, a number of contributions are concerned with the generation of variety in the utterances of the dialogue system (e.g (Wen et al., 2015; Kozlowski et al., 2003; Langkilde and Knight, 1998)). However, those efforts are mainly focused on word-level paraphrases and little work has been dedicated to the generation of contextual paraphrases. Pragst and Ultes (2018) have made a first effort in this direction by proposing an approach for exchanging sentences, using a dialogue vector model to assess whether two sentences can be used interchangeably. They find that the ability to identify contextual paraphrases is of great importance to the overall performance of the approach. However, the model used in that work is highly specialised to the evaluation corpu"
2020.lrec-1.845,S14-2055,0,0.0127612,"sentence paraphrases: each sentence must entail the other to be considered a paraphrase. The first task of SemEval-2014 (Marelli et al., 2014) expands on this definition by adding a semantic relatedness score to sentence pairs. The task was solved by 21 participating teams, with 17 submission for the semantic relatedness subtask and 18 for the entailment subtask. However, only 14 of those entries were accompanied by papers (Alves et al., 2014; Beltagy et al., 2014; Bestgen, 2014; Bic¸ici and Way, 2014; Bjerva et al., 2014; Ferrone and Zanzotto, 2014; Gupta et al., 2014; Jimenez et al., 2014; Lai and Hockenmaier, 2014; Le´on et al., 2014; 6841 Lien and Kouylekov, 2014; Proisl et al., 2014; Vo et al., 2014; Zhao et al., 2014). Often, evaluations of the semantic meaning of sentence embeddings are carried out as classification task on paraphrase corpora such as the MSR paraphrase corpus (Dolan et al., 2004). Another, similar option is the natural language entailment task where, instead of labelling sentence pairs as either paraphrases or unrelated, they are labelled as either entailment, neutral or contradiction. Such corpora include the Stanford Natural Language Inference corpus (Bowman et al., 2015) and the"
2020.lrec-1.845,P98-1116,0,0.451854,"onsidered word-level paraphrases. However, the sentence ‘I just got on the bus.’ can fulfil the same function of providing the expected time of arrival, while differing greatly with regard to syntactic structure and the words used. We refer to such sentences that can fulfil the same function in the context of a conversation despite being dissimilar in their surface realisation as contextual paraphrases. In the area of dialogue systems, a number of contributions are concerned with the generation of variety in the utterances of the dialogue system (e.g (Wen et al., 2015; Kozlowski et al., 2003; Langkilde and Knight, 1998)). However, those efforts are mainly focused on word-level paraphrases and little work has been dedicated to the generation of contextual paraphrases. Pragst and Ultes (2018) have made a first effort in this direction by proposing an approach for exchanging sentences, using a dialogue vector model to assess whether two sentences can be used interchangeably. They find that the ability to identify contextual paraphrases is of great importance to the overall performance of the approach. However, the model used in that work is highly specialised to the evaluation corpus and unlikely to perform as"
2020.lrec-1.845,S14-2021,0,0.0606465,"Missing"
2020.lrec-1.845,S14-2125,0,0.064281,"other to be considered a paraphrase. The first task of SemEval-2014 (Marelli et al., 2014) expands on this definition by adding a semantic relatedness score to sentence pairs. The task was solved by 21 participating teams, with 17 submission for the semantic relatedness subtask and 18 for the entailment subtask. However, only 14 of those entries were accompanied by papers (Alves et al., 2014; Beltagy et al., 2014; Bestgen, 2014; Bic¸ici and Way, 2014; Bjerva et al., 2014; Ferrone and Zanzotto, 2014; Gupta et al., 2014; Jimenez et al., 2014; Lai and Hockenmaier, 2014; Le´on et al., 2014; 6841 Lien and Kouylekov, 2014; Proisl et al., 2014; Vo et al., 2014; Zhao et al., 2014). Often, evaluations of the semantic meaning of sentence embeddings are carried out as classification task on paraphrase corpora such as the MSR paraphrase corpus (Dolan et al., 2004). Another, similar option is the natural language entailment task where, instead of labelling sentence pairs as either paraphrases or unrelated, they are labelled as either entailment, neutral or contradiction. Such corpora include the Stanford Natural Language Inference corpus (Bowman et al., 2015) and the Multi-Genre NLI corpus (Williams et al., 2018), wh"
2020.lrec-1.845,L16-1147,0,0.0609891,"n task in two steps: first, a traditional paraphrase classification task is performed on the MSR paraphrase corpus (Dolan et al., 2004). This corpus contains 5,800 sentences pairs from news sources on the web, human-annotated as either paraphrases or unrelated. The results of this first part serve as comparison for the results obtained in the second part, the contextual paraphrase classification task. Here, we utilise the Opusparcus (Creutz, 2018) corpus. Opusparcus is a paraphrase corpus for six languages, including English. The sentence pairs are extracted from the Opensubtitles2016 corpus (Lison and Tiedemann, 2016), a corpus of movie and TV subtitles. Opusparcus consists of manually annotated development and test sets, as well as a larger automatically ranked training set. The annotations consist of four categories that rate the degree to which two sentences are paraphrases, ‘good’, ‘mostly good’, ‘mostly bad’ and ‘bad’. As opposed to a binary classification, this annotation allows for sentences that are contextual paraphrases to be rated accordingly. Most contextual paraphrases can be found in the category ‘mostly good’, for example the pairs ‘No different.’/‘That ’s the same thing.’ or ‘I think I got"
2020.lrec-1.845,S14-2001,0,0.103578,"There have been some efforts to address this issue, e.g. (White et al., 2015). Here, the authors evaluate sentence embeddings with a semantic classification task in a generalised manner. Additionally, the RepEval 2017 Shared Task (Nangia et al., 2017) compares the performance of seven sentence embedding approaches (Chen et al., 2017; Nie and Bansal, 2017; Balazs et al., 2017; Vu et al., 2017; Yang et al., 2017) on a shared task. Those works operate on a strict definition of sentence paraphrases: each sentence must entail the other to be considered a paraphrase. The first task of SemEval-2014 (Marelli et al., 2014) expands on this definition by adding a semantic relatedness score to sentence pairs. The task was solved by 21 participating teams, with 17 submission for the semantic relatedness subtask and 18 for the entailment subtask. However, only 14 of those entries were accompanied by papers (Alves et al., 2014; Beltagy et al., 2014; Bestgen, 2014; Bic¸ici and Way, 2014; Bjerva et al., 2014; Ferrone and Zanzotto, 2014; Gupta et al., 2014; Jimenez et al., 2014; Lai and Hockenmaier, 2014; Le´on et al., 2014; 6841 Lien and Kouylekov, 2014; Proisl et al., 2014; Vo et al., 2014; Zhao et al., 2014). Often,"
2020.lrec-1.845,W17-5308,0,0.0173111,"cannot be found in a single work but must be gathered from numerous individual contributions. Furthermore, the same procedure is not necessarily used across evaluations, further impeding a thorough understanding of advantages and disadvantages of any given model. There have been some efforts to address this issue, e.g. (White et al., 2015). Here, the authors evaluate sentence embeddings with a semantic classification task in a generalised manner. Additionally, the RepEval 2017 Shared Task (Nangia et al., 2017) compares the performance of seven sentence embedding approaches (Chen et al., 2017; Nie and Bansal, 2017; Balazs et al., 2017; Vu et al., 2017; Yang et al., 2017) on a shared task. Those works operate on a strict definition of sentence paraphrases: each sentence must entail the other to be considered a paraphrase. The first task of SemEval-2014 (Marelli et al., 2014) expands on this definition by adding a semantic relatedness score to sentence pairs. The task was solved by 21 participating teams, with 17 submission for the semantic relatedness subtask and 18 for the entailment subtask. However, only 14 of those entries were accompanied by papers (Alves et al., 2014; Beltagy et al., 2014; Bestgen"
2020.lrec-1.845,W18-5002,1,0.942245,"ith regard to syntactic structure and the words used. We refer to such sentences that can fulfil the same function in the context of a conversation despite being dissimilar in their surface realisation as contextual paraphrases. In the area of dialogue systems, a number of contributions are concerned with the generation of variety in the utterances of the dialogue system (e.g (Wen et al., 2015; Kozlowski et al., 2003; Langkilde and Knight, 1998)). However, those efforts are mainly focused on word-level paraphrases and little work has been dedicated to the generation of contextual paraphrases. Pragst and Ultes (2018) have made a first effort in this direction by proposing an approach for exchanging sentences, using a dialogue vector model to assess whether two sentences can be used interchangeably. They find that the ability to identify contextual paraphrases is of great importance to the overall performance of the approach. However, the model used in that work is highly specialised to the evaluation corpus and unlikely to perform as well in different scenarios. This work examines existing approaches to measuring sentence similarity and determines how well they capture information relevant to contextual p"
2020.lrec-1.845,S14-2093,0,0.0332972,"Missing"
2020.lrec-1.845,D11-1014,0,0.0210413,"we include a greater number of sentence pairs in our understanding of paraphrases: two sentences are considered contextual paraphrases if they can fulfil the same function in the context of a conversation. 3. Sentence Similarity Models Several approaches to sentence encoding exist, however, not all of them are equally promising to perform well on contextual paraphrasing tasks. Many focus mainly on word-level features such as lexical similarity or word order (e.g. (Sutskever et al., 2014; Palangi et al., 2016; Tsunoo et al., 2017; Shen et al., 2014; Kalchbrenner et al., 2014; Hu et al., 2014; Socher et al., 2011)). The identification of contextual paraphrases is likely to be dependent mainly on the ability of a model to capture functional similarity. Therefore, we choose the following four approaches for our comparative study: a similarity measure based on semantic nets and corpus statistics (Li et al., 2006), BERT (Devlin et al., 2018), skip-thought vectors (Kiros et al., 2015) and InferSent (Conneau et al., 2017). Those models do not rely solely on word similarities for their encoding, but take context into account. In the following, a short overview of the chosen approaches as well as a discussion"
2020.lrec-1.845,J00-3003,0,0.0886784,"Missing"
2020.lrec-1.845,S14-2047,0,0.012785,"task of SemEval-2014 (Marelli et al., 2014) expands on this definition by adding a semantic relatedness score to sentence pairs. The task was solved by 21 participating teams, with 17 submission for the semantic relatedness subtask and 18 for the entailment subtask. However, only 14 of those entries were accompanied by papers (Alves et al., 2014; Beltagy et al., 2014; Bestgen, 2014; Bic¸ici and Way, 2014; Bjerva et al., 2014; Ferrone and Zanzotto, 2014; Gupta et al., 2014; Jimenez et al., 2014; Lai and Hockenmaier, 2014; Le´on et al., 2014; 6841 Lien and Kouylekov, 2014; Proisl et al., 2014; Vo et al., 2014; Zhao et al., 2014). Often, evaluations of the semantic meaning of sentence embeddings are carried out as classification task on paraphrase corpora such as the MSR paraphrase corpus (Dolan et al., 2004). Another, similar option is the natural language entailment task where, instead of labelling sentence pairs as either paraphrases or unrelated, they are labelled as either entailment, neutral or contradiction. Such corpora include the Stanford Natural Language Inference corpus (Bowman et al., 2015) and the Multi-Genre NLI corpus (Williams et al., 2018), which has been used in the RepEval 2017"
2020.lrec-1.845,D15-1199,0,0.040663,"Missing"
2020.lrec-1.845,N18-1101,0,0.0164121,"1 Lien and Kouylekov, 2014; Proisl et al., 2014; Vo et al., 2014; Zhao et al., 2014). Often, evaluations of the semantic meaning of sentence embeddings are carried out as classification task on paraphrase corpora such as the MSR paraphrase corpus (Dolan et al., 2004). Another, similar option is the natural language entailment task where, instead of labelling sentence pairs as either paraphrases or unrelated, they are labelled as either entailment, neutral or contradiction. Such corpora include the Stanford Natural Language Inference corpus (Bowman et al., 2015) and the Multi-Genre NLI corpus (Williams et al., 2018), which has been used in the RepEval 2017 Shared Task. The SICK corpus (Marelli et al., 2014) developed for SemEval-2014 Task 1 expands entailment annotations by more fine-grained, human-annotated semantic relatedness scores. In our work, we include a greater number of sentence pairs in our understanding of paraphrases: two sentences are considered contextual paraphrases if they can fulfil the same function in the context of a conversation. 3. Sentence Similarity Models Several approaches to sentence encoding exist, however, not all of them are equally promising to perform well on contextual p"
2020.lrec-1.845,W17-5309,0,0.0488764,"Missing"
2020.lrec-1.845,S14-2044,0,0.0178977,"2014 (Marelli et al., 2014) expands on this definition by adding a semantic relatedness score to sentence pairs. The task was solved by 21 participating teams, with 17 submission for the semantic relatedness subtask and 18 for the entailment subtask. However, only 14 of those entries were accompanied by papers (Alves et al., 2014; Beltagy et al., 2014; Bestgen, 2014; Bic¸ici and Way, 2014; Bjerva et al., 2014; Ferrone and Zanzotto, 2014; Gupta et al., 2014; Jimenez et al., 2014; Lai and Hockenmaier, 2014; Le´on et al., 2014; 6841 Lien and Kouylekov, 2014; Proisl et al., 2014; Vo et al., 2014; Zhao et al., 2014). Often, evaluations of the semantic meaning of sentence embeddings are carried out as classification task on paraphrase corpora such as the MSR paraphrase corpus (Dolan et al., 2004). Another, similar option is the natural language entailment task where, instead of labelling sentence pairs as either paraphrases or unrelated, they are labelled as either entailment, neutral or contradiction. Such corpora include the Stanford Natural Language Inference corpus (Bowman et al., 2015) and the Multi-Genre NLI corpus (Williams et al., 2018), which has been used in the RepEval 2017 Shared Task. The SIC"
2021.sigdial-1.39,2020.acl-main.371,0,0.0153061,"Results for the annotated structure and the automatically generated ones on the topic Marriage. Upper table: Ratio of positive and overall ratings. Lower table: p-values of pairwise comparison with Fisher’s exact test and Benjamini-Hochberg correction. As for the written feedback, multiple annotators reported confusing formulations of the argument as the major difficulty of the task. Since this is a direct consequence of the heterogeneous sources the arguments are retrieved from, it is hard to address in the pipeline. Therefore, approaches to automatically summarize or reformulate arguments (Bar-Haim et al., 2020; Schiller et al., 2021) could be beneficial to improve the performance. tree is directly influenced by the relation classification (and hence by the clustering as well), it varies between the structures with and without clustering. Therefore, the individual arguments can appear in a different context, which arguably also leads to a different perception through the study participants. On average, no significant difference between the two approaches could be found and the choice of the optimal configuration hence depends on the available data for each topic. The direct comparison with an annota"
2021.sigdial-1.39,W15-0504,0,0.0410028,"Missing"
2021.sigdial-1.39,P19-3022,0,0.0113662,"ning (Lawrence and Reed, 2020). Argument search engines provide users with a (ranked) list of arguments related to a given search query, in some instances also including their stance/polarity towards the topic. 369 3.1 General Approach Over the last years, different approaches to argument search were investigated that follow different paradigms (Ajjour et al., 2019). Systems introduced so far include the one developed in the scope of IBM project debater (Levy et al., 2018), ArgumenText (Stab et al., 2018), args.me (Wachsmuth et al., 2017b), TARGER (Chernodub et al., 2019) and PerspectroScope (Chen et al., 2019). The general applicability of argument search engines in the context of dialogue systems was assessed in (Rach et al., 2020a) where ArgumenText and args.me were compared to a baseline system. Although a mapping into argument structures was not addressed, we use the discussed results to select a suitable search engine for the present work. Our model of choice is ArgumenText since it retrieves arguments on a sentence level (which is preferable in a dialogue context), performs reliable in comparison with the investigated baseline and additionally provides an API that allows for clustering the re"
2021.sigdial-1.39,P19-3031,0,0.0150314,"an application from the field of argument mining (Lawrence and Reed, 2020). Argument search engines provide users with a (ranked) list of arguments related to a given search query, in some instances also including their stance/polarity towards the topic. 369 3.1 General Approach Over the last years, different approaches to argument search were investigated that follow different paradigms (Ajjour et al., 2019). Systems introduced so far include the one developed in the scope of IBM project debater (Levy et al., 2018), ArgumenText (Stab et al., 2018), args.me (Wachsmuth et al., 2017b), TARGER (Chernodub et al., 2019) and PerspectroScope (Chen et al., 2019). The general applicability of argument search engines in the context of dialogue systems was assessed in (Rach et al., 2020a) where ArgumenText and args.me were compared to a baseline system. Although a mapping into argument structures was not addressed, we use the discussed results to select a suitable search engine for the present work. Our model of choice is ArgumenText since it retrieves arguments on a sentence level (which is preferable in a dialogue context), performs reliable in comparison with the investigated baseline and additionally provides"
2021.sigdial-1.39,D18-1241,0,0.0124862,"uctures that encode arguments and their relations to ensure a consistent and challenging interaction (Rach et al., 2018b; Sakai et al., 2020). Despite the advantages on the formal side, this dependency limits the range of topics that can be discussed by a system as the required structures are often either annotated by hand (Rach et al., 2019; Sakai et al., 2018b) or acquired in time-consuming data collections (Chalaguine and Hunter, 2019). This limitation renders the corresponding systems inflexible, especially in comparison to recent data-driven approaches in domains like question answering (Choi et al., 2018). To address this issue, we propose a combination of argument search technology (Ajjour et al., 2019) with dialogue systems of the discussed kind. Our approach maps the list of pro and con arguments retrieved with an argument search engine for a given topic into a general tree structure that encodes bipolar relations (support and attack) between the individual arguments (see Figure 1). In doing so, our approach combines the strong points of both data-driven and formal models for argumentation and enables a corresponding system to discuss literally any topic on which the search engine can find"
2021.sigdial-1.39,N19-1423,0,0.0112947,"Missing"
2021.sigdial-1.39,W19-4004,0,0.0211506,"ing is good and compared different combinations of the approaches to create the tree structure. Clustering prior to the relation classification was not considered in this step, as it is investigated thoroughly in the final evaluation. Five annotators without task-related background were asked to label each argument pair with a relation in the resulting tree structure in each of the annotation categories contradiction, entailment, specificity, paraphrase and local relevance with yes or no. The first four categories are based on an investigation of the interactions between semantic relations by Gold et al. (2019), the last category was proposed in (Wachsmuth et al., 2017a). As in this latter work, we use the labels of the three most agreeing annotators for each category in order to eliminate outliers. The Fleiss’ Kappa (Fleiss, 1971) values yields a substantial (0.66) up to perfect (0.82) agreement (Landis and Koch, 1977). A pair of arguments is concluded to actually hold a relation if it is rated with yes in at least one category by majority vote. For our baseline (SVM), this is the case with BILP 371 Speech Act Attacks Surrenders claim(φi ) why(φi ) concede(φi ) retract(φi ) argue(φj → φi ) argue ex"
2021.sigdial-1.39,W18-5215,0,0.0181282,"in their system. Although the underlying formal frameworks of all these systems allow for complex dialogues, the topics that can be addressed are limited by the time-consuming generation of the argument structures. We propose an approach to generate structures of this kind automatically and independently of a specific topic. In addition, data-based approaches were also investigated. The chatbot introduced by Rakshit et al. (2019) utilizes semantic similarity measures to retrieve arguments from an argument corpus to generate a response. A similar approach was compared to a generative model by Le et al. (2018) that was trained on a corpus of debate posts on various topics. Although especially the generative approach is focused on providing topic flexibility, aspects like user adaptation or strategy optimization as addressed in some of the previously discussed works are not (yet) considered in these systems. Our approach bridges the gap between formal and data-driven argumentation through a combination of argument search with formal models. Related Work 3 This section provides an overview of related work with a focus on argument retrieval for argumentative dialogue systems. The arguably most promine"
2021.sigdial-1.39,C18-1176,0,0.0239379,"investigated in different scenarios. The Argument Search Argument search has recently evolved as an application from the field of argument mining (Lawrence and Reed, 2020). Argument search engines provide users with a (ranked) list of arguments related to a given search query, in some instances also including their stance/polarity towards the topic. 369 3.1 General Approach Over the last years, different approaches to argument search were investigated that follow different paradigms (Ajjour et al., 2019). Systems introduced so far include the one developed in the scope of IBM project debater (Levy et al., 2018), ArgumenText (Stab et al., 2018), args.me (Wachsmuth et al., 2017b), TARGER (Chernodub et al., 2019) and PerspectroScope (Chen et al., 2019). The general applicability of argument search engines in the context of dialogue systems was assessed in (Rach et al., 2020a) where ArgumenText and args.me were compared to a baseline system. Although a mapping into argument structures was not addressed, we use the discussed results to select a suitable search engine for the present work. Our model of choice is ArgumenText since it retrieves arguments on a sentence level (which is preferable in a dialogu"
2021.sigdial-1.39,2020.lrec-1.65,1,0.880032,"arch query, in some instances also including their stance/polarity towards the topic. 369 3.1 General Approach Over the last years, different approaches to argument search were investigated that follow different paradigms (Ajjour et al., 2019). Systems introduced so far include the one developed in the scope of IBM project debater (Levy et al., 2018), ArgumenText (Stab et al., 2018), args.me (Wachsmuth et al., 2017b), TARGER (Chernodub et al., 2019) and PerspectroScope (Chen et al., 2019). The general applicability of argument search engines in the context of dialogue systems was assessed in (Rach et al., 2020a) where ArgumenText and args.me were compared to a baseline system. Although a mapping into argument structures was not addressed, we use the discussed results to select a suitable search engine for the present work. Our model of choice is ArgumenText since it retrieves arguments on a sentence level (which is preferable in a dialogue context), performs reliable in comparison with the investigated baseline and additionally provides an API that allows for clustering the retrieved arguments thematically. In the following, a sentence retrieved by the search engine is denoted as argument φ and its"
2021.sigdial-1.39,D19-1410,0,0.0120256,"ment (Stab et al., 2018). Besides the arguments and their stance, the search engine also provides multiple confidence values of which we use the one for stance (cs ) and argument detection (ca ) to derive the final confidence as c = ca × cs and rank the retrieved arguments accordingly. In addition, we utilize ArgumenText’s Cluster API to group the retrieved arguments thematically. It determines similarity scores for argument pairs which are then applied to form clusters based on aspects addressed within the arguments. The Cluster API relies on an optimized version of the Sentence-BERT method (Reimers and Gurevych, 2019) that makes use of an efficient bi-encoder that has been trained with additional samples (“Augmented SBERT”) from a cross-encoder (Thakur et al., 2021). The utilized supervised approach to learn argument similarity was shown to outperform 1 2 https://api.argumentsearch.com http://commoncrawl.org unsupervised approaches based on BERT embeddings by 10pp (Reimers et al., 2019). 4 From Arguments to Structures In the following, the mapping of the retrieved arguments into an argument structure is discussed. Although some structures utilized by the systems discussed in Section 2 differ to a certain e"
2021.sigdial-1.39,P19-1054,1,0.815101,". It determines similarity scores for argument pairs which are then applied to form clusters based on aspects addressed within the arguments. The Cluster API relies on an optimized version of the Sentence-BERT method (Reimers and Gurevych, 2019) that makes use of an efficient bi-encoder that has been trained with additional samples (“Augmented SBERT”) from a cross-encoder (Thakur et al., 2021). The utilized supervised approach to learn argument similarity was shown to outperform 1 2 https://api.argumentsearch.com http://commoncrawl.org unsupervised approaches based on BERT embeddings by 10pp (Reimers et al., 2019). 4 From Arguments to Structures In the following, the mapping of the retrieved arguments into an argument structure is discussed. Although some structures utilized by the systems discussed in Section 2 differ to a certain extent, they all require information about the relations between the individual arguments. We hence pursue a modular pipeline approach that first determines possible relations between the arguments and subsequently maps them into a specific structure. In case the required structure cannot be inferred from the herein discussed one, the second module can be adapted accordingly"
2021.sigdial-1.39,C14-1142,0,0.0152276,"extent, they all require information about the relations between the individual arguments. We hence pursue a modular pipeline approach that first determines possible relations between the arguments and subsequently maps them into a specific structure. In case the required structure cannot be inferred from the herein discussed one, the second module can be adapted accordingly. This section builds on the work in (Schindler, 2020). The code of the complete pipeline is publicly available3 . 4.1 Target Structure The herein considered target structure is based on the argument annotation scheme in (Stab and Gurevych, 2014), which distinguishes three different types of argument components (Major Claim, Claim, Premise) and two directed relations between them (support and attack). Each component has one unique relation towards another component but can be targeted by multiple others. To keep the structure as general as possible, we abstract from this framework in the sense that we are not distinguishing different component types for the retrieved arguments and only focus on finding the best fitting relation of each component towards another (or the main topic, i.e. the search query). Consequently, the resulting st"
2021.sigdial-1.39,J17-3005,0,0.0511466,"Missing"
2021.sigdial-1.39,W18-5008,0,0.155158,"uding full scale debates against a human debater (Slonim et al., 2021), persuasion (Chalaguine and Hunter, 2020) and customer support (Galitsky, 2019). Due to the complex nature of this type of interaction, many systems rely on topic-specific argument structures that encode arguments and their relations to ensure a consistent and challenging interaction (Rach et al., 2018b; Sakai et al., 2020). Despite the advantages on the formal side, this dependency limits the range of topics that can be discussed by a system as the required structures are often either annotated by hand (Rach et al., 2019; Sakai et al., 2018b) or acquired in time-consuming data collections (Chalaguine and Hunter, 2019). This limitation renders the corresponding systems inflexible, especially in comparison to recent data-driven approaches in domains like question answering (Choi et al., 2018). To address this issue, we propose a combination of argument search technology (Ajjour et al., 2019) with dialogue systems of the discussed kind. Our approach maps the list of pro and con arguments retrieved with an argument search engine for a given topic into a general tree structure that encodes bipolar relations (support and attack) betwe"
2021.sigdial-1.39,2021.naacl-main.28,1,0.80651,"e (cs ) and argument detection (ca ) to derive the final confidence as c = ca × cs and rank the retrieved arguments accordingly. In addition, we utilize ArgumenText’s Cluster API to group the retrieved arguments thematically. It determines similarity scores for argument pairs which are then applied to form clusters based on aspects addressed within the arguments. The Cluster API relies on an optimized version of the Sentence-BERT method (Reimers and Gurevych, 2019) that makes use of an efficient bi-encoder that has been trained with additional samples (“Augmented SBERT”) from a cross-encoder (Thakur et al., 2021). The utilized supervised approach to learn argument similarity was shown to outperform 1 2 https://api.argumentsearch.com http://commoncrawl.org unsupervised approaches based on BERT embeddings by 10pp (Reimers et al., 2019). 4 From Arguments to Structures In the following, the mapping of the retrieved arguments into an argument structure is discussed. Although some structures utilized by the systems discussed in Section 2 differ to a certain extent, they all require information about the relations between the individual arguments. We hence pursue a modular pipeline approach that first determ"
2021.sigdial-1.39,L18-1627,0,0.0248159,"Missing"
2021.sigdial-1.39,2021.naacl-main.34,1,0.731978,"ted structure and the automatically generated ones on the topic Marriage. Upper table: Ratio of positive and overall ratings. Lower table: p-values of pairwise comparison with Fisher’s exact test and Benjamini-Hochberg correction. As for the written feedback, multiple annotators reported confusing formulations of the argument as the major difficulty of the task. Since this is a direct consequence of the heterogeneous sources the arguments are retrieved from, it is hard to address in the pipeline. Therefore, approaches to automatically summarize or reformulate arguments (Bar-Haim et al., 2020; Schiller et al., 2021) could be beneficial to improve the performance. tree is directly influenced by the relation classification (and hence by the clustering as well), it varies between the structures with and without clustering. Therefore, the individual arguments can appear in a different context, which arguably also leads to a different perception through the study participants. On average, no significant difference between the two approaches could be found and the choice of the optimal configuration hence depends on the available data for each topic. The direct comparison with an annotated structure revealed r"
2021.sigdial-1.39,N18-5005,1,0.749415,"Missing"
2021.sigdial-1.39,E17-1017,0,0.425777,"ent search has recently evolved as an application from the field of argument mining (Lawrence and Reed, 2020). Argument search engines provide users with a (ranked) list of arguments related to a given search query, in some instances also including their stance/polarity towards the topic. 369 3.1 General Approach Over the last years, different approaches to argument search were investigated that follow different paradigms (Ajjour et al., 2019). Systems introduced so far include the one developed in the scope of IBM project debater (Levy et al., 2018), ArgumenText (Stab et al., 2018), args.me (Wachsmuth et al., 2017b), TARGER (Chernodub et al., 2019) and PerspectroScope (Chen et al., 2019). The general applicability of argument search engines in the context of dialogue systems was assessed in (Rach et al., 2020a) where ArgumenText and args.me were compared to a baseline system. Although a mapping into argument structures was not addressed, we use the discussed results to select a suitable search engine for the present work. Our model of choice is ArgumenText since it retrieves arguments on a sentence level (which is preferable in a dialogue context), performs reliable in comparison with the investigated"
2021.sigdial-1.39,W17-5106,0,0.381848,"ent search has recently evolved as an application from the field of argument mining (Lawrence and Reed, 2020). Argument search engines provide users with a (ranked) list of arguments related to a given search query, in some instances also including their stance/polarity towards the topic. 369 3.1 General Approach Over the last years, different approaches to argument search were investigated that follow different paradigms (Ajjour et al., 2019). Systems introduced so far include the one developed in the scope of IBM project debater (Levy et al., 2018), ArgumenText (Stab et al., 2018), args.me (Wachsmuth et al., 2017b), TARGER (Chernodub et al., 2019) and PerspectroScope (Chen et al., 2019). The general applicability of argument search engines in the context of dialogue systems was assessed in (Rach et al., 2020a) where ArgumenText and args.me were compared to a baseline system. Although a mapping into argument structures was not addressed, we use the discussed results to select a suitable search engine for the present work. Our model of choice is ArgumenText since it retrieves arguments on a sentence level (which is preferable in a dialogue context), performs reliable in comparison with the investigated"
bertrand-etal-2010-towards,H01-1028,0,\N,Missing
dybkjaer-etal-2004-usability,P97-1035,0,\N,Missing
dybkjaer-etal-2004-usability,A00-1046,0,\N,Missing
dybkjaer-etal-2004-usability,bernsen-etal-2004-evaluating,1,\N,Missing
heinroth-etal-2012-adaptive,W07-0305,0,\N,Missing
heinroth-etal-2012-adaptive,P99-1026,0,\N,Missing
heinroth-etal-2012-adaptive,heinroth-etal-2010-efficient,1,\N,Missing
I17-1092,W03-1601,0,0.0435491,"lection of System Actions User Action Elaborateness/Indirectness Generator Core Statement Knowledge Integration Subject/Predicate/Object Related RDF Triplets Knowledge Base Figure 1: Partial architecture of the KRISTINA system, enhanced by the proposed algorithm. tences from a more structured representation. With regard to surface realisation, one characteristic of good generators is their ability to provide variation in the generated sentences, which has been explored, among others, by Wen et al. (2015). With a similar goal, efforts towards the paraphrasing of sentences have been made (e.g. (Kozlowski et al., 2003; Langkilde and Knight, 1998)). Those approaches provide variation at the word level and preserve the semantic content of a sentence. They are complemented by our approach that focuses on variations of the semantic content of a system action. The content selection task is concerned with choosing relevant information that is to be communicated in the generated text, often with the goal of creating summaries (e.g. (Duboue and McKeown, 2003; Barzilay and Lapata, 2005)). While this research area certainly provides important insights to the generation of elaborateness, they need to be considered wi"
I17-1092,P98-1116,0,0.0767059,"s User Action Elaborateness/Indirectness Generator Core Statement Knowledge Integration Subject/Predicate/Object Related RDF Triplets Knowledge Base Figure 1: Partial architecture of the KRISTINA system, enhanced by the proposed algorithm. tences from a more structured representation. With regard to surface realisation, one characteristic of good generators is their ability to provide variation in the generated sentences, which has been explored, among others, by Wen et al. (2015). With a similar goal, efforts towards the paraphrasing of sentences have been made (e.g. (Kozlowski et al., 2003; Langkilde and Knight, 1998)). Those approaches provide variation at the word level and preserve the semantic content of a sentence. They are complemented by our approach that focuses on variations of the semantic content of a system action. The content selection task is concerned with choosing relevant information that is to be communicated in the generated text, often with the goal of creating summaries (e.g. (Duboue and McKeown, 2003; Barzilay and Lapata, 2005)). While this research area certainly provides important insights to the generation of elaborateness, they need to be considered with respect to the peculiariti"
I17-1092,H05-1042,0,0.0533774,"explored, among others, by Wen et al. (2015). With a similar goal, efforts towards the paraphrasing of sentences have been made (e.g. (Kozlowski et al., 2003; Langkilde and Knight, 1998)). Those approaches provide variation at the word level and preserve the semantic content of a sentence. They are complemented by our approach that focuses on variations of the semantic content of a system action. The content selection task is concerned with choosing relevant information that is to be communicated in the generated text, often with the goal of creating summaries (e.g. (Duboue and McKeown, 2003; Barzilay and Lapata, 2005)). While this research area certainly provides important insights to the generation of elaborateness, they need to be considered with respect to the peculiarities of dialogue. Instead of providing an overview over the most important information in a larger amount of data, the goal of our work is to augment an already determined piece of information with relevant further information. Hence, content selection is more concerned with filtering information, while our approach focuses on adding information. Related Work Adaptive DMs can be beneficial to the user experience (Ultes et al., 2015; Bertr"
I17-1092,W16-3610,1,0.828813,"their preferences in a conversation is demanding work. Approaches to the automatic generation of system actions, such as (Kadlec et al., 2015), have been presented to facilitate that process. However, those approaches often consider only system actions that are necessary from a functional point of view. There is no variety of system actions produced that would enable the DM to adapt to specific users characteristics or preferences. However, automatically generating variants of system actions can greatly increase the adaptability of a DM and thereby improve the user experience. Studies (e.g. (Miehle et al., 2016; Pragst et al., 2017)) have shown that elaborateness and indirectness can be useful in adaptive DM. Here, elaborateness refers to the amount of additional information provided to the user and the level of indirectness describes how concretely information is addressed by a speaker. We have proposed the automatic generation of elaborateness and indirectness in (Pragst et al., 2016). In this work, we introduce an algorithm that, given a core statement on a semantic level, automatically creates more elaborated or indirect versions of that statement by retrieving semantic content from a knowledge"
I17-1092,W03-1016,0,0.0849349,"sentences, which has been explored, among others, by Wen et al. (2015). With a similar goal, efforts towards the paraphrasing of sentences have been made (e.g. (Kozlowski et al., 2003; Langkilde and Knight, 1998)). Those approaches provide variation at the word level and preserve the semantic content of a sentence. They are complemented by our approach that focuses on variations of the semantic content of a system action. The content selection task is concerned with choosing relevant information that is to be communicated in the generated text, often with the goal of creating summaries (e.g. (Duboue and McKeown, 2003; Barzilay and Lapata, 2005)). While this research area certainly provides important insights to the generation of elaborateness, they need to be considered with respect to the peculiarities of dialogue. Instead of providing an overview over the most important information in a larger amount of data, the goal of our work is to augment an already determined piece of information with relevant further information. Hence, content selection is more concerned with filtering information, while our approach focuses on adding information. Related Work Adaptive DMs can be beneficial to the user experienc"
I17-1092,W16-5506,1,0.794544,"dapt to specific users characteristics or preferences. However, automatically generating variants of system actions can greatly increase the adaptability of a DM and thereby improve the user experience. Studies (e.g. (Miehle et al., 2016; Pragst et al., 2017)) have shown that elaborateness and indirectness can be useful in adaptive DM. Here, elaborateness refers to the amount of additional information provided to the user and the level of indirectness describes how concretely information is addressed by a speaker. We have proposed the automatic generation of elaborateness and indirectness in (Pragst et al., 2016). In this work, we introduce an algorithm that, given a core statement on a semantic level, automatically creates more elaborated or indirect versions of that statement by retrieving semantic content from a knowledge base (KB) and assessing its relevance to the original In a dialogue system, the dialogue manager selects one of several system actions and thereby determines the system’s behaviour. Defining all possible system actions in a dialogue system by hand is a tedious work. While efforts have been made to automatically generate such system actions, those approaches are mostly focused on p"
I17-1092,W15-4649,1,0.912882,"show that the results of our algorithm are mostly perceived similarly to human generated elaborateness and indirectness and can be used to adapt a conversation to the current user and situation. We also discuss where the results of our algorithm are still lacking and how this could be improved: Taking into account the conversation topic as well as the culture of the user is likely to have beneficial effect on the user’s perception. 1 Introduction In a dialogue system (DS), the dialogue manager (DM) is responsible for choosing the system’s contribution to a conversation. Several studies (e.g. (Ultes et al., 2015; Bertrand et al., 2011; 915 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 915–925, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP statement. Additionally, we ascertain that elaborateness and indirectness are suitable options for providing adaptability to the DM, and that our automatically generated system actions are mostly perceived similarly compared to human instances of elaborated and indirect statements. We further examine the circumstances under which the perception of automatically generated system actions deviates from hum"
I17-1092,D15-1199,0,0.0128131,"ction 5. Finally, we draw a conclusion in Section 6. 2 Dialogue Manager System Action Language Generation Selection of System Actions User Action Elaborateness/Indirectness Generator Core Statement Knowledge Integration Subject/Predicate/Object Related RDF Triplets Knowledge Base Figure 1: Partial architecture of the KRISTINA system, enhanced by the proposed algorithm. tences from a more structured representation. With regard to surface realisation, one characteristic of good generators is their ability to provide variation in the generated sentences, which has been explored, among others, by Wen et al. (2015). With a similar goal, efforts towards the paraphrasing of sentences have been made (e.g. (Kozlowski et al., 2003; Langkilde and Knight, 1998)). Those approaches provide variation at the word level and preserve the semantic content of a sentence. They are complemented by our approach that focuses on variations of the semantic content of a system action. The content selection task is concerned with choosing relevant information that is to be communicated in the generated text, often with the goal of creating summaries (e.g. (Duboue and McKeown, 2003; Barzilay and Lapata, 2005)). While this rese"
I17-1092,P14-1004,0,0.0151015,"13) or emotion (Andr´e et al., 2004; Gnjatovi´c and R¨osner, 2008; Pittermann and Pittermann, 2007). Komatani et al. (2005) use the amount of information presented as adaptation mechanism to the user’s knowledge and the degree of urgency. Such architectures provide the decision making process necessary for choosing the best suited system action. However, they depend on the availability of suitable system actions to perform optimally. To facilitate the process of defining system actions, efforts have been made to model dialogues automatically, e.g (Beveridge and Fox, 2006; Kadlec et al., 2015; Zhai and Williams, 2014; Niraula et al., 2014). Those approaches are mostly focused on functional system behaviour. Only system actions that are necessary to solve a task are defined, limiting the possibilities for adaptation. Our goal is to generate variants of system actions that address the same functionality, and thereby increase the adaptability. Our efforts to generate variants of system actions is paralleled by a number of tasks in the area of natural language generation. Natural language generators produce human-readable sen3 System Architecture We embed our approach to the generation of elaborateness and in"
I17-1092,C98-1112,0,\N,Missing
L16-1010,schmitt-etal-2012-parameterized,1,0.895924,"Missing"
L16-1010,sidorov-etal-2014-comparison,1,0.715415,"onsideration, we have used just pleasantness (a synonym for evaluation) and arousal axes from the AVEC-2014, VAM, and UUDB corpora. The corresponding quadrant (anticlockwise, starting in the positive quadrant, assuming arousal as abscissa) can also be assigned emotional labels: happy-exciting, angry-anxious, sad-bored and relaxed-serene (Schuller et al., 2009b). There is a description of the used corpora in Table 1. 4. very straightforward way is to add this information to the set of features as an additional variable; we will refer to this approach as System Aug for augmented feature vector (Sidorov et al., 2014a). Another way is to create speakerdependent models: While, for conventional emotion recognition, one statistical model is created independently of the speaker, one may create a separate emotion model for each speaker, we will refer to this approach as System Sep for separate model (Sidorov et al., 2014b). Both approaches result in a two-stage recognition procedure: First, the speaker is identified and then this information is included into the feature set directly (for the System Aug), or the corresponding emotion model is used for estimating the emotions (for the System Sep). Both emotion r"
L16-1010,vogt-andre-2006-improving,0,0.0968277,"Missing"
L16-1288,W14-2615,1,0.91162,"includes procedures such as stop-words filtering (Fox, 1989) and stemming (Porter, 2001). The second stage is the numerical feature extraction based on term weighting. The most well-known unsupervised term weighting method is TF-IDF (Salton and Buckley, 1988). The following supervised term weighting methods are also considered in our paper: Gain Ratio (GR) (Debole and Sebastiani, 2004), Confident Weights (CW) (Soucy and Mineau, 2005), Term Second Moment (TM2) (Xu and Li, 2007), Relevance Frequency (RF) (Lan et al., 2009), Term Relevance Ratio (TRR) (Ko, 2012), and Novel Term Weighting (NTW) (Gasanova et al., 2014a); these methods involve information about the classes of the documents. In this paper we propose collectives of term weighting methods that could improve classification effectiveness. As a rule, the dimensionality for text classification problems is high even after stop-words filtering and stemming. Due to the high dimensionality, the classification may be inappropriately time-consuming, especially for real-time spoken dialogue systems. The third stage of text preprocessing is dimensionality reduction based on numerical features performing using feature selection or feature transformation. I"
L16-1288,W06-1302,0,\N,Missing
L18-1124,W15-1830,0,0.0609982,"Missing"
L18-1124,D11-1014,0,0.195484,"discuss related work in Section 2, before introducing our approach to the generation of DVMs in Section 3. Here, we also evaluate the validity of the generated DVMs. In Section 4, we propose potential applications for those models. Finally, we draw our conclusion in Section 5. 2. Related Work A number of different approaches to the representation of sentences in vector space have been proposed, e.g. utilising recurrent neural networks (Sutskever et al., 2014; Palangi et al., 2016), convolutional neural networks (Shen et al., 2014; Kalchbrenner et al., 2014; Hu et al., 2014) and autoencoders (Socher et al., 2011). Those approaches typically do not take into account surrounding sentences for the generation of the sentence vector, instead relying on the words in the sentence only. Tsunoo et al. (2017) implement sentence vectors with recurrent neural networks and additionally use a bidirectional Long Short-Term Memory to capture the impact of adjacent sentences. However, this additional information is not utilised to improve the vector representation of the sentence, but to model a story transition. Some machine translation approaches, such as (Zhang et al., 2014; Hermann and Blunsom, 2014), rely on mapp"
L18-1124,N15-1020,0,0.0361407,"ctors (Kiros et al., 2015) are sentence embeddings that are generated in a similar manner as word vector representations, and therefore similar to the dialogue vector models we propose. Rather than using the words in the sentence itself as basis to create a vector representation, those vectors are generated taking into account surrounding sentences. However, this representation is trained on novels rather than dialogue. In our work, we focus specifically on dialogue and its peculiarities. In the area of conversational response generation, neural network approaches are commonly utilised (e.g. (Sordoni et al., 2015)). Here, previous utterances in a conversation are used to generate a vector representation of the dialogue context that the response generation is based on. While the vector representation is based on adjacent sentences, a vector in such a model does not represent a singular utterance, but rather the entirety of the preceding utterances. Cerisara et al. (2017) investigate the usability of word2vec representations for dialogue act recognition. Similarly to our work, their goal is to determine the function of an utterance in the dialogue context. In this endeavour, they use word vectors in comb"
L18-1124,P14-1011,0,0.0248421,"014; Hu et al., 2014) and autoencoders (Socher et al., 2011). Those approaches typically do not take into account surrounding sentences for the generation of the sentence vector, instead relying on the words in the sentence only. Tsunoo et al. (2017) implement sentence vectors with recurrent neural networks and additionally use a bidirectional Long Short-Term Memory to capture the impact of adjacent sentences. However, this additional information is not utilised to improve the vector representation of the sentence, but to model a story transition. Some machine translation approaches, such as (Zhang et al., 2014; Hermann and Blunsom, 2014), rely on mapping sentences in different languages into a joint vector space. Here, the correct mapping is determined taking into account not adjacent sentences, but corresponding sentences in another language. Skip thought vectors (Kiros et al., 2015) are sentence embeddings that are generated in a similar manner as word vector representations, and therefore similar to the dialogue vector models we propose. Rather than using the words in the sentence itself as basis to create a vector representation, those vectors are generated taking into account surrounding sente"
L18-1625,W15-4603,0,0.149687,"ffer and Pennebaker, 2002; Brennan, 1996; Pickering and Garrod, 2004; Nenkova et al., 2008). Moreover, various studies suggest to adapt Spoken Dialogue Systems to the user in a similar way (Cassell and Bickmore, 2003; ForbesRiley et al., 2008; Stenchikova and Stent, 2007; Reitter et al., 2006; Mairesse and Walker, 2010). By adapting the system’s behaviour to the user, the conversation agent may appear more familiar and trustworthy and the dialogue may be more effective. Therefore, current research focuses on user-adaptive Spoken Dialogue Systems, e.g. (Honold et al., 2014; Ultes et al., 2015; Casanueva et al., 2015). Pragst et al. (2015) specifically focus on the adaptiveness of Dialogue Management to the cultural background and the emotional state of the user. Our aim is to design a Spoken Dialogue System which adapts to the user’s communication idiosyncrasies. According to various cultural models for Human-Human Interaction (Hofstede, 2009; Elliott et al., 2016; Kaplan, 1966; Lewis, 2010), different cultures prefer different communication styles. Moreover, Burleson (2003) presents a study of culture and gender differences in close relationships, emotion and interpersonal communication. Empirical resear"
L18-1625,W16-3610,1,0.419002,"cultural differences is reviewed. It is shown that social constructionist theories, like the different cultures view of gender, anticipate differences among social groups. These differences influence forms and functions of social relationships, the character of emotional experiences and the uses to which communication is put. However, it is unclear which cultural idiosyncrasies found in Human-Human Interaction may be transferred to Human-Computer Interaction as it has been shown that there exist clear differences in Human-Human Interaction and Human-Computer Interaction (Doran et al., 2003). Miehle et al. (2016) showed that communication idiosyncrasies found in Human-Human Interaction may also be observed during Human-Computer Interaction in a Spoken Dialogue System context. Moreover, cultural differences between Germany and Japan have been identified. However, not all results are consistent with the existing cultural models for Human-Human Interaction and the authors infer that the communication patterns are not only influenced by the culture, but also by the dialogue domain and other user states and traits. In order to obtain a more detailed view, the study described in the work at hand is composed"
L18-1625,P08-2043,0,0.692125,"anguage. Today, we are able to communicate with various computer applications via speech. However, the usability and acceptance of Spoken Dialogue Systems is rather low and in public opinion, such systems often fall into disrepute (Hempel, 2008). For Human-Human Interaction, it has been shown that people adapt their interaction styles to one another across many levels of utterance production when they communicate, e.g. by matching each other’s behaviour or synchronising the timing of behaviour (Burgoon et al., 2007; Niederhoffer and Pennebaker, 2002; Brennan, 1996; Pickering and Garrod, 2004; Nenkova et al., 2008). Moreover, various studies suggest to adapt Spoken Dialogue Systems to the user in a similar way (Cassell and Bickmore, 2003; ForbesRiley et al., 2008; Stenchikova and Stent, 2007; Reitter et al., 2006; Mairesse and Walker, 2010). By adapting the system’s behaviour to the user, the conversation agent may appear more familiar and trustworthy and the dialogue may be more effective. Therefore, current research focuses on user-adaptive Spoken Dialogue Systems, e.g. (Honold et al., 2014; Ultes et al., 2015; Casanueva et al., 2015). Pragst et al. (2015) specifically focus on the adaptiveness of Dia"
L18-1625,N06-2031,0,0.677405,"s often fall into disrepute (Hempel, 2008). For Human-Human Interaction, it has been shown that people adapt their interaction styles to one another across many levels of utterance production when they communicate, e.g. by matching each other’s behaviour or synchronising the timing of behaviour (Burgoon et al., 2007; Niederhoffer and Pennebaker, 2002; Brennan, 1996; Pickering and Garrod, 2004; Nenkova et al., 2008). Moreover, various studies suggest to adapt Spoken Dialogue Systems to the user in a similar way (Cassell and Bickmore, 2003; ForbesRiley et al., 2008; Stenchikova and Stent, 2007; Reitter et al., 2006; Mairesse and Walker, 2010). By adapting the system’s behaviour to the user, the conversation agent may appear more familiar and trustworthy and the dialogue may be more effective. Therefore, current research focuses on user-adaptive Spoken Dialogue Systems, e.g. (Honold et al., 2014; Ultes et al., 2015; Casanueva et al., 2015). Pragst et al. (2015) specifically focus on the adaptiveness of Dialogue Management to the cultural background and the emotional state of the user. Our aim is to design a Spoken Dialogue System which adapts to the user’s communication idiosyncrasies. According to vario"
L18-1625,2007.sigdial-1.29,0,0.809958,"n public opinion, such systems often fall into disrepute (Hempel, 2008). For Human-Human Interaction, it has been shown that people adapt their interaction styles to one another across many levels of utterance production when they communicate, e.g. by matching each other’s behaviour or synchronising the timing of behaviour (Burgoon et al., 2007; Niederhoffer and Pennebaker, 2002; Brennan, 1996; Pickering and Garrod, 2004; Nenkova et al., 2008). Moreover, various studies suggest to adapt Spoken Dialogue Systems to the user in a similar way (Cassell and Bickmore, 2003; ForbesRiley et al., 2008; Stenchikova and Stent, 2007; Reitter et al., 2006; Mairesse and Walker, 2010). By adapting the system’s behaviour to the user, the conversation agent may appear more familiar and trustworthy and the dialogue may be more effective. Therefore, current research focuses on user-adaptive Spoken Dialogue Systems, e.g. (Honold et al., 2014; Ultes et al., 2015; Casanueva et al., 2015). Pragst et al. (2015) specifically focus on the adaptiveness of Dialogue Management to the cultural background and the emotional state of the user. Our aim is to design a Spoken Dialogue System which adapts to the user’s communication idiosyncrasi"
L18-1625,W15-4649,1,0.604704,"al., 2007; Niederhoffer and Pennebaker, 2002; Brennan, 1996; Pickering and Garrod, 2004; Nenkova et al., 2008). Moreover, various studies suggest to adapt Spoken Dialogue Systems to the user in a similar way (Cassell and Bickmore, 2003; ForbesRiley et al., 2008; Stenchikova and Stent, 2007; Reitter et al., 2006; Mairesse and Walker, 2010). By adapting the system’s behaviour to the user, the conversation agent may appear more familiar and trustworthy and the dialogue may be more effective. Therefore, current research focuses on user-adaptive Spoken Dialogue Systems, e.g. (Honold et al., 2014; Ultes et al., 2015; Casanueva et al., 2015). Pragst et al. (2015) specifically focus on the adaptiveness of Dialogue Management to the cultural background and the emotional state of the user. Our aim is to design a Spoken Dialogue System which adapts to the user’s communication idiosyncrasies. According to various cultural models for Human-Human Interaction (Hofstede, 2009; Elliott et al., 2016; Kaplan, 1966; Lewis, 2010), different cultures prefer different communication styles. Moreover, Burleson (2003) presents a study of culture and gender differences in close relationships, emotion and interpersonal commun"
N13-1064,W09-3926,0,0.0705595,"Missing"
N13-1064,hara-etal-2010-estimation,0,0.482579,"he ratings ranging from 1-5 are used as target variable for statistical classifiers using a set of automatically derivable interaction parameters as input. They achieve a MR/R of 0.58. 1 MR/R is equal to Unweighted Average Recall (UAR) which is explained in Section 4. 571 2.2 User Ratings An approach presented by Engelbrecht et al. (2009) uses Hidden Markov Models (HMMs) to model the SDS as a process evolving over time. User Satisfaction was predicted at any point within the dialogue on a 5 point scale. Evaluation was performed based on labels the users applied themselves during the dialogue. Hara et al. (2010) derived turn level ratings from an overall score applied by the users after the dialogue. Using n-gram models reflecting the dialogue history, the achieved results for recognizing User Satisfaction on a 5 point scale showed to be hardly above chance. Work by Schmitt et al. (2011b) deals with determining User Satisfaction from ratings applied by the users themselves during the dialogues. A statistical classification model was trained using automatically derived interaction parameter to predict User Satisfaction for each system-user-exchange on a 5-point scale achieving an MR/R of 0.49. 3 Corpu"
N13-1064,W10-4304,0,0.100695,"complete a questionnaire after completing the dialogue. Moreover, in the PARADISE framework, only quality measurement for the whole dialogue (or system) is allowed. However, this is not suitable for using quality information for online adaption of the dialogue (cf. (Ultes et al., 2012)). Furthermore, PARADISE relies on questionnaires while we focus on work using singlevalued ratings. Numerous work on predicting User Satisfaction as a single-valued rating task for each system-userexchange has been performed in both categories. This work is briefly presented in the following. 2.1 Expert Ratings Higashinaka et al. (2010a) proposed a model to predict turn-wise ratings for human-human dialogues (transcribed conversation) and human-machine dialogues (text from chat system). Ratings ranging from 1-7 were applied by two expert raters labeling “Smoothness”, “Closeness”, and “Willingness” not achieving a Match Rate per Rating (MR/R)1 of more than 0.2-0.24. This results are only slightly above the random baseline of 0.14. Further work by Higashinaka et al. (2010b) uses ratings for overall dialogues to predict ratings for each systemuser-exchange. Again, evaluating in three user satisfaction categories “Smoothness”,"
N13-1064,W11-2020,1,0.23612,"heir expertise in the field of Human Computer Interaction or Spoken Dialogue Systems: They may be novices or have a high expertise. With experts or expert raters, we refer to people who are not participating in the dialogue thus constituting a completely different set of people. Expert raters listen to recorded dialogues after the interactions and rate them by assuming the point of view of the actual person performing the dialogue. These experts are supposed to have some experience with dialogue systems. In this work, expert raters were “advanced students of computer science and engineering” (Schmitt et al., 2011a). For User Satisfaction, ratings applied by the users seem to be clearly the better choice over ratings applied by third persons. However, determining true User Satisfaction is only possible by asking real users interacting with the system. Ideally, the ratings are applied by users talking to a system employed in the field, e.g., commercial systems, as these users have real concerns. For such Spoken Dialogue Systems, though, it is not easy to get users to apply quality ratings to the dialogue – especially for each system-userexchange. The users would have to rate either by pressing a button"
N13-1064,schmitt-etal-2012-parameterized,1,0.552417,"Missing"
N13-1064,W12-1819,1,0.363191,"ser Interfaces enabling speech communication of different complexity reaching from simple spoken commands up to complex dialogues. Besides the spoken words, the speech signal also may be used to acquire information about the user state, e.g., about their emotional state (cf., e.g., (Polzehl et al., 2011))). By additional analysis of the humancomputer-dialogues, even more abstract information may be derived, e.g., the quality of the system (cf., e.g., (Engelbrecht and M¨oller, 2010)). System quality information may be used to adapt the system’s behavior online during the ongoing dialogue (cf. (Ultes et al., 2012)). For determining the quality of Spoken Dialogue Systems, several aspects are of interest. M¨oller et al. (2009) presented a taxonomy of quality criteria. They describe quality as a bipartite issue consisting of Quality of Service (QoS) and Quality of Experience (QoE). Quality of Service describes objective criteria like dialogue duration or number of turns. While these are well-defined items that can be determined easily, Quality of Experience, which describes the user experience with subjective criteria, is more vague and without a sound definition, e.g., User Satisfaction (US). Subjective"
N13-1064,P97-1035,0,0.754825,"ows. First, we give a brief overview of work done in both categories (user ratings vs. expert ratings) in Section 2 and present our choice of data the analysis in this paper is based on in Section 3. Further, evaluation metrics are illustrated in Section 4 and approaches on facilitating prediction of user rater scores by expert rater information are presented in Section 5 followed by an evaluation and discussion of the results in Section 6. 2 Significant Related Work Predicting User Satisfaction for SDSs has been in the focus of research for many years, most famously the PARADISE framework by Walker et al. (1997). The authors assume a linear dependency between quantitative parameters derived from the dialogue and US, modeling this dependency using linear regression. Unfortunately, for generating the regression model, weighting factors have to be computed for each system anew. This generates high costs as dialogues have to be performed with real users where each user further has to complete a questionnaire after completing the dialogue. Moreover, in the PARADISE framework, only quality measurement for the whole dialogue (or system) is allowed. However, this is not suitable for using quality information"
schmitt-etal-2010-influence,W07-0304,0,\N,Missing
schmitt-etal-2010-influence,W09-3918,1,\N,Missing
schmitt-etal-2010-witchcraft,N04-1006,0,\N,Missing
schmitt-etal-2010-witchcraft,geoffrois-etal-2000-transcribing,0,\N,Missing
schmitt-etal-2010-witchcraft,W09-3918,1,\N,Missing
schmitt-etal-2012-parameterized,W09-3950,0,\N,Missing
schmitt-etal-2012-parameterized,W11-2020,1,\N,Missing
schmitt-etal-2012-parameterized,rieser-lemon-2008-automatic,0,\N,Missing
schmitt-etal-2012-parameterized,N04-1006,0,\N,Missing
sidorov-etal-2014-speech,batliner-etal-2004-stupid,0,\N,Missing
sidorov-etal-2014-speech,W09-3918,0,\N,Missing
strauss-etal-2008-pit,scherer-strauss-2008-flexible,1,\N,Missing
strauss-etal-2008-pit,strauss-etal-2006-wizard,1,\N,Missing
ultes-etal-2014-first,W12-1819,1,\N,Missing
ultes-etal-2014-first,W11-2020,1,\N,Missing
ultes-etal-2014-first,schmitt-etal-2012-parameterized,1,\N,Missing
ultes-etal-2014-first,N13-1064,1,\N,Missing
W10-4349,W09-3926,0,0.0461673,"Missing"
W10-4349,schmitt-etal-2010-witchcraft,1,0.925512,"ers, differentiate between genders (Burkhardt et al., 2007), spotting dialects, estimating the cooperativeness of users or user satisfaction (Engelbrecht et al., 2009) and finally, predicting task completion (Walker et al., 2002). When applied online, i.e. during the interaction between user and system, these models can add valuable information to the dialogue system which would allow for an adaption of the dialogue strategy, see Figure 1. Until now we can report that these models1 1 Parsing 2 The Role of Witchcraft For a more detailed introduction on the Witchcraft Workbench please refer to (Schmitt et al., 2010a). In a nutshell, Witchcraft allows managing, mining and analyzing large dialogue corpora. It brings logged conversations back to life in such that it simulates the interaction between user and system based on system logs and audio recordings. Witchcraft is first of all not an annotation or transcription tool in contrast to other workbenches such as NITE (Bernsen et al., 2002), Transcriber2 fier and prediction model interchanging in this context 2 http://trans.sourceforge.net please note that we use the expression recognizer, classiProceedings of SIGDIAL 2010: the 11th Annual Meeting of the S"
W10-4349,bernsen-etal-2002-nite,0,\N,Missing
W11-2020,W09-3926,0,0.18689,"Missing"
W11-2020,hara-etal-2010-estimation,0,0.301092,"n-level ratings. The open issues that arise from the cited work are addressed in the following. 3 Issues Our aim is to create a general model that may be used to predict the quality of the interaction - or ideally the actual satisfaction of the user - at arbitrary system-user exchanges in an SDS. It has become obvious from the cited work that current models are not suited for deployment due to low prediction accuracy. Crucial for a successful recognition of user satisfaction is the choice and appropriateness of the input variables. (Higashinaka et al., 2010a), (Higashinaka et al., 2010b) and (Hara et al., 2010) employ a - mostly hand annotated - “dialogue act” feature to predict the target variable. Dialogue acts are frequently highly system-dependent and do not model the full bandwidth of the interaction. (Engelbrecht et al., 2009) additionally employed contextual appropriateness, confirmation strategy and task success, of which many require hand annotation. Yet it is mandatory for an automatic prediction of user satisfaction to design and derive completely automatic features that do not require manual intervention. It is further easy to comprehend that the modeling of user satisfaction in ongoing"
W11-2020,W10-4304,0,0.602077,"ed user satisfaction as process evolving over time with Hidden Markov Models (HMM). In the experiment, users were asked to interact with a Wizard-of-Oz restaurant information system. Each participant followed dialogues which have previously been defined following predefined scripts, i.e. specific scenarios. This resulted in equally long dialogue transcripts for each scenario. The users were constrained to rate their satisfaction on a 5-point scale with “bad”, “poor”, “fair”, “good” and “excellent” after each dialogue step. The interaction was halted while the user voted. In a similar spirit, (Higashinaka et al., 2010a) developed a model for predicting turn-wise ratings, which was evaluated on human-machine and human-human dialogues. The data employed was not spoken dialogue but text dialogues from a chat system and a transcribed conversation between humans. The labels in the model originated from two expert raters that listened to the recorded interactions and provided turn-wise scores from 1-7 on smoothness (“Smoothness of the conversation”), closeness (“Closeness perceived by the user towards the system”) and willingness (“Willingness to continue the conversation”). Rater-independent performance scores"
W11-2020,W09-3918,1,0.863597,"Missing"
W12-1617,P08-2043,0,0.0174179,"will be compared between these groups. 3 Measuring Adaptation There exist different approaches for measuring adaptation of dialogue partners. Reitter et al. (2006) used regression models to show that a speaker in human-human interactions aligns his syntactic structures with those of his dialogue partner. Ward and Litman (2007) modified the measures of convergence offered by Reitter. According to this modification, prime words of the first dialogue partner were determined. For measuring lexical convergence, the use of prime words by the second dialogue partner for each turn was calculated. In (Nenkova et al., 2008) the measurements of adaptation between dialogue partners were based on the usage of high-frequency words. Stoyanchev and Stent (2009) analysed adaptation calculating the number of reused verbs and prepositions by a speaker that occurred in his dialogue partner’s turns. In this work we measure adaptation as cross referencing, proportion of “I”, “You” and “We” words, between-subject correlation and similarity between two texts. These approaches are described in the following sections. 3.1 Cross Referencing Cross referencing is calculated as a number of repeated nouns and adjectives by a speaker"
W12-1617,N06-2031,0,0.192429,"and similarity of texts. It was shown that lower verbal intelligence speakers repeated more nouns and adjectives from the other and used the same linguistic categories more often than higher verbal intelligence speakers. In dialogues between strangers, participants with higher verbal intelligence showed a greater level of adaptation. 1 Introduction When two speakers are talking to each other, they try to adapt to their dialogue partner and synchronize their verbal behaviours. The adaptation may occur at different levels: lexical (Garrod and Anderson, 1987; Brennan and Clark, 1996), syntactic (Reitter et al., 2006), acoustic (Ward and Litman, 2007), articulation (Bard et al., 2000), comprehension (Levelt and Kelter, 1982), etc. Moreover, synchronization of dialogue partners at one level may cause the adaptation process at any other level (Pickering and Garrod, 2004; Cleland and Pickering, 2003). In this paper we analyse to what degree dialogue partners W.Minker Inst. of Communications Engineering, University of Ulm, Germany wolfgang.minker@ uni-ulm.de with different verbal intelligence and levels of acquaintance may adapt to each other during a conversation. Verbal intelligence (VI) is “the ability to a"
W12-1617,N09-2048,0,0.0319204,"gue partners. Reitter et al. (2006) used regression models to show that a speaker in human-human interactions aligns his syntactic structures with those of his dialogue partner. Ward and Litman (2007) modified the measures of convergence offered by Reitter. According to this modification, prime words of the first dialogue partner were determined. For measuring lexical convergence, the use of prime words by the second dialogue partner for each turn was calculated. In (Nenkova et al., 2008) the measurements of adaptation between dialogue partners were based on the usage of high-frequency words. Stoyanchev and Stent (2009) analysed adaptation calculating the number of reused verbs and prepositions by a speaker that occurred in his dialogue partner’s turns. In this work we measure adaptation as cross referencing, proportion of “I”, “You” and “We” words, between-subject correlation and similarity between two texts. These approaches are described in the following sections. 3.1 Cross Referencing Cross referencing is calculated as a number of repeated nouns and adjectives by a speaker P1 from his dialogue partner P2 divided by the total number of P1 ’s words (Sillars et al., 1997). A one-way analysis of variance (AN"
W12-1617,zablotskaya-etal-2012-investigating,1,0.867924,"Missing"
W12-1617,zablotskaya-etal-2010-speech,1,0.59769,"ords on an average. For the dialogue collection, the participants were asked to have a 10minute conversation with another test person. The topic of the discussions was always the same: the education system in Germany. The average number of turns in the dialogues is 55. Afterwards, verbal intelligence of the candidates was measured using the Hamburg Wechsler Intelligence Test for Adults (Wechsler, 1982). Using this test, we obtained verbal intelligence scores of the test persons with a mean value of 113 and a standard deviation of 7.2. A more detailed description of the corpus can be found in (Zablotskaya et al., 2010; Zablotskaya et al., 2012). 2.2 Clustering Using the k-means algorithm, the verbal intelligence scores of the test persons were partitioned into: a) 2 clusters (Cluster L consisted of test persons with lower verbal intelligence, H contained candidates with higher verbal intelligence); b) 3 clusters (L - lower verbal intelligence, M average verbal intelligence, H - higher verbal intelligence). Using the two clusters L and H, all the dialogues were partitioned into the following groups: c) L-L is a group of dialogues where both partners had lower verbal intelligence scores; d) H-H is a group of"
W12-1819,W09-3926,0,0.166392,"Missing"
W12-1819,hara-etal-2010-estimation,0,0.128251,"adaption of the dialogue strategy to the dialogue performance requires exchange-level performance measures. Therefor, Dialogue-level approaches are of no use. Furthermore, previous presented methods for exchange-level quality measuring could not achieve satisfying accuracy in predicting dialogue quality (Engelbrecht et al., 2009; Higashinaka et al., 2010). Features serving as input variables for a classification algorithm must be automatically derivable from the dialogue system modules. This is important because other features, e.g., manually annotated dialogue acts (Higashinaka et al., 2010; Hara et al., 2010), produce high costs and are also not available immediately during run-time in order to use them as additional input to the Dialogue Manager. Furthermore, for creating a general quality metric, features have to be domain-independent, i.e., not depending on the task domain of the dialogue system. Another important issue is the consistency of the labels. Labels applied by the users themselves are subject to large fluctuations among the different users (Lindgaard and Dudek, 2003). As this results in inconsistent labels, which do not suffice for creating a generally valid quality model, ratings ap"
W12-1819,W10-4304,0,0.89034,"n Dialogue Systems. However, the issue of 2 Related Work In recent years, several studies have been published on determining the qualitative performance of a SDS. Engelbrecht et al. (2009) predicted User Satisfaction on a five-point scale at any point within the dialogue using Hidden Markov Models (HMMs). Evaluation was based on labels the users applied themselves during a Wizard-of-Oz experiment. To guarantee for comparable conditions, the dialogue flow was controlled by predefined scenarios creating transcripts with equal length for each scenario. Further work based on HMMs was presented by Higashinaka et al. (2010). The HMM was trained on US rated at each exchange. These exchange ratings were derived from ratings for the whole dialogue. The authors compare their approach with HMMs trained on manually annotated exchanges achieving a better performance for the latter. 49 NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 49–52, c Montr´eal, Canada, June 7, 2012. 2012 Association for Computational Linguistics In order to predict US, Hara et al. (2010) created n-gram models from dialogue acts (DA). Based on dialogues from real users interacting with"
W12-1819,W11-2020,1,0.73144,"om the dialogue modules they constitute the exchange level. Based on this, counts, sums, means, and frequencies of exchange level parameters from multiple exchanges are computed to constitute the dialogue level (all exchanges up to the current one) and the window level (the three previous exchanges). A corpus containing the labeled data has been published recently (Schmitt et al., in press) containing 200 calls annotated by three expert labelers, resulting in a total of 4,885 labeled exchanges. Using statistical classification of IQ based on SVMs achieves an Unweighted Average Recall of 0.58 (Schmitt et al., 2011a). 4 Quality-Adaptive Spoken Dialogue Management The goal of our work is to enable Dialogue Managers to directly adapt to information about the quality of the ongoing dialogue. We present two different approaches that outline our ongoing and future work. 4.1 Dialogue Design-Patterns for Quality Adaption Rule-based Dialogue Managers are still state-of-theart for commercial SDSs. It is hardly arguable that making the rules quality-dependent is a promising 51 way for dialogue improvement. However, the number of possibilities for adapting the dialogue strategy to the dialogue quality is high. Bas"
W12-1819,schmitt-etal-2012-parameterized,1,\N,Missing
W13-4018,W12-1819,1,0.764449,".g., on the exchange level. To overcome this issue, work by Schmitt et al. (2011) introduced a new metric for measuring the performance of an SDS on the exchange level called Interaction Quality (IQ). They used statistical classification methods to automatically derive the quality based on interaction parameters. Quality labels were applied by expert raters after the dialogue on the exchange level, i.e., for each systemuser-exchange. Automatically derived parameters were then used as features for creating a statistical classification model using static feature vectors. Based on the same data, Ultes et al. (2012a) put an emphasis on the sequential character of the IQ measure by applying temporal statistical classification using Hidden Markov Models (HMMs) and Continuous Hidden Markov Models (CHMMs). However, statistical classifiers usually do not achieve perfect performance, i.e., there will always be misclassification. While most work focuses on applying different statistical models and improving them (Section 2), learning the error to correct the result afterwards represents a different approach. Therefore, we present our approach on estimating the error of IQ recognition models to correct their hy"
W13-4018,W09-3926,0,0.327545,"Missing"
W13-4018,hara-etal-2010-estimation,0,0.168312,"level parameters Figure 1: The three different modeling levels representing the interaction at exchange en . the SDS as a process evolving over time. Performance ratings on a 5 point scale (“bad”, “poor”, “fair”, “good”, “excellent”) have been applied by the users during the dialogue. Higashinaka et al. (2010) proposed a model for predicting turn-wise ratings for human-human dialogues analyzed on a transcribed conversation and human-machine dialogues with text from a chat system. Ratings ranging from 1 to 7 were applied by two expert raters labeling for smoothness, closeness, and willingness. Hara et al. (2010) derived turn level ratings from overall ratings of the dialogue which were applied by the users afterwards on a five point scale. Using n-grams to model the dialogue, results for distinguishing between six classes at any point in the dialogue showed to be hardly above chance. 3 4 Error Estimation Model Error correction may be incorporated into the statistical classification process by a two-stage approach, which is depicted in Figure 2. At the first stage, a statistical classification model is created using interaction parameters as input and IQ as target variable. For this work, a Support Ve"
W13-4018,P97-1035,0,0.36458,"m’s performance (cf. (Ultes et al., 2012b)). In human-machine dialogues, however, there is no easy way of deriving the user’s satisfaction level. Moreover, asking real users for answering questions about the system performance requires them to spend more time talking to the machine than necessary. It can be assumed that a regular user does not want to do this as human-machine dialogues usually have no conversational character but are task oriented. Hence, automatic approaches are the preferred choice. Famous work on determining the satisfaction level automatically is the PARADISE framework by Walker et al. (1997). Assuming a linear dependency between objective measures and User Satisfaction (US), a linear regression model is applied to determine US on the dialogue level. This is not 2 Related Work on Dialogue Quality Besides Schmitt et al., other research groups have performed numerous work on predicting subjective quality measures on an exchange level, all not incorporating any form of error correction. Engelbrecht et al. (2009) presented an approach using Hidden Markov Models (HMMs) to model 122 Proceedings of the SIGDIAL 2013 Conference, pages 122–126, c Metz, France, 22-24 August 2013. 2013 Associ"
W13-4018,W11-2020,1,0.944347,"rmed numerous work on predicting subjective quality measures on an exchange level, all not incorporating any form of error correction. Engelbrecht et al. (2009) presented an approach using Hidden Markov Models (HMMs) to model 122 Proceedings of the SIGDIAL 2013 Conference, pages 122–126, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics e1 … en-2 en-1 en en+1 … are computed to constitute the dialogue level (all exchanges up to the current one) and the window level (the three previous exchanges). A complete list of parameters is listed in (Schmitt et al., 2012). Schmitt et al. (2011) performed IQ recognition on this data using linear SVMs. They achieved an Unweighted Average Recall (UAR) of 0.58 based on 10-fold cross-validation. Ultes et al. (2012a) applied HMMs and CHMMs using 6-fold cross validation and a reduced feature set achieving an UAR of 0.44 for HMMs and 0.39 for CHMMs. exchange level parameters window level parameters dialogue level parameters Figure 1: The three different modeling levels representing the interaction at exchange en . the SDS as a process evolving over time. Performance ratings on a 5 point scale (“bad”, “poor”, “fair”, “good”, “excellent”) hav"
W13-4018,schmitt-etal-2012-parameterized,1,0.841934,"earch groups have performed numerous work on predicting subjective quality measures on an exchange level, all not incorporating any form of error correction. Engelbrecht et al. (2009) presented an approach using Hidden Markov Models (HMMs) to model 122 Proceedings of the SIGDIAL 2013 Conference, pages 122–126, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics e1 … en-2 en-1 en en+1 … are computed to constitute the dialogue level (all exchanges up to the current one) and the window level (the three previous exchanges). A complete list of parameters is listed in (Schmitt et al., 2012). Schmitt et al. (2011) performed IQ recognition on this data using linear SVMs. They achieved an Unweighted Average Recall (UAR) of 0.58 based on 10-fold cross-validation. Ultes et al. (2012a) applied HMMs and CHMMs using 6-fold cross validation and a reduced feature set achieving an UAR of 0.44 for HMMs and 0.39 for CHMMs. exchange level parameters window level parameters dialogue level parameters Figure 1: The three different modeling levels representing the interaction at exchange en . the SDS as a process evolving over time. Performance ratings on a 5 point scale (“bad”, “poor”, “fair”, “"
W14-2615,W13-4053,1,0.916964,"ts of two parts: text preprocessing and classification using obtained numerical data. All text preprocessing methods are based on the idea that the category of the document depends on the words or phrases from this document. The simplest approach is to take each word of the document as a binary coordinate and the dimension of the feature space will be the number of words in our dictionary. There exist more advanced approaches for text preprocessing to overcome this problem such as TF-IDF (Salton and Buckley, 1988) and ConfWeight methods (Soucy and Mineau, 2005). A novel term weighting method (Gasanova et al., 2013) is also considered, which has Abstract In this paper we investigate the efficiency of the novel term weighting algorithm for opinion mining and topic categorization of articles from newspapers and Internet. We compare the novel term weighting technique with existing approaches such as TF-IDF and ConfWeight. The performance on the data from the text-mining campaigns DEFT’07 and DEFT’08 shows that the proposed method can compete with existing information retrieval models in classification quality and that it is computationally faster. The proposed text preprocessing method can be applied in lar"
W14-4328,W11-2020,1,0.673831,"resented in Section 6 and their results discussion in Section 7. 2 s1 u1 s2 u2 e1 e2 s3 u3 … sn un e3 en Figure 1: A dialogue may be separated into a sequence of system-user-exchanges where each exchange ei consists of a system turn si followed by a user turn ui . Significant Related Work their work about Interaction Quality (IQ) for Spoken Dialogue Systems. In contrast to user satisfaction, the labels were applied by expert annotators after the dialogue at the exchange level. Automatically derived parameters were used as features for creating a statistical model using static feature vectors. Schmitt et al. (2011) performed IQ recognition on the LEGO corpus (see Section 3) using linear SVMs. They achieved an UAR2 of 0.58 based on 10-fold cross-validation which is clearly above the random baseline of 0.2. Ultes et al. (2012a) put an emphasis on the sequential character of the IQ measure by applying a Hidden Markov Models (HMMs) and a Conditioned Hidden Markov Models (CHMMs). Both have been applied using 6-fold cross validation and a reduced feature set of the LEGO corpus achieving an UAR2 of 0.44 for HMMs and 0.39 for CHMMs. While Ultes et al. (2012a) used generic Gaussian Mixture Models to model the ob"
W14-4328,W09-3926,0,0.281798,"Missing"
W14-4328,schmitt-etal-2012-parameterized,1,0.915818,"t al. (2010) derived turn level ratings from overall ratings of the dialogue which were applied by the users after the interaction on a five point scale within an online questionnaire. Using ngrams to model the dialogue by calculating n-gram occurrence frequencies for each satisfaction value showed that results for distinguishing between six classes at any point in the dialogue to be hardly above chance. A more robust measure for user satisfaction has been presented by Schmitt et al. (2011) within 2 3 The LEGO Corpus For Interaction Quality (IQ) estimation, we use the LEGO corpus published by Schmitt et al. (2012). Interaction Quality is defined similarly to user satisfaction: While the latter represents the true disposition of the user, IQ is the disposition of the user assumed by an expert annotator. Here, expert annotators are people who listen to recorded dialogues after the interactions and rate them by assuming the point of view of the actual person performing the dialogue. These experts are supposed to have some experience with dialogue systems. In this work, expert annotators were “advanced students of computer science and engineering” (Schmitt et al., 2011), i.e., grad students. The LEGO corpu"
W14-4328,hara-etal-2010-estimation,0,0.128202,"ing) and a 6-dimensional input vector. While Engelbrecht et al. (2009) relied on only 6 input variables, we will pursue an approach with 29 input variables. Moreover, we will investigate dialogues of a real world dialogue system annotated with quality labels by expert annotators. Higashinaka et al. (2010) proposed a model for predicting turn-wise ratings for human-human dialogues. Ratings ranging from 1 to 7 were applied by two expert annotators labeling for smoothness, closeness, and willingness. They achieved an UAR2 of only 0.2-0.24 which is only slightly above the random baseline of 0.14. Hara et al. (2010) derived turn level ratings from overall ratings of the dialogue which were applied by the users after the interaction on a five point scale within an online questionnaire. Using ngrams to model the dialogue by calculating n-gram occurrence frequencies for each satisfaction value showed that results for distinguishing between six classes at any point in the dialogue to be hardly above chance. A more robust measure for user satisfaction has been presented by Schmitt et al. (2011) within 2 3 The LEGO Corpus For Interaction Quality (IQ) estimation, we use the LEGO corpus published by Schmitt et a"
W14-4328,W12-1819,1,0.829654,"abling automatically derived user satisfaction within the dialogue management allows for adaption of the ongoing dialogue (Ultes et al., 2012b). First work on deriving subjective metrics automatically has been performed by Walker et al. (1997) resulting in the PARADISE framework, which is the current quasi-standard in this field. Briefly explained, a linear dependency is assumed between dialogue parameters and user satisfaction to estimate qualitative performance on the dialogue level. Measuring the performance of complete dialogues does not allow for adapting to the user during the dialogue (Ultes et al., 2012b). Hence, performance measures which provide a measurement for each system-user-exchange1 are of interest. Approaches based on Hidden Markov Models (HMMs) are widely used for sequence modeling. Therefore, Engelbrecht et al. (2009) used these models for predicting the dialogue quality on the exchange level. Similar to this, we presented work on estimating Interaction Quality using HMMs and Conditioned HMMs (Ultes et al., 2012a). In this contribution, we investigate an approach for recognizing the dialogue quality using a hybrid Markovian model. Here, hybrid means combining statistical approach"
W14-4328,N13-1064,1,0.327902,"Missing"
W14-4328,P97-1035,0,0.802975,"Missing"
W15-4646,W14-4307,1,0.922424,"e.g., “show only exercises which do not require gym access”), to the user’s requirements (e.g., “show only beginner exercises”) or adapted to the user’s dialog history (e.g., “preselect exercises which were used the last time”) and preferences (e.g., “present only exercises with dumbbells”). Additionally, integrating pro-active as well as requested explanations into the interaction is an important part of imparting used domain knowledge and clarifying system behavior. Using a coherent knowledge source to create dialog and planning domains enables us to use predefined declarative explanations (Nothdurft et al., 2014) together with dynamically generated plan explanation (Seegebarth et al., 2012) and explanations for ontological inferences (Schiller and Glimm, 2013) without dealing with inconsistency issues. This way Plan Steps (e.g., exercises) can be explained in detail, dependencies between plan steps can be explained to exemplify the necessity of tasks (i.e., plan explanation), and ontology explanations can justify decompositions from which the planning model and the dialog domain where generated. All of which increase the user’s perceived system transparency. 5.2 The Decision Model also records the dia"
W15-4649,W12-1606,0,0.0249534,"the system” are excluded from all statistics in this paper. Three objective metrics are used to evaluate the dialogue performance: the average dialogue length (ADL), the dialogue completion rate (DCR) and task success rate (TSR). The ADL is modeled by the average number of exchanges per completed dialogue. A dialogue is regarded as being completed if the system provides a result— whether correct or not—to the user. Hence, DCR represents the ratio of dialogues for which the system was able to provide a result, i.e., provide schedule information: the user. The Let’s Go User Simulator (LGUS) by Lee and Eskenazi (2012) is used for evaluation to replace the need for human evaluators. The dialogue goal of Let’s Go consists of four slots: bus number, departure place, arrival place, and travel time. However, the bus number is not mandatory. The original system contains more than 300,000 arrival or departure places, respectively. To acquire information about the specific goal of the user, the system may use one out of nine system actions to which the user responds with a subset of six user actions. In LGUS, the user actions are accompanied with a confidence score simulating automatic speech recognition performan"
W15-4649,W09-3926,0,0.0119963,"Missing"
W15-4649,hara-etal-2010-estimation,0,0.013805,"o the user knowledge. For both, only simulated or predefined user states are used while this work uses a real estimation module deriving the user satisfaction. Using user ratings to improve the dialogue performance in a reinforcement learning (RL) approach has been presented by Walker (2000), Rieser and Lemon (2008), Janarthanam and Lemon (2008), and Gaˇsi´c et al. (2013). Walker applied RL to a MDP-based dialogue system ELVIS 2.2 Interaction Quality While there is numerous work on investigating turn-wise quality ratings for SDSs, e.g., Engelbrecht et al. (2009), Higashinaka et al. (2010) and Hara et al. (2010), the Interaction Quality paradigm by Schmitt et al. (2011) seems to be the only metric fulfilling the requirements for adapting the dialogue online (Ultes et al., 2012). For rendering an SDS adaptive to the user’s satisfaction level, a module is needed to automatically derive the satisfaction from the ongoing interaction. For creating this module, usually, dialogues have to be annotated with ratings describing the user’s satisfaction level. Schmitt et al. (2015) proposed a measure called “Interaction Quality” (IQ) which fulfills the requirements of a 375 e1 e1 e2 e2 e3 e3 … en en-2 en-1 en en"
W15-4649,P08-1073,0,0.428925,"ech Republic, 2-4 September 2015. 2015 Association for Computational Linguistics for accessing emails over the phone. They modeled the reward function using the PARADISE framework (Walker et al., 1997) showing that the resulting policy improved the system performance in terms of user satisfaction significantly. The resulting best policy showed, among other aspects, that the system-initiative strategy was found to work best. The group of Lemon also employed PARADISE for modelling the reward function. Using reinforcement learning, they found an optimal dialogue strategy for result presentation (Rieser and Lemon, 2008) or referring expressions (Janarthanam and Lemon, 2008) for natural language generation. For a POMDP-based dialogue manager, Gaˇsi´c et al. use a reward function based on user ratings to train the optimized policy. The user ratings are acquired using Amazon Mechanical Turk. They show that their approach converges much faster than conventional approaches using a user simulator. However, their approach does not allow for adapting the course of the dialogue online but relies on a pre-optimized dialogue strategy. Finally, not directly providing user adaptivity but allowing for reacting to specific"
W15-4649,W11-2020,1,0.838042,"poken Dialogue (SDS) research. Today, commercial systems are still inflexible and do not adapt to users or the dialogue flow. This usually results in bad performance and in frequently unsuccessful dialogues. In recent years, adaptation strategies have been investigated for rendering SDS more flexible and robust. The aim of those strategies is to adapt the dialogue flow based on observations that are made during an ongoing dialogue. One approach to observe and score the interaction between the system and the user is the Interaction Quality (IQ) (Schmitt and Ultes, 2015) originally presented by Schmitt et al. (2011). Their Interaction Quality paradigm is one of the first metrics which can be used for this purpose. A pilot user study on adapting the dialogue to the Interaction Quality by Ultes et al. (2014b) in a limited 1 Automatic optimization aims at maximizing a reward function. If IQ was contributing positively to this reward function, optimisation would naturally result in an increase in IQ. As we do not perform optimization, this correlation does not automatically exist 374 Proceedings of the SIGDIAL 2015 Conference, pages 374–383, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for"
W15-4649,schmitt-etal-2012-parameterized,1,0.939853,"vel parameters are only computed out of the last three exchanges. These interaction parameters are used as input variables to a statistical classification module. The statistical model is trained based on annotated dialogues of the Lets Go Bus Information System in Pittsburgh, USA (Raux et al., 2006). Each of the 4,885 exchanges (200 calls) has been annotated by three different raters resulting in a rating agreement of κ = 0.54. The final IQ value of the three raters is derived using the median. Furthermore, the raters had to follow labeling guidelines to enable a consistent labeling process (Schmitt et al., 2012). Schmitt et al. (2011) estimated IQ with a Support Vector Machine using only automatically 2 derivable parameters achieving an unweighted average recall of 0.59. 3 Quality-Adaptive Dialogue Within this section, we describe one part of the main contribution of rendering the dialogue initiative adaptive to Interaction Quality and compare the resulting strategy to several non-adaptive strategies. Conventional dialogue initiative categories are user initiative, system initiative, and mixed initiative (McTear, 2004). As there are different interpretations of what these initiative categories mean,"
W15-4649,W12-1819,1,0.806174,"er ratings to improve the dialogue performance in a reinforcement learning (RL) approach has been presented by Walker (2000), Rieser and Lemon (2008), Janarthanam and Lemon (2008), and Gaˇsi´c et al. (2013). Walker applied RL to a MDP-based dialogue system ELVIS 2.2 Interaction Quality While there is numerous work on investigating turn-wise quality ratings for SDSs, e.g., Engelbrecht et al. (2009), Higashinaka et al. (2010) and Hara et al. (2010), the Interaction Quality paradigm by Schmitt et al. (2011) seems to be the only metric fulfilling the requirements for adapting the dialogue online (Ultes et al., 2012). For rendering an SDS adaptive to the user’s satisfaction level, a module is needed to automatically derive the satisfaction from the ongoing interaction. For creating this module, usually, dialogues have to be annotated with ratings describing the user’s satisfaction level. Schmitt et al. (2015) proposed a measure called “Interaction Quality” (IQ) which fulfills the requirements of a 375 e1 e1 e2 e2 e3 e3 … en en-2 en-1 en en+1 … exchange level parameters window level parameters: {#}, {Mean}, etc. dialogue level parameters: #, Mean, etc. Figure 1: The three different modeling levels represen"
W15-4649,N13-1064,1,0.669835,"e current exchange; the window level, capturing important parameters from the previous n dialogue steps (here n = 3); the dialogue level, measuring overall performance values from the entire previous interaction. quality metric for adaptive dialogue identified by Ultes et al. (2012). For Schmitt et al., the main aspect of user satisfaction is that it is assigned by real users. However, this seems to be impractical in many real world scenarios. Hence, the usage of expert raters is proposed. Further studies have also shown a high correlation between quality ratings applied by experts and users (Ultes et al., 2013). The IQ paradigm is based on automatically deriving interaction parameters from the SDS and feed these parameters into a statistical classification module which predicts the IQ level of the ongoing interaction at the current system-userexchange 2 . The interaction parameters are rendered on three levels (see Figure 1): the exchange level, the window level, and the dialogue level. The exchange level comprises parameters derived from SDS modules Automatic Speech Recognizer, Spoken Language Understanding, and Dialogue Management directly. Parameters on the window and the dialogue level are sums,"
W15-4649,ultes-etal-2014-first,1,0.782437,"dialogues. In recent years, adaptation strategies have been investigated for rendering SDS more flexible and robust. The aim of those strategies is to adapt the dialogue flow based on observations that are made during an ongoing dialogue. One approach to observe and score the interaction between the system and the user is the Interaction Quality (IQ) (Schmitt and Ultes, 2015) originally presented by Schmitt et al. (2011). Their Interaction Quality paradigm is one of the first metrics which can be used for this purpose. A pilot user study on adapting the dialogue to the Interaction Quality by Ultes et al. (2014b) in a limited 1 Automatic optimization aims at maximizing a reward function. If IQ was contributing positively to this reward function, optimisation would naturally result in an increase in IQ. As we do not perform optimization, this correlation does not automatically exist 374 Proceedings of the SIGDIAL 2015 Conference, pages 374–383, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for Computational Linguistics for accessing emails over the phone. They modeled the reward function using the PARADISE framework (Walker et al., 1997) showing that the resulting policy improved the"
W15-4649,P97-1035,0,0.760123,"pting the dialogue to the Interaction Quality by Ultes et al. (2014b) in a limited 1 Automatic optimization aims at maximizing a reward function. If IQ was contributing positively to this reward function, optimisation would naturally result in an increase in IQ. As we do not perform optimization, this correlation does not automatically exist 374 Proceedings of the SIGDIAL 2015 Conference, pages 374–383, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for Computational Linguistics for accessing emails over the phone. They modeled the reward function using the PARADISE framework (Walker et al., 1997) showing that the resulting policy improved the system performance in terms of user satisfaction significantly. The resulting best policy showed, among other aspects, that the system-initiative strategy was found to work best. The group of Lemon also employed PARADISE for modelling the reward function. Using reinforcement learning, they found an optimal dialogue strategy for result presentation (Rieser and Lemon, 2008) or referring expressions (Janarthanam and Lemon, 2008) for natural language generation. For a POMDP-based dialogue manager, Gaˇsi´c et al. use a reward function based on user ra"
W16-5506,W03-1601,0,0.319133,"erceived as aggressive. An example for different levels of directness is saying either ‘Take an aspirin.’ or ‘People often use aspirin when they have a headache.’ tovi´c and R¨osner, 2008; Pittermann and Pittermann, 2007), among many others, has been implemented. While such architectures provide the means to execute an adaptive dialogue strategy, they rely on predefined system actions to provide a variety of system actions for adaptation. In the area of language generation, paraphrasing is a major field of research and a lot of approaches for automating paraphrasing have been discussed, e.g. (Kozlowski et al., 2003; Langkilde and Knight, 1998). While this research addresses the generation of variations in the language output with the same semantic content, we focus on variations of the semantic content of a system action. There have been efforts to model dialogue flows automatically, e.g (Beveridge and Fox, 2006; Niraula et al., 2014; Zhai and Williams, 2014; Kadlec et al., 2015). Their focus lies on the automatic extraction of complete dialogue flows and is strictly task-oriented. While in some of those examples system actions are extracted automatically, those system actions are reproductions. Also, n"
W16-5506,P98-1116,0,0.20944,"An example for different levels of directness is saying either ‘Take an aspirin.’ or ‘People often use aspirin when they have a headache.’ tovi´c and R¨osner, 2008; Pittermann and Pittermann, 2007), among many others, has been implemented. While such architectures provide the means to execute an adaptive dialogue strategy, they rely on predefined system actions to provide a variety of system actions for adaptation. In the area of language generation, paraphrasing is a major field of research and a lot of approaches for automating paraphrasing have been discussed, e.g. (Kozlowski et al., 2003; Langkilde and Knight, 1998). While this research addresses the generation of variations in the language output with the same semantic content, we focus on variations of the semantic content of a system action. There have been efforts to model dialogue flows automatically, e.g (Beveridge and Fox, 2006; Niraula et al., 2014; Zhai and Williams, 2014; Kadlec et al., 2015). Their focus lies on the automatic extraction of complete dialogue flows and is strictly task-oriented. While in some of those examples system actions are extracted automatically, those system actions are reproductions. Also, no variants are provided as th"
W16-5506,C98-1112,0,\N,Missing
W16-5506,P14-1004,0,\N,Missing
W17-5520,W09-3926,0,0.0879334,"Missing"
W17-5520,hara-etal-2010-estimation,0,0.06097,"Missing"
W17-5520,W10-4304,0,0.104423,"itute of Communication Engineering, Ulm University {vorname.nachname}@uni-ulm.de 2 Department of Engineering, University of Cambridge, UK su259@cam.ac.uk Abstract manually optimized, pre-computed temporal information (as employed in previous work) is no longer required. Diverse approaches for estimating the US were already proposed, including n-gram models (Hara et al., 2010) and Hidden Markov Models (Higashinaka et al., 2010a; Engelbrech et al., 2009) in different scenarios. Although the results were above the random baseline, the respective improvement was only minor. As it was discussed by Higashinaka et al. (2010b), one difficulty of this task lies in the subjective nature of US since it depends on the appreciation of the user. IQ is a more objective approach to US that relies on the rating of experts instead of users (Schmitt and Ultes, 2015) and thus closes the gap between subjective valuation and objective criteria. The respective rating is given on a scale between 1 (extremely unsatisfied) and five (satisfied) after listening to audio records of the dialogue in question. A detailed study on the correlation between the IQ and a measure of the real US was provided by Ultes et al. (2013) and various"
W17-5520,W11-2020,1,0.836784,"Missing"
W17-5520,schmitt-etal-2012-parameterized,1,0.876318,"ewed scenario. The herein employed architecture is thus built of a LSTM unit, consisting of two stacked LSTM cells, followed by a two-layer perceptron unit with sigmoid activation functions. The latter one is given as FM LP : yt → (g2 ◦ g1 )(yt ) (1) gi (yt ) = (2) sigm(WiT yt + bi ) Figure 1: Sketch of the deep learning architecture in use. The left part contains the two stacked LSTM cells followed by a softmax normalization unit. The output is fed into a two layer perceptron with sigmoid activation functions. 3 The LEGO Corpus To appropriately compare our results, we employ the LEGO coprus (Schmitt et al., 2012)—the same corpus as the authors of previous work. It is based on the ”Let’s Go Bus Information System” of the Carnegie Mellon university in Pittsburg (Raux et al., 2006) and consists of 200 dialogues including 4884 system-user exchanges. Each exchange was assigned with features from three instances of where Wi denotes the weight matrix, bi a bias vector and sigm the element-wise sigmoid function. A LSTM cell on the other hand can be seen as function f : xt , ct−1 , ht−1 → ht , ct (4) (3) 165 e1 e1 e2 e2 e3 e3 … en en-2 en-1 en en+1 … exchange level parameters window level parameters: {#}, {Mea"
W17-5520,W12-1819,1,0.751024,"Missing"
W17-5520,N13-1064,1,0.690172,"ssed by Higashinaka et al. (2010b), one difficulty of this task lies in the subjective nature of US since it depends on the appreciation of the user. IQ is a more objective approach to US that relies on the rating of experts instead of users (Schmitt and Ultes, 2015) and thus closes the gap between subjective valuation and objective criteria. The respective rating is given on a scale between 1 (extremely unsatisfied) and five (satisfied) after listening to audio records of the dialogue in question. A detailed study on the correlation between the IQ and a measure of the real US was provided by Ultes et al. (2013) and various approaches including Hidden Markov Models (Ultes et al., 2014b; Ultes and Minker, 2014), Support Vector Machines (Schmitt et al., 2011; Ultes and Minker, 2013), Ordinal Regression (El Asri et al., 2014) and Recurrent Neural Networks (Pragst et al., 2017) have been employed to estimate the IQ from exchange parameters. Although the results show a significant improvement to alternative approaches, the classification relies in each case on precomputed features modeling the dialogue history (so called temporal features). Despite the good results, using temporal features requires insigh"
W17-5520,ultes-etal-2014-first,1,0.705424,"subjective nature of US since it depends on the appreciation of the user. IQ is a more objective approach to US that relies on the rating of experts instead of users (Schmitt and Ultes, 2015) and thus closes the gap between subjective valuation and objective criteria. The respective rating is given on a scale between 1 (extremely unsatisfied) and five (satisfied) after listening to audio records of the dialogue in question. A detailed study on the correlation between the IQ and a measure of the real US was provided by Ultes et al. (2013) and various approaches including Hidden Markov Models (Ultes et al., 2014b; Ultes and Minker, 2014), Support Vector Machines (Schmitt et al., 2011; Ultes and Minker, 2013), Ordinal Regression (El Asri et al., 2014) and Recurrent Neural Networks (Pragst et al., 2017) have been employed to estimate the IQ from exchange parameters. Although the results show a significant improvement to alternative approaches, the classification relies in each case on precomputed features modeling the dialogue history (so called temporal features). Despite the good results, using temporal features requires insight into the correlations between the dialogue history and the IQ score as t"
W17-5520,W13-4018,1,0.647679,"tive approach to US that relies on the rating of experts instead of users (Schmitt and Ultes, 2015) and thus closes the gap between subjective valuation and objective criteria. The respective rating is given on a scale between 1 (extremely unsatisfied) and five (satisfied) after listening to audio records of the dialogue in question. A detailed study on the correlation between the IQ and a measure of the real US was provided by Ultes et al. (2013) and various approaches including Hidden Markov Models (Ultes et al., 2014b; Ultes and Minker, 2014), Support Vector Machines (Schmitt et al., 2011; Ultes and Minker, 2013), Ordinal Regression (El Asri et al., 2014) and Recurrent Neural Networks (Pragst et al., 2017) have been employed to estimate the IQ from exchange parameters. Although the results show a significant improvement to alternative approaches, the classification relies in each case on precomputed features modeling the dialogue history (so called temporal features). Despite the good results, using temporal features requires insight into the correlations between the dialogue history and the IQ score as the timespan covered by the temporal information significantly influences the outcome (Ultes et al."
W17-5520,W14-4328,1,0.574142,"US since it depends on the appreciation of the user. IQ is a more objective approach to US that relies on the rating of experts instead of users (Schmitt and Ultes, 2015) and thus closes the gap between subjective valuation and objective criteria. The respective rating is given on a scale between 1 (extremely unsatisfied) and five (satisfied) after listening to audio records of the dialogue in question. A detailed study on the correlation between the IQ and a measure of the real US was provided by Ultes et al. (2013) and various approaches including Hidden Markov Models (Ultes et al., 2014b; Ultes and Minker, 2014), Support Vector Machines (Schmitt et al., 2011; Ultes and Minker, 2013), Ordinal Regression (El Asri et al., 2014) and Recurrent Neural Networks (Pragst et al., 2017) have been employed to estimate the IQ from exchange parameters. Although the results show a significant improvement to alternative approaches, the classification relies in each case on precomputed features modeling the dialogue history (so called temporal features). Despite the good results, using temporal features requires insight into the correlations between the dialogue history and the IQ score as the timespan covered by the"
zablotskaya-etal-2012-investigating,zablotskaya-etal-2010-speech,1,\N,Missing
zablotskaya-etal-2012-investigating,kupietz-etal-2010-german,0,\N,Missing
zablotskaya-etal-2012-relating,zablotskaya-etal-2010-speech,1,\N,Missing
