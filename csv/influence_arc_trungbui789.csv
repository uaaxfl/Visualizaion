2020.aacl-demo.2,S15-1013,0,0.0242533,"Missing"
2020.aacl-demo.2,D19-3028,0,0.0234818,"ormat used by AUTO NLU. ∗ Equal contributions. The work was conducted while the first two authors interned at Adobe Research. 8 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations, pages 8–13 c December 4 - 7, 2020. 2020 Association for Computational Linguistics 2 Related work Web app Model ID Scheduler Closely related branches of work to ours are toolkits and frameworks designed to provide a suite of state-of-the-art NLP models to users (Gong et al., 2019; Akbik et al., 2019; Wang et al., 2019; Zhu et al., 2020; Qi et al., 2020). However, several of these works do not have a user-friendly interface. For example, Flair (Akbik et al., 2019), NeuronBlocks (Gong et al., 2019), and jiant (Wang et al., 2019) require users to work with command-line interfaces. Different from these works, an end-user with no programming skill can still create powerful NLU models using our system. Furthermore, most previous works are not explicitly designed for enterprise settings where use-cases and business needs can be vastly different from team to team. On the othe"
2020.aacl-demo.2,H90-1021,0,0.575306,"Missing"
2020.aacl-demo.2,W03-1028,0,0.0117125,"extraction (Sahrawat et al., 2020), we formulate the task as a sequence labeling task. Given an input sequence of tokens x = {x1 , x2 , ..., xn }, the goal is to predict a sequence of labels y = {y1 , y2 , ..., yn } where yi ∈ {B, I, O}. Here, label B denotes the beginning of a keyphrase, I denotes the continuation of a keyphrase, and O corresponds to tokens that are not part of any keyphrase. This formulation is naturally supported by our platform, as the task of slot filling in NLU is basically a sequence labeling task. We first collect two public datasets for keyphrase extraction: Inspec (Hulth, 2003) and SE2017 (Augenstein et al., 2017). We then convert them to the common intermediate representation. After that, we simply use AUTO NLU to train and tune models. We employ the BiLSTM-CRF archi11 Model KEA (2005) TextRank (2004) SingeRank (2008) SGRank (2015) Transformer (2020) BERT (AUTO NLU) SciBERT (AUTO NLU) Datasets Inspec SE-2017 0.137 0.129 0.122 0.157 0.123 0.155 0.271 0.211 0.595 0.522 0.596 0.537 0.598 0.544 References A. Akbik, T. Bergmann, Duncan Blythe, K. Rasul, Stefan Schweter, and Roland Vollgraf. 2019. Flair: An easy-to-use framework for state-of-the-art nlp. In NAACL-HLT. Is"
2020.aacl-demo.2,D19-1367,0,0.0225465,"rformance on multiple public datasets. AU TO NLU also supports hyperparameter tuning using grid search, allowing users to fine-tune the models even further. • Scalability. AUTO NLU aims to be deployed in enterprises where computing costs could be a limiting factor. We provide an on-demand architecture so that the system could be utilized as much as possible. Introduction In recent years, many deep learning methods have achieved impressive results on a wide range of tasks, ranging from question answering (Seo et al., 2017; Lai et al., 2018b) to named entity recognition (NER) (Lin et al., 2019; Jiang et al., 2019) to intent detection and slot filling (Wang et al., 2018; Chen et al., 2019). Even though the source codes of many models are publicly available, going from an open-sourced implementation of a model for a public dataset to a production-ready model for an inhouse dataset is not a simple task. Furthermore, in an enterprise, only few engineers are familiar with At Adobe, AUTO NLU has been used to train NLU models for different product teams, ranging from Photoshop to Document Cloud. To demonstrate the effectiveness of AUTO NLU, we present two case studies. i) We build a practical NLU model for ha"
2020.aacl-demo.2,D19-1610,1,0.892878,"Missing"
2020.aacl-demo.2,C18-1181,1,0.83393,"ormance for ease-of-use. Our built-in models provide state-of-the-art performance on multiple public datasets. AU TO NLU also supports hyperparameter tuning using grid search, allowing users to fine-tune the models even further. • Scalability. AUTO NLU aims to be deployed in enterprises where computing costs could be a limiting factor. We provide an on-demand architecture so that the system could be utilized as much as possible. Introduction In recent years, many deep learning methods have achieved impressive results on a wide range of tasks, ranging from question answering (Seo et al., 2017; Lai et al., 2018b) to named entity recognition (NER) (Lin et al., 2019; Jiang et al., 2019) to intent detection and slot filling (Wang et al., 2018; Chen et al., 2019). Even though the source codes of many models are publicly available, going from an open-sourced implementation of a model for a public dataset to a production-ready model for an inhouse dataset is not a simple task. Furthermore, in an enterprise, only few engineers are familiar with At Adobe, AUTO NLU has been used to train NLU models for different product teams, ranging from Photoshop to Document Cloud. To demonstrate the effectiveness of AUTO"
2020.aacl-demo.2,N18-2050,0,0.021248,"orts hyperparameter tuning using grid search, allowing users to fine-tune the models even further. • Scalability. AUTO NLU aims to be deployed in enterprises where computing costs could be a limiting factor. We provide an on-demand architecture so that the system could be utilized as much as possible. Introduction In recent years, many deep learning methods have achieved impressive results on a wide range of tasks, ranging from question answering (Seo et al., 2017; Lai et al., 2018b) to named entity recognition (NER) (Lin et al., 2019; Jiang et al., 2019) to intent detection and slot filling (Wang et al., 2018; Chen et al., 2019). Even though the source codes of many models are publicly available, going from an open-sourced implementation of a model for a public dataset to a production-ready model for an inhouse dataset is not a simple task. Furthermore, in an enterprise, only few engineers are familiar with At Adobe, AUTO NLU has been used to train NLU models for different product teams, ranging from Photoshop to Document Cloud. To demonstrate the effectiveness of AUTO NLU, we present two case studies. i) We build a practical NLU model for handling various image-editing requests in Photoshop. ii)"
2020.aacl-demo.2,P19-1016,0,0.0166559,"tate-of-the-art performance on multiple public datasets. AU TO NLU also supports hyperparameter tuning using grid search, allowing users to fine-tune the models even further. • Scalability. AUTO NLU aims to be deployed in enterprises where computing costs could be a limiting factor. We provide an on-demand architecture so that the system could be utilized as much as possible. Introduction In recent years, many deep learning methods have achieved impressive results on a wide range of tasks, ranging from question answering (Seo et al., 2017; Lai et al., 2018b) to named entity recognition (NER) (Lin et al., 2019; Jiang et al., 2019) to intent detection and slot filling (Wang et al., 2018; Chen et al., 2019). Even though the source codes of many models are publicly available, going from an open-sourced implementation of a model for a public dataset to a production-ready model for an inhouse dataset is not a simple task. Furthermore, in an enterprise, only few engineers are familiar with At Adobe, AUTO NLU has been used to train NLU models for different product teams, ranging from Photoshop to Document Cloud. To demonstrate the effectiveness of AUTO NLU, we present two case studies. i) We build a pract"
2020.aacl-demo.2,L18-1683,1,0.834868,"0.850 0.726 0.833 0.605 0.869 0.854 SF1 0.783 0.701 0.862 Table 1: Results on the image-editing requests dataset. Intent accuracy, slot precision, slot recall, and slot F1 scores are reported. Scores of our models are averaged over three random seeds. Case studies NLU Models for Image-Editing Requests ing model created using RASA (Bocklisch et al., 2017) and a joint model of intent determination and slot filling (JIS) (Zhang and Wang, 2016) by a large margin. One of the first clients of AUTO NLU was the Photoshop team, as we want to build a chatbot using their image-editing requests dataset (Manuvinakurike et al., 2018; Brixey et al., 2018). The dataset was collected in many years, annotated both using Amazon Mechanical Turk and by our in-house annotators. Cleaning this dataset is a challenge in itself, and in this case study, we aim to create an effective workflow to train a state-of-theart model and clean the dataset at the same time. We first convert the dataset into our IR, and train a simple model using the fastest algorithm provided by AUTO NLU. This initial model provides us with a rough confusion matrix, and we manually inspect cells with the biggest values. Those cells give us an insight into some"
2020.aacl-demo.2,W04-3252,0,0.180457,"Missing"
2020.aacl-demo.2,2020.findings-emnlp.92,0,0.0586156,"Missing"
2020.aacl-demo.2,2020.acl-demos.14,0,0.0233694,"the first two authors interned at Adobe Research. 8 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations, pages 8–13 c December 4 - 7, 2020. 2020 Association for Computational Linguistics 2 Related work Web app Model ID Scheduler Closely related branches of work to ours are toolkits and frameworks designed to provide a suite of state-of-the-art NLP models to users (Gong et al., 2019; Akbik et al., 2019; Wang et al., 2019; Zhu et al., 2020; Qi et al., 2020). However, several of these works do not have a user-friendly interface. For example, Flair (Akbik et al., 2019), NeuronBlocks (Gong et al., 2019), and jiant (Wang et al., 2019) require users to work with command-line interfaces. Different from these works, an end-user with no programming skill can still create powerful NLU models using our system. Furthermore, most previous works are not explicitly designed for enterprise settings where use-cases and business needs can be vastly different from team to team. On the other hand, since AUTO NLU is an on-demand cloud-based system, it provides more"
2020.aacl-demo.3,D19-1610,1,0.884915,"Missing"
2020.aacl-demo.3,P17-4017,0,0.0223972,"h other customers. Therefore, automated solutions can increase customer satisfaction and retention. In this paper, we introduce a mobile-based intelligent shopping assistant, ISA, which is based on advanced techniques in computer vision, speech processing, and natural language processing. A user just needs to take a picture or scan the barcode of the product of interest. After that, the user can ask ISA a variety of questions such as product 2 Related Work The most closely related branches of work to ours are probably customer service chatbots for e-commerce websites. For example, SuperAgent (Cui et al., 2017) is a powerful chatbot that leverages large-scale and publicly available e-commerce data. The researchers demonstrate SuperAgent as an add-on extension to mainstream web browsers. When a user visits a product page, SuperAgent crawls the information of the product from multi1 The work was conducted while the first author interned at Adobe Research. 14 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations, pages 14–19 c December 4 - 7, 2020. 2"
2020.aacl-demo.3,D15-1166,0,0.101892,"Missing"
2020.aacl-demo.3,2020.findings-emnlp.92,0,0.0244159,"ssing. A user only needs to take a picture or scan the barcode of the product of interest, and then the user can ask ISA a variety of questions about the product. The system can also guide the user through the purchase decision or recommend other similar products to the user. There are many fronts on which we will be exploring in the future. Currently the product specification QA engine answers only questions regarding the specifications of a product. We will implement engines for addressing other kinds of questions. We will also extend ISA to better support other languages and informal text (Nguyen and Nguyen, 2020; Nguyen et al., 2020; Martin et al., 2020). In addition, we will conduct a user study to evaluate our system in the future. Finally, we wish to extend this work to other domains such as building an asRecommendation The recommendation engine is responsible for giving new suggestions and recommendations to users. When a user wants to look for similar products (e.g., by saying “Are there any other similar products?”), the engine will search the database for related products and then send the information of them to the app for displaying to the user (Figure 4). 3.5 Chit Chat Purchase The purchase"
2020.aacl-demo.3,2020.emnlp-demos.2,0,0.0221024,"to take a picture or scan the barcode of the product of interest, and then the user can ask ISA a variety of questions about the product. The system can also guide the user through the purchase decision or recommend other similar products to the user. There are many fronts on which we will be exploring in the future. Currently the product specification QA engine answers only questions regarding the specifications of a product. We will implement engines for addressing other kinds of questions. We will also extend ISA to better support other languages and informal text (Nguyen and Nguyen, 2020; Nguyen et al., 2020; Martin et al., 2020). In addition, we will conduct a user study to evaluate our system in the future. Finally, we wish to extend this work to other domains such as building an asRecommendation The recommendation engine is responsible for giving new suggestions and recommendations to users. When a user wants to look for similar products (e.g., by saying “Are there any other similar products?”), the engine will search the database for related products and then send the information of them to the app for displaying to the user (Figure 4). 3.5 Chit Chat Purchase The purchase engine is responsibl"
2020.aacl-demo.3,W18-3105,1,0.878582,"Missing"
2020.aacl-demo.3,C10-1131,0,0.0228108,"Missing"
2020.aacl-demo.3,D19-1540,0,0.218866,"Missing"
2020.aacl-demo.3,D07-1003,0,0.180043,"methods have been proposed to tackle the answer selection problem (Rao et al., 2016; Zhiguo Wang, 2017; Bian et al., 2017; Shen et al., 2017; Tran et al., 2018; Lai et al., 2018a; Tay et al., 2018; Lai et al., 2018b,c; Rao et al., 2019; Lai et al., 2019; Garg et al., 2019; Kamath et al., 2019; Laskar et al., 2020). These deep learning based methods typically outperform traditional techniques without relying on any feature engineering or expensive external resources. For example, the IWAN model proposed in (Shen et al., 2017) achieves competitive performance on public datasets such as TrecQA (Wang et al., 2007) and WikiQA (Yang et al., 2015). Product Specification QA The product specification QA engine is used to answer questions regarding the specifications of a product. For every product, there is a list of specifications in the form of (specification name, specification value). We formalize the task of the engine as follows: Given a question Q about a product P and the list of specifications (s1 , s2 , ..., sM ) of P , the goal is to identify the specification that is most relevant to the question Q. M is the number of specifications of the product, and si is the sequence of words in the name of"
2020.aacl-demo.3,D15-1237,0,0.214044,"Missing"
2020.aacl-demo.3,D17-1122,0,0.0282532,"Missing"
2020.aacl-demo.3,N13-1106,0,0.0309008,"Missing"
2020.aacl-demo.3,P13-1171,0,0.069585,"Missing"
2020.aacl-demo.3,N18-1115,1,0.55345,"Missing"
2020.acl-main.728,D17-1238,1,0.873663,"Missing"
2020.acl-main.728,D14-1162,0,0.0892691,"a relevance score of the options during fine-tuning (casting it as multi-label classification). 3 Implementation 1 We use PyTorch (Paszke et al., 2017) for our exper2 iments . Following Anderson et al. (2018), we use bottom-up features of 36 proposals from images using a Faster-RCNN (Ren et al., 2015) pre-trained on Visual Genome (Krishna et al., 2017) to get a bag of object-level 2048-d image representations. Input question and candidate options are tokenized to a maximum length of 20 while the conversational history to 200. Token embeddings in text are initialized with 300-d GloVe vectors (Pennington et al., 2014) and shared among all text-based encoders. The RNN encodings are implemented using LSTMs (Hochreiter and Schmidhuber, 1997). 1 https://pytorch.org/ Code available at https://github.com/ shubhamagarwal92/visdial_conv 8185 2 We use the Adam optimizer (Kingma and Ba, 2015) both for training and fine-tuning. More training details can be found in Appendix A. where selecting one answer using sparse annotation 4 is an easier task and fine-tuning more difficult. 4.4 4 Task Description 4.1 Dataset We use VisDial v1.0 for our experiments and eval3 uation. The dataset contains 123K/2K/8K dialogs for trai"
2020.acl-main.728,C18-1104,0,0.110062,"Missing"
2020.acl-main.728,D19-1514,0,0.0355175,"ovikova et al., 2017; Reiter, 2018). As a first step, BERT score (Zhang et al., 2019) could be explored to measure ground-truth similarity replacing the noisy NDCG annotations of semantic equivalence. 8 Conclusion and Future Work In sum, this paper shows that we can get SOTA performance on the VisDial task by using transformerbased models with Guided-Attention (Yu et al., 2019b), and by encoding dialog history and finetuning we can improve results even more. Of course, we expect pre-trained visual BERT models to show even more improvements on this task, e.g. Vilbert (Lu et al., 2019), LXMert (Tan and Bansal, 2019), UNITER (Chen et al., 2019) etc. However, we also show the limitations of this shared task in terms of dialog phenomena and evaluation metrics. We, thus, argue that progress needs to be carefully measured by posing the right task in terms of dataset and evaluation procedure. Acknowledgments We thank the anonymous reviewers for their insightful comments. Shubham would like to thank Raghav Goyal for the discussions during ‘Pikabot’ submission to Visual Dialog Challenge 2018. This work received continued support by Adobe Research gift funding for further collaboration. This research also receive"
2020.acl-main.728,D18-1432,1,0.907846,"Missing"
2020.acl-main.728,D17-1137,0,\N,Missing
2020.acl-main.728,J18-3002,0,\N,Missing
2020.acl-main.728,L18-1683,1,\N,Missing
2020.acl-main.728,W18-5033,1,\N,Missing
2020.acl-main.728,N19-1058,0,\N,Missing
2020.acl-main.728,N19-1423,0,\N,Missing
2020.acl-main.728,N16-1014,0,\N,Missing
2020.coling-main.56,S17-2091,0,0.0191643,"6.13 29.11 38.80 39.59 27.20 34.98 36.45 21.91 30.13 33.22 25.19 31.53 33.30 22.76 27.30 28.42 24.69 33.19 35.42 15.73 19.07 20.17 Table 2: Comparison of our models and unsupervised models (F1@5, F1@10, and F1@15 test scores on the Inspec dataset are reported). To save space, standard deviations of our results are not shown. 4 Experiments and Results Data and Experiments Setup In this work, we experimented with two target datasets: Inspec and SemEval-2017. The Inspec dataset (Hulth, 2003) has 1000/500/500 abstracts of scientific articles for the train/dev/test split. The SemEval-2017 dataset (Augenstein et al., 2017) has 350/50/100 scientific articles for the train/dev/test split. In our experiments, we use the KP20k dataset (Meng et al., 2017) as the source dataset, because it contains more than 500,000 articles collected from various online digital libraries. Even though each article in the dataset has author-provided keyphrases, our proposed method does not require such supervised signals. We implemented two baseline models (Section 2) with different pre-trained contextual embeddings: BERT (base-cased)1 and SciBERT (scivocab-cased)2 (Wolf et al., 2019). From this point, we will refer to baseline models"
2020.coling-main.56,K18-1022,0,0.0575377,"as a sequence labeling task. Let D = (t1 , t2 , ..., tn ) be a document consisting of n tokens, where ti represents the ith token of the document. The task is to predict a sequence of labels y = (y1 , y2 , ..., yn ), where yi ∈ {I, B, O} is the label corresponding to token ti . Label B denotes the beginning of a keyphrase, I denotes the continuation of a keyphrase, and O corresponds to tokens that are not part of any keyphrase. An advantage of this formulation is that it completely avoids the candidate generation step required in previous ranking-based approaches (El-Beltagy and Rafea, 2009; Bennani-Smires et al., 2018). Instead of having to generate a list of candidate phrases and then ranking them, we directly predict the target outputs in one go. This formulation also provides a unified approach to keyphrase extraction, as it has the same format as other sequence labeling tasks. Baseline Models In this work, we employ the BiLSTM-CRF architecture as the baseline architecture (Huang et al., 2015; Alzaidy et al., 2019; Sahrawat et al., 2020; Zhu et al., 2020). Figure 1 shows a high-level overview of our baseline model. Given a sequence of input tokens, the model first forms a contextualized representation fo"
2020.coling-main.56,N13-1030,0,0.0207203,"onsistently improves the performance of baseline models for keyphrase extraction. Furthermore, our best models outperform previous methods for the task, achieving new state-of-the-art results on two public benchmarks: Inspec and SemEval-2017. 1 Introduction Keyphrase extraction is the task of automatically extracting a set of representative phrases from a document that concisely describe its content. As keyphrases provide a brief yet precise description of a document, they can be utilized for various downstream applications (D’Avanzo and Magnini, 2005; Litvak and Last, 2008; Kim et al., 2010; Boudin and Morin, 2013). Over the past years, researchers have proposed many methods for the task, which can be divided into two major categories: supervised (Sterckx et al., 2016; Zhang et al., 2017; Alzaidy et al., 2019) and unsupervised techniques (Florescu and Caragea, 2017b; Boudin, 2018; Mahata et al., 2018). In the presence of sufficient domain-specific labeled data, supervised keyphrase extraction methods are often reported to outperform unsupervised methods (Kim et al., 2013; Caragea et al., 2014; Sahrawat et al., 2020). Recently, many deep learning based methods have achieved promising performance in a wid"
2020.coling-main.56,N18-2105,0,0.0149423,"the task of automatically extracting a set of representative phrases from a document that concisely describe its content. As keyphrases provide a brief yet precise description of a document, they can be utilized for various downstream applications (D’Avanzo and Magnini, 2005; Litvak and Last, 2008; Kim et al., 2010; Boudin and Morin, 2013). Over the past years, researchers have proposed many methods for the task, which can be divided into two major categories: supervised (Sterckx et al., 2016; Zhang et al., 2017; Alzaidy et al., 2019) and unsupervised techniques (Florescu and Caragea, 2017b; Boudin, 2018; Mahata et al., 2018). In the presence of sufficient domain-specific labeled data, supervised keyphrase extraction methods are often reported to outperform unsupervised methods (Kim et al., 2013; Caragea et al., 2014; Sahrawat et al., 2020). Recently, many deep learning based methods have achieved promising performance in a wide range of NLP tasks (Lai et al., 2018b; Tran et al., 2018; Peters et al., 2018; Devlin et al., 2019; Lewis et al., 2019; Lai et al., 2020). However, most existing benchmark datasets for keyphrase extraction typically have limited numbers of annotated documents, making"
2020.coling-main.56,I13-1062,0,0.0247814,"ormance 1 2 https://huggingface.co/bert-base-cased https://huggingface.co/allenai/scibert_scivocab_cased 652 on the Inspec dataset, while the [BERT + JLSD] also outperforms previous methods on the SemEval2017 dataset. These results demonstrate the effectiveness of our joint learning approach. Comparison with Previous Unsupervised Methods We also compare our models with previous stateof-the-art unsupervised approaches, including SIFRank (Sun et al., 2020), EmbedRank (Bennani-Smires et al., 2018), RVA (Papagiannopoulou and Tsoumakas, 2018), PositionRank (Florescu and Caragea, 2017a), TopicRank (Bougouin et al., 2013), SingleRank (Wan and Xiao, 2008), and YAKE (Campos et al., 2018). Table 2 presents the comparison on the Inspec dataset. In this case, since the unsupervised methods are ranking-based methods, the performances are evaluated in terms of F1-measure when a fixed number of top keyphrases are extracted (i.e., F1@5, F1@10, and F1@15 measures are used). We see that our baseline models [BERT] and [SciBERT] already outperform previous unsupervised methods by a large margin. This agrees with previous studies, suggesting that in the presence of sufficient labeled data, supervised methods typically perfo"
2020.coling-main.56,D14-1150,0,0.144442,"zed for various downstream applications (D’Avanzo and Magnini, 2005; Litvak and Last, 2008; Kim et al., 2010; Boudin and Morin, 2013). Over the past years, researchers have proposed many methods for the task, which can be divided into two major categories: supervised (Sterckx et al., 2016; Zhang et al., 2017; Alzaidy et al., 2019) and unsupervised techniques (Florescu and Caragea, 2017b; Boudin, 2018; Mahata et al., 2018). In the presence of sufficient domain-specific labeled data, supervised keyphrase extraction methods are often reported to outperform unsupervised methods (Kim et al., 2013; Caragea et al., 2014; Sahrawat et al., 2020). Recently, many deep learning based methods have achieved promising performance in a wide range of NLP tasks (Lai et al., 2018b; Tran et al., 2018; Peters et al., 2018; Devlin et al., 2019; Lewis et al., 2019; Lai et al., 2020). However, most existing benchmark datasets for keyphrase extraction typically have limited numbers of annotated documents, making it challenging to train an effective deep learning model for the task. In contrast, digital libraries store millions of scientific articles online, covering a wide range of topics. While a significant portion of these"
2020.coling-main.56,N19-1423,0,0.0251693,"can be divided into two major categories: supervised (Sterckx et al., 2016; Zhang et al., 2017; Alzaidy et al., 2019) and unsupervised techniques (Florescu and Caragea, 2017b; Boudin, 2018; Mahata et al., 2018). In the presence of sufficient domain-specific labeled data, supervised keyphrase extraction methods are often reported to outperform unsupervised methods (Kim et al., 2013; Caragea et al., 2014; Sahrawat et al., 2020). Recently, many deep learning based methods have achieved promising performance in a wide range of NLP tasks (Lai et al., 2018b; Tran et al., 2018; Peters et al., 2018; Devlin et al., 2019; Lewis et al., 2019; Lai et al., 2020). However, most existing benchmark datasets for keyphrase extraction typically have limited numbers of annotated documents, making it challenging to train an effective deep learning model for the task. In contrast, digital libraries store millions of scientific articles online, covering a wide range of topics. While a significant portion of these articles have author-provided keyphrases, most other articles lack such kind of annotations. For example, major NLP conferences (i.e., ACL, EMNLP, COLING) normally do not require authors to provide keywords of th"
2020.coling-main.56,P17-1102,0,0.0816406,"ction Keyphrase extraction is the task of automatically extracting a set of representative phrases from a document that concisely describe its content. As keyphrases provide a brief yet precise description of a document, they can be utilized for various downstream applications (D’Avanzo and Magnini, 2005; Litvak and Last, 2008; Kim et al., 2010; Boudin and Morin, 2013). Over the past years, researchers have proposed many methods for the task, which can be divided into two major categories: supervised (Sterckx et al., 2016; Zhang et al., 2017; Alzaidy et al., 2019) and unsupervised techniques (Florescu and Caragea, 2017b; Boudin, 2018; Mahata et al., 2018). In the presence of sufficient domain-specific labeled data, supervised keyphrase extraction methods are often reported to outperform unsupervised methods (Kim et al., 2013; Caragea et al., 2014; Sahrawat et al., 2020). Recently, many deep learning based methods have achieved promising performance in a wide range of NLP tasks (Lai et al., 2018b; Tran et al., 2018; Peters et al., 2018; Devlin et al., 2019; Lewis et al., 2019; Lai et al., 2020). However, most existing benchmark datasets for keyphrase extraction typically have limited numbers of annotated doc"
2020.coling-main.56,W03-1028,0,0.0622038,"58.05 66.01 66.88 BERT Unsupervised Ranking-based Models SciBERT SIF Embed RVA Position Topic Single YAKE 42.39 57.25 66.13 29.11 38.80 39.59 27.20 34.98 36.45 21.91 30.13 33.22 25.19 31.53 33.30 22.76 27.30 28.42 24.69 33.19 35.42 15.73 19.07 20.17 Table 2: Comparison of our models and unsupervised models (F1@5, F1@10, and F1@15 test scores on the Inspec dataset are reported). To save space, standard deviations of our results are not shown. 4 Experiments and Results Data and Experiments Setup In this work, we experimented with two target datasets: Inspec and SemEval-2017. The Inspec dataset (Hulth, 2003) has 1000/500/500 abstracts of scientific articles for the train/dev/test split. The SemEval-2017 dataset (Augenstein et al., 2017) has 350/50/100 scientific articles for the train/dev/test split. In our experiments, we use the KP20k dataset (Meng et al., 2017) as the source dataset, because it contains more than 500,000 articles collected from various online digital libraries. Even though each article in the dataset has author-provided keyphrases, our proposed method does not require such supervised signals. We implemented two baseline models (Section 2) with different pre-trained contextual"
2020.coling-main.56,S10-1004,0,0.0263801,"hat our approach consistently improves the performance of baseline models for keyphrase extraction. Furthermore, our best models outperform previous methods for the task, achieving new state-of-the-art results on two public benchmarks: Inspec and SemEval-2017. 1 Introduction Keyphrase extraction is the task of automatically extracting a set of representative phrases from a document that concisely describe its content. As keyphrases provide a brief yet precise description of a document, they can be utilized for various downstream applications (D’Avanzo and Magnini, 2005; Litvak and Last, 2008; Kim et al., 2010; Boudin and Morin, 2013). Over the past years, researchers have proposed many methods for the task, which can be divided into two major categories: supervised (Sterckx et al., 2016; Zhang et al., 2017; Alzaidy et al., 2019) and unsupervised techniques (Florescu and Caragea, 2017b; Boudin, 2018; Mahata et al., 2018). In the presence of sufficient domain-specific labeled data, supervised keyphrase extraction methods are often reported to outperform unsupervised methods (Kim et al., 2013; Caragea et al., 2014; Sahrawat et al., 2020). Recently, many deep learning based methods have achieved promi"
2020.coling-main.56,C18-1181,1,0.932213,"researchers have proposed many methods for the task, which can be divided into two major categories: supervised (Sterckx et al., 2016; Zhang et al., 2017; Alzaidy et al., 2019) and unsupervised techniques (Florescu and Caragea, 2017b; Boudin, 2018; Mahata et al., 2018). In the presence of sufficient domain-specific labeled data, supervised keyphrase extraction methods are often reported to outperform unsupervised methods (Kim et al., 2013; Caragea et al., 2014; Sahrawat et al., 2020). Recently, many deep learning based methods have achieved promising performance in a wide range of NLP tasks (Lai et al., 2018b; Tran et al., 2018; Peters et al., 2018; Devlin et al., 2019; Lewis et al., 2019; Lai et al., 2020). However, most existing benchmark datasets for keyphrase extraction typically have limited numbers of annotated documents, making it challenging to train an effective deep learning model for the task. In contrast, digital libraries store millions of scientific articles online, covering a wide range of topics. While a significant portion of these articles have author-provided keyphrases, most other articles lack such kind of annotations. For example, major NLP conferences (i.e., ACL, EMNLP, COL"
2020.coling-main.56,D19-1610,1,0.837297,"a way to compute confidence scores for predicted keyphrases. In order to do so, we first calculate marginal probabilities after the CRF layer and simply make an independence assumption to compute the probability for a predicted keyphrase. Comparison with Other Transfer Learning Techniques Finally, we compare our joint learning approach to other popular transfer learning techniques, including simple pretraining and simple joint training. Despite their simplicity, these strategies have been shown to be effective for a wide range of tasks (Min et al., 2017; Lai et al., 2018a; Yoon et al., 2019; Lai et al., 2019; Langenfeld et al., 2019). In simple pretraining, we first train a baseline model on the scientific articles in the source dataset. After that, we simply finetune the same model on a target dataset (i.e., Inspec or SemEval-2017). In simple joint training, we train a baseline model using examples from both the source dataset and the target dataset at the same time. Before each new training epoch, we sample a new set of examples from the source dataset and add them to the pool of examples of the target dataset. Different from our proposed approach, these two transfer learning techniques are onl"
2020.coling-main.56,W08-1404,0,0.0647574,"rimental results show that our approach consistently improves the performance of baseline models for keyphrase extraction. Furthermore, our best models outperform previous methods for the task, achieving new state-of-the-art results on two public benchmarks: Inspec and SemEval-2017. 1 Introduction Keyphrase extraction is the task of automatically extracting a set of representative phrases from a document that concisely describe its content. As keyphrases provide a brief yet precise description of a document, they can be utilized for various downstream applications (D’Avanzo and Magnini, 2005; Litvak and Last, 2008; Kim et al., 2010; Boudin and Morin, 2013). Over the past years, researchers have proposed many methods for the task, which can be divided into two major categories: supervised (Sterckx et al., 2016; Zhang et al., 2017; Alzaidy et al., 2019) and unsupervised techniques (Florescu and Caragea, 2017b; Boudin, 2018; Mahata et al., 2018). In the presence of sufficient domain-specific labeled data, supervised keyphrase extraction methods are often reported to outperform unsupervised methods (Kim et al., 2013; Caragea et al., 2014; Sahrawat et al., 2020). Recently, many deep learning based methods h"
2020.coling-main.56,N18-2100,0,0.0128829,"utomatically extracting a set of representative phrases from a document that concisely describe its content. As keyphrases provide a brief yet precise description of a document, they can be utilized for various downstream applications (D’Avanzo and Magnini, 2005; Litvak and Last, 2008; Kim et al., 2010; Boudin and Morin, 2013). Over the past years, researchers have proposed many methods for the task, which can be divided into two major categories: supervised (Sterckx et al., 2016; Zhang et al., 2017; Alzaidy et al., 2019) and unsupervised techniques (Florescu and Caragea, 2017b; Boudin, 2018; Mahata et al., 2018). In the presence of sufficient domain-specific labeled data, supervised keyphrase extraction methods are often reported to outperform unsupervised methods (Kim et al., 2013; Caragea et al., 2014; Sahrawat et al., 2020). Recently, many deep learning based methods have achieved promising performance in a wide range of NLP tasks (Lai et al., 2018b; Tran et al., 2018; Peters et al., 2018; Devlin et al., 2019; Lewis et al., 2019; Lai et al., 2020). However, most existing benchmark datasets for keyphrase extraction typically have limited numbers of annotated documents, making it challenging to trai"
2020.coling-main.56,P17-1054,0,0.014228,"2: Comparison of our models and unsupervised models (F1@5, F1@10, and F1@15 test scores on the Inspec dataset are reported). To save space, standard deviations of our results are not shown. 4 Experiments and Results Data and Experiments Setup In this work, we experimented with two target datasets: Inspec and SemEval-2017. The Inspec dataset (Hulth, 2003) has 1000/500/500 abstracts of scientific articles for the train/dev/test split. The SemEval-2017 dataset (Augenstein et al., 2017) has 350/50/100 scientific articles for the train/dev/test split. In our experiments, we use the KP20k dataset (Meng et al., 2017) as the source dataset, because it contains more than 500,000 articles collected from various online digital libraries. Even though each article in the dataset has author-provided keyphrases, our proposed method does not require such supervised signals. We implemented two baseline models (Section 2) with different pre-trained contextual embeddings: BERT (base-cased)1 and SciBERT (scivocab-cased)2 (Wolf et al., 2019). From this point, we will refer to baseline models trained only on labeled data as [BERT] and [SciBERT]. We refer to models trained using our joint learning approach as [BERT + JLS"
2020.coling-main.56,P17-2081,0,0.025405,"ed to convert our models into ranking models by deriving a way to compute confidence scores for predicted keyphrases. In order to do so, we first calculate marginal probabilities after the CRF layer and simply make an independence assumption to compute the probability for a predicted keyphrase. Comparison with Other Transfer Learning Techniques Finally, we compare our joint learning approach to other popular transfer learning techniques, including simple pretraining and simple joint training. Despite their simplicity, these strategies have been shown to be effective for a wide range of tasks (Min et al., 2017; Lai et al., 2018a; Yoon et al., 2019; Lai et al., 2019; Langenfeld et al., 2019). In simple pretraining, we first train a baseline model on the scientific articles in the source dataset. After that, we simply finetune the same model on a target dataset (i.e., Inspec or SemEval-2017). In simple joint training, we train a baseline model using examples from both the source dataset and the target dataset at the same time. Before each new training epoch, we sample a new set of examples from the source dataset and add them to the pool of examples of the target dataset. Different from our proposed"
2020.coling-main.56,N18-1202,0,0.0168311,"s for the task, which can be divided into two major categories: supervised (Sterckx et al., 2016; Zhang et al., 2017; Alzaidy et al., 2019) and unsupervised techniques (Florescu and Caragea, 2017b; Boudin, 2018; Mahata et al., 2018). In the presence of sufficient domain-specific labeled data, supervised keyphrase extraction methods are often reported to outperform unsupervised methods (Kim et al., 2013; Caragea et al., 2014; Sahrawat et al., 2020). Recently, many deep learning based methods have achieved promising performance in a wide range of NLP tasks (Lai et al., 2018b; Tran et al., 2018; Peters et al., 2018; Devlin et al., 2019; Lewis et al., 2019; Lai et al., 2020). However, most existing benchmark datasets for keyphrase extraction typically have limited numbers of annotated documents, making it challenging to train an effective deep learning model for the task. In contrast, digital libraries store millions of scientific articles online, covering a wide range of topics. While a significant portion of these articles have author-provided keyphrases, most other articles lack such kind of annotations. For example, major NLP conferences (i.e., ACL, EMNLP, COLING) normally do not require authors to p"
2020.coling-main.56,D16-1198,0,0.0176184,"ing new state-of-the-art results on two public benchmarks: Inspec and SemEval-2017. 1 Introduction Keyphrase extraction is the task of automatically extracting a set of representative phrases from a document that concisely describe its content. As keyphrases provide a brief yet precise description of a document, they can be utilized for various downstream applications (D’Avanzo and Magnini, 2005; Litvak and Last, 2008; Kim et al., 2010; Boudin and Morin, 2013). Over the past years, researchers have proposed many methods for the task, which can be divided into two major categories: supervised (Sterckx et al., 2016; Zhang et al., 2017; Alzaidy et al., 2019) and unsupervised techniques (Florescu and Caragea, 2017b; Boudin, 2018; Mahata et al., 2018). In the presence of sufficient domain-specific labeled data, supervised keyphrase extraction methods are often reported to outperform unsupervised methods (Kim et al., 2013; Caragea et al., 2014; Sahrawat et al., 2020). Recently, many deep learning based methods have achieved promising performance in a wide range of NLP tasks (Lai et al., 2018b; Tran et al., 2018; Peters et al., 2018; Devlin et al., 2019; Lewis et al., 2019; Lai et al., 2020). However, most e"
2020.coling-main.56,N18-1115,1,0.822225,"roposed many methods for the task, which can be divided into two major categories: supervised (Sterckx et al., 2016; Zhang et al., 2017; Alzaidy et al., 2019) and unsupervised techniques (Florescu and Caragea, 2017b; Boudin, 2018; Mahata et al., 2018). In the presence of sufficient domain-specific labeled data, supervised keyphrase extraction methods are often reported to outperform unsupervised methods (Kim et al., 2013; Caragea et al., 2014; Sahrawat et al., 2020). Recently, many deep learning based methods have achieved promising performance in a wide range of NLP tasks (Lai et al., 2018b; Tran et al., 2018; Peters et al., 2018; Devlin et al., 2019; Lewis et al., 2019; Lai et al., 2020). However, most existing benchmark datasets for keyphrase extraction typically have limited numbers of annotated documents, making it challenging to train an effective deep learning model for the task. In contrast, digital libraries store millions of scientific articles online, covering a wide range of topics. While a significant portion of these articles have author-provided keyphrases, most other articles lack such kind of annotations. For example, major NLP conferences (i.e., ACL, EMNLP, COLING) normally do not"
2020.coling-main.56,D18-1447,0,0.0181891,"better pseudo-labels. To the best of our knowledge, the only other work exploring self-learning for keyphrase extraction is that of Zhu et al. (2020). However, during each training epoch, their method needs to label all unlabeled examples and then selects the ones with high confidence scores. This would incur a lot of overhead for every single training epoch as the number of unlabeled examples is typically very large. Our approach does not suffer from these issues. Our teacher model generates pseudo labels on-the-fly during each training iteration. Another highly related work is the paper by Ye and Wang (2018). However, the work focuses on the task of keyphrase generation instead of keyphrase extraction. Algorithm 1: Joint Learning based on Self-Distillation (JLSD)   f1 , D f2 , ..., D g Input: Labeled documents (D1 , y1 ), ..., (Dn , yn ) and unlabeled documents D m Train a teacher model using labeled documents. Initialize a student model with same architecture and parameters as the teacher. for i = 1 . . . T do Sample a batch of labeled documents L uniformly  at random. e= D g g Sample a batch of unlabeled documents U i1 , ..., Dik uniformly at random (k = r|L|).  e to get U = (D g g Use the"
2020.emnlp-main.274,P17-2090,0,0.0485458,"Missing"
2020.emnlp-main.274,D17-1206,0,0.0268247,"in the same dialog state tracker three times with different seeds. We compare the aggregated results from all nine trials with the baseline results. Ultimately, we repeat this procedure for all combinations of state trackers and datasets. For non-augmented baselines, we repeat the experiments ten times. Implementation Details. The hidden size of dialog vectors is 1000, and the hidden size of utterance, dialog act specification, turn state, and turn goal representations is 500. The dimensionality for latent variables is between 100 and 200. We use GloVe (Pennington et al., 2014) and character (Hashimoto et al., 2017) embeddings as pre-trained word emebddings (400 dimensions total) for word and dialog act tokens. All models used Adam optimizer (Kingma and Ba, 2014) with the initial learning rate of 1e-3, We annealed the KL-divergence weights over 250,000 training steps. For data synthesis, we employ ancestral sampling to generate samples from the empirical posterior distribution. We fixed the ratio of synthetic to original data samples to 1. Datasets. We conduct experiments on four state tracking corpora: WoZ2.0 (Wen et al., 2017), DSTC2 (Henderson et al., 2014), MultiWoZ (Budzianowski et al., 2018), and D"
2020.emnlp-main.274,W14-4337,0,0.481331,"iscovery of novel class-preserving samples to machine learning. In this work, we explore GDA in the context of dialog modeling and contextual understanding. Goal-oriented dialogs occur between a user and a system that communicates verbally to accomplish the user’s goals (Table 6). However, because the user’s goals and the system’s possible actions are not transparent to each other, both parties must rely on verbal communications to infer and take appropriate actions to resolve the goals. Dialog state tracker is a core component of such systems, enabling it to track the dialog’s latest status (Henderson et al., 2014). A dialog state typically consists of inform and request types of slot values. 3406 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3406–3425, c November 16–20, 2020. 2020 Association for Computational Linguistics For example, a user may verbally refer to a previously mentioned food type as the preferred one - e.g., Asian (inform(food=asian)). Given the user utterance and historical turns, the state tracker must infer the user’s current goals. As such, we can view dialog state tracking as a sparse sequential multi-class classification problem. Mod"
2020.emnlp-main.274,K16-1002,0,0.0131797,"(r) (r) (g) (g) (g) (s) (s) (s) (u) (u) (u) pθ (rt |ht , z(c) , zt ) = D θ (ht , z(c) , zt ) pθ (gt |ht , . . . , zt ) = D θ (ht , . . . , zt ) pθ (st |ht , . . . , zt ) = D θ (ht , . . . , zt ) Inference collapse is a relatively common phenomenon among autoregressive VAE structures (Zhao et al., 2017). The hierarchical and recurrent nature of our model makes it especially vulnerable. The standard treatment for alleviating the inference collapse problem include (1) annealing the KL-divergence term weight during the initial training stage and (2) employing word dropouts on the decoder inputs (Bowman et al., 2016). For our model, we observe that the basic techniques are insufficient (Table 3). While more recent treatments exist (Kim et al., 2018; He et al., 2019), they incur high computational costs that prohibit practical deployment in our cases. We introduce two simpler but effective methods to prevent encoder degeneration. Mutual Information Maximization. The KLdivergence term in the standard VAE ELBO can be decomposed to reveal the mutual information term (Hoffman and Johnson, 2016): pθ (ut |ht , . . . , zt ) = D θ (ht , . . . , zt ). Epd [DKL (qφ (z |x)kp(z))] = The utterance decoder D (u) is impl"
2020.emnlp-main.274,W04-1013,0,0.0100549,"81 0.153 0.162 0.154 1 User Ours 3 User W O Z2.0 a VHUS VHDAb VHDAb a W/O GT (G¨ur et al., 2018) E NT ACC E NT 0.322 0.408 0.460 0.056 0.079 0.080 0.367 0.460 0.554 0.024 0.034 0.043 b 5 User Ours 7 User dropouts while generating more coherent samples (evident from higher data augmentation results). Language Evaluation To understand the effect of joint learning of various dialog features on language generation, we compare our model with a model that only learns linguistic features. Following the evaluation protocol from prior work (Wen et al., 2017; Bak and Oh, 2019), we use ROUGE-L F1-score (Lin, 2004) to evaluate the linguistic quality and utterance-level unigram cross-entropy (Serban et al., 2017) (regarding the training corpus distribution) to evaluate diversity. Table 4 shows that our model generates better and more diverse utterances than the previous strong baseline on conversation modeling. These results supports the idea that joint learning of dialog annotations improves utterance generation, thereby increasing the chance of generating novel samples that improve the downstream trackers. 4.4 User Simulation Evaluation Simulating human participants has become a crucial feature for tra"
2020.emnlp-main.274,W18-4701,1,0.844142,"gs as pre-trained word emebddings (400 dimensions total) for word and dialog act tokens. All models used Adam optimizer (Kingma and Ba, 2014) with the initial learning rate of 1e-3, We annealed the KL-divergence weights over 250,000 training steps. For data synthesis, we employ ancestral sampling to generate samples from the empirical posterior distribution. We fixed the ratio of synthetic to original data samples to 1. Datasets. We conduct experiments on four state tracking corpora: WoZ2.0 (Wen et al., 2017), DSTC2 (Henderson et al., 2014), MultiWoZ (Budzianowski et al., 2018), and DialEdit (Manuvinakurike et al., 2018). These corpora cover a variety of domains (restaurant booking, hotel reservation, and image editing). Note that, because the MultiWoZ dataset is a multi-domain corpus, we extract single-domain dialog samples from the two most prominent domains (hotel and restaurant, denoted by MultiWoZ-H and MultiWoZ-R, respectively). Dialog State Trackers. We use GLAD and GCE as the two competitive baselines for state tracking. Besides, modifications are applied to these trackers to stabilize the performance on random seeds (denoted as GLAD+ and GCE+ ). Specifically, we enrich the word embeddings with subwor"
2020.emnlp-main.274,W14-4340,0,\N,Missing
2020.emnlp-main.274,D14-1162,0,\N,Missing
2020.emnlp-main.274,Q17-1010,0,\N,Missing
2020.emnlp-main.274,P18-1135,0,\N,Missing
2020.emnlp-main.274,P18-1133,0,\N,Missing
2020.emnlp-main.274,N18-1162,0,\N,Missing
2020.emnlp-main.274,D19-1202,0,\N,Missing
2020.emnlp-main.274,D19-1670,0,\N,Missing
2020.emnlp-main.274,W13-4067,0,\N,Missing
2020.emnlp-main.274,N07-2038,0,\N,Missing
2020.emnlp-main.274,P17-1163,0,\N,Missing
2020.eval4nlp-1.4,D19-1220,0,0.0210595,"). Related Work Caption Evaluation We provide a summary of the widely used metrics for evaluating image captions such as n-gram similarity metrics, embedding based metrics, and other task-specific metrics for captioning. Captioning Specific Metrics After CIDEr is introduced, several metrics for image captioning are proposed. SPICE (Anderson et al., 2016) uses scene graph and LEIC (Cui et al., 2018) uses the trainable model to evaluate the captions. VIFIDEL (Madhyastha et al., 2019) is an extension of Wasserstein distance that utilizes the information from detected objects in the image. TIGEr (Jiang et al., 2019) uses the output of the visual grounding task. BERT-TBR (Yi et al., 2020) focuses on the variance of the captions and combine multiple reference captions to get improved BERTScore. N-gram Similarity Metrics The most widely used metrics for evaluating the quality of text generation tasks are n-gram similarity metrics that compute the exact number of n-gram matches between reference and generated text. One example of these metrics is BLEU (Papineni et al., 2002) that computes the precision of overlap n-gram between reference and candidate. ROUGE (Lin, 2004) is a set of commonly used metrics for"
2020.eval4nlp-1.4,D19-1051,0,0.0539631,"Missing"
2020.eval4nlp-1.4,P19-1264,0,0.0161365,"s with consideration to the image. Most of the previous studies on evaluating image captioning tasks rely on n-gram similarity metrics such as BLEU (Papineni et al., 2002) or CIDEr (Vedantam et al., 2015). These approaches bear limitations in dealing with the text’s diverse nature, similarly found in other text generation tasks (e.g., abstractive summarization and dialog) (Kryscinski et al., 2019; Liu et al., 2016). To alleviate the issues in the n-gram based approaches, researchers proposed word embedding-based techniques (Kusner et al., 2015; Zhang et al., 2019; Zhao et al., 2019; Lo, 2019; Clark et al., 2019). These techniques shows robust performance and achieve higher correlation with human judgment than that of other previous metrics in many text 1 https://github.com/hwanheelee1993/ViLBERTScore 34 Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP), pages 34–39, c November 20, 2020. 2020 Association for Computational Linguistics Reference Caption ? A boy with an orange shirt smiles , while a boy in a blue shirt looks on ?? ? ?? ? Pairwise Cosine Similarity Maximum Similarity ?? ? : ViLBERT ෝ Candidate Caption ? A young boy in a blue shirt is sitting on a wo"
2020.eval4nlp-1.4,D16-1230,0,0.0145981,"ation algorithms (Vinyals et al., 2015; Anderson et al., 2018) and target datasets (Fang et al., 2015; Sharma et al., 2018), few studies have focused on assessing the quality of the generated captions with consideration to the image. Most of the previous studies on evaluating image captioning tasks rely on n-gram similarity metrics such as BLEU (Papineni et al., 2002) or CIDEr (Vedantam et al., 2015). These approaches bear limitations in dealing with the text’s diverse nature, similarly found in other text generation tasks (e.g., abstractive summarization and dialog) (Kryscinski et al., 2019; Liu et al., 2016). To alleviate the issues in the n-gram based approaches, researchers proposed word embedding-based techniques (Kusner et al., 2015; Zhang et al., 2019; Zhao et al., 2019; Lo, 2019; Clark et al., 2019). These techniques shows robust performance and achieve higher correlation with human judgment than that of other previous metrics in many text 1 https://github.com/hwanheelee1993/ViLBERTScore 34 Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP), pages 34–39, c November 20, 2020. 2020 Association for Computational Linguistics Reference Caption ? A boy with a"
2020.eval4nlp-1.4,N19-1423,0,0.0360551,"I, reference caption x and candidate caption x ˆ, we compute contextual embeddings with ViLBERT for x and x ˆ respectively. Then, we extract the text embeddings HX V and HXˆ V for each output embedding. Finally, we compute the pairwise cosine similarity between HX V and HXˆ V as in (Zhang et al., 2019). effective in evaluating image captioning tasks. 2 2.1 WMD computes minimum transportation distance among tokens using pre-trained word embeddings (i.e., GloVe (Pennington et al., 2014)). On the other hand, BERTScore computes cosine similarity among tokens using contextual embeddings from BERT (Devlin et al., 2019). Related Work Caption Evaluation We provide a summary of the widely used metrics for evaluating image captions such as n-gram similarity metrics, embedding based metrics, and other task-specific metrics for captioning. Captioning Specific Metrics After CIDEr is introduced, several metrics for image captioning are proposed. SPICE (Anderson et al., 2016) uses scene graph and LEIC (Cui et al., 2018) uses the trainable model to evaluate the captions. VIFIDEL (Madhyastha et al., 2019) is an extension of Wasserstein distance that utilizes the information from detected objects in the image. TIGEr (J"
2020.eval4nlp-1.4,W19-5358,0,0.0153432,"ed captions with consideration to the image. Most of the previous studies on evaluating image captioning tasks rely on n-gram similarity metrics such as BLEU (Papineni et al., 2002) or CIDEr (Vedantam et al., 2015). These approaches bear limitations in dealing with the text’s diverse nature, similarly found in other text generation tasks (e.g., abstractive summarization and dialog) (Kryscinski et al., 2019; Liu et al., 2016). To alleviate the issues in the n-gram based approaches, researchers proposed word embedding-based techniques (Kusner et al., 2015; Zhang et al., 2019; Zhao et al., 2019; Lo, 2019; Clark et al., 2019). These techniques shows robust performance and achieve higher correlation with human judgment than that of other previous metrics in many text 1 https://github.com/hwanheelee1993/ViLBERTScore 34 Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP), pages 34–39, c November 20, 2020. 2020 Association for Computational Linguistics Reference Caption ? A boy with an orange shirt smiles , while a boy in a blue shirt looks on ?? ? ?? ? Pairwise Cosine Similarity Maximum Similarity ?? ? : ViLBERT ෝ Candidate Caption ? A young boy in a blue shi"
2020.eval4nlp-1.4,P19-1654,0,0.0896887,"., 2014)). On the other hand, BERTScore computes cosine similarity among tokens using contextual embeddings from BERT (Devlin et al., 2019). Related Work Caption Evaluation We provide a summary of the widely used metrics for evaluating image captions such as n-gram similarity metrics, embedding based metrics, and other task-specific metrics for captioning. Captioning Specific Metrics After CIDEr is introduced, several metrics for image captioning are proposed. SPICE (Anderson et al., 2016) uses scene graph and LEIC (Cui et al., 2018) uses the trainable model to evaluate the captions. VIFIDEL (Madhyastha et al., 2019) is an extension of Wasserstein distance that utilizes the information from detected objects in the image. TIGEr (Jiang et al., 2019) uses the output of the visual grounding task. BERT-TBR (Yi et al., 2020) focuses on the variance of the captions and combine multiple reference captions to get improved BERTScore. N-gram Similarity Metrics The most widely used metrics for evaluating the quality of text generation tasks are n-gram similarity metrics that compute the exact number of n-gram matches between reference and generated text. One example of these metrics is BLEU (Papineni et al., 2002) th"
2020.eval4nlp-1.4,D19-1053,0,0.0203989,"lity of the generated captions with consideration to the image. Most of the previous studies on evaluating image captioning tasks rely on n-gram similarity metrics such as BLEU (Papineni et al., 2002) or CIDEr (Vedantam et al., 2015). These approaches bear limitations in dealing with the text’s diverse nature, similarly found in other text generation tasks (e.g., abstractive summarization and dialog) (Kryscinski et al., 2019; Liu et al., 2016). To alleviate the issues in the n-gram based approaches, researchers proposed word embedding-based techniques (Kusner et al., 2015; Zhang et al., 2019; Zhao et al., 2019; Lo, 2019; Clark et al., 2019). These techniques shows robust performance and achieve higher correlation with human judgment than that of other previous metrics in many text 1 https://github.com/hwanheelee1993/ViLBERTScore 34 Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP), pages 34–39, c November 20, 2020. 2020 Association for Computational Linguistics Reference Caption ? A boy with an orange shirt smiles , while a boy in a blue shirt looks on ?? ? ?? ? Pairwise Cosine Similarity Maximum Similarity ?? ? : ViLBERT ෝ Candidate Caption ? A young boy in"
2020.eval4nlp-1.4,P02-1040,0,0.107127,"rics. This result demonstrates that the use of contextualized embedding from vision and language is Introduction Image captioning is a task that aims to generate a text that describes a given image. While there have been many advances for caption generation algorithms (Vinyals et al., 2015; Anderson et al., 2018) and target datasets (Fang et al., 2015; Sharma et al., 2018), few studies have focused on assessing the quality of the generated captions with consideration to the image. Most of the previous studies on evaluating image captioning tasks rely on n-gram similarity metrics such as BLEU (Papineni et al., 2002) or CIDEr (Vedantam et al., 2015). These approaches bear limitations in dealing with the text’s diverse nature, similarly found in other text generation tasks (e.g., abstractive summarization and dialog) (Kryscinski et al., 2019; Liu et al., 2016). To alleviate the issues in the n-gram based approaches, researchers proposed word embedding-based techniques (Kusner et al., 2015; Zhang et al., 2019; Zhao et al., 2019; Lo, 2019; Clark et al., 2019). These techniques shows robust performance and achieve higher correlation with human judgment than that of other previous metrics in many text 1 https:"
2020.eval4nlp-1.4,D14-1162,0,0.0946659,"t is sitting on a wooden bench : Image Embedding ?? ? : Text Embedding Figure 2: Overall computation of ViLBERTScore. Given the image I, reference caption x and candidate caption x ˆ, we compute contextual embeddings with ViLBERT for x and x ˆ respectively. Then, we extract the text embeddings HX V and HXˆ V for each output embedding. Finally, we compute the pairwise cosine similarity between HX V and HXˆ V as in (Zhang et al., 2019). effective in evaluating image captioning tasks. 2 2.1 WMD computes minimum transportation distance among tokens using pre-trained word embeddings (i.e., GloVe (Pennington et al., 2014)). On the other hand, BERTScore computes cosine similarity among tokens using contextual embeddings from BERT (Devlin et al., 2019). Related Work Caption Evaluation We provide a summary of the widely used metrics for evaluating image captions such as n-gram similarity metrics, embedding based metrics, and other task-specific metrics for captioning. Captioning Specific Metrics After CIDEr is introduced, several metrics for image captioning are proposed. SPICE (Anderson et al., 2016) uses scene graph and LEIC (Cui et al., 2018) uses the trainable model to evaluate the captions. VIFIDEL (Madhyast"
2020.eval4nlp-1.4,P18-1238,0,0.151175,"Missing"
2020.eval4nlp-1.4,2020.acl-main.93,0,0.654145,"etrics for evaluating image captions such as n-gram similarity metrics, embedding based metrics, and other task-specific metrics for captioning. Captioning Specific Metrics After CIDEr is introduced, several metrics for image captioning are proposed. SPICE (Anderson et al., 2016) uses scene graph and LEIC (Cui et al., 2018) uses the trainable model to evaluate the captions. VIFIDEL (Madhyastha et al., 2019) is an extension of Wasserstein distance that utilizes the information from detected objects in the image. TIGEr (Jiang et al., 2019) uses the output of the visual grounding task. BERT-TBR (Yi et al., 2020) focuses on the variance of the captions and combine multiple reference captions to get improved BERTScore. N-gram Similarity Metrics The most widely used metrics for evaluating the quality of text generation tasks are n-gram similarity metrics that compute the exact number of n-gram matches between reference and generated text. One example of these metrics is BLEU (Papineni et al., 2002) that computes the precision of overlap n-gram between reference and candidate. ROUGE (Lin, 2004) is a set of commonly used metrics for text summarization. In particular, ROUGE-N, the longest common subsequenc"
2020.findings-emnlp.65,D16-1257,0,0.0569226,"Missing"
2020.findings-emnlp.65,D18-1217,0,0.0516224,"Missing"
2020.findings-emnlp.65,D19-1422,0,0.0851046,"edictions. For example, for their machine translation predictions, Bahdanau et al. (2014) show a heat map of attention weights from source language words to target language words. Similarly, in transformer architectures (Vaswani et al., 2017), a selfattention head produces attention distributions from the input words to the same input words, as shown in the second row on the right side of Figure 1. However, self-attention mechanisms have multiple heads, making the combined outputs difficult to interpret. Recent work in multi-label text classification (Xiao et al., 2019) and sequence labeling (Cui and Zhang, 2019) shows the efficiency and interpretability of label-specific representations. We introduce the Label Attention Layer: a modified version of self-attention, where each classification label corresponds to one or more attention heads. We project the output at the attention head level, rather than after aggregating all outputs, to preserve the source of head-specific information, thus allowing us to match labels to heads. To test our proposed Label Attention Layer, we build upon the parser of Zhou and Zhao (2019) and establish a new state of the art for both constituency and dependency parsing, in"
2020.findings-emnlp.65,P81-1022,0,0.702861,"Missing"
2020.findings-emnlp.65,N16-1024,0,0.0664124,"Missing"
2020.findings-emnlp.65,N19-1076,0,0.0204044,"Missing"
2020.findings-emnlp.65,P18-2075,0,0.0254246,"Missing"
2020.findings-emnlp.65,P17-2025,0,0.0345951,"Missing"
2020.findings-emnlp.65,N18-1091,0,0.0365574,"Missing"
2020.findings-emnlp.65,N19-1357,0,0.0237396,"age modeling (Salton et al., 2017). Self-attention and transformer architectures (Vaswani et al., 2017) are now the state of the art in language understanding (Devlin et al., 2018; Yang et al., 2019), extractive summarization (Liu, 2019), semantic role labeling (Strubell et al., 2018) and machine translation for low-resource languages (Rikters, 2018; Rikters et al., 2018). While attention mechanisms can provide explanations for model predictions, Serrano and Smith (2019) challenge that assumption and find that attention weights only noisily predict overall importance with regard to the model. Jain and Wallace (2019) find that attention distributions rarely correlate with feature importance weights. However, Wiegreffe and Pinter (2019) show through alternative tests that prior work does not discredit the usefulness of attention for interpretability. Xiao et al. (2019) introduce the Label-Specific Attention Network (LSAN) for multi-label document classification. They use label descriptions to compute attention scores for words, and follow the self-attention of Lin et al. (2017). Cui and Zhang (2019) introduce a Label Attention Inference Layer for sequence labeling, which uses the self-attention of Vaswani"
2020.findings-emnlp.65,P19-1237,0,0.0435874,"Missing"
2020.findings-emnlp.65,P18-1249,0,0.121788,"final word representation, as shown in the color-coded vectors in Figure 4. The activation functions of the position-wise feed-forward layer make it difficult to follow the path of the contributions. Therefore we can remove the position-wise feed-forward layer, and compute the contributions from each label. We provide an example in Figure 6, where the contributions are computed using normalization and averaging. In this case, we are computing the contributions of each head to the span vector. The span representation for “the person” is computed following the method of Gaddy et al. (2018) and Kitaev and Klein (2018). However, forward and backward represenEncoder Our parser is an encoder-decoder model. The encoder has self-attention layers (Vaswani et al., 2017), preceding the Label Attention Layer. We follow the attention partition of Kitaev and Klein (2018), who show that separating content embeddings from position ones improves performance. Sentences are pre-processed following Zhou and Zhao (2019). Trees are represented using a simplified Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994). In Zhou and Zhao (2019), two kinds of span representations are proposed: the division span and t"
2020.findings-emnlp.65,E17-1117,0,0.039069,"Missing"
2020.findings-emnlp.65,D16-1180,0,0.0639129,"Missing"
2020.findings-emnlp.65,L18-1595,0,0.0609438,"Missing"
2020.findings-emnlp.65,C18-1271,0,0.0199651,"Missing"
2020.findings-emnlp.65,Q17-1029,0,0.0147098,"lar, the last row shows that replacing our LAL with a self-attention layer with an equal number of attention heads decreases performance: the difference between the performance of the first row and the last row is due to the Label Attention Layer’s architecture novelties. 4.4 English and Chinese Results Our best-performing English-language parser does not have residual dropout, but has a position-wise feed-forward layer. We train Chinese-language parsers using the same configuration. The Chinese Treebank has two data splits for the training, development and testing sets: one for Constituency (Liu and Zhang, 2017b) and one for Dependency parsing (Zhang and Clark, 2008). Finally, we compare our results with the state of the art in constituency and dependency parsing in both English and Chinese. We show our Constituency Parsing results in Table 3, and our Dependency Parsing results in Table 4. Our LAL parser establishes new state-of-the-art results in both languages, improving significantly in dependency parsing. 4.5 Interpreting Head Contributions We follow the method in Figure 6 to identify which attention heads contribute to predictions. We collect the span vectors from the Penn Treebank test set, an"
2020.findings-emnlp.65,Q17-1004,0,0.0235094,"lar, the last row shows that replacing our LAL with a self-attention layer with an equal number of attention heads decreases performance: the difference between the performance of the first row and the last row is due to the Label Attention Layer’s architecture novelties. 4.4 English and Chinese Results Our best-performing English-language parser does not have residual dropout, but has a position-wise feed-forward layer. We train Chinese-language parsers using the same configuration. The Chinese Treebank has two data splits for the training, development and testing sets: one for Constituency (Liu and Zhang, 2017b) and one for Dependency parsing (Zhang and Clark, 2008). Finally, we compare our results with the state of the art in constituency and dependency parsing in both English and Chinese. We show our Constituency Parsing results in Table 3, and our Dependency Parsing results in Table 4. Our LAL parser establishes new state-of-the-art results in both languages, improving significantly in dependency parsing. 4.5 Interpreting Head Contributions We follow the method in Figure 6 to identify which attention heads contribute to predictions. We collect the span vectors from the Penn Treebank test set, an"
2020.findings-emnlp.65,I17-1045,0,0.0284859,"ise, for the 193 spans labelled as S but not predicted as such, the top-contributing head of 141 of them is one of the four top-contributing heads for spans predicted as S. This suggests that a stronger prediction link to the label attention heads, through a loss function for instance, may increase the performance. 5 Related Work Since their introduction in Machine Translation, attention mechanisms (Bahdanau et al., 2014; Luong et al., 2015) have been extended to other tasks, such as text classification (Yang et al., 2016), natural language inference (Chen et al., 2016) and language modeling (Salton et al., 2017). Self-attention and transformer architectures (Vaswani et al., 2017) are now the state of the art in language understanding (Devlin et al., 2018; Yang et al., 2019), extractive summarization (Liu, 2019), semantic role labeling (Strubell et al., 2018) and machine translation for low-resource languages (Rikters, 2018; Rikters et al., 2018). While attention mechanisms can provide explanations for model predictions, Serrano and Smith (2019) challenge that assumption and find that attention weights only noisily predict overall importance with regard to the model. Jain and Wallace (2019) find that"
2020.findings-emnlp.65,P19-1282,0,0.0200858,"., 2015) have been extended to other tasks, such as text classification (Yang et al., 2016), natural language inference (Chen et al., 2016) and language modeling (Salton et al., 2017). Self-attention and transformer architectures (Vaswani et al., 2017) are now the state of the art in language understanding (Devlin et al., 2018; Yang et al., 2019), extractive summarization (Liu, 2019), semantic role labeling (Strubell et al., 2018) and machine translation for low-resource languages (Rikters, 2018; Rikters et al., 2018). While attention mechanisms can provide explanations for model predictions, Serrano and Smith (2019) challenge that assumption and find that attention weights only noisily predict overall importance with regard to the model. Jain and Wallace (2019) find that attention distributions rarely correlate with feature importance weights. However, Wiegreffe and Pinter (2019) show through alternative tests that prior work does not discredit the usefulness of attention for interpretability. Xiao et al. (2019) introduce the Label-Specific Attention Network (LSAN) for multi-label document classification. They use label descriptions to compute attention scores for words, and follow the self-attention of"
2020.findings-emnlp.65,P18-1108,0,0.0932308,"Missing"
2020.findings-emnlp.65,P17-1076,0,0.217892,"erson Categ <VP> HEAD driving 3.3 Dependency Parsing NP NP DT the 2 DT the 2 VP NN person 3 NN person 3 VBG driving 4 VBG driving 4 Figure 5: Parsing representations of the example sentence in Figure 2. where LN is Layer Normalization, and W1 , W2 , b1 and b2 are learned parameters. For the l-th syntactic category, the corresponding score s(i, j, l) is then the l-th value in the S(i, j) vector. Consequently, the score of a constituency parse tree T is the sum of all of the scores of its spans and their syntactic categories: s(T ) = X s(i, j, l) (7) (i,j,l)∈T We then use a CKY-style algorithm (Stern et al., 2017; Gaddy et al., 2018) to find the highest scoring tree Tˆ. The model is trained to find the correct parse tree T ∗ , such that for all trees T , the following margin constraint is satisfied: s(T ∗ ) ≥ s(T ) + ∆(T, T ∗ ) (8) where ∆ is the Hamming loss on labeled spans. The corresponding loss function is the hinge loss: We use the biaffine attention mechanism (Dozat and Manning, 2016) to compute a probability distribution for the dependency head of each word. The child-parent score αij for the j-th word to be the head of the i-th word is: (d) T αij = hi (h) (d) (h) Whj +UT hi +VT hj +b (10) (d)"
2020.findings-emnlp.65,D15-1166,0,0.336292,"arn relations between syntactic categories and show pathways to analyze errors. WiVX Repeated Self-Attention Head Ai WiVX Select the person Aggregating with output from other heads Select the person Select the person X WiKX Matrix Projection Matrix Projection From other heads X Select the Select the person Select the person person Figure 1: Comparison of the attention head architectures of our proposed Label Attention Layer and a SelfAttention Layer (Vaswani et al., 2017). The matrix X represents the input sentence “Select the person”. Introduction Attention mechanisms (Bahdanau et al., 2014; Luong et al., 2015) provide arguably explainable attention distributions that can help to interpret predictions. For example, for their machine translation predictions, Bahdanau et al. (2014) show a heat map of attention weights from source language words to target language words. Similarly, in transformer architectures (Vaswani et al., 2017), a selfattention head produces attention distributions from the input words to the same input words, as shown in the second row on the right side of Figure 1. However, self-attention mechanisms have multiple heads, making the combined outputs difficult to interpret. Recent"
2020.findings-emnlp.65,D18-1548,0,0.0554408,"Missing"
2020.findings-emnlp.65,I17-1007,0,0.040274,"Missing"
2020.findings-emnlp.65,P18-2097,0,0.0224856,"Missing"
2020.findings-emnlp.65,P18-1130,0,0.0342223,"Missing"
2020.findings-emnlp.65,D18-1489,0,0.0224491,"Missing"
2020.findings-emnlp.65,J93-2004,0,0.0719252,"g Prediction: Noun Phrase (NP) Span Representation of “the person” Normalization and Average Fraction of contribution from the heads to the span vector Heads #1 #2 #3 #4 Here, heads #1 and #2 have the highest contributions to predicting “the person” as a noun phrase. Figure 6: If we remove the position-wise feed-forward layer, we can compute the contributions from each label attention head to the span representation, and thus interpret head contributions. This illustrative example follows the label color scheme in Figure 4. 3.4 Decoder We evaluate our model on the English Penn Treebank (PTB) (Marcus et al., 1993) and on the Chinese Treebank (CTB) (Xue et al., 2005). We use the Stanford tagger (Toutanova et al., 2003) to predict part-of-speech tags and follow standard data splits. Following standard practice, we use the EVALB algorithm (Sekine and Collins, 1997) for constituency parsing, and report results without punctuation for dependency parsing. the model is not designed to have a one-on-one correspondence between attention heads and syntactic categories. The Chinese Treebank is a smaller dataset, and therefore we use 64 heads in Chineselanguage experiments, even though the number of Chinese syntac"
2020.findings-emnlp.65,C18-1011,0,0.0340458,"Missing"
2020.findings-emnlp.65,N03-1033,0,0.0900673,"f contribution from the heads to the span vector Heads #1 #2 #3 #4 Here, heads #1 and #2 have the highest contributions to predicting “the person” as a noun phrase. Figure 6: If we remove the position-wise feed-forward layer, we can compute the contributions from each label attention head to the span representation, and thus interpret head contributions. This illustrative example follows the label color scheme in Figure 4. 3.4 Decoder We evaluate our model on the English Penn Treebank (PTB) (Marcus et al., 1993) and on the Chinese Treebank (CTB) (Xue et al., 2005). We use the Stanford tagger (Toutanova et al., 2003) to predict part-of-speech tags and follow standard data splits. Following standard practice, we use the EVALB algorithm (Sekine and Collins, 1997) for constituency parsing, and report results without punctuation for dependency parsing. the model is not designed to have a one-on-one correspondence between attention heads and syntactic categories. The Chinese Treebank is a smaller dataset, and therefore we use 64 heads in Chineselanguage experiments, even though the number of Chinese syntactic categories is much higher. For both languages, the query, key and value vectors, as well as the output"
2020.findings-emnlp.65,D18-1311,0,0.0280675,"Missing"
2020.findings-emnlp.65,P19-1230,0,0.0870595,"nt work in multi-label text classification (Xiao et al., 2019) and sequence labeling (Cui and Zhang, 2019) shows the efficiency and interpretability of label-specific representations. We introduce the Label Attention Layer: a modified version of self-attention, where each classification label corresponds to one or more attention heads. We project the output at the attention head level, rather than after aggregating all outputs, to preserve the source of head-specific information, thus allowing us to match labels to heads. To test our proposed Label Attention Layer, we build upon the parser of Zhou and Zhao (2019) and establish a new state of the art for both constituency and dependency parsing, in both English and Chinese. We also release our pre-trained parsers, as well as our code to encourage experiments with the Label Attention Layer 1 . 2 Label Attention Layer The self-attention mechanism of Vaswani et al. (2017) propagates information between the words of a sentence. Each resulting word representation 1 Available at: GitHub.com/KhalilMrini/LAL-Parser 731 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 731–742 c November 16 - 20, 2020. 2020 Association for Computation"
2020.findings-emnlp.65,D19-1002,0,0.0192979,"tate of the art in language understanding (Devlin et al., 2018; Yang et al., 2019), extractive summarization (Liu, 2019), semantic role labeling (Strubell et al., 2018) and machine translation for low-resource languages (Rikters, 2018; Rikters et al., 2018). While attention mechanisms can provide explanations for model predictions, Serrano and Smith (2019) challenge that assumption and find that attention weights only noisily predict overall importance with regard to the model. Jain and Wallace (2019) find that attention distributions rarely correlate with feature importance weights. However, Wiegreffe and Pinter (2019) show through alternative tests that prior work does not discredit the usefulness of attention for interpretability. Xiao et al. (2019) introduce the Label-Specific Attention Network (LSAN) for multi-label document classification. They use label descriptions to compute attention scores for words, and follow the self-attention of Lin et al. (2017). Cui and Zhang (2019) introduce a Label Attention Inference Layer for sequence labeling, which uses the self-attention of Vaswani et al. (2017). In this case, the key and value vectors are learned label embeddings, and the query vectors are hidden vec"
2020.findings-emnlp.65,D19-1044,0,0.126153,"istributions that can help to interpret predictions. For example, for their machine translation predictions, Bahdanau et al. (2014) show a heat map of attention weights from source language words to target language words. Similarly, in transformer architectures (Vaswani et al., 2017), a selfattention head produces attention distributions from the input words to the same input words, as shown in the second row on the right side of Figure 1. However, self-attention mechanisms have multiple heads, making the combined outputs difficult to interpret. Recent work in multi-label text classification (Xiao et al., 2019) and sequence labeling (Cui and Zhang, 2019) shows the efficiency and interpretability of label-specific representations. We introduce the Label Attention Layer: a modified version of self-attention, where each classification label corresponds to one or more attention heads. We project the output at the attention head level, rather than after aggregating all outputs, to preserve the source of head-specific information, thus allowing us to match labels to heads. To test our proposed Label Attention Layer, we build upon the parser of Zhou and Zhao (2019) and establish a new state of the art for"
2020.findings-emnlp.65,N16-1174,0,0.0527521,"them is either head 35 or 47, both top-contributing heads of spans predicted as NP. Likewise, for the 193 spans labelled as S but not predicted as such, the top-contributing head of 141 of them is one of the four top-contributing heads for spans predicted as S. This suggests that a stronger prediction link to the label attention heads, through a loss function for instance, may increase the performance. 5 Related Work Since their introduction in Machine Translation, attention mechanisms (Bahdanau et al., 2014; Luong et al., 2015) have been extended to other tasks, such as text classification (Yang et al., 2016), natural language inference (Chen et al., 2016) and language modeling (Salton et al., 2017). Self-attention and transformer architectures (Vaswani et al., 2017) are now the state of the art in language understanding (Devlin et al., 2018; Yang et al., 2019), extractive summarization (Liu, 2019), semantic role labeling (Strubell et al., 2018) and machine translation for low-resource languages (Rikters, 2018; Rikters et al., 2018). While attention mechanisms can provide explanations for model predictions, Serrano and Smith (2019) challenge that assumption and find that attention weights only noi"
2020.findings-emnlp.87,D18-1217,0,0.0243125,"ed sparse transformer and cross attention information fusion outperform previous systems adapted from the machine translation and graph generation literature. We further contribute our large graph modification datasets to the research community to encourage future research for this new problem. 1 Introduction Parsing text into structured semantics representation is one of the most long-standing and active research problems in NLP. Numerous parsing methods have been developed for many different semantic structure representations (Chen and Manning, 2014; Mrini et al., 2019; Zhou and Zhao, 2019; Clark et al., 2018; Wang et al., 2018). However, most of these previous works focus on parsing a single sentence, while a typical human-computer interaction session or conversation is not singleturn. A prominent example is image search. Users usually start with short phrases describing the main objects or topics they are looking for. Depending on the result, the users may then modify their query to add more constraints or give additional information. In this case, without the modification capability, a static representation is not suitable to track the changing intent of the user. We argue that the back-and-for"
2020.findings-emnlp.87,Q19-1019,0,0.355462,"e (i), the system copies the source graph to the target graph3 . In the “Text2Text” baseline (ii), we flatten the graph and reconstruct the natural sentence similarly to the modification query. In the “Modified GraphRNN” baseline (iii), we use the breadth-first-search (BFS) based node ordering to flatten the graph4 , and use RNNs as the encoders (You et al., 2018) and a decoder similar to our systems. In the final two baselines, “Graph Transformer” (iv) and “Deep Convolutional Graph Networks” (DCGCN) (v), we use the Graph Transformers (Cai and Lam, 2019) and Deep Convolutional Graph Networks (Guo et al., 2019) to encode the source graph (the decoder is identical to ours). boy boy in shirt shirt in &lt;attribute> black black &lt;null> &lt;attribute> Figure 6: Adjacency matrix style decoder. We use an attentional decoder using GRU units for generating edges. It operates similarly to the node-level decoder using Equation 11 and Equation 12. For more accurate typed edge generation, however, we incorporate the hidden states of the source and target nodes (from the node decoder) as inputs when updating the hidden state of the edge decoder: N E hEi,j = GRUE (zi,j−1 , hN i , hj , hi,j−1 ), (14) where hEi,j is the h"
2020.findings-emnlp.87,U19-1013,1,0.829644,"nodes from the source graph are not preserved. We believe that the proposed approach can reduce the noise in graph generation, and retain fine-grained details better than the baselines. 5 Related Work Semantic parsing is a sequence-to-graph transduction task, mapping natural language sentences to their meaning representation, e.g. see (Buys and Blunsom, 2017; Iyer et al., 2017; Dong and Lapata, 2018); this is different from our graph conditional semantic parsing. Recently, context-dependent semantic parsing has gained attraction (Iyyer et al., 2017; Srivastava et al., 2017; Suhr et al., 2018; He et al., 2019). Our work focuses on the update of scene graphs based on users’ queries, while previous works model the modifications of semantic representations in multi-turn dialogue systems. Due to their effectiveness, GCNs and graph transformer have been used as graph encoder for graphto-sequence transduction in semantic-based text generation (Bastings et al., 2017; Beck et al., 2018; Guo et al., 2019; Cai and Lam, 2019; Song et al., 2018; Wu et al., 2020). 6 Conclusion In this paper, we explore a novel problem of conditional graph modification, in which a system needs to modify a source graph according"
2020.findings-emnlp.87,P17-1089,0,0.0249495,"evidenced by the first example A. In addition, example B demonstrates when graph transformer observes a longer description, it lacks the capability of fusing the semantics between the source graph and the modification query; then certain nodes from the source graph are not preserved. We believe that the proposed approach can reduce the noise in graph generation, and retain fine-grained details better than the baselines. 5 Related Work Semantic parsing is a sequence-to-graph transduction task, mapping natural language sentences to their meaning representation, e.g. see (Buys and Blunsom, 2017; Iyer et al., 2017; Dong and Lapata, 2018); this is different from our graph conditional semantic parsing. Recently, context-dependent semantic parsing has gained attraction (Iyyer et al., 2017; Srivastava et al., 2017; Suhr et al., 2018; He et al., 2019). Our work focuses on the update of scene graphs based on users’ queries, while previous works model the modifications of semantic representations in multi-turn dialogue systems. Due to their effectiveness, GCNs and graph transformer have been used as graph encoder for graphto-sequence transduction in semantic-based text generation (Bastings et al., 2017; Beck"
2020.findings-emnlp.87,P17-1167,0,0.0327182,"Missing"
2020.findings-emnlp.87,W15-3014,0,0.0302254,"denote the nodes and edges of the graph zG . Given a training dataset of input-output pairs, denoted by D ≡ {(xGd , yd , zGd )}D d=1 , we train the model by maximizing the conditional loglikelihood `CLL = `Node + `Edge where, `Node = X log p(zN |x, y; θN ) (2) (x,y,z)∈D `Edge = X log p(zE |x, y, zN ; θE ). (3) (x,y,z)∈D During learning and decoding, we sort the nodes according to a topological order which exists for all the directed graphs in our user-generated and synthetic datasets. 3.2 Graph-based Encoder-Decoder Model Inspired by the machine translation literature (Bahdanau et al., 2014; Jean et al., 2015), we build our model based on the encoder-decoder framework. Since our task takes a source graph and a modification query as inputs, we need two encoders to model the graph and text information separately. Thus, there are four main components in our model: the query encoder, the graph encoder, the edge decoder and the node decoder. The information flow between the components is shown in Figure 4. In general, we encode the graph and text modification query into a joint representation, then we generate the target graph in two stages. Firstly, the target 975 nodes are generated via a node-level r"
2020.findings-emnlp.87,P18-1068,0,0.0199741,"rst example A. In addition, example B demonstrates when graph transformer observes a longer description, it lacks the capability of fusing the semantics between the source graph and the modification query; then certain nodes from the source graph are not preserved. We believe that the proposed approach can reduce the noise in graph generation, and retain fine-grained details better than the baselines. 5 Related Work Semantic parsing is a sequence-to-graph transduction task, mapping natural language sentences to their meaning representation, e.g. see (Buys and Blunsom, 2017; Iyer et al., 2017; Dong and Lapata, 2018); this is different from our graph conditional semantic parsing. Recently, context-dependent semantic parsing has gained attraction (Iyyer et al., 2017; Srivastava et al., 2017; Suhr et al., 2018; He et al., 2019). Our work focuses on the update of scene graphs based on users’ queries, while previous works model the modifications of semantic representations in multi-turn dialogue systems. Due to their effectiveness, GCNs and graph transformer have been used as graph encoder for graphto-sequence transduction in semantic-based text generation (Bastings et al., 2017; Beck et al., 2018; Guo et al."
2020.findings-emnlp.87,P81-1022,0,0.561842,"Missing"
2020.findings-emnlp.87,D18-1045,0,0.0220468,", 2017; Ren et al., 2018), we consider the scene graph modification problem as follows. Given an initial scene graph and a new query issued by the user, the goal is to generate a new scene graph taking into account the original graph and the new query. We formulate the problem as conditional graph modification, and create three datasets for this problem. We propose novel encoder-decoder architectures for conditional graph modification. More specifically, our graph encoder is built upon the self-attention architecture popular in state-of-theart machine translation models (Vaswani et al., 2017; Edunov et al., 2018), which is superior to, according to our study, Graph Convolutional Networks (GCN) (Kipf and Welling, 2016). Unique to our problem, however, is the fact that we have an open set of relation types in the graphs. Thus, we propose a novel graph-conditioned sparse transformer, in which the relation information is embed972 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 972–990 c November 16 - 20, 2020. 2020 Association for Computational Linguistics A young boy in a black shirt ded directly into the self-attention grid. For the decoder, we treat the graph modification t"
2020.findings-emnlp.87,D15-1166,0,0.0214868,"according to the connections of the sparsely connected transformer as well as all query tokens. The final representation m is taken from the output of transformer. Figure 5 shows the information flow in the cross-attention mechanism. 3.2.4 We use GRU cells (Cho et al., 2014) for our RNN decoders. The node-level decoder is a vanilla autoregressive model described as, N N hN t = GRU (zt−1 , ht−1 ) (10) N N cN t = ATTN (ht , m) (11) Figure 5: Cross-attention fusion. (12) N softmax(W[hN t , ct ] + b), (13) where z&lt;t denotes the nodes generated before time step t, ATTNN is a Luong-style attention (Luong et al., 2015), and m is the memory vectors from information fusion of the encoders (see §3.2.3). 3.2.5 Recall that the parameters of the graph and query encoders are shared to enable encoding of the two sources in the same semantic space. That is, we use the same transformer encoder for both sources. In cross-attention, we concatenate the x (from Equation 4) and y before rather than after the transformer encoder. As such, the encoder’s input is [x, y]. In the transformer, the representation of each query token gets updated by self-attending to the representations of all the query tokens and graph nodes in"
2020.lrec-1.51,D18-1547,0,0.0230663,"Missing"
2020.lrec-1.51,P17-1045,0,0.0268893,"n et al., 2018; Wang et al., 2018; Cheng et al., 2018). Other works focus more on the language side, either by determining whether a caption is true considering a pair of images (Suhr et al., 2019), or generating captions that describes the difference with latent variables (Jhamtani and Berg-Kirkpatrick, 2018) or attention (Tan et al., 2019). Our system is simpler as it grounds language at a low-level, and processes visual information with an independent segmentation module. Dialogue It is easy to imagine a dialogue for information-seeking domains (Raux et al., 2005; Bohus and Rudnicky, 2009; Dhingra et al., 2017; Wen et al., 2017; El Asri et al., 2017; Budzianowski et al., 2018) such as restaurant booking or movie booking, as these conversations exist in everyday life. Nevertheless, the same could not be said for image editing, as users interact with software tools (without language input) (Bychkovsky et al., 2011) or post on online communities (without multi-turn interaction) (Mohapatra, 2018; Tan et al., 2019). For NLIE, several works have crowdsourced novice IERs by giving image or paired images (Manuvinakurike et al., 2018a; Wang et al., 2018; Cheng et al., 2018). However, they did not consider i"
2020.lrec-1.51,W17-5526,0,0.0400704,"Missing"
2020.lrec-1.51,D18-1436,0,0.0196089,"ntics between visual information and language, usually with an image and a natural language description. Our work falls under the category of a more recent setup, where language is related to the difference between paired images. Several works have been proposed for edited image generation using attention for alignment (Chen et al., 2018; Wang et al., 2018; Cheng et al., 2018). Other works focus more on the language side, either by determining whether a caption is true considering a pair of images (Suhr et al., 2019), or generating captions that describes the difference with latent variables (Jhamtani and Berg-Kirkpatrick, 2018) or attention (Tan et al., 2019). Our system is simpler as it grounds language at a low-level, and processes visual information with an independent segmentation module. Dialogue It is easy to imagine a dialogue for information-seeking domains (Raux et al., 2005; Bohus and Rudnicky, 2009; Dhingra et al., 2017; Wen et al., 2017; El Asri et al., 2017; Budzianowski et al., 2018) such as restaurant booking or movie booking, as these conversations exist in everyday life. Nevertheless, the same could not be said for image editing, as users interact with software tools (without language input) (Bychko"
2020.lrec-1.51,D14-1086,0,0.0133264,"e form, and includes all the low-level edit arguments. We refer to these specific type of sentences as “Imperative Low-Level Complete Image Edit Requests (ILLC-IERs)”. Collection We crowd-sourced ILLC-IERs using the Amazon Mechanical Turk (AMT) platform. We asked workers to provide a sentence that specifies an image edit and contains REFER, ATTRIBUTE, VALUE elements. For these slots, we provide (i) an image with a highlighted object, (ii) several referring expressions of the highlighted object, (iii) a randomly sampled ATTRIBUTE, and (iv) a randomly sampled VALUE. We used the RefCOCO dataset (Kazemzadeh et al., 2014) for (i) and (ii), and sampled 900 from train, 100 from dev, and 100 from test. Annotation Similar to the data collection, we also crowdsourced annotations using AMT. We choose 4 categories for ILLC-IER BIO tagging (i) ACTION (ii) REFER (iii) ATTRIBUTE (iv) VALUE. REFER , ATTRIBUTE , VALUE are slots in our ontology. ACTION is the imperative verb that corresponds to the edit action (e.g., “increase”, “decrease”, “modify”) and indicates the sign of VALUE. For example, “decrease brightness by 10” is equivalent to “change brightness by -10”. Statistics We collected 2,537 ILLC-IERs. Based on the da"
2020.lrec-1.51,J12-1006,0,0.0303099,"Missing"
2020.lrec-1.51,L18-1683,1,0.90669,"e images via a two-stage process: (i) First, experts look at the original image and the IER and interprets the high-level concepts expressed by the novice. (ii) Second, they come up with one or more low-level edit operations for these concepts and apply these edits. Can we build machines that do the same thing? This motivates the study on Natural Language Image Editing (NLIE). There are mainly two approaches towards NLIE. The two-stage approach follows the editing process of experts. Several works have developed semantic parsers for the first stage, with datasets collected from crowdsourcing (Manuvinakurike et al., 2018a), online image editing communities (Mohapatra, 2018), or spoken conversation with experts (Manuvinakurike et al., 2018b). Though these semantic parsers are able to capture the high-level intent of novices, little has been discussed about the second stage – how to infer edits with these parsers. These datasets still require expert annotations for generating edits, which is a difficult one-to-many mapping. On the other hand, the image generation approach directly generates an edited image given the original image and an IER using the end-toend adversarial learning (Goodfellow et al., 2014) fra"
2020.lrec-1.51,W18-5033,1,0.918865,"e images via a two-stage process: (i) First, experts look at the original image and the IER and interprets the high-level concepts expressed by the novice. (ii) Second, they come up with one or more low-level edit operations for these concepts and apply these edits. Can we build machines that do the same thing? This motivates the study on Natural Language Image Editing (NLIE). There are mainly two approaches towards NLIE. The two-stage approach follows the editing process of experts. Several works have developed semantic parsers for the first stage, with datasets collected from crowdsourcing (Manuvinakurike et al., 2018a), online image editing communities (Mohapatra, 2018), or spoken conversation with experts (Manuvinakurike et al., 2018b). Though these semantic parsers are able to capture the high-level intent of novices, little has been discussed about the second stage – how to infer edits with these parsers. These datasets still require expert annotations for generating edits, which is a difficult one-to-many mapping. On the other hand, the image generation approach directly generates an edited image given the original image and an IER using the end-toend adversarial learning (Goodfellow et al., 2014) fra"
2020.lrec-1.51,W18-4701,1,0.842383,"ce language. Having little or no knowledge of terminologies and techniques, novices use open-domain vocabulary to express their needs. As a result, novice IERs have certain characteristics: (i) ambiguous, (ii) abstract, and (iii) imprecise. Though experts are able to disambiguate or fill-in missing details for novice IERs, grounding opendomain vocabulary have posed difficulties for NLIE. For the two-stage approach, annotators have low or near chance level agreement on certain entities (Brixey et al., 2018), and datasets collected under different scenarios require different annotation schemas (Manuvirakurike et al., 2018). For the image-generation approach, IERs are often imprecise and includes multiple edit operations in a single request (e.g., enhance white balance and contrast) (Wang et al., 2018). If novices have some knowledge of image editing tools, machines could understand IERs more easily and will be able to perform edits more precisely. In this paper we investigate the potential of low-level language for image editing. Motivated by the difficulty of open-domain vocabulary, we propose to use dialogue to bridge novice language (open-domain vocabulary) to image editing terminologies (in-domain vocabular"
2020.lrec-1.51,P19-1644,0,0.0160898,"arning curve. 2.2. Language and Vision There are many research fields which learns to align semantics between visual information and language, usually with an image and a natural language description. Our work falls under the category of a more recent setup, where language is related to the difference between paired images. Several works have been proposed for edited image generation using attention for alignment (Chen et al., 2018; Wang et al., 2018; Cheng et al., 2018). Other works focus more on the language side, either by determining whether a caption is true considering a pair of images (Suhr et al., 2019), or generating captions that describes the difference with latent variables (Jhamtani and Berg-Kirkpatrick, 2018) or attention (Tan et al., 2019). Our system is simpler as it grounds language at a low-level, and processes visual information with an independent segmentation module. Dialogue It is easy to imagine a dialogue for information-seeking domains (Raux et al., 2005; Bohus and Rudnicky, 2009; Dhingra et al., 2017; Wen et al., 2017; El Asri et al., 2017; Budzianowski et al., 2018) such as restaurant booking or movie booking, as these conversations exist in everyday life. Nevertheless, th"
2020.lrec-1.51,P19-1182,1,0.847594,"lly with an image and a natural language description. Our work falls under the category of a more recent setup, where language is related to the difference between paired images. Several works have been proposed for edited image generation using attention for alignment (Chen et al., 2018; Wang et al., 2018; Cheng et al., 2018). Other works focus more on the language side, either by determining whether a caption is true considering a pair of images (Suhr et al., 2019), or generating captions that describes the difference with latent variables (Jhamtani and Berg-Kirkpatrick, 2018) or attention (Tan et al., 2019). Our system is simpler as it grounds language at a low-level, and processes visual information with an independent segmentation module. Dialogue It is easy to imagine a dialogue for information-seeking domains (Raux et al., 2005; Bohus and Rudnicky, 2009; Dhingra et al., 2017; Wen et al., 2017; El Asri et al., 2017; Budzianowski et al., 2018) such as restaurant booking or movie booking, as these conversations exist in everyday life. Nevertheless, the same could not be said for image editing, as users interact with software tools (without language input) (Bychkovsky et al., 2011) or post on on"
2020.lrec-1.51,E17-1042,0,0.0205266,"Missing"
2020.lrec-1.664,D18-2029,0,0.135977,"esentations for the question and sentence, respectively (where d0 is the dimensionality of the RNN hidden units). As computing the node representation is an essential process for acquiring information from a text, we investigate various approaches for encoding sentences, such as replacing the ELMo word representations using different methods (the GloVe (Pennington et al., 2014) or the BERT (Devlin et al., 2019)) and replacing the RNN function in equation (6) with the pooling method. Furthermore, we adopt the universal sentence encoding method based on the recently developed transformer model (Cer et al., 2018). Detailed information will be given in the section 5.5. Aggregation: An iterative attentive aggregation function to the neighbor nodes is utilized to compute the amount of information to be propagated to each node in the graph as follows: A(k) v = σ( X (k) a(k) · N(k) vu W u ), u∈N (v) exp(Svu ) , a(k) vu = P k exp(Svk ) (7) (k) | (k) S(k) · N(k) vu = (Nv ) · W u , 0 where Av ∈ Rd is the aggregated information for the vth node computed by attentive weighted summation of its neighbor nodes, avu is the attention weight between node 5402 0 v and its neighbor nodes u (u∈N (v)), Nu ∈ Rd is the uth"
2020.lrec-1.664,P18-1078,0,0.0117688,"Tran et al., 2018; Yoon et al., 2019), the proposed method achieves better performance when classifying supporting sentences. 5400 2. Related Work Previous researchers have also investigated neural networkbased models for MRQA. One line of inquiry employs an attention mechanism between tokens in the question and passage to compute the answer span from the given text (Seo et al., 2016; Wang et al., 2017). As the task scope was extended from specific- to open-domain QA, several models have been proposed to select a relevant paragraph from the text to predict the answer span (Wang et al., 2018; Clark and Gardner, 2018). However, none of these methods have addressed reasoning over multiple sentences. To understand the relational patterns in the dataset, researchers have also proposed graph neural network algorithms. (Kipf and Welling, 2017) proposed a graph convolutional network to classify graph-structured data. This model was further investigated for applications involving large-scale graphs (Hamilton et al., 2017), for the effectiveness of aggregating and combining graph nodes by employing an attention mechanism (Veliˇckovi´c et al., 2018), and for adopting recurrent node updates (Palm et al., 2018). Thes"
2020.lrec-1.664,N19-1240,0,0.0312282,"Missing"
2020.lrec-1.664,N19-1423,0,0.059869,"n the text (see the discussion in section 5.4.). Node representation: Question Q ∈ Rd×Q and sentence Si ∈ Rd×Si (where d is the dimensionality of the word embedding and Q and Si represent the lengths of the sequences in Q and Si , respectively) are processed to acquire the sentence-level information. Recent studies have shown that a pretrained language model helps the model capture Passage 1 ? ? ? ? Passage N q ? ? ? Figure 2: Topology of the proposed model. Each node represents a sentence from the passage and the question. the contextual meaning of words in the sentence (Peters et al., 2018; Devlin et al., 2019). Following this study, we select an ELMo (Peters et al., 2018) language model for the word-embedding layer of our model as follows: LQ = ELMo(Q), (LQ ∈Rd×Q ), LS = ELMo(S), (LS ∈Rd×S ). (5) Using these new representations, we compute the sentence representation as follows: Q Q hQ t = fθ (ht−1 , Lt ), hSt = fθ (hSt−1 , LSt ), Q N = hQ last , S N = (6) hSlast , where fθ is the RNN function with the weight parameters 0 0 θ and NQ ∈ Rd and NS ∈ Rd are node representations for the question and sentence, respectively (where d0 is the dimensionality of the RNN hidden units). As computing the node re"
2020.lrec-1.664,P17-1147,0,0.0467384,"Missing"
2020.lrec-1.664,D14-1181,0,0.00268216,"e question Q ∈ Rd×Q and target sentence S ∈ Rd×S (where d is a dimensionality of word embedding and Q and S are the length of the sequences in the question and sentence, respectively) is computed by applying an attention mechanism over the column vector in Q for each column vector in S. With the computed alignment, we obtain a corresponding vector AQ ∈ Rd×S as follows: | AQ = Q · softmax((WQ) S), (1) where W is a learned model parameter matrix. Comparison: An element-wise multiplication is employed as a comparison function to combine each pair of AQ and S into a vector C ∈ Rd×S . Aggregation: Kim (2014)’s CNN with n-types of filters is applied to aggregate all information in the vector C. Finally, the model employs a fully connected layer to compute the matching score between the question and the target sentence as follows: R = CNN(C), (R ∈ Rnd ), yˆc = softmax((R) |W + b ), (2) where yˆc is the predicted probability for the target class, c, and W ∈ Rnd×c and bias b are learned model parameters. The loss function for the model is cross-entropy between predicted labels and true-labels as follows: L = − log N X C X yi,c log(ˆ yi,c ), (3) i=1 c=1 where yi,c is the true label vector and yˆi,c is"
2020.lrec-1.664,D14-1162,0,0.0827075,"Missing"
2020.lrec-1.664,N18-1202,0,0.282844,"onsidered strong baselines since they can be directly applied to our task with the same objective function. Then we describe our proposed method. 4.1.2. CompAggr-kMax. This model (Bian et al., 2017) is an extension of the CompAggr model. The only differences lie in applying attention (in equation (1)) to both the Q and S side and applying k-max pooling before the softmax function as follows: | AQ = Q · softmax(kMax((WQ) S)), | AS = S · softmax(kMax((WS) Q)). 5401 (4) topology 4.1.3. CompClip-LM-LC. This model (Yoon et al., 2019) is an extension of the CompAggr-kMax model. It employs the ELMo (Peters et al., 2018) model to enhance the word embedding layer for the question and target sentence by adopting the pretrained contextual language model. Additionally, it develops a latent clustering method to compute topic information in texts automatically and to use it as auxiliary information to improve the model performance. 4.1.4. IWAN. This model (Shen et al., 2017a) is a variation model based on the compare aggregate framework. Unlike CompAggr, it employs RNNs to encode a sequence of the words in the text (question and target sentence independently). At the same time, it computes an inter-alignment weight"
2020.lrec-1.664,D16-1264,0,0.0604174,"Missing"
2020.lrec-1.664,D17-1122,0,0.302335,"standing texts and being able to answer a question posed by a human is a long-standing goal in the artificial intelligence field. With the rapid advancement of neural network-based models and the availability of large-scale datasets, such as SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017), researchers have begun to concentrate on building automatic question-answering (QA) systems. One example of such a system is the machine-reading question-answering (MRQA) model, which provides answers to questions from given passages (Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Shen et al., 2017b). Recently, research has revealed that most questions in existing MRQA datasets do not require reasoning across sentences in the given context (passage); instead, they can be answered by looking at only a single sentence (Weissenborn et al., 2017). Using this characteristic, a simple model can achieve performance competitive with that of a sophisticated model. However, in most real scenarios of QA applications, more than one sentence should be utilized to extract a correct answer. To alleviate this limitation of previous datasets, another type of dataset was developed in which answering the"
2020.lrec-1.664,N18-1115,1,0.865107,"the propagation process, the model learns to understand information that cannot be inferred when considering sentences in isolation. Unlike the previous studies, this work does not use the exact “answer span” information while detecting the supporting sentences. It shows a different way of using the HotPotQA dataset and provides researchers new opportunities to develop a subsystem that is integrated into the full-QA systems (i.e., MRQA). Through experiments, we demonstrate that compared with the widely used answer-selection models (Wang and Jiang, 2016; Bian et al., 2017; Shen et al., 2017a; Tran et al., 2018; Yoon et al., 2019), the proposed method achieves better performance when classifying supporting sentences. 5400 2. Related Work Previous researchers have also investigated neural networkbased models for MRQA. One line of inquiry employs an attention mechanism between tokens in the question and passage to compute the answer span from the given text (Seo et al., 2016; Wang et al., 2017). As the task scope was extended from specific- to open-domain QA, several models have been proposed to select a relevant paragraph from the text to predict the answer span (Wang et al., 2018; Clark and Gardner,"
2020.lrec-1.664,P17-1018,0,0.14571,"ment&apos;s MVP. … Understanding texts and being able to answer a question posed by a human is a long-standing goal in the artificial intelligence field. With the rapid advancement of neural network-based models and the availability of large-scale datasets, such as SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017), researchers have begun to concentrate on building automatic question-answering (QA) systems. One example of such a system is the machine-reading question-answering (MRQA) model, which provides answers to questions from given passages (Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Shen et al., 2017b). Recently, research has revealed that most questions in existing MRQA datasets do not require reasoning across sentences in the given context (passage); instead, they can be answered by looking at only a single sentence (Weissenborn et al., 2017). Using this characteristic, a simple model can achieve performance competitive with that of a sophisticated model. However, in most real scenarios of QA applications, more than one sentence should be utilized to extract a correct answer. To alleviate this limitation of previous datasets, another type of dataset was developed in w"
2020.lrec-1.664,K17-1028,0,0.0146767,"SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017), researchers have begun to concentrate on building automatic question-answering (QA) systems. One example of such a system is the machine-reading question-answering (MRQA) model, which provides answers to questions from given passages (Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Shen et al., 2017b). Recently, research has revealed that most questions in existing MRQA datasets do not require reasoning across sentences in the given context (passage); instead, they can be answered by looking at only a single sentence (Weissenborn et al., 2017). Using this characteristic, a simple model can achieve performance competitive with that of a sophisticated model. However, in most real scenarios of QA applications, more than one sentence should be utilized to extract a correct answer. To alleviate this limitation of previous datasets, another type of dataset was developed in which answering the question requires reasoning over multiple sentences in the given passages (Yang et al., 2018; Welbl et al., 2018). Figure 1 shows an example of a recently released dataset, the HotpotQA. This dataset consists of not only question-answer pairs with c"
2020.lrec-1.664,Q18-1021,0,0.020632,"re reasoning across sentences in the given context (passage); instead, they can be answered by looking at only a single sentence (Weissenborn et al., 2017). Using this characteristic, a simple model can achieve performance competitive with that of a sophisticated model. However, in most real scenarios of QA applications, more than one sentence should be utilized to extract a correct answer. To alleviate this limitation of previous datasets, another type of dataset was developed in which answering the question requires reasoning over multiple sentences in the given passages (Yang et al., 2018; Welbl et al., 2018). Figure 1 shows an example of a recently released dataset, the HotpotQA. This dataset consists of not only question-answer pairs with context passages but also supporting sentence information for answering the question annotated by a human. In this study, we build a model that exploits the relational information among sentences in passages to classify the supporting sentences that contain the essential information for answering the question. To this end, we propose a novel graph neural network model named propagateselector (PS), which can be directly employed as a subsystem in the QA pipeline"
2020.lrec-1.664,D18-1259,0,0.0680385,"tasets do not require reasoning across sentences in the given context (passage); instead, they can be answered by looking at only a single sentence (Weissenborn et al., 2017). Using this characteristic, a simple model can achieve performance competitive with that of a sophisticated model. However, in most real scenarios of QA applications, more than one sentence should be utilized to extract a correct answer. To alleviate this limitation of previous datasets, another type of dataset was developed in which answering the question requires reasoning over multiple sentences in the given passages (Yang et al., 2018; Welbl et al., 2018). Figure 1 shows an example of a recently released dataset, the HotpotQA. This dataset consists of not only question-answer pairs with context passages but also supporting sentence information for answering the question annotated by a human. In this study, we build a model that exploits the relational information among sentences in passages to classify the supporting sentences that contain the essential information for answering the question. To this end, we propose a novel graph neural network model named propagateselector (PS), which can be directly employed as a subsyst"
2020.nli-1.4,W17-5522,0,0.102804,"1:SELECT name FROM Employees WHERE last_name = &apos;Smith&apos; AND dept_name = &apos;IT&apos;; Turn 2: How many of them started working after Jan 1, 2020? Turn 2:SELECT Count(name) FROM Employees WHERE last_name = &apos;Smith&apos; AND dept_name = &apos;IT&apos; AND hire_date > &apos;01-01-2020&apos;; Turn 3: What are their phone numbers? Turn 3:SELECT phone_number FROM Employees WHERE last_name = &apos;Smith&apos; AND dept_name = &apos;IT&apos; AND hire_data > &apos;01-01-2020&apos;; Figure 1: Example illustrating a three-turn dialogue, featuring the natural language (first column) and query language (second column) representations. guages (Ngonga Ngomo et al., 2013; Braun et al., 2017; Dubey et al., 2016; Giordani and Moschitti, 2009; Finegan-Dollak et al., 2018; Giordani, 2008; Xu et al., 2017; Zhong et al., 2017), the state-of-the-art systems typically involve a large amount of training data. Therefore, in order to fully utilize these models that translate a natural language (NL) question into query language (QL), one would need to collect large amounts of both NL-QL pairs. Although there are works which involve the collection of NL-QL pairs in different domains (Hemphill et al., 1990; Zelle and Mooney, 1996; Zhong et al., 2017; Yu et al., 2018, 2019b), data is still not"
2020.nli-1.4,P17-4017,0,0.0275416,"collection process for use in NL-to-QL models. Additionally, our approach focuses on generating conversation data, where the context of a dialogue turn is used to generate a subsequent pair. In this way, we better simulate the data necessary for real world chatbots and voice assistants, as exemplified in Figure 1. Our contributions are as follows: Introduction Chatbots and AI task assistants are widely used today to help users with their everyday needs. One use for these assistants is asking them questions on various areas of knowledge or how to accomplish different tasks (Braun et al., 2017; Cui et al., 2017). Because data is usually stored in a structured database, in order to answer a user’s questions, it is essential that the system should first understand the question, and convert it into a structured language query, such as SQL or SPARQL, to fetch the correct answer. While much research has focused on translating natural languages into query lan27 Proceedings of the First Workshop on Natural Language Interfaces, pages 27–36 c July 10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 • We develop a novel approach that accelerates the creation of NL-to-QL dat"
2020.nli-1.4,P16-1004,0,0.0133595,", Shah et al. (2018) develop a multi-turn semantic parser. Their approach begins with a task schema and API which is used to create dialogue outlines for the provided domain. In the field of natural language interfaces for structured data there are bodies of work that 1) focus on translating natural language to a specific query language and that 2) relate to collecting semantic parsing data for natural language interfaces. 2.1 Data Collection for Semantic Parsing NL-to-QL NL-to-QL models have worked to transform natural language queries into their respective logical form (LF) representations (Dong and Lapata, 2016), SQL queries (Xu et al., 2017; Zhong et al., 2017; Finegan-Dollak et al., 2018; Cai et al., 2018), or SPARQL queries (Ngonga Ngomo et al., 2013; Dubey et al., 2016). While work in the SPARQL domain first normalize and match the queries, stateof-the-art work in translating NL to SQL involves neural architectures. Dong and Lapata (2016) utilize and encoder-decoder framework to translate NL questions into their LF representation. Xu et al. (2017) propose a sketch-based model where a neural network predicts each slot of the sketch. The ar1 28 https://www.wikidata.org/wiki/Wikidata:Main Page Domai"
2020.nli-1.4,P18-1033,0,0.226845,"= &apos;IT&apos;; Turn 2: How many of them started working after Jan 1, 2020? Turn 2:SELECT Count(name) FROM Employees WHERE last_name = &apos;Smith&apos; AND dept_name = &apos;IT&apos; AND hire_date > &apos;01-01-2020&apos;; Turn 3: What are their phone numbers? Turn 3:SELECT phone_number FROM Employees WHERE last_name = &apos;Smith&apos; AND dept_name = &apos;IT&apos; AND hire_data > &apos;01-01-2020&apos;; Figure 1: Example illustrating a three-turn dialogue, featuring the natural language (first column) and query language (second column) representations. guages (Ngonga Ngomo et al., 2013; Braun et al., 2017; Dubey et al., 2016; Giordani and Moschitti, 2009; Finegan-Dollak et al., 2018; Giordani, 2008; Xu et al., 2017; Zhong et al., 2017), the state-of-the-art systems typically involve a large amount of training data. Therefore, in order to fully utilize these models that translate a natural language (NL) question into query language (QL), one would need to collect large amounts of both NL-QL pairs. Although there are works which involve the collection of NL-QL pairs in different domains (Hemphill et al., 1990; Zelle and Mooney, 1996; Zhong et al., 2017; Yu et al., 2018, 2019b), data is still not available in most domains, and thus this collection process can be both time-c"
2020.nli-1.4,P15-1129,0,0.069728,"Missing"
2020.nli-1.4,N13-1092,0,0.0871696,"Missing"
2020.nli-1.4,D19-1204,0,0.0263013,"Saha et al. (2018) first approach the problem of complex sequential question-answering (CSQA) by first building a large-scale QA dataset made to answer questions found in Wikidata 1 . However, their data collection process was extremely laborious, as their process required in-house annotators, crowdsourced workers, and multiple iterations. Additionally, their approach was end-to-end, meaning the output was an expected answer. Nevertheless, because their approach incorporate the query representation, we plan to further incorporate their approach into our data collection process in future work.Yu et al. (2019a) also develop the first general-purpose DB querying dialogue system. However, their system dialogues focus on clarifying a NL question for user verification, before returning an answer. Our work focuses on generating conversational data about specific database entities and properties. 2 2.2 • We showcase our data collection system on two different QLs, SQL and SPARQL, demonstrating the flexibility of our system. • Finally, we demonstrate the use of current single-turn state-of-the-art approaches on these two domains to prove the adaptability of our system to current models. Related Work NL q"
2020.nli-1.4,D18-1425,0,0.0178291,"nga Ngomo et al., 2013; Braun et al., 2017; Dubey et al., 2016; Giordani and Moschitti, 2009; Finegan-Dollak et al., 2018; Giordani, 2008; Xu et al., 2017; Zhong et al., 2017), the state-of-the-art systems typically involve a large amount of training data. Therefore, in order to fully utilize these models that translate a natural language (NL) question into query language (QL), one would need to collect large amounts of both NL-QL pairs. Although there are works which involve the collection of NL-QL pairs in different domains (Hemphill et al., 1990; Zelle and Mooney, 1996; Zhong et al., 2017; Yu et al., 2018, 2019b), data is still not available in most domains, and thus this collection process can be both time-consuming and expensive. In this work, we address the problem of having insufficient data collection methodologies by proposing a novel approach that accelerates the data collection process for use in NL-to-QL models. Additionally, our approach focuses on generating conversation data, where the context of a dialogue turn is used to generate a subsequent pair. In this way, we better simulate the data necessary for real world chatbots and voice assistants, as exemplified in Figure 1. Our cont"
2020.nli-1.4,H90-1021,0,0.737381,"mn) and query language (second column) representations. guages (Ngonga Ngomo et al., 2013; Braun et al., 2017; Dubey et al., 2016; Giordani and Moschitti, 2009; Finegan-Dollak et al., 2018; Giordani, 2008; Xu et al., 2017; Zhong et al., 2017), the state-of-the-art systems typically involve a large amount of training data. Therefore, in order to fully utilize these models that translate a natural language (NL) question into query language (QL), one would need to collect large amounts of both NL-QL pairs. Although there are works which involve the collection of NL-QL pairs in different domains (Hemphill et al., 1990; Zelle and Mooney, 1996; Zhong et al., 2017; Yu et al., 2018, 2019b), data is still not available in most domains, and thus this collection process can be both time-consuming and expensive. In this work, we address the problem of having insufficient data collection methodologies by proposing a novel approach that accelerates the data collection process for use in NL-to-QL models. Additionally, our approach focuses on generating conversation data, where the context of a dialogue turn is used to generate a subsequent pair. In this way, we better simulate the data necessary for real world chatbo"
2020.nli-1.4,P19-1443,0,0.0155581,"Saha et al. (2018) first approach the problem of complex sequential question-answering (CSQA) by first building a large-scale QA dataset made to answer questions found in Wikidata 1 . However, their data collection process was extremely laborious, as their process required in-house annotators, crowdsourced workers, and multiple iterations. Additionally, their approach was end-to-end, meaning the output was an expected answer. Nevertheless, because their approach incorporate the query representation, we plan to further incorporate their approach into our data collection process in future work.Yu et al. (2019a) also develop the first general-purpose DB querying dialogue system. However, their system dialogues focus on clarifying a NL question for user verification, before returning an answer. Our work focuses on generating conversational data about specific database entities and properties. 2 2.2 • We showcase our data collection system on two different QLs, SQL and SPARQL, demonstrating the flexibility of our system. • Finally, we demonstrate the use of current single-turn state-of-the-art approaches on these two domains to prove the adaptability of our system to current models. Related Work NL q"
2020.nli-1.4,P17-1089,0,0.0141643,"emonstrate the use of current single-turn state-of-the-art approaches on these two domains to prove the adaptability of our system to current models. Related Work NL question semantic parsers have been developed for single-turn QA in order to translate simple NL questions into their respective LFs (Wang et al., 2015). In their approach, Wang et al. (2015) first begin with a domain, building a seed lexicon of that domain. Next, they find the LF and canonical utterance templates corresponding based on the lexicon. Wang et al. (2015) then paraphrase their canonical utterances via crowd-sourcing. Iyer et al. (2017) learn a semantic parser via an encoderdecoder model by using NL/SQL templates. This model is tuned through user feedback, where incorrect queries are annotated by crowd-workers. Paraphrasing is accomplished through the Paraphrasing Database (PPDB) (Ganitkevitch et al., 2013). While the two previously mentioned works are single-turn semantic parsers, Shah et al. (2018) develop a multi-turn semantic parser. Their approach begins with a task schema and API which is used to create dialogue outlines for the provided domain. In the field of natural language interfaces for structured data there are"
2020.nli-1.4,P16-1002,0,0.020093,"eveloped by Finegan-Dollak et al. (2018), where the blue boxes represent LSTM cells and the green box represents a feedforward neural network. ‘Photos’ is classified as a slot value, while the template chosen (Tempalte 42), is depicted above the model. In the template, the entity slot is highlighted in yellow and the properties which make the template unique are in red. In our experiments we utilize single-turn NL-QL models. Specifically, we utilize the baselines defined by Finegan-Dollak et al. (2018). The first baseline is a seq2seq model with attention-based copying, originally proposed by Jia and Liang (2016). This model takes an NL utterance as input and outputs a structured query. Included in the output is a COPY token, which signifies the copying of an input token. In the copying mechanism model, the loss is calculated based on the accumulation of both the probability of distribution of the tokens in the output and the probability of copying from an input token. This copying probability is calculated as the categorical cross entropy of the distributed attention scores across the input’s tokens, where the token with the max attention score is chosen as the output token. The second baseline is a"
2021.acl-long.119,W19-5059,0,0.0364186,"Missing"
2021.acl-long.119,S17-2057,0,0.0571488,"Missing"
2021.acl-long.119,P19-1215,0,0.349036,"e? Figure 1: We highlight the main four aspects of the CHQ. Our method learns from the task of Recognizing Question Entailment to generate more informative summaries compared to the baseline. Introduction In order to retrieve relevant answers, one of the basic steps in Question Answering (QA) systems is understanding the intent of questions (Chen et al., 2012; Cai et al., 2017). This is particularly important for medical QA systems (Wu et al., 2020), as consumer health questions – questions asked by patients – may use a vocabulary distinct from doctors to describe similar health concepts (Ben Abacha and Demner-Fushman, 2019a). Consumer health questions may also contain peripheral information like patient history (Roberts and Demner-Fushman, 2016), that are not necessary to answer questions. There is a growing number of approaches to medical question understanding, including query relaxation (Ben Abacha and Zweigenbaum, 2015; Lei et al., 2020), question entailment (Ben Abacha and Demner-Fushman, 2016, 2019b; Agrawal et al., 2019), question summarization (Ben Abacha and Demner-Fushman, 2019a), and question similarity (Ben Abacha and Demner-Fushman, 2017; Yan and Li, 2018; McCreery et al., 2019). Medical question s"
2021.acl-long.119,W19-5039,0,0.226837,"benefit from language models that use multi-task learning and transfer learning. There are pretrained language models that are geared towards BioNLP applications, that are based on BERT (Devlin et al., 2019). Those include SciBERT (Beltagy et al., 2019) which has been fine-tuned using biomedical text from PubMed. BioBERT (Lee et al., 2020) has been finetuned on the PMC dataset, whereas models named ClinicalBERT (Huang et al., 2019; Alsentzer et al., 2019) additionally use the MIMIC III dataset (Johnson et al., 2016). Transfer learning was a popular approach at the 2019 MEDIQA shared task (Ben Abacha et al., 2019) on medical NLI, RQE and QA. The question answering task involved re-ranking answers, not generating them (Demner-Fushman et al., 2020). For the RQE task, the best-performing model (Zhu et al., 2019) uses transfer learning on NLI and ensemble methods. 3 Methodology We consider the multi-task learning of medical question summarization and medical RQE. The input to both tasks is a pair of medical questions. The first question is called a Consumer Health Question (CHQ), and the second question is called a Frequently Asked Question (FAQ). The CHQ is written by a patient and is usually longer and m"
2021.acl-long.119,D15-1075,0,0.0339988,"sharing configurations in RQE accuracy, and in the sum of ROUGE F1 scores. These results show our proposed smoother parameter-sharing transition between encoder and decoder layers brings about higher performance. 4.5 4.5.1 Results and Discussion Summarization Results Baselines. We consider three main baselines. The first one is BART (Lewis et al., 2019), where we only train on the summarization task. The second baseline trains BART on the same MTL settings as Pasunuru et al. (2017), using alternative training with entailment generation on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) and having a shared decoder and taskspecific encoders. The third baseline trains BART on the same MTL settings as Guo et al. (2018), where, on top of the entailment generation task, we add the question generation task using the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), and all parameters are soft-shared, except for the task-specific first encoder layer and last decoder layer. In addition, we also report the baselines assessed by Ben Abacha and Demner-Fushman (2019a) for MeQSum. For data augmentation, they use semantically-selected relevant question pairs from the Q"
2021.acl-long.119,W19-1909,0,0.0577331,"Missing"
2021.acl-long.119,D19-1371,0,0.0148395,"r method outperforms the pointer-generator networks of See et al. (2017) on the CNN-Dailymail news summarization baseline. Here, the authors show performance increase in entailment on some batch sizes and decrease on other batch sizes, and they consider entailment as an auxiliary task. 1506 Transfer Learning for Medical QA. BioNLP is one of many NLP applications to benefit from language models that use multi-task learning and transfer learning. There are pretrained language models that are geared towards BioNLP applications, that are based on BERT (Devlin et al., 2019). Those include SciBERT (Beltagy et al., 2019) which has been fine-tuned using biomedical text from PubMed. BioBERT (Lee et al., 2020) has been finetuned on the PMC dataset, whereas models named ClinicalBERT (Huang et al., 2019; Alsentzer et al., 2019) additionally use the MIMIC III dataset (Johnson et al., 2016). Transfer learning was a popular approach at the 2019 MEDIQA shared task (Ben Abacha et al., 2019) on medical NLI, RQE and QA. The question answering task involved re-ranking answers, not generating them (Demner-Fushman et al., 2020). For the RQE task, the best-performing model (Zhu et al., 2019) uses transfer learning on NLI and"
2021.acl-long.119,N19-1423,0,0.0274121,"ng improves over hard parameter-sharing. Their method outperforms the pointer-generator networks of See et al. (2017) on the CNN-Dailymail news summarization baseline. Here, the authors show performance increase in entailment on some batch sizes and decrease on other batch sizes, and they consider entailment as an auxiliary task. 1506 Transfer Learning for Medical QA. BioNLP is one of many NLP applications to benefit from language models that use multi-task learning and transfer learning. There are pretrained language models that are geared towards BioNLP applications, that are based on BERT (Devlin et al., 2019). Those include SciBERT (Beltagy et al., 2019) which has been fine-tuned using biomedical text from PubMed. BioBERT (Lee et al., 2020) has been finetuned on the PMC dataset, whereas models named ClinicalBERT (Huang et al., 2019; Alsentzer et al., 2019) additionally use the MIMIC III dataset (Johnson et al., 2016). Transfer learning was a popular approach at the 2019 MEDIQA shared task (Ben Abacha et al., 2019) on medical NLI, RQE and QA. The question answering task involved re-ranking answers, not generating them (Demner-Fushman et al., 2020). For the RQE task, the best-performing model (Zhu e"
2021.acl-long.119,P19-1213,0,0.0226625,"Missing"
2021.acl-long.119,W07-1401,0,0.0840394,"received by the U.S. National Library of Medicine (NLM) and a FAQ from NIH institutes. Whereas the train and dev sets have automatically generated CHQs, the test set has manually written CHQs. This results in significantly higher dev set results than for test sets, as has been observed during the 2019 MEDIQA shared task. In addition, we use two pretraining datasets. We use the XSum dataset (Narayan et al., 2018), an abstractive summarization benchmark, for question summarization. For the RQE task, we use the Recognizing Textual Entailment (RTE) dataset (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) from the GLUE benchmark (Wang et al., 2018). 4 4.2 LGS (θ) = γ ∗ N −1  X e N −n N 2  QS RQE − 1 θdec,n − θdec,n n=1 4.1 Experiments Datasets We consider 3 medical question summarization datasets and 1 medical RQE dataset. We show dataset statistics in Table 1. MeQSum and MEDIQA RQE can be considered low-resource, whereas the other two are far larger. Our datasets are in the English language. Due to space constraints, we briefly introduce the datasets and leave additional details in the appendix. Setup and Training Settings All of our models use the BART large archi"
2021.acl-long.119,P18-1064,0,0.369914,"uistics and the 11th International Joint Conference on Natural Language Processing, pages 1505–1515 August 1–6, 2021. ©2021 Association for Computational Linguistics Task Learning (MTL) problem involving the two tasks of question summarization and Recognizing Question Entailment. We use a simple sum of learning objectives in Mrini et al. (2021b). In this paper, we introduce a novel, gradually soft multi-task and data-augmented approach to medical question understanding.1 Previous work on combining summarization and entailment uses at least 2 datasets – 1 from each task (Pasunuru et al., 2017; Guo et al., 2018). We first establish an equivalence between both tasks. This equivalence is the inspiration behind the data augmentation schemes introduced in our previous work (Mrini et al., 2021b). The goal of the data augmentation is to use a single dataset for MultiTask Learning. We propose to use a weighted loss function to simultaneously optimize for both tasks. Then, we propose a gradually soft parametersharing MTL approach. We conduct ablation studies to show that our two novelties – data augmentation and gradually soft parameter-sharing – improve performance in both tasks. Our proposed gradually soft"
2021.acl-long.119,S14-1010,0,0.0176936,"A either partially or fully. It differs from traditional definitions of entailment, where we consider that the premise entails the hypothesis if and only if the hypothesis is true only if the premise is true. Ben Abacha and Demner-Fushman (2016) define RQE within the context of Medical Question Answering. The goal is to match a Consumer Health Question (CHQ) to a Frequently Asked Question (FAQ), and ultimately match the CHQ to an expertwritten answer. Summarization and Entailment. There is a growing body of work combining summarization and entailment (Lloret et al., 2008; Mehdad et al., 2013; Gupta et al., 2014). Falke et al. (2019) use textual entailment predictions to detect factual errors in abstractive summaries generated by state-of-the-art models. Pasunuru and Bansal (2018) propose an entailment reward for their reinforced abstractive summarizer, where the entailment score is obtained from a pre-trained and frozen natural language inference model. Pasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. The authors optimize alternatively between the two tasks, and use separate Natural Language Inference (NLI) and abstrac"
2021.acl-long.119,2020.acl-main.703,0,0.044172,"Missing"
2021.acl-long.119,C18-1121,0,0.0150455,"ctual errors in abstractive summaries generated by state-of-the-art models. Pasunuru and Bansal (2018) propose an entailment reward for their reinforced abstractive summarizer, where the entailment score is obtained from a pre-trained and frozen natural language inference model. Pasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. The authors optimize alternatively between the two tasks, and use separate Natural Language Inference (NLI) and abstractive summarization datasets. Only the decoder parameters are shared. Li et al. (2018) closely follow the MTL setting of Pasunuru et al. (2017), and propose a model with a shared encoder, an NLI classifier and an NLI-rewarded summarization decoder. Guo et al. (2018) introduce a pointer-generator summarization model with coverage loss (See et al., 2017). They build upon the work of Pasunuru et al. (2017), and add question generation on top of the two tasks of abstractive summarization and entailment generation. They also alternate between the three different objectives. The authors propose to share all parameters except the first layer of the encoder and the last layer of the de"
2021.acl-long.119,P19-1441,0,0.0149728,"y, and the entailment label lentail ∈ {0, 1}, we optimize the following multi-task learning loss function: LMTL (θ) = − λ ∗ logp(y|x; θ) + (1 − λ) ∗ BCE ([x; y] , lentail ; θ) (1) where BCE is binary cross entropy, and λ is a hyperparameter between 0 and 1. 1508 3.4 DATASET MeQSum HealthCareMagic iCliniq MEDIQA RQE Gradually Soft Parameter-Sharing In multi-task learning, there are two widely used approaches: hard parameter-sharing and soft parameter-sharing. Guo et al. (2018) propose soft parameter-sharing for all parameters except the first layer of the encoder and last layer of the decoder. Liu et al. (2019) introduce MT-DNN and show that hard parameter-sharing of all of the transformer encoder layers, and only having task-specific classification heads produces results that set a new state of the art for the GLUE benchmark (Wang et al., 2018). We propose a hybrid approach, where we apply hard parameter-sharing for the encoder, and a novel gradually soft parameter-sharing approach for the decoder layers. We define gradually soft parametersharing as a smooth transition from hard parametersharing to task-specific layers. It is a soft parametersharing approach that is gradually toned down from the fi"
2021.acl-long.119,W13-2117,0,0.023717,"er to A, and answers A either partially or fully. It differs from traditional definitions of entailment, where we consider that the premise entails the hypothesis if and only if the hypothesis is true only if the premise is true. Ben Abacha and Demner-Fushman (2016) define RQE within the context of Medical Question Answering. The goal is to match a Consumer Health Question (CHQ) to a Frequently Asked Question (FAQ), and ultimately match the CHQ to an expertwritten answer. Summarization and Entailment. There is a growing body of work combining summarization and entailment (Lloret et al., 2008; Mehdad et al., 2013; Gupta et al., 2014). Falke et al. (2019) use textual entailment predictions to detect factual errors in abstractive summaries generated by state-of-the-art models. Pasunuru and Bansal (2018) propose an entailment reward for their reinforced abstractive summarizer, where the entailment score is obtained from a pre-trained and frozen natural language inference model. Pasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. The authors optimize alternatively between the two tasks, and use separate Natural Language Infere"
2021.acl-long.119,P17-1099,0,0.513219,"rence model. Pasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. The authors optimize alternatively between the two tasks, and use separate Natural Language Inference (NLI) and abstractive summarization datasets. Only the decoder parameters are shared. Li et al. (2018) closely follow the MTL setting of Pasunuru et al. (2017), and propose a model with a shared encoder, an NLI classifier and an NLI-rewarded summarization decoder. Guo et al. (2018) introduce a pointer-generator summarization model with coverage loss (See et al., 2017). They build upon the work of Pasunuru et al. (2017), and add question generation on top of the two tasks of abstractive summarization and entailment generation. They also alternate between the three different objectives. The authors propose to share all parameters except the first layer of the encoder and the last layer of the decoder, and show that soft parameter-sharing improves over hard parameter-sharing. Their method outperforms the pointer-generator networks of See et al. (2017) on the CNN-Dailymail news summarization baseline. Here, the authors show performance increase in entailment o"
2021.acl-long.119,2021.nlpmc-1.8,1,0.269857,"Missing"
2021.acl-long.119,2021.bionlp-1.28,1,0.864351,"Missing"
2021.acl-long.119,K16-1028,0,0.050147,"Missing"
2021.acl-long.119,D18-1206,0,0.012065,"iCliniq’s patient-written summaries. The medical RQE dataset is the MEDIQA RQE dataset from the 2019 MEDIQA shared task (Ben Abacha et al., 2019). Similarly to MeQSum, the question pairs match a longer CHQ received by the U.S. National Library of Medicine (NLM) and a FAQ from NIH institutes. Whereas the train and dev sets have automatically generated CHQs, the test set has manually written CHQs. This results in significantly higher dev set results than for test sets, as has been observed during the 2019 MEDIQA shared task. In addition, we use two pretraining datasets. We use the XSum dataset (Narayan et al., 2018), an abstractive summarization benchmark, for question summarization. For the RQE task, we use the Recognizing Textual Entailment (RTE) dataset (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) from the GLUE benchmark (Wang et al., 2018). 4 4.2 LGS (θ) = γ ∗ N −1  X e N −n N 2  QS RQE − 1 θdec,n − θdec,n n=1 4.1 Experiments Datasets We consider 3 medical question summarization datasets and 1 medical RQE dataset. We show dataset statistics in Table 1. MeQSum and MEDIQA RQE can be considered low-resource, whereas the other two are far larger. Our datase"
2021.acl-long.119,N18-2102,0,0.0156156,"othesis is true only if the premise is true. Ben Abacha and Demner-Fushman (2016) define RQE within the context of Medical Question Answering. The goal is to match a Consumer Health Question (CHQ) to a Frequently Asked Question (FAQ), and ultimately match the CHQ to an expertwritten answer. Summarization and Entailment. There is a growing body of work combining summarization and entailment (Lloret et al., 2008; Mehdad et al., 2013; Gupta et al., 2014). Falke et al. (2019) use textual entailment predictions to detect factual errors in abstractive summaries generated by state-of-the-art models. Pasunuru and Bansal (2018) propose an entailment reward for their reinforced abstractive summarizer, where the entailment score is obtained from a pre-trained and frozen natural language inference model. Pasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. The authors optimize alternatively between the two tasks, and use separate Natural Language Inference (NLI) and abstractive summarization datasets. Only the decoder parameters are shared. Li et al. (2018) closely follow the MTL setting of Pasunuru et al. (2017), and propose a model with a"
2021.acl-long.119,W18-5446,0,0.132671,"er between 0 and 1. 1508 3.4 DATASET MeQSum HealthCareMagic iCliniq MEDIQA RQE Gradually Soft Parameter-Sharing In multi-task learning, there are two widely used approaches: hard parameter-sharing and soft parameter-sharing. Guo et al. (2018) propose soft parameter-sharing for all parameters except the first layer of the encoder and last layer of the decoder. Liu et al. (2019) introduce MT-DNN and show that hard parameter-sharing of all of the transformer encoder layers, and only having task-specific classification heads produces results that set a new state of the art for the GLUE benchmark (Wang et al., 2018). We propose a hybrid approach, where we apply hard parameter-sharing for the encoder, and a novel gradually soft parameter-sharing approach for the decoder layers. We define gradually soft parametersharing as a smooth transition from hard parametersharing to task-specific layers. It is a soft parametersharing approach that is gradually toned down from the first layer of the decoder to the last layer, which is entirely task-specific. In gradually soft parameter-sharing, we constrain decoder parameters to be close by penalizing their l2 distances, and the higher the layer the looser the constra"
2021.acl-long.119,W19-5046,0,0.0179303,"gic and MeQSum. However, we note an increase in correctness for the more extractive iCliniq dataset. On average, our gradually soft multi-task and dataaugmented method outputs summarized questions that are more fluent and more informative than the single-task BART baseline. 4.5.2 RQE Results and Discussion Baselines. We compare our method to three baselines. The first one trains a single-task BART on RQE, with a classification head pre-trained on RTE. The second baseline is a feature-based SVM from Ben Abacha and Demner-Fushman (2016) who introduced the MEDIQA RQE dataset. The third baseline (Zhou et al., 2019) is an adversarial MTL method combining medical question answering and RQE. The architecture consists of a shared transformer encoder using BioBERT embeddings (Lee et al., 2020), separate classification heads for RQE and medical QA, and a task discriminator for adversarial training. A separate dataset is used for medical QA (Ben Abacha et al., 2019). 1511 DATASET M ETRIC BASELINES Seq2seq Attentional Model (Nallapati et al., 2016) Pointer-Generator Networks (PG) (See et al., 2017) PG + Data Augmentation (Ben Abacha and DemnerFushman, 2019a) PG + Coverage Loss (See et al., 2017) PG + Coverage L"
2021.acl-long.119,W19-5040,0,0.0168083,"2019). Those include SciBERT (Beltagy et al., 2019) which has been fine-tuned using biomedical text from PubMed. BioBERT (Lee et al., 2020) has been finetuned on the PMC dataset, whereas models named ClinicalBERT (Huang et al., 2019; Alsentzer et al., 2019) additionally use the MIMIC III dataset (Johnson et al., 2016). Transfer learning was a popular approach at the 2019 MEDIQA shared task (Ben Abacha et al., 2019) on medical NLI, RQE and QA. The question answering task involved re-ranking answers, not generating them (Demner-Fushman et al., 2020). For the RQE task, the best-performing model (Zhu et al., 2019) uses transfer learning on NLI and ensemble methods. 3 Methodology We consider the multi-task learning of medical question summarization and medical RQE. The input to both tasks is a pair of medical questions. The first question is called a Consumer Health Question (CHQ), and the second question is called a Frequently Asked Question (FAQ). The CHQ is written by a patient and is usually longer and more informal, whereas the FAQ is usually a singlesentence question written by a medical expert. The purpose of both tasks is to match a CHQ to an FAQ, and ultimately to an expert-written answer that"
2021.acl-long.119,W17-4504,0,0.202468,"for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1505–1515 August 1–6, 2021. ©2021 Association for Computational Linguistics Task Learning (MTL) problem involving the two tasks of question summarization and Recognizing Question Entailment. We use a simple sum of learning objectives in Mrini et al. (2021b). In this paper, we introduce a novel, gradually soft multi-task and data-augmented approach to medical question understanding.1 Previous work on combining summarization and entailment uses at least 2 datasets – 1 from each task (Pasunuru et al., 2017; Guo et al., 2018). We first establish an equivalence between both tasks. This equivalence is the inspiration behind the data augmentation schemes introduced in our previous work (Mrini et al., 2021b). The goal of the data augmentation is to use a single dataset for MultiTask Learning. We propose to use a weighted loss function to simultaneously optimize for both tasks. Then, we propose a gradually soft parametersharing MTL approach. We conduct ablation studies to show that our two novelties – data augmentation and gradually soft parameter-sharing – improve performance in both tasks. Our prop"
2021.acl-long.119,D16-1264,0,0.0512791,"ee main baselines. The first one is BART (Lewis et al., 2019), where we only train on the summarization task. The second baseline trains BART on the same MTL settings as Pasunuru et al. (2017), using alternative training with entailment generation on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) and having a shared decoder and taskspecific encoders. The third baseline trains BART on the same MTL settings as Guo et al. (2018), where, on top of the entailment generation task, we add the question generation task using the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), and all parameters are soft-shared, except for the task-specific first encoder layer and last decoder layer. In addition, we also report the baselines assessed by Ben Abacha and Demner-Fushman (2019a) for MeQSum. For data augmentation, they use semantically-selected relevant question pairs from the Quora Question Pairs dataset (Iyer et al., 2017). Their results show that coverage loss (See et al., 2017) diminishes the added value of data augmentation in pointer-generator networks. Our summarization-only BART baseline exceeds all of the reported MeQSum baselines in ROUGE-1 F1. Summarization R"
2021.acl-short.29,2020.eval4nlp-1.4,1,0.891318,"ataset and pre-trained models to compute the UMIC1 . 1 Human Judgments : 1.875 out of 5 Figure 1: An example where the metric score for a given candidate caption varies significantly depending on the reference type. Introduction Image captioning is a task that aims to generate a description that explains the given image in a natural language. While there have been many advances for caption generation algorithms (Vinyals et al., 2015; Anderson et al., 2018) and target datasets (Fang et al., 2015; Sharma et al., 2018), few studies (Vedantam et al., 2015; Anderson et al., 2016; Cui et al., 2018; Lee et al., 2020) have focused on assessing the quality of the generated captions. Especially, most of the evaluation metrics only use reference captions to evaluate the caption although the main context is an image. However, as shown in Figure 1, since there are many possible reference captions for a single image, a candidate caption can receive completely different scores depending on the type of reference (Yi 1 https://github.com/hwanheelee1993/UMIC et al., 2020). Because of this diverse nature of image captions, reference-based metrics usually use multiple references which are difficult to obtain. To overc"
2021.acl-short.29,2021.naacl-main.253,0,0.0328394,"valuate our proposed metric on four benchmark datasets, including our new dataset. Experimental results show that our proposed unreferenced metric is highly correlated with human judgments than all of the previous metrics that use reference captions. 2 UNITER UNITER Quality Estimation Quality Estimation (QE) is a task that estimates the quality of the generated text without using the human references and this task is same as developing an unreferenced metric. QE is widely established in machine translation (MT) tasks (Specia et al., 2013; Martins et al., 2017; Specia et al., 2018). Recently, (Levinboim et al., 2021) introduces a large scale human ratings on image-caption pairs for training QE models in image captioning tasks. Our work also trains caption QE model, (i.e. unreferenced captioning metric) but we do not use human ratings to train the metric. Instead, we create diverse synthetic negative samples and train the metric with these samples via ranking loss. ????� Ranking Loss ???? ????� A person on bike going through green light with red truck nearby in a sunny day. Figure 2: Overall training procedure of UMIC. Given an image I, a positive caption x and a negative caption x ˆ, we compute the score"
2021.acl-short.29,W05-0909,0,0.436479,"?� Ranking Loss ???? ????� A person on bike going through green light with red truck nearby in a sunny day. Figure 2: Overall training procedure of UMIC. Given an image I, a positive caption x and a negative caption x ˆ, we compute the score of each image-caption pair Sx and Sxˆ using UNITER respectively. Then, we fine-tune UNITER using raking loss that Sx is higher than Sxˆ . Related Work Image Captioning Metrics Following other text generation tasks such as dialogue systems and machine translation, n-gram similarity metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) are widely used to evaluate an image caption. Especially, CIDEr (Vedantam et al., 2015), which weights each n-gram using TF-IDF, is widely used. SPICE (Anderson et al., 2016) is a captioning metric based on scene graph. BERTScore (Zhang et al., 2019), which computes the similarity of the contextualized embeddings, are also used. BERT-TBR (Yi et al., 2020) focuses on the variance in multiple hypothesis and ViLBERTScore (VBTScore) (Lee et al., 2020) utilizes ViLBERT (Lu et al., 2019) to improve BERTScore. Different from these metrics, VIFIDEL (Madhyastha et al., 2019) computes the word mover di"
2021.acl-short.29,P19-1654,0,0.0163619,"E (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) are widely used to evaluate an image caption. Especially, CIDEr (Vedantam et al., 2015), which weights each n-gram using TF-IDF, is widely used. SPICE (Anderson et al., 2016) is a captioning metric based on scene graph. BERTScore (Zhang et al., 2019), which computes the similarity of the contextualized embeddings, are also used. BERT-TBR (Yi et al., 2020) focuses on the variance in multiple hypothesis and ViLBERTScore (VBTScore) (Lee et al., 2020) utilizes ViLBERT (Lu et al., 2019) to improve BERTScore. Different from these metrics, VIFIDEL (Madhyastha et al., 2019) computes the word mover distance (Kusner et al., 2015) between the object labels in the image and the candidate captions, and it does not require reference captions. Similar to VIFIDEL, our proposed UMIC does not utilize the reference captions. However, UMIC directly uses image features and evaluates a caption in various perspectives compared to VIFIDEL. ???? 3 UMIC We propose UMIC, an unreferenced metric for image captioning using UNITER. We construct negative captions using the reference captions through the pre-defined rules. Then, we fine-tune UNITER to distinguish the reference captions"
2021.acl-short.29,Q17-1015,0,0.0243491,"udgments for the model-generated caption. Finally, we evaluate our proposed metric on four benchmark datasets, including our new dataset. Experimental results show that our proposed unreferenced metric is highly correlated with human judgments than all of the previous metrics that use reference captions. 2 UNITER UNITER Quality Estimation Quality Estimation (QE) is a task that estimates the quality of the generated text without using the human references and this task is same as developing an unreferenced metric. QE is widely established in machine translation (MT) tasks (Specia et al., 2013; Martins et al., 2017; Specia et al., 2018). Recently, (Levinboim et al., 2021) introduces a large scale human ratings on image-caption pairs for training QE models in image captioning tasks. Our work also trains caption QE model, (i.e. unreferenced captioning metric) but we do not use human ratings to train the metric. Instead, we create diverse synthetic negative samples and train the metric with these samples via ranking loss. ????� Ranking Loss ???? ????� A person on bike going through green light with red truck nearby in a sunny day. Figure 2: Overall training procedure of UMIC. Given an image I, a positive c"
2021.acl-short.29,P02-1040,0,0.113307,"in the metric with these samples via ranking loss. ????� Ranking Loss ???? ????� A person on bike going through green light with red truck nearby in a sunny day. Figure 2: Overall training procedure of UMIC. Given an image I, a positive caption x and a negative caption x ˆ, we compute the score of each image-caption pair Sx and Sxˆ using UNITER respectively. Then, we fine-tune UNITER using raking loss that Sx is higher than Sxˆ . Related Work Image Captioning Metrics Following other text generation tasks such as dialogue systems and machine translation, n-gram similarity metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) are widely used to evaluate an image caption. Especially, CIDEr (Vedantam et al., 2015), which weights each n-gram using TF-IDF, is widely used. SPICE (Anderson et al., 2016) is a captioning metric based on scene graph. BERTScore (Zhang et al., 2019), which computes the similarity of the contextualized embeddings, are also used. BERT-TBR (Yi et al., 2020) focuses on the variance in multiple hypothesis and ViLBERTScore (VBTScore) (Lee et al., 2020) utilizes ViLBERT (Lu et al., 2019) to improve BERTScore. Different from these metrics, VIF"
2021.acl-short.29,P18-1238,0,0.0645235,"Missing"
2021.acl-short.29,W18-6451,0,0.0639535,"Missing"
2021.acl-short.29,P13-4014,0,0.0638536,"Missing"
2021.acl-short.29,2020.acl-main.93,0,0.0325461,"r than Sxˆ . Related Work Image Captioning Metrics Following other text generation tasks such as dialogue systems and machine translation, n-gram similarity metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) are widely used to evaluate an image caption. Especially, CIDEr (Vedantam et al., 2015), which weights each n-gram using TF-IDF, is widely used. SPICE (Anderson et al., 2016) is a captioning metric based on scene graph. BERTScore (Zhang et al., 2019), which computes the similarity of the contextualized embeddings, are also used. BERT-TBR (Yi et al., 2020) focuses on the variance in multiple hypothesis and ViLBERTScore (VBTScore) (Lee et al., 2020) utilizes ViLBERT (Lu et al., 2019) to improve BERTScore. Different from these metrics, VIFIDEL (Madhyastha et al., 2019) computes the word mover distance (Kusner et al., 2015) between the object labels in the image and the candidate captions, and it does not require reference captions. Similar to VIFIDEL, our proposed UMIC does not utilize the reference captions. However, UMIC directly uses image features and evaluates a caption in various perspectives compared to VIFIDEL. ???? 3 UMIC We propose UMIC"
2021.bionlp-1.28,P19-1215,0,0.0689377,"f 2019 MEDIQA participants (Zhu et al., 2019), and add the validation set to training for the leaderboard submissions only. We notice that the validation results for the BART + XSum base model are significantly lower than other models. The corresponding test results are also the lowest-ranking, even though the difference is not as large as we trained on the validation set. These results show that training on an out-ofdomain abstractive summarization dataset is not efficient for this task. In addition to the XSum base model, we train on two additional datasets. The first dataset is MeQSum (Ben Abacha and Demner-Fushman, 2019). It is an abstractive medical question summarization We consider now the training on the medical dataset, which consists of 1,000 consumer health questions (CHQs) and their corresponding one- question summarization datasets. First, the valsentence-long frequently asked questions (FAQs). idation results show that training on MeQSum achieves comparable F1 scores as training on It was released by the U.S. National Institutes of Health (NIH), and the FAQs are written by med- HealthCareMagic. The main contrasting point is ical experts. Whereas Ben Abacha and Demner- that training on HealthCareMagi"
2021.bionlp-1.28,2021.bionlp-1.8,0,0.10773,"tractive summarization cannot be as long as the multiple documents in this task. We therefore propose to mitigate this weakness by proposing to cut up the input into pairs of sentences, where the first sentence is the input question, and the second one is a candidate answer. We then train our BART model to score the relevance of each candidate answer with regards to its corresponding question. We also describe in this paper the algorithm used to extract an AS2 dataset from an multi-document extractive summarization dataset. The 2021 Medical NLP and Question Answering (MEDIQA) shared task (Ben Abacha et al., 2021) is comprised of three tasks, centered around summarization in the medical domain: Question Summarization, Multi-Answer Summarization, and Radiology Report Summarization. In this paper, we focus on the first two tasks. In Question Summarization, the goal is to generate a one-sentence 2 Question Summarization formal question summary from a consumer health question – a relatively long question asked by a user. In Multi-Answer Summarization, we are given a Our approach to question summarization involves one-sentence question and multiple relevant answer two kinds of transfer learning. First, we t"
2021.bionlp-1.28,2020.emnlp-main.743,0,0.0708107,"on (AS2) problem. We show how we can preprocess the MEDIQA-AnS dataset such that it can be trained in an AS2 setting. Our AS2 model is able to generate extractive summaries achieving high ROUGE scores. 1 Introduction high results for question summarization. Sequenceto-sequence language model BART (Lewis et al., 2020) has achieved state-of-the-art results in various NLP benchmarks, including in the CNNDailymail news article summarization dataset (Hermann et al., 2015). We leverage this success and train BART on summarization datasets from the medical domain (Ben Abacha and DemnerFushman, 2019; Zeng et al., 2020; Mrini et al., 2021). Moreover, we find that training on a different task in the medical domain – Recognizing Question Entailment (RQE) (Ben Abacha and DemnerFushman, 2016) – can yield better improvements, especially in terms of ROUGE precision scores. Second, we tackle the extractive track of the multi-answer summarization task, and we cast multi-answer extractive summarization as an Answer Sentence Selection (AS2) problem. A limitation of BART is that the input to its abstractive summarization cannot be as long as the multiple documents in this task. We therefore propose to mitigate this we"
2021.bionlp-1.28,W19-5040,0,0.176069,"ing from Medical Summarization Summarization Datasets formal style in the FAQs of the U.S. National Library of Medicine (NLM), whereas iCliniq question summaries are noisier and more extractive. Given that MeQSum is 180 times smaller than HealthCareMagic, we train for 100 epochs on MeQSum, and 10 epochs for HealthCareMagic. We use the validation set of the MEDIQA question summarization task to select the best parameters. 2.2.2 Results and Discussion We show the validation results in Table 1 and the test results in Table 2. In all test results, we follow approaches of 2019 MEDIQA participants (Zhu et al., 2019), and add the validation set to training for the leaderboard submissions only. We notice that the validation results for the BART + XSum base model are significantly lower than other models. The corresponding test results are also the lowest-ranking, even though the difference is not as large as we trained on the validation set. These results show that training on an out-ofdomain abstractive summarization dataset is not efficient for this task. In addition to the XSum base model, we train on two additional datasets. The first dataset is MeQSum (Ben Abacha and Demner-Fushman, 2019). It is an ab"
2021.bionlp-1.28,W19-5039,0,0.0698702,"eQSum to the training (RQE + MeQSum) seems to decrease precision, increase recall, achieve similar ROUGE-1 F1, but lower ROUGE-2 and ROUGE-L F1 scores. In Table 2, we notice that the test results that the RQE + MeQSum model is the clear winner, providing the highest scores across the board, with the exception of ROUGE-2 precision. Overall, it seems that pre-training on a similar task in the medical domain is beneficial for this medical question summarization task. 3 Multi-Answer Extractive Summarization For the RQE task, we use the RQE dataset from 3.1 Dataset the 2019 MEDIQA shared task (Ben Abacha et al., 2019). The training set was introduced in The dataset for this task is the MEDIQA-AnS Ben Abacha and Demner-Fushman (2016). Sim- dataset (Savery et al., 2020). It contains 156 userilarly to MeQSum, this dataset is released by the written medical questions, and answer articles to U.S. National Institutes of Health. The MEDIQA- these questions, such that one question usually RQE dataset contains 8,588 training question pairs. has more than one answer article. There are also We train for 10 epochs and choose the best parame- manually-written abstractive and extractive sumters using the validation set"
2021.bionlp-1.28,2020.acl-main.703,0,0.267742,"to train on the task of Recognizing Question Entailment (RQE) in the medical domain. We show that both transfer learning methods combined achieve the highest ROUGE scores. Finally, we cast the question-driven extractive summarization of multiple relevant answer documents as an Answer Sentence Selection (AS2) problem. We show how we can preprocess the MEDIQA-AnS dataset such that it can be trained in an AS2 setting. Our AS2 model is able to generate extractive summaries achieving high ROUGE scores. 1 Introduction high results for question summarization. Sequenceto-sequence language model BART (Lewis et al., 2020) has achieved state-of-the-art results in various NLP benchmarks, including in the CNNDailymail news article summarization dataset (Hermann et al., 2015). We leverage this success and train BART on summarization datasets from the medical domain (Ben Abacha and DemnerFushman, 2019; Zeng et al., 2020; Mrini et al., 2021). Moreover, we find that training on a different task in the medical domain – Recognizing Question Entailment (RQE) (Ben Abacha and DemnerFushman, 2016) – can yield better improvements, especially in terms of ROUGE precision scores. Second, we tackle the extractive track of the m"
2021.bionlp-1.28,W02-0109,0,0.281306,"n: Mean In the AS2 setting, we train BART to predict Average Precision (MAP) and Mean Reciprocal the relevance score of a candidate answer given a Rank (MRR). MAP measures how many of the topquestion. To obtain the pairs of questions and canranked answers are relevant, whereas MRR meadidate answers from the MEDIQA-AnS dataset, we sures how highly a first relevant answer is ranked. proceed as follows. First, we concatenate for each We compute the scores as follows, given a set Q of question the text data of its corresponding answer questions: articles. Then, we use the NLTK sentence tokenizer (Loper and Bird, 2002) to split this text data into P individual sentences. Finally, we form questionq∈Q average_precision(q) MAP(Q) = (1) sentence pairs for AS2 by pairing the user question |Q| with each sentence from the corresponding answer P article text data. 1 q∈Q rank(q) In this training context, AS2 is a binary clasMRR(Q) = (2) |Q| sification task, where each pair of question and We take as base models the BART + XSum candidate answer is labeled as relevant (1) or irrelevant (0). We use cross-entropy as the loss function. model, as well as the best-performing model in We label sentences contained in the ref"
2021.bionlp-1.28,2021.nlpmc-1.8,1,0.794107,"e show how we can preprocess the MEDIQA-AnS dataset such that it can be trained in an AS2 setting. Our AS2 model is able to generate extractive summaries achieving high ROUGE scores. 1 Introduction high results for question summarization. Sequenceto-sequence language model BART (Lewis et al., 2020) has achieved state-of-the-art results in various NLP benchmarks, including in the CNNDailymail news article summarization dataset (Hermann et al., 2015). We leverage this success and train BART on summarization datasets from the medical domain (Ben Abacha and DemnerFushman, 2019; Zeng et al., 2020; Mrini et al., 2021). Moreover, we find that training on a different task in the medical domain – Recognizing Question Entailment (RQE) (Ben Abacha and DemnerFushman, 2016) – can yield better improvements, especially in terms of ROUGE precision scores. Second, we tackle the extractive track of the multi-answer summarization task, and we cast multi-answer extractive summarization as an Answer Sentence Selection (AS2) problem. A limitation of BART is that the input to its abstractive summarization cannot be as long as the multiple documents in this task. We therefore propose to mitigate this weakness by proposing t"
2021.bionlp-1.28,D18-1206,0,0.0672727,"ransfer learning from other tasks in the medical domain increases from pre-trained language models can achieve very ROUGE scores. 257 Proceedings of the BioNLP 2021 workshop, pages 257–262 June 11, 2021. ©2021 Association for Computational Linguistics 2.1 Training Details We adopt the BART Large architecture (Lewis et al., 2020), as it set a state of the art in abstractive summarization benchmarks, and allows us to train a single model on generation and classification tasks. We use a base model, which is trained on BART’s language modeling tasks and the XSum abstractive summarization dataset (Narayan et al., 2018). We use a learning rate of 3 ∗ 10−5 for summarization tasks and 1 ∗ 10−5 for the recognizing question entailment task. We use 512 as the maximum number of token positions. Following the MEDIQA instructions and leaderboard, we use precision, recall and F1 scores for the ROUGE-1, ROUGE-2 and ROUGE-L metrics (Lin, 2004). 2.2 2.2.1 Transfer Learning from Medical Summarization Summarization Datasets formal style in the FAQs of the U.S. National Library of Medicine (NLM), whereas iCliniq question summaries are noisier and more extractive. Given that MeQSum is 180 times smaller than HealthCareMagic,"
2021.emnlp-main.144,2020.nlp4convai-1.5,0,0.342313,"eral respects: Firstly, we specifically tackle the few-shot intent detection task rather than the general full-shot learning; Secondly, we design a schema and employ contrastive learning in both self-supervised pretraining and supervised fine-tuning stages. Since this work is related to few-shot intent detection and contrastive learning, we review recent work from both areas in this section. The few-shot intent detection task typically includes three scenarios: (1) learn a intent detection model with only K examples for each intent (Zhang et al., 2020a; Mehri et al., 2020a; 3 CPFT Methodology Casanueva et al., 2020); (3) learn to identify both We consider a few-shot intent detection task that in-domain and out-of-scope queries with only K examples for each intent (Zhang et al., 2020a, 2021; handles C user intents, where the task is to classify Xia et al., 2021b). (2) given a model trained on ex- a user utterance u into one of the C classes. We set balanced K-shot learning for each intent (Zhang isting intents with all examples, learn to generalize et al., 2020a; Casanueva et al., 2020), i.e., each the model to new intents with only K examples for intent only includes K examples in the training each new i"
2021.emnlp-main.144,N19-1423,0,0.194539,"s a data augmentation schema, which preance understanding before introducing the supertrains a model on annotated pairs from natural lanvised fine-tuning for few-shot intent detection. guage inference (NLI) datasets and designs the nearest neighbor classification schema to adopt the 3.1 Self-supervised Pre-training transfer learning and classify user intents. However, We retrieve the feature representation hi for the i-th the training is expensive and hard to scale to tasks user utterance through an encoder model, which in with hundreds of intents (Liu et al., 2020). Mehri this paper is BERT (Devlin et al., 2019), i.e., hi = et al. (2020b); Casanueva et al. (2020) propose the BERT(ui ). We implicitly learn the sentence-level task-adaptive training, which leverages models preutterance understanding and discriminate semantitrained from a few hundred million dialogues to cally similar utterances through the self-supervised tackle few-shot intent detection. It also includes contrastive learning method (Wu et al., 2020b; Liu an unsupervised mask language modeling loss on et al., 2021a; Gao et al., 2021): the target intent datasets and shows promising imN provements. ¯ i )/τ ) exp(sim(hi , h 1 X L = − log P"
2021.emnlp-main.144,2021.emnlp-main.552,0,0.222075,"150 intents. Many intents in the datasets are user utterances, is a key component in task-oriented similar. Therefore, training models is rather chaldialog systems. In real systems such as Amazon lenging when there are only limited examples. Alexa, correctly identifying user intents is crucial Inspired by the recent success of contrastive for downstream tasks (Zhang et al., 2020b; Ham learning (He et al., 2020; Gunel et al., 2020; Chen et al., 2020). A practical challenge is data scarcity et al., 2020; Radford et al., 2021; Liu et al., 2021a; as it is expensive to annotate enough examples for Gao et al., 2021; Liu et al., 2021b), which aims emerging intents, and how to accurately identify to enhance discrimination abilities of models, this intents in few-shot learning has raised attention. Existing methods address the few-shot intent de- work proposes improving few-shot intent detectection tasks mainly from two perspectives: (1) tion via Contrastive Pre-training and Fine-Tuning (CPFT). Intuitively, we first learn to implicitly data augmentation and (2) task-adaptive training with pre-trained models. For the first category, discriminate semantically similar utterances via Zhang et al. (2020a) and M"
2021.emnlp-main.144,D18-1300,0,0.0633331,"Missing"
2021.emnlp-main.144,2020.acl-main.54,0,0.0784602,"Missing"
2021.emnlp-main.144,D19-1131,0,0.0511889,"Missing"
2021.emnlp-main.144,2021.emnlp-main.109,0,0.0588791,"Missing"
2021.emnlp-main.144,2021.naacl-main.237,0,0.0606744,"Missing"
2021.emnlp-main.144,2020.emnlp-main.66,0,0.0211991,"or the i-th the training is expensive and hard to scale to tasks user utterance through an encoder model, which in with hundreds of intents (Liu et al., 2020). Mehri this paper is BERT (Devlin et al., 2019), i.e., hi = et al. (2020b); Casanueva et al. (2020) propose the BERT(ui ). We implicitly learn the sentence-level task-adaptive training, which leverages models preutterance understanding and discriminate semantitrained from a few hundred million dialogues to cally similar utterances through the self-supervised tackle few-shot intent detection. It also includes contrastive learning method (Wu et al., 2020b; Liu an unsupervised mask language modeling loss on et al., 2021a; Gao et al., 2021): the target intent datasets and shows promising imN provements. ¯ i )/τ ) exp(sim(hi , h 1 X L = − log PN , (1) uns cl Contrastive learning has shown superior perfor¯ N i=1 exp(sim(h i , hj )/τ ) j=1 mance on various domains, such as visual representation (He et al., 2020; Chen et al., 2020; Radford where N is the number of sentences in a batch. τ et al., 2021), graph representation (Qiu et al., 2020; is a temperature parameter that controls the penalty You et al., 2020), and recommender systems (Liu to nega"
2021.emnlp-main.144,2020.findings-emnlp.303,1,0.665942,"wo perspectives: (1) tion via Contrastive Pre-training and Fine-Tuning (CPFT). Intuitively, we first learn to implicitly data augmentation and (2) task-adaptive training with pre-trained models. For the first category, discriminate semantically similar utterances via Zhang et al. (2020a) and Mehri et al. (2020b) pro- contrastive self-supervised pre-training on intent datasets without using any intent labels. We then pose a nearest neighbor classification schema with jointly perform few-shot intent detection and sufull use of the limited training examples in both training and inference stages. Xia et al. (2020b) pervised contrastive learning. The supervised conand Peng et al. (2020) propose to generate utter- trastive learning helps the model explicitly learn to pull utterances from the same intent close and push ances for emerging intents based on variational utterances across different intents apart. ∗ Work done while the first author was an intern at Adobe Research. Our contributions are summarized as follows: 1906 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1906–1912 c November 7–11, 2021. 2021 Association for Computational Linguistics 1) We des"
2021.emnlp-main.144,2021.naacl-main.106,1,0.769223,". Since this work is related to few-shot intent detection and contrastive learning, we review recent work from both areas in this section. The few-shot intent detection task typically includes three scenarios: (1) learn a intent detection model with only K examples for each intent (Zhang et al., 2020a; Mehri et al., 2020a; 3 CPFT Methodology Casanueva et al., 2020); (3) learn to identify both We consider a few-shot intent detection task that in-domain and out-of-scope queries with only K examples for each intent (Zhang et al., 2020a, 2021; handles C user intents, where the task is to classify Xia et al., 2021b). (2) given a model trained on ex- a user utterance u into one of the C classes. We set balanced K-shot learning for each intent (Zhang isting intents with all examples, learn to generalize et al., 2020a; Casanueva et al., 2020), i.e., each the model to new intents with only K examples for intent only includes K examples in the training each new intent (Xia et al., 2020a,b, 2021a). data. As such, there are in total C · K training In this work, we focus on the first scenario, examples. and several methods have been proposed to tackle In the following section, we first describe the the challen"
2021.emnlp-main.144,2020.acl-demos.12,0,0.0695776,"Missing"
2021.emnlp-main.144,2020.emnlp-main.411,1,0.903982,"and 10-shot settings. tents. For instance, BANKING77 (Casanueva et al., 1 Introduction 2020) has a single domain with 77 intents, and CLINC150 (Larson et al., 2019) has ten domains Intent detection, aiming to identify intents from with 150 intents. Many intents in the datasets are user utterances, is a key component in task-oriented similar. Therefore, training models is rather chaldialog systems. In real systems such as Amazon lenging when there are only limited examples. Alexa, correctly identifying user intents is crucial Inspired by the recent success of contrastive for downstream tasks (Zhang et al., 2020b; Ham learning (He et al., 2020; Gunel et al., 2020; Chen et al., 2020). A practical challenge is data scarcity et al., 2020; Radford et al., 2021; Liu et al., 2021a; as it is expensive to annotate enough examples for Gao et al., 2021; Liu et al., 2021b), which aims emerging intents, and how to accurately identify to enhance discrimination abilities of models, this intents in few-shot learning has raised attention. Existing methods address the few-shot intent de- work proposes improving few-shot intent detectection tasks mainly from two perspectives: (1) tion via Contrastive Pre-training and"
2021.emnlp-main.144,2020.starsem-1.17,1,0.93319,"and 10-shot settings. tents. For instance, BANKING77 (Casanueva et al., 1 Introduction 2020) has a single domain with 77 intents, and CLINC150 (Larson et al., 2019) has ten domains Intent detection, aiming to identify intents from with 150 intents. Many intents in the datasets are user utterances, is a key component in task-oriented similar. Therefore, training models is rather chaldialog systems. In real systems such as Amazon lenging when there are only limited examples. Alexa, correctly identifying user intents is crucial Inspired by the recent success of contrastive for downstream tasks (Zhang et al., 2020b; Ham learning (He et al., 2020; Gunel et al., 2020; Chen et al., 2020). A practical challenge is data scarcity et al., 2020; Radford et al., 2021; Liu et al., 2021a; as it is expensive to annotate enough examples for Gao et al., 2021; Liu et al., 2021b), which aims emerging intents, and how to accurately identify to enhance discrimination abilities of models, this intents in few-shot learning has raised attention. Existing methods address the few-shot intent de- work proposes improving few-shot intent detectection tasks mainly from two perspectives: (1) tion via Contrastive Pre-training and"
2021.emnlp-main.520,J08-4004,0,0.14758,"ange of what is normally found in annotating speech transcripts for extractive summaries (0.1∼0.3; Marge 4 We use 10-second intervals rather than utterances as measuring units as the duration of utterances vary. If annotators all selected some content, or no content at all, from a 10-second interval, they are in agreement. et al., 2010), as annotating spoken text is a highly challenging task. We find that annotators tend to perceive the same region as salient but they may disagree as to which utterances should be included in the summary due to verbosity of spoken text. We refer the reader to (Artstein and Poesio, 2008) for interpretations and improvements to IAA. 4 Summarization Let X denote a sequence of spoken utterances from a segment of the transcript. Our summarizer aims to extract a subset of utterances Y ⊂ X that convey the essential content of the input. We experiment with an unsupervised summarizer that leverages vector-quantized variational autoencoders (VQVAE; van den Oord et al., 2017) to learn utterance representations and identifies summary utterances. The method was explored for opinion summarization (Angelidis et al., 2021) and machine translation (Prato et al., 2020). We are interested in u"
2021.emnlp-main.520,P18-1063,0,0.0232141,"ng baselines on the dataset and shed light on the task of livestream transcript summarization. Our contributions are as follows. • We create a detailed annotation interface and new benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with t"
2021.emnlp-main.520,N18-2097,1,0.84334,"transcript summarization. Our contributions are as follows. • We create a detailed annotation interface and new benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with the audience (Figure 2). We investifrom livestream transcripts to f"
2021.emnlp-main.520,D18-1409,0,0.0460202,"Missing"
2021.emnlp-main.520,P19-1102,0,0.0217495,"s are as follows. • We create a detailed annotation interface and new benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with the audience (Figure 2). We investifrom livestream transcripts to form preview sum- gate extractive rather tha"
2021.emnlp-main.520,D18-1443,0,0.0204063,"the task of livestream transcript summarization. Our contributions are as follows. • We create a detailed annotation interface and new benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with the audience (Figure 2). We investifrom livestr"
2021.emnlp-main.520,D19-1620,0,0.0182944,"eamer talks about Digital Painting with Maddy Bellwoar to create fairytale themed images. The annotators are asked to write a concise summary of this clip using their own words (Task A) and identify summary utterances (Task B). indicates the number of minutes since the beginning of the recording. When a user hovers over the thumbnail or scrolls past a video, we expect a textual summary to give a glimpse of the verbal content. This view of summarization leads us to annotate salient content across the video in an equally detailed manner. It naturally avoids lead bias that is ubiquitous in news (Grenander et al., 2019). We segment a video into 5-minute clips and annotate each clip for summary-worthy content. A clip contains an average of 51 utterances and 460 words. Due to time and budget constraints, we select 370 streamed video for summary annotation.3 Table 1 provides a detailed comparison of our annotated corpus with previous datasets, including Switchboard (Godfrey et al., 1992), ICSI (Janin et al., 2003) and AMI (Carletta et al., 2006) that contain both transcripts and human-annotated ex6459 3 Details of video selection are provided in Supplementary. Dataset Switchboard ICSI AMI StreamHover Type Telep"
2021.emnlp-main.520,2020.acl-main.437,0,0.0372448,"multiple as the latter are prone to generate hallucinated condimensions and discuss its strengths and weak- tent that does not exist in the source text (Cao nesses. Empirical results show that our method et al., 2017; Kryscinski et al., 2019; Lebanoff et al., 1 outperforms strong summarization baselines. 2019; Maynez et al., 2020). The problem could be exacerbated by ungrammatical spoken utterances and transcription errors. Instead, we consider VQ2 Related Work VAE, an unsupervised representation learning techClosed captions are often provided onscreen, turn- nique (van den Oord et al., 2017; Jin et al., 2020; ing streaming videos into text on an unprecedented Angelidis et al., 2021) for content extraction. Unsuscale (Besik, 2020). However, there are very few pervised training of the VQ-VAE model and its insummarization studies that attempt to generate text ference could potentially be performed at the same previews of streaming videos to help users browse time, allowing important utterances to be extracted or refind information that has been watched before. from a transcript segment on-the-fly during streamNeural text summarizers have focused primarily on ing, without interrupting the learning pr"
2021.emnlp-main.520,L18-1016,0,0.0280773,"017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Palaskar et al., 2019; Li et al., 2020) as little information could be gleaned from videos that mirror the artists’ desktop. Instead, we focus on generating short descriptions from transcripts and leave for future work crossmodality resea"
2021.emnlp-main.520,D18-1208,0,0.0382294,"Missing"
2021.emnlp-main.520,D19-5413,1,0.902295,"Missing"
2021.emnlp-main.520,2020.acl-main.703,0,0.0764347,"o training, validation and test splits: • 3,884 clips (320 videos / 323 hours) in training, • 728 clips (25 videos / 61 hours) in validation, • 809 clips (25 videos / 67 hours) in test split. lead bias in news writing. Our setting is challenging as not only are there few utterances deemed to be summary-worthy but such utterances can occur anywhere in a video clip. Baselines. We compare StreamHover with stateof-the-art extractive and abstractive summarizers. The abstractive summarizers generate an abstract from the transcript of a clip without tuning.6 These include BART-large, BART-large-cnn (Lewis et al., 2020) and T5 (Raffel et al., 2020), which are some of the strongest performing neural abstractive summarizers that are pre-trained on language modeling and summarization tasks. The unsupervised extractive summarizers extract salient utterances from a clip. LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based models that extract relevant sentences based on eigenvector centrality. SumBasic (Vanderwende et al., 2007) assigns higher scores to sentences containing frequently occurring content words. We further compare to a novel unsupervised graph-based summarization m"
2021.emnlp-main.520,D19-1370,0,0.154554,"t al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Palaskar et al., 2019; Li et al., 2020) as little information could be gleaned from videos that mirror the artists’ desktop. Instead, we focus"
2021.emnlp-main.520,P19-1210,0,0.100629,"t al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Palaskar et al., 2019; Li et al., 2020) as little information could be gleaned from videos that mirror the artists’ desktop. Instead, we focus"
2021.emnlp-main.520,2020.emnlp-main.752,0,0.0312864,"), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Palaskar et al., 2019; Li et al., 2020) as little information could be gleaned from videos that mirror the artists’ desktop. Instead, we focus on generating short descriptions from transcripts and leave for future work crossmodality research. We describe our data annotation process in the following section. 3 Our Dataset We aim to create a large and representative corpus containing transcripts and summaries of streamed videos. We explore a leading social media platform (Behance.net) supported by Adobe Creative Cloud that features livestreams of creative work by artists and designers. The website boasts over 10 million users, who wa"
2021.emnlp-main.520,W04-1013,0,0.109384,"Missing"
2021.emnlp-main.520,P08-2051,0,0.0604866,"4, 2048}. The coefficient β used for commitment loss is set 7 The recent automatic metrics (Zhang et al., 2020; Sellam to 0.25 (Eq. (6)). These hyperparameters are tuned et al., 2020) have not been tested on speech transcripts. Spoken text contains filled pauses (um, uh, well), disfluencies (go-goon the validation set. We keep only utterances that go away), repetitions and verbal interruptions. ROUGE is the contain >5 words in consideration. The final train- only metric that has been validated to attain good correlation ing set contains 168,111 utterances. with human judgments on transcripts (Liu and Liu, 2008). 6463 FluCovRank Utterances • top left bottom / cloud studies today / find links to their original posts / hey jennifer saw the images / love the top left and bottom / info tab and i uploaded / colors are beautiful but im partial through colorful sky scenes / pretty large about 4000 by 4000 pixels / photo studies of today / moment 0 Hello good morning everybody welcome high foster highly art. 1 Hi Lisa, welcome everyone. 2 I hope you guys are having a good day so far. 3 Good to see you were going to be doing cloud studies today. 4 So if anybody is interested in joining in, if you want to work"
2021.emnlp-main.520,D19-1387,0,0.0905612,"tion. Our contributions are as follows. • We create a detailed annotation interface and new benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with the audience (Figure 2). We investifrom livestream transcripts to form preview sum- gate"
2021.emnlp-main.520,W10-0716,0,0.0167421,"Missing"
2021.emnlp-main.520,2020.acl-main.174,0,0.0249024,"is written text, including news articles, reviews, scien- also easier to tailor the model to specific domains tific papers and book chapters (See et al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Pala"
2021.emnlp-main.520,P08-1054,0,0.137101,"Missing"
2021.emnlp-main.520,2020.findings-emnlp.1,0,0.0164094,"r the reader to (Artstein and Poesio, 2008) for interpretations and improvements to IAA. 4 Summarization Let X denote a sequence of spoken utterances from a segment of the transcript. Our summarizer aims to extract a subset of utterances Y ⊂ X that convey the essential content of the input. We experiment with an unsupervised summarizer that leverages vector-quantized variational autoencoders (VQVAE; van den Oord et al., 2017) to learn utterance representations and identifies summary utterances. The method was explored for opinion summarization (Angelidis et al., 2021) and machine translation (Prato et al., 2020). We are interested in using the method to account for domain characteristics of livestreams, which showcase new and creative work of artists and designers on their use of Photoshop, Illustrator, and other tools.5 VQ-VAE is a powerful framework for learning latent variable models using deep neural networks. It learns discrete vector representations for an utterance, which is then used to categorize the utterance along various dimensions. E.g., “Good morning Hi Everybody” suggests a greeting and opens up a dialogue; “I had probably 3 or 4 different customers on YouTube and ... on Facebook asked"
2021.emnlp-main.520,P17-1099,0,0.0301439,"w pervised training of the VQ-VAE model and its insummarization studies that attempt to generate text ference could potentially be performed at the same previews of streaming videos to help users browse time, allowing important utterances to be extracted or refind information that has been watched before. from a transcript segment on-the-fly during streamNeural text summarizers have focused primarily on ing, without interrupting the learning process. It is written text, including news articles, reviews, scien- also easier to tailor the model to specific domains tific papers and book chapters (See et al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al.,"
2021.emnlp-main.520,2020.acl-main.704,0,0.0382223,"Missing"
2021.emnlp-main.520,P18-1062,0,0.125413,"ome of the strongest performing neural abstractive summarizers that are pre-trained on language modeling and summarization tasks. The unsupervised extractive summarizers extract salient utterances from a clip. LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based models that extract relevant sentences based on eigenvector centrality. SumBasic (Vanderwende et al., 2007) assigns higher scores to sentences containing frequently occurring content words. We further compare to a novel unsupervised graph-based summarization method for speech transcripts: FluCovRank (Shang et al., 2018) groups utterances into clusters, generates an abstractive sentence from each cluster, then selects the best elements from abstractive sentences under a budget constraint. Finally, we compare our approach with the Quantized Transformer (Angelidis et al., 2021), which uses a clustering interpretation of the quantized space and two-step sampling algorithm to extract summary sentences from reviews. Settings. We use pretrained BERT-BASE as our embedder Embedθ (·). The model has 12 layers, 12 heads per layer and a hidden size (H) of 768. A 6layer Transformer decoder is used as the generator Generat"
2021.emnlp-main.520,D19-1298,0,0.0170271,"t, livestreaming platforms may not of general-domain models. We refrain from using fully meet the needs of their customers. sequential methods for utterance selection. First, 6457 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6457–6474 c November 7–11, 2021. 2021 Association for Computational Linguistics it is difficult to scale up sequential prediction to process transcripts that exceed the maximum allowed length, even with models that handle long text (Beltagy et al., 2020; Zhao et al., 2020). Second, sequential methods (Narayan et al., 2018b; Xiao and Carenini, 2019) may not give enough flexibility to select salient utterances on-the-fly when content is being streamed live, thus they are unsuitable for our case. There has been a shortage of annotated datasets that are necessary for livestream transcript summarization. We build a browser-based user interface for summary annotation that provides to the annotators a clip of the livestream recording alongside a synchronized display of the transcript. The interface allows annotators to conveniently label summary utterances and write an abstractive summary using their own words (Figure 3). With a total of 500 h"
2021.emnlp-main.520,2021.naacl-main.110,1,0.73255,"ew benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with the audience (Figure 2). We investifrom livestream transcripts to form preview sum- gate extractive rather than abstractive approaches maries. We evaluate the method across mul"
2021.emnlp-main.520,2020.acl-main.553,0,0.0121756,"ortant utterances to be extracted or refind information that has been watched before. from a transcript segment on-the-fly during streamNeural text summarizers have focused primarily on ing, without interrupting the learning process. It is written text, including news articles, reviews, scien- also easier to tailor the model to specific domains tific papers and book chapters (See et al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, nove"
2021.emnlp-main.520,P13-1137,0,0.030033,"nd book chapters (See et al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Palaskar et al., 2019; Li et al., 2020) as little information could be gleaned from videos that mirror the artists’ desktop. I"
2021.findings-acl.98,S17-2001,0,0.0281359,"Missing"
2021.findings-acl.98,P19-1004,0,0.152725,"ragraph), we only re-ordered the words in one sentence from only one input while keeping the rest of the inputs unchanged. Constraining the modifications to a single sentence enables us to measure (1) the importance of word order in a single sentence; and (2) the interaction between the shuffled words and the unchanged, real context. Random shuffling methods To understand model behaviors across varying degrees of wordorder distortions, we experimented with three tests: randomly shuffling n-grams where n = {1, 2, 3}. Shuffling 1-grams is a common technique for analyzing word-order sensitivity (Sankar et al., 2019; Zanzotto et al., 2020). We split a given sentence by whitespace into a list of n-grams, and re-combined them, in a random order, back into a “shuffled” sentence (see Table 1 for examples). The ending punctuation was kept intact. We re-sampled a new random permutation until the shuffled sentence was different from the original sentence. As the label distributions, dev-set sizes, and the performance of models vary across GLUE tasks, 2. We only selected the examples that were correctly classified by the classifier (to study what features were important for high accuracy). 3. We balanced the num"
2021.findings-acl.98,D16-1255,0,0.0696696,"ved model dev-set performance on SQuAD 2.0 (b) and all the tested tasks in GLUE (a), except SST-2. pretrained BERT, we instead study BERT-based models finetuned on downstream tasks. Word-ordering as an objective In text generation, Elman (1990) found that recurrent neural networks were sensitive to regularities in word order in simple sentences. Language models (Mikolov et al., 2010) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) were able to recover the original word order of a sentence from randomly-shuffled words even without any explicit syntactic information (Schmaltz et al., 2016). Wang et al. 2020 also observed an increase in GLUE performance after pretraining BERT with two additional objectives of wordordering and sentence-ordering. Their work differs from ours in three points: (1) they did not study the importance of word order alone; (2) StructBERT improvements were inconsistent across tasks and models (Table 2d) and motivated us to compare the word-order importance between GLUE tasks; and (3) we proposed to improve model performance by finetuning not pretraining. Word-order insensitivity in other NLP tasks ML models have been shown to be insensitive to word order"
2021.findings-acl.98,D13-1170,0,0.00584457,"Missing"
2021.findings-acl.98,D16-1244,0,0.111841,"Missing"
2021.naacl-main.170,W05-0909,0,0.449127,"hat have been used to evaluate GenQA systems. BLEU is a popular evaluation metric for generated text based on n-gram precision. BLEU scores a candidate by counting the number present in the reference among the n-gram of the candidate. In general, n varies from 1 to 4, and the scores for varying n are aggregated with a geometric mean. ROUGE is a set of evaluation metrics used for automatic text generation such as summarization and machine translation. Typically, most studies use ROUGE-L, which is a F-measure based on the longest common subsequence between a candidate and the reference. METEOR (Banerjee and Lavie, 2005) is an F1 score of a set of unigram alignments. METEOR has a unique property that it considers stemmed words, synonyms, and paraphrases, as well as the standard exact word matches. CIDER (Vedantam et al., 2015) is a consensusbased evaluation metric that is designed for a high correlation with human judgment in the image captioning problem. CIDEr uses Term FrequencyInverse Document Frequency (TF-IDF) weights for human-like evaluation. BERTScore is a recently proposed text evaluation metric that use pre-trained representations from BERT (Devlin et al., 2019). BERTScore first computes the context"
2021.naacl-main.170,D18-1454,0,0.0373618,"Missing"
2021.naacl-main.170,D19-1255,0,0.0186571,"2019). attention from the natural language processing comFor instance, in the example in Figure 1 from munity. Recently, research on QA systems has the MS-MARCO (Bajaj et al., 2016), the generated reached the stage of generating free-form answers, answer receives a high score on BLEU-1 (0.778) called GenQA, beyond extracting the answer to a and ROUGE-L (0.713) due to the many overlaps given question from the context (Yin et al., 2016; of words with those in the reference. However, huSong et al., 2017; Bauer et al., 2018; Nishida et al., mans assign a low score of 0.063 on the scale from 2019; Bi et al., 2019, 2020). However, as a bot- 0 to 1 due to the mismatch of critical information. tleneck in developing GenQA models, there are As in this example, we find that existing metrics ofno proper automatic metrics to evaluate generated ten fail to capture the correctness of the generated answers (Chen et al., 2019). answer that considers the key information for the In evaluating a GenQA model, it is essential to question. consider whether a generated response correctly To overcome this shortcoming of the existing contains vital information to answer the question. metrics, we propose a new metric calle"
2021.naacl-main.170,D19-5817,0,0.3038,"QA, beyond extracting the answer to a and ROUGE-L (0.713) due to the many overlaps given question from the context (Yin et al., 2016; of words with those in the reference. However, huSong et al., 2017; Bauer et al., 2018; Nishida et al., mans assign a low score of 0.063 on the scale from 2019; Bi et al., 2019, 2020). However, as a bot- 0 to 1 due to the mismatch of critical information. tleneck in developing GenQA models, there are As in this example, we find that existing metrics ofno proper automatic metrics to evaluate generated ten fail to capture the correctness of the generated answers (Chen et al., 2019). answer that considers the key information for the In evaluating a GenQA model, it is essential to question. consider whether a generated response correctly To overcome this shortcoming of the existing contains vital information to answer the question. metrics, we propose a new metric called KPQAThere exist several n-gram similarity metrics such metric for evaluating GenQA systems. To derive ∗ the metric, we first develop Keyphrase Predictor This research was done while the author was affiliated with Adobe Research. for Question Answering (KPQA). KPQA computes 2105 Proceedings of the 2021 Con"
2021.naacl-main.170,N19-1423,0,0.0275875,"idate and the reference. METEOR (Banerjee and Lavie, 2005) is an F1 score of a set of unigram alignments. METEOR has a unique property that it considers stemmed words, synonyms, and paraphrases, as well as the standard exact word matches. CIDER (Vedantam et al., 2015) is a consensusbased evaluation metric that is designed for a high correlation with human judgment in the image captioning problem. CIDEr uses Term FrequencyInverse Document Frequency (TF-IDF) weights for human-like evaluation. BERTScore is a recently proposed text evaluation metric that use pre-trained representations from BERT (Devlin et al., 2019). BERTScore first computes the contextual embeddings for given references and candidates independently with BERT, and then computes pairwise cosine similarity scores. When computing similarity, BERTScore adopts Inverse Document Frequency (IDF) to apply importance weighting. 3 Proposed Metric for Evaluating GenQA To build a better metric for GenQA, we first propose KPQA. By considering the question, the KPQA assigns different weights to each token in the answer sentence such that salient tokens receive a high value. We then integrate the KPQA into existing metrics to make them evaluate correctn"
2021.naacl-main.170,P19-1346,0,0.0737762,"n “-KP"" metric for all of the three variants. These results show that training keyphrase predictor to find the short answer candidate in the sentence is effective for capturing the key information in the generated answer, but it is more effective when the question information is integrated. 5.4 Analysis is lower for BERTScore. We speculate that this is because the original BERTScore uses IDF-based importance weighting, unlike other metrics. Multiple Sentence Answers: Most of the answers in MS-MARCO and AVSD consist of single sentences, but the answers for GenQA can be multiple sentences like (Fan et al., 2019). To verify our KPQA-metric on multiple sentence answers, we collect additional 100 human judgments for the generated answer whose answers are multiple sentences in the MS-MARCO like the example in Figure 5, and evaluate the various metrics on this dataset. As shown in Table 6, our KPQA integrated metric shows still higher correlations than other metrics. We observe that the gap between KPQA integrated metrics and existing metrics is relatively lower than that of Table 3. We speculate this is because many of the multiple sentence answers are DESCRIPTION type answers whose keyphrases are someti"
2021.naacl-main.170,W18-2605,0,0.0175729,"useful for comparing the generated answers from different models. 6 Related Work are due to higher weights on units such as “million"" or “years."" There exist a total of ten error cases for One important next step for current QA systems this type, and we believe that there is room for is to generate answers in natural language for a improvement with regard to these errors through given question and context. Following this interest, post-processing. In the case of the DESCRIPTION several generative (abstractive) QA datasets (Bajaj question type, 17 out of 31 cases are due to inap- et al., 2016; He et al., 2018; Koˇciský et al., 2018; propriate importance weights. We speculate this Fan et al., 2019), where the answer is not necesresult is because the keyphrases for the answers sarily in the passage, have recently been released. to questions belonging to the DESCRIPTION type Since the task is to generate natural language for are sometimes vague; thus, the entire answer needs the given question, the QA system is often trained to be considered when it is evaluated. with seq2seq (Sutskever et al., 2014) objective simi2112 larly to other natural generation tasks such as neural machine translation. Hence,"
2021.naacl-main.170,2020.tacl-1.5,0,0.0172133,"utput example of KPQA. KPQA classifies whether each word in the answer sentences is in the answer span for a given question. We use the output probability KPW as an importance weight to be integrated into KPQA-metric. since these sentences are short summaries for the given question. Specifically, for a single-hop QA dataset such as SQuAD, we pick a single sentence that includes answer-span as the answer sentence. For the answers in a multi-hop QA dataset such as HotpotQA (Yang et al., 2018b), there are multiple supporting sentences for the single answer span. For these cases, we use SpanBERT (Joshi et al., 2020) to resolve the coreferences in the paragraphs and extract all of the supporting sentences to compose answer sentences. The {question, [SEP], answer-sentences} is then fed into the KPQA to classify the answer-span, which is a set of salient tokens, in the given answer-sentences considering the question. 3.2 KPQA Metric Since KPQA’s training process allows KPQA to find essential words in the answer sentences to a given question, we use a pre-trained KPQA to get the importance weights that are useful for evaluaterated answer. As shown in Figure 1, there exist keywords or keyphrases that are cons"
2021.naacl-main.170,Q18-1023,0,0.0408319,"Missing"
2021.naacl-main.170,D19-1051,0,0.0494586,"Missing"
2021.naacl-main.170,P19-1564,0,0.012562,"metrics for general form of GenQA. To fill this gap, we collect the human judgments of correctness for model generated answers on two other GenQA datasets, MS-MARCO and AVSD, which have longer answers than NarrativeQA and SemEval as shown in Table 1. For the MS-MARCO, we use the Natural Language Generation (NLG) subset, which has more abstractive and longer answers than the Q&A subset. GenQA Models: For each of the two datasets, we first generate answers for questions on validation sets using two trained GenQA models: UniLM (Dong et al., 2019) and MHPGM (Bauer et al., 2018) for MS-MARCO, MTN (Le et al., 2019) and AMF (Alamri et al., 2018; Hori et al., 2017) for AVSD. Details on these QA models are in Appendix. After training, we select 1k samples for each dataset in the validation set. Specifically, we first randomly pick the 500 questions in the validation set of each dataset and collect the corresponding model generated answers for each model so that we have two generated answers for each sample. Therefore, we collect a total of 1k samples, two different answers for 500 questions for each dataset. Also, we discard samples if one of two GenQA models exactly generates the ground-truth answer since"
2021.naacl-main.170,W04-1013,0,0.238546,"many steps are involved in a hypothesis test? Reference Answer : Four steps are involved in a hypothesis test. Generated Answer : There are seven steps involved in a hypothesis test . Human Judgment : 0.063 BLEU-1 : 0.778 ROUGE-L : 0.713 BLEU-1-KPQA : 0.057 ROUGE-L-KPQA : 0.127 Figure 1: An example from MS-MARCO (Bajaj et al., 2016) where widely used n-gram similarity metrics does not align with human judgments of correctness. On the other hand, our KPQA-metrics focus on the key information and give low scores to incorrect answers similar to humans. as BLEU (Papineni et al., 2002) and ROUGEL (Lin, 2004), that measure the word overlaps between the generated response and the reference answer; however, these metrics are insufficient to 1 Introduction evaluate a GenQA system (Yang et al., 2018a; Chen Question answering (QA) has received consistent et al., 2019). attention from the natural language processing comFor instance, in the example in Figure 1 from munity. Recently, research on QA systems has the MS-MARCO (Bajaj et al., 2016), the generated reached the stage of generating free-form answers, answer receives a high score on BLEU-1 (0.778) called GenQA, beyond extracting the answer to a and"
2021.naacl-main.170,D16-1230,0,0.0631099,"Missing"
2021.naacl-main.170,N06-1048,0,0.103548,"-gram metrics including BLEU were originally developed to evaluate machine translation and previous works (Liu et al., 2016; Nema and Khapra, 2018; Kryscinski et al., 2019) have shown that these metrics have poor correlations with human judgments in other language generation tasks such as dialogue systems. As with other text generation systems, for GenQA, it is difficult to assess the performance through ngram metrics. Especially, n-gram similarity metrics can give a high score to a generated answer that is incorrect but shares many unnecessary words with the reference answer. Previous works (Marton and Radul, 2006; Yang et al., 2018a; Chen et al., 2019) have pointed out the difficulty of similar problems and studied automated metrics for evaluating QA systems. Inspired by these works, we focus on studying and developing evaluation metrics for GenQA datasets that have more abstractive and diverse answers. We analyze the problem of using existing n-gram similarity metrics across multiple GenQA datasets and propose alternative metrics for GenQA. 7 Conclusion In this paper, we create high-quality human judgments on two GenQA datasets, MS-MARCO and AVSD, and show that previous evaluation metrics are poorly"
2021.naacl-main.170,D18-1429,0,0.0173087,"natural language for are sometimes vague; thus, the entire answer needs the given question, the QA system is often trained to be considered when it is evaluated. with seq2seq (Sutskever et al., 2014) objective simi2112 larly to other natural generation tasks such as neural machine translation. Hence, researchers often use n-gram based similarity metrics such as BLEU to evaluate the GenQA systems, following other natural language generation tasks. However, most of these n-gram metrics including BLEU were originally developed to evaluate machine translation and previous works (Liu et al., 2016; Nema and Khapra, 2018; Kryscinski et al., 2019) have shown that these metrics have poor correlations with human judgments in other language generation tasks such as dialogue systems. As with other text generation systems, for GenQA, it is difficult to assess the performance through ngram metrics. Especially, n-gram similarity metrics can give a high score to a generated answer that is incorrect but shares many unnecessary words with the reference answer. Previous works (Marton and Radul, 2006; Yang et al., 2018a; Chen et al., 2019) have pointed out the difficulty of similar problems and studied automated metrics f"
2021.naacl-main.170,P19-1220,0,0.0319396,"Missing"
2021.naacl-main.170,S18-1119,0,0.0617337,"Missing"
2021.naacl-main.170,W16-0106,0,0.0300988,"erence answer; however, these metrics are insufficient to 1 Introduction evaluate a GenQA system (Yang et al., 2018a; Chen Question answering (QA) has received consistent et al., 2019). attention from the natural language processing comFor instance, in the example in Figure 1 from munity. Recently, research on QA systems has the MS-MARCO (Bajaj et al., 2016), the generated reached the stage of generating free-form answers, answer receives a high score on BLEU-1 (0.778) called GenQA, beyond extracting the answer to a and ROUGE-L (0.713) due to the many overlaps given question from the context (Yin et al., 2016; of words with those in the reference. However, huSong et al., 2017; Bauer et al., 2018; Nishida et al., mans assign a low score of 0.063 on the scale from 2019; Bi et al., 2019, 2020). However, as a bot- 0 to 1 due to the mismatch of critical information. tleneck in developing GenQA models, there are As in this example, we find that existing metrics ofno proper automatic metrics to evaluate generated ten fail to capture the correctness of the generated answers (Chen et al., 2019). answer that considers the key information for the In evaluating a GenQA model, it is essential to question. cons"
2021.naacl-main.170,P02-1040,0,0.109942,"f four steps. , ... Question : How many steps are involved in a hypothesis test? Reference Answer : Four steps are involved in a hypothesis test. Generated Answer : There are seven steps involved in a hypothesis test . Human Judgment : 0.063 BLEU-1 : 0.778 ROUGE-L : 0.713 BLEU-1-KPQA : 0.057 ROUGE-L-KPQA : 0.127 Figure 1: An example from MS-MARCO (Bajaj et al., 2016) where widely used n-gram similarity metrics does not align with human judgments of correctness. On the other hand, our KPQA-metrics focus on the key information and give low scores to incorrect answers similar to humans. as BLEU (Papineni et al., 2002) and ROUGEL (Lin, 2004), that measure the word overlaps between the generated response and the reference answer; however, these metrics are insufficient to 1 Introduction evaluate a GenQA system (Yang et al., 2018a; Chen Question answering (QA) has received consistent et al., 2019). attention from the natural language processing comFor instance, in the example in Figure 1 from munity. Recently, research on QA systems has the MS-MARCO (Bajaj et al., 2016), the generated reached the stage of generating free-form answers, answer receives a high score on BLEU-1 (0.778) called GenQA, beyond extract"
2021.naacl-main.170,D16-1264,0,0.344673,".., xn ) and generated answer X x1 , shown in Figure 3, KPQA is a BERT-based (Devlin ..., x ˆm ) using pre-trained KPQA. We provide each et al., 2019) classifier that predicts salient tokens in the answer sentences depending on the ques- pair {question, generated answer} and {question, reference answer} to pre-trained KPQA and get the tion. We regard it as a multi-class classification output of the softmax layer. We define these parts task where each token is a single class. To train as KeyPhrase Weight (KPW) as shown in Figure 3. KPQA, we first prepare extractive QA datasets ˆ such as SQuAD (Rajpurkar et al., 2016), which con- We note that KPW(Q,X) = (w1 , ..., wm ) is an imporˆ for a given sist of {passage, question, answer-span}. We trans- tance weight of generated answer X form these datasets into pairs of {answer-sentences, question Q. These weights reflect the importance question, answer-span}. We extract the answer- of each token for evaluating the correctness. sentences that contain answer-span in the passage We then compute KPQA-metric by incorporat2107 ing the KPW into several existing metrics modifying the precision and recall to compute the weighted similarity. (Q,X) KP QA RBERT = BLEU-1-KPQA"
2021.naacl-main.170,D18-1259,0,0.43377,"Human Judgment : 0.063 BLEU-1 : 0.778 ROUGE-L : 0.713 BLEU-1-KPQA : 0.057 ROUGE-L-KPQA : 0.127 Figure 1: An example from MS-MARCO (Bajaj et al., 2016) where widely used n-gram similarity metrics does not align with human judgments of correctness. On the other hand, our KPQA-metrics focus on the key information and give low scores to incorrect answers similar to humans. as BLEU (Papineni et al., 2002) and ROUGEL (Lin, 2004), that measure the word overlaps between the generated response and the reference answer; however, these metrics are insufficient to 1 Introduction evaluate a GenQA system (Yang et al., 2018a; Chen Question answering (QA) has received consistent et al., 2019). attention from the natural language processing comFor instance, in the example in Figure 1 from munity. Recently, research on QA systems has the MS-MARCO (Bajaj et al., 2016), the generated reached the stage of generating free-form answers, answer receives a high score on BLEU-1 (0.778) called GenQA, beyond extracting the answer to a and ROUGE-L (0.713) due to the many overlaps given question from the context (Yin et al., 2016; of words with those in the reference. However, huSong et al., 2017; Bauer et al., 2018; Nishida e"
2021.naacl-main.274,P15-1017,0,0.0851188,"Missing"
2021.naacl-main.274,W09-3208,1,0.881814,"rly describes the event, vir- not actually happen (i.e., its modality attribute is tually all previous approaches employ features re- OTHER). Therefore, our model should be able to avoid the mistake if it utilizes additional symbolic lated to event triggers in one form or another. To features such as the modality attribute in this case. achieve better performance, many methods also There are several previous methods that use conneed to use a variety of additional symbolic featextual embeddings together with type-based or tures such as event types, attributes, and arguments (Chen et al., 2009; Chen and Ji, 2009; Zhang et al., argument-based information (Lu et al., 2020; Yu 2015; Sammons et al., 2015; Lu and Ng, 2016; et al., 2020). For example, Lu et al. (2020) proposes a new mechanism to better exploit event type inChen and Ng, 2016; Duncan et al., 2017). Previous formation for coreference resolution. Despite their neural methods (Nguyen et al., 2016; Choubey and Huang, 2017; Huang et al., 2019) also use non- impressive performance, these methods are specific to one particular type of additional information. contextual word embeddings such as word2vec 1 In this paper, we propose general and effecti"
2021.naacl-main.274,D19-1610,1,0.829458,"t straightforward way to build the final pair representation fij of mi and mj is to simply concatenate the trigger-based representation and all the feature-based representations together: (1) (2) (K) fij = [tij , hij , hij , . . . , hij ] (4) However, this approach is not always optimal. First, as the symbolic features are predicted, they can be noisy and contain errors. The performance of most symbolic feature predictors is far from perfect (Table 2). Also, depending on the specific context, some features can be more useful than others. Inspired by studies on gated modules (Lin et al., 2019; Lai et al., 2019), we propose ContextDependent Gated Module (CDGM), which uses a gating mechanism to extract information from the input symbolic features selectively (Figure 1). Given two mentions mi and mj , we use their trigger feature vector tij as the main controlling con(u) ti = ei X j=si xj ei − si + 1 text to compute the filtered representation hij : (1) (u) hij 3492 (u)  = CDGM(u) tij , hij (5) Figure 1: Overall architecture of our mention-pair encoder, which uses CDGMs to incorporate symbolic features. where u ∈ {1, 2, . . . , K}. More specifically: (u) (u)  (u)  gij = σ FFNNg tij , hij (u) (u) ("
2021.naacl-main.274,D17-1018,0,0.0441632,"ersa. Finally, after using CDGMs to distill symbolic features, the final pair representation fij of mi and mj can be computed as follows: (1) (2) (K) fij = [tij , hij , hij , . . . , hij ] (8) And the coreference score s(i, j) of mi and mj is: s(i, j) = FFNNa (fij ) where FFNNa is a mapping from R(K+1)×p → R. 2.4 Training and Inference Algorithm 1: Noise Addition for Symbolic Features Input: Document D Hyperparameters: {1 , 2 , · · · , K } for i = 1 . . . k do for u = 1 . . . K do (u) With prob. u , replace ci by (u) cˆi ∼ Uniform(Nu ) end end Training We use the same loss function as in (Lee et al., 2017). Also, notice that the training accuracy of a feature predictor is typically much higher than its accuracy on the dev/test set (Table 2). If we simply train our model without any regularization, our CDGMs will rarely come across noisy symbolic features during training. Therefore, to encourage our CDGMs to actually learn to distill reliable signals, we also propose a simple but effective noisy training method. Before passing a training data batch to the model, we randomly add noise to the predicted features. More specifically, for each document D in the batch, we go through every symbolic feat"
2021.naacl-main.274,2020.acl-main.713,1,0.626665,"d indices of its trigger by si and ei respectively. We assume the mentions are ordered based on si (i.e., If i ≤ j then si ≤ sj ). We also assume each mi has K (predicted) cat(1) (2) (K) egorical features {ci , ci , . . . , ci }, with each (u) ci ∈ {1, 2, . . . , Nu } taking one of Nu different discrete values. Table 2 lists the symbolic features we consider in this work. The definitions of the features and their possible values are in ACE and Rich ERE guidelines (LDC, 2005; Mitamura et al., 2016). The accuracy scores of the symbolic feature predictors are also shown in Table 2. We use OneIE (Lin et al., 2020) to identify event mentions along with their subtypes. For other symbolic features, we train a joint classification model based on SpanBERT. The appendix contains more details. 2.2 Dataset Single-Mention Encoder Given a document D, our model first forms a contextualized representation for each input token using a Transformer encoder (Joshi et al., 2020). Let X = (x1 , ..., xn ) be the output of the encoder, where xi ∈ Rd . Then, for each mention mi , its trigger’s representation ti is defined as the average of its token embeddings: where FFNNt is a feedforward network mapping from R3×d → Rp ,"
2021.naacl-main.274,P19-1016,1,0.846259,"Rp . Now, the most straightforward way to build the final pair representation fij of mi and mj is to simply concatenate the trigger-based representation and all the feature-based representations together: (1) (2) (K) fij = [tij , hij , hij , . . . , hij ] (4) However, this approach is not always optimal. First, as the symbolic features are predicted, they can be noisy and contain errors. The performance of most symbolic feature predictors is far from perfect (Table 2). Also, depending on the specific context, some features can be more useful than others. Inspired by studies on gated modules (Lin et al., 2019; Lai et al., 2019), we propose ContextDependent Gated Module (CDGM), which uses a gating mechanism to extract information from the input symbolic features selectively (Figure 1). Given two mentions mi and mj , we use their trigger feature vector tij as the main controlling con(u) ti = ei X j=si xj ei − si + 1 text to compute the filtered representation hij : (1) (u) hij 3492 (u)  = CDGM(u) tij , hij (5) Figure 1: Overall architecture of our mention-pair encoder, which uses CDGMs to incorporate symbolic features. where u ∈ {1, 2, . . . , K}. More specifically: (u) (u)  (u)  gij = σ FFNNg"
2021.naacl-main.274,L16-1631,0,0.207491,"ches employ features re- OTHER). Therefore, our model should be able to avoid the mistake if it utilizes additional symbolic lated to event triggers in one form or another. To features such as the modality attribute in this case. achieve better performance, many methods also There are several previous methods that use conneed to use a variety of additional symbolic featextual embeddings together with type-based or tures such as event types, attributes, and arguments (Chen et al., 2009; Chen and Ji, 2009; Zhang et al., argument-based information (Lu et al., 2020; Yu 2015; Sammons et al., 2015; Lu and Ng, 2016; et al., 2020). For example, Lu et al. (2020) proposes a new mechanism to better exploit event type inChen and Ng, 2016; Duncan et al., 2017). Previous formation for coreference resolution. Despite their neural methods (Nguyen et al., 2016; Choubey and Huang, 2017; Huang et al., 2019) also use non- impressive performance, these methods are specific to one particular type of additional information. contextual word embeddings such as word2vec 1 In this paper, we propose general and effective The code is publicly available at https://github.com/ laituan245/eventcoref. methods for incorporating a"
2021.naacl-main.274,P17-1009,0,0.159707,"Missing"
2021.naacl-main.274,D16-1038,0,0.128683,"Missing"
2021.naacl-main.274,D14-1162,0,0.0903407,"bservations, we propose a novel context-dependent gated module to adaptively control the information flows from the input symbolic features. Combined with a simple noisy training method, our best models achieve state-of-the-art results on two datasets: ACE 2005 and KBP 2016.1 ... we are seeing these soldiers {head out}ev1 ... ... these soldiers were set to {leave}ev2 in January ... ev1 (Movement:Transport): Modality = ASSERTED ev2 (Movement:Transport): Modality = OTHER Table 1: An example of using the modality attribute to improve event coreference resolution. (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). With the recent remarkable success of language models such as BERT (Devlin et al., 2019) and SpanBERT (Joshi et al., 2020), one natural question is whether we can simply use these models for coreference resolution without relying on any additional features. We argue that it is still highly beneficial to utilize symbolic features, especially when they are clean and have complementary information. Table 1 shows an example in the ACE 2005 dataset, where our baseline SpanBERT model 1 Introduction incorrectly predicts the highlighted event mentions to be coreferential. The event triggers are sema"
2021.naacl-main.283,2020.acl-main.421,0,0.0358265,". (Tjong Kim Sang, 2002; Tjong Kim Sang and ∗ De Meulder, 2003). On the other hand, crossWork was started while the first author was a research intern at Adobe. lingual Natural Language Understanding (NLU) 3617 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3617–3632 June 6–11, 2021. ©2021 Association for Computational Linguistics tasks have gained less attention, with smaller benchmark datasets that cover a handful of languages and don’t truly model linguistic variety (Conneau et al., 2018; Artetxe et al., 2020). Natural Language Understanding tasks are critical for dialog systems, as they make up an integral part of the dialog pipeline. Understanding and improving the mechanism behind cross-lingual transfer for natural language understanding in dialog systems require evaluations on more challenging and typologically diverse benchmarks. Numerous approaches have attempted to build stronger cross-lingual representations on top of those multilingual models; however, most require parallel corpora (Wang et al., 2019; Lample and Conneau, 2019) and are biased towards highresource and balanced setups. This f"
2021.naacl-main.283,2020.repl4nlp-1.1,0,0.0435506,"learning is a technique used et al., 2019). to adapt a model trained on a downstream task in a The generalization of such representations has source language to directly generalize to the task in been extensively evaluated on traditional tasks such new languages. It aims to come up with common as Part-of-Speech (POS) tagging, Named Entity cross-lingual representations and leverages them to Recognition (NER) and Cross-lingual Document bridge the divide between resources to make any Classification (CLDC) (Ahmad et al., 2019; Wu NLP application scale to multiple languages. This and Dredze, 2019; Bari et al., 2020a; Schwenk is particularly useful for data-scarce scenarios, as it and Li, 2018), with ever-growing open commureduces the need for API calls implied by machine nity annotation efforts like Universal Dependentranslation or costly task-specific annotation for cies (Nivre et al., 2020) and CoNLL shared tasks new languages. (Tjong Kim Sang, 2002; Tjong Kim Sang and ∗ De Meulder, 2003). On the other hand, crossWork was started while the first author was a research intern at Adobe. lingual Natural Language Understanding (NLU) 3617 Proceedings of the 2021 Conference of the North American Chapter of t"
2021.naacl-main.283,D18-1398,0,0.115069,"oss-lingual representations on top of those multilingual models; however, most require parallel corpora (Wang et al., 2019; Lample and Conneau, 2019) and are biased towards highresource and balanced setups. This fuels the need for a method that doesn’t require explicit crosslingual alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019). Recently, Nooralahzadeh et al. (2020) apply the MAML (Finn et al., 2017) algorithm to crosslingual transfer learning for XNLI (Conneau et al., 2018) and MLQA (Lewis et al., 2020), NLU tasks that are naturally biased towards machine translation-based solutions. Nooralahzadeh et al. are able to show improvement over strong multilingual models, including M-BERT. However, they mainly show the effects of meta-learning as a first step in a framework that relies on supervised finetuning, making i"
2021.naacl-main.283,D19-1252,0,0.0471437,"Missing"
2021.naacl-main.283,2020.clssts-1.5,0,0.161057,"are biased towards highresource and balanced setups. This fuels the need for a method that doesn’t require explicit crosslingual alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019). Recently, Nooralahzadeh et al. (2020) apply the MAML (Finn et al., 2017) algorithm to crosslingual transfer learning for XNLI (Conneau et al., 2018) and MLQA (Lewis et al., 2020), NLU tasks that are naturally biased towards machine translation-based solutions. Nooralahzadeh et al. are able to show improvement over strong multilingual models, including M-BERT. However, they mainly show the effects of meta-learning as a first step in a framework that relies on supervised finetuning, making it difficult to properly compare and contrast both approaches. sification task, MTOD is a joint classification and sequence labelling task and i"
2021.naacl-main.283,2020.acl-main.653,0,0.0411571,"l alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019). Recently, Nooralahzadeh et al. (2020) apply the MAML (Finn et al., 2017) algorithm to crosslingual transfer learning for XNLI (Conneau et al., 2018) and MLQA (Lewis et al., 2020), NLU tasks that are naturally biased towards machine translation-based solutions. Nooralahzadeh et al. are able to show improvement over strong multilingual models, including M-BERT. However, they mainly show the effects of meta-learning as a first step in a framework that relies on supervised finetuning, making it difficult to properly compare and contrast both approaches. sification task, MTOD is a joint classification and sequence labelling task and is more typologically diverse. TyDiQA is not a classification task, but we show how meta-learning can be applied usefully to it. We also show"
2021.naacl-main.283,2020.emnlp-main.484,0,0.0460222,"Missing"
2021.naacl-main.283,D19-1129,0,0.0634266,"Missing"
2021.naacl-main.283,K19-1061,1,0.840535,"Missing"
2021.naacl-main.283,D19-1575,0,0.0235775,"a handful of languages and don’t truly model linguistic variety (Conneau et al., 2018; Artetxe et al., 2020). Natural Language Understanding tasks are critical for dialog systems, as they make up an integral part of the dialog pipeline. Understanding and improving the mechanism behind cross-lingual transfer for natural language understanding in dialog systems require evaluations on more challenging and typologically diverse benchmarks. Numerous approaches have attempted to build stronger cross-lingual representations on top of those multilingual models; however, most require parallel corpora (Wang et al., 2019; Lample and Conneau, 2019) and are biased towards highresource and balanced setups. This fuels the need for a method that doesn’t require explicit crosslingual alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019)."
2021.naacl-main.283,2020.acl-main.348,0,0.292112,"are biased towards highresource and balanced setups. This fuels the need for a method that doesn’t require explicit crosslingual alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019). Recently, Nooralahzadeh et al. (2020) apply the MAML (Finn et al., 2017) algorithm to crosslingual transfer learning for XNLI (Conneau et al., 2018) and MLQA (Lewis et al., 2020), NLU tasks that are naturally biased towards machine translation-based solutions. Nooralahzadeh et al. are able to show improvement over strong multilingual models, including M-BERT. However, they mainly show the effects of meta-learning as a first step in a framework that relies on supervised finetuning, making it difficult to properly compare and contrast both approaches. sification task, MTOD is a joint classification and sequence labelling task and i"
2021.naacl-main.283,D19-1077,0,0.046422,"Missing"
2021.naacl-srw.9,D18-2029,0,0.0202166,"on retriever but use Google USE for the passage retriever and the individual question retriever. We record the answer ai associated to the top-ranked question set qi as Answer 3. The pipeline in Figure 2 that goes from Input Corpus to Candidate Answers, QA Space, {Q}A Space and finally Answer 3 is a valid reader-retriever workflow. We denote this workflow as Reader-Retriever-{Q}A-Space. 3.2.2 Passage Retriever and QA Reader Given a query, the passage retriever uses the dot product of the query embedding and passage embedding vectors generated by Google Universal Sentence Encoder (Google USE) (Cer et al., 2018) to retrieve from the corpus a passage that is semantically most similar to the query. We then use BERT (Devlin et al., 2019), fine-tuned on SQuAD, to read the retrieved passage, predict the answer, and record the predicted answer as Answer 1. The pipeline in Figure 2 that goes from Input Corpus 3.2.5 Answer Aggregator Now that we have Answer 1, {Answer 2}, and Answer 3, the last step is to aggregate them into one single answer to return to the user. Our answer aggregation works as follows: if Answer 1 appears in the set {Answer 2}, then accept Answer 1 and return it; otherwise reject Answer 1"
2021.naacl-srw.9,P17-1171,0,0.130857,"One family, namely retriever-readers (Fig. 1, left branch), first retrieves from the corpus some documents or paragraphs that are likely to be relevant to the question, and then uses neural networks to read the retrieved passages and locate the answer. Another line of work, namely question answering using knowledge bases (abbreviated as QA using KB in this paper; Fig. 1, middle branch), first constructs a knowledge 2 2.1 Related Work Retriever-Readers Retriever-readers solve OpenQA by converting it to easier single-passage QA tasks. Examples of popular algorithms in this family include DrQA (Chen et al., 2017), which has a TF-IDF retriever followed by a recurrent neural network reader, and BERTserini (Yang et al., 2019), which consists of a BM25 retriever and a BERT reader. All retriever-readers face a trade-off between efficiency and accuracy. When the retriever module is 61 Proceedings of NAACL-HLT 2021: Student Research Workshop, pages 61–67 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 1: Retriever-readers (left), QA using KB (middle), and reader-retrievers (right). computationally efficient, the retrieved results are not very reliable, and the performance of the subse"
2021.naacl-srw.9,D16-1264,0,0.0347826,"agree at all. We denote the complete workflow depicted in Figure 2 as R6 . 4 greatest extent, so that we can make sure we correctly reproduce others’ work and do not put their models into disadvantages when comparing them with ours. More experimental details are available in Section 4.2. Although not critical to this study, using different datasets for training and testing has one additional benefit that it shows the ability of the systems to adapt to new corpora. Experiments We evaluate the OpenQA performance of our proposed method R6 and baseline methods using two public QA datasets, SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017). We adopt a rather challenging setting that all trainable components of the models are trained on SQuAD, while the final models are tested on TriviaQA. Furthermore, we use TriviaQA in an open-domain setting by removing all annotated associations between questions and documents and enforcing the systems to answer every question with the entire corpus. We write TriviaQA-Open to distinguish such an opendomain setting from those officially adopted by TriviaQA. One may wonder why we choose to use different datasets for training and testing. Because our goal of the"
2021.naacl-srw.9,N19-1423,0,0.0156048,"ssociated to the top-ranked question set qi as Answer 3. The pipeline in Figure 2 that goes from Input Corpus to Candidate Answers, QA Space, {Q}A Space and finally Answer 3 is a valid reader-retriever workflow. We denote this workflow as Reader-Retriever-{Q}A-Space. 3.2.2 Passage Retriever and QA Reader Given a query, the passage retriever uses the dot product of the query embedding and passage embedding vectors generated by Google Universal Sentence Encoder (Google USE) (Cer et al., 2018) to retrieve from the corpus a passage that is semantically most similar to the query. We then use BERT (Devlin et al., 2019), fine-tuned on SQuAD, to read the retrieved passage, predict the answer, and record the predicted answer as Answer 1. The pipeline in Figure 2 that goes from Input Corpus 3.2.5 Answer Aggregator Now that we have Answer 1, {Answer 2}, and Answer 3, the last step is to aggregate them into one single answer to return to the user. Our answer aggregation works as follows: if Answer 1 appears in the set {Answer 2}, then accept Answer 1 and return it; otherwise reject Answer 1 and return Answer 3. In other words, the answer aggregator checks the consistency between the retrieverreader results and th"
2021.naacl-srw.9,N19-1237,0,0.0141348,"gator Given a corpus, a named entity recognition (NER) tool called TAGME (Ferragina and Scaiella, 2010, 2012) is applied to detect named entities from the corpus and link the entities to Wikipedia titles. Those entities form the set of candidate answers A in Definition 1. Then a question-generating (QG) reader is applied to the set of candidate answers to generate a question for each answer based on the local context. This reader features an encoderdecoder model structure with a question-answering reward and a question fluency reward tuned with policy gradient optimization (Yuan et al., 2017; Hosking and Riedel, 2019). Then we use a question aggregator to build the {Q}A Space by putting together all the questions with the same answer entity. Given a query, the aggregated question retriever uses the BM25 score (Robertson and Zaragoza, 2009) to retrieve from the {Q}A space the answer whose associated set of questions is most similar to the given query. We query the {Q}A Space by treating each qi as a single document which contains qi,j for all j as sentences. In practice, we observe that BM25 works better for long documents and Google USE works better for short passages. That is why we use BM25 as the aggreg"
2021.naacl-srw.9,N19-4013,0,0.0314758,"Missing"
2021.naacl-srw.9,N18-4017,0,0.012718,"lowed by a recurrent neural network reader, and BERTserini (Yang et al., 2019), which consists of a BM25 retriever and a BERT reader. All retriever-readers face a trade-off between efficiency and accuracy. When the retriever module is 61 Proceedings of NAACL-HLT 2021: Student Research Workshop, pages 61–67 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 1: Retriever-readers (left), QA using KB (middle), and reader-retrievers (right). computationally efficient, the retrieved results are not very reliable, and the performance of the subsequent reader is also constrained (Htut et al., 2018). On the other hand, there exist systems such as R3 (Wang et al., 2018) and DS-QA (Lin et al., 2018) that have sophisticated retrievers jointly trained with the readers, but they are computationally expensive and thus not scalable to large corpora (Das et al., 2019). the KB construction step to the graph query step, and how to handle questions whose answers do not fall within the KB schema. Due to those complexities, the community is observing a recent trend that retriever-readers are dominating the leaderboards of public QA datasets but KB-based methods are not. Therefore, we choose to focus"
2021.naacl-srw.9,W17-2603,0,0.0211891,"and Question Aggregator Given a corpus, a named entity recognition (NER) tool called TAGME (Ferragina and Scaiella, 2010, 2012) is applied to detect named entities from the corpus and link the entities to Wikipedia titles. Those entities form the set of candidate answers A in Definition 1. Then a question-generating (QG) reader is applied to the set of candidate answers to generate a question for each answer based on the local context. This reader features an encoderdecoder model structure with a question-answering reward and a question fluency reward tuned with policy gradient optimization (Yuan et al., 2017; Hosking and Riedel, 2019). Then we use a question aggregator to build the {Q}A Space by putting together all the questions with the same answer entity. Given a query, the aggregated question retriever uses the BM25 score (Robertson and Zaragoza, 2009) to retrieve from the {Q}A space the answer whose associated set of questions is most similar to the given query. We query the {Q}A Space by treating each qi as a single document which contains qi,j for all j as sentences. In practice, we observe that BM25 works better for long documents and Google USE works better for short passages. That is wh"
2021.naacl-srw.9,P17-1147,0,0.0260436,"orkflow depicted in Figure 2 as R6 . 4 greatest extent, so that we can make sure we correctly reproduce others’ work and do not put their models into disadvantages when comparing them with ours. More experimental details are available in Section 4.2. Although not critical to this study, using different datasets for training and testing has one additional benefit that it shows the ability of the systems to adapt to new corpora. Experiments We evaluate the OpenQA performance of our proposed method R6 and baseline methods using two public QA datasets, SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017). We adopt a rather challenging setting that all trainable components of the models are trained on SQuAD, while the final models are tested on TriviaQA. Furthermore, we use TriviaQA in an open-domain setting by removing all annotated associations between questions and documents and enforcing the systems to answer every question with the entire corpus. We write TriviaQA-Open to distinguish such an opendomain setting from those officially adopted by TriviaQA. One may wonder why we choose to use different datasets for training and testing. Because our goal of the experiments is to compare the eff"
C18-1181,D15-1075,0,0.0167856,"ing semantic similarity (Mueller and Thyagarajan, 2016), paraphrase identification (Hu et al., 2014), and natural language inference (Conneau et al., 2017). Figure 2 shows the general architecture of a Siamese model. The vector representations of the input sentences are built separately by the encoder. Two input sentences have no influence on the computation of each other’s representation. After that, the encoded vectors are compared using measures such as cosine similarity (Feng et al., 2015; Yang et al., 2015), element-wise operations (Tai et al., 2015), or neural network-based combination (Bowman et al., 2015). An advantage of this architecture is that applying the same encoder to each input sentence makes the model smaller. In addition, the sentence vectors can be used for visualization, sentence clustering and many other purposes (Wang et al., 2016a). One of the first attempts at applying deep learning to answer selection was the bag-of-words model proposed by (Yu et al., 2014). The model generates the vector representation of a sentence by simply taking the average of all the word vectors in the sentence - having previously removed all the stop words. Integrating additional overlapping word coun"
C18-1181,D17-1070,0,0.0244326,"tentive architectures. Even though the boundaries are not crystal clear, separating the existing different neural architectures into the three categories can provide the big picture more easily. 2135 Figure 2: The general architecture of a Siamese model. The same encoder is used to generate the vector representations for the input sentences. 2.2.1 Siamese Architecture Siamese neural networks have been proposed for a number of sentence pair modeling tasks, including semantic similarity (Mueller and Thyagarajan, 2016), paraphrase identification (Hu et al., 2014), and natural language inference (Conneau et al., 2017). Figure 2 shows the general architecture of a Siamese model. The vector representations of the input sentences are built separately by the encoder. Two input sentences have no influence on the computation of each other’s representation. After that, the encoded vectors are compared using measures such as cosine similarity (Feng et al., 2015; Yang et al., 2015), element-wise operations (Tai et al., 2015), or neural network-based combination (Bowman et al., 2015). An advantage of this architecture is that applying the same encoder to each input sentence makes the model smaller. In addition, the"
C18-1181,N16-1108,0,0.0646547,"ean TrecQA) - TRAIN-ALL unigram+count (Yu et al., 2014) TRAIN-ALL bigram+count (Yu et al., 2014) QA-LSTM (Tan et al., 2015) QA-LSTM with attention (Tan et al., 2015) QA-LSTM/CNN (Tan et al., 2015) Attentive Pooling CNN (dos Santos et al., 2016) (Severyn and Moschitti, 2015) L.D.C Model (Wang et al., 2016b) Pointwise Pointwise Siamese 0.711 - Pairwise Pairwise Siamese Attentive - 0.682 0.690 Pairwise Pairwise Siamese Attentive - 0.706 0.753 Pointwise Pointwise 0.746 - 0.771 0.758 - Pointwise Pairwise Siamese CompareAggregate CompareAggregate Siamese Siamese Pairwise Word Interaction Modelling (He and Lin, 2016) Multi-Perspective CNN (He et al., 2015) HyperQA (Hyperbolic Embeddings) (Tay et al., 2018a) PairwiseRank+Multi-Perspective CNN (Rao et al., 2016) BiMPM (Shen et al., 2017) Pointwise 0.762 0.770 0.777 0.784 Pairwise Siamese 0.780 0.801 Pointwise CompareAggregate CompareAggregate CompareAggregate CompareAggregate CompareAggregate - 0.802 Dynamic-Clip Attention (Bian et al., 2017) IWAN (Shen et al., 2017) Listwise - 0.821 - 0.822 IWAN+CARNN (Tran et al., 2018) Pointwise - 0.829 MCAN (Tay et al., 2018b) Pointwise - 0.838 Pointwise Table 1: Overview of existing deep learning methods to answer sele"
C18-1181,D15-1181,0,0.304897,"et al., 2014) TRAIN-ALL bigram+count (Yu et al., 2014) QA-LSTM (Tan et al., 2015) QA-LSTM with attention (Tan et al., 2015) QA-LSTM/CNN (Tan et al., 2015) Attentive Pooling CNN (dos Santos et al., 2016) (Severyn and Moschitti, 2015) L.D.C Model (Wang et al., 2016b) Pointwise Pointwise Siamese 0.711 - Pairwise Pairwise Siamese Attentive - 0.682 0.690 Pairwise Pairwise Siamese Attentive - 0.706 0.753 Pointwise Pointwise 0.746 - 0.771 0.758 - Pointwise Pairwise Siamese CompareAggregate CompareAggregate Siamese Siamese Pairwise Word Interaction Modelling (He and Lin, 2016) Multi-Perspective CNN (He et al., 2015) HyperQA (Hyperbolic Embeddings) (Tay et al., 2018a) PairwiseRank+Multi-Perspective CNN (Rao et al., 2016) BiMPM (Shen et al., 2017) Pointwise 0.762 0.770 0.777 0.784 Pairwise Siamese 0.780 0.801 Pointwise CompareAggregate CompareAggregate CompareAggregate CompareAggregate CompareAggregate - 0.802 Dynamic-Clip Attention (Bian et al., 2017) IWAN (Shen et al., 2017) Listwise - 0.821 - 0.822 IWAN+CARNN (Tran et al., 2018) Pointwise - 0.829 MCAN (Tay et al., 2018b) Pointwise - 0.838 Pointwise Table 1: Overview of existing deep learning methods to answer selection whole candidate sentence space. An"
C18-1181,N10-1145,0,0.0586564,"iera et al., 2017). Figure 1 depicts a typical question answering pipeline. In this setup, answer selection can be applied to identify the sentences that are most relevant to the question within the retrieved documents. Besides its application in open domain question answering, the techniques developed for answer selection can be potentially used to predict answer quality in community question answering (cQA) sites (Nakov et al., 2015). Previous work on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, many deep learning based methods have been proposed for the task (Bian et al., 2017; Shen et al., 2017; Tran et al., 2018). They outperform traditional techniques. In addition, they do not need any feature-engineering effort or hand-coded resources beyond some large unlabeled corpus on which to learn the initial word embeddings, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http"
C18-1181,P14-1062,0,0.0360067,"15) proposed the QA-LSTM model that employs a bidirectional long short-term memory (biLSTM) network (Hochreiter and Schmidhuber, 1997) and a pooling layer to construct distributed vector representations of the input sentences independently. Then the model utilizes cosine similarity to measure the distance of the sentence representations. Severyn and Moschitti (2015) proposed a model that employs a convolutional neural network (CNN) to generate the representations of the input sentences. The CNN is based on an architecture that has previously been applied to many sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014). In (He et al., 2015), each input sentence is modeled using a CNN that extracts features at multiple levels of granularity and uses multiple types of pooling. The representations of the input sentences are then compared at several granularities using multiple similarity metrics. Finally, the comparison results are fed into a fully connected layer to obtain the final relevance score. The proposed model outperforms many other Siamese models. Tay et al. (2018a) proposed a simple but novel deep learning architecture that models the relationship between a question and a candidate sente"
C18-1181,D14-1181,0,0.00553191,"del that employs a bidirectional long short-term memory (biLSTM) network (Hochreiter and Schmidhuber, 1997) and a pooling layer to construct distributed vector representations of the input sentences independently. Then the model utilizes cosine similarity to measure the distance of the sentence representations. Severyn and Moschitti (2015) proposed a model that employs a convolutional neural network (CNN) to generate the representations of the input sentences. The CNN is based on an architecture that has previously been applied to many sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014). In (He et al., 2015), each input sentence is modeled using a CNN that extracts features at multiple levels of granularity and uses multiple types of pooling. The representations of the input sentences are then compared at several granularities using multiple similarity metrics. Finally, the comparison results are fed into a fully connected layer to obtain the final relevance score. The proposed model outperforms many other Siamese models. Tay et al. (2018a) proposed a simple but novel deep learning architecture that models the relationship between a question and a candidate sentence in Hyper"
C18-1181,W18-3105,1,0.871041,"Missing"
C18-1181,D15-1166,0,0.0275785,"iamese Architecture. In a Siamese architecture (Bromley et al., 1993), the same encoder (e.g., a CNN or a RNN) is used to build the vector representations for the input sentences (i.e., the candidate answer and the question) individually. After that, the relevance score is determined solely based on the encoded vectors. There is no explicit interaction between the input sentences during the encoding process. • Attentive Architecture. Instead of generating representations for the candidate answer and the question independently, attention mechanisms (Bahdanau et al., 2014; Hermann et al., 2015; Luong et al., 2015) can be used to allow the information from an input sentence to influence the computation of the other’s representation (Tan et al., 2015; dos Santos et al., 2016). Even though the weakness of the Siamese models is alleviated, the interaction between the input sentences during the encoding process is still minimal in most Attentive architectures. • Compare-Aggregate Architecture. The Compare-Aggregate architectures can capture more interactive features between input sentences than the Siamese architectures and the Attentive architectures, therefore typically have better performance when evalua"
C18-1181,P17-2081,0,0.100225,"e, and listwise) (ii) neural network architectures (Siamese architecture, Attentive architecture, and Compare-Aggregate architecture). In addition, we examine the most popular datasets and the evaluation metrics for answer selection. Below we discuss several promising future research directions. Transfer learning (Pan and Yang, 2010) has achieved success in domains such as speech recognition (Huang et al., 2013), computer vision (Razavian et al., 2014), and natural language processing (Zhang et al., 2017). Its applicability to question answering and answer selection has recently been studied (Min et al., 2017; Chung et al., 2017). Min et al. (2017) created SQuAD-T, a modification of the original large-scale SQuAD dataset (Rajpurkar et al., 2016) to allow for directly training and evaluating answer selection systems. Through a basic transfer learning technique from SQuAD-T, the state-of-the-art result in the WikiQA dataset can be improved. This demonstrates the potential of developing novel transfer learning techniques for the answer selection task. Many deep learning methods for answer selection are applicable to other sentence pair modeling tasks such as natural language inference (He and Lin, 20"
C18-1181,D14-1162,0,0.0805594,"k on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, many deep learning based methods have been proposed for the task (Bian et al., 2017; Shen et al., 2017; Tran et al., 2018). They outperform traditional techniques. In addition, they do not need any feature-engineering effort or hand-coded resources beyond some large unlabeled corpus on which to learn the initial word embeddings, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2132 Proceedings of the 27th International Conference on Computational Linguistics, pages 2132–2144 Santa Fe, New Mexico, USA, August 20-26, 2018. Figure 1: A typical question answering pipeline architecture, adapted from (Sequiera et al., 2017) While previous work has recognized the increasing use of deep learning techniques in natural language processing (Young et al., 2017), no systematic survey of deep learning methods for answer selection to"
C18-1181,D17-1122,0,0.0688256,"most relevant to the question within the retrieved documents. Besides its application in open domain question answering, the techniques developed for answer selection can be potentially used to predict answer quality in community question answering (cQA) sites (Nakov et al., 2015). Previous work on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, many deep learning based methods have been proposed for the task (Bian et al., 2017; Shen et al., 2017; Tran et al., 2018). They outperform traditional techniques. In addition, they do not need any feature-engineering effort or hand-coded resources beyond some large unlabeled corpus on which to learn the initial word embeddings, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2132 Proceedings of the 27th International Conference on Computational Linguistics, pages 2132–2144 Santa Fe, New Mexico, USA, August 20-26, 2018."
C18-1181,P15-1150,0,0.0284415,"sed for a number of sentence pair modeling tasks, including semantic similarity (Mueller and Thyagarajan, 2016), paraphrase identification (Hu et al., 2014), and natural language inference (Conneau et al., 2017). Figure 2 shows the general architecture of a Siamese model. The vector representations of the input sentences are built separately by the encoder. Two input sentences have no influence on the computation of each other’s representation. After that, the encoded vectors are compared using measures such as cosine similarity (Feng et al., 2015; Yang et al., 2015), element-wise operations (Tai et al., 2015), or neural network-based combination (Bowman et al., 2015). An advantage of this architecture is that applying the same encoder to each input sentence makes the model smaller. In addition, the sentence vectors can be used for visualization, sentence clustering and many other purposes (Wang et al., 2016a). One of the first attempts at applying deep learning to answer selection was the bag-of-words model proposed by (Yu et al., 2014). The model generates the vector representation of a sentence by simply taking the average of all the word vectors in the sentence - having previously removed all t"
C18-1181,N18-1115,1,0.531494,"he question within the retrieved documents. Besides its application in open domain question answering, the techniques developed for answer selection can be potentially used to predict answer quality in community question answering (cQA) sites (Nakov et al., 2015). Previous work on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, many deep learning based methods have been proposed for the task (Bian et al., 2017; Shen et al., 2017; Tran et al., 2018). They outperform traditional techniques. In addition, they do not need any feature-engineering effort or hand-coded resources beyond some large unlabeled corpus on which to learn the initial word embeddings, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2132 Proceedings of the 27th International Conference on Computational Linguistics, pages 2132–2144 Santa Fe, New Mexico, USA, August 20-26, 2018. Figure 1: A typical"
C18-1181,C10-1131,0,0.0475597,"06; Ferrucci, 2012; Sequiera et al., 2017). Figure 1 depicts a typical question answering pipeline. In this setup, answer selection can be applied to identify the sentences that are most relevant to the question within the retrieved documents. Besides its application in open domain question answering, the techniques developed for answer selection can be potentially used to predict answer quality in community question answering (cQA) sites (Nakov et al., 2015). Previous work on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, many deep learning based methods have been proposed for the task (Bian et al., 2017; Shen et al., 2017; Tran et al., 2018). They outperform traditional techniques. In addition, they do not need any feature-engineering effort or hand-coded resources beyond some large unlabeled corpus on which to learn the initial word embeddings, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4"
C18-1181,D07-1003,0,0.390006,"uestion (Prager, 2006; Ferrucci, 2012; Sequiera et al., 2017). Figure 1 depicts a typical question answering pipeline. In this setup, answer selection can be applied to identify the sentences that are most relevant to the question within the retrieved documents. Besides its application in open domain question answering, the techniques developed for answer selection can be potentially used to predict answer quality in community question answering (cQA) sites (Nakov et al., 2015). Previous work on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, many deep learning based methods have been proposed for the task (Bian et al., 2017; Shen et al., 2017; Tran et al., 2018). They outperform traditional techniques. In addition, they do not need any feature-engineering effort or hand-coded resources beyond some large unlabeled corpus on which to learn the initial word embeddings, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativec"
C18-1181,K16-1004,0,0.360352,"lue indicating whether cij contains the correct answer to qi . It is enough to train a binary classifier: hθ (qi , cij ) → yˆij , where 0 ≤ yˆij ≤ 1. For example, in (Yu et al., 2014), the training objective is to minimize the cross entropy of all labelled questioncandidate pairs in the training set. During inference, given a question, the trained classifier hθ is used to rank every candidate sentence, and the top-ranked candidate is selected (i.e., argmaxcij hθ (qi , cij ) should be selected as the answer to qi ). Many work adopted this approach (Yu et al., 2014; Severyn and Moschitti, 2015; Wang et al., 2016b; Shen et al., 2017). The second approach to ranking is the pairwise approach, where the ranking function hθ is explicitly trained to score correct candidate sentences higher than incorrect sentences. Given a question, the approach takes a pair of candidate answer sentences and explicitly learns to predict which sentence is more − relevant to the question. For example, in (Feng et al., 2015), the training instances are triples (qi , c+ i , ci ), + − where qi is a question, ci is a correct sentence for qi , and ci is an incorrect sentence sampled from the 2133 Method Learning Approach Model Ar"
C18-1181,C16-1127,0,0.153924,"lue indicating whether cij contains the correct answer to qi . It is enough to train a binary classifier: hθ (qi , cij ) → yˆij , where 0 ≤ yˆij ≤ 1. For example, in (Yu et al., 2014), the training objective is to minimize the cross entropy of all labelled questioncandidate pairs in the training set. During inference, given a question, the trained classifier hθ is used to rank every candidate sentence, and the top-ranked candidate is selected (i.e., argmaxcij hθ (qi , cij ) should be selected as the answer to qi ). Many work adopted this approach (Yu et al., 2014; Severyn and Moschitti, 2015; Wang et al., 2016b; Shen et al., 2017). The second approach to ranking is the pairwise approach, where the ranking function hθ is explicitly trained to score correct candidate sentences higher than incorrect sentences. Given a question, the approach takes a pair of candidate answer sentences and explicitly learns to predict which sentence is more − relevant to the question. For example, in (Feng et al., 2015), the training instances are triples (qi , c+ i , ci ), + − where qi is a question, ci is a correct sentence for qi , and ci is an incorrect sentence sampled from the 2133 Method Learning Approach Model Ar"
C18-1181,D15-1237,0,0.653065,"n et al., 2017) adopted the pointwise approach, this approach is not close to the nature of ranking. The pairwise approach and the listwise approach exploit more information about the ground truth ordering of candidate sentences. (Rao et al., 2016) proposed a pairwise ranking approach that can directly exploit existing pointwise neural network models as base components. The approach outperforms many competitive pointwise baselines. (Bian et al., 2017) showed that the listwise approach performs better than the pointwise approach on public datasets such as TrecQA (Wang et al., 2007) and WikiQA (Yang et al., 2015). In the next section, we describe various neural network architectures for modeling the ranking function hθ , which takes a question-candidate pair and returns a score indicating whether the candidate is relevant to the question. 2.2 Neural Network Architectures There are three main types of general architectures for measuring the relevance of a candidate sentence to a question. • Siamese Architecture. In a Siamese architecture (Bromley et al., 1993), the same encoder (e.g., a CNN or a RNN) is used to build the vector representations for the input sentences (i.e., the candidate answer and the"
C18-1181,N13-1106,0,0.0765149,"Missing"
C18-1181,Q17-1036,0,0.0412026,"deep learning methods for answer selection along two dimensions: (i) learning approaches (pointwise, pairwise, and listwise) (ii) neural network architectures (Siamese architecture, Attentive architecture, and Compare-Aggregate architecture). In addition, we examine the most popular datasets and the evaluation metrics for answer selection. Below we discuss several promising future research directions. Transfer learning (Pan and Yang, 2010) has achieved success in domains such as speech recognition (Huang et al., 2013), computer vision (Razavian et al., 2014), and natural language processing (Zhang et al., 2017). Its applicability to question answering and answer selection has recently been studied (Min et al., 2017; Chung et al., 2017). Min et al. (2017) created SQuAD-T, a modification of the original large-scale SQuAD dataset (Rajpurkar et al., 2016) to allow for directly training and evaluating answer selection systems. Through a basic transfer learning technique from SQuAD-T, the state-of-the-art result in the WikiQA dataset can be improved. This demonstrates the potential of developing novel transfer learning techniques for the answer selection task. Many deep learning methods for answer selecti"
D09-1118,W08-0125,1,0.439883,"Missing"
D09-1118,P04-1085,0,0.023791,"med that meeting participants consider this to be the case, and Whittaker et al. (2006) found that the development of an automatic decision detection component is critical to the re-use of meeting archives. As a result, with the new availability of substantial meeting corpora such as the ISL (Burger et al., 2002), ICSI (Janin et al., 2004) and AMI (McCowan et al., 2005) Meeting Corpora, recent years have seen an increasing amount of research on decision-making dialogue. This recent research has tackled issues such as the automatic detection of agreement and disagreement (Hillard et al., 2003; Galley et al., 2004), and of the level of involvement of conversational participants (Wrede and Shriberg, 2003; Gatica-Perez et al., 2005). In addition, Verbree et al. (2006) created an argumentation scheme intended to support automatic production of argument structure diagrams from decision-oriented meeting transcripts. Only very recent research has specifically investigated the automatic detection of decisions, namely (Hsueh and Moore, 2007) and (Fern´andez et al., 2008). Hsueh and Moore (2007) used the AMI Meeting Corpus, and attempted to automatically identify dialogue acts (DAs) in meeting transcripts which"
D09-1118,N03-2012,0,0.0329188,"al., 2005) have confirmed that meeting participants consider this to be the case, and Whittaker et al. (2006) found that the development of an automatic decision detection component is critical to the re-use of meeting archives. As a result, with the new availability of substantial meeting corpora such as the ISL (Burger et al., 2002), ICSI (Janin et al., 2004) and AMI (McCowan et al., 2005) Meeting Corpora, recent years have seen an increasing amount of research on decision-making dialogue. This recent research has tackled issues such as the automatic detection of agreement and disagreement (Hillard et al., 2003; Galley et al., 2004), and of the level of involvement of conversational participants (Wrede and Shriberg, 2003; Gatica-Perez et al., 2005). In addition, Verbree et al. (2006) created an argumentation scheme intended to support automatic production of argument structure diagrams from decision-oriented meeting transcripts. Only very recent research has specifically investigated the automatic detection of decisions, namely (Hsueh and Moore, 2007) and (Fern´andez et al., 2008). Hsueh and Moore (2007) used the AMI Meeting Corpus, and attempted to automatically identify dialogue acts (DAs) in meet"
D09-1118,lisowska-etal-2004-user,0,0.113495,"tion scheme for decision discussions, and our experimental data. Next, Section 4 explains the hierarchical classification approach in more detail, and Section 5 considers how it can be applied in real-time. Section 6 describes the experiments in which we test the real-time detector, and finally, Section 7 presents conclusions and ideas for future work. 1133 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1133–1141, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP 2 Related Work Decisions are one of the most important meeting outputs. User studies (Lisowska et al., 2004; Banerjee et al., 2005) have confirmed that meeting participants consider this to be the case, and Whittaker et al. (2006) found that the development of an automatic decision detection component is critical to the re-use of meeting archives. As a result, with the new availability of substantial meeting corpora such as the ISL (Burger et al., 2002), ICSI (Janin et al., 2004) and AMI (McCowan et al., 2005) Meeting Corpora, recent years have seen an increasing amount of research on decision-making dialogue. This recent research has tackled issues such as the automatic detection of agreement and"
D09-1118,2007.sigdial-1.4,1,0.93474,"A class, and then based on the hypotheses of these socalled sub-classifiers, a super-classifier, (a further SVM), determined which regions of dialogue represented decision discussions. This approach produced better results than the kind of “flat classification” approach pursued by Hsueh and Moore (2007) where a single classifier looks for examples of a single decision-related DA class. Using manual transcripts, and a variety of lexical, utterance, speaker, DA and prosodic features for the sub-classifiers, the super-classifier’s F1-score was 0.58 according to a lenient match metric. Note that (Purver et al., 2007) had previously pursued the same basic approach as Fern´andez et al. (2008) in order to detect action items. While both Hsueh and Moore (2007), and Fern´andez et al. (2008) attempted off-line decision detection, in this paper, we attempt real-time decision detection. We take the same basic approach as Fern´andez et al. (2008), and make changes to its implementation so that it can work effectively in real-time. 3 Data The AMI corpus (McCowan et al., 2005), is a freely available corpus of multi-party meetings containing both audio and video recordings, as well as a wide range of annotated inform"
D19-1610,N18-1143,0,0.0151627,"t domains and topics. The code for constructing the StackExchangeQA dataset is available online 2 . In this work, we employ a basic transfer learning technique. The first step is to pre-train our answer selection model on the StackExchangeQA dataset. Then, the second step is to fine-tune the same model on a target dataset of interest such as TrecQA or WikiQA. Despite the simplicity of the technique, the performance of our model improves substantially compared to not using transfer learning. Different from previous works which use source datasets that were manually annotated (Min et al., 2017; Chung et al., 2018), our source dataset required minimal effort to obtain and preprocess. The choice of crawling question-answer pairs from the Stack Exchange website was arbitrary. We could also have crawled data from websites such as Yahoo Answers instead. Experiments and Results To evaluate the effectiveness of our proposed answer selection model, we use two datasets: TrecQA and WikiQA. The TrecQA dataset (Wang et al., 2007) was created from the TREC Question Answering tracks. There are two versions of TrecQA: raw and clean. Both versions have the same training set but their development and test sets differ."
D19-1610,P17-1168,0,0.0684735,"selection in Section 2. We then go into details our transfer learning approach in Section 3. After that, we describe the conducted experiments and their results in Section 4. Finally, we conclude this 5953 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5953–5959, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics work in Section 5. 2 2.1 Gated Self-Attention Memory Network The gated self-attention mechanism The gated attention mechanism (Dhingra et al., 2017; Tran et al., 2017) extends the popular scalarbased attention mechanism by calculating a real vector gate to control the flow of information, instead of a scalar value. Let’s denote the sequence of input vectors as X = [x1 ..xn ]. If we have context information c, then in traditional attention mechanism, association score αi is usually calculated as a normalized dot product between the two vectors c and xi (Equation 1) where i ∈ [1..n]. exp(cT xi ) T j∈[1..n] exp(c xj ) αi = P (1) For the gated attention mechanism, the association between two vectors c and xi is represented by gate vector gi"
D19-1610,N10-1145,0,0.0432679,"sfer learning technique from a large-scale online corpus, our model outperforms previous methods by a large margin, achieving new state-ofthe-art results on two standard answer selection datasets: TrecQA and WikiQA. 1 Introduction and Related Work Answer selection is an important task, with applications in many areas (Lai et al., 2018). Given a question and a set of candidate answers, the task is to identify the most relevant candidate. Previous work on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, with the renaissance of neural network models, many deep learning based methods have been proposed to address the task (Tay et al., 2017b; Shen et al., 2017; Wang et al., 2017; Bian et al., 2017; Tymoshenko and Moschitti, 2018; Tay et al., 2018; Tayyar Madabushi et al., 2018; Yoon et al., 2019). They outperform traditional techniques. A common trait of a number of these deep learning methods is the use of the Compare-Aggregate architecture (Wang and Jiang, 2017). Typically in this architecture, contextualized vector representations of small unit"
D19-1610,C18-1181,1,0.841002,"e Compare-Aggregate architecture that performs word-level comparison followed by aggregation. In this work, we take a departure from the popular CompareAggregate architecture, and instead, propose a new gated self-attention memory network for the task. Combined with a simple transfer learning technique from a large-scale online corpus, our model outperforms previous methods by a large margin, achieving new state-ofthe-art results on two standard answer selection datasets: TrecQA and WikiQA. 1 Introduction and Related Work Answer selection is an important task, with applications in many areas (Lai et al., 2018). Given a question and a set of candidate answers, the task is to identify the most relevant candidate. Previous work on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, with the renaissance of neural network models, many deep learning based methods have been proposed to address the task (Tay et al., 2017b; Shen et al., 2017; Wang et al., 2017; Bian et al., 2017; Tymoshenko and Moschitti, 2018; Tay et al., 2018; Tayyar Madabushi et"
D19-1610,D16-1147,0,0.0220978,"s, and linear decay of the learning rate. We did hyper-parameter tuning on the development sets. It is worth noting that, we have experimented with various values for the number of reasoning hops. We found that using 2 hops gives the best performance on the tested datasets while using larger number of hops decreases the performance slightly. We attribute the diminishing returns in increasing the number of hops to the limited size of the TrecQA and WikiQA datasets. Many previous works related to memory networks also use small number of memory hops (Weston et al., 2015; Sukhbaatar et al., 2015; Miller et al., 2016; Zhang et al., 2018). 4.1 Comparison with Previous Methods Table 2 summarizes the performances of our proposed models and compares them to the baselines on the TrecQA and WikiQA datasets. The 3 https://aclweb.org/aclwiki/Question_ Answering_(State_of_the_art) full model [BERT + GSAMN+ Transfer Learning] outperforms the previous state-of-the-art methods by a large margin. Note that by simply fine-tuning the pre-trained BERT embeddings, one can easily achieve very competitive performance on both datasets. This is expected as BERT has been pretrained on a massive amount of unlabeled data. Howeve"
D19-1610,P17-2081,0,0.0349355,"from many different domains and topics. The code for constructing the StackExchangeQA dataset is available online 2 . In this work, we employ a basic transfer learning technique. The first step is to pre-train our answer selection model on the StackExchangeQA dataset. Then, the second step is to fine-tune the same model on a target dataset of interest such as TrecQA or WikiQA. Despite the simplicity of the technique, the performance of our model improves substantially compared to not using transfer learning. Different from previous works which use source datasets that were manually annotated (Min et al., 2017; Chung et al., 2018), our source dataset required minimal effort to obtain and preprocess. The choice of crawling question-answer pairs from the Stack Exchange website was arbitrary. We could also have crawled data from websites such as Yahoo Answers instead. Experiments and Results To evaluate the effectiveness of our proposed answer selection model, we use two datasets: TrecQA and WikiQA. The TrecQA dataset (Wang et al., 2007) was created from the TREC Question Answering tracks. There are two versions of TrecQA: raw and clean. Both versions have the same training set but their development a"
D19-1610,D14-1162,0,0.0870222,"ection, we concatenate question Q and candidate answer A to a single sequence and treat the task as a binary classification problem. Given the GSAMN architecture above, we can use final controller state cT as the representation of the sequence. The matching probability P (A |Q) is finally calculated as follows:   P (A |Q) = σ Wc cT + bc (6) 5954 Figure 1: Simplified computation flow of the Gated Self-Attention Memory Network where Wc and bc are learnable parameters. We can initialize the memory values x01 ... x0n using any representation model such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018), or BERT (Devlin et al., 2018). The control vector c is a randomly initialized learnable vector. 3 4 Transfer Learning Previous studies on answer selection have focused mostly on small-scale datasets. On the other hand, many community question answering (CQA) platforms such as Yahoo Answers and Stack Exchange have become an essential source of information for many people. The amount of data (i.e., questions and answers) in these CQA platforms is huge and encompasses many domains and topics. This provides a great opportunity to apply transfer learning techniques to"
D19-1610,N18-1202,0,0.346815,"Q and candidate answer A to a single sequence and treat the task as a binary classification problem. Given the GSAMN architecture above, we can use final controller state cT as the representation of the sequence. The matching probability P (A |Q) is finally calculated as follows:   P (A |Q) = σ Wc cT + bc (6) 5954 Figure 1: Simplified computation flow of the Gated Self-Attention Memory Network where Wc and bc are learnable parameters. We can initialize the memory values x01 ... x0n using any representation model such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018), or BERT (Devlin et al., 2018). The control vector c is a randomly initialized learnable vector. 3 4 Transfer Learning Previous studies on answer selection have focused mostly on small-scale datasets. On the other hand, many community question answering (CQA) platforms such as Yahoo Answers and Stack Exchange have become an essential source of information for many people. The amount of data (i.e., questions and answers) in these CQA platforms is huge and encompasses many domains and topics. This provides a great opportunity to apply transfer learning techniques to improve answer selection sys"
D19-1610,D17-1122,0,0.187356,"iQA. 1 Introduction and Related Work Answer selection is an important task, with applications in many areas (Lai et al., 2018). Given a question and a set of candidate answers, the task is to identify the most relevant candidate. Previous work on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, with the renaissance of neural network models, many deep learning based methods have been proposed to address the task (Tay et al., 2017b; Shen et al., 2017; Wang et al., 2017; Bian et al., 2017; Tymoshenko and Moschitti, 2018; Tay et al., 2018; Tayyar Madabushi et al., 2018; Yoon et al., 2019). They outperform traditional techniques. A common trait of a number of these deep learning methods is the use of the Compare-Aggregate architecture (Wang and Jiang, 2017). Typically in this architecture, contextualized vector representations of small units such as words of the question and the candidate ∗ Equal contributions. The work was conducted while the first author interned at Adobe Research. are first compared and aligned. After that, these comparis"
D19-1610,C18-1278,0,0.230001,"t al., 2018). Given a question and a set of candidate answers, the task is to identify the most relevant candidate. Previous work on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, with the renaissance of neural network models, many deep learning based methods have been proposed to address the task (Tay et al., 2017b; Shen et al., 2017; Wang et al., 2017; Bian et al., 2017; Tymoshenko and Moschitti, 2018; Tay et al., 2018; Tayyar Madabushi et al., 2018; Yoon et al., 2019). They outperform traditional techniques. A common trait of a number of these deep learning methods is the use of the Compare-Aggregate architecture (Wang and Jiang, 2017). Typically in this architecture, contextualized vector representations of small units such as words of the question and the candidate ∗ Equal contributions. The work was conducted while the first author interned at Adobe Research. are first compared and aligned. After that, these comparison results are then aggregated to calculate a score indicating the relevance between the question and the candidate. On"
D19-1610,P17-2083,1,0.845833,"2. We then go into details our transfer learning approach in Section 3. After that, we describe the conducted experiments and their results in Section 4. Finally, we conclude this 5953 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5953–5959, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics work in Section 5. 2 2.1 Gated Self-Attention Memory Network The gated self-attention mechanism The gated attention mechanism (Dhingra et al., 2017; Tran et al., 2017) extends the popular scalarbased attention mechanism by calculating a real vector gate to control the flow of information, instead of a scalar value. Let’s denote the sequence of input vectors as X = [x1 ..xn ]. If we have context information c, then in traditional attention mechanism, association score αi is usually calculated as a normalized dot product between the two vectors c and xi (Equation 1) where i ∈ [1..n]. exp(cT xi ) T j∈[1..n] exp(c xj ) αi = P (1) For the gated attention mechanism, the association between two vectors c and xi is represented by gate vector gi as follows:  gi = σ"
D19-1610,N18-1115,1,0.725257,"Missing"
D19-1610,D18-1240,0,0.309655,"an important task, with applications in many areas (Lai et al., 2018). Given a question and a set of candidate answers, the task is to identify the most relevant candidate. Previous work on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, with the renaissance of neural network models, many deep learning based methods have been proposed to address the task (Tay et al., 2017b; Shen et al., 2017; Wang et al., 2017; Bian et al., 2017; Tymoshenko and Moschitti, 2018; Tay et al., 2018; Tayyar Madabushi et al., 2018; Yoon et al., 2019). They outperform traditional techniques. A common trait of a number of these deep learning methods is the use of the Compare-Aggregate architecture (Wang and Jiang, 2017). Typically in this architecture, contextualized vector representations of small units such as words of the question and the candidate ∗ Equal contributions. The work was conducted while the first author interned at Adobe Research. are first compared and aligned. After that, these comparison results are then aggregated to calculate a score indicating the rel"
D19-1610,C10-1131,0,0.189271,"Missing"
D19-1610,D07-1003,0,0.723653,"k for the task. Combined with a simple transfer learning technique from a large-scale online corpus, our model outperforms previous methods by a large margin, achieving new state-ofthe-art results on two standard answer selection datasets: TrecQA and WikiQA. 1 Introduction and Related Work Answer selection is an important task, with applications in many areas (Lai et al., 2018). Given a question and a set of candidate answers, the task is to identify the most relevant candidate. Previous work on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, with the renaissance of neural network models, many deep learning based methods have been proposed to address the task (Tay et al., 2017b; Shen et al., 2017; Wang et al., 2017; Bian et al., 2017; Tymoshenko and Moschitti, 2018; Tay et al., 2018; Tayyar Madabushi et al., 2018; Yoon et al., 2019). They outperform traditional techniques. A common trait of a number of these deep learning methods is the use of the Compare-Aggregate architecture (Wang and Jiang, 2017). Typically in this architecture, co"
D19-1610,D15-1237,0,0.550695,"number of these deep learning methods is the use of the Compare-Aggregate architecture (Wang and Jiang, 2017). Typically in this architecture, contextualized vector representations of small units such as words of the question and the candidate ∗ Equal contributions. The work was conducted while the first author interned at Adobe Research. are first compared and aligned. After that, these comparison results are then aggregated to calculate a score indicating the relevance between the question and the candidate. On standard answer selection datasets such as TrecQA (Wang et al., 2007) or WikiQA (Yang et al., 2015), CompareAggregate approaches achieve very competitive performance. However, they still have some limitations. For example, the first few layers of most previous Compare-Aggregate models encode the question-candidate pair into sequences of contextualized vector representations separately (Wang et al., 2017; Shen et al., 2017; Bian et al., 2017). These sequences are independent and completely ignore the information from the other sequence. In this work, we take a departure from the popular Compare-Aggregate architecture, so instead, we propose a mix between two very successful architectures in"
D19-1610,N13-1106,0,0.0895392,"Missing"
D19-1610,P18-1205,0,0.0126596,"f the learning rate. We did hyper-parameter tuning on the development sets. It is worth noting that, we have experimented with various values for the number of reasoning hops. We found that using 2 hops gives the best performance on the tested datasets while using larger number of hops decreases the performance slightly. We attribute the diminishing returns in increasing the number of hops to the limited size of the TrecQA and WikiQA datasets. Many previous works related to memory networks also use small number of memory hops (Weston et al., 2015; Sukhbaatar et al., 2015; Miller et al., 2016; Zhang et al., 2018). 4.1 Comparison with Previous Methods Table 2 summarizes the performances of our proposed models and compares them to the baselines on the TrecQA and WikiQA datasets. The 3 https://aclweb.org/aclwiki/Question_ Answering_(State_of_the_art) full model [BERT + GSAMN+ Transfer Learning] outperforms the previous state-of-the-art methods by a large margin. Note that by simply fine-tuning the pre-trained BERT embeddings, one can easily achieve very competitive performance on both datasets. This is expected as BERT has been pretrained on a massive amount of unlabeled data. However, our proposed techn"
L18-1438,D13-1160,0,0.0361933,"ins more than 2,000 questions associated with ten most relevant comments (answers). It also shares some characteristics with Yahoo’s Webscope5 L4 used by (Surdeanu et al., 2008) and (Jansen et al., 2014), L6 and with dard raster graphics editor developed and published by Adobe Systems for macOS and Windows. https://en.wikipedia.org/wiki/Adobe Photoshop 5 https://webscope.sandbox.yahoo.com/ catalog.php?datatype=l Amazon Baidu Allen AI Science Challenge Quora Description for training semantic parsers, which map natural language utterances to denotations (answers) via intermediate logical forms (Berant et al., 2013) 2,180 questions extracted from the datasets from TREC (Baudiˇs and ˇ Sediv` y, 2015) 3,000 questions sampled from Bing query logs associated with a Wikipedia page presumed to be the topic of the question (Yang et al., 2015) 30M natural language questions in English and their corresponding facts in the knowledge base Freebase (Serban et al., 2016) 100,000 question-answer pairs on more than 500 Wikipedia articles (Rajpurkar et al., 2016) 1.4 million answered questions from Amazon (Wan and McAuley, 2016) 42K questions and 579K evidences, which are a piece of text containing information for answe"
L18-1438,blanco-etal-2008-causal,0,0.0536581,"Missing"
L18-1438,P13-1042,0,0.0244451,"s or short videos which would be otherwise lost. We analyze the (WhyQ, A) pairs for presence of certain linguistic cues such as causality markers (e.g., the reason for, because or due to). 2. CuratedTREC WikiQA 30M Factoid QA Corpus SQuAD Related Work & Datasets In recent years, numerous datasets have been released in the domain of question-answering (QA) systems to promote new methods that integrate natural language processing, information retrieval, artificial intelligence and knowledge discovery. The majority of these datasets were opendomain (Bollacker et al., 2008; Ignatova et al., 2009; Cai and Yates, 2013; Yang et al., 2015; Chen et al., 2017). There are still a few QA datasets for specific fields such as BioASQ and WikiMovies. The BioASQ dataset contains questions in English, along with reference answers constructed by a team of biomedical experts (Tsatsaronis et al., 2015). The WikiMovies dataset contains 96K question-answer pairs in the domain of movies (Miller et al., 2016). Table 1 introduces selected recent open-domain QA datasets. Several existing datasets focus on the data taken from CQA web sites. The data structure of our dataset (questionanswer(s) pair with the best answer labeled),"
L18-1438,P17-1171,0,0.0135719,"se lost. We analyze the (WhyQ, A) pairs for presence of certain linguistic cues such as causality markers (e.g., the reason for, because or due to). 2. CuratedTREC WikiQA 30M Factoid QA Corpus SQuAD Related Work & Datasets In recent years, numerous datasets have been released in the domain of question-answering (QA) systems to promote new methods that integrate natural language processing, information retrieval, artificial intelligence and knowledge discovery. The majority of these datasets were opendomain (Bollacker et al., 2008; Ignatova et al., 2009; Cai and Yates, 2013; Yang et al., 2015; Chen et al., 2017). There are still a few QA datasets for specific fields such as BioASQ and WikiMovies. The BioASQ dataset contains questions in English, along with reference answers constructed by a team of biomedical experts (Tsatsaronis et al., 2015). The WikiMovies dataset contains 96K question-answer pairs in the domain of movies (Miller et al., 2016). Table 1 introduces selected recent open-domain QA datasets. Several existing datasets focus on the data taken from CQA web sites. The data structure of our dataset (questionanswer(s) pair with the best answer labeled), resembles the one in (Hoogeveen et al."
L18-1438,W17-0812,0,0.0457172,"Missing"
L18-1438,W03-1210,0,0.117019,"Missing"
L18-1438,I08-1055,0,0.435627,"or whether domain adaptation is needed. The authors are aware of the corpora from (Prasad et al., 2007) and (Dunietz et al., 2017), but these were not considered for this work, because their datasets do not address CQA and/or Why-QA. Some of the previous studies in Why-QA systems tried to extract why-questions from QA datasets related to general questions; however, the size and quality of why-questions were limited. Previous datasets used in Why-QA task contain few (WhyQ, A) pairs (under 1,000), are handcrafted, are not available online anymore (Verberne et al., 2007; Mrozinski et al., 2008; Higashinaka and Isozaki, 2008) or target Japanese (Higashinaka and Isozaki, 2008; Oh et al., 2012). There is a need for a public specific why-question dataset for English to advance the research and development in Why-QA. 3. are more reliable and correct - two thirds of the (WhyQ,A) pairs have an accepted answer authored by a Photoshop expert. 3.2. • an initial list of URLs which the Spider will begin to crawl from. This is provided in start urls attribute or via start requests() method. • an implementation of the default parse() callback method, which is a generator function under the hood. This method is accountable for"
L18-1438,P14-1092,0,0.0213759,"ta taken from CQA web sites. The data structure of our dataset (questionanswer(s) pair with the best answer labeled), resembles the one in (Hoogeveen et al., 2015). However, it does not include comments and tags, making it more suitable for Why-QA than previous structures which include only questions (Iyer et al., 2017). Our work is mostly related to the SemEval-2016 Task 3 dataset (Nakov et al., 2016) which contains more than 2,000 questions associated with ten most relevant comments (answers). It also shares some characteristics with Yahoo’s Webscope5 L4 used by (Surdeanu et al., 2008) and (Jansen et al., 2014), L6 and with dard raster graphics editor developed and published by Adobe Systems for macOS and Windows. https://en.wikipedia.org/wiki/Adobe Photoshop 5 https://webscope.sandbox.yahoo.com/ catalog.php?datatype=l Amazon Baidu Allen AI Science Challenge Quora Description for training semantic parsers, which map natural language utterances to denotations (answers) via intermediate logical forms (Berant et al., 2013) 2,180 questions extracted from the datasets from TREC (Baudiˇs and ˇ Sediv` y, 2015) 3,000 questions sampled from Bing query logs associated with a Wikipedia page presumed to be the"
L18-1438,D16-1147,0,0.0143628,"w methods that integrate natural language processing, information retrieval, artificial intelligence and knowledge discovery. The majority of these datasets were opendomain (Bollacker et al., 2008; Ignatova et al., 2009; Cai and Yates, 2013; Yang et al., 2015; Chen et al., 2017). There are still a few QA datasets for specific fields such as BioASQ and WikiMovies. The BioASQ dataset contains questions in English, along with reference answers constructed by a team of biomedical experts (Tsatsaronis et al., 2015). The WikiMovies dataset contains 96K question-answer pairs in the domain of movies (Miller et al., 2016). Table 1 introduces selected recent open-domain QA datasets. Several existing datasets focus on the data taken from CQA web sites. The data structure of our dataset (questionanswer(s) pair with the best answer labeled), resembles the one in (Hoogeveen et al., 2015). However, it does not include comments and tags, making it more suitable for Why-QA than previous structures which include only questions (Iyer et al., 2017). Our work is mostly related to the SemEval-2016 Task 3 dataset (Nakov et al., 2016) which contains more than 2,000 questions associated with ten most relevant comments (answer"
L18-1438,P08-1051,0,0.614703,"sed domain such as ours, or whether domain adaptation is needed. The authors are aware of the corpora from (Prasad et al., 2007) and (Dunietz et al., 2017), but these were not considered for this work, because their datasets do not address CQA and/or Why-QA. Some of the previous studies in Why-QA systems tried to extract why-questions from QA datasets related to general questions; however, the size and quality of why-questions were limited. Previous datasets used in Why-QA task contain few (WhyQ, A) pairs (under 1,000), are handcrafted, are not available online anymore (Verberne et al., 2007; Mrozinski et al., 2008; Higashinaka and Isozaki, 2008) or target Japanese (Higashinaka and Isozaki, 2008; Oh et al., 2012). There is a need for a public specific why-question dataset for English to advance the research and development in Why-QA. 3. are more reliable and correct - two thirds of the (WhyQ,A) pairs have an accepted answer authored by a Photoshop expert. 3.2. • an initial list of URLs which the Spider will begin to crawl from. This is provided in start urls attribute or via start requests() method. • an implementation of the default parse() callback method, which is a generator function under the hood."
L18-1438,S16-1083,0,0.0612617,"Missing"
L18-1438,D12-1034,0,0.0602377,"Missing"
L18-1438,D16-1264,0,0.0757634,"Missing"
L18-1438,P16-1056,0,0.0517625,"Missing"
L18-1438,P08-1082,0,0.0375661,"ing datasets focus on the data taken from CQA web sites. The data structure of our dataset (questionanswer(s) pair with the best answer labeled), resembles the one in (Hoogeveen et al., 2015). However, it does not include comments and tags, making it more suitable for Why-QA than previous structures which include only questions (Iyer et al., 2017). Our work is mostly related to the SemEval-2016 Task 3 dataset (Nakov et al., 2016) which contains more than 2,000 questions associated with ten most relevant comments (answers). It also shares some characteristics with Yahoo’s Webscope5 L4 used by (Surdeanu et al., 2008) and (Jansen et al., 2014), L6 and with dard raster graphics editor developed and published by Adobe Systems for macOS and Windows. https://en.wikipedia.org/wiki/Adobe Photoshop 5 https://webscope.sandbox.yahoo.com/ catalog.php?datatype=l Amazon Baidu Allen AI Science Challenge Quora Description for training semantic parsers, which map natural language utterances to denotations (answers) via intermediate logical forms (Berant et al., 2013) 2,180 questions extracted from the datasets from TREC (Baudiˇs and ˇ Sediv` y, 2015) 3,000 questions sampled from Bing query logs associated with a Wikipedi"
L18-1438,P10-1078,0,0.0275408,"or Google Assistant) have strengthened the interest in the Question Answering field. These systems have in common the fact that they mostly tackle factoid questions. These are questions that “can be answered with simple facts expressed in short text answers”; usually, their answers include “short strings expressing a personal name, temporal expression, or location” (Jurafsky and Martin, 2017). An example of a factoid question and its answer is: Q: Who is Canada’s prime minister? A: Justin Trudeau. By contrast, non-factoid questions ask for “opinions, suggestions, interpretations and the like”(Tomasoni and Huang, 2010). Answering and evaluating the quality of the provided answers for non-factoid questions have proved to be non-trivial due to the difficulty of the task complexity as well as the lack of training data. To address the latter issue, numerous researchers have tried to take advantage of user-generated content on Community Question Answering (CQA) web sites such as Yahoo! Answers 1 , Stack Overflow 2 or Quora 3 . These web forums allow users to post their own questions, answer others’ questions, comment on others’ replies, and upvote or downvote answers. 1 https://answers.yahoo.com https://stackove"
L18-1438,J10-2003,0,0.0274667,"to answerer) and the response time (immediate vs. several hours or days). Among all categories of non-factoid questions, namely list, confirmation, causal and hypothetical (Mishra and Jain, 2016), we are especially interested in why-questions that are related to causal relations. Why-questions are difficult to answer automatically since the answers often need to be constructed based on different information extracted from multiple knowledge sources. For this reason, whyquestions need a different approach than factoid questions because their answers usually cannot be stated in a single phrase (Verberne et al., 2010). A why Question Answering (Why-QA) system trying to answer questions using CQA data needs to be able to distinguish between relevant and irrelevant answers (answer selection task). Most of the time these systems also produce a sorted output of relevant answers (answer re-ranking task). Both tasks require curated and informative datasets on which to evaluate proposed methods. In this paper, we introduce the PhotoshopQuiA dataset, a corpus consisting of 2,854 (WhyQ, A) pairs covering various questions and answers about Adobe Photoshop 4 . We 4 2763 Adobe Photoshop is the de facto industry stanc"
L18-1438,D15-1237,0,0.241298,"ch would be otherwise lost. We analyze the (WhyQ, A) pairs for presence of certain linguistic cues such as causality markers (e.g., the reason for, because or due to). 2. CuratedTREC WikiQA 30M Factoid QA Corpus SQuAD Related Work & Datasets In recent years, numerous datasets have been released in the domain of question-answering (QA) systems to promote new methods that integrate natural language processing, information retrieval, artificial intelligence and knowledge discovery. The majority of these datasets were opendomain (Bollacker et al., 2008; Ignatova et al., 2009; Cai and Yates, 2013; Yang et al., 2015; Chen et al., 2017). There are still a few QA datasets for specific fields such as BioASQ and WikiMovies. The BioASQ dataset contains questions in English, along with reference answers constructed by a team of biomedical experts (Tsatsaronis et al., 2015). The WikiMovies dataset contains 96K question-answer pairs in the domain of movies (Miller et al., 2016). Table 1 introduces selected recent open-domain QA datasets. Several existing datasets focus on the data taken from CQA web sites. The data structure of our dataset (questionanswer(s) pair with the best answer labeled), resembles the one"
L18-1683,P16-1170,0,0.0764105,"rovide a crowd-sourcing methodology to offload complex annotation between expert users and novice users and evaluate them. This is particularly useful for creating a sizable corpus. 2. Related Work Recently, there has been a lot of work on applications that combine vision and language, e.g., understanding and generating image descriptions (Kulkarni et al., 2013), identifying visual reference in the presence of distractors (Paetzel et al., 2015; de Vries et al., 2016), visual question answering (Antol et al., 2015), visual storytelling (Huang et al., 2016), generating questions about an image (Mostafazadeh et al., 2016), and question-answer interactions grounded on information shown in an image (Mostafazadeh et al., 2017). Current image and language corpora typically consist of digital photographs paired with crowd-sourced captions (Lin et al., 2014; Krishna et al., 2017), or in some cases with questions related to those images (Mostafazadeh et al., 2016). Much of the work above is relevant to the problem at hand. For example, understanding image descriptions is crucial for interpreting the requests quoted above, as all of them contain image descriptions (my wedding dress; my dog’s eyes; the people in the ba"
L18-1683,I17-1047,0,0.0714646,"and evaluate them. This is particularly useful for creating a sizable corpus. 2. Related Work Recently, there has been a lot of work on applications that combine vision and language, e.g., understanding and generating image descriptions (Kulkarni et al., 2013), identifying visual reference in the presence of distractors (Paetzel et al., 2015; de Vries et al., 2016), visual question answering (Antol et al., 2015), visual storytelling (Huang et al., 2016), generating questions about an image (Mostafazadeh et al., 2016), and question-answer interactions grounded on information shown in an image (Mostafazadeh et al., 2017). Current image and language corpora typically consist of digital photographs paired with crowd-sourced captions (Lin et al., 2014; Krishna et al., 2017), or in some cases with questions related to those images (Mostafazadeh et al., 2016). Much of the work above is relevant to the problem at hand. For example, understanding image descriptions is crucial for interpreting the requests quoted above, as all of them contain image descriptions (my wedding dress; my dog’s eyes; the people in the background; my ex). However, to our knowledge, no work has yet attempted to tackle the specific task of au"
L18-1683,W15-4610,1,0.637423,"reference to objects in the images. Second, a framework for understanding these natural language instructions and mapping them to actionable computer commands. Finally, we provide a crowd-sourcing methodology to offload complex annotation between expert users and novice users and evaluate them. This is particularly useful for creating a sizable corpus. 2. Related Work Recently, there has been a lot of work on applications that combine vision and language, e.g., understanding and generating image descriptions (Kulkarni et al., 2013), identifying visual reference in the presence of distractors (Paetzel et al., 2015; de Vries et al., 2016), visual question answering (Antol et al., 2015), visual storytelling (Huang et al., 2016), generating questions about an image (Mostafazadeh et al., 2016), and question-answer interactions grounded on information shown in an image (Mostafazadeh et al., 2017). Current image and language corpora typically consist of digital photographs paired with crowd-sourced captions (Lin et al., 2014; Krishna et al., 2017), or in some cases with questions related to those images (Mostafazadeh et al., 2016). Much of the work above is relevant to the problem at hand. For example, under"
L18-1683,W15-4622,0,0.098124,"Missing"
N18-1115,W17-5526,0,0.0464322,"Missing"
N18-1115,D15-1181,0,0.0838655,"Missing"
N18-1115,N16-1037,1,0.926505,"code the 1277 Figure 2: CARNN for dialog. responses as shown in Figure 2, which depicts our architecture of CARNN for dialog. ∀l ∈ [1..L] : el = P osition Encoder(ylc ) (9) We then put a distribution over the candidate responses conditioned on the summarized dialog history hhis (Equation 10). P(y) = sof tmax(hThis ey1 , ..., hThis eyL ) 4.2 (10) Contextual language model Typically, language models operate at the sentence level, i.e., the sentences are treated independently. Several researchers have explored inter-sentence and inter-document level contextual information for language modelling (Ji et al., 2016a,b; Tran et al., 2016; Lau et al., 2017). Following Ji et al. (2016a,b), we investigate two types of contextual information: (i) the previous sentence context; and (ii) a latent variable capturing the connection information between sentences, such as discourse relation in the Penn Discourse Tree Bank dataset or Dialog Acts in the Switchboard dataset. Previous sentence context. The previous sentence (time-step t − 1) contextual information is encoded by a simplified version of the nCARNN, where the global context is absent. The final hidden vector of this sequence is then fed into the current"
N18-1115,P17-1033,0,0.0291302,"g. responses as shown in Figure 2, which depicts our architecture of CARNN for dialog. ∀l ∈ [1..L] : el = P osition Encoder(ylc ) (9) We then put a distribution over the candidate responses conditioned on the summarized dialog history hhis (Equation 10). P(y) = sof tmax(hThis ey1 , ..., hThis eyL ) 4.2 (10) Contextual language model Typically, language models operate at the sentence level, i.e., the sentences are treated independently. Several researchers have explored inter-sentence and inter-document level contextual information for language modelling (Ji et al., 2016a,b; Tran et al., 2016; Lau et al., 2017). Following Ji et al. (2016a,b), we investigate two types of contextual information: (i) the previous sentence context; and (ii) a latent variable capturing the connection information between sentences, such as discourse relation in the Penn Discourse Tree Bank dataset or Dialog Acts in the Switchboard dataset. Previous sentence context. The previous sentence (time-step t − 1) contextual information is encoded by a simplified version of the nCARNN, where the global context is absent. The final hidden vector of this sequence is then fed into the current recurrent computation (time-step t) as th"
N18-1115,E17-1001,0,0.204613,"understanding a piece of text may require far more than just extracting the information from that piece itself. If the piece of text is a paragraph of a document, the reader may have to consider it together with other paragraphs in the document and the topic of the document. To understand an utterance in a conversation, the utterance has to be put into the context of the conversation, which includes the goals of the participants and the dialog history. Hence the notion of context is an intrinsic component of language understanding. Inspired by recent works in dialog systems (Seo et al., 2017; Liu and Perez, 2017), we formalize the contextual sequence mapping problem as a sequence mapping problem with a strong controlling contextual element that regulates the flow of information. The system has two sources of signals: (i) the main text input, for example, the history utterance sequence in dialog systems or the sequence of words in language modelling; and (ii) the context signal, e.g., the previous utterance in a dialog system, the discourse information in contextual language modelling or the question in question answering. Our contribution in this work is two-fold. First, we propose a new family of rec"
N18-1115,miltsakaki-etal-2004-penn,0,0.0719031,"ot necessary. In the same spirit, our CARNN unit minimizes the use of non-linearity in the model to facilitate the ease of gradient flow. We also seek to keep the number of parameters to a minimum to improve trainability. We experiment with our models on a broad range of problems: dialog systems, contextual language modelling and question answering. Our systems outperform previous methods on several public datasets, which include the Babi Task 6 (Bordes and Weston, 2017) and the Frame dataset (Asri et al., 2017) for dialog, the Switchboard (Jurafsky et al., 1997) and Penn Discourse Tree Bank (Miltsakaki et al., 2004) for contextual language modelling, and the TrecQA 1274 Proceedings of NAACL-HLT 2018, pages 1274–1283 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics dataset (Wang et al., 2007) for question answering. We propose a different architecture for each task, but all models share the basic building block, the CARNN. 2 transition dynamic of RNN by removing the tanh non-linearity from the ˜cm . The equations for RAN are as follows: ˜cm = Wcx em Background and Notation gim = σ(Wih hm−1 + Wix em + bi ) Notation. As our paper describes several architectures with"
N18-1115,D17-1122,0,0.193232,"Missing"
N18-1115,I17-1057,1,0.797527,"d as follows. gum = σ(Wcu c + Weu em + bu ) gfm = σ(Wcf c + Wef em + bf ) hm = gum (gfm em ) + (1 − gum ) hm−1 (6) sCARNN can still be decomposed into a weighted sum of the sequence of input elements, and retains the parallel computation capability of the iCARNN. hM = guM gfM em + (1 − guM ) hM −1 = M X i=1 4 (gui gfi M Y (1 − guj )) ei j=i+1 (7) Summarizing the dialog history. The CARNN models take the embeddings of the sequence of utterances and produce the final representation hhis . We further enhance the output of the CARNN by adding the residual connection to the input (He et al., 2016; Tran et al., 2017), and the attention mechanism (Bahdanau et al., 2015) over the history. CARNN-based models for NLP problems In this section, we explain the details of our CARNN-based architectures for end-to-end dialog, language modelling and question answering. In each of these applications, one of the main design concerns is the choice of contextual information. As we will demonstrate in this section, the controlling context c can be derived from various sources: a sequence of words (dialog and question answering), a class variable (language modelling). Virtually any sources of strong information that can b"
N18-1115,N16-1090,1,0.796675,"2: CARNN for dialog. responses as shown in Figure 2, which depicts our architecture of CARNN for dialog. ∀l ∈ [1..L] : el = P osition Encoder(ylc ) (9) We then put a distribution over the candidate responses conditioned on the summarized dialog history hhis (Equation 10). P(y) = sof tmax(hThis ey1 , ..., hThis eyL ) 4.2 (10) Contextual language model Typically, language models operate at the sentence level, i.e., the sentences are treated independently. Several researchers have explored inter-sentence and inter-document level contextual information for language modelling (Ji et al., 2016a,b; Tran et al., 2016; Lau et al., 2017). Following Ji et al. (2016a,b), we investigate two types of contextual information: (i) the previous sentence context; and (ii) a latent variable capturing the connection information between sentences, such as discourse relation in the Penn Discourse Tree Bank dataset or Dialog Acts in the Switchboard dataset. Previous sentence context. The previous sentence (time-step t − 1) contextual information is encoded by a simplified version of the nCARNN, where the global context is absent. The final hidden vector of this sequence is then fed into the current recurrent computation"
N18-1115,D07-1003,0,0.710665,"periment with our models on a broad range of problems: dialog systems, contextual language modelling and question answering. Our systems outperform previous methods on several public datasets, which include the Babi Task 6 (Bordes and Weston, 2017) and the Frame dataset (Asri et al., 2017) for dialog, the Switchboard (Jurafsky et al., 1997) and Penn Discourse Tree Bank (Miltsakaki et al., 2004) for contextual language modelling, and the TrecQA 1274 Proceedings of NAACL-HLT 2018, pages 1274–1283 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics dataset (Wang et al., 2007) for question answering. We propose a different architecture for each task, but all models share the basic building block, the CARNN. 2 transition dynamic of RNN by removing the tanh non-linearity from the ˜cm . The equations for RAN are as follows: ˜cm = Wcx em Background and Notation gim = σ(Wih hm−1 + Wix em + bi ) Notation. As our paper describes several architectures with vastly different setups and input types, we introduce the following notation to maintain consistency and improve readability. First, the mth input to the recurrent unit will be denoted em . In language modelling, em is t"
N18-1115,P17-1062,0,0.0307967,"he second set of experiments, we use our end-to-end systems as “dialog managers”. The only difference compared to the end-to-end dialog setting is that the systems produce templatized responses instead of complete responses. Our motivation for this dialog manager setting is that in our preliminary experiments with the Babi dataset, we found out that many of the classification errors are due to very closely related responses, all of which fit the corresponding context. We argue that if we treat the systems as dialog managers, then we can delexicalize and group similar responses. Thus following Williams et al. (2017), we construct a templatized set of responses. For example, all the 1 Among the Babi tasks, we focus mainly on task 6, which is based on real human-machine interactions. The other five Babi datasets comprise synthetically generated data. 1279 Figure 4: CARNN for Question Answering. responses similar to “india house is in the west part of town” will be grouped into “ name is in the loc part of town”. The set of responses is reduced to 75 templatized responses. We call this new dataset “Babi reduced”.2 The third set of experiments is conducted on the Frame dataset. The general theme in this data"
N18-2097,D17-1221,0,0.0155952,"ated according to: (t) βj = (s) (d) softmax(score(hj , ht−1 )) j   (e) (t) (d) (t) (t) α(j,i) = softmax βj score(h(j,i) , cov(j,i) , ht−1 ) (6) (i,j) (d) 4 At each timestep t, the decoder state ht and the context vector ct are used to estimate the probability distribution of next word yt :  (d) > Neural abstractive summarization models have been studied in the past (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016) and later extended by source copying (Miao and Blunsom, 2016; See et al., 2017), reinformcement learning (Paulus et al., 2017), and sentence salience information (Li et al., 2017). One model variant of Nallapati et al. (2016) is related to our model in using sentence-level information in attention. However, our model is different as it contains a hierarchical encoder, uses discourse sections in the decoding step, and has a coverage mechanism. Similarly, Ling and Rush (2017) proposed a coarseto-fine attention model that uses hard attention to find the text chunks of importance and then only attend to words in that chunk. In contrast, we consider all the discourse sections using soft attention. The closest model to ours is that of See et al. (2017) and Paulus et al. (201"
N18-2097,N16-1012,0,0.715294,"tion j in the document consisting of a sequence of tokens.  (s) hj = RNNsec x(j,1) , ...x(j,M ) } at a time. Given an input document along with the corresponding ground-truth summary y, the ˆ that is model is trained to output a summary y close to y. The output at timestep t is predicted using the decoder input x0t , decoder hidden state (d) ht , and some information about the input sequence. This framework is the general seq2seq framework employed in many generation tasks including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014) and summarization (Nallapati et al., 2016; Chopra et al., 2016). Attentive decoding The attention mechanism maps the decoder state and the encoder states to an output vector, which is a weighted sum of the encoder states and is called context vector (Bahdanau et al., 2014). Incorporating this context vector at each decoding timestep (attentive decoding) is proven effective in seq2seq models. Formally, the context vector ct is defined as: ct = PN (t) (e) (t) i=1 αi hi where αi are the attention weights calculated as follows: i (3) We now describe our discourse-aware summarization model (shown in Figure 1). Encoder Our encoder extends the RNN encoder to a h"
N18-2097,W04-1013,0,0.304618,"ization. Our datasets are obtained from scientific papers. Scientific document summarization has been recently received extended attention (Qazvinian et al., 2013; Cohan and Goharian, 2015, 2017b,a). In contrast to ours, existing approaches are extractive and rely on external information such as citations, which may not be available for all papers. 5 6 Data Experiments Setup Similar to the majority of published research in the summarization literature (Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017), evaluation was done using the ROUGE automatic summarization evaluation metric (Lin, 2004) with full-length F-1 ROUGE scores. We lowercase all tokens and perform sentence and word tokenization using spaCy (Honnibal and Johnson, 2015). Implementation details We use Tensorflow 1.4 for implementing our models. We use the hyperparameters suggested by See et al. (2017). In particular, we use two bidirectional LSTMs with cell size of 256 and embedding dimensions of 128. Embeddings are trained from scratch and we did not find any gain using pre-trained embeddings. The vocabulary size is constrained to 50,000; using larger vocabulary size did not result in any improvement. We use mini-batc"
N18-2097,D15-1045,1,0.889316,"Missing"
N18-2097,W17-4505,0,0.0313005,"(d) > Neural abstractive summarization models have been studied in the past (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016) and later extended by source copying (Miao and Blunsom, 2016; See et al., 2017), reinformcement learning (Paulus et al., 2017), and sentence salience information (Li et al., 2017). One model variant of Nallapati et al. (2016) is related to our model in using sentence-level information in attention. However, our model is different as it contains a hierarchical encoder, uses discourse sections in the decoding step, and has a coverage mechanism. Similarly, Ling and Rush (2017) proposed a coarseto-fine attention model that uses hard attention to find the text chunks of importance and then only attend to words in that chunk. In contrast, we consider all the discourse sections using soft attention. The closest model to ours is that of See et al. (2017) and Paulus et al. (2017) who used a joint pointer-generator network for summarization. However, our model extends theirs by (i) a hierarchical encoder for modeling long documents and (ii) a discourse-aware decoder that captures the information flow from all discourse sections of the document. Finally, in a recent work,"
N18-2097,D15-1166,0,0.082704,"in and the nature of the document, they write about important points from different discourse sections of the document. For example, scientific paper abstracts typically include the description of the problem, discussion of the methods, and finally results and conclusions (Suppe, 1998). Motivated by this observation, we propose a discourse-aware attention method. Intuitively, at each decoding timestep, in addition to the words (1) where softmax means that the denominator’s sum i in the softmax function is over i. The score function can be defined in bilinear, additive, or multiplicative ways (Luong et al., 2015). We use the additive scoring function:  (e) (d) (e) (d) > Model RNN(.) denotes a function which is a recurrent neural network whose output is the final state of the network encoding the entire sequence. N is (s) the number of sections in the document and hj is representation of section j in the document consisting of a sequence of tokens.  (s) hj = RNNsec x(j,1) , ...x(j,M ) } at a time. Given an input document along with the corresponding ground-truth summary y, the ˆ that is model is trained to output a summary y close to y. The output at timestep t is predicted using the decoder input x0"
N18-2097,D16-1031,0,0.0162803,"e(h(j,i) , ht−1 ) (5) (i,j) The score function is the additive attention func(t) tion (Equation 2) and the weights βj are updated according to: (t) βj = (s) (d) softmax(score(hj , ht−1 )) j   (e) (t) (d) (t) (t) α(j,i) = softmax βj score(h(j,i) , cov(j,i) , ht−1 ) (6) (i,j) (d) 4 At each timestep t, the decoder state ht and the context vector ct are used to estimate the probability distribution of next word yt :  (d) > Neural abstractive summarization models have been studied in the past (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016) and later extended by source copying (Miao and Blunsom, 2016; See et al., 2017), reinformcement learning (Paulus et al., 2017), and sentence salience information (Li et al., 2017). One model variant of Nallapati et al. (2016) is related to our model in using sentence-level information in attention. However, our model is different as it contains a hierarchical encoder, uses discourse sections in the decoding step, and has a coverage mechanism. Similarly, Ling and Rush (2017) proposed a coarseto-fine attention model that uses hard attention to find the text chunks of importance and then only attend to words in that chunk. In contrast, we consider all the"
N18-2097,P16-1154,0,0.036547,"om all discourse sections of the document. Finally, in a recent work, Liu et al. (2018) proposed a model based on the transformer network (Vaswani et al., 2017) for abstractive generation of Wikipedia articles. However, their focus (7) p(yt |y1:t−1 ) = softmax V linear ht , ct where V is a vocabulary weight matrix and softmax is over the entire vocabulary. Copying from source There has been a surge of recent works in sequence learning tasks to address the problem of unkown token prediction by allowing the model to occasionally copy words directly from source instead of generating a new token (Gu et al., 2016; See et al., 2017; Paulus et al., 2017; Wiseman et al., 2017). Following these works, we add an additional binary variable zt to the decoder, indicating generating a word from vocabulary (zt =0) or copying a word from the source (zt =1). The probability is learnt during training according to the following equation: (d) p(zt =1|y1:t−1 ) = σ(linear(ht , ct , x0t )) (8) Then the next word yt is generated according to: X p(yt |y1:t−1 ) = p(yt , zt =z|y1:t−1 ); z = {0, 1} z The joint probability is decomposed as: ( pc (yt |y1:t−1 ) p(zt =z|y1:t−1 ), p(yt , zt =z) = pg (yt |y1:t−1 ) p(zt =z|y1:t−1"
N18-2097,K16-1028,0,0.534593,"is representation of section j in the document consisting of a sequence of tokens.  (s) hj = RNNsec x(j,1) , ...x(j,M ) } at a time. Given an input document along with the corresponding ground-truth summary y, the ˆ that is model is trained to output a summary y close to y. The output at timestep t is predicted using the decoder input x0t , decoder hidden state (d) ht , and some information about the input sequence. This framework is the general seq2seq framework employed in many generation tasks including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014) and summarization (Nallapati et al., 2016; Chopra et al., 2016). Attentive decoding The attention mechanism maps the decoder state and the encoder states to an output vector, which is a weighted sum of the encoder states and is called context vector (Bahdanau et al., 2014). Incorporating this context vector at each decoding timestep (attentive decoding) is proven effective in seq2seq models. Formally, the context vector ct is defined as: ct = PN (t) (e) (t) i=1 αi hi where αi are the attention weights calculated as follows: i (3) We now describe our discourse-aware summarization model (shown in Figure 1). Encoder Our encoder extends"
N18-2097,D15-1162,0,0.0135992,"ed attention (Qazvinian et al., 2013; Cohan and Goharian, 2015, 2017b,a). In contrast to ours, existing approaches are extractive and rely on external information such as citations, which may not be available for all papers. 5 6 Data Experiments Setup Similar to the majority of published research in the summarization literature (Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017), evaluation was done using the ROUGE automatic summarization evaluation metric (Lin, 2004) with full-length F-1 ROUGE scores. We lowercase all tokens and perform sentence and word tokenization using spaCy (Honnibal and Johnson, 2015). Implementation details We use Tensorflow 1.4 for implementing our models. We use the hyperparameters suggested by See et al. (2017). In particular, we use two bidirectional LSTMs with cell size of 256 and embedding dimensions of 128. Embeddings are trained from scratch and we did not find any gain using pre-trained embeddings. The vocabulary size is constrained to 50,000; using larger vocabulary size did not result in any improvement. We use mini-batches of size 16 and we limit the document length to 2000 and section length to 500 tokens, and number of sections to 4. We use batch-padding and"
N18-2097,D15-1044,0,0.115968,"(t) The scalar weights α(j,i) are obtained according to:   (t) (t) (e) (d) α(j,i) = softmax βj score(h(j,i) , ht−1 ) (5) (i,j) The score function is the additive attention func(t) tion (Equation 2) and the weights βj are updated according to: (t) βj = (s) (d) softmax(score(hj , ht−1 )) j   (e) (t) (d) (t) (t) α(j,i) = softmax βj score(h(j,i) , cov(j,i) , ht−1 ) (6) (i,j) (d) 4 At each timestep t, the decoder state ht and the context vector ct are used to estimate the probability distribution of next word yt :  (d) > Neural abstractive summarization models have been studied in the past (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016) and later extended by source copying (Miao and Blunsom, 2016; See et al., 2017), reinformcement learning (Paulus et al., 2017), and sentence salience information (Li et al., 2017). One model variant of Nallapati et al. (2016) is related to our model in using sentence-level information in attention. However, our model is different as it contains a hierarchical encoder, uses discourse sections in the decoding step, and has a coverage mechanism. Similarly, Ling and Rush (2017) proposed a coarseto-fine attention model that uses hard attention to find"
N18-2097,P17-1099,0,0.681695,"x` is defined as: X (t) α(j,i) (9) pc (yt = x` |y1:t−1 ) = in the document, we also attend to the relevant discourse section (the “section attention” block in Figure 1). Then we use the discourse-related information to modify the word-level attention function. Specifically, the context vector representing the source document is: XN XM (t) (e) α(j,i) h(j,i) (4) ct = j=1 (j,i):x(j,i) =x` Decoder coverage In long sequences, the neural generation models tend to repeat phrases where the softmax layer predicts the same phrase multiple times over multiple timesteps. To address this issue, following See et al. (2017), we track attention coverage to avoid repeatedly attending to the same steps. This is done with a coverage vector cov(t) , the sum of attention weight vectors at prePt−1 (k) vious timesteps: cov(t) k=0 α(j,i) (j,i) = The coverage implicitly includes information about the attended document discourse sections. We incorporate the decoder coverage as an additional input to the attention function: i=1 (e) where h(j,i) shows the encoder state of word i (t) in discourse section j and α(j,i) shows the corresponding attention weight to that encoder state. (t) The scalar weights α(j,i) are obtained acc"
N18-2097,D17-1235,0,0.0315041,"017). These approaches employ a general framework of sequence-to-sequence (seq2seq) models (Sutskever et al., 2014) where the document is fed to an encoder network and another (recurrent) network learns to decode the summary. While promising, these methods focus on summarizing news articles which are relatively short. Many other document types, however, are longer and structured. Seq2seq models tend to struggle with longer sequences because at each decoding step, the decoder needs to learn to construct a context vector capturing relevant information from all the tokens in the source sequence (Shao et al., 2017). Our main contribution is an abstractive model for summarizing scientific papers which are an example of long-form structured document types. Our model includes a hierarchical encoder, capturing the discourse structure of the document and a discourse-aware decoder that generates the summary. Our decoder attends to different discourse sections and allows the model to more accurately represent important information from the source resulting in a better context vector. We also introduce two large-scale datasets of long and structured scientific papers obtained from arXiv and PubMed to support bo"
N18-2097,D17-1239,0,0.0207893,"recent work, Liu et al. (2018) proposed a model based on the transformer network (Vaswani et al., 2017) for abstractive generation of Wikipedia articles. However, their focus (7) p(yt |y1:t−1 ) = softmax V linear ht , ct where V is a vocabulary weight matrix and softmax is over the entire vocabulary. Copying from source There has been a surge of recent works in sequence learning tasks to address the problem of unkown token prediction by allowing the model to occasionally copy words directly from source instead of generating a new token (Gu et al., 2016; See et al., 2017; Paulus et al., 2017; Wiseman et al., 2017). Following these works, we add an additional binary variable zt to the decoder, indicating generating a word from vocabulary (zt =0) or copying a word from the source (zt =1). The probability is learnt during training according to the following equation: (d) p(zt =1|y1:t−1 ) = σ(linear(ht , ct , x0t )) (8) Then the next word yt is generated according to: X p(yt |y1:t−1 ) = p(yt , zt =z|y1:t−1 ); z = {0, 1} z The joint probability is decomposed as: ( pc (yt |y1:t−1 ) p(zt =z|y1:t−1 ), p(yt , zt =z) = pg (yt |y1:t−1 ) p(zt =z|y1:t−1 ), Related work z=1 z=0 pg is the probability of generating a"
N18-2097,J02-4006,0,\N,Missing
P10-2057,W09-3934,1,0.734375,"matic decision detection component is critical for re-using meeting archives. With the new availability of substantial meeting corpora such as the AMI corpus (McCowan et al., 2005), recent years have seen an increasing amount of research on decisionmaking dialogue. This research has tackled issues such as the automatic detection of agreement and disagreement (Galley et al., 2004), and of the level of involvement of conversational participants (Gatica-Perez et al., 2005). Recent work on automatic detection of decisions has been conducted by Hsueh and Moore (2007), Fern´andez et al. (2008), and Bui et al. (2009). Fern´andez et al. (2008) proposed an approach to modeling the structure of decision-making dialogue. These authors designed an annotation scheme that takes account of the different roles that utterances can play in the decision-making process—for example it distinguishes between DDAs that initiate a decision discussion by raising an issue, those that propose a resolution of the issue, and those that express agreement to a proposed resolution. The authors annotated a portion of the AMI corpus, and then applied what Introduction In work environments, people share information and make decisions"
P10-2057,W09-3951,0,0.0298489,"ral Language Processing (NLP), and information extraction. In particular, Dynamic Bayesian Networks (DBNs) are a popular model for probabilistic sequence modeling because they exploit structure in the problem to compactly represent distributions over multi-state and observation variables. Hidden Markov Models (HMMs), a special case of DBNs, are a classical method for important NLP applications such as unsupervised part-of-speech tagging (Gael et al., 2009) and grammar induction (Johnson et al., 2007) as well as for ASR. More complex DBNs have been used for applications such as DA recognition (Crook et al., 2009) and activity recognition (Bui et al., 2002). Undirected graphical models (UGMs) are also valuable for building probabilistic models for segmenting and labeling sequence data. Conditional 1 308 http://corpus.amiproject.org/ (1) A: Are we going to have a backup? Or we do just– B: But would a backup really be necessary? A: I think maybe we could just go for the kinetic energy and be bold and innovative. C: Yeah. B: I think– yeah. A: It could even be one of our selling points. C: Yeah –laugh–. D: Environmentally conscious or something. A: Yeah. B: Okay, fully kinetic energy. D: Good. (UTT) such a"
P10-2057,W08-0125,1,0.750552,"Missing"
P10-2057,D09-1071,0,0.0647444,"Missing"
P10-2057,P04-1085,0,0.108318,"rom 0.55. 1 2 Related work User studies (Banerjee et al., 2005) have confirmed that meeting participants consider decisions to be one of the most important meeting outputs, and Whittaker et al. (2006) found that the development of an automatic decision detection component is critical for re-using meeting archives. With the new availability of substantial meeting corpora such as the AMI corpus (McCowan et al., 2005), recent years have seen an increasing amount of research on decisionmaking dialogue. This research has tackled issues such as the automatic detection of agreement and disagreement (Galley et al., 2004), and of the level of involvement of conversational participants (Gatica-Perez et al., 2005). Recent work on automatic detection of decisions has been conducted by Hsueh and Moore (2007), Fern´andez et al. (2008), and Bui et al. (2009). Fern´andez et al. (2008) proposed an approach to modeling the structure of decision-making dialogue. These authors designed an annotation scheme that takes account of the different roles that utterances can play in the decision-making process—for example it distinguishes between DDAs that initiate a decision discussion by raising an issue, those that propose a"
P10-2057,N07-1018,0,0.0183509,"d for labeling and segmenting sequences of observations in many different fields—including bioinformatics, ASR, Natural Language Processing (NLP), and information extraction. In particular, Dynamic Bayesian Networks (DBNs) are a popular model for probabilistic sequence modeling because they exploit structure in the problem to compactly represent distributions over multi-state and observation variables. Hidden Markov Models (HMMs), a special case of DBNs, are a classical method for important NLP applications such as unsupervised part-of-speech tagging (Gael et al., 2009) and grammar induction (Johnson et al., 2007) as well as for ASR. More complex DBNs have been used for applications such as DA recognition (Crook et al., 2009) and activity recognition (Bui et al., 2002). Undirected graphical models (UGMs) are also valuable for building probabilistic models for segmenting and labeling sequence data. Conditional 1 308 http://corpus.amiproject.org/ (1) A: Are we going to have a backup? Or we do just– B: But would a backup really be necessary? A: I think maybe we could just go for the kinetic energy and be bold and innovative. C: Yeah. B: I think– yeah. A: It could even be one of our selling points. C: Yeah"
P10-2057,2007.sigdial-1.4,1,0.806118,"f speech labeling problem; they showed that this model performs better than a nonhierarchical counterpart. they refer to as “hierarchical classification.” Here, one sub-classifier per DDA class hypothesizes occurrences of that type of DDA and then, based on these hypotheses, a super-classifier determines which regions of dialogue are decision discussions. All of the classifiers, (sub and super), were linear kernel binary SVMs. Results were better than those obtained with (Hsueh and Moore, 2007)’s approach—the F1-score for detecting decision discussions in manual transcripts was 0.58 vs. 0.50. Purver et al. (2007) had earlier detected action items with the approach Fern´andez et al. (2008) extended to decisions. 3 Data Bui et al. (2009) built on the promising results of (Fern´andez et al., 2008), by employing DGMs in place of SVMs. DGMs are attractive because they provide a natural framework for modeling sequence and dependencies between variables, including the DDAs. Bui et al. (2009) were especially interested in whether DGMs better exploit non-lexical features. Fern´andez et al. (2008) obtained much more value from lexical than nonlexical features (and indeed no value at all from prosodic features),"
P19-1182,D16-1125,0,0.0208753,"., 2016; Johnson et al., 2017). In terms of model progress, recent years witnessed strong research progress in generating natural language sentences to describe visual contents, such as Vinyals et al. (2015); Xu et al. (2015); Ranzato et al. (2016); Anderson et al. (2018) in single image captioning, Venugopalan et al. (2015); Pan et al. (2016); Pasunuru and Bansal (2017) in video captioning, Mao et al. (2016); Liu et al. (2017a); Yu et al. (2017); Luo and Shakhnarovich (2017) in referring expressions, Jain et al. (2017); Li et al. (2018); Misra et al. (2018) in visual question generation, and Andreas and Klein (2016); CohnGordon et al. (2018); Luo et al. (2018); Vedantam et al. (2017) in other setups. Single image captioning is the most relevant problem to the two-images captioning. Vinyals et al. (2015) created a powerful encoder-decoder (i.e., CNN to LSTM) framework in solving the captioning problem. Xu et al. (2015) further equipped it with an attention module to handle the memorylessness of fixed-size vectors. Ranzato et al. (2016) used reinforcement learning to eliminate exposure bias. Recently, Anderson et al. (2018) brought the information from object detection system to further boost the performan"
P19-1182,W05-0909,0,0.384545,", we compare the performance of our models on all three datasets with various automated metrics. Results on the test sets are reported. Following the setup in Jhamtani and Berg-Kirkpatrick (2018), we takes CIDEr (Vedantam et al., 2015) as the main metric in evaluating the Spot-the-Diff and NLVR2 datasets. However, CIDEr is known as its problem in up-weighting unimportant details (Kilickaya et al., 2017; Liu et al., 2017b). In our dataset, we find that instructions generated from a small set of short phrases could get a high CIDEr score. We thus change the main metric of our dataset to METEOR (Banerjee and Lavie, 2005), which is manually verified to be aligned with human judgment on the validation set in our dataset. To avoid over-fitting, the model is 1879 Ours(IEdit) Spot-the-Diff NLVR2 Basic 11 22 24 Full 24 37 37 Both Good 5 6 17 Both Not 60 35 22 Table 3: Human evaluation on 100 examples. Image pair and two captions generated by our basic model and full model are shown to the user. The user chooses one from ‘Basic’ model wins, ‘Full’ model wins, ‘Both Good’, or ‘Both Not’. Better model marked in bold font. we do not explicitly model the pixel-level differences; however, we still find that the model cou"
P19-1182,N18-1150,0,0.0223056,"e also report the BLEU-4 (Papineni et al., 2002) and ROUGE-L (Lin, 2004) scores. The results on various datasets shows the gradual improvement made by our novel neural components, which are designed to better describe the relationship between 2 images. Our full model has a significant improvement in result over baseline. The improvement on the NLVR2 dataset is limited because the comparison of two images was not forced to be considered when generating instructions. 4.3 Human Evaluation and Qualitative Analysis We conduct a pairwise human evaluation on our generated sentences, which is used in Celikyilmaz et al. (2018) and Pasunuru and Bansal (2017). Agarwala (2018) also shows that the pairwise comparison is better than scoring sentences individually. We randomly select 100 examples from the test set in each dataset and generate captions via our full speaker model. We ask users to choose a better instruction between the captions generated by our full model and the basic model, or alternatively indicate that the two captions are equal in quality. The Image Editing Request dataset is specifically annotated by the image editing expert. The winning rate of our full model (dynamic relation attention) versus the"
P19-1182,N18-2070,0,0.0416472,"Missing"
P19-1182,D18-1436,0,0.345133,"Missing"
P19-1182,D14-1086,0,0.420249,"n social 1 Our data and code are publicly available at: https://github.com/airsplay/ VisualRelationships media, etc. This task has drawn significant attention in the research community with numerous studies (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018), and recent state of the art methods have achieved promising results on large captioning datasets, such as MS COCO (Lin et al., 2014). Besides single image captioning, the community has also explored other visual captioning problems such as video captioning (Venugopalan et al., 2015; Xu et al., 2016), and referring expressions (Kazemzadeh et al., 2014; Yu et al., 2017). However, the problem of two-image captioning, especially the task of describing the relationships and differences between two images, is still underexplored. In this paper, we focus on advancing research in this challenging problem by introducing a new dataset and proposing novel neural relational-speaker models.2 To the best of our knowledge, Jhamtani and Berg-Kirkpatrick (2018) is the only public dataset aimed at generating natural language descriptions for two real images. This dataset is about ‘spotting the difference’, and hence focuses more on describing exhaustive di"
P19-1182,N18-2120,0,0.029005,"e DDLA (Difference Description with Latent Alignment) method proposed in Jhamtani and Berg-Kirkpatrick (2018) learns the alignment between descriptions and visual differences. It relies on the nature of the particular dataset and thus could not be easily transferred to other dataset where the visual relationship is not obvious. The two-images captioning could also be considered as a two key-frames video captioning problem, and our sequential multi-heads attention is a modified version of the seq-to-seq model (Venugopalan et al., 2015). Some existing work (Chen et al., 2018; Wang et al., 2018; Manjunatha et al., 2018) also learns how to modify images. These datasets and methods focus on the image colorization and adjustment tasks, while our dataset aims to study the general image editing request task. 6 Conclusion In this paper, we explored the task of describing the visual relationship between two images. We collected the Image Editing Request dataset, which contains image pairs and human annotated editing instructions. We designed novel relational speaker models and evaluate them on our collected and other public existing dataset. Based on automatic and human evaluations, our relational speaker model imp"
P19-1182,P02-1040,0,0.103959,"The user chooses one from ‘Basic’ model wins, ‘Full’ model wins, ‘Both Good’, or ‘Both Not’. Better model marked in bold font. we do not explicitly model the pixel-level differences; however, we still find that the model could learn these differences in the Spot-the-Diff dataset. Since the descriptions in Spot-the-Diff is relatively simple, the errors mostly come from wrong entities or undetected differences as shown in Fig. 5. Our model is also sensitive to the image contents as shown in the NLVR2 dataset. 5 early-stopped based on the main metric on validation set. We also report the BLEU-4 (Papineni et al., 2002) and ROUGE-L (Lin, 2004) scores. The results on various datasets shows the gradual improvement made by our novel neural components, which are designed to better describe the relationship between 2 images. Our full model has a significant improvement in result over baseline. The improvement on the NLVR2 dataset is limited because the comparison of two images was not forced to be considered when generating instructions. 4.3 Human Evaluation and Qualitative Analysis We conduct a pairwise human evaluation on our generated sentences, which is used in Celikyilmaz et al. (2018) and Pasunuru and Bansa"
P19-1182,D17-1103,1,0.908355,"neni et al., 2002) and ROUGE-L (Lin, 2004) scores. The results on various datasets shows the gradual improvement made by our novel neural components, which are designed to better describe the relationship between 2 images. Our full model has a significant improvement in result over baseline. The improvement on the NLVR2 dataset is limited because the comparison of two images was not forced to be considered when generating instructions. 4.3 Human Evaluation and Qualitative Analysis We conduct a pairwise human evaluation on our generated sentences, which is used in Celikyilmaz et al. (2018) and Pasunuru and Bansal (2017). Agarwala (2018) also shows that the pairwise comparison is better than scoring sentences individually. We randomly select 100 examples from the test set in each dataset and generate captions via our full speaker model. We ask users to choose a better instruction between the captions generated by our full model and the basic model, or alternatively indicate that the two captions are equal in quality. The Image Editing Request dataset is specifically annotated by the image editing expert. The winning rate of our full model (dynamic relation attention) versus the basic model is shown in Table 3"
P19-1182,P19-1644,0,0.199322,"further extend it by designing a dynamic relational attention module to combine the advantages of these two components, which finds the relationship between two images while decoding. The computation of dynamic relational attention is mathematically equivalent to attention over all visual “relationships”. Thus, our method provides a direct way to model visual relationships in language. To show the effectiveness of our models, we evaluate them on three datasets: our new dataset, the ”Spot-the-Diff” dataset (Jhamtani and BergKirkpatrick, 2018), and the two-image visual reasoning NLVR2 dataset (Suhr et al., 2019) (adapted for our task). We train models separately on each dataset with the same hyper-parameters and evaluate them on the same test set across all methods. Experimental results demonstrate that our model outperforms all the baselines and existing methods. The main contributions of our paper are: (1) We create a novel human language guided image editing dataset to boost the study in describing visual relationships; (2) We design novel relationalspeaker models, including a dynamic relational attention module, to handle the problem of twoimage captioning by focusing on all their visual relation"
W09-3934,P93-1008,1,0.451146,"on spoken dialogue summarization (e.g. (Zechner, 2002)). Most has attempted to generate summaries of full dialogues, but some very recent research has focused on specific dialogue events, namely action items (Purver et al., 2007), and decisions (Fern´andez et al., 2008a). (Fern´andez et al., 2008a) used the DDA annotation scheme mentioned above, and began by extracting the DDAs which raise issues or provide accepted resolutions. Only manual transcripts were used and the DDAs were extracted by hand rather than automatically. The next step was to parse each DDA with a general rule-based parser (Dowding et al., 1993), producing multiple short fragments rather than one full utterance parse. Then, for each DDA, an SVM regression model used various features (including parse, semantic and lexical features) to select the fragment which was most likely to appear in a gold-standard extractive decision summary. The entire manual utterance transcriptions were used as the baseline, and although the SVM’s precision was high, it was not enough to offset the baseline’s perfect recall, and so its F-score was lower. The “Oracle”, which always chooses the fragment with the highest F1score produced very good results. This"
W09-3934,2007.sigdial-1.4,1,0.884157,"The authors annotated a portion of the AMI corpus, and then applied what they refer to as “hierarchical classification”. Here, one sub-classifier per DDA class hypothesizes occurrences of that DDA class, and then based on these hypotheses, a super-classifier determines which regions of dialogue are decision discussions. All of the classifiers, (sub and super), were linear kernel binary Support Vector Machines (SVMs). Results were better than those obtained with (Hsueh and Moore, 2007)’s approach—the F1-score for detecting decision discussions in manual transcripts was .58 vs. .50. Note that (Purver et al., 2007) had previously pursued the same basic approach as (Fern´andez et al., 2008b) in order to detect action items. In this paper, we build on the promising results Decision summarization: Recent years have seen research on spoken dialogue summarization (e.g. (Zechner, 2002)). Most has attempted to generate summaries of full dialogues, but some very recent research has focused on specific dialogue events, namely action items (Purver et al., 2007), and decisions (Fern´andez et al., 2008a). (Fern´andez et al., 2008a) used the DDA annotation scheme mentioned above, and began by extracting the DDAs whi"
W09-3934,W08-0125,1,0.854754,"Missing"
W09-3934,P04-1085,0,0.022285,"Related Work User studies (Banerjee et al., 2005) have confirmed that meeting participants consider decisions to be one of the most important meeting outputs, and (Whittaker et al., 2006) found that the development of an automatic decision detection component is critical to the re-use of meeting archives. With the new availability of substantial meeting corpora such as the AMI corpus (McCowan et al., 2005), recent years have therefore seen an increasing amount of research on decisionmaking dialog. This research has tackled issues such as the automatic detection of agreement and disagreement (Galley et al., 2004), and of the Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 235–243, c Queen Mary University of London, September 2009. 2009 Association for Computational Linguistics 235 level of involvement of conversational participants (Gatica-Perez et al., 2005). In addition, (Verbree et al., 2006) created an argumentation scheme intended to support automatic production of argument structure diagrams from decision-oriented meeting transcripts. As yet, there has been relatively little work which specifically addresses the automatic detect"
W09-3934,J02-4003,0,0.347915,"regions of dialogue are decision discussions. All of the classifiers, (sub and super), were linear kernel binary Support Vector Machines (SVMs). Results were better than those obtained with (Hsueh and Moore, 2007)’s approach—the F1-score for detecting decision discussions in manual transcripts was .58 vs. .50. Note that (Purver et al., 2007) had previously pursued the same basic approach as (Fern´andez et al., 2008b) in order to detect action items. In this paper, we build on the promising results Decision summarization: Recent years have seen research on spoken dialogue summarization (e.g. (Zechner, 2002)). Most has attempted to generate summaries of full dialogues, but some very recent research has focused on specific dialogue events, namely action items (Purver et al., 2007), and decisions (Fern´andez et al., 2008a). (Fern´andez et al., 2008a) used the DDA annotation scheme mentioned above, and began by extracting the DDAs which raise issues or provide accepted resolutions. Only manual transcripts were used and the DDAs were extracted by hand rather than automatically. The next step was to parse each DDA with a general rule-based parser (Dowding et al., 1993), producing multiple short fragme"
W09-3934,H93-1008,1,\N,Missing
W18-3105,D17-1122,0,0.59262,"the performance of our model is shown to be comparable to a more complex state-of-the-art baseline. 1 2 2.1 Related Work Answer Selection Answer selection is an active research field and has drawn a lot of attention. Given a question and a set of candidate answers, the task is to identify which of the candidates contains the correct answer to the question. Two types of deep learning frameworks have been proposed for tackling the answer selection problem. One is the Siamese framework (Bromley et al., 1993) and the other is the Compare-Aggregate framework (Wang et al., 2017; Bian et al., 2017; Shen et al., 2017). In the Siamese framework, the same encoder (e.g., a CNN or a RNN) is used to map each input sentence to a vector representation individually. After that, the final output is determined solely based on the encoded vectors. There is no explicit interaction between the sentences during the encoding process. On the other hand, the CompareAggregate framework aims to capture more interactive features between sentences in consideration, therefore typically has better performance when evaluated on public datasets such as TrecQA (Wang et al., 2007) and WikiQA (Yang et al., 2015). Introduction Custome"
W18-3105,P17-4017,0,0.295706,". We formalize the task as follows: Given a question Q about a product P and the list of specifications (s1 , s2 , ..., sM ) of P , the goal is to identify the specification that is most relevant to Q. M is the number of specifications of P , and si is the ith specification of P . In this formulation, the task is similar to the answer selection problem (Rao et al., 2016; Bian et al., 2017; Shen et al., 2017). ‘Answers’ shall be individual product specifications in this case. After identifying the most relevant specification, the final response sentence is generated using predefined templates (Cui et al., 2017). Figure 1 illus38 Proceedings of the First Workshop on Economics and Natural Language Processing, pages 38–43 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics Figure 1: Answering questions regarding product facts and specifications 2.2 Customer Service Chatbot A common trait of a number of recent state-ofthe-art methods for answer selection is the use of the Compare-Aggregate architecture (Wang et al., 2017; Bian et al., 2017; Shen et al., 2017). Under this architecture, vector representations of smaller units (such as words) of the input sentences are com"
W18-3105,P15-1150,0,0.171296,"Missing"
W18-3105,D07-1003,0,0.798296,"ate framework (Wang et al., 2017; Bian et al., 2017; Shen et al., 2017). In the Siamese framework, the same encoder (e.g., a CNN or a RNN) is used to map each input sentence to a vector representation individually. After that, the final output is determined solely based on the encoded vectors. There is no explicit interaction between the sentences during the encoding process. On the other hand, the CompareAggregate framework aims to capture more interactive features between sentences in consideration, therefore typically has better performance when evaluated on public datasets such as TrecQA (Wang et al., 2007) and WikiQA (Yang et al., 2015). Introduction Customers ask many questions before buying products. Developing a general question answering system to assist customers is challenging, due to the diversity of questions. In this paper, we focus on the task of answering questions regarding product facts and specifications. We formalize the task as follows: Given a question Q about a product P and the list of specifications (s1 , s2 , ..., sM ) of P , the goal is to identify the specification that is most relevant to Q. M is the number of specifications of P , and si is the ith specification of P ."
W18-3105,D14-1162,0,0.0849795,"tecture Given a question and a set of candidate specifications, the goal is to identify the most relevant specification. We aim to train a classifier that takes a question and a specification name as input and predicts whether the specification is relevant to the question. During inference, given a question, the trained classifier is used to assign a score to every candidate specification based on how relevant the specification is. After that, the top-ranked specification is selected. 1 Word Representation Layer. Using word embeddings pre-trained with word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), we transform Q and S into two sequences Qe = Q Q S S S [eQ 1 , e2 , ..., em ] and Se = [e1 , e2 , ..., en ], where Q ei is the embedding of the ith word of the question and eSj is the embedding of the j th word of the specification name. m and n are the lengths of Q and S, respectively. BiLSTM Layer. https://shopbot.ebay.com 39 We use a bi-directional Figure 2: Architecture of our model LSTM (Hochreiter and Schmidhuber, 1997) to obtain a context-aware vector representation for each position of Q and S. We feed Qe and Se individually into a parameter shared bi-directional LSTM model. For the"
W18-4701,E09-1022,0,0.0174719,"lso involve a gamified scenario with the interlocutors playing a yes-no questionanswer game as in de Vries et al. (2017). In these works the focus is less on the dialogue aspects and more on the factual aspects of the images, i.e., if an object is present or what a certain component of the image is. Mostafazadeh et al. (2017) extended this line of work with conversations grounded on images. Furthermore, Huang et al. (2016) built a data set of images with corresponding descriptions in sequence, for the task of visual storytelling. Other gamified real-world scenarios involve object arrangement (DeVault and Stone, 2009), puzzle completion (Iida et al., 2010; Takenobu et al., 2012), map navigation (Anderson et al., 1991; Lemon et al., 2001; Johnston et al., 2002), furniture-buying scenarios (Di Eugenio et al., 2000), and treasure-hunt tasks in a virtual environment (Byron and Fosler-Lussier, 2006). A multi-modal interface for image editing combining speech and direct manipulation was developed by (Laput et al., 2013). With this interface a user can for example select a person’s hat in an image and say “this is a hat”. Then the system learns to associate the tag “hat” with the selected region of the image. Fin"
W18-4701,N16-1147,0,0.0178558,"nvolving dialogue and vision has been in the context of answering factual questions on images (Das et al., 2017; Antol et al., 2015) using the MSCOCO data set (Lin et al., 2014). The task may also involve a gamified scenario with the interlocutors playing a yes-no questionanswer game as in de Vries et al. (2017). In these works the focus is less on the dialogue aspects and more on the factual aspects of the images, i.e., if an object is present or what a certain component of the image is. Mostafazadeh et al. (2017) extended this line of work with conversations grounded on images. Furthermore, Huang et al. (2016) built a data set of images with corresponding descriptions in sequence, for the task of visual storytelling. Other gamified real-world scenarios involve object arrangement (DeVault and Stone, 2009), puzzle completion (Iida et al., 2010; Takenobu et al., 2012), map navigation (Anderson et al., 1991; Lemon et al., 2001; Johnston et al., 2002), furniture-buying scenarios (Di Eugenio et al., 2000), and treasure-hunt tasks in a virtual environment (Byron and Fosler-Lussier, 2006). A multi-modal interface for image editing combining speech and direct manipulation was developed by (Laput et al., 201"
W18-4701,P10-1128,0,0.0270435,"rlocutors playing a yes-no questionanswer game as in de Vries et al. (2017). In these works the focus is less on the dialogue aspects and more on the factual aspects of the images, i.e., if an object is present or what a certain component of the image is. Mostafazadeh et al. (2017) extended this line of work with conversations grounded on images. Furthermore, Huang et al. (2016) built a data set of images with corresponding descriptions in sequence, for the task of visual storytelling. Other gamified real-world scenarios involve object arrangement (DeVault and Stone, 2009), puzzle completion (Iida et al., 2010; Takenobu et al., 2012), map navigation (Anderson et al., 1991; Lemon et al., 2001; Johnston et al., 2002), furniture-buying scenarios (Di Eugenio et al., 2000), and treasure-hunt tasks in a virtual environment (Byron and Fosler-Lussier, 2006). A multi-modal interface for image editing combining speech and direct manipulation was developed by (Laput et al., 2013). With this interface a user can for example select a person’s hat in an image and say “this is a hat”. Then the system learns to associate the tag “hat” with the selected region of the image. Finally, Manuvinakurike et al. (2018a) re"
W18-4701,P02-1048,0,0.161474,"is less on the dialogue aspects and more on the factual aspects of the images, i.e., if an object is present or what a certain component of the image is. Mostafazadeh et al. (2017) extended this line of work with conversations grounded on images. Furthermore, Huang et al. (2016) built a data set of images with corresponding descriptions in sequence, for the task of visual storytelling. Other gamified real-world scenarios involve object arrangement (DeVault and Stone, 2009), puzzle completion (Iida et al., 2010; Takenobu et al., 2012), map navigation (Anderson et al., 1991; Lemon et al., 2001; Johnston et al., 2002), furniture-buying scenarios (Di Eugenio et al., 2000), and treasure-hunt tasks in a virtual environment (Byron and Fosler-Lussier, 2006). A multi-modal interface for image editing combining speech and direct manipulation was developed by (Laput et al., 2013). With this interface a user can for example select a person’s hat in an image and say “this is a hat”. Then the system learns to associate the tag “hat” with the selected region of the image. Finally, Manuvinakurike et al. (2018a) recently introduced a corpus containing one-shot image editing instructions. 3 Data The task of image editing"
W18-4701,D14-1086,0,0.0765644,"ersation in the context of visual information has been studied for a long time. Clark and WilkesGibbs (1986) studied reference resolution of simple figures called tangrams. Kennington and Schlangen (2015) and Manuvinakurike et al. (2016) performed incremental understanding and incremental reference resolution respectively in a domain of geometric shape descriptions, while Schlangen et al. (2016) resolved references to objects in real-world example images. Much work has been done in the context of gamified scenarios where the interlocutors interact and resolve references to real-world objects (Kazemzadeh et al., 2014; Paetzel et al., 2014; Manuvinakurike and DeVault, 2015). Also, such gamified scenarios have served as platforms for developing/learning incremental dialogue policies regarding whether the system should respond immediately or wait for more information (Paetzel et al., 2015; Manuvinakurike et al., 2017). Referential domains in the context of dialogue have also been studied using virtual reality technologies and spatial constraints (Stoia et al., 2008; Das et al., 2018) as well as robots (Whitney et al., 2016; Skantze, 2017). A more recent direction of research involving dialogue and vision has"
W18-4701,P15-1029,0,0.0167866,"the role of a future dialogue system). We introduce a novel annotation scheme for this task, and discuss challenging sub-tasks in this domain. Conversational image editing combines spoken language, dialogue, and computer vision, and our real-world domain extends the literature on domains that are at the intersection of language and computer vision. We will publicly release our corpus in the near future. 2 Related Work Conversation in the context of visual information has been studied for a long time. Clark and WilkesGibbs (1986) studied reference resolution of simple figures called tangrams. Kennington and Schlangen (2015) and Manuvinakurike et al. (2016) performed incremental understanding and incremental reference resolution respectively in a domain of geometric shape descriptions, while Schlangen et al. (2016) resolved references to objects in real-world example images. Much work has been done in the context of gamified scenarios where the interlocutors interact and resolve references to real-world objects (Kazemzadeh et al., 2014; Paetzel et al., 2014; Manuvinakurike and DeVault, 2015). Also, such gamified scenarios have served as platforms for developing/learning incremental dialogue policies regarding whe"
W18-4701,W16-3630,0,0.0131278,"m). We introduce a novel annotation scheme for this task, and discuss challenging sub-tasks in this domain. Conversational image editing combines spoken language, dialogue, and computer vision, and our real-world domain extends the literature on domains that are at the intersection of language and computer vision. We will publicly release our corpus in the near future. 2 Related Work Conversation in the context of visual information has been studied for a long time. Clark and WilkesGibbs (1986) studied reference resolution of simple figures called tangrams. Kennington and Schlangen (2015) and Manuvinakurike et al. (2016) performed incremental understanding and incremental reference resolution respectively in a domain of geometric shape descriptions, while Schlangen et al. (2016) resolved references to objects in real-world example images. Much work has been done in the context of gamified scenarios where the interlocutors interact and resolve references to real-world objects (Kazemzadeh et al., 2014; Paetzel et al., 2014; Manuvinakurike and DeVault, 2015). Also, such gamified scenarios have served as platforms for developing/learning incremental dialogue policies regarding whether the system should respond im"
W18-4701,W17-5539,1,0.843575,"resolution respectively in a domain of geometric shape descriptions, while Schlangen et al. (2016) resolved references to objects in real-world example images. Much work has been done in the context of gamified scenarios where the interlocutors interact and resolve references to real-world objects (Kazemzadeh et al., 2014; Paetzel et al., 2014; Manuvinakurike and DeVault, 2015). Also, such gamified scenarios have served as platforms for developing/learning incremental dialogue policies regarding whether the system should respond immediately or wait for more information (Paetzel et al., 2015; Manuvinakurike et al., 2017). Referential domains in the context of dialogue have also been studied using virtual reality technologies and spatial constraints (Stoia et al., 2008; Das et al., 2018) as well as robots (Whitney et al., 2016; Skantze, 2017). A more recent direction of research involving dialogue and vision has been in the context of answering factual questions on images (Das et al., 2017; Antol et al., 2015) using the MSCOCO data set (Lin et al., 2014). The task may also involve a gamified scenario with the interlocutors playing a yes-no questionanswer game as in de Vries et al. (2017). In these works the fo"
W18-4701,L18-1683,1,0.883106,"Missing"
W18-4701,W18-5033,1,0.93435,"le completion (Iida et al., 2010; Takenobu et al., 2012), map navigation (Anderson et al., 1991; Lemon et al., 2001; Johnston et al., 2002), furniture-buying scenarios (Di Eugenio et al., 2000), and treasure-hunt tasks in a virtual environment (Byron and Fosler-Lussier, 2006). A multi-modal interface for image editing combining speech and direct manipulation was developed by (Laput et al., 2013). With this interface a user can for example select a person’s hat in an image and say “this is a hat”. Then the system learns to associate the tag “hat” with the selected region of the image. Finally, Manuvinakurike et al. (2018a) recently introduced a corpus containing one-shot image editing instructions. 3 Data The task of image editing is challenging for the following reasons: (i) The user needs to understand whether changes applied to a given image fit the target narrative or not. (ii) Image editing is a time consuming task. The user typically experiments with various features often undoing, redoing, altering in increments, or even completely removing previously performed edits before settling on the final image edit. (iii) The users may know at an abstract level what changes they want to perform, but be unaware"
W18-4701,I17-1047,0,0.0261199,"al., 2018) as well as robots (Whitney et al., 2016; Skantze, 2017). A more recent direction of research involving dialogue and vision has been in the context of answering factual questions on images (Das et al., 2017; Antol et al., 2015) using the MSCOCO data set (Lin et al., 2014). The task may also involve a gamified scenario with the interlocutors playing a yes-no questionanswer game as in de Vries et al. (2017). In these works the focus is less on the dialogue aspects and more on the factual aspects of the images, i.e., if an object is present or what a certain component of the image is. Mostafazadeh et al. (2017) extended this line of work with conversations grounded on images. Furthermore, Huang et al. (2016) built a data set of images with corresponding descriptions in sequence, for the task of visual storytelling. Other gamified real-world scenarios involve object arrangement (DeVault and Stone, 2009), puzzle completion (Iida et al., 2010; Takenobu et al., 2012), map navigation (Anderson et al., 1991; Lemon et al., 2001; Johnston et al., 2002), furniture-buying scenarios (Di Eugenio et al., 2000), and treasure-hunt tasks in a virtual environment (Byron and Fosler-Lussier, 2006). A multi-modal inter"
W18-4701,paetzel-etal-2014-multimodal,0,0.0225781,"f visual information has been studied for a long time. Clark and WilkesGibbs (1986) studied reference resolution of simple figures called tangrams. Kennington and Schlangen (2015) and Manuvinakurike et al. (2016) performed incremental understanding and incremental reference resolution respectively in a domain of geometric shape descriptions, while Schlangen et al. (2016) resolved references to objects in real-world example images. Much work has been done in the context of gamified scenarios where the interlocutors interact and resolve references to real-world objects (Kazemzadeh et al., 2014; Paetzel et al., 2014; Manuvinakurike and DeVault, 2015). Also, such gamified scenarios have served as platforms for developing/learning incremental dialogue policies regarding whether the system should respond immediately or wait for more information (Paetzel et al., 2015; Manuvinakurike et al., 2017). Referential domains in the context of dialogue have also been studied using virtual reality technologies and spatial constraints (Stoia et al., 2008; Das et al., 2018) as well as robots (Whitney et al., 2016; Skantze, 2017). A more recent direction of research involving dialogue and vision has been in the context o"
W18-4701,W15-4610,0,0.0243606,"incremental reference resolution respectively in a domain of geometric shape descriptions, while Schlangen et al. (2016) resolved references to objects in real-world example images. Much work has been done in the context of gamified scenarios where the interlocutors interact and resolve references to real-world objects (Kazemzadeh et al., 2014; Paetzel et al., 2014; Manuvinakurike and DeVault, 2015). Also, such gamified scenarios have served as platforms for developing/learning incremental dialogue policies regarding whether the system should respond immediately or wait for more information (Paetzel et al., 2015; Manuvinakurike et al., 2017). Referential domains in the context of dialogue have also been studied using virtual reality technologies and spatial constraints (Stoia et al., 2008; Das et al., 2018) as well as robots (Whitney et al., 2016; Skantze, 2017). A more recent direction of research involving dialogue and vision has been in the context of answering factual questions on images (Das et al., 2017; Antol et al., 2015) using the MSCOCO data set (Lin et al., 2014). The task may also involve a gamified scenario with the interlocutors playing a yes-no questionanswer game as in de Vries et al."
W18-4701,P16-1115,0,0.0126772,"logue, and computer vision, and our real-world domain extends the literature on domains that are at the intersection of language and computer vision. We will publicly release our corpus in the near future. 2 Related Work Conversation in the context of visual information has been studied for a long time. Clark and WilkesGibbs (1986) studied reference resolution of simple figures called tangrams. Kennington and Schlangen (2015) and Manuvinakurike et al. (2016) performed incremental understanding and incremental reference resolution respectively in a domain of geometric shape descriptions, while Schlangen et al. (2016) resolved references to objects in real-world example images. Much work has been done in the context of gamified scenarios where the interlocutors interact and resolve references to real-world objects (Kazemzadeh et al., 2014; Paetzel et al., 2014; Manuvinakurike and DeVault, 2015). Also, such gamified scenarios have served as platforms for developing/learning incremental dialogue policies regarding whether the system should respond immediately or wait for more information (Paetzel et al., 2015; Manuvinakurike et al., 2017). Referential domains in the context of dialogue have also been studied"
W18-4701,stoia-etal-2008-scare,0,0.0374439,"Much work has been done in the context of gamified scenarios where the interlocutors interact and resolve references to real-world objects (Kazemzadeh et al., 2014; Paetzel et al., 2014; Manuvinakurike and DeVault, 2015). Also, such gamified scenarios have served as platforms for developing/learning incremental dialogue policies regarding whether the system should respond immediately or wait for more information (Paetzel et al., 2015; Manuvinakurike et al., 2017). Referential domains in the context of dialogue have also been studied using virtual reality technologies and spatial constraints (Stoia et al., 2008; Das et al., 2018) as well as robots (Whitney et al., 2016; Skantze, 2017). A more recent direction of research involving dialogue and vision has been in the context of answering factual questions on images (Das et al., 2017; Antol et al., 2015) using the MSCOCO data set (Lin et al., 2014). The task may also involve a gamified scenario with the interlocutors playing a yes-no questionanswer game as in de Vries et al. (2017). In these works the focus is less on the dialogue aspects and more on the factual aspects of the images, i.e., if an object is present or what a certain component of the im"
W18-4701,tokunaga-etal-2012-rex,0,0.0131817,"yes-no questionanswer game as in de Vries et al. (2017). In these works the focus is less on the dialogue aspects and more on the factual aspects of the images, i.e., if an object is present or what a certain component of the image is. Mostafazadeh et al. (2017) extended this line of work with conversations grounded on images. Furthermore, Huang et al. (2016) built a data set of images with corresponding descriptions in sequence, for the task of visual storytelling. Other gamified real-world scenarios involve object arrangement (DeVault and Stone, 2009), puzzle completion (Iida et al., 2010; Takenobu et al., 2012), map navigation (Anderson et al., 1991; Lemon et al., 2001; Johnston et al., 2002), furniture-buying scenarios (Di Eugenio et al., 2000), and treasure-hunt tasks in a virtual environment (Byron and Fosler-Lussier, 2006). A multi-modal interface for image editing combining speech and direct manipulation was developed by (Laput et al., 2013). With this interface a user can for example select a person’s hat in an image and say “this is a hat”. Then the system learns to associate the tag “hat” with the selected region of the image. Finally, Manuvinakurike et al. (2018a) recently introduced a corp"
W18-4701,W15-4622,0,0.0162603,"Update requests (IER-U) are refinements to a previous request (users often request updates until the target is achieved). Revert requests (IER-R) occur when users want to undo the changes done to the image until a certain point. Compare requests (IER-C) occur when users want to compare the current version of the image to a previous version (before the most recent changes took place). The image edit requests IER-N and IER-U are labeled further with action and entity labels, which specify the nature of the edit request (the use of actions and entities is inspired by the intents and entities of Williams et al. (2015)). These labels serve as an intermediary language to map a user’s utterance to executable commands that can be carried out in an image editing program. Actions are a predefined list of 18 functions common to most 4 Segments Dialogue Act Action Attribute Loc/Obj Mod/Val uh make the tree brighter like a 100 nope too much O IER-N IER-U COM-D Adjust Adjust - brightness brightness - tree tree - 100 - perfect let’s work on sharpness COM-L IER-N Adjust sharpness - - Table 2: Example annotations of dialogue acts, actions, and entities. Dialogue Act IER-N IER-U IER-R IER-C COM-L COM-D COM-I RQR QF QIA"
W18-5033,N16-1147,0,0.336152,"Missing"
W18-5033,N16-1037,0,0.0605698,"n the context of image-grounded conversations. Some recent work has started investigating the potential of building dialogue systems that can help users efficiently explore data through visualizations (Kumar et al., 2017). The problem of intent recognition or dialogue act detection has been extensively studied. Below we focus on recent work on dialogue act detection that employs deep learning. People have used recurrent neural networks (RNNs) including long short term memory networks (LSTMs), and CNNs (Kalchbrenner and Blunsom, 2013; Li and Wu, 2016; Khanpour et al., 2016; Shen and Lee, 2016; Ji et al., 2016; Tran et al., 2017). The works that are most similar to ours are by Lee and Dernoncourt (2016) and Ortega and Vu (2017) who compared LSTMs and CNNs on the same data sets. However, neither Lee and Dernoncourt (2016) nor Ortega and Vu (2017) experimented with incremental dialogue act detection as we do. Regarding incrementality in dialogue, there has been a lot of work on predicting the next user action, generating fast system responses, and turntaking (Schlangen et al., 2009; Schlangen and Skantze, 2011; Dethlefs et al., 2012; Baumann and Schlangen, 2013; Selfridge et al., 2013; Ghigi et al.,"
W18-5033,W13-3214,0,0.0296417,"adeh et al. (2017) extended this work to natural language question and response generation in the context of image-grounded conversations. Some recent work has started investigating the potential of building dialogue systems that can help users efficiently explore data through visualizations (Kumar et al., 2017). The problem of intent recognition or dialogue act detection has been extensively studied. Below we focus on recent work on dialogue act detection that employs deep learning. People have used recurrent neural networks (RNNs) including long short term memory networks (LSTMs), and CNNs (Kalchbrenner and Blunsom, 2013; Li and Wu, 2016; Khanpour et al., 2016; Shen and Lee, 2016; Ji et al., 2016; Tran et al., 2017). The works that are most similar to ours are by Lee and Dernoncourt (2016) and Ortega and Vu (2017) who compared LSTMs and CNNs on the same data sets. However, neither Lee and Dernoncourt (2016) nor Ortega and Vu (2017) experimented with incremental dialogue act detection as we do. Regarding incrementality in dialogue, there has been a lot of work on predicting the next user action, generating fast system responses, and turntaking (Schlangen et al., 2009; Schlangen and Skantze, 2011; Dethlefs et a"
W18-5033,P15-1029,0,0.127401,"sed on real user data. Each image was Related Work Combining computer vision and language is a topic that has recently drawn much attention. 285 tion and prediction of utterance meaning, while Manuvinakurike et al. (2016b) and Petukhova and Bunt (2014) built models for incremental dialogue act recognition. associated with certain descriptions and the game worked for a specific data set of images without actually using computer vision. Manuvinakurike et al. (2016a) developed a model for incremental understanding of the described scenes among a set of complex configurations of geometric shapes. Kennington and Schlangen (2015) learned perceptually grounded word meanings for incremental reference resolution in the same domain of geometric shape descriptions, using visual features. Huang et al. (2016) built a data set of sequential images with corresponding descriptions that could potentially be used for the task of visual storytelling. Mostafazadeh et al. (2016) introduced the task of “visual question generation” where the system generates natural language questions when given an image, and then Mostafazadeh et al. (2017) extended this work to natural language question and response generation in the context of image"
W18-5033,W13-4042,0,0.026956,"u, 2016; Khanpour et al., 2016; Shen and Lee, 2016; Ji et al., 2016; Tran et al., 2017). The works that are most similar to ours are by Lee and Dernoncourt (2016) and Ortega and Vu (2017) who compared LSTMs and CNNs on the same data sets. However, neither Lee and Dernoncourt (2016) nor Ortega and Vu (2017) experimented with incremental dialogue act detection as we do. Regarding incrementality in dialogue, there has been a lot of work on predicting the next user action, generating fast system responses, and turntaking (Schlangen et al., 2009; Schlangen and Skantze, 2011; Dethlefs et al., 2012; Baumann and Schlangen, 2013; Selfridge et al., 2013; Ghigi et al., 2014; Kim et al., 2014; Khouzaimi et al., 2015). Recently Skantze (2017) presented a general continuous model of turn-taking based on LSTMs. Most related to our work, DeVault et al. (2011) built models for incremental interpreta3 Data We use a Wizard of Oz setup to collect a dialogue corpus in our image edit domain. The Wizard-user conversational session is set up over Skype and the conversation recorded on the Wizard’s system. The screen share feature is enabled on the Wizard’s screen so that the user can see in real time the changes requested. There ar"
W18-5033,C16-1189,0,0.0330376,"language question and response generation in the context of image-grounded conversations. Some recent work has started investigating the potential of building dialogue systems that can help users efficiently explore data through visualizations (Kumar et al., 2017). The problem of intent recognition or dialogue act detection has been extensively studied. Below we focus on recent work on dialogue act detection that employs deep learning. People have used recurrent neural networks (RNNs) including long short term memory networks (LSTMs), and CNNs (Kalchbrenner and Blunsom, 2013; Li and Wu, 2016; Khanpour et al., 2016; Shen and Lee, 2016; Ji et al., 2016; Tran et al., 2017). The works that are most similar to ours are by Lee and Dernoncourt (2016) and Ortega and Vu (2017) who compared LSTMs and CNNs on the same data sets. However, neither Lee and Dernoncourt (2016) nor Ortega and Vu (2017) experimented with incremental dialogue act detection as we do. Regarding incrementality in dialogue, there has been a lot of work on predicting the next user action, generating fast system responses, and turntaking (Schlangen et al., 2009; Schlangen and Skantze, 2011; Dethlefs et al., 2012; Baumann and Schlangen, 2013; S"
W18-5033,Q17-1010,0,0.019052,"of commonly occurring dialogue acts, actions, and entities. Utterance 1 add a vignette since it’s also encircled better 2 can we go down to fifteen on that 3 go back to .5 4 actually let’s revert back 5 can you compare for me before and after 6 I like it leave it there please 7 no I don’t like this color Tag IER-N 6.1 We convert the words into vector representations to train our deep learning models (and a variation of the random forests). We use out-of-thebox word vectors available in the form of GloVe embeddings (Pennington et al., 2014) (trained with Wikipedia data), or we employ fastText (Bojanowski et al., 2017) to construct embeddings using the data from the Visual Genome image region description phrases, the dialogue training set collected during this experiment, and other data related to image editing that we have collected (image edit requests out of a dialogue context). From now on these embeddings trained with fastText will be referred to as “trained embeddings”. As we can see in Table 4, for models E (LSTMs) and I (CNNs) we use word embeddings trained with fastText on the aforementioned data sets. The Vanilla LSTM (model D) does not use GloVe or trained embeddings, i.e., there is no dimensiona"
W18-5033,W15-4643,0,0.0342771,"Missing"
W18-5033,D12-1008,0,0.0323739,"Blunsom, 2013; Li and Wu, 2016; Khanpour et al., 2016; Shen and Lee, 2016; Ji et al., 2016; Tran et al., 2017). The works that are most similar to ours are by Lee and Dernoncourt (2016) and Ortega and Vu (2017) who compared LSTMs and CNNs on the same data sets. However, neither Lee and Dernoncourt (2016) nor Ortega and Vu (2017) experimented with incremental dialogue act detection as we do. Regarding incrementality in dialogue, there has been a lot of work on predicting the next user action, generating fast system responses, and turntaking (Schlangen et al., 2009; Schlangen and Skantze, 2011; Dethlefs et al., 2012; Baumann and Schlangen, 2013; Selfridge et al., 2013; Ghigi et al., 2014; Kim et al., 2014; Khouzaimi et al., 2015). Recently Skantze (2017) presented a general continuous model of turn-taking based on LSTMs. Most related to our work, DeVault et al. (2011) built models for incremental interpreta3 Data We use a Wizard of Oz setup to collect a dialogue corpus in our image edit domain. The Wizard-user conversational session is set up over Skype and the conversation recorded on the Wizard’s system. The screen share feature is enabled on the Wizard’s screen so that the user can see in real time th"
W18-5033,N16-1062,0,0.0223863,"hile Manuvinakurike et al. (2016b) and Petukhova and Bunt (2014) built models for incremental dialogue act recognition. associated with certain descriptions and the game worked for a specific data set of images without actually using computer vision. Manuvinakurike et al. (2016a) developed a model for incremental understanding of the described scenes among a set of complex configurations of geometric shapes. Kennington and Schlangen (2015) learned perceptually grounded word meanings for incremental reference resolution in the same domain of geometric shape descriptions, using visual features. Huang et al. (2016) built a data set of sequential images with corresponding descriptions that could potentially be used for the task of visual storytelling. Mostafazadeh et al. (2016) introduced the task of “visual question generation” where the system generates natural language questions when given an image, and then Mostafazadeh et al. (2017) extended this work to natural language question and response generation in the context of image-grounded conversations. Some recent work has started investigating the potential of building dialogue systems that can help users efficiently explore data through visualizatio"
W18-5033,W17-5530,0,0.0122664,"dialogue systems that can help users efficiently explore data through visualizations (Kumar et al., 2017). The problem of intent recognition or dialogue act detection has been extensively studied. Below we focus on recent work on dialogue act detection that employs deep learning. People have used recurrent neural networks (RNNs) including long short term memory networks (LSTMs), and CNNs (Kalchbrenner and Blunsom, 2013; Li and Wu, 2016; Khanpour et al., 2016; Shen and Lee, 2016; Ji et al., 2016; Tran et al., 2017). The works that are most similar to ours are by Lee and Dernoncourt (2016) and Ortega and Vu (2017) who compared LSTMs and CNNs on the same data sets. However, neither Lee and Dernoncourt (2016) nor Ortega and Vu (2017) experimented with incremental dialogue act detection as we do. Regarding incrementality in dialogue, there has been a lot of work on predicting the next user action, generating fast system responses, and turntaking (Schlangen et al., 2009; Schlangen and Skantze, 2011; Dethlefs et al., 2012; Baumann and Schlangen, 2013; Selfridge et al., 2013; Ghigi et al., 2014; Kim et al., 2014; Khouzaimi et al., 2015). Recently Skantze (2017) presented a general continuous model of turn-ta"
W18-5033,C16-1185,0,0.0244276,"work to natural language question and response generation in the context of image-grounded conversations. Some recent work has started investigating the potential of building dialogue systems that can help users efficiently explore data through visualizations (Kumar et al., 2017). The problem of intent recognition or dialogue act detection has been extensively studied. Below we focus on recent work on dialogue act detection that employs deep learning. People have used recurrent neural networks (RNNs) including long short term memory networks (LSTMs), and CNNs (Kalchbrenner and Blunsom, 2013; Li and Wu, 2016; Khanpour et al., 2016; Shen and Lee, 2016; Ji et al., 2016; Tran et al., 2017). The works that are most similar to ours are by Lee and Dernoncourt (2016) and Ortega and Vu (2017) who compared LSTMs and CNNs on the same data sets. However, neither Lee and Dernoncourt (2016) nor Ortega and Vu (2017) experimented with incremental dialogue act detection as we do. Regarding incrementality in dialogue, there has been a lot of work on predicting the next user action, generating fast system responses, and turntaking (Schlangen et al., 2009; Schlangen and Skantze, 2011; Dethlefs et al., 2012; Baumann"
W18-5033,W15-4610,1,0.770321,"ing” task. Here the goal is to provide a natural language answer, given an image and a natural language question about the image. Convolutional neural networks (CNNs) were employed for encoding the images (Krizhevsky et al., 2012). This was later modeled as a dialogue-based question-answering task in Das et al. (2017). These works used images from the MS COCO data set. de Vries et al. (2017) introduced “GuessWhat?!”, a two-player game where the goal is to find an unknown object in a rich image scene by asking a series of questions. They used images from MS COCO and CNNs for image recognition. Paetzel et al. (2015) built an incremental dialogue system called “Eve”, which could guess the correct image, out of a set of possible candidates, based on descriptions given by a human. The system was shown to perform nearly as well as humans. Then in the same domain, Manuvinakurike et al. (2017) used reinforcement learning to learn an incremental dialogue policy, which outperformed the high performance baseline policy of Paetzel et al. (2015) in offline simulations based on real user data. Each image was Related Work Combining computer vision and language is a topic that has recently drawn much attention. 285 ti"
W18-5033,N18-1049,0,0.0432174,"can see in Table 4, for models E (LSTMs) and I (CNNs) we use word embeddings trained with fastText on the aforementioned data sets. The Vanilla LSTM (model D) does not use GloVe or trained embeddings, i.e., there is no dimensionality reduction. Model H (CNN) uses GloVe embeddings. The vectors used in this work (both GloVe and trained embeddings) have a dimension of 50. For trained embeddings, the vectors were constructed using skipgrams over 50 epochs with a learning rate of 0.5. Recent advancements in creating a vector representation for a sentence were also evaluated. We used the Sent2Vec (Pagliardini et al., 2018) toolkit to get a vector representation of the sentence and then used these vectors as features for models G and J. Note that LSTMs are sequential models where every word needs a vector representation and thus we could not use Sent2Vec. IER-U IER-U IER-R IER-C COM-L COM-D Table 3: Examples of some of the most commonly occurring dialogue acts in our corpus. Not only is this more efficient but also more natural. The human Wizard can begin to take action even before the utterance completion, e.g., in utterance 1 the Wizard clicks the “vignette” feature in the tool before the user has finished utt"
W18-5033,D14-1162,0,0.0814878,"e brightness tree brightness tree sharpness - Value 100 50 - Table 2: Examples of commonly occurring dialogue acts, actions, and entities. Utterance 1 add a vignette since it’s also encircled better 2 can we go down to fifteen on that 3 go back to .5 4 actually let’s revert back 5 can you compare for me before and after 6 I like it leave it there please 7 no I don’t like this color Tag IER-N 6.1 We convert the words into vector representations to train our deep learning models (and a variation of the random forests). We use out-of-thebox word vectors available in the form of GloVe embeddings (Pennington et al., 2014) (trained with Wikipedia data), or we employ fastText (Bojanowski et al., 2017) to construct embeddings using the data from the Visual Genome image region description phrases, the dialogue training set collected during this experiment, and other data related to image editing that we have collected (image edit requests out of a dialogue context). From now on these embeddings trained with fastText will be referred to as “trained embeddings”. As we can see in Table 4, for models E (LSTMs) and I (CNNs) we use word embeddings trained with fastText on the aforementioned data sets. The Vanilla LSTM ("
W18-5033,L18-1683,1,0.583791,"Missing"
W18-5033,W09-3905,0,0.0394657,"memory networks (LSTMs), and CNNs (Kalchbrenner and Blunsom, 2013; Li and Wu, 2016; Khanpour et al., 2016; Shen and Lee, 2016; Ji et al., 2016; Tran et al., 2017). The works that are most similar to ours are by Lee and Dernoncourt (2016) and Ortega and Vu (2017) who compared LSTMs and CNNs on the same data sets. However, neither Lee and Dernoncourt (2016) nor Ortega and Vu (2017) experimented with incremental dialogue act detection as we do. Regarding incrementality in dialogue, there has been a lot of work on predicting the next user action, generating fast system responses, and turntaking (Schlangen et al., 2009; Schlangen and Skantze, 2011; Dethlefs et al., 2012; Baumann and Schlangen, 2013; Selfridge et al., 2013; Ghigi et al., 2014; Kim et al., 2014; Khouzaimi et al., 2015). Recently Skantze (2017) presented a general continuous model of turn-taking based on LSTMs. Most related to our work, DeVault et al. (2011) built models for incremental interpreta3 Data We use a Wizard of Oz setup to collect a dialogue corpus in our image edit domain. The Wizard-user conversational session is set up over Skype and the conversation recorded on the Wizard’s system. The screen share feature is enabled on the Wiza"
W18-5033,W17-5539,1,0.793603,"ased question-answering task in Das et al. (2017). These works used images from the MS COCO data set. de Vries et al. (2017) introduced “GuessWhat?!”, a two-player game where the goal is to find an unknown object in a rich image scene by asking a series of questions. They used images from MS COCO and CNNs for image recognition. Paetzel et al. (2015) built an incremental dialogue system called “Eve”, which could guess the correct image, out of a set of possible candidates, based on descriptions given by a human. The system was shown to perform nearly as well as humans. Then in the same domain, Manuvinakurike et al. (2017) used reinforcement learning to learn an incremental dialogue policy, which outperformed the high performance baseline policy of Paetzel et al. (2015) in offline simulations based on real user data. Each image was Related Work Combining computer vision and language is a topic that has recently drawn much attention. 285 tion and prediction of utterance meaning, while Manuvinakurike et al. (2016b) and Petukhova and Bunt (2014) built models for incremental dialogue act recognition. associated with certain descriptions and the game worked for a specific data set of images without actually using co"
W18-5033,W16-3630,1,0.62526,"Eve”, which could guess the correct image, out of a set of possible candidates, based on descriptions given by a human. The system was shown to perform nearly as well as humans. Then in the same domain, Manuvinakurike et al. (2017) used reinforcement learning to learn an incremental dialogue policy, which outperformed the high performance baseline policy of Paetzel et al. (2015) in offline simulations based on real user data. Each image was Related Work Combining computer vision and language is a topic that has recently drawn much attention. 285 tion and prediction of utterance meaning, while Manuvinakurike et al. (2016b) and Petukhova and Bunt (2014) built models for incremental dialogue act recognition. associated with certain descriptions and the game worked for a specific data set of images without actually using computer vision. Manuvinakurike et al. (2016a) developed a model for incremental understanding of the described scenes among a set of complex configurations of geometric shapes. Kennington and Schlangen (2015) learned perceptually grounded word meanings for incremental reference resolution in the same domain of geometric shape descriptions, using visual features. Huang et al. (2016) built a data"
W18-5033,W13-4063,0,0.030657,"6; Shen and Lee, 2016; Ji et al., 2016; Tran et al., 2017). The works that are most similar to ours are by Lee and Dernoncourt (2016) and Ortega and Vu (2017) who compared LSTMs and CNNs on the same data sets. However, neither Lee and Dernoncourt (2016) nor Ortega and Vu (2017) experimented with incremental dialogue act detection as we do. Regarding incrementality in dialogue, there has been a lot of work on predicting the next user action, generating fast system responses, and turntaking (Schlangen et al., 2009; Schlangen and Skantze, 2011; Dethlefs et al., 2012; Baumann and Schlangen, 2013; Selfridge et al., 2013; Ghigi et al., 2014; Kim et al., 2014; Khouzaimi et al., 2015). Recently Skantze (2017) presented a general continuous model of turn-taking based on LSTMs. Most related to our work, DeVault et al. (2011) built models for incremental interpreta3 Data We use a Wizard of Oz setup to collect a dialogue corpus in our image edit domain. The Wizard-user conversational session is set up over Skype and the conversation recorded on the Wizard’s system. The screen share feature is enabled on the Wizard’s screen so that the user can see in real time the changes requested. There are no time constraints, a"
W18-5033,W16-3632,1,0.859604,"Eve”, which could guess the correct image, out of a set of possible candidates, based on descriptions given by a human. The system was shown to perform nearly as well as humans. Then in the same domain, Manuvinakurike et al. (2017) used reinforcement learning to learn an incremental dialogue policy, which outperformed the high performance baseline policy of Paetzel et al. (2015) in offline simulations based on real user data. Each image was Related Work Combining computer vision and language is a topic that has recently drawn much attention. 285 tion and prediction of utterance meaning, while Manuvinakurike et al. (2016b) and Petukhova and Bunt (2014) built models for incremental dialogue act recognition. associated with certain descriptions and the game worked for a specific data set of images without actually using computer vision. Manuvinakurike et al. (2016a) developed a model for incremental understanding of the described scenes among a set of complex configurations of geometric shapes. Kennington and Schlangen (2015) learned perceptually grounded word meanings for incremental reference resolution in the same domain of geometric shape descriptions, using visual features. Huang et al. (2016) built a data"
W18-5033,W17-5527,0,0.0136096,"urs are by Lee and Dernoncourt (2016) and Ortega and Vu (2017) who compared LSTMs and CNNs on the same data sets. However, neither Lee and Dernoncourt (2016) nor Ortega and Vu (2017) experimented with incremental dialogue act detection as we do. Regarding incrementality in dialogue, there has been a lot of work on predicting the next user action, generating fast system responses, and turntaking (Schlangen et al., 2009; Schlangen and Skantze, 2011; Dethlefs et al., 2012; Baumann and Schlangen, 2013; Selfridge et al., 2013; Ghigi et al., 2014; Kim et al., 2014; Khouzaimi et al., 2015). Recently Skantze (2017) presented a general continuous model of turn-taking based on LSTMs. Most related to our work, DeVault et al. (2011) built models for incremental interpreta3 Data We use a Wizard of Oz setup to collect a dialogue corpus in our image edit domain. The Wizard-user conversational session is set up over Skype and the conversation recorded on the Wizard’s system. The screen share feature is enabled on the Wizard’s screen so that the user can see in real time the changes requested. There are no time constraints, and the Wizard and the user can talk freely until the user is happy with the changes perf"
W18-5033,E12-1076,0,0.172729,"of number of words (or time) is explicitly measured. DeVault et al. (2011) measured the stability of natural language understanding results as a function of time but did not explicitly measure savings in terms of number of words or time. 2 Yao et al. (2010) is an example of a work relying on manual input. They developed a semiautomatic method for parsing images from the Internet to build visual knowledge representation graphs. On the other hand, the following works did not rely on manual annotations. Feng and Lapata (2013) generated captions from news articles and their corresponding images. Mitchell et al. (2012) and Kulkarni et al. (2013) built systems for understanding and generating image descriptions. Due to space constraints, below we focus on work that combines computer vision or visual references (enabled through manual annotations) and language in the context of a dialogue task, which is most relevant to our work. Antol et al. (2015) introduced the “visual question answering” task. Here the goal is to provide a natural language answer, given an image and a natural language question about the image. Convolutional neural networks (CNNs) were employed for encoding the images (Krizhevsky et al., 2"
W18-5033,P17-2083,0,0.0185615,"image-grounded conversations. Some recent work has started investigating the potential of building dialogue systems that can help users efficiently explore data through visualizations (Kumar et al., 2017). The problem of intent recognition or dialogue act detection has been extensively studied. Below we focus on recent work on dialogue act detection that employs deep learning. People have used recurrent neural networks (RNNs) including long short term memory networks (LSTMs), and CNNs (Kalchbrenner and Blunsom, 2013; Li and Wu, 2016; Khanpour et al., 2016; Shen and Lee, 2016; Ji et al., 2016; Tran et al., 2017). The works that are most similar to ours are by Lee and Dernoncourt (2016) and Ortega and Vu (2017) who compared LSTMs and CNNs on the same data sets. However, neither Lee and Dernoncourt (2016) nor Ortega and Vu (2017) experimented with incremental dialogue act detection as we do. Regarding incrementality in dialogue, there has been a lot of work on predicting the next user action, generating fast system responses, and turntaking (Schlangen et al., 2009; Schlangen and Skantze, 2011; Dethlefs et al., 2012; Baumann and Schlangen, 2013; Selfridge et al., 2013; Ghigi et al., 2014; Kim et al., 20"
W18-5033,I17-1047,0,0.0444592,"tanding of the described scenes among a set of complex configurations of geometric shapes. Kennington and Schlangen (2015) learned perceptually grounded word meanings for incremental reference resolution in the same domain of geometric shape descriptions, using visual features. Huang et al. (2016) built a data set of sequential images with corresponding descriptions that could potentially be used for the task of visual storytelling. Mostafazadeh et al. (2016) introduced the task of “visual question generation” where the system generates natural language questions when given an image, and then Mostafazadeh et al. (2017) extended this work to natural language question and response generation in the context of image-grounded conversations. Some recent work has started investigating the potential of building dialogue systems that can help users efficiently explore data through visualizations (Kumar et al., 2017). The problem of intent recognition or dialogue act detection has been extensively studied. Below we focus on recent work on dialogue act detection that employs deep learning. People have used recurrent neural networks (RNNs) including long short term memory networks (LSTMs), and CNNs (Kalchbrenner and B"
W18-5033,P16-1170,0,0.0562369,"nd the game worked for a specific data set of images without actually using computer vision. Manuvinakurike et al. (2016a) developed a model for incremental understanding of the described scenes among a set of complex configurations of geometric shapes. Kennington and Schlangen (2015) learned perceptually grounded word meanings for incremental reference resolution in the same domain of geometric shape descriptions, using visual features. Huang et al. (2016) built a data set of sequential images with corresponding descriptions that could potentially be used for the task of visual storytelling. Mostafazadeh et al. (2016) introduced the task of “visual question generation” where the system generates natural language questions when given an image, and then Mostafazadeh et al. (2017) extended this work to natural language question and response generation in the context of image-grounded conversations. Some recent work has started investigating the potential of building dialogue systems that can help users efficiently explore data through visualizations (Kumar et al., 2017). The problem of intent recognition or dialogue act detection has been extensively studied. Below we focus on recent work on dialogue act dete"
