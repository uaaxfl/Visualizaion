C00-1026,O97-1011,1,0.752792,"Missing"
C00-1026,C92-1019,1,0.720694,"y look-up for matching morphemes and resolution methods for the inherent ambiguous segmentations, such as the examples in 1). However conventional word segmentation algorithms cannot apply for the morphological analysis without modification, since the morpho-syntactic behavior is different from syntactic behavior. Since the structure of the Chinese compound nouns is head final and the most productive morphemes are monosyllabic, there is a simple and effective algorithm, which agrees with these facts. This algorithm segments input compounds from left to right by the longest matching criterion (Chen & Liu 1992). It is clear that the left to right longest matching algorithm prefers shorter head and longer modifier structures. 3.3 Semantic categories of morphemes The semantic categories of morphemes are followed from the thesaurus Chilin. This thesaurus is a lattice structure of concept taxonomy. Morphemes/words may have multiple classification due to either ambiguous classification or inherent semantic ambiguities. For the ambiguous semantic categories of a morpheme, the lower ranking semantic categories will be eliminated and leave the higher-ranking semantic categories to compete during the identif"
C00-1026,O98-3002,1,\N,Missing
C00-1026,J98-1001,0,\N,Missing
C00-1026,O93-1004,0,\N,Missing
C02-1049,O97-4005,0,0.606936,"Missing"
C02-1049,C92-1019,1,0.940069,"Missing"
C02-1049,Y96-1018,1,0.868122,"Missing"
C02-1049,O98-3002,1,0.876146,"Missing"
C02-1049,O92-1003,0,0.240942,"Missing"
C02-1049,J93-1001,0,0.0459521,"Missing"
C02-1049,J96-1001,0,\N,Missing
C02-1049,J96-3004,0,\N,Missing
C02-1049,J93-1007,0,\N,Missing
C02-1049,C00-1027,0,\N,Missing
C02-1049,O97-4003,1,\N,Missing
C02-1049,O91-1003,1,\N,Missing
C02-1049,O93-1004,0,\N,Missing
C10-3012,I05-7001,1,\N,Missing
C90-2010,C88-1060,0,0.0339078,"Missing"
C90-2010,J88-1004,0,0.024294,"Missing"
C90-2010,C86-1045,0,\N,Missing
C90-2010,J88-1001,0,\N,Missing
C92-1019,O91-1004,0,0.060226,"ive heuristics. 1. Introduction Chinese sentences arc cx)mposed with string of characters without blanks to mark words. However the basic unit for sentence parsing and understanding is word. Therefore the first step of processing Chinese sentences is to identify the words( i.e. segment the character strings of the sentences into word strings). Most of the current Chinese natural language processing systems include a processor for word identification. Also there are many word segmentation techniques been developed. Usually they use a lexicon with a large set of entries to match input sentences [2,10,12,13,14,21]. It is very often that there are many l~)ssible different successful matchings. Therefore the major focus for word identification were on thc resolution of ambiguities. However many other important aspects, such as what should be done, in what depth and what are considered to be the correct identifications were totally ignored. High identification rates are claimed to be achieved, but none of them were measured u n d e r equal bases. There is no agreement in what extend words are considered to be correctly identified. For instance, compounds occur very often in Chinese text, but none of the e"
C92-1019,O91-1003,1,0.816819,"inese and should be handled differently [3, 6, 7, 11, 17, 191. a. determinative-measure compounds (DM) A determinative-measure compound is composed of one or more determinatives, together with an optional mcasm&apos;e. (2) je san ben this three CL ""these three"" It is used to determine the reference or the quantity of the noun phrase that co-~w.curs with it. D e ~ i t e the fact that I~)th categories of determinatives and measures are closed, the comhinations of them are not. However the set of DMs is a regular language which can be expressed by regular expressions and reCOgnized by finite automata [19]. Mo [191 also point out that the structure of I)Ms are exocentric. They are hardly similar to other phrase structures which are endocentric and context-free and can bc analyzed by head driven parsing strategies. Therefore we suggest that tile identification of DMs should be done in parallel with the identification of common words. There are 76 rules for DMs which covers ahnost all of the DMs [19]. Dor word identification, those rules function as a supplement for the lexicon, which works as ff the lexicon contains all of the DMs. We will show the test result in the section 3.3. Currently we ha"
C92-1019,C90-2010,1,\N,Missing
C92-4194,C92-1019,1,0.820253,", i.e. where the sccnnd key word does not (recur in the leftcontext of the first key word. 6) &lt; kwl > / r &lt; kw2 > : context where a key word (kw2) is 'right-disassociated' from another key word (kwl), i.e. where tile secured key word does not occur in the right-context of the first key word. t ;ommands 1)-3) are helpful tools in studying morphological rules and identifying morphological constmctions for Chinese. Since Chinese writing systems do not include word-breaks, and since no lexicon can ever offer a complete list of words, word segmentatkm is non-trivial in Chinese Language Processing (Chert and Liu 1992). Identifying and util~ing morphological information is therefore essential Ix)th in lexical computing and in natural language processing. PROC.OF COLING-92, NAN-rES,AUG. 23-28, 1992 C o m m a n d 4) is a handy tool to discover cooccurrence restrictions and their semantic consequences. Commands 5) and 6) are used to eliminate ambiguity and to cut down the size of search results. It can be noted that since our tagger is not running yet and since the Chinese running text seldom defines a sentence by a period (a whole paragraph often contains only one period and many commas), the above commands u"
C92-4194,C90-2010,1,\N,Missing
C94-1088,C92-1019,1,0.823636,"mmonly used to resolve categorical or sense ambiguities. Combining both N-gram search and string filtering, fi'equcncy-based word collocatipn is achieved without segmentation. V. Collocation After Segmentation When lexical or phrasal relation is the focus of the study, the above collocation module may sometimes be 541 inadequate. In tiffs case, we will necd to apply the automatic segmentation/tagging program such that we can acquire information involving word pairs as well as grammatical categories. The automatic segmentation proccdurc is an revised version of the program reported in Chen and Liu (1992). The on-line lexicon is the CKIP lexicon of more than 80 thousand cntries (Chen 1994). We did not automatically segment and tag the whole corpus for very good reasons. First, without a correctly tagged corpus, no statistically-based tagger can perform satisfactorily yet. Second, tllcrc is no practical way to recover incorrectly identified words. That is, when the automatic taggcr takes a character fi'om a target word to form an inal~propriate word with a neighboring character; that target word is lost and cannot be identified in this context. Tiros, it will be linguistically more felicitous t"
C94-1088,J93-1001,0,0.0297929,"ntation but is awe to word-based collocational properties can be obtained allow both lexical and sub-lexical information be through an auxiliary modttle of automatic segmentation. automatically extracted. PI(OJ1;CT NOTE: I~ARGI; TEXT COI(I'ORA II. Background: Corpus and Computational Platfonn collocation system has the 1. Introduction To take the full This collocation system is developed on the 20 million charactcr modern Chinese corpus at Academia Collocation has been established as an essential tool in Sinica (Huang and Chen 1992, lquang In Press). This computational linguistics (Church and Mercer 1993). In corpus is composed mostly of newspaper texts. It is addition, various col[ocatiomd programs have been cstimatcd to have proven to bc indispensable in automatic acquisition of"" industrial standard in Taiwan, our collocation system [exical information (e.g. Sinclair 1991, and Bibcr i993). can deal with any corpora encoded by BIG-5 code. The Sincc words arc the natural and undisputed units in program is dcvclopcd undcr a UNiX cnvironmcnt on HP available workstation. text corpora, virtually all the current 14 million words. Following It should, howcver, be portable to any collocationa[ progra"
C94-1088,C92-4194,1,0.91907,"sing sub-lexical information. Furthermore, processing of automatic segmentation but is awe to word-based collocational properties can be obtained allow both lexical and sub-lexical information be through an auxiliary modttle of automatic segmentation. automatically extracted. PI(OJ1;CT NOTE: I~ARGI; TEXT COI(I'ORA II. Background: Corpus and Computational Platfonn collocation system has the 1. Introduction To take the full This collocation system is developed on the 20 million charactcr modern Chinese corpus at Academia Collocation has been established as an essential tool in Sinica (Huang and Chen 1992, lquang In Press). This computational linguistics (Church and Mercer 1993). In corpus is composed mostly of newspaper texts. It is addition, various col[ocatiomd programs have been cstimatcd to have proven to bc indispensable in automatic acquisition of"" industrial standard in Taiwan, our collocation system [exical information (e.g. Sinclair 1991, and Bibcr i993). can deal with any corpora encoded by BIG-5 code. The Sincc words arc the natural and undisputed units in program is dcvclopcd undcr a UNiX cnvironmcnt on HP available workstation. text corpora, virtually all the current 14 million w"
C94-1088,J93-3004,0,\N,Missing
C94-1088,J90-1003,0,\N,Missing
C94-1088,O94-1005,1,\N,Missing
C96-2184,C92-1019,1,0.814526,"entable heuristic guidelines which deal with specific linguistic categories. Data uniformity is achieved by stratification of the standard itself and by defining a standard lexicon as part of the segmentation standard. I. Introduction One important feature of Chinese texts is that they are character-based, not wordbased. Each Chinese character stands for one phonological syllable and in most cases represents a morpheme. The fact that Chinese writing does not mark word boundaries poses the unique question of word segmentation in Chinese computational linguistics (e.g. Sproat and Shih 1990, and Chert and Liu 1992). Since words are the linguistically significant basic elements that are entered in the lexicon and manipulated by grammar rules, no language processing can be done unless words are identified. In theoretical terms, the primacy of the concept of word can be more firmly established if its existence can be empirically supported in a language that does not mark it conventionally in texts (e.g. Bates et al. 1993, Huang et al. 1993). In computational terms, no serious Chinese language processing can be done without segmentation. No efficient sharing of electronic resources or computational tools is"
C96-2184,O92-1003,0,0.105633,"Missing"
C96-2184,J96-3004,0,\N,Missing
C96-2184,J90-1003,0,\N,Missing
C96-2184,Y96-1018,1,\N,Missing
C98-1038,Y96-1018,1,\N,Missing
D09-1050,W08-0409,0,0.0115469,"nt based methods, which are highly dependent on the probability information at the lexical level, are not well suited for this type of translation. To address the above problem, some methods have been proposed for extending word alignments to phrase alignments. For example, Och et al. (1999) proposed the so-called grow-diagfinal heuristic method for extending word alignments to phrase alignments. The method is widely used and has achieved good results for phrase-based statistical machine translation. (Och et al., 1999; Koehn et al., 2003; Liang et al., 2006). Instead of using heuristic rules, Ma et al. (2008) showed that syntactic information, e.g., phrase or dependency structures, is useful in extending the word-level alignment. However, the above methods still depend on word-based alignment models, so they are not well suited to extracting the translation equivalences of semantically opaque MWEs due to the lack of word level relations between the translational correspondences. Moreover, the aligned phrases are not precise enough to be used in many NLP applications like dictionary compilation, which require high quality translations. Association-based methods, e.g., the Dice coefficient, are wide"
D09-1050,P00-1056,0,0.408126,"Missing"
D09-1050,W99-0604,0,0.422703,"wan mhbai@sinica.edu.tw, swimming@hp.iis.sinica.edu.tw, kchen@iis.sinica.edu.tw, jschang@cs.nthu.edu.tw Since many concepts are expressed by idiomatic multiword expressions instead of single words, and different languages may realize the same concept using different numbers of words (Ma et al., 2007; Wu, 1997), word alignment based methods, which are highly dependent on the probability information at the lexical level, are not well suited for this type of translation. To address the above problem, some methods have been proposed for extending word alignments to phrase alignments. For example, Och et al. (1999) proposed the so-called grow-diagfinal heuristic method for extending word alignments to phrase alignments. The method is widely used and has achieved good results for phrase-based statistical machine translation. (Och et al., 1999; Koehn et al., 2003; Liang et al., 2006). Instead of using heuristic rules, Ma et al. (2008) showed that syntactic information, e.g., phrase or dependency structures, is useful in extending the word-level alignment. However, the above methods still depend on word-based alignment models, so they are not well suited to extracting the translation equivalences of semant"
D09-1050,P02-1040,0,0.0937907,"Missing"
D09-1050,J96-1001,0,0.447428,"ndency structures, is useful in extending the word-level alignment. However, the above methods still depend on word-based alignment models, so they are not well suited to extracting the translation equivalences of semantically opaque MWEs due to the lack of word level relations between the translational correspondences. Moreover, the aligned phrases are not precise enough to be used in many NLP applications like dictionary compilation, which require high quality translations. Association-based methods, e.g., the Dice coefficient, are widely used to extract translations of MWEs. (Kupiec, 1993; Smadja et al., 1996; Kitamura and Matsumoto, 1996; Yamamoto and Matsumoto, 2000; Melamed, 2001). The advantage of such methods is that association relations are established at the phrase level instead of the lexical level, so they have the potential to resolve the above-mentioned translation problem. However, when applying association-based methods, we have to consider the following complications. The first complication, which we call the contextual effect, causes the extracted translation to contain noisy words. For Abstract In this paper, we present an algorithm for extracting translations of any given multiwo"
D09-1050,J97-3002,0,0.0648867,"Missing"
D09-1050,P03-1016,0,0.0218972,"eriments show that our approach outperforms the word alignmentbased and other naive association-based methods. We also demonstrate that adopting the extracted translations can significantly improve the performance of the Moses machine translation system. 1 Introduction Translation of multiword expressions (MWEs), such as compound words, phrases, collocations and idioms, is important for many NLP tasks, including the techniques are helpful for dictionary compilation, cross language information retrieval, second language learning, and machine translation. (Smadja et al., 1996; Gao et al., 2002; Wu and Zhou, 2003). However, extracting exact translations of MWEs is still an open problem, possibly because the senses of many MWEs are not compositional (Yamamoto and Matsumoto, 2000), i.e., their translations are not compositions of the translations of individual words. For example, the Chinese idiom 坐視不理 should be translated as “turn a blind eye,” which has no direct relation with respect to the translation of each constituent (i.e., “to sit”, “to see” and “to ignore”) at the word level. Previous SMT systems (e.g., Brown et al., 1993) used a word-based translation model which assumes that a sentence can be"
D09-1050,W96-0107,0,0.220037,"useful in extending the word-level alignment. However, the above methods still depend on word-based alignment models, so they are not well suited to extracting the translation equivalences of semantically opaque MWEs due to the lack of word level relations between the translational correspondences. Moreover, the aligned phrases are not precise enough to be used in many NLP applications like dictionary compilation, which require high quality translations. Association-based methods, e.g., the Dice coefficient, are widely used to extract translations of MWEs. (Kupiec, 1993; Smadja et al., 1996; Kitamura and Matsumoto, 1996; Yamamoto and Matsumoto, 2000; Melamed, 2001). The advantage of such methods is that association relations are established at the phrase level instead of the lexical level, so they have the potential to resolve the above-mentioned translation problem. However, when applying association-based methods, we have to consider the following complications. The first complication, which we call the contextual effect, causes the extracted translation to contain noisy words. For Abstract In this paper, we present an algorithm for extracting translations of any given multiword expression from parallel co"
D09-1050,C00-2135,0,0.625212,"level alignment. However, the above methods still depend on word-based alignment models, so they are not well suited to extracting the translation equivalences of semantically opaque MWEs due to the lack of word level relations between the translational correspondences. Moreover, the aligned phrases are not precise enough to be used in many NLP applications like dictionary compilation, which require high quality translations. Association-based methods, e.g., the Dice coefficient, are widely used to extract translations of MWEs. (Kupiec, 1993; Smadja et al., 1996; Kitamura and Matsumoto, 1996; Yamamoto and Matsumoto, 2000; Melamed, 2001). The advantage of such methods is that association relations are established at the phrase level instead of the lexical level, so they have the potential to resolve the above-mentioned translation problem. However, when applying association-based methods, we have to consider the following complications. The first complication, which we call the contextual effect, causes the extracted translation to contain noisy words. For Abstract In this paper, we present an algorithm for extracting translations of any given multiword expression from parallel corpora. Given a multiword expre"
D09-1050,N03-1017,0,0.177942,"using different numbers of words (Ma et al., 2007; Wu, 1997), word alignment based methods, which are highly dependent on the probability information at the lexical level, are not well suited for this type of translation. To address the above problem, some methods have been proposed for extending word alignments to phrase alignments. For example, Och et al. (1999) proposed the so-called grow-diagfinal heuristic method for extending word alignments to phrase alignments. The method is widely used and has achieved good results for phrase-based statistical machine translation. (Och et al., 1999; Koehn et al., 2003; Liang et al., 2006). Instead of using heuristic rules, Ma et al. (2008) showed that syntactic information, e.g., phrase or dependency structures, is useful in extending the word-level alignment. However, the above methods still depend on word-based alignment models, so they are not well suited to extracting the translation equivalences of semantically opaque MWEs due to the lack of word level relations between the translational correspondences. Moreover, the aligned phrases are not precise enough to be used in many NLP applications like dictionary compilation, which require high quality tran"
D09-1050,W04-3250,0,0.145883,"Missing"
D09-1050,P07-2045,0,0.00646883,"shold 2 . The second type of information is the phrase translation probability and lexical weighting. Computing the phrase translation probability is trivial in the training corpora, but lexical weighting (Koehn et al., 2003) needs lexical-level alignment. For convenience, we assume that each word in an MWE links to each word in the translations. Under this assumption, the lexical weighting is simplified as follows: n p w (f |e, a ) = ∏ i =1 n ≅∏ i =1 1 ∑ p( f i |e j ) |{ j |(i, j ) ∈ a} |∀ ( i , j )∈a 1 ∑ p( fi |e j ) . |e |∀e j ∈e 5 Evaluation Results We trained a model using Moses toolkit (Koehn et al., 2007) on the training data as our baseline system. Table 9 shows the influence of adding the MWE translations to the test data. In the first 2 Conclusions and Future Work We have proposed a high precision algorithm for extracting translations of multiword expressions from parallel corpora. The algorithm can be used to translate any language pair and any type of word sequence, including rigid sequences and discontinuous sequences. Our evaluation results show that the algorithm can cope with the difficulties caused by indirect association and the common subsequence effects, leading to significant imp"
D09-1050,P93-1003,0,0.424216,"phrase or dependency structures, is useful in extending the word-level alignment. However, the above methods still depend on word-based alignment models, so they are not well suited to extracting the translation equivalences of semantically opaque MWEs due to the lack of word level relations between the translational correspondences. Moreover, the aligned phrases are not precise enough to be used in many NLP applications like dictionary compilation, which require high quality translations. Association-based methods, e.g., the Dice coefficient, are widely used to extract translations of MWEs. (Kupiec, 1993; Smadja et al., 1996; Kitamura and Matsumoto, 1996; Yamamoto and Matsumoto, 2000; Melamed, 2001). The advantage of such methods is that association relations are established at the phrase level instead of the lexical level, so they have the potential to resolve the above-mentioned translation problem. However, when applying association-based methods, we have to consider the following complications. The first complication, which we call the contextual effect, causes the extracted translation to contain noisy words. For Abstract In this paper, we present an algorithm for extracting translations"
D09-1050,N06-1014,0,0.0430051,"bers of words (Ma et al., 2007; Wu, 1997), word alignment based methods, which are highly dependent on the probability information at the lexical level, are not well suited for this type of translation. To address the above problem, some methods have been proposed for extending word alignments to phrase alignments. For example, Och et al. (1999) proposed the so-called grow-diagfinal heuristic method for extending word alignments to phrase alignments. The method is widely used and has achieved good results for phrase-based statistical machine translation. (Och et al., 1999; Koehn et al., 2003; Liang et al., 2006). Instead of using heuristic rules, Ma et al. (2008) showed that syntactic information, e.g., phrase or dependency structures, is useful in extending the word-level alignment. However, the above methods still depend on word-based alignment models, so they are not well suited to extracting the translation equivalences of semantically opaque MWEs due to the lack of word level relations between the translational correspondences. Moreover, the aligned phrases are not precise enough to be used in many NLP applications like dictionary compilation, which require high quality translations. Association"
D09-1050,P07-1039,0,0.0255853,"Missing"
D09-1050,J93-2003,0,\N,Missing
D14-1100,I08-1012,0,0.190676,"oncepts in an upper-level ontology. One important characteristic of adjectival verbs is that they have conjunctive morphological structures, i.e., the words are conjunct with two nearly synonymous verbs, e.g., 研/study 究 /search (research), 探 /explore 測 /detect (explore), and 搜/search 尋/find (search). Therefore, we need a morphological classifier that can detect the conjunctive morphological structure of a Related Work Most works on V-N structure identification focus on two types of relation classification: modifierhead relations and predicate-object relations (Wu, 2003; Qiu, 2005; Chen, 2008; Chen et al., 2008; Yu et al., 2008). They exclude the independent structure and conjunctive head-head relation, but the cross-bracket relation does exist between two adjacent words in real language. For example, if “遍佈/all over 世界/world ” was included in the short sentence “遍佈/all over 世界/world 各國 /countries”, it would be an independent structure. A conjunctive head-head relation between a verb and a noun is rare. However, in the sentence “服 務 設備 都 甚 周到” (Both service and equipment are very thoughtful.), there is a conjunctive head-head relation between the verb 服 務 /service and the noun 設備/equipment. Therefor"
D14-1100,O04-1014,1,0.748969,"ghtful.), there is a conjunctive head-head relation between the verb 服 務 /service and the noun 設備/equipment. Therefore, we use four types of relations to describe the VN structures in our experiments. The symbol ‘H/X’ denotes a predicate-object relation; ‘X/H’ denotes a modifier-head relation; ‘H/H’ denotes a conjunctive head-head relation; and ‘X/X’ denotes an independent relation. Feature selection is an important task in V-N disambiguation. Hence, a number of studies have suggested features that may help resolve the ambiguity of V-N structures (Zhao and Huang, 1999; Sun and Jurafsky, 2003; Chiu et al., 2004; Qiu, 2005; Chen, 2008). Zhao and Huang used lexicons, semantic knowledge, and word length in929 formation to increase the accuracy of identification. Although they used the Chinese thesaurus CiLin (Mei et al., 1983) to derive lexical semantic knowledge, the word coverage of CiLin is insufficient. Moreover, none of the above papers tackle the problem of unknown words. Sun and Jurafsky exploit the probabilistic rhythm feature (i.e., the number of syllables in a word or the number of words in a phrase) in their shallow parser. Their results show that the feature improves the parsing performance"
D14-1100,W12-6338,1,0.740419,"Missing"
D14-1100,P06-1055,0,0.0630008,"Missing"
D14-1100,W03-1719,0,0.0865533,"Missing"
D14-1100,W03-1706,0,0.510426,"equipment are very thoughtful.), there is a conjunctive head-head relation between the verb 服 務 /service and the noun 設備/equipment. Therefore, we use four types of relations to describe the VN structures in our experiments. The symbol ‘H/X’ denotes a predicate-object relation; ‘X/H’ denotes a modifier-head relation; ‘H/H’ denotes a conjunctive head-head relation; and ‘X/X’ denotes an independent relation. Feature selection is an important task in V-N disambiguation. Hence, a number of studies have suggested features that may help resolve the ambiguity of V-N structures (Zhao and Huang, 1999; Sun and Jurafsky, 2003; Chiu et al., 2004; Qiu, 2005; Chen, 2008). Zhao and Huang used lexicons, semantic knowledge, and word length in929 formation to increase the accuracy of identification. Although they used the Chinese thesaurus CiLin (Mei et al., 1983) to derive lexical semantic knowledge, the word coverage of CiLin is insufficient. Moreover, none of the above papers tackle the problem of unknown words. Sun and Jurafsky exploit the probabilistic rhythm feature (i.e., the number of syllables in a word or the number of words in a phrase) in their shallow parser. Their results show that the feature improves the"
D14-1100,W12-6335,0,0.118167,"Missing"
D14-1100,W03-1717,0,0.502643,"class of events which that are concepts in an upper-level ontology. One important characteristic of adjectival verbs is that they have conjunctive morphological structures, i.e., the words are conjunct with two nearly synonymous verbs, e.g., 研/study 究 /search (research), 探 /explore 測 /detect (explore), and 搜/search 尋/find (search). Therefore, we need a morphological classifier that can detect the conjunctive morphological structure of a Related Work Most works on V-N structure identification focus on two types of relation classification: modifierhead relations and predicate-object relations (Wu, 2003; Qiu, 2005; Chen, 2008; Chen et al., 2008; Yu et al., 2008). They exclude the independent structure and conjunctive head-head relation, but the cross-bracket relation does exist between two adjacent words in real language. For example, if “遍佈/all over 世界/world ” was included in the short sentence “遍佈/all over 世界/world 各國 /countries”, it would be an independent structure. A conjunctive head-head relation between a verb and a noun is rare. However, in the sentence “服 務 設備 都 甚 周到” (Both service and equipment are very thoughtful.), there is a conjunctive head-head relation between the verb 服 務 /s"
D14-1100,C08-1132,0,0.602334,"-level ontology. One important characteristic of adjectival verbs is that they have conjunctive morphological structures, i.e., the words are conjunct with two nearly synonymous verbs, e.g., 研/study 究 /search (research), 探 /explore 測 /detect (explore), and 搜/search 尋/find (search). Therefore, we need a morphological classifier that can detect the conjunctive morphological structure of a Related Work Most works on V-N structure identification focus on two types of relation classification: modifierhead relations and predicate-object relations (Wu, 2003; Qiu, 2005; Chen, 2008; Chen et al., 2008; Yu et al., 2008). They exclude the independent structure and conjunctive head-head relation, but the cross-bracket relation does exist between two adjacent words in real language. For example, if “遍佈/all over 世界/world ” was included in the short sentence “遍佈/all over 世界/world 各國 /countries”, it would be an independent structure. A conjunctive head-head relation between a verb and a noun is rare. However, in the sentence “服 務 設備 都 甚 周到” (Both service and equipment are very thoughtful.), there is a conjunctive head-head relation between the verb 服 務 /service and the noun 設備/equipment. Therefore, we use four typ"
D14-1100,O00-2004,1,\N,Missing
D14-1100,W03-1726,1,\N,Missing
I05-1016,O04-1015,1,0.798504,"uperfluous structures R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 177 – 187, 2005. © Springer-Verlag Berlin Heidelberg 2005 178 Y.-M. Hsieh, D.-C. Yang, and K.-J. Chen and increase the precision of grammar representation. Recently, probabilistic preferences for grammar rules were incorporated to resolve structure-ambiguities and had great improvements on parsing performances [2, 6, 10]. Regarding feature constrains, it was shown that contexture information of categories of neighboring nodes, mother nodes, or head words are useful for improving grammar precision and parsing performances [1, 2, 7, 10, 12]. However tradeoffs between grammar coverage and grammar precision are always inevitable. Excessive grammatical constraints will reduce grammar coverage and hence reduce parsing performances. On the other hand, loosely constrained grammars cause structure-ambiguities and also reduce parsing performances. In this paper, we consider grammar optimization in particular for Chinese language. Linguistically-motivated feature constraints were added to the grammar rules and evaluated to maintain both grammar coverage and precision. In section 2, the experimental environments were introduced. Grammar g"
I05-1016,J98-4004,0,0.308317,"Missing"
I05-1016,N04-1032,0,0.0305608,"Missing"
I05-1016,J03-4003,0,\N,Missing
I05-1016,P03-1054,0,\N,Missing
I05-1016,O99-4004,1,\N,Missing
I05-3007,O98-3004,1,0.821376,"inese VerbNet Project: that ren.wei ᇡࣁ‘to quotation. Yi.wei and jue.de, on the other hand, can only be used in reportage and cannot think’ behaves most like biao.shi ߄ Ң ‘to introduce direct quotation. express, to state’ (salience 0.451), while yi.wei а Distinction between near synonymous pairs ࣁ ‘to take somebody/something as’ is more like can be obtained from Sketch Difference. This jue.de ள ‘to feel, think’ (salience 0.488). The function is verified with results from Tsai et al.’s study on gao.xing ଯᑫ ‘glad’ and kuai.le ז! synonymous relation can be illustrated by (4) and (5). ‘happy’ (Tsai et al., 1998). Gao.xing ‘glad’ 4a. дᇡࣁੇډѦၗԖঁᢀࡐۺख़ाǴ൩ा specific patterns include the negative imperative bie ձ ‘don’t’. It also has a dominant collocation ޕၰӦޑၯᔍೕ߾Ǵௗڙ೭٤చҹǶ ta ren.wei dao hai.wai tou.zi you yi ge guan.nian with the potentiality complement marker de ள hen zhong.yao, jiu shi yao zhi.dao dang.di de (e.g. ta gao.xing de you jiao you tiao Ӵଯᑫள you.xi gui.ze ΞћΞၢ ‘she was so happy that she cried and ‘He believes that for those investing overseas, danced’). In contrast, kuai.le ‘happy’ has the there is a very important principle-one must know specific collocation with holiday nouns s"
I05-3007,xia-etal-2000-developing,0,0.0096387,"n a list of types and distribution of the keyword’s syntactic category. In addition, users can find possible collocations of the keyword from the output of Mutual Information (MI). The most salient grammatical information, such as grammatical functions (subject, object, adjunct etc.) is beyond the scope of the traditional corpus interface tools. Traditional corpora rely on the human users to arrive at these kinds of generalizations. 3. Sketch Engine: A New Corpus-based approach to Grammatical Information Several existing linguistically annotated corpus of Chinese, e.g. Penn Chinese Tree Bank (Xia et al., 2000), Sinica Treebank (Chen et al., 2003), Proposition Bank (Xue and Palmer, 2003, 2005) and Mandarin VerbNet (Wu and Liu, 50 Linguistic Knowledge Net anchored by a lexicon (Huang et al., 2001). A Word Sketch is a one-page list of a Information (MI) to measure the salience of a keyword’s functional distribution and collocation Engine output. MI provides a measure of the in the corpus. The functional distribution degree of association of a given segment with includes: subject, object, prepositional object, others. Pointwise MI, calculated by Equation 3, and modifier. Its collocations are described"
I05-3007,C90-2010,1,\N,Missing
I05-3007,O97-4003,1,\N,Missing
I05-7001,Y00-1012,0,\N,Missing
I05-7001,O00-2002,1,\N,Missing
I08-1033,P06-1002,0,0.0289443,"Missing"
I08-1033,O06-4003,1,0.845107,"are not allowed alignment sequences. The constraint is applied to each possible aligning result. If the alignment violates the constraint, it will be rejected. Since the new alignment algorithm must enumerate all of the possible alignments, the process is very time consuming. Therefore, it is advantageous to use a bilingual terminology bank rather than a parallel corpus. The average length of terminologies is short and much shorter than a typical sentence in a parallel corpus. This makes words to morphemes alignment computationally feasible and the results highly accurate (Chang et al., 2001; Bai et al., 2006). This makes it possible to use the result as pseudo gold standards to evaluate affix rules as described in section 4.3. 1 The bilingual terminology bank was compiled by the National Institute for Compilation and Translation. It is freely download at http://terms.nict.gov.tw by registering your information. 251 resulting in rules R1..Rn . And then for each Ri , we scan R1 to Ri-1, if there is a rule, Rj, have the same English word condition and the affix condition of Ri subsume that of Rj, then we add affix condition of Rj as exception condition of Ri. For example, _ 業 , industry and _工業, indu"
I08-1033,J93-2003,0,0.0301349,"Missing"
I08-1033,O98-3002,1,0.757578,"eriments showed that both of the adjusting methods improve the performance of word alignment significantly. 1 Introduction Word alignment is an important preprocessing task for statistical machine translation. There have been many statistical word alignment methods proposed since the IBM models have been introduced. Most existing methods treat word tokens as basic alignment units (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005), however, many languages have no explicit word boundary markers, such as Chinese and Japanese. In these languages, word segmentation (Chen and Liu, 1992; Chen and Bai, 1998; Chen and Ma, 2002; Ma and Chen, 2003; Gao et al., 2005) is often carried out firstly to identify words before word alignment (Wu and Xia, 1994). However, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (Ma et al., 2007; Wu, 1997). For instance, Chinese multi-syllabic words composed of more than one meaningful morpheme which may be translated to several English words. For example, the Chinese word 教育署 is composed of two meaning units, 教育 and 署 , and is translated to Department of Ed"
I08-1033,C02-1049,1,0.804801,"both of the adjusting methods improve the performance of word alignment significantly. 1 Introduction Word alignment is an important preprocessing task for statistical machine translation. There have been many statistical word alignment methods proposed since the IBM models have been introduced. Most existing methods treat word tokens as basic alignment units (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005), however, many languages have no explicit word boundary markers, such as Chinese and Japanese. In these languages, word segmentation (Chen and Liu, 1992; Chen and Bai, 1998; Chen and Ma, 2002; Ma and Chen, 2003; Gao et al., 2005) is often carried out firstly to identify words before word alignment (Wu and Xia, 1994). However, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (Ma et al., 2007; Wu, 1997). For instance, Chinese multi-syllabic words composed of more than one meaningful morpheme which may be translated to several English words. For example, the Chinese word 教育署 is composed of two meaning units, 教育 and 署 , and is translated to Department of Education in English."
I08-1033,C92-1019,1,0.71565,"cision tree. Our experiments showed that both of the adjusting methods improve the performance of word alignment significantly. 1 Introduction Word alignment is an important preprocessing task for statistical machine translation. There have been many statistical word alignment methods proposed since the IBM models have been introduced. Most existing methods treat word tokens as basic alignment units (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005), however, many languages have no explicit word boundary markers, such as Chinese and Japanese. In these languages, word segmentation (Chen and Liu, 1992; Chen and Bai, 1998; Chen and Ma, 2002; Ma and Chen, 2003; Gao et al., 2005) is often carried out firstly to identify words before word alignment (Wu and Xia, 1994). However, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (Ma et al., 2007; Wu, 1997). For instance, Chinese multi-syllabic words composed of more than one meaningful morpheme which may be translated to several English words. For example, the Chinese word 教育署 is composed of two meaning units, 教育 and 署 , and is translated"
I08-1033,P07-1003,0,0.0248751,"Missing"
I08-1033,H05-1022,0,0.0177728,". The first method is learning affix rules from a bilingual terminology bank. The second method is using the concept of impurity measure motivated by the decision tree. Our experiments showed that both of the adjusting methods improve the performance of word alignment significantly. 1 Introduction Word alignment is an important preprocessing task for statistical machine translation. There have been many statistical word alignment methods proposed since the IBM models have been introduced. Most existing methods treat word tokens as basic alignment units (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005), however, many languages have no explicit word boundary markers, such as Chinese and Japanese. In these languages, word segmentation (Chen and Liu, 1992; Chen and Bai, 1998; Chen and Ma, 2002; Ma and Chen, 2003; Gao et al., 2005) is often carried out firstly to identify words before word alignment (Wu and Xia, 1994). However, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (Ma et al., 2007; Wu, 1997). For instance, Chinese multi-syllabic words composed of more than one meaningful mo"
I08-1033,J05-4005,0,0.0303634,"the performance of word alignment significantly. 1 Introduction Word alignment is an important preprocessing task for statistical machine translation. There have been many statistical word alignment methods proposed since the IBM models have been introduced. Most existing methods treat word tokens as basic alignment units (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005), however, many languages have no explicit word boundary markers, such as Chinese and Japanese. In these languages, word segmentation (Chen and Liu, 1992; Chen and Bai, 1998; Chen and Ma, 2002; Ma and Chen, 2003; Gao et al., 2005) is often carried out firstly to identify words before word alignment (Wu and Xia, 1994). However, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (Ma et al., 2007; Wu, 1997). For instance, Chinese multi-syllabic words composed of more than one meaningful morpheme which may be translated to several English words. For example, the Chinese word 教育署 is composed of two meaning units, 教育 and 署 , and is translated to Department of Education in English. The morphemes 教育 and 署 have their own"
I08-1033,H05-1085,0,0.0935192,"Missing"
I08-1033,N03-1017,0,0.138776,"rce sentence. In this case, a many-to-one alignment, links a phrase in the source sentence to a single token in the target sentence, is not allowed, forcing most links of a phrase in the source sentence to be abolished. As in the previous example, when aligning from English to Chinese, 教育署 can only be linked to one of the English words, say Education, because of the limitation of the IBM model. However for remedy, many of the current word alignment methods combine the results of both alignment directions, via intersection or 249 grow-diag-final heuristic, to improve the alignment reliability (Koehn et al., 2003; Liang et al., 2006; Ayan et al., 2006; DeNero et al., 2007). However the many-to-one link limitation will undermine the reliability due to the fact that some links are not allowed in one of the directions. In this paper, we propose two novel methods to adjust word segmentation so as to decrease the effect of lexicalization differences to improve word alignment performance. The main idea of our methods is to adjust Chinese word segmentation according to their translation derived from parallel sentences in order to make the tokens compatible to 1-to-1 mapping between the corresponding sentence"
I08-1033,N04-4015,0,0.203541,"he performance of word alignment for several reasons. The first reason is that it will reduce the cooccurrence counts of Chinese and English tokens. Consider the previous example. Since 教育署 is treated as a single unit, it does not contribute to the occurrence counts of Education/ 教育 and Department/署 token pairs. Secondly, the rarely occurring compound word may cause the garbage collectors effect (Moore, 2004; Liang et al., 2006), aligning a rare word in source language to too many words in the target language, due to the frequency imbalance with the corresponding translation words in English (Lee, 2004). Finally, the IBM models (Moore, 2004) impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence. In this case, a many-to-one alignment, links a phrase in the source sentence to a single token in the target sentence, is not allowed, forcing most links of a phrase in the source sentence to be abolished. As in the previous example, when aligning from English to Chinese, 教育署 can only be linked to one of the English words, say Education, because of the limitation of the IBM model. However for remedy, many of the current word alignment"
I08-1033,N06-1014,0,0.0868452,"Education in English. The morphemes 教育 and 署 have their own meanings and are translated to Education and Department respectively. The phenomenon of lexicalization mismatch will degrade the performance of word alignment for several reasons. The first reason is that it will reduce the cooccurrence counts of Chinese and English tokens. Consider the previous example. Since 教育署 is treated as a single unit, it does not contribute to the occurrence counts of Education/ 教育 and Department/署 token pairs. Secondly, the rarely occurring compound word may cause the garbage collectors effect (Moore, 2004; Liang et al., 2006), aligning a rare word in source language to too many words in the target language, due to the frequency imbalance with the corresponding translation words in English (Lee, 2004). Finally, the IBM models (Moore, 2004) impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence. In this case, a many-to-one alignment, links a phrase in the source sentence to a single token in the target sentence, is not allowed, forcing most links of a phrase in the source sentence to be abolished. As in the previous example, when aligning from English"
I08-1033,W03-1705,1,0.843322,"ing methods improve the performance of word alignment significantly. 1 Introduction Word alignment is an important preprocessing task for statistical machine translation. There have been many statistical word alignment methods proposed since the IBM models have been introduced. Most existing methods treat word tokens as basic alignment units (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005), however, many languages have no explicit word boundary markers, such as Chinese and Japanese. In these languages, word segmentation (Chen and Liu, 1992; Chen and Bai, 1998; Chen and Ma, 2002; Ma and Chen, 2003; Gao et al., 2005) is often carried out firstly to identify words before word alignment (Wu and Xia, 1994). However, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (Ma et al., 2007; Wu, 1997). For instance, Chinese multi-syllabic words composed of more than one meaningful morpheme which may be translated to several English words. For example, the Chinese word 教育署 is composed of two meaning units, 教育 and 署 , and is translated to Department of Education in English. The morphemes 教育 a"
I08-1033,P07-1039,0,0.0635679,"Missing"
I08-1033,P00-1056,0,0.065896,"lel corpus in three ways: First we use a state-of-the-art word segmenter to tokenize the Chinese part of the corpus. Then, we used the affix rules to adjust word segmentation. Finally, we do the same but by using the impurity measure method. We used the GIZA++ package (Och and Ney, 2003) as the word alignment tool to align tokens on the three copies of preprocessed parallel corpora. We used the first 100,000 sentences of Hong Kong News parallel corpus from LDC as our training data. And 112 randomly selected parallel sentences were aligned manually with sure and possible tags, as described in (Och and Ney, 2000), 4 and we used these annotated data as our gold standard in testing. Because of the modification of Chinese tokens caused by the word segmentation adjustment, a problem has been created when we wanted to compare the results to the copy which did not undergo adjustment. Therefore, after the alignment was done, we merged the alignment links related to tokens that were split up during adjustment. For example, the two links of foreign/外交 minister/部 長 were merged as foreign minister/外交部長. The evaluation of word alignment results are shown in Table 1, including precision-recall and AER evaluation m"
I08-1033,C96-2141,0,0.214769,"responding sentences. The first method is learning affix rules from a bilingual terminology bank. The second method is using the concept of impurity measure motivated by the decision tree. Our experiments showed that both of the adjusting methods improve the performance of word alignment significantly. 1 Introduction Word alignment is an important preprocessing task for statistical machine translation. There have been many statistical word alignment methods proposed since the IBM models have been introduced. Most existing methods treat word tokens as basic alignment units (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005), however, many languages have no explicit word boundary markers, such as Chinese and Japanese. In these languages, word segmentation (Chen and Liu, 1992; Chen and Bai, 1998; Chen and Ma, 2002; Ma and Chen, 2003; Gao et al., 2005) is often carried out firstly to identify words before word alignment (Wu and Xia, 1994). However, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (Ma et al., 2007; Wu, 1997). For instance, Chinese multi-syllabic words composed of more"
I08-1033,1994.amta-1.26,0,0.0808119,"portant preprocessing task for statistical machine translation. There have been many statistical word alignment methods proposed since the IBM models have been introduced. Most existing methods treat word tokens as basic alignment units (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005), however, many languages have no explicit word boundary markers, such as Chinese and Japanese. In these languages, word segmentation (Chen and Liu, 1992; Chen and Bai, 1998; Chen and Ma, 2002; Ma and Chen, 2003; Gao et al., 2005) is often carried out firstly to identify words before word alignment (Wu and Xia, 1994). However, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (Ma et al., 2007; Wu, 1997). For instance, Chinese multi-syllabic words composed of more than one meaningful morpheme which may be translated to several English words. For example, the Chinese word 教育署 is composed of two meaning units, 教育 and 署 , and is translated to Department of Education in English. The morphemes 教育 and 署 have their own meanings and are translated to Education and Department respectively. The phenomenon of"
I08-1033,J97-3002,0,0.329284,"okens as basic alignment units (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005), however, many languages have no explicit word boundary markers, such as Chinese and Japanese. In these languages, word segmentation (Chen and Liu, 1992; Chen and Bai, 1998; Chen and Ma, 2002; Ma and Chen, 2003; Gao et al., 2005) is often carried out firstly to identify words before word alignment (Wu and Xia, 1994). However, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (Ma et al., 2007; Wu, 1997). For instance, Chinese multi-syllabic words composed of more than one meaningful morpheme which may be translated to several English words. For example, the Chinese word 教育署 is composed of two meaning units, 教育 and 署 , and is translated to Department of Education in English. The morphemes 教育 and 署 have their own meanings and are translated to Education and Department respectively. The phenomenon of lexicalization mismatch will degrade the performance of word alignment for several reasons. The first reason is that it will reduce the cooccurrence counts of Chinese and English tokens. Consider t"
I08-1033,P03-1051,0,\N,Missing
I08-1033,J03-1002,0,\N,Missing
I08-2098,I05-1016,1,0.815807,"abilities of hypothesis conjunctive boundary pairs by the feature patterns listed above. The top ranked candidates are selected according to the CRF model. In general, for further improvement, a final step of semantic evaluation will be performed to select the best candidate from top-N boundary structures ranked by the CRF model, which is described in the next section. 3,484 sentences of the Sinica Treebank are used as training data. The development data and testing data are extracted from three different set of corpora the Sinica corpus, Sinorama magazines and textbooks of elementary school (Hsieh et al. 2005). They are totally 202 sentences (244 conjunctions) with 6-10 words and 107 sentences (159 conjunctions) with more than 11 words. We only test the sentences which contain the coordinate conjunction category or categories. We adopt the standard PARSEVAL metrics (Manning et al., 1999) including bracket f-score to evaluate the performance of the tree structures of sentences and accuracies of boundary detection of conjunction structures. 3.2 4.1 The word-association evaluation model For the purpose of selecting the best candidates of complex conjunctive structures, a word association evaluation mo"
I08-2098,O07-4005,1,0.454238,"Missing"
I08-2098,J94-4001,0,0.153815,"Missing"
I08-2098,I05-1014,0,0.0264272,"ail of the semantic evaluation model is described in the section 3.2. The reason for using a two-stage approach is that the size of the Treebank is limited, but the semantic evaluation model requires the values of association strengths between words. The current Treebank cannot provide enough coverage and reliable values of wordassociation strengths. 3.1 Derive and evaluate possible candidates CRF is a well-known probabilistic framework for segmenting and labeling sequence data (Lafferty, et al. 2001). In our experiments, we regard the problem of boundary detection as a chunking-like problem (Lee et al., 2005). Due to this reason, we use CRF model to generate candidates and their ranks. The features used in CRF model included some global syntactic information, such as syntactic category of a partial structure and its phrasal head. Such global syntactic information is crucial for the success of boundary detection and is not available if without the step of parsing process. WL,i+1 (WLi WL,i-1 …. WL1 W0 WR1 …WR,j )WR,j+1, Some example feature values of the above hypothesis boundaries. WLi = 我; CLi =Nh; WR,j =車禍; CR,j =Na; PL, =S; PR, =NP; HwL=發明; HcL= VC; HwR =車禍; HcR=Na; DL = 5; DR = 2; Figure 1. The"
I08-2098,P92-1003,0,0.187469,"Missing"
I13-1103,N06-1003,0,0.0314116,"to a set of suggested translations. Our experiment results demonstrate that the translations suggested by the unknown word translation template model significantly improve the performance of the Moses machine translation system. 1 Introduction Automatic translation of unknown words is still an open problem. As a result, most statistical machine translation (SMT) systems treat such words as unknown tokens and leave them untranslated. (Koehn et al., 2003; Chiang, 2005; Koehn et al., 2007) The unknown word translation problem has generated considerable interest in recent years. Some works (e.g., Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009) focus on finding in-vocabulary paraphrases, which are then used as bridges to translate target unknown words. Li and Yarowsky (2008) proposed an unsupervised method for extracting the mappings from Chinese abbreviations and their full-forms. The method exploits the full-forms as bridges to translate the abbreviations. A prerequisite of the above methods is that the unknown words must have paraphrases (or full-forms). However, many types of unknown words do not have paraphrases (full-forms) naturally. In contrast to paraphrasing methods, Huang et al."
I13-1103,N03-1017,0,0.0405684,"nknown word is detected during translation, the model applies translation templates to the word to get a set of matched templates, and then translates the word into a set of suggested translations. Our experiment results demonstrate that the translations suggested by the unknown word translation template model significantly improve the performance of the Moses machine translation system. 1 Introduction Automatic translation of unknown words is still an open problem. As a result, most statistical machine translation (SMT) systems treat such words as unknown tokens and leave them untranslated. (Koehn et al., 2003; Chiang, 2005; Koehn et al., 2007) The unknown word translation problem has generated considerable interest in recent years. Some works (e.g., Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009) focus on finding in-vocabulary paraphrases, which are then used as bridges to translate target unknown words. Li and Yarowsky (2008) proposed an unsupervised method for extracting the mappings from Chinese abbreviations and their full-forms. The method exploits the full-forms as bridges to translate the abbreviations. A prerequisite of the above methods is that the unknown words mus"
I13-1103,W04-3250,0,0.0930513,"expansion methods significantly improved the underlying SMT system. To verify the stability of this method, we also rebuilt a baseline system and an unknown word translation model based on the FBIS parallel corpus, as shown in Table 2. Baseline Trans. table Phonetic Numeric All MT06 24.38 24.54 (+0.16) 24.78 (+0.40) 24.64 (+0.26) 25.09 (+0.71) MT08_sub 19.94 20.21 (+0.27) 20.28 (+0.34) 20.09 (+0.15) 20.65 (+0.71) Table 2. Evaluation results based on the FBIS parallel corpus. The improvement in the BLEU score is statistically significant (p < 0.01) under the paired bootstrap re-sampling test (Koehn, 2004). The experimental results show that the proposed translation template model significantly improves the performance of the statistical machine translation system. 5 Conclusion We have proposed a method that utilizes a translation template model to translate Chinese unknown words. The translation templates can be automatically extracted from a word-aligned parallel corpus and evaluated without using extra information. Experimental results show that the model can suggest accurate unknown word translations for an existing SMT system and improve the translation quality. References Berger, Adam L.,"
I13-1103,P07-2045,0,0.0285114,"nslation, the model applies translation templates to the word to get a set of matched templates, and then translates the word into a set of suggested translations. Our experiment results demonstrate that the translations suggested by the unknown word translation template model significantly improve the performance of the Moses machine translation system. 1 Introduction Automatic translation of unknown words is still an open problem. As a result, most statistical machine translation (SMT) systems treat such words as unknown tokens and leave them untranslated. (Koehn et al., 2003; Chiang, 2005; Koehn et al., 2007) The unknown word translation problem has generated considerable interest in recent years. Some works (e.g., Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009) focus on finding in-vocabulary paraphrases, which are then used as bridges to translate target unknown words. Li and Yarowsky (2008) proposed an unsupervised method for extracting the mappings from Chinese abbreviations and their full-forms. The method exploits the full-forms as bridges to translate the abbreviations. A prerequisite of the above methods is that the unknown words must have paraphrases (or full-forms)."
I13-1103,P08-1049,0,0.022749,"rmance of the Moses machine translation system. 1 Introduction Automatic translation of unknown words is still an open problem. As a result, most statistical machine translation (SMT) systems treat such words as unknown tokens and leave them untranslated. (Koehn et al., 2003; Chiang, 2005; Koehn et al., 2007) The unknown word translation problem has generated considerable interest in recent years. Some works (e.g., Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009) focus on finding in-vocabulary paraphrases, which are then used as bridges to translate target unknown words. Li and Yarowsky (2008) proposed an unsupervised method for extracting the mappings from Chinese abbreviations and their full-forms. The method exploits the full-forms as bridges to translate the abbreviations. A prerequisite of the above methods is that the unknown words must have paraphrases (or full-forms). However, many types of unknown words do not have paraphrases (full-forms) naturally. In contrast to paraphrasing methods, Huang et al. (2011) developed a sublexical translation method that translates an unknown word by combining the translations of its sublexicals. However, to deal with the reordering problem,"
I13-1103,W03-1726,1,0.722188,"n rules instead of translation table to generate the translations of morphemes. We use two types of morphological translation rules: numerical and phonetic morphological translation rules. 3 Experimental Setting We evaluate the model on Moses (Koehn et al., 2007) by embedding the translations of the unknown words to test data as suggestion translations. 3.1 Baseline SMT System and Data Sets We used the Hong Kong Parallel Text (LDC2004T08) as the training data for the Moses SMT system and our template model. The Chinese sentences were pre-processed by the CKIP Chinese word segmentation system (Ma and Chen, 2003). The language model was trained on the English Gigaword corpus (LDC2003T05). We randomly selected 340 841 sentences from the NIST MT08 test data as our development set, the NIST MT06 test data and the rest of the NIST MT08 as our test set. 3.2 Training The parallel text was word-aligned by the GIZA++ toolkit (Och and Ney, 2003). Then, we utilized the word-aligned corpus to extract translation templates. This process yielded a set of translation templates and a translation template tagged corpus, which was used to train the fitting probability model. To evaluate the fitting probability model,"
I13-1103,D09-1040,0,0.0151906,"tions. Our experiment results demonstrate that the translations suggested by the unknown word translation template model significantly improve the performance of the Moses machine translation system. 1 Introduction Automatic translation of unknown words is still an open problem. As a result, most statistical machine translation (SMT) systems treat such words as unknown tokens and leave them untranslated. (Koehn et al., 2003; Chiang, 2005; Koehn et al., 2007) The unknown word translation problem has generated considerable interest in recent years. Some works (e.g., Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009) focus on finding in-vocabulary paraphrases, which are then used as bridges to translate target unknown words. Li and Yarowsky (2008) proposed an unsupervised method for extracting the mappings from Chinese abbreviations and their full-forms. The method exploits the full-forms as bridges to translate the abbreviations. A prerequisite of the above methods is that the unknown words must have paraphrases (or full-forms). However, many types of unknown words do not have paraphrases (full-forms) naturally. In contrast to paraphrasing methods, Huang et al. (2011) developed a su"
I13-1103,P09-1089,0,0.0642456,"Missing"
I13-1103,P03-1021,0,0.0147245,"ty model. We also rebuilt the experiments based on the FBIS Parallel Text (LDC2003E14), which contains about 300,000 parallel sentences to verify the stability of our model. The rebuilding process is the same as that for the Hong Kong Parallel Text. 4 Experimental Results We evaluated the translation template model on the NIST MT06 test set and NIST 08 subset. During the evaluation, the test sets were translated by the Moses SMT system with/without the embedded translation suggestions derived by the translation template model. The parameters in Moses were tuned by minimum-error-rate training (Och, 2003) on the development set. Baseline Trans. table Phonetic Numeric All MT06 23.36 23.47 (+0.11) 23.83 (+0.47) 23.43 (+0.07) 23.89 (+0.53) MT08_sub 19.36 19.46 (+0.10) 19.65 (+0.29) 19.44 (+0.08) 19.80 (+0.44) rules. In our experiments, we exploit phonetic and numerical morphological translation rules to generate translations of morphemes. Table 1 shows the performances of the translation results with/without unknown word translation suggestions. As it shows, all of the translation expansion methods significantly improved the underlying SMT system. To verify the stability of this method, we also r"
I13-1103,J03-1002,0,0.0031684,"suggestion translations. 3.1 Baseline SMT System and Data Sets We used the Hong Kong Parallel Text (LDC2004T08) as the training data for the Moses SMT system and our template model. The Chinese sentences were pre-processed by the CKIP Chinese word segmentation system (Ma and Chen, 2003). The language model was trained on the English Gigaword corpus (LDC2003T05). We randomly selected 340 841 sentences from the NIST MT08 test data as our development set, the NIST MT06 test data and the rest of the NIST MT08 as our test set. 3.2 Training The parallel text was word-aligned by the GIZA++ toolkit (Och and Ney, 2003). Then, we utilized the word-aligned corpus to extract translation templates. This process yielded a set of translation templates and a translation template tagged corpus, which was used to train the fitting probability model. To evaluate the fitting probability model, the translation template tagged corpus was randomly split into two parts to obtain a translation template tagged training set (about 1,800,000 sentences) and a translation template tagged test set (about 200,000 sentences). We used the translation template tagged training set to train the rule fitting probability model. Then, we"
I13-1103,J96-1002,0,\N,Missing
I13-1103,P05-1033,0,\N,Missing
N12-1036,C90-3008,0,0.123876,"(b) are not placed directly under the caret (current input focus) for space limit. (c) and (d) depict predominant grammar constructs which follow and (e) summarizes the confident translations of the source’s character-based ngrams. The frequency of grammar pattern is shown in round brackets while the history (i.e., keyword) based on the user input is shown in shades. emphasis on language learning. Specifically, our goal is to build a translation assistant to help translator (or learner-translator) with inline grammar help and translation. Unlike recent research focusing on professional (e.g., Brown and Nirenburg, 1990), we target on both professional and student translators. More recently, interactive MT (IMT) systems have begun to shift the user’s role from postediting machine output to collaborating with the machine to produce the target text. Foster et al (2000) describe TransType, a pioneering system that supports next word predictions. Along the similar line, Koehn (2009) develops caitra which predicts and displays phrasal translation suggestions one phrase at a time. The main difference between their systems and TransAhead is that we also display grammar patterns to provide the general patterns of pre"
N12-1036,W10-1005,0,0.0149145,"eturns top N predominant 2 syntactic patterns of the query. Such patterns characterizing the query’s word usages in the spirit of pattern grammar in (Hunston and Francis, 2000) and are collected across the target language. In the fourth and final stage, we exploit Cst for bilingual phrase acquisition, rather than a manual dictionary, to achieve better translation coverage and variety. We obtain phrase pairs through a number of steps, namely, leveraging IBM models for bidirectional word alignments, grow-diagonalfinal heuristics to extract phrasal equivalences (Koehn et al., 2003). Inspired by (Gamon and Leacock, 2010). 354 We first slice the source text S into characterlevel ngrams, represented by {si}. We also find the word-level ngrams of the translation prefix Tp. But this time we concentrate on the ngrams, may skipped, ending with the last word of Tp (i.e., pivoted on the last word) since these ngrams are most related to the subsequent grammar patterns. Step (3) and (4) retrieve translations and patterns learned from Section 3.2. Step (3) acquires the target-language active vocabulary that may be used to translate the source. To alleviate the word boundary issue in MT (Ma et al. (2007)), the word bound"
N12-1036,W11-1412,1,0.850846,"es of our working prototype. 4 Preliminary Results In developing TransAhead, we used British National Corpus and Hong Kong Parallel Text as target-language reference corpus and parallel training corpus respectively, and deployed GENIA tagger for lemma and POS analyses. To evaluate TransAhead in CAT and CALL, we introduced it to a class of 34 (Chinese) college freshmen learning English as foreign language. We designed TransAhead to be accessible and intuitive, so the user training tutorial took only one minute. After the tutorial, the participants were asked to translate 15 Chinese texts from (Huang et al., 2011) (half with TransAhead assistance called experimental group, and the other without any system help whatsoever called control group). The evaluation results show that the experimental group achieved much better translation quality than the control group with an average BLEU score (Papineni et al., 2002) of 35.49 vs. 26.46. Admittedly, the MT system Google Translate produced translations with a higher BLEU score of 44.82. Google Translate obviously has much more parallel training data and bilingual translation knowledge. No previous work in CAT uses Google Translate for comparison. Although ther"
N12-1036,N03-1017,0,0.0122139,"the query. The procedure finally returns top N predominant 2 syntactic patterns of the query. Such patterns characterizing the query’s word usages in the spirit of pattern grammar in (Hunston and Francis, 2000) and are collected across the target language. In the fourth and final stage, we exploit Cst for bilingual phrase acquisition, rather than a manual dictionary, to achieve better translation coverage and variety. We obtain phrase pairs through a number of steps, namely, leveraging IBM models for bidirectional word alignments, grow-diagonalfinal heuristics to extract phrasal equivalences (Koehn et al., 2003). Inspired by (Gamon and Leacock, 2010). 354 We first slice the source text S into characterlevel ngrams, represented by {si}. We also find the word-level ngrams of the translation prefix Tp. But this time we concentrate on the ngrams, may skipped, ending with the last word of Tp (i.e., pivoted on the last word) since these ngrams are most related to the subsequent grammar patterns. Step (3) and (4) retrieve translations and patterns learned from Section 3.2. Step (3) acquires the target-language active vocabulary that may be used to translate the source. To alleviate the word boundary issue i"
N12-1036,P09-4005,0,0.135772,"on language learning. Specifically, our goal is to build a translation assistant to help translator (or learner-translator) with inline grammar help and translation. Unlike recent research focusing on professional (e.g., Brown and Nirenburg, 1990), we target on both professional and student translators. More recently, interactive MT (IMT) systems have begun to shift the user’s role from postediting machine output to collaborating with the machine to produce the target text. Foster et al (2000) describe TransType, a pioneering system that supports next word predictions. Along the similar line, Koehn (2009) develops caitra which predicts and displays phrasal translation suggestions one phrase at a time. The main difference between their systems and TransAhead is that we also display grammar patterns to provide the general patterns of predicted translations so a student translator can learn and become more proficient. Recent work has been done on using fullyfledged statistical MT systems to produce target hypotheses completing user-validated translation prefix in IMT paradigm. Barrachina et al. (2008) investigate the applicability of different MT kernels within IMT framework. Nepveu et al. (2004)"
N12-1036,P07-1039,0,0.0192618,"ired by (Gamon and Leacock, 2010). 354 We first slice the source text S into characterlevel ngrams, represented by {si}. We also find the word-level ngrams of the translation prefix Tp. But this time we concentrate on the ngrams, may skipped, ending with the last word of Tp (i.e., pivoted on the last word) since these ngrams are most related to the subsequent grammar patterns. Step (3) and (4) retrieve translations and patterns learned from Section 3.2. Step (3) acquires the target-language active vocabulary that may be used to translate the source. To alleviate the word boundary issue in MT (Ma et al. (2007)), the word boundary in our system is loosely decided. Initially, TransAhead non-deterministically segments the source text using character ngrams for translations and proceeds with collaborations with the user to obtain the segmentation for MT and to complete the translation. Note that Tp may reflect some translated segments, reducing the size of the active vocabulary, and that a user vocabulary of preference (due to users’ domain knowledge or errors of the system) may be exploited for better system performance. In addition, Step (4) extracts patterns preceding with the history ngrams of {tj}"
N12-1036,W04-3225,0,0.0303112,"r line, Koehn (2009) develops caitra which predicts and displays phrasal translation suggestions one phrase at a time. The main difference between their systems and TransAhead is that we also display grammar patterns to provide the general patterns of predicted translations so a student translator can learn and become more proficient. Recent work has been done on using fullyfledged statistical MT systems to produce target hypotheses completing user-validated translation prefix in IMT paradigm. Barrachina et al. (2008) investigate the applicability of different MT kernels within IMT framework. Nepveu et al. (2004) and Ortiz-Martinez et al. (2011) further exploit user feedbacks for better IMT systems and user experience. Instead of triggered by user correction, our method is triggered by word 353 delimiter and assists both translation and learning the target language. In contrast to the previous CAT research, we present a writing assistant that suggests grammar constructs as well as lexical translations following users’ partial translation, aiming to provide users with choice to ease mental burden and enhance performance. 3 The TransAhead System 3.1 Problem Statement We focus on predicting a set of gram"
N12-1036,P11-4012,0,0.0117831,"lops caitra which predicts and displays phrasal translation suggestions one phrase at a time. The main difference between their systems and TransAhead is that we also display grammar patterns to provide the general patterns of predicted translations so a student translator can learn and become more proficient. Recent work has been done on using fullyfledged statistical MT systems to produce target hypotheses completing user-validated translation prefix in IMT paradigm. Barrachina et al. (2008) investigate the applicability of different MT kernels within IMT framework. Nepveu et al. (2004) and Ortiz-Martinez et al. (2011) further exploit user feedbacks for better IMT systems and user experience. Instead of triggered by user correction, our method is triggered by word 353 delimiter and assists both translation and learning the target language. In contrast to the previous CAT research, we present a writing assistant that suggests grammar constructs as well as lexical translations following users’ partial translation, aiming to provide users with choice to ease mental burden and enhance performance. 3 The TransAhead System 3.1 Problem Statement We focus on predicting a set of grammar patterns with lexical transla"
N12-1036,P02-1040,0,0.083121,"and CALL, we introduced it to a class of 34 (Chinese) college freshmen learning English as foreign language. We designed TransAhead to be accessible and intuitive, so the user training tutorial took only one minute. After the tutorial, the participants were asked to translate 15 Chinese texts from (Huang et al., 2011) (half with TransAhead assistance called experimental group, and the other without any system help whatsoever called control group). The evaluation results show that the experimental group achieved much better translation quality than the control group with an average BLEU score (Papineni et al., 2002) of 35.49 vs. 26.46. Admittedly, the MT system Google Translate produced translations with a higher BLEU score of 44.82. Google Translate obviously has much more parallel training data and bilingual translation knowledge. No previous work in CAT uses Google Translate for comparison. Although there is a 355 difference in average translation quality between the experimental TransAhead group and the Google Translate, it is not hard for us to notice the source sentences were better translated by language learners with the help of TransAhead. Take the sentence “我們在完成這筆交易上扮演重要角 色” for example. A tot"
N12-1036,J09-1002,0,\N,Missing
O00-2001,Y99-1005,1,0.780744,"Missing"
O00-2004,J93-2005,0,\N,Missing
O01-1012,O98-3002,1,0.895819,"Missing"
O01-1012,W00-1203,1,0.863178,"Missing"
O02-2002,J94-4005,0,\N,Missing
O02-2002,O96-2006,1,\N,Missing
O02-2002,C94-2119,0,\N,Missing
O02-2002,P98-2127,0,\N,Missing
O02-2002,C98-2122,0,\N,Missing
O03-1010,W93-0305,0,0.0759435,"Missing"
O03-1010,H92-1022,0,0.0659015,"instead of P(c k |c k −1 ) . The probability of each candidate PoS ck in Bayesian classifier is calculated by P(c k −1 |wk , c k ) * P(c k +1 |wk , c k ) * P(c k |wk ) . The Bayesian classifier tags the PoS c k for wk , such that c k maximizes the probability of P(c k −1 |wk , c k ) * P(c k +1 |wk , c k ) * P(c k |wk ) . 3.3 Context-Rule Model Dependency features utilized in determining the best PoS-tag in both Markov and Bayesian models are categories of context words. As a matter of fact, for some cases the best PoS-tags might be determined by other context features, such as context words (Brill, 1992). In the context-rule model, broader scope of context information is utilized in determining the best PoS-tag. We extend the scope of the dependency context of a target word into its 2 by 2 context windows. Therefore the context features of a word can be represented by the vector of [ w− 2 , c − 2 , w−1 , c −1 , w1 , c1 , w2 , c 2 ] . Each feature vector may be associated with a unique PoS-tag or many ambiguous PoS-tags. Their association probability of a possible PoS c 0′ is P( c 0′ |w0 , feature vector). If for some ( w0 , c 0′ ), the value of P( c 0′ |w0 , feature vector) is not 1, it means"
O03-1010,Y96-1018,1,0.791254,"視(VJ) 研究(Nv) 開發(Nv) ，(COMMACATEGORY) 內(Ncd) 重點(Na) 研究(Nv) 需求(Na) 。(PERIODCATEGORY) 仍(D) 限於(VJ) 研究(Nv) 階段(Na) 。(PERIODCATEGORY) 民族(Na) 音樂(Na) 研究(VE) 者(Na) 明立國(Nb) 赴(VCL) 香港(Nc) 研究(VE) 該(Nes) 地(Na) 亦(D) 值得(VH) 研究(VE) 。(PERIODCATEGORY) 合宜性(Na) 值得(VH) 研究(VE) 。(PERIODCATEGORY) 更(D) 值得(VH) 研究(Nv) 。(PERIODCATEGORY) Table 1 Sample keyword-in-context file of the words ‘研究’ sorted by its left/right context Markov bi-gram model, Bayesian classifier, and context-rule classifier. The training data and testing data are extracted from Sinica corpus, a 5 million word balanced Chinese corpus with PoS tagging (Chen et al., 1996). The confidence measure will be defined for each algorithm and the best accuracy will be estimated at the constraint of only a fixed amount of testing data being proofread. It is easier to proofread and make more consistent tagging results, if proofreading processes were done by checking the keyword-in-context file for each ambivalence word and only the tagging results of ambivalence word need to be proofread. The words with single PoS need not be rechecked their PoS tagging. For instance, in Table 1, the keyword-in-context file of the word ‘研究’ (research), which has PoS of verb type VE and n"
O03-1010,C02-1021,0,0.0617115,"Missing"
O03-1010,C02-1101,0,0.0612564,"Missing"
O04-1014,P97-1073,0,0.0479654,"Missing"
O04-1015,J98-4004,0,0.189104,"Missing"
O04-1015,W04-1116,1,0.754535,"Missing"
O04-2005,H92-1022,0,0.0873467,"Missing"
O04-2005,Y96-1018,1,0.723207,"Missing"
O04-2005,C02-1021,0,0.0289768,"Missing"
O04-2005,C02-1101,0,0.0439909,"Missing"
O04-2005,W93-0305,0,\N,Missing
O05-1016,W03-1726,1,0.823656,"ation are 5 6 The symbol “ Nc”stands for Place Noun in Sinica Corpus. The symbol “ Nh”stands for Pronoun. different. The morpheme zhi yi in (21) is tagged as a unit whose POS is Nc, in (22) is segmented into two units whose POS is individually DE7 and Neu, while in (23) is the part of the whole quantitative determinative sanfenzhiyi tagged as Neqa8. To reduce these structural ambiguities, the DM rule (24) is necessary. According to the above discussions, to resolve structural ambiguities, we conclude the following resolution principles which were implemented at the word segmentation system by Ma and Chen (2003). a) D-M compounds are expressed and matched by regular expressions. b) Lexical words have higher precedence than D-M compounds (cf. 11). c) Long D-M has higher precedence than short D-M (cf. 12, 16, 17, 18, 21, 22, 23). d) Covering ambiguities are resolved by collocation context (cf. 10, 14, 15). The structural ambiguity is caused by different possible segmentation. Although example (10) has structural ambiguities, after the application of the resolution principles, the ambiguous segmentation is resolved and the correct segmentation has higher priority. 3.2 Sense Ambiguities of DMs Senses and"
O05-1016,O91-1003,1,0.790699,"me point specifying the event-time of the verb, or denotes the period of time delimitating the time length of the event. The former temporal adverb is tagged as Nd; the latter is separated into two morphemes and individually tagged as Neu and Nf.2 Examples (1) to (3) show the different degree of ambiguity. 1 The symbol in Sinica Corpus, “ VJ”stands for Stative Transitive Verb and “ Nf”for Measure. detailed parts of speech can be referred to Sinica Corpus website. 2 The symbol “ Nd”stands for Time Noun and “ Neu”for Numeral Determinatives. The Due to the infinite of the number of possible DMs, Mo et al. (1991) propose to identify DMs by regular expression before parsing as part of their morphological module in NLP. The adoption of DMs rules really improves the accuracy of recognition, but we still have some difficulties in segmentation as the preceding examples. In this paper, the discussion and classification of ambiguities of DMs are the focus. In addition to the typical DM structure with the combination of one or more determinatives with a measure, the reduplicative DMs and the ellipsis of determinatives will be also included under investigation. After the analyses of multiple ambiguities, we tr"
O05-1016,W04-1116,1,0.846384,"Missing"
O05-1016,J00-4006,0,\N,Missing
O05-1020,P00-1056,0,0.0361479,"Missing"
O05-1020,P04-1037,0,0.0222436,"Missing"
O05-1020,P99-1020,0,0.0916688,"Missing"
O05-1020,P02-1033,0,0.0374565,"Missing"
O05-5001,W98-1120,0,\N,Missing
O05-5001,O99-4001,0,\N,Missing
O05-5001,Y03-1022,0,\N,Missing
O05-5001,J94-4001,0,\N,Missing
O05-5001,M95-1012,0,\N,Missing
O05-5001,A94-1007,0,\N,Missing
O05-5001,W03-1402,0,\N,Missing
O05-5001,W03-1509,0,\N,Missing
O05-5001,C02-1012,0,\N,Missing
O05-5001,J04-2002,0,\N,Missing
O05-5001,Y03-1014,0,\N,Missing
O05-5001,W03-1405,1,\N,Missing
O05-5001,W03-1403,0,\N,Missing
O05-5001,P05-1015,0,\N,Missing
O05-5001,P92-1003,0,\N,Missing
O05-5001,O03-1006,1,\N,Missing
O05-5001,O00-2002,1,\N,Missing
O05-5001,huang-etal-2004-sinica,1,\N,Missing
O05-5001,Y95-1011,1,\N,Missing
O05-5001,A97-1029,0,\N,Missing
O05-5001,O98-3003,1,\N,Missing
O05-5003,O00-2002,1,0.895838,"Missing"
O05-5003,C02-1031,0,0.0612216,"Missing"
O05-5003,Y00-1012,0,\N,Missing
O06-1005,P05-1022,0,0.059975,"Missing"
O06-1005,O04-1014,1,0.899547,"Missing"
O06-1005,I05-1016,1,0.82672,"only instances of phrasal structures and word dependencies but also their statistical distributions. Recently, probabilistic preferences for grammar rules and feature dependencies were incorporated to resolve structure-ambiguities and had great improvements on parsing performances. However, the automatic extracted grammars and feature-dependence pairs suffer the problem of low coverage. We proposed different approaches to solve these two different types of low coverage problems. For the low coverage of extracted grammar, a linguistically-motivated grammar generalization method is proposed in Hsieh et al. (2005). And the low coverage of word association pairs is resolved by a self-learning method of automatic parsing and extracting word dependency pairs from very large corpora. The linguistically-motivated generalized grammars are derived from probabilistic context-free grammars (PCFG) by right-association binarization and feature embedding (Hsieh et al., 2005). The binarized grammars have better coverage than the original grammars directly extracted from treebank. Features are embedded in the lexical and phrasal categories to improve the precision of generalized grammar. The important features adopt"
O06-1005,J98-4004,0,0.0260084,"odels, such as the Dependency Parsing in Chen et al. (2004). They take advantage of statistic information of word dependency in the parsing process to produce dependency structures. However, word association methods suffer low coverage for lacking very large tree-annotated training corpora, while checking dependency relation between word pairs. z to add on word semantic knowledge. CiLin and HowNet information are used in the statistic model in the experiment of Xiong et al. (2005). Their results prove to solve common parsing mistakes efficiently. z to use re-annotation method in grammar rule. Johnson (1998) thinks that re-annotating each node with the category of its parent category in Treebank is able to improve parsing performance. Klein et al. (2003) proposes internal/external/tag-splitting annotation strategies to obtain better results. z to build evaluator. Some people re-rank the structure values and find out the best parse (Collins, 2000; Charniak et al., 2005). At first hand, their parser produces a set of candidate parses for each sentence. Later, the reranker finds out the best tree through relevance features. The performance is better that without the reranker. This paper is going to"
O06-1005,P03-1054,0,0.0146227,"Missing"
O06-1005,Y03-1016,1,0.887186,"Missing"
O06-1005,I05-1007,0,0.0377159,"Missing"
O06-4003,P04-1037,0,0.032274,"Missing"
O06-4003,P06-1009,0,0.0254899,"Missing"
O06-4003,O03-5003,1,0.869064,"Missing"
O06-4003,P02-1033,0,0.0376345,"Missing"
O06-4003,C96-2141,0,0.521313,"Missing"
O06-4003,W04-0807,0,\N,Missing
O06-4003,J93-2003,0,\N,Missing
O06-4003,P00-1056,0,\N,Missing
O06-4004,O05-4005,0,0.0921473,"Missing"
O06-4004,J00-4006,0,0.024812,"Missing"
O06-4004,W03-1726,1,0.88115,"Missing"
O06-4004,O91-1003,1,0.604618,"Missing"
O06-4004,W04-1116,1,0.8907,"Missing"
O07-1012,I05-7001,1,0.891991,"Missing"
O07-4005,H91-1060,0,0.111268,"Missing"
O07-4005,P05-1022,0,0.147028,"Missing"
O07-4005,O04-1014,1,0.843104,"Missing"
O07-4005,I05-1016,1,0.850387,"Missing"
O07-4005,J98-4004,0,0.0418595,"Missing"
O07-4005,Y03-1016,1,0.877522,"Missing"
O07-4005,I05-1007,0,0.0260906,"Missing"
O07-4005,J03-4003,0,\N,Missing
O07-4005,P03-1054,0,\N,Missing
O08-1002,O06-4004,1,0.930659,"concepts. Based on the framework of E-HowNet, we intend to establish an automatic semantic composition mechanism to derive sense of compounds and phrases from lexical senses [2][3]. Determinative-Measure compounds (abbreviated as DM) are most common compounds in Chinese. Because a determinative and a measure normally coin a compound with unlimited versatility, the CKIP group does not define the E-HowNet representations for all DM compounds. Although the demonstrative, numerals, and measures may be listed exhaustively, their combination is inexhaustible. However their constructions are regular [4]. Therefore, an automatic identification schema in regular expression [4] and a semantic composition method under the framework of E-HowNet for DM compounds were developed. In this paper, we take DMs as an example to demonstrate how the E-HowNet semantic composition mechanism works in deriving the sense representations for all DM compounds. The remainder of this paper is organized as follows. The section 2 presents the background knowledge of DM compounds and sense representation in E-HowNet. We’ll describe our method in the section 3 and discuss the experiment result in the section 4 before w"
O08-1002,O91-1003,1,0.558056,"measures. They conclude not only does a measure word generally not take a classifier, but any measure word can be a classifier. In Tai’s opinion [5], in order to better understand the nature of categorization in a classifier system, it is not only desirable but also necessary to differentiate classifiers from measure words. These studies on the distinction between classifiers and measures are not very clear-cut. In this paper, we adopt the CKIP DM rule patterns and Part-of-Speeches for morpho-syntactic analysis, and therefore inherit the definition of determinative-measure compounds (DMs) in [10]. Mo et al. define a DM as the composition of one or more determinatives together with an optional measure. It is used to determine the reference or the quantity of the noun phrase that co-occurs with it. We use the definition of Mo et al. to apply to NLP and somewhat different from traditional linguistics definitions. 2.1 Regular Expression Approach for Identifying DMs Due to the infinite of the number of possible DMs, Mo et al. [10] and Li et al. [4] propose to identify DMs by regular expression before parsing as part of their morphological module in NLP. For example, when the DM compound is"
O08-1002,I05-7001,1,0.83937,"cal structures of DM compounds, but do not derive the senses of complex DM compounds. 2.2 Lexical Sense Representation in E-HowNet Core senses of natural language are compositions of relations and entities. Lexical senses are processing units for sense composition. Conventional linguistic theories classify words into content words and function words. Content words denote entities and function words without too much content sense mainly serve grammatical function which links relations between entities/events. In E-HowNet, the senses of function words are represented by semantic roles/relations [11]. For example, ‘because’ is a function word. Its E-HowNet definition is shown in (1). (1) because|因為 def: reason={}; which means reason(x)={y} where x is the dependent head and y is the dependent daughter of ‘因為’. In following sentence (2), we’ll show how the lexical concepts are combined into the sense representation of the sentence. (2) Because of raining, clothes are all wet. 因為下雨，衣服都濕了 In the above sentence, ‘濕 wet’, ‘衣服 clothes’ and ‘下雨 rain’ are content words while ‘都 all’, ‘了 Le’ and ‘因為 because’ are function words. The difference of their representation is that function words start wit"
O08-1012,O07-6001,1,0.467596,"Missing"
O08-1012,W96-0213,0,0.193723,"Missing"
O08-1012,O04-2005,1,0.894283,"Missing"
O08-2008,O04-1030,1,0.840861,"Missing"
O08-2008,huang-etal-2004-sinica,1,0.898065,"Missing"
O08-2008,W03-1726,1,0.856717,"Missing"
O08-2008,W03-1705,1,0.849533,"Missing"
O08-5001,I05-7001,1,0.847687,"Missing"
O08-5001,Y07-1013,1,0.796911,"Missing"
O08-5001,O08-5001,1,0.0513221,"Missing"
O09-3002,I05-7001,1,0.904762,"Missing"
O09-3002,O06-4004,1,0.860805,"Missing"
O09-3002,O91-1003,1,0.547838,"Missing"
O09-5001,1995.tmi-1.18,0,0.0358754,"Missing"
O09-5001,O07-6001,1,0.387968,"Missing"
O09-5001,W96-0213,0,0.304119,"Missing"
O09-5001,O04-2005,1,0.866255,"Missing"
O11-1009,I05-7001,1,0.838089,"Missing"
O12-3001,I05-7001,1,\N,Missing
O13-1013,P97-1018,0,\N,Missing
O13-5004,P97-1018,0,\N,Missing
O14-3003,I05-7001,1,\N,Missing
O96-2006,O89-1001,1,0.877053,"Missing"
O96-2006,C90-2010,1,0.880244,"Missing"
O96-2006,C92-1019,1,0.930215,"Missing"
O96-2006,J90-1003,0,0.012331,"Missing"
O96-2006,P96-1025,0,0.0313819,"Missing"
O96-2006,C86-1045,0,0.0581974,"Missing"
O96-2006,J91-4001,0,\N,Missing
O97-4003,C92-1019,1,0.799397,"Missing"
O97-4003,Y96-1018,1,0.914189,"Missing"
O97-4003,O92-1003,0,0.0681692,"Missing"
O97-4003,P89-1010,0,0.0545131,"Missing"
O97-4003,P94-1010,0,0.0461372,"Missing"
O98-3002,J95-4004,0,0.0846872,"Missing"
O98-3002,J96-3004,0,\N,Missing
O98-3002,C92-1019,1,\N,Missing
O98-3002,O97-4003,1,\N,Missing
O98-3002,O93-1004,0,\N,Missing
O98-3002,Y96-1018,1,\N,Missing
O98-3002,O92-1003,0,\N,Missing
O98-3004,C96-1055,0,0.0585671,"Missing"
O98-3004,J93-2005,0,0.323339,"Missing"
P12-3010,P04-3019,1,0.650314,"ents of the query are identified in the target sentences. It helps not only on finding translation equivalents of the query but also presenting various contexts of occurrence. As a result, it is extremely useful for bilingual 1 lexicographers, human translators and second language learners (Bowker and Barlow 2004; Bourdaillet et al., 2010; Gao 2011). Identifying the translation equivalents, translation spotting, is the most challenging part of a bilingual concordancer. Recently, most of the existing bilingual concordancers spot translation equivalents in terms of word alignment-based method. (Jian et al., 2004; Callison-Burch et al., 2005; Bourdaillet et al., 2010). However, word alignment-based translation spotting has some drawbacks. First, aligning a rare (low frequency) term may encounter the garbage collection effect (Moore, 2004; Liang et al., 2006) that cause the term to align to many unrelated words. Second, the statistical word alignment model is not good at many-to-many alignment due to the fact that translation equivalents are not always correlated in lexical level. Unfortunately, the above effects will be intensified in a domain-specific concordancer because the queries are usually doma"
P12-3010,D09-1050,1,0.870206,"Missing"
P12-3010,P04-3031,0,0.0591779,"lnf function is that: when counting the local frequency of t in a sentence pair, if t is a subsequence of H, then the count of t should be reasonably reduced by considering the strength of the correlation between the words in H-t and the query. 3 Experimental Results Experimental Setting We use the Chinese/English web pages of the National Palace Museum 2 as our underlying parallel corpus. It contains about 30,000 sentences in each language. We exploited the Champollion Toolkit (Ma et al., 2006) to align the sentence pairs. The English sentences are tokenized and lemmatized by using the NLTK (Bird and Loper, 2004) and the Chinese sentences are segmented by the CKIP Chinese segmenter (Ma and Chen, 2003). To evaluate the performance of the translation spotting, we selected 12 domain-specific terms to query the concordancer. Then, the returned spotted translation equivalents are evaluated against a manually annotated gold standard in terms of recall and precision metrics. We also build two different translation spotting modules by using the GIZA++ toolkit (Och and Ney, 2000) with the intersection/union of the bidirectional word alignment as baseline systems. To evaluate the performance of the ranking crit"
P12-3010,W04-1408,0,0.0298981,"tion spotting module and the ranking module are evaluated in terms of precision-recall measures and coverage rate respectively. 1 Introduction A bilingual concordancer is a tool that can retrieve aligned sentence pairs in a parallel corpus whose source sentences contain the query and the translation equivalents of the query are identified in the target sentences. It helps not only on finding translation equivalents of the query but also presenting various contexts of occurrence. As a result, it is extremely useful for bilingual 1 lexicographers, human translators and second language learners (Bowker and Barlow 2004; Bourdaillet et al., 2010; Gao 2011). Identifying the translation equivalents, translation spotting, is the most challenging part of a bilingual concordancer. Recently, most of the existing bilingual concordancers spot translation equivalents in terms of word alignment-based method. (Jian et al., 2004; Callison-Burch et al., 2005; Bourdaillet et al., 2010). However, word alignment-based translation spotting has some drawbacks. First, aligning a rare (low frequency) term may encounter the garbage collection effect (Moore, 2004; Liang et al., 2006) that cause the term to align to many unrelated"
P12-3010,2005.eamt-1.9,0,0.0293366,"re identified in the target sentences. It helps not only on finding translation equivalents of the query but also presenting various contexts of occurrence. As a result, it is extremely useful for bilingual 1 lexicographers, human translators and second language learners (Bowker and Barlow 2004; Bourdaillet et al., 2010; Gao 2011). Identifying the translation equivalents, translation spotting, is the most challenging part of a bilingual concordancer. Recently, most of the existing bilingual concordancers spot translation equivalents in terms of word alignment-based method. (Jian et al., 2004; Callison-Burch et al., 2005; Bourdaillet et al., 2010). However, word alignment-based translation spotting has some drawbacks. First, aligning a rare (low frequency) term may encounter the garbage collection effect (Moore, 2004; Liang et al., 2006) that cause the term to align to many unrelated words. Second, the statistical word alignment model is not good at many-to-many alignment due to the fact that translation equivalents are not always correlated in lexical level. Unfortunately, the above effects will be intensified in a domain-specific concordancer because the queries are usually domain-specific terms, which are"
P12-3010,P93-1003,0,0.287529,"until no word add to H ej←left neighbor of H If nc(ej ) θ: add ej to H ek←right neighbor of H If nc( ek ) θ: add ek to H Figure 4: Pseudo-code of translation spotting process. 2.3 Ranking The ranking mechanism of a bilingual concordancer is used to provide the most related translation of the query on the top of the outputs for the user. So, an association metric is needed to evaluate the relations between the query and the spotted translations. The Dice coefficient is a widely used measure for assessing the association strength between a multi-word expression and its translation candidates. (Kupiec, 1993; Smadja et al., 1996; Kitamura and Matsumoto, 1996; Yamamoto and Matsumoto, 2000; Melamed, 2001) The following is the definition of the Dice coefficient: Then, we modify the Dice coefficient by replacing the co-occurrence frequency with normalized frequency as follows: nf_dice(t, q)  (4) where q denotes a multi-word expression to be translated, t denotes a translation candidate of q. However, the Dice coefficient has the common subsequence effect (as mentioned in Section 1) due to the fact that the co-occurrence frequency of the common subsequence is usually larger than that of the full tran"
P12-3010,N06-1014,0,0.0370217,"n translators and second language learners (Bowker and Barlow 2004; Bourdaillet et al., 2010; Gao 2011). Identifying the translation equivalents, translation spotting, is the most challenging part of a bilingual concordancer. Recently, most of the existing bilingual concordancers spot translation equivalents in terms of word alignment-based method. (Jian et al., 2004; Callison-Burch et al., 2005; Bourdaillet et al., 2010). However, word alignment-based translation spotting has some drawbacks. First, aligning a rare (low frequency) term may encounter the garbage collection effect (Moore, 2004; Liang et al., 2006) that cause the term to align to many unrelated words. Second, the statistical word alignment model is not good at many-to-many alignment due to the fact that translation equivalents are not always correlated in lexical level. Unfortunately, the above effects will be intensified in a domain-specific concordancer because the queries are usually domain-specific terms, which are mostly multi-word low-frequency terms and semantically non-compositional terms. Wu et al. (2003) employed a statistical association criterion to spot translation equivalents in their bilingual concordancer. The associatio"
P12-3010,W03-1726,1,0.520862,"bsequence of H, then the count of t should be reasonably reduced by considering the strength of the correlation between the words in H-t and the query. 3 Experimental Results Experimental Setting We use the Chinese/English web pages of the National Palace Museum 2 as our underlying parallel corpus. It contains about 30,000 sentences in each language. We exploited the Champollion Toolkit (Ma et al., 2006) to align the sentence pairs. The English sentences are tokenized and lemmatized by using the NLTK (Bird and Loper, 2004) and the Chinese sentences are segmented by the CKIP Chinese segmenter (Ma and Chen, 2003). To evaluate the performance of the translation spotting, we selected 12 domain-specific terms to query the concordancer. Then, the returned spotted translation equivalents are evaluated against a manually annotated gold standard in terms of recall and precision metrics. We also build two different translation spotting modules by using the GIZA++ toolkit (Och and Ney, 2000) with the intersection/union of the bidirectional word alignment as baseline systems. To evaluate the performance of the ranking criterion, we compiled a reference translation set for each query by collecting the manually a"
P12-3010,ma-2006-champollion,0,0.0709389,"Missing"
P12-3010,P00-1056,0,0.1259,"lion Toolkit (Ma et al., 2006) to align the sentence pairs. The English sentences are tokenized and lemmatized by using the NLTK (Bird and Loper, 2004) and the Chinese sentences are segmented by the CKIP Chinese segmenter (Ma and Chen, 2003). To evaluate the performance of the translation spotting, we selected 12 domain-specific terms to query the concordancer. Then, the returned spotted translation equivalents are evaluated against a manually annotated gold standard in terms of recall and precision metrics. We also build two different translation spotting modules by using the GIZA++ toolkit (Och and Ney, 2000) with the intersection/union of the bidirectional word alignment as baseline systems. To evaluate the performance of the ranking criterion, we compiled a reference translation set for each query by collecting the manually annotated translation spotting set and selecting 1 to 3 frequently used translations. Then, the outputs of each query are ranked by the nf_dice function and evaluated against the reference translation set. We also compared the ranking performance with the Dice coefficient. 3.2 Evaluation of Translation Spotting We evaluate the translation spotting in terms of the Recall and P"
P12-3010,J96-1001,0,0.228554,"add to H ej←left neighbor of H If nc(ej ) θ: add ej to H ek←right neighbor of H If nc( ek ) θ: add ek to H Figure 4: Pseudo-code of translation spotting process. 2.3 Ranking The ranking mechanism of a bilingual concordancer is used to provide the most related translation of the query on the top of the outputs for the user. So, an association metric is needed to evaluate the relations between the query and the spotted translations. The Dice coefficient is a widely used measure for assessing the association strength between a multi-word expression and its translation candidates. (Kupiec, 1993; Smadja et al., 1996; Kitamura and Matsumoto, 1996; Yamamoto and Matsumoto, 2000; Melamed, 2001) The following is the definition of the Dice coefficient: Then, we modify the Dice coefficient by replacing the co-occurrence frequency with normalized frequency as follows: nf_dice(t, q)  (4) where q denotes a multi-word expression to be translated, t denotes a translation candidate of q. However, the Dice coefficient has the common subsequence effect (as mentioned in Section 1) due to the fact that the co-occurrence frequency of the common subsequence is usually larger than that of the full translation; hence, the D"
P12-3010,P03-2040,1,0.65415,"has some drawbacks. First, aligning a rare (low frequency) term may encounter the garbage collection effect (Moore, 2004; Liang et al., 2006) that cause the term to align to many unrelated words. Second, the statistical word alignment model is not good at many-to-many alignment due to the fact that translation equivalents are not always correlated in lexical level. Unfortunately, the above effects will be intensified in a domain-specific concordancer because the queries are usually domain-specific terms, which are mostly multi-word low-frequency terms and semantically non-compositional terms. Wu et al. (2003) employed a statistical association criterion to spot translation equivalents in their bilingual concordancer. The associationbased criterion can avoid the above mentioned effects. However, it has other drawbacks in translation spotting task. First, it will encounter the contextual effect that causes the system incorrectly spot the translations of the strongly collocated context. Second, the association-based translation spotting tends to spot the common subsequence of a set of similar translations instead of the full translations. Figure 1 illustrates an example of contextual effect, in which"
P12-3010,C00-2135,0,0.0541979,"j to H ek←right neighbor of H If nc( ek ) θ: add ek to H Figure 4: Pseudo-code of translation spotting process. 2.3 Ranking The ranking mechanism of a bilingual concordancer is used to provide the most related translation of the query on the top of the outputs for the user. So, an association metric is needed to evaluate the relations between the query and the spotted translations. The Dice coefficient is a widely used measure for assessing the association strength between a multi-word expression and its translation candidates. (Kupiec, 1993; Smadja et al., 1996; Kitamura and Matsumoto, 1996; Yamamoto and Matsumoto, 2000; Melamed, 2001) The following is the definition of the Dice coefficient: Then, we modify the Dice coefficient by replacing the co-occurrence frequency with normalized frequency as follows: nf_dice(t, q)  (4) where q denotes a multi-word expression to be translated, t denotes a translation candidate of q. However, the Dice coefficient has the common subsequence effect (as mentioned in Section 1) due to the fact that the co-occurrence frequency of the common subsequence is usually larger than that of the full translation; hence, the Dice coefficient tends to choose the common subsequence. To r"
P12-3010,J93-2003,0,\N,Missing
P12-3010,P06-4018,0,\N,Missing
P12-3010,W02-0109,0,\N,Missing
P12-3010,W96-0107,0,\N,Missing
P12-3010,P04-1066,0,\N,Missing
P98-1038,Y96-1018,1,\N,Missing
W00-1203,O97-4005,0,0.0322996,"Missing"
W00-1203,C00-1026,1,0.802939,"Missing"
W00-1203,C92-1019,1,0.852382,"Missing"
W00-1203,C96-1039,0,0.03033,"Missing"
W00-1203,O94-1010,0,0.0515548,"Missing"
W00-1203,O93-1004,0,0.0829331,"Missing"
W00-1203,O98-3002,1,\N,Missing
W00-1203,J93-1007,0,\N,Missing
W00-1205,Y96-1018,1,0.934457,"Missing"
W00-1205,C92-1019,1,0.846302,"Missing"
W00-1205,C96-2184,1,0.877286,"Missing"
W00-1205,O96-2006,1,\N,Missing
W00-1205,J93-2004,0,\N,Missing
W00-1205,O97-4003,1,\N,Missing
W00-1205,O94-1005,1,\N,Missing
W00-1205,O99-4004,1,\N,Missing
W02-1811,W02-0603,0,\N,Missing
W03-1705,P98-2206,0,0.108378,"Missing"
W03-1705,P97-1041,0,0.0806142,"Missing"
W03-1705,O92-1003,0,0.102765,"Missing"
W03-1705,O97-4005,0,0.360836,"Missing"
W03-1705,C02-1049,1,0.834408,"Missing"
W03-1705,O98-3002,1,0.926275,"Missing"
W03-1705,C00-1027,0,0.0346846,"Missing"
W03-1705,C92-1019,1,0.879861,"Missing"
W03-1705,O97-4003,1,0.882541,"Missing"
W03-1705,J93-1007,0,0.0856207,"Missing"
W03-1705,J96-1001,0,\N,Missing
W03-1705,J90-1003,0,\N,Missing
W03-1705,C96-2184,1,\N,Missing
W03-1705,C98-2201,0,\N,Missing
W03-1705,O93-1004,0,\N,Missing
W03-1726,C92-1019,1,0.5056,"Missing"
W03-1726,O98-3002,1,0.84534,"Missing"
W03-1726,C02-1049,1,0.845325,"Missing"
W03-1726,W02-1811,1,0.806482,"Missing"
W03-1726,W03-1705,1,0.798817,"Missing"
W03-1726,O03-4001,0,\N,Missing
W03-1726,O97-4003,1,\N,Missing
W04-1116,J02-3001,0,0.133175,"Missing"
W04-1116,W03-1008,0,\N,Missing
W06-0101,I05-1030,0,0.0611299,"Missing"
W06-0101,W04-1116,1,\N,Missing
W06-0101,W02-0908,0,\N,Missing
W06-0101,N03-1036,0,\N,Missing
W12-6338,W00-1205,1,0.66075,"ter. The training scale is 407 outcomes, 2438366 parameters and 1593985 predicates. Experiments and Results In this section, we describe the experiment design, and then evaluate the proposed models based on Sinica Treebank. We also analyze the results, and compare them with the results derived by the open source Berkeley statistical parser on the same test set. 4.1 4.2 Experimental Settings Treebank: We employ Sinica Treebank as our experimental corpus. It contains 61,087 syntactic tree structures and 361,834 words. The syntactic theory of Sinica Treebank is based on the HeadDriven Principle (Huang et al., 2000); that is, a sentence or phrase is composed of a phrasal head and its arguments or adjuncts. We divide the treebank into four parts: the training data (55,888 sentences), the development set (1,068 sentences), the test data T06 (867 sentences), and the test data T07 (689 sentences). The test datasets (T06, T07) were used in CoNLL06 and CoNLL07 dependent parsing evaluation individually. The main difference between Sinica Treebank data and CoNLL data is that the CoNLL is in dependency format. Word Sense: With regard to semantic features, we use the head senses of words expressed in EHowNet (http"
W12-6338,P03-1054,0,0.0249263,"to their structural evaluation scores, which may be an accumulated score of rule probabilities and featurebased scores. In general, the evaluation functions are derived from very limited and biased resources, such as treebanks. Therefore we need to find a way to improve the evaluation functions under the constraint of very limited resources. Suppose that the parsing environment is a model of probabilistic context-free grammar (PCFG). Several researchers are attaching many useful features to the grammar rules to improve the precision of the grammar rules (Johnson, 1998; Sun and Jurafsky, 2003; Klein and Manning, 2003; Hsieh et al., 2005). In this paper, we follow grammar representation in Hsieh et al. (2005), and propose a context-dependent probability re-estimation model (CDM) to enhance the performance of the original PCFG model. CDM combines rule probabilities and machine learning techniques in structure evaluation. Similar to other machine learning methods (Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2006), the CDM has the flexibility to adjust the features, and to obtain better re-estimated structure probabilities. The remainder of this paper is organized as follows. Section 2 provides background"
W12-6338,A00-2018,0,0.404183,"probabilistic context-free grammar (PCFG). Several researchers are attaching many useful features to the grammar rules to improve the precision of the grammar rules (Johnson, 1998; Sun and Jurafsky, 2003; Klein and Manning, 2003; Hsieh et al., 2005). In this paper, we follow grammar representation in Hsieh et al. (2005), and propose a context-dependent probability re-estimation model (CDM) to enhance the performance of the original PCFG model. CDM combines rule probabilities and machine learning techniques in structure evaluation. Similar to other machine learning methods (Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2006), the CDM has the flexibility to adjust the features, and to obtain better re-estimated structure probabilities. The remainder of this paper is organized as follows. Section 2 provides background on PCFG parsing with grammar rule representation. Section 3 describes the proposed CDM and our selected features. The experimental evaluation and results are in Section 4. The last section contains some concluding remarks. 216 Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 216–221, Tianjin, China, 20-21 DEC. 2012 2 Background 2.1 The ba"
W12-6338,W03-1706,0,0.40213,"uld be ranked according to their structural evaluation scores, which may be an accumulated score of rule probabilities and featurebased scores. In general, the evaluation functions are derived from very limited and biased resources, such as treebanks. Therefore we need to find a way to improve the evaluation functions under the constraint of very limited resources. Suppose that the parsing environment is a model of probabilistic context-free grammar (PCFG). Several researchers are attaching many useful features to the grammar rules to improve the precision of the grammar rules (Johnson, 1998; Sun and Jurafsky, 2003; Klein and Manning, 2003; Hsieh et al., 2005). In this paper, we follow grammar representation in Hsieh et al. (2005), and propose a context-dependent probability re-estimation model (CDM) to enhance the performance of the original PCFG model. CDM combines rule probabilities and machine learning techniques in structure evaluation. Similar to other machine learning methods (Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2006), the CDM has the flexibility to adjust the features, and to obtain better re-estimated structure probabilities. The remainder of this paper is organized as follows. Sect"
W12-6338,W04-1116,1,0.731418,"Missing"
W12-6338,P10-1113,0,0.0233934,"hinese. Suppose we need to calculate CDPi based on the related features, while the i-th rule is applied for covering a span of words [L…R]. The used context and contextual features are as follows: tion to derive the rule probabilities from transformed Sinica Treebank (http://TreeBank.sinica.edu.tw). 3 Context-Dependent Probability Reestimation Model Many works try to improve rule probability estimation by using context-dependent probabilities in PCFG model, and show that rules with dependent context features perform better than PCFG alone (Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2006; Li et al., 2010). Charniak (2000) presented a maximum-entropy-inspired model to estimate probabilities in Markov grammar. The model uses a standard bottom-up best-first probabilistic chart parser to generate possible candidate parses in the first pass, and then evaluates the candidates with the proposed probabilistic model in the second pass. Therefore Charniak’s method (2000) generates possible candidate parses first and then evaluates these candidates without early pruning. We adopt the maximum entropy method for structure evaluation, and integrate it into present PCFG model, called as CDM. CDM integrates t"
W12-6338,P06-1054,0,0.77856,"ontext-free grammar (PCFG). Several researchers are attaching many useful features to the grammar rules to improve the precision of the grammar rules (Johnson, 1998; Sun and Jurafsky, 2003; Klein and Manning, 2003; Hsieh et al., 2005). In this paper, we follow grammar representation in Hsieh et al. (2005), and propose a context-dependent probability re-estimation model (CDM) to enhance the performance of the original PCFG model. CDM combines rule probabilities and machine learning techniques in structure evaluation. Similar to other machine learning methods (Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2006), the CDM has the flexibility to adjust the features, and to obtain better re-estimated structure probabilities. The remainder of this paper is organized as follows. Section 2 provides background on PCFG parsing with grammar rule representation. Section 3 describes the proposed CDM and our selected features. The experimental evaluation and results are in Section 4. The last section contains some concluding remarks. 216 Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 216–221, Tianjin, China, 20-21 DEC. 2012 2 Background 2.1 The baseline model, PCFG P"
W12-6338,P06-1055,0,0.109175,"Missing"
W12-6338,O04-2005,1,0.802886,"Berkeley’s parser are closed to F-PCFG model in Table 2. Either Berkely’s parser or F-PCFG represents the ceiling results of a general method, and they both outperform the naïve PCFG model. 4.4 Experiments for Task4 of CLP2012 Task 4 of CLP2012 includes two sub-tasks: sentence parsing and semantic role labeling task. For each sub-task, the testing data are complete Chinese sentence with gold standard word segmentation. Therefore, a pipeline process is needed to solve the POS tagging, syntactic parsing and semantic role assignment in our experiment. We adopt the context-rule tagger proposed by Tsai and Chen (2004) for the POS tagging. For syntactic parsing, we use the CDM parser with same training data in Section 4.1. For semantic role labeling, we follow You and Chen’s (2004) method to assignment semantic role automatically. The detail parsing results of our systems on the test set can be found on the official evaluation report. Our system obtains acceptable results on both sentence parsing and semantic role labeling tasks. F1-Score Task 4-1 Task 4-2 MicroAveraging 0.7287 0.6034 MacroAveraging 0.7448 0.6249 Table 4. Official scores of sentence parsing (task4-1) and semantic role labeling (task4-2). Ta"
W12-6338,I05-1016,1,0.949763,"ation scores, which may be an accumulated score of rule probabilities and featurebased scores. In general, the evaluation functions are derived from very limited and biased resources, such as treebanks. Therefore we need to find a way to improve the evaluation functions under the constraint of very limited resources. Suppose that the parsing environment is a model of probabilistic context-free grammar (PCFG). Several researchers are attaching many useful features to the grammar rules to improve the precision of the grammar rules (Johnson, 1998; Sun and Jurafsky, 2003; Klein and Manning, 2003; Hsieh et al., 2005). In this paper, we follow grammar representation in Hsieh et al. (2005), and propose a context-dependent probability re-estimation model (CDM) to enhance the performance of the original PCFG model. CDM combines rule probabilities and machine learning techniques in structure evaluation. Similar to other machine learning methods (Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2006), the CDM has the flexibility to adjust the features, and to obtain better re-estimated structure probabilities. The remainder of this paper is organized as follows. Section 2 provides background on PCFG parsing with"
W12-6338,H91-1060,0,0.0289007,"皮球 ball] by the maximal entropy model. Some examples of con218 textual features are “LW0=李四, RW0=皮球, LW1=叫, LW1=撿, RW-1=撿, RW1=X, LW-1&LW0= 叫&李四, LW0&LW1=李四&撿, RW-1&RW0=撿 & 皮 球 , RW0&RW1= 皮 球 &X, RhsW1= 李 四 , RhsW2= 撿 , RhsC1=Nb, RhsC2=VC, RHS=NPHead:Nb_VP-Head:VC, …”, etc. Afterwards, we integrate and calculate the evaluation score by Formula 2. 車:quantity={mass|眾}}, and its head sense is “LandVehicle|車”. For detailed description about E-HowNet, readers may refer to Huang et al. (2008). Estimate Parsing Performance: To evaluate a model, we compare the parsing results with the gold standard. Black et al. (1991) proposed a structural evaluation system is called PARSEVAL. In all the experiments, we used the bracketed f-score (BF) as the parsing performance metric. S'-Head:VF+0+NP NP-Head:Nb Bracketed F - score (BF)  VP-Head:VC NP-Head:Na Head:Nb Head:VC 他 He 叫 ask 李四 Li-si LW0 撿 pick Bracketed Precision (BP)  # bracket correct consitituents in parser's parse of testing data # bracket constituents in parser's of testing data Head:Na 皮球 ball RW0 Bracketed Recall (BR)  # bracket correct consitituents in parser' s parse of testing data # bracket constituents in treebank' s of testing data Figure 3. A p"
W12-6338,J03-4003,0,\N,Missing
W13-4404,I05-7001,1,0.77688,"used by the semantic characteristics of the language. We consider a similar semantic relation of cause-result between process verbs and state verbs to show the dichotomy and interactions between them. In fact, Levin’s result verbs are verb-result compounds in Chinese and our result verbs refer to pure states. The above cited verbs pairs, such as stab and kill in (2), are both process verbs. By our notion of process and state dichotomy wounded and die are result states of stab and kill, respectively. 2.3 The Polarity and Interaction between Process and State E-HowNet’s Classification E-HowNet (Chen et al. 2005) is a frame-based entity-relation model that constructs events, objects and relations in a hierarchically-structured ontology. By following the conventional event classification theories, verbs are partitioned into process and state first, which is a higher priority dichotomous classification criterion than the syntactic classification in E-HowNet, since EHowNet is a primarily semantic classification system. Furthermore, semantic classification is more intuitive, and more in line with the general 22 cess and state, and then in the next section, we introduce several approaches we adopted while"
W13-4410,O98-3002,1,0.574499,"ssible candidate characters, and a large-scale corpus for constructing language model for validation and correction of words. In order to accomplish these two spelling check tasks, we designed two error detection systems with the capability of providing suggested correction candidates. Each system uses different dictionary for its knowledge source. The first system uses the CKIP dictionary, called CKIPWS; another uses the correction pair dictionary extracted from Google 1T uni-gram data, called G1-WS. In CKIP-WS, we detect possible occurrences of errors through unknown word detection process (Chen and Bai, 1998). So that deeper morphological analysis is carried out only where morphemes of unknown word are detected (Chen and Ma, 2002). As a result, some false alarms caused by proper names and determinantmeasure compounds can be avoided. For G1-WS, we build an error suggestion dictionary (or template) to match potential error spellings and suggest correction candidates. Finally we use an ngram language model to select the corrected characters as our system output. Abstract In order to accomplish the tasks of identifying incorrect characters and error correction, we developed two error detection systems"
W13-4410,C02-1049,1,0.684566,"In order to accomplish these two spelling check tasks, we designed two error detection systems with the capability of providing suggested correction candidates. Each system uses different dictionary for its knowledge source. The first system uses the CKIP dictionary, called CKIPWS; another uses the correction pair dictionary extracted from Google 1T uni-gram data, called G1-WS. In CKIP-WS, we detect possible occurrences of errors through unknown word detection process (Chen and Bai, 1998). So that deeper morphological analysis is carried out only where morphemes of unknown word are detected (Chen and Ma, 2002). As a result, some false alarms caused by proper names and determinantmeasure compounds can be avoided. For G1-WS, we build an error suggestion dictionary (or template) to match potential error spellings and suggest correction candidates. Finally we use an ngram language model to select the corrected characters as our system output. Abstract In order to accomplish the tasks of identifying incorrect characters and error correction, we developed two error detection systems with different dictionaries. First system, called CKIP-WS, adopted the CKIP word segmentation system which based on CKIP di"
W13-4410,W03-1726,1,0.704363,"g Hsieh1,2 Ming-Hong Bai1,2 Keh-Jiann Chen1 1 Institute of Information Science, Academia Sinica, Taiwan 2 Department of Computer Science, National Tsing-Hua University, Taiwan morris@iis.sinica.edu.tw, mhbai@sinica.edu.tw, kchen@iis.sinica.edu.tw 2011). Therefore, most Chinese character detection systems are built based on confusion sets and a language model. Some new systems also incorporate NLP technologies for Chinese character error detection in recent years (Huang et al., 2007; Wu et al., 2010). Huang et al. (2007) used a new word detection function in the CKIP word segmentation toolkit (Ma and Chen, 2003) to detect error candidates. With the help of a dictionary and confusion set, the system will be able to judge whether a monosyllabic word is probably error or not. The system we designed for this contest adopts CKIP word segmentation module for unknown word detection too, confusion sets for providing possible candidate characters, and a large-scale corpus for constructing language model for validation and correction of words. In order to accomplish these two spelling check tasks, we designed two error detection systems with the capability of providing suggested correction candidates. Each sys"
W13-4410,W10-4107,0,0.0229608,"Missing"
Y03-1016,W93-0305,0,0.0996631,"Missing"
Y03-1016,Y96-1018,1,0.751089,"accuracy of tagging results only. In this paper, we proposed a context-rule model to achieve both the above goals for pos tagging. 2 Tagging Algorithms In this study, we are going to test two different tagging algorithms based on same training data and testing data. The two tagging algorithms are Markov bi-gram model, and context-rule classifier. For Markov bi-gram model, we propose a new form named word-dependent Markov bi-gram model, which will be described later. The training data and testing data are extracted from Sinica corpus, a 5 million word balanced Chinese corpus with pos tagging (Chen et al., 1996). The confidence measure will be defined for each algorithm and the best accuracy will be estimated at the constraint of only a fixed amount of testing data being proofread. It is easier to proofread and make more consistent tagging results, if proofreading processes were done by checking the keyword-in-context file for each ambivalence word and only the tagging results of ambivalence word need to be proofread. The words with single category need not be rechecked their pos tagging. For instance, in Table 1, the keyword-in-context file of the word '1111:F9: (research), which has pos-categories"
Y03-1016,C02-1021,0,0.0217541,"Missing"
Y06-1052,Y96-1018,1,0.822741,"Missing"
Y06-1052,I05-7001,1,0.768198,"sound recording is “~” which denotes the head concept of the definition, i.e. “machine|機器. Thus, the definition can be glossed as: “a sound recorder is a machine which functions as the instrument of sound-recording activity”. Complex concepts are represented by simpler concepts which are not necessary to be primitive concepts. The simple concepts used in the definitions can be further decomposed into even simpler concepts, until primitive or basic concepts are reached. Therefore the representation of a concept can be dynamically decomposed and unified into different levels of representations [6, 7]. In (1), the concept “sound recorder” is defined by simpler concepts “machine” and “sound recording”. The concept “sound recording” is not a primitive concept and can be further decomposed into primitive concepts “record” and “sound” as in (2). Thus the definition in (1) can be extended to reach the ground-level representation as in (3). Such multi-level representations are easier to understand, for it accords with human cognition models. (2) 錄音 “sound recording” def: {record|記錄:content={sound|聲}} (3) 錄音機 “sound recorder” def: {machine|機器: telic={record|記錄: content={sound|聲}, instrument={~}}}"
Y06-1052,C00-1026,1,0.672187,"scover the sense and semantic structure of a noun compound is to disambiguate the semantic ambiguity of the morphological head of a 379 compound noun and find the proper semantic relation between constituents of the compound. For example, when we see the unknown/undefined compounds such as 牧工 “hired herdsman”, 核工 “nuclear industry” ,or 唱工 “art of singing”, firstly, we have to find the appropriate meaning for each head of these unknown compound. Secondly, we have to build the correct relation between their modifiers and the heads, such as the relation between 牧 and 工, 核 and 工, etc. Chen & Chen [9] proposed an example-based similarity measure to disambiguate the polysemous heads. They extracted some examples with the polysemous head morpheme from corpora and dictionaries, and classified them into different groups according to their meaning. Let’s take “工” as example and add E-HowNet definitions for each class, shown as table 1: Table 1. =================================================================== Sense 工人 “labor” 工業 “industry” example 搬運工 “porter” 女工 “female labor” 童工 “child labor” 化工 “chemical industry” 機工 “engineering industry” 技術 “skill” 刀工 “cutting skill” 畫工 “painting skill”"
Y07-1013,I05-7001,1,0.833881,"in E-HowNet. In Section 3, we explain how modals are represented as single words and as components of larger linguistic constituents. In Section 4, we deal with the co-occurrence of negation markers and modals. We show with that E-HowNet is able to cope with meanings that are determined by its relative position with other elements in a sentence. Following that, in Section 5, we conclude that E-HowNet can represent a semantic category like modality that (a) involves pragmatics and (b) belongs to function words but is like content words in some aspects. 2. The scope of modality Following Hsieh (2005), we do not assume that modals have to be auxiliaries but identify them on semantic grounds. They all refer to speakers’ judgment. There are two meanings unanimously recognized as central to modality: epistemic and deontic. The former refers to a speaker’s judgment of whether a situation will happen and the latter to a speaker’s attitude toward whether something is required to be done. Another two categories admitted by many researchers are words that denote abilities and volition (Hwang 1999, Li 2003, Hsieh 2003, Hsieh 2005). Another modal category that we recognize is expectation, which incl"
Y09-1001,O09-3002,1,0.826338,"Missing"
Y09-1001,Y06-1052,1,\N,Missing
Y96-1018,C92-1019,1,0.27843,"Missing"
Y96-1018,A88-1019,0,0.028653,"Missing"
Y96-1018,C96-2184,1,0.915665,"to neighboring words to form a segmentation unit when possible. (b) A string of characters that has a high frequency in the language or high cooccurrence frequency among the components should be treated as a segmentation unit when possible. (c) A string separated by overt segmentation markers should be segmented. (d) A string with complex internal structures should be segmented when possible. 170 Lastly, the lexicon contains a standard list of words as well as productive morphological suffixes, and obligatory segment markers. For more detail on the word segmentation standard, please refer to Huang et al. (1996). 3.2 Part-of-speech Tagging The possible part-of-speech (abbr:pos) of each word was given after segmentation process. To resolve the ambiguities, a two-stage automatic tagging process was designed to disambiguate multi-category words. In the first stage, a small portion of the corpus was resolved by a hybrid method which combines rule-based and relaxation methods to select the most plausible pos tag for each multi-category word (Liu et al. 95). This initial corpus was post-edited manually and then became training data for the second stage statistical tagging model adapted from (Church 88). Th"
Y96-1018,P94-1010,0,0.025655,"Missing"
Y96-1018,J90-1003,0,\N,Missing
Y96-1018,J90-3007,0,\N,Missing
Y99-1005,J93-2005,0,0.12393,"Missing"
Y99-1005,O98-3004,1,0.831966,"ocus on the inchoative stage or the homogeneous stage of the event. In addition, since VV compounding has the function of type-lifting an event to a referential term, or to refer to its generic properties, it is natural to predict that VV compounding is a predominant source for the verbs of indicating homogeneity. 1. Introduction Many recent linguistic studies explored how lexical meaning predicts syntactic regularities (Levin 1993, Pustejovsky 1995). One important approach is to study the contrasts in near synonym pairs to identify the minimal semantic attributes that motivate the contrasts (Tsai et al 1998, Liu et al 1997 & 1998). In this current study, we extend the range of the study to a semantic field, which contains more than one synonym pairs. Thus we can attest to the primary status of the proposed semantic attributes by showing that the generalization can be extended to the other synonym pairs in the same semantic field. Tsai et al (1998) discussed the contrast between the synonym pair KUAILE and GAOXING AR, and based on their findings we re-examine the contrast in a broader range, i.e. the verbs of emotion. We have four results from this study: 1) we find that the contrast is not speci"
